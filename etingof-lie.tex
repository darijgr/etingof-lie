% -------------------------------------------------------------
% NOTE ON THE DETAILED AND SHORT VERSIONS:
% -------------------------------------------------------------
% This paper comes in two versions, a detailed and a short one.
% The short version should be more than sufficient for any
% reasonable use; the detailed one was written purely to
% convince the author of its correctness.
% To switch between the two versions, find the line containing
% "\newenvironment{noncompile}{}{}" in this LaTeX file.
% Look at the two lines right beneath this line.
% To compile the detailed version, they should be as follows:
%   \includecomment{verlong}
%   \excludecomment{vershort}
% To compile the short version, they should be as follows:
%   \excludecomment{verlong}
%   \includecomment{vershort}
% As a rule, the line
%   \excludecomment{noncompile}
% should stay as it is.
% -------------------------------------------------------------
% NOTES ON SOME HACKS USED IN THIS FILE:
% -------------------------------------------------------------
% One of my pet peeves with amsthm is its use of italics in the theorem and
% proposition environments; this makes math and text indistinguishable in said
% enviroments. To avoid this, I redefine the enviroments to use the standard
% font and to use a hanging indent, along with a bold vertical bar to its
% left, to distinguish these environments from surrounding text. (Along with
% the advantage of distinguishing math from text, this also allows nesting
% several such environments inside each other, like a definition inside a
% remark. I'm not sure how good of an idea this is, though. There are also
% downsides related to the hanging indentation, such as footnotes out of it
% being painful to do right.) This is done starting from the line
%   \theoremstyle{definition}
% and until the line
%   {\end{leftbar}\end{exmp}}

\documentclass
[numbers=enddot,12pt,final,onecolumn,german,notitlepage]{scrartcl}%
\usepackage[all,cmtip]{xy}
\usepackage{lscape}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{amsthm}
\usepackage{pdflscape}
\usepackage{hyperref}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Tuesday, February 23, 2016 19:31:56}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newcommand{\Ker}{\operatorname*{Ker}}
\newcommand{\id}{\operatorname*{id}}
\newcommand{\inc}{\operatorname*{inc}}
\newcommand{\gr}{\operatorname*{gr}}
\newcommand{\Hom}{\operatorname*{Hom}}
\newcommand{\calA}{\mathcal A}
\newcommand{\arinj}{\ar@{_{(}->}}
\newcommand{\arsurj}{\ar@{->>}}
\newcommand{\arelem}{\ar@{|->}}
\newcommand{\fraka}{\mathfrak{a}}
\newcommand{\frakb}{\mathfrak{b}}
\newcommand{\frakc}{\mathfrak{c}}
\newcommand{\PBW}{\operatorname*{PBW}}
\newcommand{\xycs}{\xymatrixcolsep}
\newcommand{\xyrs}{\xymatrixrowsep}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[subsection]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{impnt}[theo]{Important Notice}
\newenvironment{impnot}[1][]
{\begin{impnt}[#1]\begin{leftbar}\color{blue}}
{\color{black}\end{leftbar}\end{impnt}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{exe}[theo]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exe}[#1]\begin{leftbar}}
{\end{leftbar}\end{exe}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{Convention}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{Warning}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{example}[theo]{Example}
\voffset=-0.5cm
\hoffset=-0.7cm
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\setlength\textheight{24cm}
\setlength\textwidth{15.5cm}
\begin{document}

\title{18.747: Infinite-dimensional Lie algebras (Spring term 2012 at MIT)}
\author{Pavel Etingof\\Scribed by Darij Grinberg\\ with edits by Raeez Lorgat}
\date{Version 0.44 (\today) (not proofread!)}
\maketitle
\tableofcontents

\begin{noncompile}
\textbf{TO-DO LIST:}

bilinear form on kac-moody and more generally.

multisolitons.

Japanese cocycle proofs.
\end{noncompile}

\subsection{Version notes}

\textbf{Only Chapters 1 and 2 of these notes are currently anywhere near
completion.} Chapter 3 is done in parts, but some material is still sketchy
and/or wrong. The beginning of Chapter 4 is done, but the rest is still an
unusable mess.

These notes are mostly based on what is being said and written on the
blackboard in the lectures, and less so on Pavel Etingof's handwritten notes
posted on \newline\texttt{\url{http://www-math.mit.edu/~etingof/}} . They
cover less material than Etingof's handwritten notes, but are more detailed in
what they do cover.

Thanks to Pavel Etingof for his patience in explaining me things until I
actually understand them. Thanks to Dorin Boger for finding mistakes.

\subsection{Remark on the level of detail}

\begin{vershort}
This is the ``brief'' version of the lecture notes, meaning that there is a
more detailed one, which can be obtained by replacing\newline\texttt{%
%TCIMACRO{\TEXTsymbol{\backslash}}%
%BeginExpansion
$\backslash$%
%EndExpansion
excludecomment\{verlong\}}\newline\texttt{%
%TCIMACRO{\TEXTsymbol{\backslash}}%
%BeginExpansion
$\backslash$%
%EndExpansion
includecomment\{vershort\}}\newline by\newline\texttt{%
%TCIMACRO{\TEXTsymbol{\backslash}}%
%BeginExpansion
$\backslash$%
%EndExpansion
includecomment\{verlong\}}\newline\texttt{%
%TCIMACRO{\TEXTsymbol{\backslash}}%
%BeginExpansion
$\backslash$%
%EndExpansion
excludecomment\{vershort\}}\newline in the preamble of the LaTeX sourcecode
and then compiling to PDF. That detailed version, however, is not recommended,
since it differs from the brief one mostly in boring computations and
straightforward arguments being carried out rather than sketched. The amount
of detail in the brief version is usually enough for understanding (unless it
is a part of the lecture I didn't understand myself and just copied from the
blackboard; but in that case the detailed version is of no help either). There
is currently a large number of proofs which are only sketched in either version.
\end{vershort}

\begin{verlong}
This is the detailed version of the lecture notes.
\end{verlong}

\subsection{Introduction}

These notes follow a one-semester graduate class by Pavel Etingof at MIT in
the Spring term of 2012. The class was also accompanied by the lecturer's
handwritten notes, downloadable from
\texttt{\href{http://www-math.mit.edu/~etingof/}{\texttt{http://www-math.mit.edu/\symbol{126}%
etingof/}}} .

The goal of these lectures is to discuss the structure and the representation
theory (mainly the latter) of some of the most important infinite-dimensional
Lie algebras.\footnote{It should be noticed that most of the
infinite-dimensional Lie algebras studied in these notes are $\mathbb{Z}%
$-graded and have both their positive and their negative parts
infinite-dimensional. This is in contrast to many Lie algebras appearing in
algebraic combinatorics (such as free Lie algebras over non-graded vector
spaces, and the Lie algebras of primitive elements of many combinatorial Hopf
algebras), which tend to be concentrated in nonnegative degrees. So a better
title for these notes might have been ``Two-sided infinite-dimensional Lie
algebras''.} Occasionally, we are also going to show some connections of this
subject to other fields of mathematics (such as conformal field theory and the
theory of integrable systems).

\begin{verlong}
Also, there are connections to the theory of quantum groups, but we won't get
to them.
\end{verlong}

The prerequisites for reading these notes vary from section to section. We are
going to liberally use linear algebra, the basics of algebra (rings, fields,
formal power series, categories, tensor products, tensor algebras, symmetric
algebras, exterior algebras, etc.) and fundamental notions of Lie algebra
theory. At certain points we will also use some results from the
representation theory of finite-dimensional Lie algebras, as well as some
properties of symmetric polynomials (Schur polynomials in particular) and
representations of associative algebras. Analysis and geometry will appear
very rarely, and mostly to provide intuition or alternative proofs.

The biggest difference between the theory of finite-dimensional Lie algebras
and that of infinite-dimensional ones is that in the finite-dimensional case,
we have a complete picture (we can classify simple Lie algebras and their
finite-dimensional representations, etc.), whereas most existing results for
the infinite-dimensional case are case studies. For example, there are lots
and lots of simple infinite-dimensional Lie algebras and we have no real hope
to classify them; what we can do is study some very specific classes and
families. As far as their representations are concerned, the amount of general
results is also rather scarce, and one mostly studies concrete
families\footnote{Though, to be honest, we are mostly talking about
infinite-dimensional representations here, and these are not very easy to
handle even for finite-dimensional Lie algebras.}.

The main classes of Lie algebras that we will study in this course are:

\textbf{1.} The Heisenberg algebra (aka oscillator algebra) $\mathcal{A}$ and
its Lie subalgebra $\mathcal{A}_{0}$.

\textbf{2.} The Virasoro algebra $\operatorname*{Vir}$.

\textbf{3.} The Lie algebra $\mathfrak{gl}_{\infty}$ and some variations on it
($\overline{\mathfrak{a}_{\infty}}$, $\mathfrak{a}_{\infty}$, $\mathfrak{u}%
_{\infty}$).

\textbf{4.} Kac-Moody algebras (this class contains semisimple Lie algebras
and also affine Lie algebras, which are central extensions of $\mathfrak{g}%
\left[  t,t^{-1}\right]  $ where $\mathfrak{g}$ is simple finite-dimensional).

\subsection{References}

The standard text on infinite-dimensional Lie algebras (although we will not
really follow it) is:

\begin{itemize}
\item V. G. Kac, A. K. Raina, \textit{(Bombay Lectures on) Highest Weight
Representations of Infinite Dimensional Lie Algebras}, World Scientific 1987.
\end{itemize}

Further recommended sources are:

\begin{itemize}
\item Victor G. Kac, \textit{Infinite dimensional Lie algebras}, Third
Edition, CUP 1995.

\item B. L. Feigin, A. Zelevinsky, \textit{Representations of contragredient
Lie algebras and the Kac-Macdonald identities}, a paper in: Representations of
Lie groups and Lie algebras (Budapest, 1971), pp. 25-77, Akad. Kiad\'{o},
Budapest, 1985.
\end{itemize}

\subsection{General conventions}

We will almost always work over $\mathbb{C}$ in this course. All algebras are
over $\mathbb{C}$ unless specified otherwise. Characteristic $p$ is too
complicated for us, although very interesting. Sometimes we will work over
$\mathbb{R}$, and occasionally even over rings (as auxiliary constructions
require this).

Some remarks on notation:

\begin{itemize}
\item In the following, $\mathbb{N}$ will always denote the set $\left\{
0,1,2,...\right\}  $ (and not $\left\{  1,2,3,...\right\}  $).

\item All rings are required to have a unity (but not necessarily be
commutative). If $R$ is a ring, then all $R$-algebras are required to have a
unity and satisfy $\left(  \lambda a\right)  b=a\left(  \lambda b\right)
=\lambda\left(  ab\right)  $ for all $\lambda\in R$ and all $a$ and $b$ in the
algebra. (Some people call such $R$-algebras \textit{central }$R$%
\textit{-algebras}, but for us this is part of the notion of an $R$-algebra.)

\item When a Lie algebra $\mathfrak{g}$ acts on a vector space $M$, we will
denote the image of an element $m\in M$ under the action of an element
$a\in\mathfrak{g}$ by any of the three notations $am$, $a\cdot m$ and
$a\rightharpoonup m$. (One day, I will probably come to an agreement with
myself and decide which of these notations to use, but for now expect to see
all of them used synonymously in this text. Some authors also use the notation
$a\circ m$ for the image of $m$ under the action of $a$, but we won't use this notation.)

\item If $V$ is a vector space, then the tensor algebra of $V$ will be denoted
by $T\left(  V\right)  $; the symmetric algebra of $V$ will be denoted by
$S\left(  V\right)  $; the exterior algebra of $V$ will be denoted by $\wedge
V$.

\item For every $n\in\mathbb{N}$, we let $S_{n}$ denote the $n$-th symmetric
group (that is, the group of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $). On occasion, the notation $S_{n}$ will denote some
other things as well; we hope that context will suffice to keep these meanings apart.
\end{itemize}

\section{The main examples}

\subsection{The Heisenberg algebra}

We start with the definition of the Heisenberg algebra. Before we formulate
it, let us introduce polynomial differential forms on $\mathbb{C}^{\times}$
(in the algebraic sense):

\begin{definition}
\label{def.diffform}Recall that $\mathbb{C}\left[  t,t^{-1}\right]  $ denotes
the $\mathbb{C}$-algebra of Laurent polynomials in the variable $t$ over
$\mathbb{C}$.

Consider the free $\mathbb{C}\left[  t,t^{-1}\right]  $-module on the basis
$\left(  dt\right)  $ (where $dt$ is just a symbol). The elements of this
module are called \textit{polynomial differential forms on }$\mathbb{C}%
^{\times}$. Thus, polynomial differential forms on $\mathbb{C}^{\times}$ are
just formal expressions of the form $fdt$ where $f\in\mathbb{C}\left[
t,t^{-1}\right]  $.

Whenever $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ is a Laurent polynomial, we
define a polynomial differential form $dg$ by $dg=g^{\prime}dt$. This notation
$dg$ does not conflict with the previously defined notation $dt$ (which was a
symbol), because the polynomial $t$ satisfies $t^{\prime}=1$.
\end{definition}

\begin{definition}
\label{def.res}For every polynomial differential form $fdt$ on $\mathbb{C}%
^{\times}$ (with $f\in\mathbb{C}\left[  t,t^{-1}\right]  $), we define a
complex number $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ to be
the coefficient of the Laurent polynomial $f$ before $t^{-1}$. In other words,
we define $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ to be
$a_{-1}$, where $f$ is written as $\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$
(with $a_{i}\in\mathbb{C}$ for all $i\in\mathbb{Z}$).

This number $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ is called
the \textit{residue} of the form $fdt$ at $0$.
\end{definition}

(The same definition could have been done for Laurent series instead of
Laurent polynomials, but this would require us to consider a slightly
different notion of differential forms, and we do not want to do this here.)

\begin{remark}
\label{rmk.res}\textbf{(a)} Every Laurent polynomial $f\in\mathbb{C}\left[
t,t^{-1}\right]  $ satisfies $\operatorname*{Res}\nolimits_{t=0}\left(
df\right)  =0$.

\textbf{(b)} Every Laurent polynomial $f\in\mathbb{C}\left[  t,t^{-1}\right]
$ satisfies $\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)  =0$.
\end{remark}

\textit{Proof of Remark \ref{rmk.res}.} \textbf{(a)} Write $f$ in the form
$\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ (with $b_{i}\in\mathbb{C}$ for all
$i\in\mathbb{Z}$). Then, $f^{\prime}=\sum\limits_{i\in\mathbb{Z}}ib_{i}%
t^{i-1}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}$. Now,
$df=f^{\prime}dt$, so that
\begin{align*}
\operatorname*{Res}\nolimits_{t=0}\left(  df\right)   &  =\operatorname*{Res}%
\nolimits_{t=0}\left(  f^{\prime}dt\right)  =\left(  \text{the coefficient of
the Laurent polynomial }f^{\prime}\text{ before }t^{-1}\right) \\
&  =\underbrace{\left(  -1+1\right)  }_{=0}b_{-1+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }f^{\prime}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)
b_{i+1}t^{i}\right) \\
&  =0,
\end{align*}
proving Remark \ref{rmk.res} \textbf{(a)}.

\textbf{(b)} \textit{First proof of Remark \ref{rmk.res} \textbf{(b)}:} By the
Leibniz identity, $\left(  f^{2}\right)  ^{\prime}=ff^{\prime}+f^{\prime
}f=2ff^{\prime}$, so that $ff^{\prime}=\dfrac{1}{2}\left(  f^{2}\right)
^{\prime}$ and thus $f\underbrace{df}_{=f^{\prime}dt}=\underbrace{ff^{\prime}%
}_{=\dfrac{1}{2}\left(  f^{2}\right)  ^{\prime}}dt=\dfrac{1}{2}%
\underbrace{\left(  f^{2}\right)  ^{\prime}dt}_{=d\left(  f^{2}\right)
}=\dfrac{1}{2}d\left(  f^{2}\right)  $. Thus,
\[
\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)  =\operatorname*{Res}%
\nolimits_{t=0}\left(  \dfrac{1}{2}d\left(  f^{2}\right)  \right)  =\dfrac
{1}{2}\underbrace{\operatorname*{Res}\nolimits_{t=0}\left(  d\left(
f^{2}\right)  \right)  }_{\substack{=0\text{ (by Remark \ref{rmk.res}
\textbf{(a)},}\\\text{applied to }f^{2}\text{ instead of }f\text{)}}}=0,
\]
and Remark \ref{rmk.res} \textbf{(b)} is proven.

\textit{Second proof of Remark \ref{rmk.res} \textbf{(b)}:} Write $f$ in the
form $\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ (with $b_{i}\in\mathbb{C}$ for
all $i\in\mathbb{Z}$). Then, $f^{\prime}=\sum\limits_{i\in\mathbb{Z}}%
ib_{i}t^{i-1}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}$.
Now,%
\[
ff^{\prime}=\left(  \sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}\right)  \left(
\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}\right)
=\sum\limits_{n\in\mathbb{Z}}\left(  \sum\limits_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\i+j=n}}b_{i}\cdot\left(  j+1\right)
b_{j+1}\right)  t^{n}%
\]
(by the definition of the product of Laurent polynomials). Also,
$df=f^{\prime}dt$, so that%
\begin{align*}
\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)   &  =\operatorname*{Res}%
\nolimits_{t=0}\left(  ff^{\prime}dt\right)  =\left(  \text{the coefficient of
the Laurent polynomial }ff^{\prime}\text{ before }t^{-1}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2}%
;\\i+j=-1}}b_{i}\cdot\left(  j+1\right)  b_{j+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }ff^{\prime}=\sum\limits_{n\in\mathbb{Z}}\left(  \sum
\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\i+j=n}}b_{i}%
\cdot\left(  j+1\right)  b_{j+1}\right)  t^{n}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2}%
;\\i+j=0}}b_{i}\cdot jb_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
substituted }\left(  i,j\right)  \text{ for }\left(  i,j+1\right)  \text{ in
the sum}\right) \\
&  =\sum\limits_{j\in\mathbb{Z}}b_{-j}\cdot jb_{j}=\underbrace{\sum
\limits_{\substack{j\in\mathbb{Z};\\j<0}}b_{-j}\cdot jb_{j}}_{\substack{=\sum
\limits_{\substack{j\in\mathbb{Z};\\j>0}}b_{-\left(  -j\right)  }\cdot\left(
-j\right)  b_{-j}\\\text{(here, we substituted }j\text{ for }-j\text{ in the
sum)}}}+\underbrace{b_{-0}\cdot0b_{0}}_{=0}+\sum\limits_{\substack{j\in
\mathbb{Z};\\j>0}}b_{-j}\cdot jb_{j}\\
&  =\sum\limits_{\substack{j\in\mathbb{Z};\\j>0}}\underbrace{b_{-\left(
-j\right)  }\cdot\left(  -j\right)  b_{-j}}_{=b_{j}\left(  -j\right)
b_{-j}=-b_{-j}\cdot jb_{j}}+\sum\limits_{\substack{j\in\mathbb{Z}%
;\\j>0}}b_{-j}\cdot jb_{j}=\sum\limits_{\substack{j\in\mathbb{Z}%
;\\j>0}}\left(  -b_{-j}\cdot jb_{j}\right)  +\sum\limits_{\substack{j\in
\mathbb{Z};\\j>0}}b_{-j}\cdot jb_{j}=0.
\end{align*}
This proves Remark \ref{rmk.res} \textbf{(b)}.

Note that the first proof of Remark \ref{rmk.res} \textbf{(b)} made use of the
fact that $2$ is invertible in $\mathbb{C}$, whereas the second proof works
over any commutative ring instead of $\mathbb{C}$.

Now, finally, we define the Heisenberg algebra:

\begin{definition}
\label{def.osc}The \textit{oscillator algebra} $\mathcal{A}$ is the vector
space $\mathbb{C}\left[  t,t^{-1}\right]  \oplus\mathbb{C}$ endowed with the
Lie bracket%
\[
\left[  \left(  f,\alpha\right)  ,\left(  g,\beta\right)  \right]  =\left(
0,\operatorname*{Res}\nolimits_{t=0}\left(  gdf\right)  \right)  .
\]
Since this Lie bracket satisfies the Jacobi identity (because the definition
quickly yields that $\left[  \left[  x,y\right]  ,z\right]  =0$ for all
$x,y,z\in\mathcal{A}$) and is skew-symmetric (due to Remark \ref{rmk.res}
\textbf{(b)}), this $\mathcal{A}$ is a Lie algebra.

This oscillator algebra $\mathcal{A}$ is also known as the \textit{Heisenberg
algebra}.
\end{definition}

Thus, $\mathcal{A}$ has a basis
\[
\left\{  a_{n}\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  K\right\}  ,
\]
where $a_{n}=\left(  t^{n},0\right)  $ and $K=\left(  0,1\right)  $. The
bracket is given by%
\begin{align*}
\left[  a_{n},K\right]   &  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{thus, }K\text{
is central}\right)  ;\\
\left[  a_{n},a_{m}\right]   &  =n\delta_{n,-m}K
\end{align*}
(in fact, $\left[  a_{n},a_{-n}\right]  =\operatorname*{Res}\nolimits_{t=0}%
\left(  t^{-n}dt^{n}\right)  K=\operatorname*{Res}\nolimits_{t=0}\left(
nt^{-1}dt\right)  K=nK$). Thus, $\mathcal{A}$ is a $1$-dimensional central
extension of the abelian Lie algebra $\mathbb{C}\left[  t,t^{-1}\right]  $;
this means that we have a short exact sequence%
\[
\xymatrix{
0 \ar[r] & \mathbb C K \ar[r] & \mathcal A \ar[r] & \mathbb C\left[t,t^{-1}\right] \ar[r] & 0
},
\]
where $\mathbb{C}K$ is contained in the center of $\mathcal{A}$ and where
$\mathbb{C}\left[  t,t^{-1}\right]  $ is an abelian Lie algebra.

Note that $\mathcal{A}$ is a $2$-nilpotent Lie algebra. Also note that the
center of $\mathcal{A}$ is spanned by $a_{0}$ and $K$.

\subsection{The Witt algebra}

The next introductory example will be the Lie algebra of vector fields:

\begin{definition}
Consider the free $\mathbb{C}\left[  t,t^{-1}\right]  $-module on the basis
$\left(  \partial\right)  $ (where $\partial$ is just a symbol). This module,
regarded as a $\mathbb{C}$-vector space, will be denoted by $W$. Thus, the
elements of $W$ are formal expressions of the form $f\partial$ where
$f\in\mathbb{C}\left[  t,t^{-1}\right]  $. (Thus, $W\cong\mathbb{C}\left[
t,t^{-1}\right]  $.)

Define a Lie bracket on the $\mathbb{C}$-vector space $W$ by%
\[
\left[  f\partial,g\partial\right]  =\left(  fg^{\prime}-gf^{\prime}\right)
\partial\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }g\in\mathbb{C}\left[  t,t^{-1}\right]  .
\]
This Lie bracket is easily seen to be skew-symmetric and satisfy the Jacobi
identity. Thus, it makes $W$ into a Lie algebra. This Lie algebra is called
the \textit{Witt algebra}.

The elements of $W$ are called \textit{polynomial vector fields on
}$\mathbb{C}^{\times}$.

The symbol $\partial$ is often denoted by $\dfrac{d}{dt}$.
\end{definition}

\begin{remark}
It is not by chance that $\partial$ is also known as $\dfrac{d}{dt}$. In fact,
this notation allows us to view the elements of $W$ as actual polynomial
vector fields on $\mathbb{C}^{\times}$ in the sense of algebraic geometry over
$\mathbb{C}$. The Lie bracket of the Witt algebra $W$ is then exactly the
usual Lie bracket of vector fields (because if $f\in\mathbb{C}\left[
t,t^{-1}\right]  $ and $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ are two
Laurent polynomials, then a simple application of the Leibniz rule shows that
the commutator of the differential operators $f\dfrac{d}{dt}$ and $g\dfrac
{d}{dt}$ is indeed the differential operator $\left(  fg^{\prime}-gf^{\prime
}\right)  \dfrac{d}{dt}$).
\end{remark}

A basis of the Witt algebra $W$ is $\left\{  L_{n}\ \mid\ n\in\mathbb{Z}%
\right\}  $, where $L_{n}$ means $-t^{n+1}\dfrac{d}{dt}=-t^{n+1}\partial$.
(Note that some other references like to define $L_{n}$ as $t^{n+1}\partial$
instead, thus getting a different sign in many formulas.) It is easy to see
that the Lie bracket of the Witt algebra is given on this basis by
\[
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ and }m\in
\mathbb{Z}.
\]


\subsection{\label{subsect.Wreal}A digression: Lie groups (and the absence
thereof)}

Let us make some remarks about the relationship between Lie algebras and Lie
groups. In analysis and geometry, linearizations (tangent spaces etc.) usually
only give a crude approximation of non-linear things (manifolds etc.). This is
what makes the theory of Lie groups special: The linearization of a
finite-dimensional Lie group (i. e., its corresponding Lie algebra) carries
very much information about the Lie group. The relation between
finite-dimensional Lie groups and finite-dimensional Lie algebras is almost a
one-to-one correspondence (at least if we restrict ourselves to simply
connected Lie groups). This correspondence breaks down in the
infinite-dimensional case. There are lots of important infinite-dimensional
Lie groups, but their relation to Lie algebras is not as close as in the
finite-dimensional case anymore. One example for this is that there is no Lie
group corresponding to the Witt algebra $W$. There are a few things that come
close to such a Lie group:

We can consider the real subalgebra $W_{\mathbb{R}}$ of $W$, consisting of the
vector fields in $W$ which are tangent to $S^{1}$ (the unit circle in
$\mathbb{C}$). This is a real Lie algebra satisfying $W_{\mathbb{R}}%
\otimes_{\mathbb{R}}\mathbb{C}\cong W$ (thus, $W_{\mathbb{R}}$ is what is
called a \textit{real form} of $W$). And we can say that
$\widehat{W_{\mathbb{R}}}=\operatorname*{Lie}\left(  \operatorname*{Diff}%
S^{1}\right)  $ (where $\operatorname*{Diff}S^{1}$ denotes the group of all
diffeomorphisms $S^{1}\rightarrow S^{1}$) for some kind of completion
$\widehat{W_{\mathbb{R}}}$ of $W_{\mathbb{R}}$ (although $W_{\mathbb{R}}$
itself is not the Lie algebra of any Lie group).\footnote{Here is how this
completion $\widehat{W_{\mathbb{R}}}$ is defined exactly: Notice that%
\[
W_{\mathbb{R}}=\left\{  \varphi\left(  \theta\right)  \dfrac{d}{d\theta}%
\ \mid\
\begin{array}
[c]{c}%
\varphi\text{ is a trigonometric polynomial, i. e.,}\\
\varphi\left(  \theta\right)  =a_{0}+\sum\limits_{n>0}a_{n}\cos n\theta
+\sum\limits_{n>0}b_{n}\sin n\theta\\
\text{where both sums are finite}%
\end{array}
\right\}  ,
\]
where $\theta=\dfrac{1}{i}\ln t$ and $\dfrac{d}{d\theta}=it\dfrac{d}{dt}$.
Now, define the completion $\widehat{W_{\mathbb{R}}}$ by%
\[
\widehat{W_{\mathbb{R}}}=\left\{  \varphi\left(  \theta\right)  \dfrac
{d}{d\theta}\ \mid\
\begin{array}
[c]{c}%
\varphi\left(  \theta\right)  =a_{0}+\sum\limits_{n>0}a_{n}\cos n\theta
+\sum\limits_{n>0}b_{n}\sin n\theta\\
\text{where both sums are infinite sums with rapidly}\\
\text{decreasing coefficients}%
\end{array}
\right\}  .
\]
} Now if we take two one-parameter families%
\begin{align*}
g_{s}  &  \in\operatorname*{Diff}S^{1},\ \ \ \ \ \ \ \ \ \ g_{s}\mid
_{s=0}=\operatorname*{id},\ \ \ \ \ \ \ \ \ \ g_{s}^{\prime}\mid_{s=0}%
=\varphi;\\
h_{u}  &  \in\operatorname*{Diff}S^{1},\ \ \ \ \ \ \ \ \ \ h_{u}\mid
_{u=0}=\operatorname*{id},\ \ \ \ \ \ \ \ \ \ h_{u}^{\prime}\mid_{u=0}=\psi,
\end{align*}
then%
\begin{align*}
g_{s}\left(  \theta\right)   &  =\theta+s\varphi\left(  \theta\right)
+O\left(  s^{2}\right)  ;\\
h_{u}\left(  \theta\right)   &  =\theta+u\psi\left(  \theta\right)  +O\left(
u^{2}\right)  ;\\
\left(  g_{s}\circ h_{u}\circ g_{s}^{-1}\circ h_{u}^{-1}\right)  \left(
\theta\right)   &  =\theta+su\left(  \varphi\psi^{\prime}-\psi\varphi^{\prime
}\right)  \left(  \theta\right)  +\left(  \text{cubic terms in }s\text{ and
}u\text{ and higher}\right)  .
\end{align*}
So we get something resembling the standard Lie-group-Lie-algebra
correspondence, but only for the completion of the real part. For the complex
one, some people have done some work yielding something like Lie semigroups
(the so-called ``semigroup of annuli'' of G. Segal), but no Lie groups.

Anyway, this was a digression, just to show that we don't have Lie groups
corresponding to our Lie algebras. Still, this should not keep us from
heuristically thinking of Lie algebras as linearizations of Lie groups. We can
even formalize this heuristic, by using the purely algebraic notion of formal groups.

\subsection{The Witt algebra acts on the Heisenberg algebra by derivations}

Let's return to topic. The following proposition is a variation on a
well-known theme:

\begin{proposition}
\label{prop.commutator.derivs}Let $\mathfrak{n}$ be a Lie algebra. Let
$f:\mathfrak{n}\rightarrow\mathfrak{n}$ and $g:\mathfrak{n}\rightarrow
\mathfrak{n}$ be two derivations of $\mathfrak{n}$. Then, $\left[  f,g\right]
$ is a derivation of $\mathfrak{n}$. (Here, the Lie bracket is to be
understood as the Lie bracket on $\operatorname*{End}\mathfrak{n}$, so that we
have $\left[  f,g\right]  =f\circ g-g\circ f$.)
\end{proposition}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.commutator.derivs}.} Let $a\in
\mathfrak{n}$ and $b\in\mathfrak{n}$. Since $f$ is a derivation, we have
$f\left(  \left[  a,b\right]  \right)  =\left[  f\left(  a\right)  ,b\right]
+\left[  a,f\left(  b\right)  \right]  $. Thus,%
\begin{align*}
\left(  g\circ f\right)  \left(  \left[  a,b\right]  \right)   &  =g\left(
\underbrace{f\left(  \left[  a,b\right]  \right)  }_{=\left[  f\left(
a\right)  ,b\right]  +\left[  a,f\left(  b\right)  \right]  }\right)
=g\left(  \left[  f\left(  a\right)  ,b\right]  +\left[  a,f\left(  b\right)
\right]  \right) \\
&  =\underbrace{g\left(  \left[  f\left(  a\right)  ,b\right]  \right)
}_{\substack{=\left[  g\left(  f\left(  a\right)  \right)  ,b\right]  +\left[
f\left(  a\right)  ,g\left(  b\right)  \right]  \\\text{(since }g\text{ is a
derivation)}}}+\underbrace{g\left(  \left[  a,f\left(  b\right)  \right]
\right)  }_{\substack{=\left[  g\left(  a\right)  ,f\left(  b\right)  \right]
+\left[  a,g\left(  f\left(  b\right)  \right)  \right]  \\\text{(since
}g\text{ is a derivation)}}}\\
&  =\left[  \underbrace{g\left(  f\left(  a\right)  \right)  }_{=\left(
g\circ f\right)  \left(  a\right)  },b\right]  +\left[  f\left(  a\right)
,g\left(  b\right)  \right]  +\left[  g\left(  a\right)  ,f\left(  b\right)
\right]  +\left[  a,\underbrace{g\left(  f\left(  b\right)  \right)
}_{=\left(  g\circ f\right)  \left(  b\right)  }\right] \\
&  =\left[  \left(  g\circ f\right)  \left(  a\right)  ,b\right]  +\left[
f\left(  a\right)  ,g\left(  b\right)  \right]  +\left[  g\left(  a\right)
,f\left(  b\right)  \right]  +\left[  a,\left(  g\circ f\right)  \left(
b\right)  \right]  .
\end{align*}
The same argument, with $f$ and $g$ replaced by $g$ and $f$, shows that%
\[
\left(  f\circ g\right)  \left(  \left[  a,b\right]  \right)  =\left[  \left(
f\circ g\right)  \left(  a\right)  ,b\right]  +\left[  g\left(  a\right)
,f\left(  b\right)  \right]  +\left[  f\left(  a\right)  ,g\left(  b\right)
\right]  +\left[  a,\left(  f\circ g\right)  \left(  b\right)  \right]  .
\]
Thus,%
\begin{align*}
\underbrace{\left[  f,g\right]  }_{=f\circ g-g\circ f}\left(  \left[
a,b\right]  \right)   &  =\left(  f\circ g-g\circ f\right)  \left(  \left[
a,b\right]  \right) \\
&  =\underbrace{\left(  f\circ g\right)  \left(  \left[  a,b\right]  \right)
}_{=\left[  \left(  f\circ g\right)  \left(  a\right)  ,b\right]  +\left[
g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  f\left(  a\right)
,g\left(  b\right)  \right]  +\left[  a,\left(  f\circ g\right)  \left(
b\right)  \right]  }-\underbrace{\left(  g\circ f\right)  \left(  \left[
a,b\right]  \right)  }_{=\left[  \left(  g\circ f\right)  \left(  a\right)
,b\right]  +\left[  f\left(  a\right)  ,g\left(  b\right)  \right]  +\left[
g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  a,\left(  g\circ
f\right)  \left(  b\right)  \right]  }\\
&  =\left(  \left[  \left(  f\circ g\right)  \left(  a\right)  ,b\right]
+\left[  g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  f\left(
a\right)  ,g\left(  b\right)  \right]  +\left[  a,\left(  f\circ g\right)
\left(  b\right)  \right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ -\left(  \left[  \left(  g\circ f\right)  \left(
a\right)  ,b\right]  +\left[  f\left(  a\right)  ,g\left(  b\right)  \right]
+\left[  g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  a,\left(
g\circ f\right)  \left(  b\right)  \right]  \right) \\
&  =\underbrace{\left[  \left(  f\circ g\right)  \left(  a\right)  ,b\right]
-\left[  \left(  g\circ f\right)  \left(  a\right)  ,b\right]  }_{=\left[
\left(  f\circ g\right)  \left(  a\right)  -\left(  g\circ f\right)  \left(
a\right)  ,b\right]  }+\underbrace{\left[  a,\left(  f\circ g\right)  \left(
b\right)  \right]  -\left[  a,\left(  g\circ f\right)  \left(  b\right)
\right]  }_{=\left[  a,\left(  f\circ g\right)  \left(  b\right)  -\left(
g\circ f\right)  \left(  b\right)  \right]  }\\
&  =\left[  \underbrace{\left(  f\circ g\right)  \left(  a\right)  -\left(
g\circ f\right)  \left(  a\right)  }_{=\left(  f\circ g-g\circ f\right)
\left(  a\right)  },b\right]  +\left[  a,\underbrace{\left(  f\circ g\right)
\left(  b\right)  -\left(  g\circ f\right)  \left(  b\right)  }_{=\left(
f\circ g-g\circ f\right)  \left(  b\right)  }\right] \\
&  =\left[  \underbrace{\left(  f\circ g-g\circ f\right)  }_{=\left[
f,g\right]  }\left(  a\right)  ,b\right]  +\left[  a,\underbrace{\left(
f\circ g-g\circ f\right)  }_{=\left[  f,g\right]  }\left(  b\right)  \right]
\\
&  =\left[  \left[  f,g\right]  \left(  a\right)  ,b\right]  +\left[
a,\left[  f,g\right]  \left(  b\right)  \right]  .
\end{align*}
We have thus proven that any $a\in\mathfrak{n}$ and $b\in\mathfrak{n}$ satisfy
$\left[  f,g\right]  \left(  \left[  a,b\right]  \right)  =\left[  \left[
f,g\right]  \left(  a\right)  ,b\right]  +\left[  a,\left[  f,g\right]
\left(  b\right)  \right]  $. In other words, $\left[  f,g\right]  $ is a
derivation. This proves Proposition \ref{prop.commutator.derivs}.
\end{verlong}

\begin{definition}
For every Lie algebra $\mathfrak{g}$, we will denote by $\operatorname*{Der}%
\mathfrak{g}$ the Lie subalgebra $\left\{  f\in\operatorname*{End}%
\mathfrak{g}\ \mid\ f\text{ is a derivation}\right\}  $ of
$\operatorname*{End}\mathfrak{g}$. (This is well-defined because Proposition
\ref{prop.commutator.derivs} shows that $\left\{  f\in\operatorname*{End}%
\mathfrak{g}\ \mid\ f\text{ is a derivation}\right\}  $ is a Lie subalgebra of
$\operatorname*{End}\mathfrak{g}$.) We call $\operatorname*{Der}\mathfrak{g}$
the \textit{Lie algebra of derivations} of $\mathfrak{g}$.
\end{definition}

\begin{lemma}
\label{lem.WtoDerA}There is a natural homomorphism $\eta:W\rightarrow
\operatorname*{Der}\mathcal{A}$ of Lie algebras given by
\[
\left(  \eta\left(  f\partial\right)  \right)  \left(  g,\alpha\right)
=\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }\alpha\in\mathbb{C}.
\]

\end{lemma}

\textit{First proof of Lemma \ref{lem.WtoDerA}.} Lemma \ref{lem.WtoDerA} can
be proven by direct calculation:

For every $f\partial\in W$, the map%
\[
\mathcal{A}\rightarrow\mathcal{A},\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)
\mapsto\left(  fg^{\prime},0\right)
\]
is a derivation of $\mathcal{A}$\ \ \ \ \footnote{\textit{Proof.} Let
$f\partial$ be an element of $W$. (In other words, let $f$ be an element of
$\mathbb{C}\left[  t,t^{-1}\right]  $.) Let $\tau$ denote the map%
\[
\mathcal{A}\rightarrow\mathcal{A},\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)
\mapsto\left(  fg^{\prime},0\right)  .
\]
Then, we must prove that $\tau$ is a derivation of $\mathcal{A}$.
\par
In fact, first it is clear that $\tau$ is $\mathbb{C}$-linear. Moreover, any
$\left(  u,\beta\right)  \in\mathcal{A}$ and $\left(  v,\gamma\right)
\in\mathcal{A}$ satisfy%
\begin{align*}
\tau\left(  \underbrace{\left[  \left(  u,\beta\right)  ,\left(
v,\gamma\right)  \right]  }_{=\left(  0,\operatorname*{Res}\nolimits_{t=0}%
\left(  vdu\right)  \right)  }\right)   &  =\tau\left(  0,\operatorname*{Res}%
\nolimits_{t=0}\left(  vdu\right)  \right)  =\left(  f0,0\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\right) \\
&  =\left(  0,0\right)
\end{align*}
and%
\begin{align*}
&  \left[  \underbrace{\tau\left(  u,\beta\right)  }_{=\left(  fu^{\prime
},0\right)  },\left(  v,\gamma\right)  \right]  +\left[  \left(
u,\beta\right)  ,\underbrace{\tau\left(  v,\gamma\right)  }_{=\left(
fv^{\prime},0\right)  }\right] \\
&  =\underbrace{\left[  \left(  fu^{\prime},0\right)  ,\left(  v,\gamma
\right)  \right]  }_{=\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(
vd\left(  fu^{\prime}\right)  \right)  \right)  }+\underbrace{\left[  \left(
u,\beta\right)  ,\left(  fv^{\prime},0\right)  \right]  }_{=\left(
0,\operatorname*{Res}\nolimits_{t=0}\left(  fv^{\prime}du\right)  \right)  }\\
&  =\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(  vd\left(  fu^{\prime
}\right)  \right)  \right)  +\left(  0,\operatorname*{Res}\nolimits_{t=0}%
\left(  fv^{\prime}du\right)  \right) \\
&  =\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(  vd\left(  fu^{\prime
}\right)  +fv^{\prime}du\right)  \right)  =\left(  0,\operatorname*{Res}%
\nolimits_{t=0}\left(  d\left(  vfu^{\prime}\right)  \right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }v\underbrace{d\left(  fu^{\prime}\right)  }_{=\left(  fu^{\prime
}\right)  ^{\prime}dt}+fv^{\prime}\underbrace{du}_{=u^{\prime}dt}=v\left(
fu^{\prime}\right)  ^{\prime}dt+fv^{\prime}u^{\prime}dt\\
=\left(  v\left(  fu^{\prime}\right)  ^{\prime}+fv^{\prime}u^{\prime}\right)
dt=\underbrace{\left(  v\left(  fu^{\prime}\right)  ^{\prime}+v^{\prime
}\left(  fu^{\prime}\right)  \right)  }_{=\left(  vfu^{\prime}\right)
^{\prime}}dt=\left(  vfu^{\prime}\right)  ^{\prime}dt=d\left(  vfu^{\prime
}\right)
\end{array}
\right) \\
&  =\left(  0,0\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since Remark
\ref{rmk.res} \textbf{(a)} (applied to }vfu^{\prime}\text{ instead of
}f\text{) yields }\operatorname*{Res}\nolimits_{t=0}\left(  d\left(
vfu^{\prime}\right)  \right)  =0\right)  ,
\end{align*}
so that $\tau\left(  \left[  \left(  u,\beta\right)  ,\left(  v,\gamma\right)
\right]  \right)  =\left[  \tau\left(  u,\beta\right)  ,\left(  v,\gamma
\right)  \right]  +\left[  \left(  u,\beta\right)  ,\tau\left(  v,\gamma
\right)  \right]  $. Thus, $\tau$ is a derivation of $\mathcal{A}$, qed.},
thus lies in $\operatorname*{Der}\mathcal{A}$. Hence, we can define a map
$\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ by%
\[
\eta\left(  f\partial\right)  =\left(  \mathcal{A}\rightarrow\mathcal{A}%
,\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)  \mapsto\left(  fg^{\prime
},0\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[
t,t^{-1}\right]  .
\]
In other words, we can define a map $\eta:W\rightarrow\operatorname*{Der}%
\mathcal{A}$ by%
\[
\left(  \eta\left(  f\partial\right)  \right)  \left(  g,\alpha\right)
=\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }\alpha\in\mathbb{C}.
\]
Now, it remains to show that this map $\eta$ is a homomorphism of Lie algebras.

In fact, any $f_{1}\in\mathbb{C}\left[  t,t^{-1}\right]  $ and $f_{2}%
\in\mathbb{C}\left[  t,t^{-1}\right]  $ and any $g\in\mathbb{C}\left[
t,t^{-1}\right]  $ and $\alpha\in\mathbb{C}$ satisfy%
\[
\left(  \eta\left(  \underbrace{\left[  f_{1}\partial,f_{2}\partial\right]
}_{=\left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  \partial}\right)
\right)  \left(  g,\alpha\right)  =\left(  \eta\left(  \left(  f_{1}%
f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  \partial\right)  \right)  \left(
g,\alpha\right)  =\left(  \left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime
}\right)  g^{\prime},0\right)
\]
and%
\begin{align*}
&  \left[  \eta\left(  f_{1}\partial\right)  ,\eta\left(  f_{2}\partial
\right)  \right]  \left(  g,\alpha\right) \\
&  =\left(  \eta\left(  f_{1}\partial\right)  \right)  \underbrace{\left(
\left(  \eta\left(  f_{2}\partial\right)  \right)  \left(  g,\alpha\right)
\right)  }_{=\left(  f_{2}g^{\prime},0\right)  }-\left(  \eta\left(
f_{2}\partial\right)  \right)  \underbrace{\left(  \left(  \eta\left(
f_{1}\partial\right)  \right)  \left(  g,\alpha\right)  \right)  }_{=\left(
f_{1}g^{\prime},0\right)  }\\
&  =\underbrace{\left(  \eta\left(  f_{1}\partial\right)  \right)  \left(
f_{2}g^{\prime},0\right)  }_{=\left(  f_{1}\left(  f_{2}g^{\prime}\right)
^{\prime},0\right)  }-\underbrace{\left(  \eta\left(  f_{2}\partial\right)
\right)  \left(  f_{1}g^{\prime},0\right)  }_{=\left(  f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right)  }=\left(  f_{1}\left(
f_{2}g^{\prime}\right)  ^{\prime},0\right)  -\left(  f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right) \\
&  =\left(  f_{1}\left(  f_{2}g^{\prime}\right)  ^{\prime}-f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right)  =\left(  \left(  f_{1}%
f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  g^{\prime},0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }f_{1}\underbrace{\left(  f_{2}g^{\prime}\right)  ^{\prime}%
}_{=f_{2}^{\prime}g^{\prime}+f_{2}g^{\prime\prime}}-f_{2}\underbrace{\left(
f_{1}g^{\prime}\right)  ^{\prime}}_{=f_{1}^{\prime}g^{\prime}+f_{1}%
g^{\prime\prime}}=f_{1}\left(  f_{2}^{\prime}g^{\prime}+f_{2}g^{\prime\prime
}\right)  -f_{2}\left(  f_{1}^{\prime}g^{\prime}+f_{1}g^{\prime\prime}\right)
\\
=f_{1}f_{2}^{\prime}g^{\prime}+f_{1}f_{2}g^{\prime\prime}-f_{2}f_{1}^{\prime
}g^{\prime}-f_{1}f_{2}g^{\prime\prime}=f_{1}f_{2}^{\prime}g^{\prime}%
-f_{2}f_{1}^{\prime}g^{\prime}=\left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime
}\right)  g^{\prime}%
\end{array}
\right)  ,
\end{align*}
so that
\[
\left(  \eta\left(  \left[  f_{1}\partial,f_{2}\partial\right]  \right)
\right)  \left(  g,\alpha\right)  =\left(  \left(  f_{1}f_{2}^{\prime}%
-f_{2}f_{1}^{\prime}\right)  g^{\prime},0\right)  =\left[  \eta\left(
f_{1}\partial\right)  ,\eta\left(  f_{2}\partial\right)  \right]  \left(
g,\alpha\right)  .
\]
Thus, any $f_{1}\in\mathbb{C}\left[  t,t^{-1}\right]  $ and $f_{2}%
\in\mathbb{C}\left[  t,t^{-1}\right]  $ satisfy $\eta\left(  \left[
f_{1}\partial,f_{2}\partial\right]  \right)  )=\left[  \eta\left(
f_{1}\partial\right)  ,\eta\left(  f_{2}\partial\right)  \right]  $. This
proves that $\eta$ is a Lie algebra homomorphism, and thus Lemma
\ref{lem.WtoDerA} is proven.

\textit{Second proof of Lemma \ref{lem.WtoDerA} (sketched).} The following
proof I don't understand, so don't expect my version of it to make any sense.
See Akhil Matthew's blog post \newline%
\texttt{\href{http://amathew.wordpress.com/2012/03/01/the-heisenberg-and-witt-algebras/}{\texttt{http://amathew.wordpress.com/2012/03/01/the-heisenberg-and-witt-algebras/}%
}} for a much better writeup.

The following proof is a bit of an overkill; however, it is supposed to
provide some motivation for Lemma \ref{lem.WtoDerA}. We won't be working
completely formally, so the reader should expect some imprecision.

\begin{noncompile}
We recall the definitions of $W_{\mathbb{R}}$ and $\widehat{W_{\mathbb{R}}}$
from Section \ref{subsect.Wreal}.
\end{noncompile}

\begin{noncompile}
The elements of $W$, being vector fields on $S^{1}$
\end{noncompile}

\begin{noncompile}
\textit{1st step:} We are going to prove that%
\begin{equation}
\left[  \left(  fu^{\prime},0\right)  ,\left(  v,\gamma\right)  \right]
+\left[  \left(  u,\beta\right)  ,\left(  fv^{\prime},0\right)  \right]
=\left(  0,0\right)  \label{pf.WtoDerA.overkill.1}%
\end{equation}
for every $f\in\mathbb{C}\left[  t,t^{-1}\right]  $, $\left(  u,\beta\right)
\in\mathcal{A}$ and $\left(  v,\gamma\right)  \in\mathcal{A}$.
\end{noncompile}

\begin{noncompile}
\textit{Proof of (\ref{pf.WtoDerA.overkill.1}):} Let $\left(  u,\beta\right)
\in\mathcal{A}$ and $\left(  v,\gamma\right)  \in\mathcal{A}$.
\end{noncompile}

\begin{noncompile}
Define a bilinear form $\kappa:\mathbb{C}\left[  t,t^{-1}\right]
\times\mathbb{C}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$ by%
\[
\kappa\left(  p,q\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
qdp\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }p\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }q\in\mathbb{C}\left[  t,t^{-1}\right]  .
\]

\end{noncompile}

\begin{noncompile}
Then, every $p\in\mathbb{C}\left[  t,t^{-1}\right]  $ and $q\in\mathbb{C}%
\left[  t,t^{-1}\right]  $ satisfy%
\[
\kappa\left(  p,q\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
qdp\right)  =\dfrac{1}{2\pi i}\oint\limits_{\left\vert t\right\vert
=1}qdp\ \ \ \ \ \ \ \ \ \ \left(  \text{by Cauchy's residue theorem}\right)
,
\]
where the integral on the right hand side is over the unit circle of $S^{1}$.
This shows that applying any diffeomorphism of $S^{1}$ to $p$ and $q$
(simultaneously) leaves $\kappa\left(  p,q\right)  $ unchanged (because
$\dfrac{1}{2\pi i}\oint\limits_{\left\vert t\right\vert =1}qdp$ is defined
invariantly). In other words, the form $\kappa$ is invariant under the action
of the ``Lie group'' of diffeomorphisms of $S^{1}$ on $\mathbb{C}\left[
t,t^{-1}\right]  $. Consequently, the form $\kappa$ is also invariant under
the corresponding ``Lie algebra'' of ``infinitesimal diffeomorphisms'' of
$S^{1}$. Since we know that the elements of $W_{\mathbb{R}}$ act as
infinitesimal diffeomorphisms of $S^{1}$ (since $W_{\mathbb{R}}\subseteq
\widehat{W_{\mathbb{R}}}$), this yields that%
\[
\kappa\left(  h\rightharpoonup p,q\right)  +\kappa\left(  p,h\rightharpoonup
q\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for any }p\in\mathbb{C}\left[
t,t^{-1}\right]  \text{, }q\in\mathbb{C}\left[  t,t^{-1}\right]  \text{ and
}h\in W_{\mathbb{R}}.
\]
Since this
\end{noncompile}

Let us really interpret the elements of $W$ as vector fields on $\mathbb{C}%
^{\times}$. The bracket $\left[  \cdot,\cdot\right]  $ of the Lie algebra
$\mathcal{A}$ was defined in an invariant way:%
\[
\left[  f,g\right]  =\operatorname*{Res}\nolimits_{t=0}\left(  gdf\right)
=\dfrac{1}{2\pi i}\oint\limits_{\left\vert z\right\vert =1}%
gdf\ \ \ \ \ \ \ \ \ \ \left(  \text{by Cauchy's residue theorem}\right)
\]
is an integral of a $1$-form, thus invariant under diffeomorphisms, thus
invariant under ``infinitesimal diffeomorphisms'' such as the ones given by
elements of $W$. Thus, Lemma \ref{lem.WtoDerA} becomes obvious. [This proof
needs revision.]

The first of these two proofs is obviously the more straightforward one (and
generalizes better to fields other than $\mathbb{C}$), but it does not offer
any explanation why Lemma \ref{lem.WtoDerA} is more than a mere coincidence.
Meanwhile, the second proof gives Lemma \ref{lem.WtoDerA} a philosophical
reason to be true.

\subsection{The Virasoro algebra}

In representation theory, one often doesn't encounter representations of $W$
directly, but instead one finds representations of a $1$-dimensional central
extension of $W$ called the Virasoro algebra. I will now construct this
extension and show that it is the only one (up to isomorphism of extensions).

Let us recollect the theory of central extensions of Lie algebras (more
precisely, the $1$-dimensional ones):

\begin{definition}
\label{def.centex}If $L$ is a Lie algebra, then a $1$-dimensional central
extension of $L$ is a Lie algebra $\widehat{L}$ along with an exact sequence%
\begin{equation}
0\rightarrow\mathbb{C}\rightarrow\widehat{L}\rightarrow L\rightarrow0,
\label{def.2-cocyc.es}%
\end{equation}
where $\mathbb{C}$ is central in $\widehat{L}$. Since all exact sequences of
vector spaces split, we can pick a splitting of this exact sequence on the
level of vector spaces, and thus identify $\widehat{L}$ with $L\oplus
\mathbb{C}$ as a vector space (not as a Lie algebra). Upon this
identification, the Lie bracket of $\widehat{L}$ can be written as%
\begin{equation}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]  =\left(
\left[  a,b\right]  ,\omega\left(  a,b\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for }a\in L\text{, }\alpha\in\mathbb{C}\text{, }b\in
L\text{, }\beta\in\mathbb{C}, \label{def.2-cocyc.form}%
\end{equation}
for some skew-symmetric bilinear form $\omega:L\times L\rightarrow\mathbb{C}$.
(We can also write this skew-symmetric bilinear form $\omega:L\times
L\rightarrow\mathbb{C}$ as a linear form $\wedge^{2}L\rightarrow\mathbb{C}$.)
But $\omega$ cannot be a completely arbitrary skew-symmetric bilinear form. It
needs to satisfy the so-called $2$\textit{-cocycle condition}%
\begin{equation}
\omega\left(  \left[  a,b\right]  ,c\right)  +\omega\left(  \left[
b,c\right]  ,a\right)  +\omega\left(  \left[  c,a\right]  ,b\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }a,b,c\in L. \label{def.2-cocyc.eq}%
\end{equation}
This condition comes from the requirement that the bracket in $\widehat{L}$
have to satisfy the Jacobi identity.

In the following, a $2$\textit{-cocycle on }$L$ will mean a skew-symmetric
bilinear form $\omega:L\times L\rightarrow\mathbb{C}$ (not necessarily
obtained from a central extension!) which satisfies the equation
(\ref{def.2-cocyc.eq}). (The name ``$2$-cocycle'' comes from Lie algebra
cohomology, where $2$-cocycles are indeed the cocycles in the $2$-nd degree.)
Thus, we have assigned a $2$-cocycle on $L$ to every $1$-dimensional central
extension of $L$ (although the assignment depended on the splitting).

Conversely, if $\omega$ is any $2$-cocycle on $L$, then we can define a
$1$-dimensional central extension $\widehat{L}_{\omega}$ of $L$ such that the
$2$-cocycle corresponding to this extension is $\omega$. In fact, we can
construct such a central extension $\widehat{L}_{\omega}$ by setting
$\widehat{L}_{\omega}=L\oplus\mathbb{C}$ as a vector space, and defining the
Lie bracket on this vector space by (\ref{def.2-cocyc.form}). (The maps
$\mathbb{C}\rightarrow\widehat{L}_{\omega}$ and $\widehat{L}_{\omega
}\rightarrow L$ are the canonical ones coming from the direct sum
decomposition $\widehat{L}_{\omega}=L\oplus\mathbb{C}$.) Thus, every
$2$-cocycle on $L$ canonically determines a $1$-dimensional central extension
of $L$.

However, our assignment of the $2$-cocycle $\omega$ to the central extension
$\widehat{L}$ was not canonical, but depended on the splitting of the exact
sequence (\ref{def.2-cocyc.es}). If we change the splitting by some $\xi\in
L^{\ast}$, then $\omega$ is changed by $d\xi$ (this means that $\omega$ is
being replaced by $\omega+d\xi$), where $d\xi$ is the $2$-cocycle on $L$
defined by
\[
d\xi\left(  a,b\right)  =\xi\left(  \left[  a,b\right]  \right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in L.
\]
The $2$-cocycle $d\xi$ is called a $2$\textit{-coboundary}. As a conclusion,
$1$-dimensional central extensions of $L$ are parametrized up to isomorphism
by the vector space%
\[
\left(  2\text{-cocycles}\right)  \diagup\left(  2\text{-coboundaries}\right)
=H^{2}\left(  L\right)  .
\]
(Note that ``up to isomorphism'' means ``up to isomorphism of extensions''
here, not ``up to isomorphism of Lie algebras''.) The vector space
$H^{2}\left(  L\right)  $ is called the $2$\textit{-nd cohomology space} (or
just the $2$-nd cohomology) of the Lie algebra $L$.
\end{definition}

\begin{theorem}
\label{thm.H^2(W)}The vector space $H^{2}\left(  W\right)  $ is $1$%
-dimensional and is spanned by the residue class of the $2$-cocycle $\omega$
given by%
\[
\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z}.
\]

\end{theorem}

Note that in this theorem, we could have replaced the factor $\dfrac{n^{3}%
-n}{6}$ by $n^{3}-n$ (since the vector space spanned by a vector obviously
doesn't change if we rescale the vector by a nonzero scalar factor), or even
by $n^{3}$ (since the $2$-cocycle $\left(  L_{n},L_{m}\right)  \mapsto
n\delta_{n,-m}$ is a coboundary, and two $2$-cocycles which differ by a
coboundary give the same residue class in $H^{2}\left(  W\right)  $). But we
prefer $\dfrac{n^{3}-n}{6}$ since this is closer to how this class appears in
representation theory (and, also, comes up in the proof below).

\textit{Proof of Theorem \ref{thm.H^2(W)}.} First of all, it is easy to prove
by computation that the bilinear form $\omega:W\times W\rightarrow\mathbb{C}$
given by%
\[
\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z}%
\]
is indeed a $2$-cocycle. Now, let us prove that every $2$-cocycle on $W$ is
congruent to a multiple of $\omega$ modulo the $2$-coboundaries.

Let $\beta$ be a $2$-cocycle on $W$. We must prove that $\beta$ is congruent
to a multiple of $\omega$ modulo the $2$-coboundaries.

Pick $\xi\in W^{\ast}$ such that $\xi\left(  L_{n}\right)  =\dfrac{1}{n}%
\beta\left(  L_{n},L_{0}\right)  $ for all $n\neq0$ (such a $\xi$ clearly
exists, but is not unique since we have complete freedom in choosing
$\xi\left(  L_{0}\right)  $). Let $\widetilde{\beta}$ be the $2$-cocycle
$\beta-d\xi$. Then,
\[
\widetilde{\beta}\left(  L_{n},L_{0}\right)  =\underbrace{\beta\left(
L_{n},L_{0}\right)  }_{\substack{=n\xi\left(  L_{n}\right)  \\\text{(since
}\xi\left(  L_{n}\right)  =\dfrac{1}{n}\beta\left(  L_{n},L_{0}\right)
\text{)}}}-\xi\left(  \underbrace{\left[  L_{n},L_{0}\right]  }_{=nL_{n}%
}\right)  =n\xi\left(  L_{n}\right)  -\xi\left(  nL_{n}\right)  =0
\]
for every $n\neq0$. Thus, by replacing $\beta$ by $\widetilde{\beta}$, we can
WLOG assume that $\beta\left(  L_{n},L_{0}\right)  =0$ for every $n\neq0$.
This clearly also holds for $n=0$ since $\beta$ is skew-symmetric. Hence,
$\beta\left(  X,L_{0}\right)  =0$ for every $X\in W$. Now, by the $2$-cocycle
condition, we have%
\[
\beta\left(  \left[  L_{0},L_{m}\right]  ,L_{n}\right)  +\beta\left(  \left[
L_{n},L_{0}\right]  ,L_{m}\right)  +\beta\left(  \left[  L_{m},L_{n}\right]
,L_{0}\right)  =0
\]
for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Thus,%
\begin{align*}
0  &  =\beta\left(  \underbrace{\left[  L_{0},L_{m}\right]  }_{=-mL_{m}}%
,L_{n}\right)  +\beta\left(  \underbrace{\left[  L_{n},L_{0}\right]
}_{=nL_{n}},L_{m}\right)  +\underbrace{\beta\left(  \left[  L_{m}%
,L_{n}\right]  ,L_{0}\right)  }_{=0\text{ (since }\beta\left(  X,L_{0}\right)
=0\text{ for every }X\in W\text{)}}\\
&  =-m\underbrace{\beta\left(  L_{m},L_{n}\right)  }_{\substack{=-\beta\left(
L_{n},L_{m}\right)  \\\text{(since }\beta\text{ is skew-symmetric)}}%
}+n\beta\left(  L_{n},L_{m}\right)  =m\beta\left(  L_{n},L_{m}\right)
+n\beta\left(  L_{n},L_{m}\right) \\
&  =\left(  n+m\right)  \beta\left(  L_{n},L_{m}\right)
\end{align*}
for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Hence, for all $n\in\mathbb{Z}$
and $m\in\mathbb{Z}$ with $n+m\neq0$, we have $\beta\left(  L_{n}%
,L_{m}\right)  =0$. In other words, there exists some sequence $\left(
b_{n}\right)  _{n\in\mathbb{Z}}\in\mathbb{C}^{\mathbb{Z}}$ such that
\begin{equation}
\beta\left(  L_{n},L_{m}\right)  =b_{n}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\label{thm.H^2(W).pf.2}%
\end{equation}
This sequence satisfies
\begin{equation}
b_{-n}=-b_{n}\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}
\label{thm.H^2(W).pf.1}%
\end{equation}
(since $\beta$ is skew-symmetric and thus $\beta\left(  L_{n},L_{-n}\right)
=-\beta\left(  L_{-n},L_{n}\right)  $) and thus, in particular, $b_{0}=0$. We
will now try to get a recursive equation for this sequence.

Let $m$, $n$ and $p$ be three integers satisfying $m+n+p=0$. Then, the
$2$-cocycle condition yields%
\[
\beta\left(  \left[  L_{p},L_{n}\right]  ,L_{m}\right)  +\beta\left(  \left[
L_{m},L_{p}\right]  ,L_{n}\right)  +\beta\left(  \left[  L_{n},L_{m}\right]
,L_{p}\right)  =0.
\]
Due to%
\begin{align*}
\beta\left(  \underbrace{\left[  L_{p},L_{n}\right]  }_{=\left(  p-n\right)
L_{p+n}},L_{m}\right)   &  =\left(  p-n\right)  \underbrace{\beta\left(
L_{p+n},L_{m}\right)  }_{\substack{=-\beta\left(  L_{m},L_{p+n}\right)
\\\text{(since }\beta\text{ is skew-symmetric)}}}=-\left(  p-n\right)
\underbrace{\beta\left(  L_{m},L_{p+n}\right)  }_{\substack{=b_{m}%
\delta_{m,-\left(  p+n\right)  }\\\text{(by (\ref{thm.H^2(W).pf.2}))}}}\\
&  =-\left(  p-n\right)  b_{m}\underbrace{\delta_{m,-\left(  p+n\right)  }%
}_{\substack{=1\\\text{(since }m+n+p=0\text{)}}}=-\left(  p-n\right)  b_{m}%
\end{align*}
and the two cyclic permutations of this equality, this rewrites as%
\[
\left(  -\left(  p-n\right)  b_{m}\right)  +\left(  -\left(  m-p\right)
b_{n}\right)  +\left(  -\left(  n-m\right)  b_{p}\right)  =0.
\]
In other words,%
\begin{equation}
\left(  n-m\right)  b_{p}+\left(  m-p\right)  b_{n}+\left(  p-n\right)
b_{m}=0. \label{thm.H^2(W).pf.3}%
\end{equation}


Now define a form $\xi_{0}\in W^{\ast}$ by $\xi_{0}\left(  L_{0}\right)  =1$
and $\xi_{0}\left(  L_{i}\right)  =0$ for all $i\neq0$.

By replacing $\beta$ with $\beta-\dfrac{b_{1}}{2}d\xi_{0}$, we can assume WLOG
that $b_{1}=0$.

Now let $n\in\mathbb{Z}$ be arbitrary. Setting $m=1$ and $p=-\left(
n+1\right)  $ in (\ref{thm.H^2(W).pf.3}) (this is allowed since $1+n+\left(
-\left(  n+1\right)  \right)  =0$), we get%
\[
\left(  n-1\right)  b_{-\left(  n+1\right)  }+\left(  1-\left(  -\left(
n+1\right)  \right)  \right)  b_{n}+\left(  n-1\right)  b_{1}=0.
\]
Thus,%
\begin{align*}
0  &  =\left(  n-1\right)  \underbrace{b_{-\left(  n+1\right)  }}%
_{=-b_{n+1}\text{ (by (\ref{thm.H^2(W).pf.1}))}}+\underbrace{\left(  1-\left(
-\left(  n+1\right)  \right)  \right)  }_{=n+2}b_{n}+\left(  n-1\right)
\underbrace{b_{1}}_{=0}\\
&  =-\left(  n-1\right)  b_{n+1}+\left(  n+2\right)  b_{n},
\end{align*}
so that $\left(  n-1\right)  b_{n+1}=\left(  n+2\right)  b_{n}$. This
recurrence equation rewrites as $b_{n+1}=\dfrac{n+2}{n-1}b_{n}$ for $n\geq2$.
Thus, by induction we see that every $n\geq2$ satisfies%
\[
b_{n}=\dfrac{n+1}{n-2}\cdot\dfrac{n}{n-3}\cdot\dfrac{n-1}{n-4}\cdot
...\cdot\dfrac{4}{1}b_{2}=\dfrac{\left(  n+1\right)  \cdot n\cdot...\cdot
4}{\left(  n-2\right)  \cdot\left(  n-3\right)  \cdot...\cdot1}b_{2}%
=\dfrac{\left(  n+1\right)  \left(  n-1\right)  n}{6}b_{2}=\dfrac{n^{3}-n}%
{6}b_{2}.
\]
But $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$ also holds for $n=1$ (since $b_{1}=0$ and
$\dfrac{1^{3}-1}{6}=0$) and for $n=0$ (since $b_{0}=0$ and $\dfrac{0^{3}-0}%
{6}=0$). Hence, $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$ holds for every $n\geq0$. By
(\ref{thm.H^2(W).pf.1}), we conclude that $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$
holds also for every $n\leq0$. Thus, every $n\in\mathbb{Z}$ satisfies
$b_{n}=\dfrac{n^{3}-n}{6}b_{2}$. From (\ref{thm.H^2(W).pf.2}), we thus see
that $\beta$ is a scalar multiple of $\omega$.

We thus have proven that every $2$-cocycle $\beta$ on $W$ is congruent to a
multiple of $\omega$ modulo the $2$-coboundaries. This yields that the space
$H^{2}\left(  W\right)  $ is \textit{at most }$1$-dimensional and is spanned
by the residue class of the $2$-cocycle $\omega$. In order to complete the
proof of Theorem \ref{thm.H^2(W)}, we have yet to prove that $H^{2}\left(
W\right)  $ is indeed $1$-dimensional (and not $0$-dimensional), i. e., that
the $2$-cocycle $\omega$ is \textit{not} a $2$-coboundary. But this is
easy\footnote{\textit{Proof.} Assume the contrary. Then, the $2$-cocycle
$\omega$ is a $2$-coboundary. This means that there exists a linear map
$\eta:W\rightarrow\mathbb{C}$ such that $\omega=d\eta$. Pick such a $\eta$.
Then,%
\[
\omega\left(  L_{2},L_{-2}\right)  =\left(  d\eta\right)  \left(  L_{2}%
,L_{-2}\right)  =\eta\left(  \underbrace{\left[  L_{2},L_{-2}\right]
}_{=4L_{0}}\right)  =4\eta\left(  L_{0}\right)
\]
and%
\[
\omega\left(  L_{1},L_{-1}\right)  =\left(  d\eta\right)  \left(  L_{1}%
,L_{-1}\right)  =\eta\left(  \underbrace{\left[  L_{1},L_{-1}\right]
}_{=2L_{0}}\right)  =2\eta\left(  L_{0}\right)  .
\]
Hence,%
\[
2\underbrace{\omega\left(  L_{1},L_{-1}\right)  }_{=2\eta\left(  L_{0}\right)
}=4\eta\left(  L_{0}\right)  =\omega\left(  L_{2},L_{-2}\right)  .
\]
But this contradicts with the equalities $\omega\left(  L_{1},L_{-1}\right)
=0$ and $\omega\left(  L_{2},L_{-2}\right)  =1$ (which easily follow from the
definition of $\omega$). This contradiction shows that our assumption was
wrong, and thus the $2$-cocycle $\omega$ is not a $2$-coboundary, qed.}. The
proof of Theorem \ref{thm.H^2(W)} is thus complete.

The $2$-cocycle $\dfrac{1}{2}\omega$ (where $\omega$ is the $2$-cocycle
introduced in Theorem \ref{thm.H^2(W)}) gives a central extension of the Witt
algebra $W$: the so-called Virasoro algebra. Let us recast the definition of
this algebra in elementary terms:

\begin{definition}
The \textit{Virasoro algebra} $\operatorname*{Vir}$ is defined as the vector
space $W\oplus\mathbb{C}$ with Lie bracket defined by%
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}%
-n}{12}\delta_{n,-m}C;\\
\left[  L_{n},C\right]   &  =0,
\end{align*}
where $L_{n}$ denotes $\left(  L_{n},0\right)  $ for every $n\in\mathbb{Z}$,
and where $C$ denotes $\left(  0,1\right)  $. Note that $\left\{  L_{n}%
\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  C\right\}  $ is a basis of
$\operatorname*{Vir}$.
\end{definition}

If we change the denominator $12$ to any other nonzero complex number, we get
a Lie algebra isomorphic to $\operatorname*{Vir}$ (it is just a rescaling of
$C$). It is easy to show that the Virasoro algebra is not isomorphic to the
Lie-algebraic direct sum $W\oplus\mathbb{C}$. Thus, $\operatorname*{Vir}$ is
the unique (up to Lie algebra isomorphism) nontrivial $1$-dimensional central
extension of $W$.

\subsection{Recollection on \texorpdfstring{$\mathfrak{g}$}{g}-invariant
forms}

Before we show the next important family of infinite-dimensional Lie algebras,
let us define some standard notions. First, let us define the notion of a
$\mathfrak{g}$-invariant form, in full generality (that is, for any two
$\mathfrak{g}$-modules):

\begin{definition}
\label{def.g-invar}Let $\mathfrak{g}$ be a Lie algebra over a field $k$. Let
$M$ and $N$ be two $\mathfrak{g}$-modules. Let $\beta:M\times N\rightarrow k$
be a $k$-bilinear form. Then, this form $\beta$ is said to be $\mathfrak{g}%
$\textit{-invariant} if and only if every $x\in\mathfrak{g}$, $a\in M$ and
$b\in N$ satisfy%
\[
\beta\left(  x\rightharpoonup a,b\right)  +\beta\left(  a,x\rightharpoonup
b\right)  =0.
\]


Instead of ``$\mathfrak{g}$-invariant'', one often says ``invariant''.
\end{definition}

The following remark gives an alternative characterization of $\mathfrak{g}%
$-invariant bilinear forms (which is occasionally used as an alternative
definition thereof):

\begin{remark}
\label{rmk.g-invar}Let $\mathfrak{g}$ be a Lie algebra over a field $k$. Let
$M$ and $N$ be two $\mathfrak{g}$-modules. Consider the tensor product
$M\otimes N$ of the two $\mathfrak{g}$-modules $M$ and $N$; this is known to
be a $\mathfrak{g}$-module again. Consider also $k$ as a $\mathfrak{g}$-module
(with the trivial $\mathfrak{g}$-module structure).

Let $\beta:M\times N\rightarrow k$ be a $k$-bilinear form. Let $B$ be the
linear map $M\otimes N\rightarrow k$ induced by the $k$-bilinear map
$\beta:M\times N\rightarrow k$ using the universal property of the tensor product.

Then, $\beta$ is $\mathfrak{g}$-invariant if and only if $B$ is a
$\mathfrak{g}$-module homomorphism.
\end{remark}

\begin{vershort}
We leave the proof of this remark as an instructive exercise for those who are
not already aware of it.
\end{vershort}

\begin{verlong}
\textit{Proof of Remark \ref{rmk.g-invar}.} We know that $B$ is the linear map
$M\otimes N\rightarrow k$ induced by the $k$-bilinear map $\beta:M\times
N\rightarrow k$ using the universal property of the tensor product. Hence, any
$a\in M$ and $b\in N$ satisfy%
\begin{equation}
B\left(  a\otimes b\right)  =\beta\left(  a,b\right)  . \label{pf.g-invar.Bb}%
\end{equation}


We are going to prove the following two assertions:

\textit{Assertion \ref{rmk.g-invar}.1:} If $\beta$ is $\mathfrak{g}%
$-invariant, then $B$ is a $\mathfrak{g}$-module homomorphism.

\textit{Assertion \ref{rmk.g-invar}.2:} If $B$ is a $\mathfrak{g}$-module
homomorphism, then $\beta$ is $\mathfrak{g}$-invariant.

\textit{Proof of Assertion \ref{rmk.g-invar}.1:} Assume that $\beta$ is
$\mathfrak{g}$-invariant. Therefore, every $x\in\mathfrak{g}$, $a\in M$ and
$b\in N$ satisfy%
\begin{equation}
\beta\left(  x\rightharpoonup a,b\right)  +\beta\left(  a,x\rightharpoonup
b\right)  =0 \label{pf.g-invar.1.1}%
\end{equation}
(because Definition \ref{def.g-invar} states that $\beta$ is $\mathfrak{g}%
$-invariant if and only if every $x\in\mathfrak{g}$, $a\in M$ and $b\in N$
satisfy (\ref{pf.g-invar.1.1})).

Now, let $x\in\mathfrak{g}$ and $u\in M\otimes N$. Since $u$ is a tensor in
$M\otimes N$, we can write $u$ in the form $u=\sum\limits_{i=1}^{n}\lambda
_{i}a_{i}\otimes b_{i}$ for some $n\in\mathbb{N}$, some elements $\lambda_{1}%
$, $\lambda_{2}$, $...$, $\lambda_{n}$ of $k$, some elements $a_{1}$, $a_{2}$,
$...$, $a_{n}$ of $M$ and some elements $b_{1}$, $b_{2}$, $...$, $b_{n}$ of
$N$. Consider this $n$, these $\lambda_{1}$, $\lambda_{2}$, $...$,
$\lambda_{n}$, these $a_{1}$, $a_{2}$, $...$, $a_{n}$, and these $b_{1}$,
$b_{2}$, $...$, $b_{n}$.

Since $u=\sum\limits_{i=1}^{n}\lambda_{i}a_{i}\otimes b_{i}$, we have%
\begin{align*}
x\rightharpoonup u  &  =x\rightharpoonup\left(  \sum\limits_{i=1}^{n}%
\lambda_{i}a_{i}\otimes b_{i}\right)  =\sum\limits_{i=1}^{n}\lambda
_{i}\underbrace{x\rightharpoonup\left(  a_{i}\otimes b_{i}\right)
}_{\substack{=\left(  x\rightharpoonup a_{i}\right)  \otimes b_{i}%
+a_{i}\otimes\left(  x\rightharpoonup b_{i}\right)  \\\text{(by the definition
of the }\mathfrak{g}\text{-module }M\otimes N\text{)}}}\\
&  =\sum\limits_{i=1}^{n}\lambda_{i}\left(  \left(  x\rightharpoonup
a_{i}\right)  \otimes b_{i}+a_{i}\otimes\left(  x\rightharpoonup b_{i}\right)
\right)  .
\end{align*}
Hence,%
\begin{align*}
B\left(  x\rightharpoonup u\right)   &  =B\left(  \sum\limits_{i=1}^{n}%
\lambda_{i}\left(  \left(  x\rightharpoonup a_{i}\right)  \otimes b_{i}%
+a_{i}\otimes\left(  x\rightharpoonup b_{i}\right)  \right)  \right) \\
&  =\sum\limits_{i=1}^{n}\lambda_{i}\underbrace{B\left(  \left(
x\rightharpoonup a_{i}\right)  \otimes b_{i}+a_{i}\otimes\left(
x\rightharpoonup b_{i}\right)  \right)  }_{\substack{=B\left(  \left(
x\rightharpoonup a_{i}\right)  \otimes b_{i}\right)  +B\left(  a_{i}%
\otimes\left(  x\rightharpoonup b_{i}\right)  \right)  \\\text{(since }B\text{
is }k\text{-linear)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\text{ is }k\text{-linear}\right)
\\
&  =\sum\limits_{i=1}^{n}\lambda_{i}\left(  \underbrace{B\left(  \left(
x\rightharpoonup a_{i}\right)  \otimes b_{i}\right)  }_{\substack{=\beta
\left(  x\rightharpoonup a_{i},b_{i}\right)  \\\text{(by (\ref{pf.g-invar.Bb}%
), applied}\\\text{to }x\rightharpoonup a_{i}\text{ and }b_{i}\text{ instead
of }a\text{ and }b\text{)}}}+\underbrace{B\left(  a_{i}\otimes\left(
x\rightharpoonup b_{i}\right)  \right)  }_{\substack{=\beta\left(
a_{i},x\rightharpoonup b_{i}\right)  \\\text{(by (\ref{pf.g-invar.Bb}),
applied}\\\text{to }a_{i}\text{ and }x\rightharpoonup b_{i}\text{ instead of
}a\text{ and }b\text{)}}}\right) \\
&  =\sum\limits_{i=1}^{n}\lambda_{i}\underbrace{\left(  \beta\left(
x\rightharpoonup a_{i},b_{i}\right)  +\beta\left(  a_{i},x\rightharpoonup
b_{i}\right)  \right)  }_{\substack{=0\\\text{(by (\ref{pf.g-invar.1.1}),
applied to}\\a=a_{i}\text{ and }b=b_{i}\text{)}}}=\sum\limits_{i=1}^{n}%
\lambda_{i}0=0.
\end{align*}
Comparing this with $x\rightharpoonup\left(  B\left(  u\right)  \right)  =0$
(because the $\mathfrak{g}$-module structure on $k$ is trivial), this yields
$B\left(  x\rightharpoonup u\right)  =x\rightharpoonup\left(  B\left(
u\right)  \right)  $.

Now, forget that we fixed $x$ and $u$. We thus have shown that $B\left(
x\rightharpoonup u\right)  =x\rightharpoonup\left(  B\left(  u\right)
\right)  $ for all $x\in\mathfrak{g}$ and $u\in M\otimes N$. In other words,
the map $B$ is a $\mathfrak{g}$-module homomorphism. This proves Assertion
\ref{rmk.g-invar}.1.

\textit{Proof of Assertion \ref{rmk.g-invar}.2:} Assume that $B$ is a
$\mathfrak{g}$-module homomorphism. Now, let $x\in\mathfrak{g}$, $a\in M$ and
$b\in N$. By the definition of the $\mathfrak{g}$-module $M\otimes N$, we have%
\[
x\rightharpoonup\left(  a\otimes b\right)  =\left(  x\rightharpoonup a\right)
\otimes b+a\otimes\left(  x\rightharpoonup b\right)  ,
\]
so that%
\begin{align*}
B\left(  x\rightharpoonup\left(  a\otimes b\right)  \right)   &  =B\left(
\left(  x\rightharpoonup a\right)  \otimes b+a\otimes\left(  x\rightharpoonup
b\right)  \right) \\
&  =\underbrace{B\left(  \left(  x\rightharpoonup a\right)  \otimes b\right)
}_{\substack{=\beta\left(  x\rightharpoonup a,b\right)  \\\text{(by
(\ref{pf.g-invar.Bb}), applied}\\\text{to }x\rightharpoonup a\text{ instead of
}a\text{)}}}+\underbrace{B\left(  a\otimes\left(  x\rightharpoonup b\right)
\right)  }_{\substack{=\beta\left(  a,x\rightharpoonup b\right)  \\\text{(by
(\ref{pf.g-invar.Bb}), applied}\\\text{to }x\rightharpoonup b\text{ instead of
}b\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\text{ is }k\text{-linear}\right)
\\
&  =\beta\left(  x\rightharpoonup a,b\right)  +\beta\left(  a,x\rightharpoonup
b\right)  .
\end{align*}
Comparing this with%
\begin{align*}
B\left(  x\rightharpoonup\left(  a\otimes b\right)  \right)   &
=x\rightharpoonup\left(  B\left(  a\otimes b\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\text{ is a }\mathfrak{g}%
\text{-module homomorphism}\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since the }\mathfrak{g}\text{-module
structure on }k\text{ is trivial}\right)  ,
\end{align*}
this yields $\beta\left(  x\rightharpoonup a,b\right)  +\beta\left(
a,x\rightharpoonup b\right)  =0$.

Now, forget that we fixed $x$, $a$ and $b$. We thus have shown that every
$x\in\mathfrak{g}$, $a\in M$ and $b\in N$ satisfy%
\begin{equation}
\beta\left(  x\rightharpoonup a,b\right)  +\beta\left(  a,x\rightharpoonup
b\right)  =0. \label{pf.g-invar.1.2}%
\end{equation}
In other words, $\beta$ is $\mathfrak{g}$-invariant (because Definition
\ref{def.g-invar} states that $\beta$ is $\mathfrak{g}$-invariant if and only
if every $x\in\mathfrak{g}$, $a\in M$ and $b\in N$ satisfy
(\ref{pf.g-invar.1.2})). This proves Assertion \ref{rmk.g-invar}.2.

Now, both Assertion \ref{rmk.g-invar}.1 and Assertion \ref{rmk.g-invar}.2 are
proven. Combining these two assertions, we conclude that $\beta$ is
$\mathfrak{g}$-invariant if and only if $B$ is a $\mathfrak{g}$-module
homomorphism. This proves Remark \ref{rmk.g-invar}.
\end{verlong}

Very often, the notion of a ``$\mathfrak{g}$-invariant'' bilinear form (as
defined in Definition \ref{def.g-invar}) is applied to forms on $\mathfrak{g}$
itself. In this case, it has to be interpreted as follows:

\begin{Convention}
\label{conv.g-invar}Let $\mathfrak{g}$ be a Lie algebra over a field $k$. Let
$\beta:\mathfrak{g}\times\mathfrak{g}\rightarrow k$ be a bilinear form. When
we say that $\beta$ is $\mathfrak{g}$-invariant without specifying the
$\mathfrak{g}$-module structure on $\mathfrak{g}$, we always tacitly
understand that the $\mathfrak{g}$-module structure on $\mathfrak{g}$ is the
adjoint one (i. e., the one defined by $x\rightharpoonup a=\left[  x,a\right]
$ for all $x\in\mathfrak{g}$ and $a\in\mathfrak{g}$).
\end{Convention}

The following remark provides two equivalent criteria for a bilinear form on
the Lie algebra $\mathfrak{g}$ itself to be $\mathfrak{g}$-invariant; they
will often be used tacitly:

\begin{remark}
\label{rmk.g-invar.g}Let $\mathfrak{g}$ be a Lie algebra over a field $k$. Let
$\beta:\mathfrak{g}\times\mathfrak{g}\rightarrow k$ be a $k$-bilinear form.

\textbf{(a)} The form $\beta$ is $\mathfrak{g}$-invariant if and only if every
elements $a$, $b$ and $c$ of $\mathfrak{g}$ satisfy $\beta\left(  \left[
a,b\right]  ,c\right)  +\beta\left(  b,\left[  a,c\right]  \right)  =0$.

\textbf{(b)} The form $\beta$ is $\mathfrak{g}$-invariant if and only if every
elements $a$, $b$ and $c$ of $\mathfrak{g}$ satisfy $\beta\left(  \left[
a,b\right]  ,c\right)  =\beta\left(  a,\left[  b,c\right]  \right)  $.
\end{remark}

\begin{vershort}
The proof of this remark is, again, completely straightforward.
\end{vershort}

\begin{verlong}
\textit{Proof of Remark \ref{rmk.g-invar.g}.} Consider $\mathfrak{g}$ as a
$\mathfrak{g}$-module using the adjoint action. Then,%
\begin{equation}
x\rightharpoonup a=\left[  x,a\right]  \ \ \ \ \ \ \ \ \ \ \text{for any }%
x\in\mathfrak{g}\text{ and }a\in\mathfrak{g}. \label{pf.g-invar.g.1}%
\end{equation}


\textbf{(a)} By Definition \ref{def.g-invar} (applied to $M=\mathfrak{g}$ and
$N=\mathfrak{g}$), we know that the form $\beta$ is $\mathfrak{g}$-invariant
if and only if every $x\in\mathfrak{g}$, $a\in\mathfrak{g}$ and $b\in
\mathfrak{g}$ satisfy $\beta\left(  x\rightharpoonup a,b\right)  +\beta\left(
a,x\rightharpoonup b\right)  =0$. Thus, we have the following equivalence of
assertions:%
\begin{align}
&  \ \left(  \text{the form }\beta\text{ is }\mathfrak{g}\text{-invariant}%
\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \text{every }x\in\mathfrak{g}\text{, }%
a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}\text{ satisfy }\beta\left(
\underbrace{x\rightharpoonup a}_{\substack{=\left[  x,a\right]  \\\text{(by
(\ref{pf.g-invar.g.1}))}}},b\right)  +\beta\left(
a,\underbrace{x\rightharpoonup b}_{\substack{=\left[  x,b\right]  \\\text{(by
(\ref{pf.g-invar.g.1}), applied}\\\text{to }b\text{ instead of }a\text{)}%
}}\right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \text{every }x\in\mathfrak{g}\text{, }%
a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}\text{ satisfy }\beta\left(
\left[  x,a\right]  ,b\right)  +\beta\left(  a,\left[  x,b\right]  \right)
=0\right) \label{pf.g-invar.g.a}\\
&  \Longleftrightarrow\ \left(  \text{every }a\in\mathfrak{g}\text{, }%
b\in\mathfrak{g}\text{ and }c\in\mathfrak{g}\text{ satisfy }\beta\left(
\left[  a,b\right]  ,c\right)  +\beta\left(  b,\left[  a,c\right]  \right)
=0\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the indices }x\text{,
}a\text{ and }b\text{ as }a\text{, }b\text{ and }c\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \text{every elements }a\text{, }b\text{ and
}c\text{ of }\mathfrak{g}\text{ satisfy }\beta\left(  \left[  a,b\right]
,c\right)  +\beta\left(  b,\left[  a,c\right]  \right)  =0\right)  .\nonumber
\end{align}
In other words, Remark \ref{rmk.g-invar.g} \textbf{(a)} is proven.

\textbf{(b)} We have the following equivalence of assertions:%
\begin{align*}
&  \ \left(  \text{the form }\beta\text{ is }\mathfrak{g}\text{-invariant}%
\right) \\
&  \Longleftrightarrow\ \left(  \text{every }x\in\mathfrak{g}\text{, }%
a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}\text{ satisfy }\beta\left(
\left[  x,a\right]  ,b\right)  +\beta\left(  a,\left[  x,b\right]  \right)
=0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.g-invar.g.a})}\right) \\
&  \Longleftrightarrow\ \left(  \text{every }b\in\mathfrak{g}\text{, }%
a\in\mathfrak{g}\text{ and }c\in\mathfrak{g}\text{ satisfy }\beta\left(
\left[  b,a\right]  ,c\right)  +\beta\left(  a,\left[  b,c\right]  \right)
=0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the indices }x\text{ and
}b\text{ as }b\text{ and }c\right) \\
&  \Longleftrightarrow\ \left(  \text{every elements }a\text{, }b\text{ and
}c\text{ of }\mathfrak{g}\text{ satisfy }\beta\left(  \left[  b,a\right]
,c\right)  +\beta\left(  a,\left[  b,c\right]  \right)  =0\right) \\
&  \Longleftrightarrow\ \left(  \text{every elements }a\text{, }b\text{ and
}c\text{ of }\mathfrak{g}\text{ satisfy }-\beta\left(  \left[  a,b\right]
,c\right)  +\beta\left(  a,\left[  b,c\right]  \right)  =0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every elements }a\text{, }b\text{ and }c\text{ of }\mathfrak{g}%
\text{ satisfy}\\
\beta\left(  \underbrace{\left[  b,a\right]  }_{=-\left[  a,b\right]
},c\right)  =\beta\left(  -\left[  a,b\right]  ,c\right)  =-\beta\left(
\left[  a,b\right]  ,c\right) \\
\text{(since }\beta\text{ is }k\text{-bilinear)}%
\end{array}
\right) \\
&  \Longleftrightarrow\ \left(  \text{every elements }a\text{, }b\text{ and
}c\text{ of }\mathfrak{g}\text{ satisfy }\beta\left(  a,\left[  b,c\right]
\right)  =\beta\left(  \left[  a,b\right]  ,c\right)  \right) \\
&  \Longleftrightarrow\ \left(  \text{every elements }a\text{, }b\text{ and
}c\text{ of }\mathfrak{g}\text{ satisfy }\beta\left(  \left[  a,b\right]
,c\right)  =\beta\left(  a,\left[  b,c\right]  \right)  \right)  .
\end{align*}
In other words, Remark \ref{rmk.g-invar.g} \textbf{(b)} is proven.
\end{verlong}

An example of a $\mathfrak{g}$-invariant bilinear form on $\mathfrak{g}$
itself for $\mathfrak{g}$ finite-dimensional is given by the so-called Killing form:

\begin{proposition}
\label{prop.killing}Let $\mathfrak{g}$ be a finite-dimensional Lie algebra
over a field $k$. Then, the form%
\begin{align*}
\mathfrak{g}\times\mathfrak{g}  &  \rightarrow k,\\
\left(  x,y\right)   &  \mapsto\operatorname*{Tr}\nolimits_{\mathfrak{g}%
}\left(  \left(  \operatorname*{ad}x\right)  \circ\left(  \operatorname*{ad}%
y\right)  \right)
\end{align*}
is a symmetric $\mathfrak{g}$-invariant bilinear form. This form is called the
\textit{Killing form} of the Lie algebra $\mathfrak{g}$.
\end{proposition}

\begin{proposition}
\label{prop.killing.simple}Let $\mathfrak{g}$ be a finite-dimensional
semisimple Lie algebra over $\mathbb{C}$.

\textbf{(a)} The Killing form of $\mathfrak{g}$ is nondegenerate.

\textbf{(b)} Any $\mathfrak{g}$-invariant bilinear form on $\mathfrak{g}$ is a
scalar multiple of the Killing form of $\mathfrak{g}$. (Hence, if
$\mathfrak{g}\neq0$, then the vector space of $\mathfrak{g}$-invariant
bilinear forms on $\mathfrak{g}$ is $1$-dimensional and spanned by the Killing form.)
\end{proposition}

\subsection{Affine Lie algebras}

Now let us introduce the so-called affine Lie algebras; this is a very general
construction from which a lot of infinite-dimensional Lie algebras emerge
(including the Heisenberg algebra defined above).

\begin{definition}
\label{def.loop}Let $\mathfrak{g}$ be a Lie algebra.

\textbf{(a)} The $\mathbb{C}$-Lie algebra $\mathfrak{g}$ induces (by extension
of scalars) a $\mathbb{C}\left[  t,t^{-1}\right]  $-Lie algebra%
\[
\mathbb{C}\left[  t,t^{-1}\right]  \otimes\mathfrak{g}=\left\{  \sum
\limits_{i\in\mathbb{Z}}a_{i}t^{i}\ \mid\ a_{i}\in\mathfrak{g}\text{; all but
finitely many }i\in\mathbb{Z}\text{ satisfy }a_{i}=0\right\}  .
\]
This Lie algebra $\mathbb{C}\left[  t,t^{-1}\right]  \otimes\mathfrak{g}$,
considered as a $\mathbb{C}$-Lie algebra, will be called the \textit{loop
algebra} of $\mathfrak{g}$, and denoted by $\mathfrak{g}\left[  t,t^{-1}%
\right]  $.

\textbf{(b)} Let $\left(  \cdot,\cdot\right)  $ be a symmetric bilinear form
on $\mathfrak{g}$ (that is, a symmetric bilinear map $\mathfrak{g}%
\times\mathfrak{g}\rightarrow\mathbb{C}$) which is $\mathfrak{g}$-invariant
(this means that $\left(  \left[  a,b\right]  ,c\right)  +\left(  b,\left[
a,c\right]  \right)  =0$ for all $a,b,c\in\mathfrak{g}$).

Then, we can define a $2$-cocycle $\omega$ on the loop algebra $\mathfrak{g}%
\left[  t,t^{-1}\right]  $ by%
\begin{equation}
\omega\left(  f,g\right)  =\sum\limits_{i\in\mathbb{Z}}i\left(  f_{i}%
,g_{-i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }f\in\mathfrak{g}\left[
t,t^{-1}\right]  \text{ and }g\in\mathfrak{g}\left[  t,t^{-1}\right]
\label{loop.w}%
\end{equation}
(where we write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$
with $f_{i}\in\mathfrak{g}$, and where we write $g$ in the form $g=\sum
\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in\mathfrak{g}$).

Proving that $\omega$ is a $2$-cocycle is an exercise. So we can define a
$1$-dimensional central extension $\mathfrak{g}\left[  t,t^{-1}\right]
_{\omega}=\mathfrak{g}\left[  t,t^{-1}\right]  \oplus\mathbb{C}$ with bracket
defined by $\omega$.

We are going to abbreviate $\mathfrak{g}\left[  t,t^{-1}\right]  _{\omega}$ by
$\widehat{\mathfrak{g}}_{\omega}$, or, more radically, by
$\widehat{\mathfrak{g}}$.
\end{definition}

\begin{remark}
The equation (\ref{loop.w}) can be rewritten in the (laconical but suggestive)
form $\omega\left(  f,g\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
df,g\right)  $. Here, $\left(  df,g\right)  $ is to be understood as follows:
Extend the bilinear form $\left(  \cdot,\cdot\right)  :\mathfrak{g}%
\times\mathfrak{g}\rightarrow\mathbb{C}$ to a bilinear form $\left(
\cdot,\cdot\right)  :\mathfrak{g}\left[  t,t^{-1}\right]  \times
\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}\left[
t,t^{-1}\right]  $ by setting
\[
\left(  at^{i},bt^{j}\right)  =\left(  a,b\right)  t^{i+j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }b\in
\mathfrak{g}\text{, }i\in\mathbb{Z}\text{ and }j\in\mathbb{Z}.
\]
Also, for every $f\in\mathfrak{g}\left[  t,t^{-1}\right]  $, define the
``derivative'' $f^{\prime}$ of $f$ to be the element $\sum\limits_{i\in
\mathbb{Z}}if_{i}t^{i-1}$ of $\mathfrak{g}\left[  t,t^{-1}\right]  $ (where we
write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$ with
$f_{i}\in\mathfrak{g}$). In analogy to the notation $dg=g^{\prime}dt$ which we
introduced in Definition \ref{def.diffform}, set $\left(  df,g\right)  $ to
mean the polynomial differential form $\left(  f^{\prime},g\right)  dt$ for
any $f\in\mathfrak{g}\left[  t,t^{-1}\right]  $ and $g\in\mathfrak{g}\left[
t,t^{-1}\right]  $. Then, it is very easy to see that $\operatorname*{Res}%
\nolimits_{t=0}\left(  df,g\right)  =\sum\limits_{i\in\mathbb{Z}}i\left(
f_{i},g_{-i}\right)  $ (where we write $f$ in the form $f=\sum\limits_{i\in
\mathbb{Z}}f_{i}t^{i}$ with $f_{i}\in\mathfrak{g}$, and where we write $g$ in
the form $g=\sum\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in
\mathfrak{g}$), so that we can rewrite (\ref{loop.w}) as $\omega\left(
f,g\right)  =\operatorname*{Res}\nolimits_{t=0}\left(  df,g\right)  $.
\end{remark}

We already know one example of the construction in Definition \ref{def.loop}:

\begin{remark}
If $\mathfrak{g}$ is the abelian Lie algebra $\mathbb{C}$, and $\left(
\cdot,\cdot\right)  $ is the bilinear form $\mathbb{C}\times\mathbb{C}%
\rightarrow\mathbb{C},\ \left(  x,y\right)  \mapsto xy$, then the $2$-cocycle
$\omega$ on the loop algebra $\mathbb{C}\left[  t,t^{-1}\right]  $ is given by%
\[
\omega\left(  f,g\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
gdf\right)  =\sum\limits_{i\in\mathbb{Z}}if_{i}g_{-i}%
\ \ \ \ \ \ \ \ \ \ \text{for every }f,g\in\mathbb{C}\left[  t,t^{-1}\right]
\]
(where we write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$
with $f_{i}\in\mathbb{C}$, and where we write $g$ in the form $g=\sum
\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in\mathbb{C}$). Hence, in this
case, the central extension $\mathfrak{g}\left[  t,t^{-1}\right]  _{\omega
}=\widehat{\mathfrak{g}}_{\omega}$ is precisely the Heisenberg algebra
$\mathcal{A}$ as introduced in Definition \ref{def.osc}.
\end{remark}

The main example that we will care about is when $\mathfrak{g}$ is a simple
finite-dimensional Lie algebra and $\left(  \cdot,\cdot\right)  $ is the
unique (up to scalar) invariant symmetric bilinear form (i. e., a multiple of
the Killing form). In this case, the Lie algebra $\widehat{\mathfrak{g}%
}=\widehat{\mathfrak{g}}_{\omega}$ is called an \textit{affine Lie algebra}.

\begin{theorem}
\label{thm.H^2(gtt)}If $\mathfrak{g}$ is a simple finite-dimensional Lie
algebra, then $H^{2}\left(  \mathfrak{g}\left[  t,t^{-1}\right]  \right)  $ is
$1$-dimensional and spanned by the cocycle $\omega$ corresponding to $\left(
\cdot,\cdot\right)  $.
\end{theorem}

\begin{corollary}
\label{cor.g_w^hat}If $\mathfrak{g}$ is a simple finite-dimensional Lie
algebra, then the Lie algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $ has a
unique (up to isomorphism of Lie algebras, not up to isomorphism of
extensions) nontrivial $1$-dimensional central extension
$\widehat{\mathfrak{g}}_{\omega}$.
\end{corollary}

\begin{definition}
\label{def.kac}The Lie algebra $\widehat{\mathfrak{g}}_{\omega}$ defined in
Corollary \ref{cor.g_w^hat} (for $\left(  \cdot,\cdot\right)  $ being the
Killing form of $\mathfrak{g}$) is called the \textit{affine Kac-Moody
algebra} corresponding to $\mathfrak{g}$. (Or, more precisely, the
\textit{untwisted affine Kac-Moody algebra} corresponding to $\mathfrak{g}$.)
\end{definition}

In order to prepare for the proof of Theorem \ref{thm.H^2(gtt)}, we recollect
some facts from the cohomology of Lie algebras:

\begin{definition}
\label{def.semidir}Let $\mathfrak{g}$ be a Lie algebra. Let $M$ be a
$\mathfrak{g}$-module. We define the \textit{semidirect product}
$\mathfrak{g}\ltimes M$ to be the Lie algebra which, as a vector space, is
$\mathfrak{g}\oplus M$, but whose Lie bracket is defined by%
\begin{align*}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]   &
=\left(  \left[  a,b\right]  ,a\rightharpoonup\beta-b\rightharpoonup
\alpha\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }a\in\mathfrak{g}\text{, }%
\alpha\in M\text{, }b\in\mathfrak{g}\text{ and }\beta\in M\right.  .
\end{align*}
(The symbol $\rightharpoonup$ means action here; i. e., a term like
$c\rightharpoonup m$ (with $c\in\mathfrak{g}$ and $m\in M$) means the action
of $c$ on $m$.) Thus, the canonical injection $\mathfrak{g}\rightarrow
\mathfrak{g}\ltimes M,$ $a\mapsto\left(  a,0\right)  $ is a Lie algebra
homomorphism, and so is the canonical projection $\mathfrak{g}\ltimes
M\rightarrow\mathfrak{g},$ $\left(  a,\alpha\right)  \mapsto a$. Also, $M$ is
embedded into $\mathfrak{g}\ltimes M$ by the injection $M\rightarrow
\mathfrak{g}\ltimes M,$ $\alpha\mapsto\left(  0,\alpha\right)  $; this makes
$M$ an abelian Lie subalgebra of $\mathfrak{g}\ltimes M$.
\end{definition}

All statements made in Definition \ref{def.semidir} (including the tacit
statement that the Lie bracket on $\mathfrak{g}\ltimes M$ defined in
Definition \ref{def.semidir} satisfies antisymmetry and the Jacobi identity)
are easy to verify by computation. The semidirect product that we have just
defined is not the most general notion of a semidirect product. We will later
(Definition \ref{def.semidir.lielie}) define a more general one, where $M$
itself may have a Lie algebra structure and this structure has an effect on
that of $\mathfrak{g}\ltimes M$. But for now, Definition \ref{def.semidir}
suffices for us.

\begin{definition}
Let $\mathfrak{g}$ be a Lie algebra. Let $M$ be a $\mathfrak{g}$-module.

\textbf{(a)} A $1$\textit{-cocycle} \textit{of }$\mathfrak{g}$\textit{ with
coefficients in }$M$ is a linear map $\eta:\mathfrak{g}\rightarrow M$ such
that%
\[
\eta\left(  \left[  a,b\right]  \right)  =a\rightharpoonup\eta\left(
b\right)  -b\rightharpoonup\eta\left(  a\right)  \ \ \ \ \ \ \ \ \ \ \text{for
all }a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}.
\]
(The symbol $\rightharpoonup$ means action here; i. e., a term like
$c\rightharpoonup m$ (with $c\in\mathfrak{g}$ and $m\in M$) means the action
of $c$ on $m$.)

It is easy to see (and known) that $1$-cocycles of $\mathfrak{g}$ with
coefficients in $M$ are in bijection with Lie algebra homomorphisms
$\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M$. This bijection sends every
$1$-cocycle $\eta$ to the map $\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,$
$a\mapsto\left(  a,\eta\left(  a\right)  \right)  $.

Notice that $1$-cocycles of $\mathfrak{g}$ with coefficients in the
$\mathfrak{g}$-module $\mathfrak{g}$ are exactly the same as derivations of
$\mathfrak{g}$.

\textbf{(b)} A $1$\textit{-coboundary of }$\mathfrak{g}$ \textit{with
coefficients in }$M$ means a linear map $\eta:\mathfrak{g}\rightarrow M$ which
has the form $a\mapsto a\rightharpoonup m$ for some $m\in M$. Every
$1$-coboundary of $\mathfrak{g}$ with coefficients in $M$ is a $1$-cocycle.

\textbf{(c)} The space of $1$-cocycles of $\mathfrak{g}$ with coefficients in
$M$ is denoted by $Z^{1}\left(  \mathfrak{g},M\right)  $. The space of
$1$-coboundaries of $\mathfrak{g}$ with coefficients in $M$ is denoted by
$B^{1}\left(  \mathfrak{g},M\right)  $. We have $B^{1}\left(  \mathfrak{g}%
,M\right)  \subseteq Z^{1}\left(  \mathfrak{g},M\right)  $. The quotient space
$Z^{1}\left(  \mathfrak{g},M\right)  \diagup B^{1}\left(  \mathfrak{g}%
,M\right)  $ is denoted by $H^{1}\left(  \mathfrak{g},M\right)  $ is called
the $1$\textit{-st cohomology space} of $\mathfrak{g}$\textit{ with
coefficients in }$M$.

Of course, these spaces $Z^{1}\left(  \mathfrak{g},M\right)  $, $B^{1}\left(
\mathfrak{g},M\right)  $ and $H^{1}\left(  \mathfrak{g},M\right)  $ are but
particular cases of more general constructions $Z^{i}\left(  \mathfrak{g}%
,M\right)  $, $B^{i}\left(  \mathfrak{g},M\right)  $ and $H^{i}\left(
\mathfrak{g},M\right)  $ which are defined for every $i\in\mathbb{N}$. (In
particular, $H^{0}\left(  \mathfrak{g},M\right)  $ is the subspace $\left\{
m\in M\ \mid\ a\rightharpoonup m=0\text{ for all }a\in\mathfrak{g}\right\}  $
of $M$, and often denoted by $M^{\mathfrak{g}}$.) The spaces $H^{i}\left(
\mathfrak{g},M\right)  $ (or, more precisely, the functors assigning these
spaces to every $\mathfrak{g}$-module $M$) can be understood as the so-called
derived functors of the functor $M\mapsto M^{\mathfrak{g}}$. However, we won't
use $H^{i}\left(  \mathfrak{g},M\right)  $ for any $i$ other than $1$ here.

We record a relation between $H^{1}\left(  \mathfrak{g},M\right)  $ and the
$\operatorname*{Ext}$ bifunctor:%
\[
H^{1}\left(  \mathfrak{g},M\right)  =\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)  .
\]
More generally, $\operatorname*{Ext}\nolimits_{\mathfrak{g}}^{1}\left(
N,M\right)  =H^{1}\left(  \mathfrak{g},\operatorname*{Hom}%
\nolimits_{\mathbb{C}}\left(  N,M\right)  \right)  $ for any two
$\mathfrak{g}$-modules $N$ and $M$.
\end{definition}

\begin{theorem}
[Whitehead]\label{thm.white}If $\mathfrak{g}$ is a simple finite-dimensional
Lie algebra, and $M$ is a finite-dimensional $\mathfrak{g}$-module, then
$H^{1}\left(  \mathfrak{g},M\right)  =0$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.white}.} Since $\mathfrak{g}$ is a simple
Lie algebra, Weyl's theorem says that finite-dimensional $\mathfrak{g}%
$-modules are completely reducible. Hence, if $N$ and $M$ are
finite-dimensional $\mathfrak{g}$-modules, we have $\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  N,M\right)  =0$. In particular,
$\operatorname*{Ext}\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)
=0$. Since $H^{1}\left(  \mathfrak{g},M\right)  =\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)  $, this yields
$H^{1}\left(  \mathfrak{g},M\right)  =0$. Theorem \ref{thm.white} is thus proven.

\begin{lemma}
\label{lem.Z^1}Let $\omega$ be a $2$-cocycle on a Lie algebra $\mathfrak{g}$.
Let $\mathfrak{g}_{0}\subseteq\mathfrak{g}$ be a Lie subalgebra, and
$M\subseteq\mathfrak{g}$ be a $\mathfrak{g}_{0}$-submodule. Then, $\omega
\mid_{\mathfrak{g}_{0}\times M}$, when considered as a map $\mathfrak{g}%
_{0}\rightarrow M^{\ast}$, belongs to $Z^{1}\left(  \mathfrak{g}_{0},M^{\ast
}\right)  $.
\end{lemma}

The proof of Lemma \ref{lem.Z^1} is a straightforward manipulation of formulas:

\textit{Proof of Lemma \ref{lem.Z^1}.} Let $\eta$ denote the $2$-cocycle
$\omega\mid_{\mathfrak{g}_{0}\times M}$, considered as a map $\mathfrak{g}%
_{0}\rightarrow M^{\ast}$. Thus, $\eta$ is defined by%
\[
\eta\left(  x\right)  =\left(  M\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ y\mapsto\omega\left(  x,y\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}_{0}.
\]
Hence,
\begin{equation}
\left(  \eta\left(  x\right)  \right)  \left(  y\right)  =\omega\left(
x,y\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}_{0}\text{ and
}y\in M. \label{pf.Z^1.eta}%
\end{equation}


Thus, any $a\in\mathfrak{g}_{0}$, $b\in\mathfrak{g}_{0}$ and $c\in M$ satisfy
$\left(  \eta\left(  \left[  a,b\right]  \right)  \right)  \left(  c\right)
=\omega\left(  \left[  a,b\right]  ,c\right)  $ and%
\begin{align*}
&  \left(  a\rightharpoonup\eta\left(  b\right)  -b\rightharpoonup\eta\left(
a\right)  \right)  \left(  c\right) \\
&  =\underbrace{\left(  a\rightharpoonup\eta\left(  b\right)  \right)  \left(
c\right)  }_{\substack{=-\left(  \eta\left(  b\right)  \right)  \left(
\left[  a,c\right]  \right)  \\\text{(by the definition of the dual of a
}\mathfrak{g}_{0}\text{-module)}}}-\underbrace{\left(  b\rightharpoonup
\eta\left(  a\right)  \right)  \left(  c\right)  }_{\substack{=-\left(
\eta\left(  a\right)  \right)  \left(  \left[  b,c\right]  \right)
\\\text{(by the definition of the dual of a }\mathfrak{g}_{0}\text{-module)}%
}}\\
&  =\left(  -\underbrace{\left(  \eta\left(  b\right)  \right)  \left(
\left[  a,c\right]  \right)  }_{\substack{=\omega\left(  b,\left[  a,c\right]
\right)  \\\text{(by (\ref{pf.Z^1.eta}))}}}\right)  -\left(
-\underbrace{\left(  \eta\left(  a\right)  \right)  \left(  \left[
b,c\right]  \right)  }_{\substack{=\omega\left(  a,\left[  b,c\right]
\right)  \\\text{(by (\ref{pf.Z^1.eta}))}}}\right)  =\left(  -\omega\left(
b,\left[  a,c\right]  \right)  \right)  -\left(  -\omega\left(  a,\left[
b,c\right]  \right)  \right) \\
&  =-\omega\left(  b,\underbrace{\left[  a,c\right]  }_{=-\left[  c,a\right]
}\right)  +\omega\left(  a,\left[  b,c\right]  \right)  =\underbrace{\omega
\left(  b,\left[  c,a\right]  \right)  }_{\substack{=-\omega\left(  \left[
c,a\right]  ,b\right)  \\\text{(since }\omega\text{ is antisymmetric)}%
}}+\underbrace{\omega\left(  a,\left[  b,c\right]  \right)  }%
_{\substack{=-\omega\left(  \left[  b,c\right]  ,a\right)  \\\text{(since
}\omega\text{ is antisymmetric)}}}\\
&  =-\omega\left(  \left[  c,a\right]  ,b\right)  -\omega\left(  \left[
b,c\right]  ,a\right)  =\omega\left(  \left[  a,b\right]  ,c\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.2-cocyc.eq})}\right)  ,
\end{align*}
so that $\left(  \eta\left(  \left[  a,b\right]  \right)  \right)  \left(
c\right)  =\left(  a\rightharpoonup\eta\left(  b\right)  -b\rightharpoonup
\eta\left(  a\right)  \right)  \left(  c\right)  $. Thus, any $a\in
\mathfrak{g}_{0}$ and $b\in\mathfrak{g}_{0}$ satisfy $\eta\left(  \left[
a,b\right]  \right)  =a\rightharpoonup\eta\left(  b\right)  -b\rightharpoonup
\eta\left(  a\right)  $. This shows that $\eta$ is a $1$-cocycle, i. e.,
belongs to $Z^{1}\left(  \mathfrak{g}_{0},M^{\ast}\right)  $. Lemma
\ref{lem.Z^1} is proven.

\textit{Proof of Theorem \ref{thm.H^2(gtt)}.} First notice that any
$a,b,c\in\mathfrak{g}$ satisfy%
\begin{equation}
\left(  \left[  a,b\right]  ,c\right)  =\left(  \left[  b,c\right]  ,a\right)
=\left(  \left[  c,a\right]  ,b\right)  \label{thm.H^2(gtt).pf.0}%
\end{equation}
\footnote{\textit{Proof.} First of all, any $a,b,c\in\mathfrak{g}$ satisfy%
\begin{align*}
\left(  \left[  a,b\right]  ,c\right)   &  =\left(  a,\left[  b,c\right]
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the form }\left(  \cdot
,\cdot\right)  \text{ is invariant}\right) \\
&  =\left(  \left[  b,c\right]  ,a\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the form }\left(  \cdot,\cdot\right)  \text{ is symmetric}\right)
.
\end{align*}
Applying this to $b,c,a$ instead of $a,b,c$, we obtain $\left(  \left[
b,c\right]  ,a\right)  =\left(  \left[  c,a\right]  ,b\right)  $. Hence,
$\left(  \left[  a,b\right]  ,c\right)  =\left(  \left[  b,c\right]
,a\right)  =\left(  \left[  c,a\right]  ,b\right)  $, so that
(\ref{thm.H^2(gtt).pf.0}) is proven.}. Moreover,%
\begin{equation}
\text{there exist }a,b,c\in\mathfrak{g}\text{ such that }\left(  \left[
a,b\right]  ,c\right)  =\left(  \left[  b,c\right]  ,a\right)  =\left(
\left[  c,a\right]  ,b\right)  \neq0. \label{thm.H^2(gtt).pf.00}%
\end{equation}
\footnote{\textit{Proof.} Since $\mathfrak{g}$ is simple, we have $\left[
\mathfrak{g},\mathfrak{g}\right]  =\mathfrak{g}$ and thus $\left(  \left[
\mathfrak{g},\mathfrak{g}\right]  ,\mathfrak{g}\right)  =\left(
\mathfrak{g},\mathfrak{g}\right)  \neq0$ (since the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate). Hence, there exist $a,b,c\in\mathfrak{g}$
such that $\left(  \left[  a,b\right]  ,c\right)  \neq0$. The rest is handled
by (\ref{thm.H^2(gtt).pf.0}).} This will be used later in our proof; but as
for now, forget about these $a,b,c$.

It is easy to see that the $2$-cocycle $\omega$ on $\mathfrak{g}\left[
t,t^{-1}\right]  $ defined by (\ref{loop.w}) is not a $2$%
-coboundary.\footnote{\textit{Proof.} Assume the contrary. Then, this
$2$-cocycle $\omega$ is a coboundary, i. e., there exists a linear map
$\xi:\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$ such that
$\omega=d\xi$.
\par
Now, pick some $a\in\mathfrak{g}$ and $b\in\mathfrak{g}$ such that $\left(
a,b\right)  \neq0$ (this is possible since the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate). Then,%
\[
\underbrace{\omega}_{=d\xi}\left(  at,bt^{-1}\right)  =\left(  d\xi\right)
\left(  at,bt^{-1}\right)  =\xi\left(  \underbrace{\left[  at,bt^{-1}\right]
}_{=\left[  a,b\right]  }\right)  =\xi\left(  \left[  a,b\right]  \right)
\]
and%
\[
\underbrace{\omega}_{=d\xi}\left(  a,b\right)  =\left(  d\xi\right)  \left(
a,b\right)  =\xi\left(  \left[  a,b\right]  \right)  ,
\]
so that $\omega\left(  at,bt^{-1}\right)  =\omega\left(  a,b\right)  $. But by
the definition of $\omega$, we easily see that $\omega\left(  at,bt^{-1}%
\right)  =1\underbrace{\left(  a,b\right)  }_{\neq0}\neq0$ and $\omega\left(
a,b\right)  =0\left(  a,b\right)  =0$, which yields a contradiction.}

Now let us consider the structure of $\mathfrak{g}\left[  t,t^{-1}\right]  $.
We have $\mathfrak{g}\left[  t,t^{-1}\right]  =\bigoplus\limits_{n\in
\mathbb{Z}}\mathfrak{g}t^{n}\supseteq\mathfrak{g}t^{0}=\mathfrak{g}$. This is,
actually, an inclusion of Lie algebras. So $\mathfrak{g}$ is a Lie subalgebra
of $\mathfrak{g}\left[  t,t^{-1}\right]  $, and $\mathfrak{g}t^{n}$ is a
$\mathfrak{g}$-submodule of $\mathfrak{g}\left[  t,t^{-1}\right]  $ isomorphic
to $\mathfrak{g}$ for every $n\in\mathbb{Z}$.

Let $\omega$ be an arbitrary $2$-cocycle on $\mathfrak{g}\left[
t,t^{-1}\right]  $ (not necessarily the one defined by (\ref{loop.w})).

Let $n\in\mathbb{Z}$. Then, $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}%
$, when considered as a map $\mathfrak{g}\rightarrow\left(  \mathfrak{g}%
t^{n}\right)  ^{\ast}$, belongs to $Z^{1}\left(  \mathfrak{g},\left(
\mathfrak{g}t^{n}\right)  ^{\ast}\right)  $ (by Lemma \ref{lem.Z^1}, applied
to $\mathfrak{g}$, $\mathfrak{g}t^{n}$ and $\mathfrak{g}\left[  t,t^{-1}%
\right]  $ instead of $\mathfrak{g}_{0}$, $M$ and $\mathfrak{g}$), i. e., is a
$1$-cocycle. But by Theorem \ref{thm.white}, we have $H^{1}\left(
\mathfrak{g},\left(  \mathfrak{g}t^{n}\right)  ^{\ast}\right)  =0$, so this
rewrites as $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}\in B^{1}\left(
\mathfrak{g},\left(  \mathfrak{g}t^{n}\right)  ^{\ast}\right)  $. In other
words, there exists some $\xi_{n}\in\left(  \mathfrak{g}t^{n}\right)  ^{\ast}$
such that $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}=d\xi_{n}$. Pick
such a $\xi_{n}$. Thus,%
\[
\omega\left(  a,bt^{n}\right)  =\underbrace{\left(  \omega\mid_{\mathfrak{g}%
\times\mathfrak{g}t^{n}}\right)  }_{=d\xi_{n}}\left(  a,bt^{n}\right)
=\left(  d\xi_{n}\right)  \left(  a,bt^{n}\right)  =\xi_{n}\left(  \left[
a,bt^{n}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in
\mathfrak{g}.
\]


Define a map $\xi:\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$
by requiring that $\xi\mid_{\mathfrak{g}t^{n}}=\xi_{n}$ for every
$n\in\mathbb{Z}$.

Now, let $\widetilde{\omega}=\omega-d\xi$. Then,%
\[
\widetilde{\omega}\left(  x,y\right)  =\omega\left(  x,y\right)  -\xi\left(
\left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
x,y\in\mathfrak{g}\left[  t,t^{-1}\right]  .
\]
Replace $\omega$ by $\widetilde{\omega}$ (this doesn't change the residue
class of $\omega$ in $H^{2}\left(  \mathfrak{g}\left[  t,t^{-1}\right]
\right)  $, since $\widetilde{\omega}$ differs from $\omega$ by a
$2$-coboundary). By doing this, we have reduced to a situation when
\[
\omega\left(  a,bt^{n}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
a,b\in\mathfrak{g}\text{ and }n\in\mathbb{Z}.
\]
\footnote{But all the $\xi$-freedom has been used up in this reduction - i.
e., if the new $\omega$ is nonzero, then the original $\omega$ was not a
$2$-coboundary. This gives us an alternative way of proving that the
$2$-cocycle $\omega$ on $\mathfrak{g}\left[  t,t^{-1}\right]  $ defined by
(\ref{loop.w}) is not a $2$-coboundary.} Since $\omega$ is antisymmetric, this
yields%
\begin{equation}
\omega\left(  bt^{n},a\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
a,b\in\mathfrak{g}\text{ and }n\in\mathbb{Z}. \label{thm.H^2(gtt).pf.1}%
\end{equation}


Now, fix some $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Since $\omega$ is a
$2$-cocycle, the $2$-cocycle condition yields%
\begin{align*}
0  &  =\omega\left(  \underbrace{\left[  a,bt^{n}\right]  }_{=\left[
a,b\right]  t^{n}},ct^{m}\right)  +\omega\left(  \underbrace{\left[
ct^{m},a\right]  }_{\substack{=\left[  c,a\right]  t^{m}\\=-\left[
a,c\right]  t^{m}}},bt^{n}\right)  +\omega\left(  \underbrace{\left[
bt^{n},ct^{m}\right]  }_{=\left[  b,c\right]  t^{n+m}},a\right) \\
&  =\omega\left(  \left[  a,b\right]  t^{n},ct^{m}\right)  +\underbrace{\omega
\left(  -\left[  a,c\right]  t^{m},bt^{n}\right)  }_{=\omega\left(
bt^{n},\left[  a,c\right]  t^{m}\right)  }+\underbrace{\omega\left(  \left[
b,c\right]  t^{n+m},a\right)  }_{\substack{=0\\\text{(by
(\ref{thm.H^2(gtt).pf.1}))}}}\\
&  =\omega\left(  \left[  a,b\right]  t^{n},ct^{m}\right)  +\omega\left(
bt^{n},\left[  a,c\right]  t^{m}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a,b,c\in\mathfrak{g}\text{.}%
\end{align*}
In other words, the bilinear form on $\mathfrak{g}$ given by $\left(
b,c\right)  \mapsto\omega\left(  bt^{n},ct^{m}\right)  $ is $\mathfrak{g}%
$-invariant. But every $\mathfrak{g}$-invariant bilinear form on
$\mathfrak{g}$ must be a multiple of our bilinear form $\left(  \cdot
,\cdot\right)  $ (since $\mathfrak{g}$ is simple, and thus the space of all
$\mathfrak{g}$-invariant bilinear forms on $\mathfrak{g}$ is $1$%
-dimensional\footnote{and spanned by the Killing form}). Hence, there exists
some constant $\gamma_{n,m}\in\mathbb{C}$ (depending on $n$ and $m$) such
that
\begin{equation}
\omega\left(  bt^{n},ct^{m}\right)  =\gamma_{n,m}\cdot\left(  b,c\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }b,c\in\mathfrak{g}.
\label{thm.H^2(gtt).pf.2}%
\end{equation}
It is easy to see that%
\begin{equation}
\gamma_{n,m}=-\gamma_{m,n}\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z},
\label{thm.H^2(gtt).pf.3}%
\end{equation}
since the bilinear form $\omega$ is skew-symmetric whereas the bilinear form
$\left(  \cdot,\cdot\right)  $ is symmetric.

Now, for any $m\in\mathbb{Z}$, $n\in\mathbb{Z}$ and $p\in\mathbb{Z}$, the
$2$-cocycle condition yields%
\[
\omega\left(  \left[  at^{n},bt^{m}\right]  ,ct^{p}\right)  +\omega\left(
\left[  bt^{m},ct^{p}\right]  ,at^{n}\right)  +\omega\left(  \left[
ct^{p},at^{n}\right]  ,bt^{m}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all
}a,b,c\in\mathfrak{g}.
\]
Due to%
\[
\omega\left(  \underbrace{\left[  at^{n},bt^{m}\right]  }_{=\left[
a,b\right]  t^{n+m}},ct^{p}\right)  =\omega\left(  \left[  a,b\right]
t^{n+m},ct^{p}\right)  =\gamma_{n+m,p}\cdot\left(  \left[  a,b\right]
,c\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.H^2(gtt).pf.2}%
)}\right)
\]
and the two cyclic permutations of this identity, this rewrites as%
\[
\gamma_{n+m,p}\cdot\left(  \left[  a,b\right]  ,c\right)  +\gamma_{m+p,n}%
\cdot\left(  \left[  b,c\right]  ,a\right)  +\gamma_{p+n,m}\cdot\left(
\left[  c,a\right]  ,b\right)  =0.
\]
Since this holds for all $a,b,c\in\mathfrak{g}$, we can use
(\ref{thm.H^2(gtt).pf.00}) to transform this into%
\[
\gamma_{n+m,p}+\gamma_{m+p,n}+\gamma_{p+n,m}=0.
\]
Due to (\ref{thm.H^2(gtt).pf.3}), this rewrites as%
\[
\gamma_{n,m+p}+\gamma_{m,p+n}+\gamma_{p,m+n}=0.
\]
Denoting by $s$ the sum $m+n+p$, we can rewrite this as%
\[
\gamma_{n,s-n}+\gamma_{m,s-m}-\gamma_{m+n,s-m-n}=0.
\]
In other words, for fixed $s\in\mathbb{Z}$, the function $\mathbb{Z}%
\rightarrow\mathbb{C},$ $n\mapsto\gamma_{n,s-n}$ is additive. Hence,
$\gamma_{n,s-n}=n\gamma_{1,s-1}$ and $\gamma_{s-n,n}=\left(  s-n\right)
\gamma_{1,s-1}$ for every $n\in\mathbb{Z}$. Thus,
\begin{align*}
\left(  s-n\right)  \gamma_{1,s-1}  &  =\gamma_{s-n,n}=-\gamma_{n,s-n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.H^2(gtt).pf.3})}\right) \\
&  =-n\gamma_{1,s-1}\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}%
\end{align*}
Hence, $s\gamma_{1,s-1}=0$. Thus, for every $s\neq0$, we conclude that
$\gamma_{1,s-1}=0$ and hence $\gamma_{n,s-n}=n\underbrace{\gamma_{1,s-1}}%
_{=0}=0$ for every $n\in\mathbb{Z}$. In other words, $\gamma_{n,m}=0$ for
every $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfying $n+m\neq0$.

What happens for $s=0$ ? For $s=0$, the equation $\gamma_{n,s-n}%
=n\gamma_{1,s-1}$ becomes $\gamma_{n,-n}=n\gamma_{1,-1}$.

Thus we have proven that $\gamma_{n,m}=0$ for every $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$ satisfying $n+m\neq0$, and that every $n\in\mathbb{Z}$
satisfies $\gamma_{n,-n}=n\gamma_{1,-1}$.

Hence, the form $\omega$ must be a scalar multiple of the form which sends
every $\left(  f,g\right)  $ to $\operatorname*{Res}\nolimits_{t=0}%
\underbrace{\left(  df,g\right)  }_{\text{scalar-valued }1\text{-form}}%
=\sum\limits_{i\in\mathbb{Z}}i\left(  f_{i},g_{-i}\right)  $. We have thus
proven that every $2$-cocycle $\omega$ is a scalar multiple of the $2$-cocycle
$\omega$ defined by (\ref{loop.w}) modulo the $2$-coboundaries. Since we also
know that the $2$-cocycle $\omega$ defined by (\ref{loop.w}) is not a
$2$-coboundary, this yields that the space $H^{2}\left(  \mathfrak{g}\left[
t,t^{-1}\right]  \right)  $ is $1$-dimensional and spanned by the residue
class of the $2$-cocycle $\omega$ defined by (\ref{loop.w}). This proves
Theorem \ref{thm.H^2(gtt)}.

\section{Representation theory: generalities}

\subsection{Representation theory: general facts}

The first step in the representation theory of any objects (groups, algebras,
etc.) is usually proving some kind of Schur's lemma. There is one form of
Schur's lemma that holds almost tautologically: This is the form that claims
that every morphism between irreducible representations is either $0$ or an
isomorphism.\footnote{There are also variations on this assertion:
\par
\textbf{1)} Every morphism from an irreducible representation to a
representation is either $0$ or injective.
\par
\textbf{2)} Every morphism from a representation to an irreducible
representation is either $0$ or surjective.
\par
Both of these variations follow very easily from the definition of
``irreducible''.} However, the more often used form of Schur's lemma is a bit
different: It claims that, over an algebraically closed field, every
endomorphism of a finite-dimensional irreducible representation is a scalar
multiple of the identity map. This is usually proven using eigenvalues, and
this proof depends on the fact that eigenvalues exist; this (in general)
requires the irreducible representation to be \textit{finite-dimensional}.
Hence, it should not come as a surprise that this latter form of Schur's lemma
does not generally hold for infinite-dimensional representations. This makes
this lemma not particularly useful in the case of infinite-dimensional Lie
algebras. But we still can show the following version of Schur's lemma over
$\mathbb{C}$:

\begin{lemma}
[Dixmier's Lemma]\label{lem.dix}Let $A$ be an algebra over $\mathbb{C}$, and
let $V$ be an irreducible $A$-module of countable dimension. Then, any
$A$-module homomorphism $\phi:V\rightarrow V$ is a scalar multiple of the identity.
\end{lemma}

This lemma is called \textit{Dixmier's lemma}, and its proof is similar to the
famous proof of the Nullstellensatz over $\mathbb{C}$ using the uncountability
of $\mathbb{C}$.

\textit{Proof of Lemma \ref{lem.dix}.} Let $D=\operatorname*{End}%
\nolimits_{A}V$. Then, $D$ is a division algebra (in fact, the endomorphism
ring of an irreducible representation always is a division algebra).

For any nonzero $v\in V$, we have $Av=V$ (otherwise, $Av$ would be a nonzero
proper $A$-submodule of $V$, contradicting the fact that $V$ is irreducible
and thus does not have any such submodules). In other words, for any nonzero
$v\in V$, every element of $V$ can be written as $av$ for some $a\in A$. Thus,
for any nonzero $v\in V$, any element $\phi\in D$ is completely determined by
$\phi\left(  v\right)  $ (because $\phi\left(  av\right)  =a\phi\left(
v\right)  $ for every $a\in A$, so that the value $\phi\left(  v\right)  $
uniquely determines the value of $\phi\left(  av\right)  $ for every $a\in A$,
and thus (since we know that every element of $V$ can be written as $av$ for
some $a\in A$) every value of $\phi$ is uniquely determined). Thus, we have an
embedding of $D$ into $V$. Hence, $D$ is countably-dimensional (since $V$ is
countably-dimensional). But a countably-dimensional division algebra $D$ over
$\mathbb{C}$ must be $\mathbb{C}$ itself\footnote{\textit{Proof.} Indeed,
assume the contrary. So there exists some $\phi\in D$ not belonging to
$\mathbb{C}$. Then, $\phi$ is transcendental over $\mathbb{C}$, so that
$\mathbb{C}\left(  \phi\right)  \subseteq D$ is the field of rational
functions in one variable $\phi$ over $\mathbb{C}$. Now, $\mathbb{C}\left(
\phi\right)  $ contains the rational function $\dfrac{1}{\phi-\lambda}$ for
every $\lambda\in\mathbb{C}$, and these rational functions for varying
$\lambda$ are linearly independent. Since $\mathbb{C}$ is uncountable, we thus
have an uncountable linearly independent set of elements of $\mathbb{C}\left(
\phi\right)  $, contradicting the fact that $\mathbb{C}\left(  \phi\right)  $
is a subspace of the countably-dimensional space $D$, qed.}, so that
$D=\mathbb{C}$, and this is exactly what we wanted to show. Lemma
\ref{lem.dix} is proven.

Note that Lemma \ref{lem.dix} is a general fact, not particular to Lie
algebras; however, it is not as general as it seems: It really makes use of
the uncountability of $\mathbb{C}$, not just of the fact that $\mathbb{C}$ is
an algebraically closed field of characteristic $0$. It would be wrong if we
would replace $\mathbb{C}$ by (for instance) the algebraic closure of
$\mathbb{Q}$.

\begin{remark}
\label{rem.dix}Let $A$ be a countably-dimensional algebra over $\mathbb{C}$,
and let $V$ be an irreducible $A$-module. Then, $V$ itself is countably dimensional.
\end{remark}

\textit{Proof of Remark \ref{rem.dix}.} For any nonzero $v\in V$, we have
$Av=V$ (by the same argument as in the proof of Lemma \ref{lem.dix}), and thus
$\dim\left(  Av\right)  =\dim V$. Since $\dim\left(  Av\right)  \leq\dim A$,
we thus have $\dim V=\dim\left(  Av\right)  \leq\dim A$, so that $V$ has
countable dimension (since $A$ has countable dimension). This proves Remark
\ref{rem.dix}.

\begin{corollary}
\label{cor.dix2}Let $A$ be an algebra over $\mathbb{C}$, and let $V$ be an
irreducible $A$-module of countable dimension. Let $C$ be a central element of
$A$. Then, $C\mid_{V}$ is a scalar (i. e., a scalar multiple of the identity map).
\end{corollary}

\textit{Proof of Corollary \ref{cor.dix2}.} Since $C$ is central, the element
$C$ commutes with any element of $A$. Thus, $C\mid_{V}$ is an $A$-module
homomorphism, and hence (by Lemma \ref{lem.dix}, applied to $\phi=C\mid_{V}$)
a scalar multiple of the identity. This proves Corollary \ref{cor.dix2}.

\subsection{Representations of the Heisenberg algebra
\texorpdfstring{$\mathcal{A}$}{A}}

\subsubsection{General remarks}

Consider the oscillator algebra (aka Heisenberg algebra) $\mathcal{A}%
=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle +\left\langle
K\right\rangle $. Recall that%
\begin{align*}
\left[  a_{i},a_{j}\right]   &  =i\delta_{i,-j}K\ \ \ \ \ \ \ \ \ \ \text{for
any }i,j\in\mathbb{Z};\\
\left[  K,a_{i}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for any }%
i\in\mathbb{Z}.
\end{align*}


Let us try to classify the irreducible $\mathcal{A}$-modules.

Let $V$ be an irreducible $\mathcal{A}$-module. Then, $V$ is
countably-dimensional (by Remark \ref{rem.dix}, since $U\left(  \mathcal{A}%
\right)  $ is countably-dimensional), so that by Corollary \ref{cor.dix2}, the
endomorphism $K\mid_{V}$ is a scalar (because $K$ is a central element of
$\mathcal{A}$ and thus also a central element of $U\left(  \mathcal{A}\right)
$).

If $K\mid_{V}=0$, then $V$ is a module over the Lie algebra $\mathcal{A}%
\diagup\mathbb{C}K=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle $.
But since $\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle $ is an
abelian Lie algebra, irreducible modules over $\left\langle a_{i}\ \mid
\ i\in\mathbb{Z}\right\rangle $ are $1$-dimensional (again by Corollary
\ref{cor.dix2}), so that $V$ must be $1$-dimensional in this case. Thus, the
case when $K\mid_{V}=0$ is not an interesting case.

Now consider the case when $K\mid_{V}=k\neq0$. Then, we can WLOG assume that
$k=1$, because the Lie algebra $\mathcal{A}$ has an automorphism sending $K$
to $\lambda K$ for any arbitrary $\lambda\neq0$ (this automorphism is given by
$a_{i}\mapsto\lambda a_{i}$ for $i>0$, and $a_{i}\mapsto a_{i}$ for $i\leq0$).

We are thus interested in irreducible representations $V$ of $\mathcal{A}$
satisfying $K\mid_{V}=1$. These are in an obvious 1-to-1 correspondence with
irreducible representations of $U\left(  \mathcal{A}\right)  \diagup\left(
K-1\right)  $.

\begin{proposition}
\label{prop.K-1}We have an algebra isomorphism%
\[
\xi:U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  \rightarrow
D\left(  x_{1},x_{2},x_{3},...\right)  \otimes\mathbb{C}\left[  x_{0}\right]
,
\]
where $D\left(  x_{1},x_{2},x_{3},...\right)  $ is the algebra of differential
operators in the variables $x_{1}$, $x_{2}$, $x_{3}$, $...$ with polynomial
coefficients. This isomorphism is given by%
\begin{align*}
\xi\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{0}\right)   &  =x_{0}.
\end{align*}

\end{proposition}

Note that we are sloppy with notation here: Since $\xi$ is a homomorphism from
$U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  $ (rather than
$U\left(  \mathcal{A}\right)  $), we should write $\xi\left(  \overline
{a_{-i}}\right)  $ instead of $\xi\left(  a_{-i}\right)  $, etc.. We are using
the same letters to denote elements of $U\left(  \mathcal{A}\right)  $ and
their residue classes in $U\left(  \mathcal{A}\right)  \diagup\left(
K-1\right)  $, and are relying on context to keep them apart. We hope that the
reader will forgive us this abuse of notation.

\textit{Proof of Proposition \ref{prop.K-1}.} It is clear\footnote{from the
universal property of the universal enveloping algebra, and the universal
property of the quotient algebra} that there exists a unique algebra
homomorphism $\xi:U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)
\rightarrow D\left(  x_{1},x_{2},x_{3},...\right)  $ satisfying%
\begin{align*}
\xi\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{0}\right)   &  =x_{0}.
\end{align*}
It is also clear that this $\xi$ is surjective (since all the generators
$x_{i}$, $\dfrac{\partial}{\partial x_{i}}$ and $x_{0}$ of the algebra
$D\left(  x_{1},x_{2},x_{3},...\right)  \otimes\mathbb{C}\left[  x_{0}\right]
$ are in its image).

In the following, a map $\varphi:A\rightarrow\mathbb{N}$ (where $A$ is some
set) is said to be \textit{finitely supported} if all but finitely many $a\in
A$ satisfy $\varphi\left(  a\right)  =0$. Sequences (finite, infinite, or
two-sided infinite) are considered as maps (from finite sets, $\mathbb{N}$ or
$\mathbb{Z}$, or occasionally other sets). Thus, a sequence is finitely
supported if and only if all but finitely many of its elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

By the easy part of the Poincar\'{e}-Birkhoff-Witt theorem (this is the part
which states that the increasing monomials \textit{span} the universal
enveloping algebra\footnote{The hard part says that these increasing monomials
are linearly independent.}), the family\footnote{Here, $\overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}$ denotes the product
$...a_{-2}^{n_{-2}}a_{-1}^{n_{-1}}a_{0}^{n_{0}}a_{1}^{n_{1}}a_{2}^{n_{2}}...$.
(This product is infinite, but still has a value since only finitely many
$n_{i}$ are nonzero.)}%
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}\cdot
K^{m}\right)  _{\left(  ...,n_{-2},n_{-1},n_{0},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}},\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}\right)  $. Hence,
the family
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}%
}\right)  _{\left(  ...,n_{-2},n_{-1},n_{0},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}}}%
\]
is a spanning set of $U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)
$, and since this family maps to a linearly independent set under $\xi$ (this
is very easy to see), it follows that $\xi$ is injective. Thus, $\xi$ is an
isomorphism, so that Proposition \ref{prop.K-1} is proven.

\begin{definition}
\label{def.A0}Define a vector subspace $\mathcal{A}_{0}$ of $\mathcal{A}$ by
$\mathcal{A}_{0}=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\diagdown\left\{
0\right\}  \right\rangle +\left\langle K\right\rangle $.
\end{definition}

\begin{proposition}
\label{prop.A0}This subspace $\mathcal{A}_{0}$ is a Lie subalgebra of
$\mathcal{A}$, and $\mathbb{C}a_{0}$ is also a Lie subalgebra of $\mathcal{A}%
$. We have $\mathcal{A}=\mathcal{A}_{0}\oplus\mathbb{C}a_{0}$ as Lie algebras.
Hence,%
\[
U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  =U\left(
\mathcal{A}_{0}\oplus\mathbb{C}a_{0}\right)  \diagup\left(  K-1\right)
\cong\underbrace{\left(  U\left(  \mathcal{A}_{0}\right)  \diagup\left(
K-1\right)  \right)  }_{\cong D\left(  x_{1},x_{2},x_{3},...\right)  }%
\otimes\underbrace{\mathbb{C}\left[  a_{0}\right]  }_{\cong\mathbb{C}\left[
x_{0}\right]  }%
\]
(since $K\in\mathcal{A}_{0}$). Here, the isomorphism $U\left(  \mathcal{A}%
_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(  x_{1},x_{2}%
,x_{3},...\right)  $ is defined as follows: In analogy to Proposition
\ref{prop.K-1}, we have an algebra isomorphism%
\[
\widetilde{\xi}:U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)
\rightarrow D\left(  x_{1},x_{2},x_{3},...\right)
\]
given by%
\begin{align*}
\widetilde{\xi}\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for
}i\geq1;\\
\widetilde{\xi}\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}%
}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1.
\end{align*}

\end{proposition}

The proof of Proposition \ref{prop.A0} is analogous to that of Proposition
\ref{prop.K-1} (where it is not completely straightforward).

\subsubsection{The Fock space}

From Proposition \ref{prop.A0}, we know that
\begin{align*}
U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(
x_{1},x_{2},x_{3},...\right)  \subseteq\operatorname*{End}\left(
\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \right)  .
\end{align*}
Hence, we have a $\mathbb{C}$-algebra homomorphism $U\left(  \mathcal{A}%
_{0}\right)  \rightarrow\operatorname*{End}\left(  \mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  \right)  $. This makes $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ into a representation of the Lie algebra
$\mathcal{A}_{0}$. Let us state this as a corollary:

\begin{corollary}
\label{cor.fock}The Lie algebra $\mathcal{A}_{0}$ has a representation
$F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ which is given by
\begin{align*}
a_{-i}  &  \mapsto x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where ``$a_{-i}\mapsto x_{i}$'' is just shorthand for ``$a_{-i}\mapsto\left(
\text{multiplication by }x_{i}\right)  $''). For every $\mu\in\mathbb{C}$, we
can upgrade $F$ to a representation $F_{\mu}$ of $\mathcal{A}$ by adding the
condition that $a_{0}\mid_{F_{\mu}}=\mu\cdot\operatorname*{id}$.
\end{corollary}

\begin{definition}
\label{def.fock}The representation $F$ of $\mathcal{A}_{0}$ introduced in
Corollary \ref{cor.fock} is called the \textit{Fock module} or the
\textit{Fock representation}. For every $\mu\in\mathbb{C}$, the representation
$F_{\mu}$ of $\mathcal{A}$ introduced in Corollary \ref{cor.fock} will be
called the $\mu$\textit{-Fock representation} of $\mathcal{A}$. The vector
space $F$ itself is called the \textit{Fock space}.
\end{definition}

Let us now define some gradings to make these infinite-dimensional spaces more manageable:

\begin{definition}
\label{def.A.grad}Let us grade the vector space $\mathcal{A}$ by
$\mathcal{A}=\bigoplus\limits_{n\in\mathbb{Z}}\mathcal{A}\left[  n\right]  $,
where $\mathcal{A}\left[  n\right]  =\left\langle a_{n}\right\rangle $ for
$n\neq0$, and where $\mathcal{A}\left[  0\right]  =\left\langle a_{0}%
,K\right\rangle $.\ With this grading, we have $\left[  \mathcal{A}\left[
n\right]  ,\mathcal{A}\left[  m\right]  \right]  \subseteq\mathcal{A}\left[
n+m\right]  $ for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. (In other words,
the Lie algebra $\mathcal{A}$ with the decomposition $\mathcal{A}%
=\bigoplus\limits_{n\in\mathbb{Z}}\mathcal{A}\left[  n\right]  $ is a
$\mathbb{Z}$-graded Lie algebra. The notion of a ``$\mathbb{Z}$-graded Lie
algebra'' that we have just used is defined in Definition \ref{def.gradLie}.)
\end{definition}

Note that we are denoting the $n$-th homogeneous component of $\mathcal{A}$ by
$\mathcal{A}\left[  n\right]  $ rather than $\mathcal{A}_{n}$, since otherwise
the notation $\mathcal{A}_{0}$ would have two different meanings.

\begin{definition}
\label{def.fock.grad}We grade the polynomial algebra $F$ by setting
$\deg\left(  x_{i}\right)  =-i$ for each $i$. Thus, $F=\bigoplus
\limits_{n\geq0}F\left[  -n\right]  $, where $F\left[  -n\right]  $ is the
space of polynomials of degree $-n$, where the degree is our degree defined by
$\deg\left(  x_{i}\right)  =-i$ (so that, for instance, $x_{1}^{2}+x_{2}$ is
homogeneous of degree $-2$). With this grading, $\dim\left(  F\left[
-n\right]  \right)  $ is the number $p\left(  n\right)  $ of all partitions of
$n$. Hence,%
\[
\sum\limits_{n\geq0}\dim\left(  F\left[  -n\right]  \right)  q^{n}%
=\sum\limits_{n\geq0}p\left(  n\right)  q^{n}=\dfrac{1}{\left(  1-q\right)
\left(  1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}=\dfrac{1}%
{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }%
\]
in the ring of power series $\mathbb{Z}\left[  \left[  q\right]  \right]  $.

We use the same grading for $F_{\mu}$ for every $\mu\in\mathbb{C}$. That is,
we define the grading on $F_{\mu}$ by $F_{\mu}\left[  n\right]  =F\left[
n\right]  $ for every $n\in\mathbb{Z}$.
\end{definition}

\begin{remark}
\label{rmk.fockgrad}Some people prefer to grade $F_{\mu}$ somewhat differently
from $F$: namely, they shift the grading for $F_{\mu}$ by $\dfrac{\mu^{2}}{2}%
$, so that $\deg1=-\dfrac{\mu^{2}}{2}$ in $F_{\mu}$, and generally $F_{\mu
}\left[  z\right]  =F\left[  \dfrac{\mu^{2}}{2}+z\right]  $ (as vector spaces)
for every $z\in\mathbb{C}$. This is a grading by complex numbers rather than
integers (in general). (The advantage of this grading is that we will
eventually find an operator whose eigenspace to the eigenvalue $n$ is $F_{\mu
}\left[  n\right]  =F\left[  \dfrac{\mu^{2}}{2}+n\right]  $ for every
$n\in\mathbb{C}$.)

With this grading, the equality $\sum\limits_{n\geq0}\dim\left(  F\left[
-n\right]  \right)  q^{n}=\dfrac{1}{\prod\limits_{i\geq1}\left(
1-q^{i}\right)  }$ rewrites as $\sum\limits_{n\in\mathbb{C}}\dim\left(
F_{\mu}\left[  -n\right]  \right)  q^{n+\dfrac{\mu^{2}}{2}}=\dfrac{q^{\mu^{2}%
}}{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }$, if we allow power series
with complex exponents. We define a ``power series'' $\operatorname*{ch}%
\left(  F_{\mu}\right)  $ by%
\[
\operatorname*{ch}\left(  F_{\mu}\right)  =\sum\limits_{n\in\mathbb{C}}%
\dim\left(  F_{\mu}\left[  -n\right]  \right)  q^{n+\dfrac{\mu^{2}}{2}}%
=\dfrac{q^{\mu^{2}}}{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }.
\]
But we will not use this grading; instead we will use the grading defined in
Definition \ref{def.fock.grad}.
\end{remark}

\begin{proposition}
\label{prop.F.irrep}The representation $F$ is an irreducible representation of
$\mathcal{A}_{0}$.
\end{proposition}

\begin{lemma}
\label{lem.F.P1=P}For every $P\in F$, we have
\[
P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1=P\ \ \ \ \ \ \ \ \ \ \text{in
}F.
\]
(Here, the term $P\left(  a_{-1},a_{-2},a_{-3},...\right)  $ denotes the
evaluation of the polynomial $P$ at $\left(  x_{1},x_{2},x_{3},...\right)
=\left(  a_{-1},a_{-2},a_{-3},...\right)  $. This evaluation is a well-defined
element of $U\left(  \mathcal{A}_{0}\right)  $, since the elements $a_{-1}$,
$a_{-2}$, $a_{-3}$, $...$ of $U\left(  \mathcal{A}_{0}\right)  $ commute.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.F.P1=P}.} For every $Q\in F$, let
$\operatorname*{mult}Q$ denote the map $F\rightarrow F,$ $R\mapsto QR$. (In
Proposition \ref{prop.K-1}, we abused notations and denoted this map simply by
$Q$; but we will not do this in this proof.) Then, by the definition of $\xi$,
we have $\xi\left(  a_{-i}\right)  =\operatorname*{mult}\left(  x_{i}\right)
$ for every $i\geq1$.

Since we have defined an endomorphism $\operatorname*{mult}Q\in
\operatorname*{End}F$ for every $Q\in F$, we thus obtain a map
$\operatorname*{mult}:F\rightarrow\operatorname*{End}F$. This map
$\operatorname*{mult}$ is an algebra homomorphism (since it describes the
action of $F$ on the $F$-module $F$).

Let $P\in F$. Since $\xi$ is an algebra homomorphism, and thus commutes with
polynomials, we have
\begin{align*}
&  \xi\left(  P\left(  a_{-1},a_{-2},a_{-3},...\right)  \right) \\
&  =P\left(  \xi\left(  a_{-1}\right)  ,\xi\left(  a_{-2}\right)  ,\xi\left(
a_{-3}\right)  ,...\right)  =P\left(  \operatorname*{mult}\left(
x_{1}\right)  ,\operatorname*{mult}\left(  x_{2}\right)  ,\operatorname*{mult}%
\left(  x_{3}\right)  ,...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\xi\left(  a_{-i}\right)
=\operatorname*{mult}\left(  x_{i}\right)  \text{ for every }i\geq1\right) \\
&  =\operatorname*{mult}\left(  \underbrace{P\left(  x_{1},x_{2}%
,x_{3},...\right)  }_{=P}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{mult}\text{ is an algebra homomorphism,}\\
\text{and thus commutes with polynomials}%
\end{array}
\right) \\
&  =\operatorname*{mult}P.
\end{align*}
Thus,%
\[
P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1=\left(  \operatorname*{mult}%
P\right)  \left(  1\right)  =P\cdot1=P.
\]
This proves Lemma \ref{lem.F.P1=P}.

\textit{Proof of Proposition \ref{prop.F.irrep}.} \textbf{1)} The
representation $F$ is generated by $1$ as a $U\left(  \mathcal{A}_{0}\right)
$-module (due to Lemma \ref{lem.F.P1=P}). In other words, $F=U\left(
\mathcal{A}_{0}\right)  \cdot1$.

\textbf{2)} Let us forget about the grading on $F$ which we defined in
Definition \ref{def.fock.grad}, and instead, once again, define a grading on
$F$ by $\deg\left(  x_{i}\right)  =1$ for every $i\in\left\{
1,2,3,...\right\}  $. Thus, the degree of a polynomial $P\in F$ with respect
to this grading is what is usually referred to as the degree of the polynomial
$P$.

\begin{verlong}
If $P\in F$ and if $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ is
a monomial in $P$ of degree $\deg P$, with $\alpha\neq0$, then $\dfrac
{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}%
!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...P=\alpha$%
\ \ \ \ \footnote{\textit{Proof.} Let $P\in F$. Let $\alpha\cdot x_{1}^{m_{1}%
}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ be a monomial in $P$ of degree $\deg P$, with
$\alpha\neq0$. Since the monomial $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...$ has degree $\deg P$, we have%
\[
\deg P=\deg\left(  \alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...\right)  =m_{1}+m_{2}+m_{3}+....
\]
\par
For every set $A$, define $\mathbb{N}_{\operatorname*{fin}}^{A}$ as in the
proof of Proposition \ref{prop.K-1}.
\par
Now, for every $\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$, let $\beta_{\left(
n_{1},n_{2},n_{3},...\right)  }$ be the coefficient of the polynomial $P$
before the monomial $x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$. Then,
$\beta_{\left(  n_{1},n_{2},n_{3},...\right)  }=0$ for every $\left(
n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$ satisfying $n_{1}+n_{2}+n_{3}+...>\deg P$ (because for
every $\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfying $n_{1}%
+n_{2}+n_{3}+...>\deg P$, the monomial $x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}%
^{n_{3}}...$ has degree $n_{1}+n_{2}+n_{3}+...>\deg P$, and thus the
coefficient of the polynomial $P$ before this monomial must be $0$). On the
other hand, $\beta_{\left(  m_{1},m_{2},m_{3},...\right)  }=\alpha$ (since
$\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ is a monomial in $P$,
and thus the coefficient of the polynomial $P$ before the monomial
$x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ is $\alpha$).
\par
On the other hand, recall that $\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}$ is the coefficient of the polynomial $P$ before the monomial $x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ for every $\left(  n_{1},n_{2},n_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}$. Hence,%
\begin{align*}
P  &  =\sum\limits_{\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\beta_{\left(
n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P}}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...+\sum\limits_{\substack{\left(
n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  };\\n_{1}+n_{2}+n_{3}+...>\deg P}}\underbrace{\beta
_{\left(  n_{1},n_{2},n_{3},...\right)  }}_{\substack{=0\\\text{(since }%
n_{1}+n_{2}+n_{3}+...>\deg P\text{)}}}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}%
}...\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P}}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...+\underbrace{\sum
\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}+n_{2}%
+n_{3}+...>\deg P}}0x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...}_{=0}\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P}}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(
m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...+\underbrace{\beta_{\left(
m_{1},m_{2},m_{3},...\right)  }}_{=\alpha}x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  m_{1},m_{2},m_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}\text{ satisfies }m_{1}+m_{2}+m_{3}+...=\deg P\right) \\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(
m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...+\alpha x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}....
\end{align*}
Thus,%
\begin{align}
&  \dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}%
}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...P\nonumber\\
&  =\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}%
}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(  \sum
\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}+n_{2}%
+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(
m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...+\alpha x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(
m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}%
}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(  x_{1}^{n_{1}}%
x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)  +\alpha\underbrace{\dfrac{\partial
_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}%
\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(  x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  }_{=\dfrac{m_{1}!}{m_{1}!}\dfrac{m_{2}!}{m_{2}%
!}\dfrac{m_{3}!}{m_{3}!}...=1}\nonumber\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(
m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}%
}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(  x_{1}^{n_{1}}%
x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)  +\alpha. \label{pf.F.irrep.long.1}%
\end{align}
\par
But now, let $\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ be a sequence
satisfying $n_{1}+n_{2}+n_{3}+...\leq\deg P$ and $\left(  n_{1},n_{2}%
,n_{3},...\right)  \neq\left(  m_{1},m_{2},m_{3},...\right)  $. Since
$n_{1}+n_{2}+n_{3}+...\leq\deg P=m_{1}+m_{2}+m_{3}+...$ but $\left(
n_{1},n_{2},n_{3},...\right)  \neq\left(  m_{1},m_{2},m_{3},...\right)  $, it
is clear that there exists at least one $\ell\in\left\{  1,2,3,...\right\}  $
satisfying $n_{\ell}<m_{\ell}$. Consider such an $\ell$. Since the
differential operators $\partial_{x_{1}}$, $\partial_{x_{2}}$, $\partial
_{x_{3}}$, $...$ commute, we have $\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}%
!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}%
}{m_{3}!}...=\left(  \prod\limits_{i\in\left\{  1,2,3,...\right\}
\setminus\left\{  \ell\right\}  }\dfrac{\partial_{x_{i}}^{m_{i}}}{m_{i}%
!}\right)  \circ\dfrac{\partial_{x_{\ell}}^{m_{\ell}}}{m_{\ell}!}$, so that%
\begin{align*}
\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}%
{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(  x_{1}^{n_{1}}%
x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)   &  =\left(  \left(  \prod
\limits_{i\in\left\{  1,2,3,...\right\}  \setminus\left\{  \ell\right\}
}\dfrac{\partial_{x_{i}}^{m_{i}}}{m_{i}!}\right)  \circ\dfrac{\partial
_{x_{\ell}}^{m_{\ell}}}{m_{\ell}!}\right)  \left(  x_{1}^{n_{1}}x_{2}^{n_{2}%
}x_{3}^{n_{3}}...\right) \\
&  =\left(  \prod\limits_{i\in\left\{  1,2,3,...\right\}  \setminus\left\{
\ell\right\}  }\dfrac{\partial_{x_{i}}^{m_{i}}}{m_{i}!}\right)
\underbrace{\left(  \dfrac{\partial_{x_{\ell}}^{m_{\ell}}}{m_{\ell}!}\left(
x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)  \right)  }%
_{\substack{=0\\\text{(since }n_{\ell}<m_{\ell}\text{)}}}\\
&  =\left(  \prod\limits_{i\in\left\{  1,2,3,...\right\}  \setminus\left\{
\ell\right\}  }\dfrac{\partial_{x_{i}}^{m_{i}}}{m_{i}!}\right)  \left(
0\right)  =0.
\end{align*}
\par
Now, forget that we fixed $\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$. We have thus
proven that every sequence $\left(  n_{1},n_{2},n_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfying
$n_{1}+n_{2}+n_{3}+...\leq\deg P$ and $\left(  n_{1},n_{2},n_{3},...\right)
\neq\left(  m_{1},m_{2},m_{3},...\right)  $ must satisfy $\dfrac
{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}%
!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(  x_{1}^{n_{1}}x_{2}%
^{n_{2}}x_{3}^{n_{3}}...\right)  =0$. Hence, (\ref{pf.F.irrep.long.1}) becomes%
\begin{align*}
&  \dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}%
}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...P\\
&  =\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\n_{1}%
+n_{2}+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(
m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2},n_{3},...\right)
}\underbrace{\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}%
}^{m_{2}}}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...\left(
x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)  }%
_{\substack{=0\\\text{(since }n_{1}+n_{2}+n_{3}+...\leq\deg P\\\text{and
}\left(  n_{1},n_{2},n_{3},...\right)  \neq\left(  m_{1},m_{2},m_{3}%
,...\right)  \text{)}}}+\alpha\\
&  =\underbrace{\sum\limits_{\substack{\left(  n_{1},n_{2},n_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }%
;\\n_{1}+n_{2}+n_{3}+...\leq\deg P;\\\left(  n_{1},n_{2},n_{3},...\right)
\neq\left(  m_{1},m_{2},m_{3},...\right)  }}\beta_{\left(  n_{1},n_{2}%
,n_{3},...\right)  }0}_{=0}+\alpha=\alpha,
\end{align*}
qed.}.
\end{verlong}

\begin{vershort}
If $P\in F$ and if $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ is
a monomial in $P$ of degree $\deg P$, with $\alpha\neq0$, then $\dfrac
{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}%
!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...P=\alpha$%
\ \ \ \ \footnote{\textit{Proof.} Let $P\in F$. Let $\alpha\cdot x_{1}^{m_{1}%
}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ be a monomial in $P$ of degree $\deg P$, with
$\alpha\neq0$.
\par
WLOG, no variable other than $x_{1}$, $x_{2}$, $...$, $x_{k}$ appears in $P$,
for some $k\in\mathbb{N}$. Thus, $x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...=x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$ and $\dfrac{\partial_{x_{1}%
}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}\dfrac
{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...=\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}%
!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}%
}{m_{k}!}$.
\par
Thus, $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}=\alpha\cdot
x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ is a monomial in $P$ of degree
$\deg P$.
\par
When we apply the differential operator $\dfrac{\partial_{x_{1}}^{m_{1}}%
}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}%
}^{m_{k}}}{m_{k}!}$ to $P$, all monomials $\beta\cdot x_{1}^{n_{1}}%
x_{2}^{n_{2}}...x_{k}^{n_{k}}$ with $\left(  n_{\ell}<m_{\ell}\text{ for at
least one }\ell\in\left\{  1,2,...,k\right\}  \right)  $ are annihilated
(because if $n_{\ell}<m_{\ell}$ for some $\ell$, then $\dfrac{\partial_{x_{1}%
}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac
{\partial_{x_{k}}^{m_{k}}}{m_{k}!}\left(  \beta\cdot x_{1}^{n_{1}}x_{2}%
^{n_{2}}...x_{k}^{n_{k}}\right)  =0$). Hence, the only monomials in $P$ which
survive under this operator are monomials of the form $\beta\cdot x_{1}%
^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}$ with each $n_{\ell}$ being $\geq$ to
the corresponding $m_{\ell}$. But since $m_{1}+m_{2}+...+m_{k}=\deg P$
(because $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$ is a
monomial of degree $\deg P$), the only such monomial in $P$ is $\alpha\cdot
x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$ (because for every other monomial
of the form $\beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}$ with each
$n_{\ell}$ being $\geq$ to the corresponding $m_{\ell}$, the sum $n_{1}%
+n_{2}+...+n_{k}$ must be greater than $m_{1}+m_{2}+...+m_{k}=\deg P$, and
thus such a monomial cannot occur in $P$). Hence, the only monomial in $P$
which survives is the monomial $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}%
...x_{k}^{m_{k}}$. This monomial clearly gets mapped to $\alpha$ by the
differential operator $\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac
{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}!}%
$. Thus, $\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}%
}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}!}P=\alpha$. Since
$\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}%
}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}!}=\dfrac{\partial_{x_{1}%
}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}\dfrac
{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...$, this rewrites as $\dfrac
{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}%
!}\dfrac{\partial_{x_{3}}^{m_{3}}}{m_{3}!}...P=\alpha$, qed.}.
\end{vershort}

Thus, for every nonzero $P\in F$, we have $1\in U\left(  \mathcal{A}%
_{0}\right)  \cdot P$\ \ \ \ \footnote{\textit{Proof.} Let $P\in F$ be
nonzero. Then, there exist a monomial $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...$ in $P$ of degree $P$ with $\alpha\neq0$. Consider such a
monomial. As shown above, we have $\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}%
!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}%
}{m_{3}!}...P=\alpha$. But we know that $a_{i}\in\mathcal{A}_{0}$ acts as
$i\dfrac{\partial}{\partial x_{i}}$ on $F$ for every $i\geq1$. Thus,
$\dfrac{1}{i}a_{i}\in\mathcal{A}_{0}$ acts as $\dfrac{\partial}{\partial
x_{i}}=\partial_{x_{i}}$ on $F$ for every $i\geq1$. Hence,%
\[
\dfrac{\left(  \dfrac{1}{1}a_{1}\right)  ^{m_{1}}}{m_{1}!}\dfrac{\left(
\dfrac{1}{2}a_{2}\right)  ^{m_{2}}}{m_{2}!}\dfrac{\left(  \dfrac{1}{3}%
a_{3}\right)  ^{m_{3}}}{m_{3}!}...P=\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}%
!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}\dfrac{\partial_{x_{3}}^{m_{3}}%
}{m_{3}!}...P=\alpha.
\]
Consequently,%
\[
\alpha=\dfrac{\left(  \dfrac{1}{1}a_{1}\right)  ^{m_{1}}}{m_{1}!}%
\dfrac{\left(  \dfrac{1}{2}a_{2}\right)  ^{m_{2}}}{m_{2}!}\dfrac{\left(
\dfrac{1}{3}a_{3}\right)  ^{m_{3}}}{m_{3}!}...P\in U\left(  \mathcal{A}%
_{0}\right)  \cdot P.
\]
Since $\alpha\neq0$, we can divide this relation by $\alpha$, and obtain
$1\in\dfrac{1}{\alpha}\cdot U\left(  \mathcal{A}_{0}\right)  \cdot P\subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot P$, qed.}. Combined with \textbf{1)},
this yields that for every nonzero $P\in F$, the representation $F$ is
generated by $P$ as a $U\left(  \mathcal{A}_{0}\right)  $-module (since
$F=U\left(  \mathcal{A}_{0}\right)  \cdot\underbrace{1}_{\in U\left(
\mathcal{A}_{0}\right)  \cdot P}\subseteq U\left(  \mathcal{A}_{0}\right)
\cdot U\left(  \mathcal{A}_{0}\right)  \cdot P=U\left(  \mathcal{A}%
_{0}\right)  \cdot P$). Consequently, $F$ is irreducible. Proposition
\ref{prop.F.irrep} is proven.

\begin{proposition}
\label{prop.V=F}Let $V$ be an irreducible $\mathcal{A}_{0}$-module on which
$K$ acts as $1$. Assume that for any $v\in V$, the space $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v$ is finite-dimensional, and the $a_{i}$
with $i>0$ act on it by nilpotent operators. Then, $V\cong F$ as
$\mathcal{A}_{0}$-modules.
\end{proposition}

Before we prove this, a simple lemma:

\begin{lemma}
\label{lem.V=F}Let $V$ be an $\mathcal{A}_{0}$-module. Let $u\in V$ be such
that $a_{i}u=0$ for all $i>0$, and such that $Ku=u$. Then, there exists a
homomorphism $\eta:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =u$. (This homomorphism $\eta$ is unique, although we
won't need this.)
\end{lemma}

We give two proofs of this lemma. The first one is conceptual and gives us a
glimpse into the more general theory (it proceeds by constructing an
$\mathcal{A}_{0}$-module $\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$, which is an example
of what we will later call a Verma highest-weight module in Definition
\ref{def.verma}). The second one is down-to-earth and proceeds by direct
construction and computation.

\textit{First proof of Lemma \ref{lem.V=F}.} Define a vector subspace
$\mathcal{A}_{0}^{+}$ of $\mathcal{A}_{0}$ by $\mathcal{A}_{0}^{+}%
=\left\langle a_{i}\ \mid\ i\text{ positive integer}\right\rangle $. It is
clear that the internal direct sum $\mathbb{C}K\oplus\mathcal{A}_{0}^{+}$ is
well-defined and an abelian Lie subalgebra of $\mathcal{A}_{0}$. We can make
$\mathbb{C}$ into an $\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
$-module by setting%
\begin{align*}
K\lambda &  =\lambda\ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in
\mathbb{C};\\
a_{i}\lambda &  =0\ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in
\mathbb{C}\text{ and every positive integer }i.
\end{align*}
Now, consider the $\mathcal{A}_{0}$-module $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}=U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }\mathbb{C}$. Denote the element
$1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\in
U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }\mathbb{C}$ of this module by $1$.

We will now show the following important property of this module:
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{For any }\mathcal{A}_{0}\text{-module }T\text{, and any }t\in T\text{
satisfying }\left(  a_{i}t=0\text{ for all }i>0\right)  \text{ and
}Kt=t\text{,}\\
\text{there exists a homomorphism }\overline{\eta}_{T,t}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow T\text{ of }\mathcal{A}_{0}\text{-modules such that
}\overline{\eta}_{T,t}\left(  1\right)  =t
\end{array}
\right)  . \label{lem.V=F.pf.A1}%
\end{equation}
Once this is proven, we will (by considering $\overline{\eta}_{F,1}$) show
that $\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}}\mathbb{C}\cong F$, so this property will translate into the
assertion of Lemma \ref{lem.V=F}.

\textit{Proof of (\ref{lem.V=F.pf.A1}).} Let $\tau:\mathbb{C}\rightarrow T$ be
the map which sends every $\lambda\in\mathbb{C}$ to $\lambda t\in T$. Then,
$\tau$ is $\mathbb{C}$-linear and satisfies%
\[
\tau\underbrace{\left(  K\lambda\right)  }_{=\lambda}=\tau\left(
\lambda\right)  =\lambda\underbrace{t}_{=Kt}=\lambda\cdot Kt=K\cdot
\underbrace{\lambda t}_{=\tau\left(  \lambda\right)  }=K\cdot\tau\left(
\lambda\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in\mathbb{C}%
\]
and%
\begin{align*}
\tau\underbrace{\left(  a_{i}\lambda\right)  }_{=0}  &  =\tau\left(  0\right)
=0=\lambda\cdot\underbrace{0}_{=a_{i}t}=\lambda\cdot a_{i}t=a_{i}%
\cdot\underbrace{\lambda t}_{=\tau\left(  \lambda\right)  }=a_{i}\tau\left(
\lambda\right) \\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in\mathbb{C}\text{ and every
positive integer }i\text{.}%
\end{align*}
Thus, $\tau$ is a $\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
$-module map. In other words, $\tau\in\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  $.

By Frobenius reciprocity, we have%
\[
\operatorname*{Hom}\nolimits_{\mathcal{A}_{0}}\left(  \operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C},T\right)  \cong\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  .
\]
The preimage of $\tau\in\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  $
under this isomorphism is an $\mathcal{A}_{0}$-module map $\overline{\eta
}_{T,t}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow T$ such that
\begin{align*}
\overline{\eta}_{T,t}\underbrace{\left(  1\right)  }_{=1\otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1}  &  =\overline{\eta}%
_{T,t}\left(  1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
}1\right)  =1\underbrace{\tau\left(  1\right)  }_{=1t=t}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the proof of Frobenius reciprocity}%
\right) \\
&  =1t=t.
\end{align*}
Hence, there exists a homomorphism $\overline{\eta}_{T,t}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow T$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{T,t}\left(  1\right)  =t$. This proves (\ref{lem.V=F.pf.A1}).

It is easy to see that the element $1\in F$ satisfies $\left(  a_{i}1=0\text{
for all }i>0\right)  $ and $K1=1$. Thus, (\ref{lem.V=F.pf.A1}) (applied to
$T=F$ and $t=1$) yields that there exists a homomorphism $\overline{\eta
}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}$-modules such
that $\overline{\eta}_{F,1}\left(  1\right)  =1$. This homomorphism
$\overline{\eta}_{F,1}$ is surjective, since
\begin{align*}
F  &  =U\left(  \mathcal{A}_{0}\right)  \cdot\underbrace{1}_{=\overline{\eta
}_{F,1}\left(  1\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{as proven in the
proof of Proposition \ref{prop.F.irrep}}\right) \\
&  =U\left(  \mathcal{A}_{0}\right)  \cdot\overline{\eta}_{F,1}\left(
1\right)  =\overline{\eta}_{F,1}\left(  U\left(  \mathcal{A}_{0}\right)
\cdot1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\overline{\eta}%
_{F,1}\text{ is an }\mathcal{A}_{0}\text{-module map}\right) \\
&  \subseteq\operatorname{Im}\overline{\eta}_{F,1}.
\end{align*}


Now we will prove that this homomorphism $\overline{\eta}_{F,1}$ is injective.

In the following, a map $\varphi:A\rightarrow\mathbb{N}$ (where $A$ is any
set) is said to be \textit{finitely supported} if all but finitely many $a\in
A$ satisfy $\varphi\left(  a\right)  =0$. Sequences (finite, infinite, or
two-sided infinite) are considered as maps (from finite sets, $\mathbb{N}$ or
$\mathbb{Z}$, or occasionally other sets). Thus, a sequence is finitely
supported if and only if all but finitely many of its elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

By the easy part of the Poincar\'{e}-Birkhoff-Witt theorem (this is the part
which states that the increasing monomials \textit{span} the universal
enveloping algebra), the family\footnote{Here, $\overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}$
denotes the product $...a_{-2}^{n_{-2}}a_{-1}^{n_{-1}}a_{1}^{n_{1}}%
a_{2}^{n_{2}}...$. (This product is infinite, but still has a value since only
finitely many $n_{i}$ are nonzero.)}%
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  _{\left(  ...,n_{-2}%
,n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  },\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}_{0}\right)  $.

Hence, the family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  \otimes
_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  },\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}_{0}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }%
\mathbb{C}=\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}$.

Let us first notice that this family is redundant: Each of its elements is
contained in the smaller family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }}.
\]
\footnote{This is because any sequence $\left(  ...,n_{-2},n_{-1},n_{1}%
,n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}%
\diagdown\left\{  0\right\}  }$ and any $m\in\mathbb{N}$ satisfy%
\begin{align*}
&  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown
\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }\underbrace{\left(  K^{m}1\right)
}_{\substack{=1\\\text{(by repeated application of }K1=1\text{)}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K^{m}\in U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  \right) \\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown
\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1.
\end{align*}
} Hence, this smaller family is also a spanning set of the vector space
$\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}$.

This smaller family is still redundant: Every of its elements corresponding to
a sequence $\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$
satisfying $n_{1}+n_{2}+n_{3}+...>0$ is zero\footnote{\textit{Proof.} Let
$\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$ be a
sequence satisfying $n_{1}+n_{2}+n_{3}+...>0$. Then, the sequence $\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  $ is finitely supported (as it is an
element of $\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{
0\right\}  }$), so that only finitely many $n_{i}$ are nonzero.
\par
There exists some positive integer $\ell$ satisfying $n_{\ell}>0$ (since
$n_{1}+n_{2}+n_{3}+...>0$). Let $j$ be the greatest such $\ell$ (this is
well-defined, since only finitely many $n_{i}$ are nonzero).
\par
Since $j$ is the greatest positive integer $\ell$ satisfying $n_{\ell}>0$, it
is clear that $j$ is the greatest integer $\ell$ satisfying $n_{\ell}>0$. In
other words, $a_{j}^{n_{j}}$ is the rightmost factor in the product
$\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}$ which is
not equal to $1$. Thus,%
\[
\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}
}}a_{i}^{n_{i}}=\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}%
\cdot\underbrace{a_{j}^{n_{j}}}_{\substack{=a_{j}^{n_{j}-1}a_{j}\\\text{(since
}n_{j}>0\text{)}}}=\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}\cdot
a_{j}^{n_{j}-1}a_{j},
\]
so that%
\begin{align*}
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1  &  =\left(  \overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}  \diagdown\left\{
j\right\}  }}a_{i}^{n_{i}}\cdot a_{j}^{n_{j}-1}a_{j}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\\
&  =\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}\cdot a_{j}^{n_{j}%
-1}\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
}\underbrace{a_{j}1}_{\substack{=0\\\text{(since }j>0\text{, so that}%
\\a_{j}1=j\dfrac{\partial}{\partial x_{j}}1=0\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\in U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  \right) \\
&  =0.
\end{align*}
We have thus proven that every sequence $\left(  ...,n_{-2},n_{-1},n_{1}%
,n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}%
\diagdown\left\{  0\right\}  }$ satisfying $n_{1}+n_{2}+n_{3}+...>0$ satisfies
$\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1=0$, qed.}, and zero elements in a spanning set
are automatically redundant. Hence, we can replace this smaller family by the
even smaller family%
\begin{align*}
&  \left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }\text{; we do \textit{not} have
}n_{1}+n_{2}+n_{3}+...>0}\\
&  =\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }\text{; }n_{1}=n_{2}=n_{3}=...=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the condition }\left(  \text{we do \textit{not} have }n_{1}%
+n_{2}+n_{3}+...>0\right) \\
\text{is equivalent to the condition }\left(  n_{1}=n_{2}=n_{3}=...=0\right)
\\
\text{(because }n_{i}\in\mathbb{N}\text{ for all }i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \text{)}%
\end{array}
\right)  ,
\end{align*}
and we still have a spanning set of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$.

Clearly, sequences $\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$
satisfying $n_{1}=n_{2}=n_{3}=...=0$ are in 1-to-1 correspondence with
sequences $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$. Hence, we can
reindex the above family as follows:
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}.
\]
So we have proven that the family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}%
\]
is a spanning set of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$.
But the map $\overline{\eta}_{F,1}$ sends this family to%
\begin{align*}
&  \left(  \overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right)  _{\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }}\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}%
\end{align*}
\footnote{\textit{Proof.} Let $\left(  ...,n_{-2},n_{-1}\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$ be
arbitrary. Then,%
\begin{align*}
&  \overline{\eta}_{F,1}\left(  \underbrace{\left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1}_{=\left(
\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}%
a_{i}^{n_{i}}\right)  \left(  1\otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1\right)  }\right) \\
&  =\overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \left(
1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right) \\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \overline{\eta}_{F,1}%
\underbrace{\left(  1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}%
^{+}\right)  }1\right)  }_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\overline{\eta}_{F,1}\text{ is an }\mathcal{A}_{0}\text{-module map}\right)
\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \underbrace{\overline{\eta
}_{F,1}\left(  1\right)  }_{=1}=\left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  1=\left(
\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
}x_{-i}^{n_{i}}\right)  1\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because each }a_{i}\text{ with negative
}i\text{ acts on }F\text{ by multiplication with }x_{-i}\right) \\
&  =\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
}x_{-i}^{n_{i}}=\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
x_{-i}^{n_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }F\text{ is
commutative}\right)  .
\end{align*}
Now forget that we fixed $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$. We thus have shown
that every $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$ satisfies
$\overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
=\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }x_{-i}^{n_{i}}$. Thus,%
\begin{align*}
&  \left(  \overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right)  _{\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }}\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }},
\end{align*}
qed.}. Since the family $\left(  \overset{\rightarrow}{\prod\limits_{i\in
\left\{  ...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(
...,n_{-2},n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}$ is a basis of the vector space $F$ (in fact, this
family consists of all monomials of the polynomial ring $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  =F$), we thus conclude that $\overline{\eta
}_{F,1}$ sends a spanning family of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$
to a basis of the vector space $F$. Thus, $\overline{\eta}_{F,1}$ must be
injective\footnote{Here we are using the following trivial fact from linear
algebra: If a linear map $\varphi:V\rightarrow W$ sends a spanning family of
the vector space $V$ to a basis of the vector space $W$ (as families, not just
as sets), then this map $\varphi$ must be injective.}.

Altogether, we now know that $\overline{\eta}_{F,1}$ is a surjective and
injective $\mathcal{A}_{0}$-module map. Thus, $\overline{\eta}_{F,1}$ is an
isomorphism of $\mathcal{A}_{0}$-modules.

Now, apply (\ref{lem.V=F.pf.A1}) to $T=V$ and $t=u$. This yields that there
exists a homomorphism $\overline{\eta}_{V,u}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{V,u}\left(  1\right)  =u$.

Now, the composition $\overline{\eta}_{V,u}\circ\overline{\eta}_{F,1}^{-1}$ is
a homomorphism $F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that
\[
\left(  \overline{\eta}_{V,u}\circ\overline{\eta}_{F,1}^{-1}\right)  \left(
1\right)  =\overline{\eta}_{V,u}\underbrace{\left(  \overline{\eta}_{F,1}%
^{-1}\left(  1\right)  \right)  }_{\substack{=1\\\text{(since }\overline{\eta
}_{F,1}\left(  1\right)  =1\text{)}}}=\overline{\eta}_{V,u}\left(  1\right)
=u.
\]
Thus, there exists a homomorphism $\eta:F\rightarrow V$ of $\mathcal{A}_{0}%
$-modules such that $\eta\left(  1\right)  =u$ (namely, $\eta=\overline{\eta
}_{V,u}\circ\overline{\eta}_{F,1}^{-1}$). This proves Lemma \ref{lem.V=F}.

\textit{Second proof of Lemma \ref{lem.V=F}.} Let $\eta$ be the map
$F\rightarrow V$ which sends every polynomial $P\in F=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ to $P\left(  a_{-1},a_{-2},a_{-3},...\right)
\cdot u\in V$.\ \ \ \ \footnote{Note that the term $P\left(  a_{-1}%
,a_{-2},a_{-3},...\right)  $ denotes the evaluation of the polynomial $P$ at
$\left(  x_{1},x_{2},x_{3},...\right)  =\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  $. This evaluation is a well-defined element of $U\left(
\mathcal{A}_{0}\right)  $, since the elements $a_{-1}$, $a_{-2}$, $a_{-3}$,
$...$ of $U\left(  \mathcal{A}_{0}\right)  $ commute.} This map $\eta$ is
clearly $\mathbb{C}$-linear, and satisfies $\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u$. In order to prove that $\eta$ is an
$\mathcal{A}_{0}$-module homomorphism, we must prove that
\begin{equation}
\eta\left(  a_{i}P\right)  =a_{i}\eta\left(  P\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{Z}\diagdown\left\{
0\right\}  \text{ and }P\in F \label{lem.V=F.pf.1}%
\end{equation}
and that%
\begin{equation}
\eta\left(  KP\right)  =K\eta\left(  P\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }P\in F. \label{lem.V=F.pf.2}%
\end{equation}


First we show that%
\begin{equation}
Kv=v\ \ \ \ \ \ \ \ \ \ \text{for every }v\in U\left(  \mathcal{A}_{0}\right)
\cdot u. \label{lem.V=F.pf.3}%
\end{equation}


\textit{Proof of (\ref{lem.V=F.pf.3}).} Since $K$ lies in the center of the
Lie algebra $\mathcal{A}_{0}$, it is clear that $K$ lies in the center of the
universal enveloping algebra $U\left(  \mathcal{A}_{0}\right)  $. Thus,
$Kx=xK$ for every $x\in U\left(  \mathcal{A}_{0}\right)  $.

Now let $v\in U\left(  \mathcal{A}_{0}\right)  \cdot u$. Then, there exists
some $x\in U\left(  \mathcal{A}_{0}\right)  $ such that $v=xu$. Thus,
$Kv=Kxu=x\underbrace{Ku}_{=u}=xu=v$. This proves (\ref{lem.V=F.pf.3}).

\textit{Proof of (\ref{lem.V=F.pf.2}).} Since $K$ acts as the identity on $F$,
we have $KP=P$ for every $P\in F$. Thus, for every $P\in F$, we have%
\[
\eta\left(  KP\right)  =\eta\left(  P\right)  =K\eta\left(  P\right)
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{lem.V=F.pf.3}) (applied to }v=\eta\left(  P\right)  \text{)
yields }K\eta\left(  P\right)  =\eta\left(  P\right) \\
\text{(because }\eta\left(  P\right)  \in\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u\text{)}%
\end{array}
\right)  .
\]
This proves (\ref{lem.V=F.pf.2}).

\textit{Proof of (\ref{lem.V=F.pf.1}).} Let $i\in\mathbb{Z}\diagdown\left\{
0\right\}  $. If $i<0$, then (\ref{lem.V=F.pf.1}) is pretty much obvious
(because in this case, $a_{i}$ acts as $x_{-i}$ on $F$, so that $a_{i}%
P=x_{-i}P$ and thus%
\[
\eta\left(  a_{i}P\right)  =\eta\left(  x_{-i}P\right)  =\left(
x_{-i}P\right)  \left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u=a_{i}%
\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}_{=\eta\left(
P\right)  }=a_{i}\eta\left(  P\right)
\]
for every $P\in F$). Hence, from now on, we can WLOG assume that $i$ is not
$<0$. Assume this. Then, $i\geq0$, so that $i>0$ (since $i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  $).

In order to prove the equality (\ref{lem.V=F.pf.1}) for all $P\in F$, it is
enough to prove it for the case when $P$ is a monomial of the form
$x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}$ for some $m\in\mathbb{N}$ and some
$\left(  \ell_{1},\ell_{2},...,\ell_{m}\right)  \in\left\{  1,2,3,...\right\}
^{m}$.\ \ \ \ \footnote{This is because such monomials generate $F$ as a
$\mathbb{C}$-vector space, and because the equality (\ref{lem.V=F.pf.1}) is
linear in $P$.} In other words, in order to prove the equality
(\ref{lem.V=F.pf.1}), it is enough to prove that%
\begin{equation}
\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}\right)
\right)  =a_{i}\eta\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}\text{ and every }\left(
\ell_{1},\ell_{2},...,\ell_{m}\right)  \in\left\{  1,2,3,...\right\}  ^{m}.
\label{lem.V=F.pf.4}%
\end{equation}


Thus, let us now prove (\ref{lem.V=F.pf.4}). In fact, we are going to prove
(\ref{lem.V=F.pf.4}) by induction over $m$. The induction base is very easy
(using $a_{i}1=i\dfrac{\partial}{\partial x_{i}}1=0$ and $a_{i}u=0$) and thus
left to the reader. For the induction step, fix some positive $M\in\mathbb{N}%
$, and assume that (\ref{lem.V=F.pf.4}) is already proven for $m=M-1$. Our
task is now to prove (\ref{lem.V=F.pf.4}) for $m=M$.

So let $\left(  \ell_{1},\ell_{2},...,\ell_{M}\right)  \in\left\{
1,2,3,...\right\}  ^{M}$ be arbitrary. Denote by $Q$ the polynomial
$x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}}$. Then, $x_{\ell_{1}}Q=x_{\ell_{1}%
}x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}}=x_{\ell_{1}}x_{\ell_{2}}%
...x_{\ell_{M}}$.

Since (\ref{lem.V=F.pf.4}) is already proven for $m=M-1$, we can apply
(\ref{lem.V=F.pf.4}) to $M-1$ and $\left(  \ell_{2},\ell_{3},...,\ell
_{M}\right)  $ instead of $m$ and $\left(  \ell_{1},\ell_{2},...,\ell
_{m}\right)  $. We obtain $\eta\left(  a_{i}\left(  x_{\ell_{2}}x_{\ell_{3}%
}...x_{\ell_{M}}\right)  \right)  =a_{i}\eta$ $\left(  x_{\ell_{2}}x_{\ell
_{3}}...x_{\ell_{M}}\right)  $. Since $x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}%
}=Q$, this rewrites as $\eta\left(  a_{i}Q\right)  =a_{i}\eta\left(  Q\right)
$.

Since any $x\in\mathcal{A}_{0}$ and $y\in\mathcal{A}_{0}$ satisfy
$xy=yx+\left[  x,y\right]  $ (by the definition of $U\left(  \mathcal{A}%
_{0}\right)  $), we have%
\[
a_{i}a_{-\ell_{1}}=a_{-\ell_{1}}a_{i}+\underbrace{\left[  a_{i},a_{-\ell_{1}%
}\right]  }_{=i\delta_{i,-\left(  -\ell_{1}\right)  }K}=a_{-\ell_{1}}%
a_{i}+i\underbrace{\delta_{i,-\left(  -\ell_{1}\right)  }}_{=\delta
_{i,\ell_{1}}}K=a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K.
\]


On the other hand, by the definition of $\eta$, every $P\in F$ satisfies the
two equalities $\eta\left(  P\right)  =P\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  \cdot u$ and%
\begin{align}
\eta\left(  x_{\ell_{1}}P\right)   &  =\underbrace{\left(  x_{\ell_{1}%
}P\right)  \left(  a_{-1},a_{-2},a_{-3},...\right)  }_{=a_{-\ell_{1}}\cdot
P\left(  a_{-1},a_{-2},a_{-3},...\right)  }\cdot u=a_{-\ell_{1}}%
\cdot\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}%
_{=\eta\left(  P\right)  }\nonumber\\
&  =a_{-\ell_{1}}\cdot\eta\left(  P\right)  . \label{lem.V=F.pf.5}%
\end{align}


Since $a_{i}$ acts on $F$ as $i\dfrac{\partial}{\partial x_{i}}$, we have
$a_{i}\left(  x_{\ell_{1}}Q\right)  =i\dfrac{\partial}{\partial x_{i}}\left(
x_{\ell_{1}}Q\right)  $ and $a_{i}Q=i\dfrac{\partial}{\partial x_{i}}Q$. Now,%
\begin{align*}
a_{i}\left(  \underbrace{x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}}%
_{=x_{\ell_{1}}Q}\right)   &  =a_{i}\left(  x_{\ell_{1}}Q\right)
=i\dfrac{\partial}{\partial x_{i}}\left(  x_{\ell_{1}}Q\right)  =i\left(
\left(  \dfrac{\partial}{\partial x_{i}}x_{\ell_{1}}\right)  Q+x_{\ell_{1}%
}\left(  \dfrac{\partial}{\partial x_{i}}Q\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the Leibniz rule}\right) \\
&  =i\underbrace{\left(  \dfrac{\partial}{\partial x_{i}}x_{\ell_{1}}\right)
}_{=\delta_{i,\ell_{1}}}Q+x_{\ell_{1}}\cdot\underbrace{i\dfrac{\partial
}{\partial x_{i}}Q}_{=a_{i}Q}=i\delta_{i,\ell_{1}}Q+x_{\ell_{1}}\cdot
a_{i}Q=x_{\ell_{1}}\cdot a_{i}Q+i\delta_{i,\ell_{1}}Q,
\end{align*}
so that%
\begin{align*}
\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}\right)
\right)   &  =\eta\left(  x_{\ell_{1}}\cdot a_{i}Q+i\delta_{i,\ell_{1}%
}Q\right)  =\underbrace{\eta\left(  x_{\ell_{1}}\cdot a_{i}Q\right)
}_{\substack{=a_{-\ell_{1}}\cdot\eta\left(  a_{i}Q\right)  \\\text{(by
(\ref{lem.V=F.pf.5}), applied to }P=a_{i}Q\text{)}}}+i\delta_{i,\ell_{1}}%
\eta\left(  Q\right) \\
&  =a_{-\ell_{1}}\cdot\underbrace{\eta\left(  a_{i}Q\right)  }_{=a_{i}%
\eta\left(  Q\right)  }+i\delta_{i,\ell_{1}}\eta\left(  Q\right)
=a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}}%
\eta\left(  Q\right)  .
\end{align*}
Compared to%
\begin{align*}
a_{i}\eta\left(  \underbrace{x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}%
}_{=x_{\ell_{1}}Q}\right)   &  =a_{i}\underbrace{\eta\left(  x_{\ell_{1}%
}Q\right)  }_{\substack{=a_{-\ell_{1}}\cdot\eta\left(  Q\right)  \\\text{(by
(\ref{lem.V=F.pf.5}), applied to }P=Q\text{)}}}=\underbrace{a_{i}a_{-\ell_{1}%
}}_{=a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K}\cdot\eta\left(  Q\right) \\
&  =\left(  a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K\right)  \cdot\eta\left(
Q\right)  =a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}%
}\underbrace{K\eta\left(  Q\right)  }_{\substack{=\eta\left(  Q\right)
\\\text{(by (\ref{lem.V=F.pf.3}), applied to }v=\eta\left(  Q\right)
\\\text{(since }\eta\left(  Q\right)  \in\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u\text{))}}}\\
&  =a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}}%
\eta\left(  Q\right)  ,
\end{align*}
this yields $\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}%
}\right)  \right)  =a_{i}\eta\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}%
}\right)  $. Since we have proven this for every $\left(  \ell_{1},\ell
_{2},...,\ell_{M}\right)  \in\left\{  1,2,3,...\right\}  ^{M}$, we have thus
proven (\ref{lem.V=F.pf.4}) for $m=M$. This completes the induction step, and
thus the induction proof of (\ref{lem.V=F.pf.4}) is complete. As we have seen
above, this proves (\ref{lem.V=F.pf.1}).

From (\ref{lem.V=F.pf.1}) and (\ref{lem.V=F.pf.2}), it is clear that $\eta$ is
$\mathcal{A}_{0}$-linear (since $\mathcal{A}_{0}$ is spanned by the $a_{i}$
for $i\in\mathbb{Z}\diagdown\left\{  0\right\}  $ and $K$). Since $\eta\left(
1\right)  =u$ is obvious, this proves Lemma \ref{lem.V=F}.

\textit{Proof of Proposition \ref{prop.V=F}.} Pick some nonzero vector $v\in
V$. Let $W=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \cdot v$. Then, by
the condition, we have $\dim W<\infty$, and $a_{i}:W\rightarrow W$ are
commuting nilpotent operators\footnote{Of course, when we write $a_{i}%
:W\rightarrow W$, we don't mean the elements $a_{i}$ of $\mathcal{A}_{0}$
themselves, but their actions on $W$.}. Hence, $\bigcap\limits_{i\geq
1}\operatorname*{Ker}a_{i}\neq0\ \ \ \ $\footnote{Here, we are using the
following linear-algebraic fact:
\par
If $T$ is a nonzero finite-dimensional vector space over an algebraically
closed field, and if $b_{1}$, $b_{2}$, $b_{3}$, $...$ are commuting linear
maps $T\rightarrow T$, then there exists a nonzero common eigenvector of
$b_{1}$, $b_{2}$, $b_{3}$, $...$. If $b_{1}$, $b_{2}$, $b_{3}$, $...$ are
nilpotent, this yields $\bigcap\limits_{i\geq1}\operatorname*{Ker}b_{i}\neq0$
(since any eigenvector of a nilpotent map must lie in its kernel).}. Hence,
there exists some nonzero $u\in\bigcap\limits_{i\geq1}\operatorname*{Ker}%
a_{i}$. Pick such a $u$. Then, $a_{i}u=0$ for all $i>0$, and $Ku=u$ (since $K$
acts as $1$ on $V$). Thus, there exists a homomorphism $\eta:F\rightarrow V$
of $\mathcal{A}_{0}$-modules such that $\eta\left(  1\right)  =u$ (by Lemma
\ref{lem.V=F}). Since both $F$ and $V$ are irreducible and $\eta\neq0$, this
yields that $\eta$ is an isomorphism. This proves Proposition \ref{prop.V=F}.

\subsubsection{Classification of
\texorpdfstring{$\mathcal{A}_{0}$}{A0}-modules with locally nilpotent action
of
\texorpdfstring{$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $}{C[a1,a2,a3,...]}}%


\begin{proposition}
\label{prop.V=F(X)U}Let $V$ be any $\mathcal{A}_{0}$-module having a locally
nilpotent action of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $. (Here,
we say that the $\mathcal{A}_{0}$-module $V$ has a \textit{locally nilpotent
action of$\mathbb{\ }$}$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ if
for any $v\in V$, the space $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]
\cdot v$ is finite-dimensional, and the $a_{i}$ with $i>0$ act on it by
nilpotent operators.) Assume that $K$ acts as $1$ on $V$. Assume that for
every $v\in V$, there exists some $N\in\mathbb{N}$ such that for every $n\geq
N$, we have $a_{n}v=0$. Then, $V\cong F\otimes U$ as $\mathcal{A}_{0}$-modules
for some vector space $U$. (The vector space $U$ is not supposed to carry any
$\mathcal{A}_{0}$-module structure.)
\end{proposition}

\begin{remark}
From Proposition \ref{prop.V=F(X)U}, we cannot remove the condition that for
every $v\in V$, there exists some $N\in\mathbb{N}$ such that for every $n\geq
N$, we have $a_{n}v=0$. In fact, here is a counterexample of how Proposition
\ref{prop.V=F(X)U} can fail without this condition:

Let $V$ be the representation $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]
\left[  y\right]  \diagup\left(  y^{2}\right)  $ of $\mathcal{A}_{0}$ given
by
\begin{align*}
a_{-i}  &  \mapsto x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto i\dfrac{\partial}{\partial x_{i}}%
+y\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where we are being sloppy and abbreviating the residue class $\overline{y}%
\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \left[  y\right]
\diagup\left(  y^{2}\right)  $ by $y$, and similarly all other residue
classes). We have an exact sequence%
\[%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%0 \ar[r] & F \ar[r]^i & V \ar[r]^{\pi} & F \ar[r] & 0
%}}}%
%BeginExpansion
\xymatrix{
0 \ar[r] & F \ar[r]^i & V \ar[r]^{\pi} & F \ar[r] & 0
}%
%EndExpansion
\]
of $\mathcal{A}_{0}$-modules, where the map $i:F\rightarrow V$ is given by%
\[
i\left(  P\right)  =yP\ \ \ \ \ \ \ \ \ \ \text{for every }p\in F=\mathbb{C}%
\left[  x_{1},x_{2},x_{3},...\right]  ,
\]
and the map $\pi:V\rightarrow F$ is the canonical projection $V\rightarrow
V\diagup\left(  y\right)  \cong F$. Thus, $V$ is an extension of $F$ by $F$.
It is easily seen that $V$ has a locally nilpotent action of$\ \mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  $. But $V$ is not isomorphic to
$F\otimes U$ as $\mathcal{A}_{0}$-modules for any vector space $U$, since
there is a vector $v\in V$ satisfying $V=U\left(  \mathcal{A}_{0}\right)
\cdot v$ (for example, $v=1$), whereas there is no vector $v\in F\otimes U$
satisfying $F\otimes U=U\left(  \mathcal{A}_{0}\right)  \cdot v$ if $\dim
U>1$, and the case $\dim U\leq1$ is easily ruled out (in this case, $\dim U$
would have to be $1$, so that $V$ would be $\cong F$ and thus irreducible, and
thus the homomorphisms $i$ and $\pi$ would have to be isomorphisms, which is absurd).
\end{remark}

Before we prove Proposition \ref{prop.V=F(X)U}, we need to define the notion
of complete coflags:

\begin{definition}
Let $k$ be a field. Let $V$ be a $k$-vector space. Let $W$ be a vector
subspace of $V$. Assume that $\dim\left(  V\diagup W\right)  <\infty$. Then, a
\textbf{complete coflag from }$V$ \textbf{to }$W$ will mean a sequence
$\left(  V_{0},V_{1},...,V_{N}\right)  $ of vector subspaces of $V$ (with $N$
being an integer) satisfying the following conditions:

- We have $V_{0}\supseteq V_{1}\supseteq...\supseteq V_{N}$.

- Every $i\in\left\{  0,1,...,N\right\}  $ satisfies $\dim\left(  V\diagup
V_{i}\right)  =i$.

- We have $V_{0}=V$ and $V_{N}=W$.

(Note that the condition $V_{0}=V$ is superfluous (since it follows from the
condition that every $i\in\left\{  0,1,...,N\right\}  $ satisfies $\dim\left(
V\diagup V_{i}\right)  =i$), but has been given for the sake of intuition.)

We will also denote the complete coflag $\left(  V_{0},V_{1},...,V_{N}\right)
$ by $V=V_{0}\supseteq V_{1}\supseteq...\supseteq V_{N}=W$.
\end{definition}

It is clear that if $k$ is a field, $V$ is a $k$-vector space, and $W$ is a
vector subspace of $V$ satisfying $\dim\left(  V\diagup W\right)  <\infty$,
then a complete coflag from $V$ to $W$ exists.\footnote{In fact, it is known
that the finite-dimensional vector space $V\diagup W$ has a complete flag
$\left(  F_{0},F_{1},...,F_{N}\right)  $; now, if we let $p$ be the canonical
projection $V\rightarrow V\diagup W$, then $\left(  p^{-1}\left(
F_{N}\right)  ,p^{-1}\left(  F_{N-1}\right)  ,...,p^{-1}\left(  F_{0}\right)
\right)  $ is easily seen to be a complete coflag from $V$ to $W$.}

\begin{definition}
Let $k$ be a field. Let $V$ be a $k$-algebra. Let $W$ be a vector subspace of
$V$. Let $\mathfrak{i}$ be an ideal of $V$. Then, an $\mathfrak{i}%
$\textbf{-coflag from }$V$\textbf{ to }$W$ means a complete coflag $\left(
V_{0},V_{1},...,V_{N}\right)  $ from $V$ to $W$ such that
\[
\text{every }i\in\left\{  0,1,...,N-1\right\}  \text{ satisfies }%
\mathfrak{i}\cdot V_{i}\subseteq V_{i+1}.
\]

\end{definition}

\begin{lemma}
\label{lem.V=F(X)U.coflags}Let $k$ be a field. Let $B$ be a commutative
$k$-algebra. Let $I$ be an ideal of $B$ such that the $k$-vector space
$B\diagup I$ is finite-dimensional. Let $\mathfrak{i}$ be an ideal of $B$. Let
$M\in\mathbb{N}$. Then, there exists an $\mathfrak{i}$-coflag from $B$ to
$\mathfrak{i}^{M}+I$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.V=F(X)U.coflags}.} We will prove Lemma
\ref{lem.V=F(X)U.coflags} by induction over $M$:

\textit{Induction base:} Lemma \ref{lem.V=F(X)U.coflags} is trivial in the
case when $M=0$, because $\underbrace{\mathfrak{i}^{0}}_{=B}+I=B+I=B$. This
completes the induction base.

\textit{Induction base:} Let $m\in\mathbb{N}$. Assume that Lemma
\ref{lem.V=F(X)U.coflags} is proven in the case when $M=m$. We now must prove
Lemma \ref{lem.V=F(X)U.coflags} in the case when $M=m+1$.

Since Lemma \ref{lem.V=F(X)U.coflags} is proven in the case when $M=m$, there
exists an $\mathfrak{i}$-coflag $\left(  J_{0},J_{1},...,J_{K}\right)  $ from
$B$ to $\mathfrak{i}^{m}+I$. This $\mathfrak{i}$-coflag clearly is a complete
coflag from $B$ to $\mathfrak{i}^{m}+I$.

Since
\begin{align*}
\dim\left(  \left(  \mathfrak{i}^{m}+I\right)  \diagup\left(  \mathfrak{i}%
^{m+1}+I\right)  \right)   &  \leq\dim\left(  B\diagup\left(  \mathfrak{i}%
^{m+1}+I\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because }\left(  \mathfrak{i}%
^{m}+I\right)  \diagup\left(  \mathfrak{i}^{m+1}+I\right)  \text{ injects into
}B\diagup\left(  \mathfrak{i}^{m+1}+I\right)  \right) \\
&  \leq\dim\left(  B\diagup I\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}B\diagup\left(  \mathfrak{i}^{m+1}+I\right)  \text{ is a quotient of
}B\diagup I\right) \\
&  <\infty\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\diagup I\text{ is
finite-dimensional}\right)  ,
\end{align*}
there exists a complete coflag $\left(  U_{0},U_{1},...,U_{P}\right)  $ from
$\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$.

Since $\left(  U_{0},U_{1},...,U_{P}\right)  $ is a complete coflag from
$\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$, we have $U_{0}=\mathfrak{i}%
^{m}+I$, and each of the vector spaces $U_{0}$, $U_{1}$, $...$, $U_{P}$
contains $\mathfrak{i}^{m+1}+I$ as a subspace.

Also, every $i\in\left\{  0,1,...,P\right\}  $ satisfies $U_{i}\subseteq
\mathfrak{i}^{m}+I$ (again since $\left(  U_{0},U_{1},...,U_{P}\right)  $ is a
complete coflag from $\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$).

Since $\left(  J_{0},J_{1},...,J_{K}\right)  $ is a complete coflag from $B$
to $\mathfrak{i}^{m}+I$, while $\left(  U_{0},U_{1},...,U_{P}\right)  $ is a
complete coflag from $\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$, it is
clear that
\[
\left(  J_{0},J_{1},...,J_{K},U_{1},U_{2},...,U_{P}\right)  =\left(
J_{0},J_{1},...,J_{K-1},U_{0},U_{1},...,U_{P}\right)
\]
is a complete coflag from $B$ to $\mathfrak{i}^{m+1}+I$. We now will prove
that this complete coflag
\[
\left(  J_{0},J_{1},...,J_{K},U_{1},U_{2},...,U_{P}\right)  =\left(
J_{0},J_{1},...,J_{K-1},U_{0},U_{1},...,U_{P}\right)
\]
actually is an $\mathfrak{i}$-coflag.

In order to prove this, we must show the following two assertions:

\textit{Assertion 1:} Every $i\in\left\{  0,1,...,K-1\right\}  $ satisfies
$\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}$.

\textit{Assertion 2:} Every $i\in\left\{  0,1,...,P-1\right\}  $ satisfies
$\mathfrak{i}\cdot U_{i}\subseteq U_{i+1}$.

Assertion 1 follows directly from the fact that $\left(  J_{0},J_{1}%
,...,J_{K}\right)  $ is an $\mathfrak{i}$-coflag.

Assertion 2 follows from the fact that $\mathfrak{i}\cdot\underbrace{U_{i}%
}_{\subseteq\mathfrak{i}^{m}+I}\subseteq\mathfrak{i}\cdot\left(
\mathfrak{i}^{m}+I\right)  \subseteq\underbrace{\mathfrak{i}\cdot
\mathfrak{i}^{m}}_{=\mathfrak{i}^{m+1}}+\underbrace{\mathfrak{i}\cdot
I}_{\substack{\subseteq I\\\text{(since }I\text{ is an ideal)}}}\subseteq
\mathfrak{i}^{m+1}+I\subseteq U_{i+1}$ (because we know that each of the
vector spaces $U_{0}$, $U_{1}$, $...$, $U_{P}$ contains $\mathfrak{i}^{m+1}+I$
as a subspace, so that (in particular) $\mathfrak{i}^{m+1}+I\subseteq U_{i+1}$).

Hence, both Assertions 1 and 2 are proven, and we conclude that
\[
\left(  J_{0},J_{1},...,J_{K},U_{1},U_{2},...,U_{P}\right)  =\left(
J_{0},J_{1},...,J_{K-1},U_{0},U_{1},...,U_{P}\right)
\]
is an $\mathfrak{i}$-coflag. This is clearly an $\mathfrak{i}$-coflag from $B$
to $\mathfrak{i}^{m+1}+I$. Thus, there exists an $\mathfrak{i}$-coflag from
$B$ to $\mathfrak{i}^{m+1}+I$. This proves Lemma \ref{lem.V=F(X)U.coflags} in
the case when $M=m+1$. The induction step is complete, and with it the proof
of Lemma \ref{lem.V=F(X)U.coflags}.

\textit{Proof of Proposition \ref{prop.V=F(X)U}.} Let $v\in V$ be arbitrary.
Let $I_{v}\subseteq\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ be the
annihilator of $v$. Then, the canonical $\mathbb{C}$-algebra map
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \rightarrow
\operatorname*{End}\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]
\cdot v\right)  $ (this map comes from the action of the $\mathbb{C}$-algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ on $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v$) gives rise to an \textit{injective}
map $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}%
\rightarrow\operatorname*{End}\left(  \mathbb{C}\left[  a_{1},a_{2}%
,a_{3},...\right]  \cdot v\right)  $. Since this map is injective, we have
$\dim\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
I_{v}\right)  \leq\dim\left(  \operatorname*{End}\left(  \mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v\right)  \right)  <\infty$ (since
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \cdot v$ is
finite-dimensional). In other words, the vector space $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$ is finite-dimensional.

Let $W$ be the $\mathcal{A}_{0}$-submodule of $V$ generated by $v$. In other
words, let $W=U\left(  \mathcal{A}_{0}\right)  \cdot v$. Then, $W$ is a
quotient of $U\left(  \mathcal{A}_{0}\right)  $ (as an $\mathcal{A}_{0}%
$-module). Since $K$ acts as $1$ on $W$, it follows that $W$ is a quotient of
$U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(
x_{1},x_{2},x_{3},...\right)  $. Since $I_{v}$ annihilates $v$, it follows
that $W$ is a quotient of $D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  $. Let us denote the
$\mathcal{A}_{0}$-module $D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  $ by $\widetilde{W}$.

We now will prove that $\widetilde{W}$ is a finite-length $\mathcal{A}_{0}%
$-module with all composition factors isomorphic to $F$.\ \ \ \ \footnote{We
can even prove that there are exactly $\dim\left(  \mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \diagup I_{v}\right)  $ composition factors.}

Let $\mathfrak{i}$ be the ideal $\left(  a_{1},a_{2},a_{3},...\right)  $ of
the commutative algebra $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $.

Since $I_{v}$ is an ideal of the commutative algebra $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $, the quotient $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  \diagup I_{v}$ is an algebra. For every
$q\in\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $, let $\overline{q}$ be
the projection of $q$ onto the quotient algebra $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  \diagup I_{v}$. Let also $\overline{\mathfrak{i}}$ be
the projection of the ideal $\mathfrak{i}$ onto the quotient algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$. Clearly,
$\overline{\mathfrak{i}}=\left(  \overline{a_{1}},\overline{a_{2}}%
,\overline{a_{3}},...\right)  $.

For every $j>0$, there exists some $i\in\mathbb{N}$ such that $a_{j}^{i}v=0$
(since $V$ has a locally nilpotent action of $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  $). Hence, for every $j>0$, the element
$\overline{a_{j}}$ of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
I_{v}$ is nilpotent (because there exists some $i\in\mathbb{N}$ such that
$a_{j}^{i}v=0$, and thus this $i$ satisfies $a_{j}^{i}\in I_{v}$, so that
$\overline{a_{j}}^{i}=0$). Hence, the ideal $\overline{\mathfrak{i}}$ is
generated by nilpotent generators (since $\overline{\mathfrak{i}}=\left(
\overline{a_{1}},\overline{a_{2}},\overline{a_{3}},...\right)  $). Since we
also know that $\overline{\mathfrak{i}}$ is finitely generated (since
$\overline{\mathfrak{i}}$ is an ideal of the finite-dimensional algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$), it follows
that $\overline{\mathfrak{i}}$ is generated by \textit{finitely many}
nilpotent generators. But if an ideal of a commutative ring is generated by
finitely many nilpotent generators, it must be nilpotent. Thus, $\overline
{\mathfrak{i}}$ is nilpotent. In other words, there exists some $M\in
\mathbb{N}$ such that $\overline{\mathfrak{i}}^{M}=0$. Consider this $M$.
Since $\overline{\mathfrak{i}}^{M}=0$, we have $\mathfrak{i}^{M}\subseteq
I_{v}$ and thus $\mathfrak{i}^{M}+I_{v}=I_{v}$.

Now, Lemma \ref{lem.V=F(X)U.coflags} (applied to $k=\mathbb{C}$,
$B=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ and $I=I_{v}$) yields
that there exists an $\mathfrak{i}$-coflag from $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  $ to $\mathfrak{i}^{M}+I_{v}$. Denote this
$\mathfrak{i}$-coflag by $\left(  J_{0},J_{1},...,J_{N}\right)  $. Since
$\mathfrak{i}^{M}+I_{v}=I_{v}$, this $\mathfrak{i}$-coflag $\left(
J_{0},J_{1},...,J_{N}\right)  $ thus is an $\mathfrak{i}$-coflag from
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ to $I_{v}$. Thus, $\left(
J_{0},J_{1},...,J_{N}\right)  $ is a complete coflag from $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $ to $I_{v}$. In other words:

\begin{itemize}
\item We have $J_{0}\supseteq J_{1}\supseteq...\supseteq J_{N}$.

\item Every $i\in\left\{  0,1,...,N\right\}  $ satisfies $\dim\left(
\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup J_{i}\right)  =i$.

\item We have $J_{0}=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ and
$J_{N}=I_{v}$.
\end{itemize}

Besides, since $\left(  J_{0},J_{1},...,J_{N}\right)  $ is an $\mathfrak{i}%
$-coflag, we have%
\begin{equation}
\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  0,1,...,N-1\right\}  . \label{prop.V=F(X)U.pf.2}%
\end{equation}


For every $i\in\left\{  0,1,...,N\right\}  $, let $D_{i}=D\left(  x_{1}%
,x_{2},...\right)  \cdot J_{i}$. Then,%
\[
D_{0}=D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{0}}_{=\mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  }=D\left(  x_{1},x_{2},...\right)
\]
and%
\[
D_{N}=D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{N}}_{=I_{v}%
}=D\left(  x_{1},x_{2},...\right)  \cdot I_{v}.
\]
Hence, $D_{0}\diagup D_{N}=D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  =\widetilde{W}$.

Now, we are going to prove that%
\begin{equation}
D_{i}\diagup D_{i+1}\cong F\text{ or }D_{i}\diagup D_{i+1}%
=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  0,1,...,N-1\right\}
\label{prop.V=F(X)U.pf.3}%
\end{equation}
(where $\cong$ means isomorphism of $\mathcal{A}_{0}$-modules).

\textit{Proof of (\ref{prop.V=F(X)U.pf.3}).} Let $i\in\left\{
0,1,...,N-1\right\}  $. Since $\dim\left(  \mathbb{C}\left[  a_{1},a_{2}%
,a_{3},...\right]  \diagup J_{i}\right)  =i$ and $\dim\left(  \mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  \diagup J_{i+1}\right)  =i+1$, there
exists some $u\in J_{i}$ such that $J_{i}=u+J_{i+1}$. Consider this $u$. By
abuse of notation, we also use the letter $u$ to denote the element $1\cdot
u\in D\left(  x_{1},x_{2},...\right)  \cdot J_{i}=D_{i}$. Then,
\begin{align*}
D_{i}  &  =D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{i}%
}_{=u+J_{i+1}}=D\left(  x_{1},x_{2},...\right)  \cdot\left(  u+J_{i+1}\right)
\\
&  =D\left(  x_{1},x_{2},...\right)  \cdot u+\underbrace{D\left(  x_{1}%
,x_{2},...\right)  \cdot J_{i+1}}_{=D_{i+1}}=D\left(  x_{1},x_{2},...\right)
\cdot u+D_{i+1}.
\end{align*}
Thus,
\[
D_{i}\diagup D_{i+1}=D\left(  x_{1},x_{2},...\right)  \cdot u^{\prime},
\]
where $u^{\prime}$ denotes the residue class of $u\in D_{i}$ modulo $D_{i+1}$.
For every $j>0$, we have $\underbrace{a_{j}}_{\in\mathfrak{i}}\underbrace{u}%
_{\in J_{i}}\in\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}$ (by
(\ref{prop.V=F(X)U.pf.2})) and thus $a_{j}u\in D\left(  x_{1},x_{2}%
,...\right)  \cdot J_{i+1}=D_{i+1}$. In other words, for every $j>0$, we have
$a_{j}u^{\prime}=0$. Also, it is pretty clear that $Ku^{\prime}=u^{\prime}$.
Thus, Lemma \ref{lem.V=F} (applied to $D_{i}\diagup D_{i+1}$ and $u^{\prime}$
instead of $V$ and $u$) yields that there exists a homomorphism $\eta
:F\rightarrow D_{i}\diagup D_{i+1}$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =u^{\prime}$. This homomorphism $\eta$ must be
surjective\footnote{since its image is $\eta\left(  \underbrace{F}_{=D\left(
x_{1},x_{2},...\right)  \cdot1}\right)  =D\left(  x_{1},x_{2},...\right)
\cdot\underbrace{\eta\left(  1\right)  }_{=u^{\prime}}=D\left(  x_{1}%
,x_{2},...\right)  \cdot u^{\prime}=D_{i}\diagup D_{i+1}$}, and thus
$D_{i}\diagup D_{i+1}$ is a factor module of $F$. Since $F$ is irreducible,
this yields that $D_{i}\diagup D_{i+1}\cong F$ or $D_{i}\diagup D_{i+1}=0$.
This proves (\ref{prop.V=F(X)U.pf.3}).

Now, clearly, the $\mathcal{A}_{0}$-module $\widetilde{W}=D_{0}\diagup D_{N}$
is filtered by the $\mathcal{A}_{0}$-modules $D_{i}\diagup D_{N}$ for
$i\in\left\{  0,1,...,N\right\}  $. Due to (\ref{prop.V=F(X)U.pf.3}), the
subquotients of this filtration are all $\cong F$ or $=0$, so that
$\widetilde{W}$ is a finite-length $\mathcal{A}_{0}$-module with all
composition factors isomorphic to $F$ (since $F$ is irreducible).

Since $W$ is a quotient module of $\widetilde{W}$, this yields that $W$ must
also be a finite-length $\mathcal{A}_{0}$-module with all composition factors
isomorphic to $F$.

Now forget that we fixed $v$. We have thus shown that for every $v\in V$, the
$\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of $V$
(this submodule is what we called $W$) is a finite-length module with
composition factors isomorphic to $F$.

By the assumption (that for every $v\in V$, there exists some $N\in\mathbb{N}$
such that for every $n\geq N$, we have $a_{n}v=0$), we can define an action of
$E=\sum\limits_{i>0}a_{-i}a_{i}\in\widehat{\mathcal{A}}$ (the so-called
\textit{Euler field}) on $V$. Note that $E$ acts on $V$ in a locally finite
way (this means that for any $v\in V$, the space $\mathbb{C}\left[  E\right]
\cdot v$ is finite-dimensional)\footnote{\textit{Proof.} Notice that $E$ acts
on $F$ as $\sum\limits_{i>0}ix_{i}\dfrac{\partial}{\partial x_{i}}$, and thus
$E$ acts on $F$ in a locally finite way (since the differential operator
$\sum\limits_{i>0}ix_{i}\dfrac{\partial}{\partial x_{i}}$ preserves the
degrees of polynomials), and thus also on $V$ (because for every $v\in V$, the
$\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of $V$
is a finite-length module with composition factors isomorphic to $F$).}. Now,
let us notice that the eigenvalues of the map $E\mid_{V}:V\rightarrow V$ (this
is the action of $E$ on $V$) are nonnegative
integers.\footnote{\textit{Proof.} Let $\rho$ be an eigenvalue of $E\mid_{V}$.
Then, there exists some nonzero eigenvector $v\in V$ to the eigenvalue $\rho$.
Consider this $v$. Clearly, $\rho$ must thus also be an eigenvalue of
$E\mid_{U\left(  \mathcal{A}_{0}\right)  \cdot v}$ (because $v$ is a nonzero
eigenvector of $E\mid_{V}$ to the eigenvalue $\rho$ and lies in $U\left(
\mathcal{A}_{0}\right)  \cdot v$). But the eigenvalues of $E\mid_{U\left(
\mathcal{A}_{0}\right)  \cdot v}$ are nonnegative integers (since we know that
the $\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of
$V$ is a finite-length module with composition factors isomorphic to $F$, and
we can easily check that the eigenvalues of $E\mid_{F}$ are nonnegative
integers). Hence, $\rho$ is a nonnegative integer. We have thus shown that
every eigenvalue of $E\mid_{V}$ is a nonnegative integer, qed.} Hence, we can
write $V$ as $V=\bigoplus\limits_{j\geq0}V\left[  j\right]  $, where $V\left[
j\right]  $ is the generalized eigenspace of $E\mid_{V}$ with eigenvalue $j$
for every $j\in\mathbb{N}$.

If some $v\in V$ satisfies $a_{i}v=0$ for all $i>0$, then $Ev=0$ and thus
$v\in V\left[  0\right]  $.

Conversely, if $v\in V\left[  0\right]  $, then $a_{i}v=0$ for all
$i>0$.\ \ \ \ \footnote{\textit{Proof.} Let $v\in V\left[  0\right]  $. Let
$j$ be positive.
\par
It is easy to check that $a_{-i}a_{i}a_{j}=a_{j}a_{-i}a_{i}-i\delta_{i,j}%
a_{i}$ for any positive $i$ (here, we use that $j>0$). Since $E=\sum
\limits_{i>0}a_{-i}a_{i}$, we have%
\begin{align*}
Ea_{j}  &  =\sum\limits_{i>0}\underbrace{a_{-i}a_{i}a_{j}}_{=a_{j}a_{-i}%
a_{i}-i\delta_{i,j}a_{i}}=\sum\limits_{i>0}\left(  a_{j}a_{-i}a_{i}%
-i\delta_{i,j}a_{i}\right) \\
&  =a_{j}\underbrace{\sum\limits_{i>0}a_{-i}a_{i}}_{=E}-\underbrace{\sum
\limits_{i>0}i\delta_{i,j}a_{i}}_{=ja_{j}}=a_{j}E-ja_{j},
\end{align*}
so that $\left(  E+j\right)  a_{j}=a_{j}E$. This yields (by induction over
$m$) that $\left(  E+j\right)  ^{m}a_{j}=a_{j}E^{m}$ for every $m\in
\mathbb{N}$.
\par
Now, since $v\in V\left[  0\right]  =\left(  \text{generalized eigenspace of
}E\mid_{V}\text{ with eigenvalue }0\right)  $, there exists an $m\in
\mathbb{N}$ such that $E^{m}v=0$. Consider this $m$. Then, from $\left(
E+j\right)  ^{m}a_{j}=a_{j}E^{m}$, we obtain $\left(  E+j\right)  ^{m}%
a_{j}v=a_{j}E^{m}v=0$, so that
\[
a_{j}v\in\left(  \text{generalized eigenspace of }E\mid_{V}\text{ with
eigenvalue }-j\right)  =0
\]
(because the eigenvalues of the map $E\mid_{V}:V\rightarrow V$ are nonnegative
integers, whereas $-j$ is not). In other words, $a_{j}v=0$.
\par
We have thus proven that $a_{j}v=0$ for every positive $j$. In other words,
$a_{i}v=0$ for all $i>0$, qed.}

So we conclude that $V\left[  0\right]  =\operatorname*{Ker}E=\bigcap
\limits_{i\geq1}\operatorname*{Ker}a_{i}$.

Now, $F\otimes V\left[  0\right]  $ is an $\mathcal{A}_{0}$-module (where
$\mathcal{A}_{0}$ acts only on the $F$ tensorand, where $V\left[  0\right]  $
is considered just as a vector space). We will now construct an isomorphism
$F\otimes V\left[  0\right]  \rightarrow V$ of $\mathcal{A}_{0}$-modules. This
will prove Proposition \ref{prop.V=F(X)U}.

For every $v\in V\left[  0\right]  $, there exists a homomorphism $\eta
_{v}:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\eta_{v}\left(
1\right)  =v$ (according to Lemma \ref{lem.V=F}, applied to $v$ instead of $u$
(since $a_{i}v=0$ for all $i>0$ and $Kv=v$)). Consider these homomorphisms
$\eta_{v}$ for various $v$. Clearly, every $v\in V\left[  0\right]  $ and
$P\in F$ satisfy%
\begin{align*}
\eta_{v}\left(  P\right)   &  =\eta_{v}\left(  P\left(  a_{-1},a_{-2}%
,a_{-3},...\right)  \cdot1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}P=P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  \underbrace{\eta_{v}\left(
1\right)  }_{=v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\eta_{v}\text{ is an
}\mathcal{A}_{0}\text{-module map}\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  v.
\end{align*}
Hence, we can define a $\mathbb{C}$-linear map $\rho:F\otimes V\left[
0\right]  \rightarrow V$ by%
\[
\rho\left(  P\otimes v\right)  =\eta_{v}\left(  P\right)  =P\left(
a_{-1},a_{-2},a_{-3},...\right)  v\ \ \ \ \ \ \ \ \ \ \text{for any }P\in
F\text{ and }v\in V\left[  0\right]  .
\]
This map $\rho$ is an $\mathcal{A}_{0}$-module map (because $\eta_{v}$ is an
$\mathcal{A}_{0}$-module map for every $v\in V\left[  0\right]  $).

The restriction of the map $\rho$ to the subspace $\mathbb{C}\cdot1\otimes
V\left[  0\right]  $ of $F\otimes V\left[  0\right]  $ is injective (since it
maps every $1\otimes v$ to $v$). Hence, the map $\rho$ is
injective\footnote{This follows from the following general
representation-theoretical fact (applied to $A=U\left(  \mathcal{A}%
_{0}\right)  $, $I=F$, $R=V\left[  0\right]  $, $S=V$, $i=1$ and $\phi=\rho$):
\par
Let $A$ be a $\mathbb{C}$-algebra. Let $I$ be an irreducible $A$-module, and
let $S$ be an $A$-module. Let $R$ be a vector space. Let $i\in I$ be nonzero.
Let $\phi:I\otimes R\rightarrow S$ be an $A$-module homomorphism such that the
restriction of $\phi$ to $\mathbb{C}i\otimes R$ is injective. Then, $\phi$ is
injective.}. Also, considering the quotient $\mathcal{A}_{0}$-module
$V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  $, we notice that
$E\mid_{V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  }$ has only
strictly positive eigenvalues (since $\rho\left(  F\otimes V\left[  0\right]
\right)  \supseteq V\left[  0\right]  $, so that all eigenvectors of
$E\mid_{V}$ to eigenvalue $0$ have been killed when factoring modulo
$\rho\left(  F\otimes V\left[  0\right]  \right)  $), and thus $V\diagup
\rho\left(  F\otimes V\left[  0\right]  \right)  =0$%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, $V\diagup
\rho\left(  F\otimes V\left[  0\right]  \right)  \neq0$. Thus, there exists
some nonzero $w\in V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  $.
Write $w$ as $\overline{v}$, where $v$ is an element of $V$ and $\overline{v}$
denotes the residue class of $v$ modulo $\rho\left(  F\otimes V\left[
0\right]  \right)  $. As we know, the $\mathcal{A}_{0}$-submodule $U\left(
\mathcal{A}_{0}\right)  \cdot v$ of $V$ is a finite-length module with
composition factors isomorphic to $F$. Thus, the $\mathcal{A}_{0}$-module
$U\left(  \mathcal{A}_{0}\right)  \cdot w$ (being a quotient module of
$U\left(  \mathcal{A}_{0}\right)  \cdot v$) must also be a finite-length
module with composition factors isomorphic to $F$. Hence, there exists a
submodule of $U\left(  \mathcal{A}_{0}\right)  \cdot w$ isomorphic to $F$
(since $w\neq0$ and thus $U\left(  \mathcal{A}_{0}\right)  \cdot w\neq0$).
This submodule contains a nonzero eigenvector of $E$ to eigenvalue $0$
(because $F$ contains a nonzero eigenvector of $E$ to eigenvalue $0$, namely
$1$). This is a contradiction to the fact that $E\mid_{V\diagup\rho\left(
F\otimes V\left[  0\right]  \right)  }$ has only strictly positive
eigenvalues. This contradiction shows that our assumption was wrong, so we do
have $V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  =0$, qed.}. In
other words, $V=\rho\left(  F\otimes V\left[  0\right]  \right)  $, so that
$\rho$ is surjective. Since $\rho$ is an injective and surjective
$\mathcal{A}_{0}$-module map, we conclude that $\rho$ is an $\mathcal{A}_{0}%
$-module isomorphism. Thus, $V\cong F\otimes V\left[  0\right]  $ as
$\mathcal{A}_{0}$-modules. This proves Proposition \ref{prop.V=F(X)U}.

\subsubsection{Remark on \texorpdfstring{$\mathcal{A}$}{A}-modules}

We will not use this until much later, but here is an analogue of Lemma
\ref{lem.V=F} for $\mathcal{A}$ instead of $\mathcal{A}_{0}$:

\begin{lemma}
\label{lem.V=F.A}Let $V$ be an $\mathcal{A}$-module. Let $\mu\in\mathbb{C}$.
Let $u\in V$ be such that $a_{i}u=0$ for all $i>0$, such that $a_{0}u=\mu u$,
and such that $Ku=u$. Then, there exists a homomorphism $\eta:F_{\mu
}\rightarrow V$ of $\mathcal{A}$-modules such that $\eta\left(  1\right)  =u$.
(This homomorphism $\eta$ is unique, although we won't need this.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.V=F.A}.} Let $\eta$ be the map $F\rightarrow
V$ which sends every polynomial $P\in F=\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to $P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u\in
V$.\ \ \ \ \footnote{Note that the term $P\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  $ denotes the evaluation of the polynomial $P$ at $\left(
x_{1},x_{2},x_{3},...\right)  =\left(  a_{-1},a_{-2},a_{-3},...\right)  $.
This evaluation is a well-defined element of $U\left(  \mathcal{A}_{0}\right)
$, since the elements $a_{-1}$, $a_{-2}$, $a_{-3}$, $...$ of $U\left(
\mathcal{A}_{0}\right)  $ commute.} Just as in the Second proof of Lemma
\ref{lem.V=F}, we can show that $\eta$ is an $\mathcal{A}_{0}$-module
homomorphism $F\rightarrow V$ such that $\eta\left(  1\right)  =u$. We are now
going to prove that this $\eta$ is also a homomorphism $F_{\mu}\rightarrow V$
of $\mathcal{A}$-modules. Clearly, in order to prove this, it is enough to
show that $\eta\left(  a_{0}P\right)  =a_{0}\eta\left(  P\right)  $ for all
$P\in F_{\mu}$.

Let $P\in F_{\mu}$. Since $a_{0}$ acts as multiplication by $\mu$ on $F_{\mu}%
$, we have $a_{0}P=\mu P$.

On the other hand, by the definition of $\eta$, we have $\eta\left(  P\right)
=P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u$, so that%
\begin{align*}
a_{0}\eta\left(  P\right)   &  =a_{0}P\left(  a_{-1},a_{-2},a_{-3},...\right)
\cdot u=P\left(  a_{-1},a_{-2},a_{-3},...\right)  a_{0}\cdot u\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{0}\text{ lies in the center of }\mathcal{A}\text{, and thus in
the center of }U\left(  \mathcal{A}\right)  \text{,}\\
\text{and thus }a_{0}P\left(  a_{-1},a_{-2},a_{-3},...\right)  =P\left(
a_{-1},a_{-2},a_{-3},...\right)  a_{0}%
\end{array}
\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  \underbrace{a_{0}u}_{=\mu u}%
=\mu\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}%
_{=\eta\left(  P\right)  }=\mu\eta\left(  P\right) \\
&  =\eta\left(  \underbrace{\mu P}_{=a_{0}P}\right)  =\eta\left(
a_{0}P\right)  .
\end{align*}
Thus, we have shown that $\eta\left(  a_{0}P\right)  =a_{0}\eta\left(
P\right)  $ for all $P\in F_{\mu}$. This completes the proof of Lemma
\ref{lem.V=F.A}.

\subsubsection{A rescaled version of the Fock space}

Here is a statement very similar to Corollary \ref{cor.fock}:

\begin{corollary}
\label{cor.focktilde}The Lie algebra $\mathcal{A}_{0}$ has a representation
$\widetilde{F}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ which is
given by
\begin{align*}
a_{-i}  &  \mapsto ix_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto\dfrac{\partial}{\partial x_{i}}\ \ \ \ \ \ \ \ \ \ \text{for
every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where ``$a_{-i}\mapsto ix_{i}$'' is just shorthand for ``$a_{-i}%
\mapsto\left(  \text{multiplication by }ix_{i}\right)  $''). For every $\mu
\in\mathbb{C}$, we can upgrade $\widetilde{F}$ to a representation
$\widetilde{F}_{\mu}$ of $\mathcal{A}$ by adding the condition that $a_{0}%
\mid_{\widetilde{F}_{\mu}}=\mu\cdot\operatorname*{id}$.
\end{corollary}

Note that the $\mathcal{A}_{0}$-module structure on $\widetilde{F}$ differs
from that on $F$ by a different choice of ``where to put the $i$ factor'': in
$F$ it is in the action of $a_{i}$, while in $\widetilde{F}$ it is in the
action of $a_{-i}$ (where $i\geq1$).

\begin{definition}
\label{def.focktilde}The representation $\widetilde{F}$ of $\mathcal{A}_{0}$
introduced in Corollary \ref{cor.focktilde} will be called the
\textit{rescaled Fock module} or the \textit{rescaled Fock representation}.
For every $\mu\in\mathbb{C}$, the representation $\widetilde{F}_{\mu}$ of
$\mathcal{A}$ introduced in Corollary \ref{cor.focktilde} will be called the
\textit{rescaled }$\mu$\textit{-Fock representation} of $\mathcal{A}$. The
vector space $\widetilde{F}$ itself, of course, is the same as the vector
space $F$ of Corollary \ref{cor.fock}, and thus we simply call it the Fock space.
\end{definition}

\begin{proposition}
\label{prop.resc}Let $\operatorname*{resc}:\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  \rightarrow\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]
$ be the $\mathbb{C}$-algebra homomorphism which sends $x_{i}$ to $ix_{i}$ for
every $i\in\left\{  1,2,3,...\right\}  $. (This homomorphism exists and is
unique by the universal property of the polynomial algebra. It is clear that
$\operatorname*{resc}$ multiplies every monomial by some scalar.)

\textbf{(a)} Then, $\operatorname*{resc}$ is an $\mathcal{A}_{0}$-module
isomorphism $F\rightarrow\widetilde{F}$. Thus, $F\cong\widetilde{F}$ as
$\mathcal{A}_{0}$-modules.

\textbf{(b)} Let $\mu\in\mathbb{C}$. Then, $\operatorname*{resc}$ is an
$\mathcal{A}$-module isomorphism $F_{\mu}\rightarrow\widetilde{F}_{\mu}$.
Thus, $F_{\mu}\cong\widetilde{F}_{\mu}$ as $\mathcal{A}$-modules.
\end{proposition}

Corollary \ref{cor.focktilde} and Proposition \ref{prop.resc} are both very
easy to prove: It is best to prove Proposition \ref{prop.resc} first (without
yet knowing that $\widetilde{F}$ and $\widetilde{F}_{\mu}$ are really an
$\mathcal{A}_{0}$-module and an $\mathcal{A}$-module, respectively), and then
use it to derive Corollary \ref{cor.focktilde} from Corollary \ref{cor.fock}
by means of $\operatorname*{resc}$. We leave all details to the reader.

The modules $\widetilde{F}$ and $F$ aren't that much different: They are
isomorphic by an isomorphism which has diagonal form with respect to the
monomial bases (due to Proposition \ref{prop.resc}). Nevertheless, it pays off
to use different notations for them so as not to let confusion arise. We are
going to work with $F$ most of the time, except when $\widetilde{F}$ is easier
to handle.

\subsubsection{An involution on \texorpdfstring{$\mathcal{A}$}{A} and a
bilinear form on the Fock space}

The following fact is extremely easy to prove:

\begin{proposition}
\label{prop.A.omega}Define a $\mathbb{C}$-linear map $\omega:\mathcal{A}%
\rightarrow\mathcal{A}$ by setting
\begin{align*}
\omega\left(  K\right)   &  =-K\ \ \ \ \ \ \ \ \ \ \text{and}\\
\omega\left(  a_{i}\right)   &  =-a_{-i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\mathbb{Z}.
\end{align*}
Then, $\omega$ is an automorphism of the Lie algebra $\mathcal{A}$. Also,
$\omega$ is an involution (this means that $\omega^{2}=\operatorname*{id}$).
Moreover, $\omega\left(  \mathcal{A}\left[  i\right]  \right)  =\mathcal{A}%
\left[  -i\right]  $ for all $i\in\mathbb{Z}$. Finally, $\omega\mid
_{\mathcal{A}\left[  0\right]  }=-\operatorname*{id}$.
\end{proposition}

Now, let us make a few conventions:

\begin{Convention}
\label{conv.fin}In the following, a map $\varphi:A\rightarrow\mathbb{N}$
(where $A$ is some set) is said to be \textit{finitely supported} if all but
finitely many $a\in A$ satisfy $\varphi\left(  a\right)  =0$. Sequences
(finite, infinite, or two-sided infinite) are considered as maps (from finite
sets, $\mathbb{N}$ or $\mathbb{Z}$, or occasionally other sets). Thus, a
sequence is finitely supported if and only if all but finitely many of its
elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.
\end{Convention}

\begin{proposition}
\label{prop.A.contravariantform}Define a $\mathbb{C}$-bilinear form $\left(
\cdot,\cdot\right)  :F\times F\rightarrow\mathbb{C}$ by setting%
\begin{align*}
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)   &  =\prod\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}%
}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!\\
&  \ \ \ \ \ \ \ \ \ \ \left.
\begin{array}
[c]{c}%
\text{for all sequences }\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\\
\text{and }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }%
\end{array}
\right.  .
\end{align*}
(This is well-defined, because each of the infinite products $\prod
\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}}$, $\prod\limits_{i=1}^{\infty
}i^{n_{i}}$ and $\prod\limits_{i=1}^{\infty}n_{i}!$ has only finitely many
terms distinct from $1$, and thus is well-defined.)

\textbf{(a)} This form $\left(  \cdot,\cdot\right)  $ is symmetric and nondegenerate.

\textbf{(b)} Every polynomial $P\in F=\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ satisfies $\left(  1,P\right)  =P\left(
0,0,0,...\right)  $.

\textbf{(c)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in F_{\mu}$ and
$Q\in F_{\mu}$ satisfy $\left(  xP,Q\right)  =-\left(  P,\omega\left(
x\right)  Q\right)  $, where $xP$ and $\omega\left(  x\right)  Q$ are
evaluated in the $\mathcal{A}$-module $F_{\mu}$.

\textbf{(d)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in F_{\mu}$ and
$Q\in F_{\mu}$ satisfy $\left(  P,xQ\right)  =-\left(  \omega\left(  x\right)
P,Q\right)  $, where $xQ$ and $\omega\left(  x\right)  P$ are evaluated in the
$\mathcal{A}$-module $F_{\mu}$.

\textbf{(e)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in
\widetilde{F}_{\mu}$ and $Q\in\widetilde{F}_{\mu}$ satisfy $\left(
xP,Q\right)  =-\left(  P,\omega\left(  x\right)  Q\right)  $, where $xP$ and
$\omega\left(  x\right)  Q$ are evaluated in the $\mathcal{A}$-module
$\widetilde{F}_{\mu}$.

\textbf{(f)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in
\widetilde{F}_{\mu}$ and $Q\in\widetilde{F}_{\mu}$ satisfy $\left(
P,xQ\right)  =-\left(  \omega\left(  x\right)  P,Q\right)  $, where $xQ$ and
$\omega\left(  x\right)  P$ are evaluated in the $\mathcal{A}$-module
$\widetilde{F}_{\mu}$.
\end{proposition}

We are going to put the form $\left(  \cdot,\cdot\right)  $ from this
proposition into a broader context in Proposition \ref{prop.invol.A}; indeed,
we will see that it is an example of a contravariant form on a Verma module of
a Lie algebra with involution. (``Contravariant'' means that $\left(
av,w\right)  =-\left(  v,\omega\left(  a\right)  w\right)  $ and $\left(
v,aw\right)  =-\left(  \omega\left(  a\right)  v,w\right)  $ for all $a$ in
the Lie algebra and $v$ and $w$ in the module. In the case of our form
$\left(  \cdot,\cdot\right)  $, the contravariantness of the form follows from
Proposition \ref{prop.A.contravariantform} \textbf{(c)} and \textbf{(d)}.)

\textit{Proof of Proposition \ref{prop.A.contravariantform}.} \textbf{(a)} For
any sequences $\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ and $\left(
m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, we have%
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  =\prod\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}%
}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!
\]
and%
\[
\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...,x_{1}^{n_{1}}x_{2}^{n_{2}%
}x_{3}^{n_{3}}...\right)  =\prod\limits_{i=1}^{\infty}\delta_{m_{i},n_{i}%
}\cdot\prod\limits_{i=1}^{\infty}i^{m_{i}}\cdot\prod\limits_{i=1}^{\infty
}m_{i}!.
\]
These two terms are equal in the case when $\left(  n_{1},n_{2},n_{3}%
,...\right)  \neq\left(  m_{1},m_{2},m_{3},...\right)  $ (because in this
case, they are both $0$ due to the presence of the $\prod\limits_{i=1}%
^{\infty}\delta_{n_{i},m_{i}}$ and $\prod\limits_{i=1}^{\infty}\delta
_{m_{i},n_{i}}$ factors), and are clearly equal in the case when $\left(
n_{1},n_{2},n_{3},...\right)  =\left(  m_{1},m_{2},m_{3},...\right)  $ as
well. Hence, these two terms are always equal. In other words, any sequences
$\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ and $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfy
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  =\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...,x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)  .
\]
This proves that the form $\left(  \cdot,\cdot\right)  $ is symmetric.

The space $F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ has a basis
consisting of monomials. With respect to this basis, the form $\left(
\cdot,\cdot\right)  $ is represented by a diagonal matrix (because whenever
$\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ and $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ are
distinct, we have
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  =\underbrace{\prod\limits_{i=1}^{\infty}%
\delta_{n_{i},m_{i}}}_{\substack{=0\\\text{(since }\left(  n_{1},n_{2}%
,n_{3},...\right)  \neq\left(  m_{1},m_{2},m_{3},...\right)  \text{)}}%
}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!=0
\]
), whose diagonal entries are all nonzero (since every $\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$ satisfies%
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{n_{1}}x_{2}^{n_{2}%
}x_{3}^{n_{3}}...\right)  =\prod\limits_{i=1}^{\infty}\underbrace{\delta
_{n_{i},n_{i}}}_{=1}\cdot\prod\limits_{i=1}^{\infty}\underbrace{i^{n_{i}}%
}_{\neq0}\cdot\prod\limits_{i=1}^{\infty}\underbrace{n_{i}!}_{\neq0}\neq0
\]
). Hence, this form is nondegenerate. Proposition
\ref{prop.A.contravariantform} \textbf{(a)} is proven.

\textbf{(b)} We must prove that every polynomial $P\in F=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ satisfies $\left(  1,P\right)  =P\left(
0,0,0,...\right)  $. In order to show this, it is enough to check that every
\textbf{monomial} $P\in F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $
satisfies $\left(  1,P\right)  =P\left(  0,0,0,...\right)  $ (because the
equation $\left(  1,P\right)  =P\left(  0,0,0,...\right)  $ is linear in $P$,
and because the monomials span $F$). In other words, we must check that every
$\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ satisfies $\left(  1,x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)  =\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...\right)  \left(  0,0,0,...\right)  $. But this is easy:%
\begin{align*}
\left(  \underbrace{1}_{=x_{1}^{0}x_{2}^{0}x_{3}^{0}...},x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)   &  =\left(  x_{1}^{0}x_{2}^{0}x_{3}%
^{0}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)  =\prod
\limits_{i=1}^{\infty}\underbrace{\delta_{0,m_{i}}}_{=0^{m_{i}}}\cdot
\prod\limits_{i=1}^{\infty}\underbrace{i^{0}}_{=1}\cdot\prod\limits_{i=1}%
^{\infty}\underbrace{0!}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  \cdot
,\cdot\right)  \right) \\
&  =\prod\limits_{i=1}^{\infty}0^{m_{i}}=0^{m_{1}}0^{m_{2}}0^{m_{3}%
}...=\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)  \left(
0,0,0,...\right)  ,
\end{align*}
qed. Proposition \ref{prop.A.contravariantform} \textbf{(b)} is proven.

\textbf{(c)} We must prove that any $x\in\mathcal{A}$, $P\in F_{\mu}$ and
$Q\in F_{\mu}$ satisfy $\left(  xP,Q\right)  =-\left(  P,\omega\left(
x\right)  Q\right)  $. Since this equation is linear in each of $x$, $P$ and
$Q$, we can WLOG assume that $x$ is an element of the basis $\left\{
a_{n}\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  K\right\}  $ of
$\mathcal{A}$ and that $P$ and $Q$ are monomials (since monomials span $F$).
So let us assume this.

Since $x$ is an element of the basis $\left\{  a_{n}\ \mid\ n\in
\mathbb{Z}\right\}  \cup\left\{  K\right\}  $ of $\mathcal{A}$, we have either
$x=a_{j}$ for some $j\in\mathbb{Z}$, or $x=K$. Since the latter case is
trivial (in fact, when $x=K$, then%
\[
\left(  xP,Q\right)  =\left(  KP,Q\right)  =\left(  P,Q\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }1\text{ on }F_{\mu
}\text{, so that }KP=P\right)
\]
and%
\begin{align*}
-\left(  P,\omega\left(  \underbrace{x}_{=K}\right)  Q\right)   &  =-\left(
P,\underbrace{\omega\left(  K\right)  }_{=-K}Q\right)  =-\left(  P,-KQ\right)
=\left(  P,KQ\right)  =\left(  P,Q\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }1\text{ on
}F_{\mu}\text{, so that }KQ=Q\right)  ,
\end{align*}
so that $\left(  xP,Q\right)  =-\left(  P,\omega\left(  x\right)  Q\right)  $
is proven), we can WLOG assume that we are in the former case, i. e., that
$x=a_{j}$ for some $j\in\mathbb{Z}$. Assume this, and consider this $j$.

Since $P$ is a monomial, there exists a $\left(  n_{1},n_{2},n_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$. Consider this $\left(
n_{1},n_{2},n_{3},...\right)  $.

Since $Q$ is a monomial, there exists a $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$Q=x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$. Consider this $\left(
m_{1},m_{2},m_{3},...\right)  $.

We must prove that $\left(  xP,Q\right)  =-\left(  P,\omega\left(  x\right)
Q\right)  $. Since $\left(  xP,Q\right)  =\left(  a_{j}P,Q\right)  $ (because
$x=a_{j}$) and $-\left(  P,\omega\left(  x\right)  Q\right)  =\left(
P,a_{-j}Q\right)  $ (because $-\left(  P,\omega\left(  \underbrace{x}_{=a_{j}%
}\right)  Q\right)  =-\left(  P,\underbrace{\omega\left(  a_{j}\right)
}_{=-a_{-j}}Q\right)  =-\left(  P,-a_{-j}Q\right)  =\left(  P,a_{-j}Q\right)
$), this rewrites as $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $.
Hence, we must only prove that $\left(  a_{j}P,Q\right)  =\left(
P,a_{-j}Q\right)  $.

We will distinguish between three cases:

\textit{Case 1:} We have $j\geq1$.

\textit{Case 2:} We have $j=0$.

\textit{Case 3:} We have $j\leq-1$.

First, let us consider Case 1. In this case, by the definition of $F_{\mu}$,
we know that $a_{j}$ acts on $F_{\mu}$ as $j\dfrac{\partial}{\partial x_{j}}$,
whereas $a_{-j}$ acts on $F_{\mu}$ as multiplication by $x_{j}$. Hence,
$a_{j}P=j\dfrac{\partial}{\partial x_{j}}P$ and $a_{-j}Q=x_{j}Q$.

Since $Q=x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$, we have $x_{j}%
Q=x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}}...$,
where the sequence $\left(  m_{1}^{\prime},m_{2}^{\prime},m_{3}^{\prime
},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}$ is defined by%
\[
m_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
m_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
m_{i}+1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,3,...\right\}
.
\]
Note that this definition immediately yields $m_{j}^{\prime}=m_{j}+1\geq1$, so
that $\delta_{0,m_{j}^{\prime}}=0$.

As a consequence of the definition of $\left(  m_{1}^{\prime},m_{2}^{\prime
},m_{3}^{\prime},...\right)  $, we have $m_{i}^{\prime}-m_{i}=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  $ for every $i\in\left\{  1,2,3,...\right\}  $.

Now, $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is easily proven
when $n_{j}=0$\ \ \ \ \footnote{\textit{Proof.} Assume that $n_{j}=0$. Then,
$P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ is a monomial that does not
involve the indeterminate $x_{j}$; hence, $\dfrac{\partial}{\partial x_{j}%
}P=0$, so that $a_{j}P=j\underbrace{\dfrac{\partial}{\partial x_{j}}P}_{=0}%
=0$, and thus $\left(  a_{j}P,Q\right)  =\left(  0,Q\right)  =0$. On the other
hand, since $n_{j}=0$, we have $\delta_{n_{j},m_{j}^{\prime}}=\delta
_{0,m_{j}^{\prime}}=0$ and thus $\prod\limits_{i=1}^{\infty}\delta
_{n_{i},m_{i}^{\prime}}=0$ (since the product $\prod\limits_{i=1}^{\infty
}\delta_{n_{i},m_{i}^{\prime}}$ contains the factor $\delta_{n_{j}%
,m_{j}^{\prime}}$). Now, since $P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$
and $a_{-j}Q=x_{j}Q=x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}%
^{m_{3}^{\prime}}...$, we have%
\begin{align*}
\left(  P,a_{-j}Q\right)   &  =\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}%
}...,x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}%
}...\right) \\
&  =\underbrace{\prod\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}^{\prime}}}%
_{=0}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(
\cdot,\cdot\right)  \right) \\
&  =0=\left(  a_{j}P,Q\right)  .
\end{align*}
Hence, $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is proven when
$n_{j}=0$.}. Hence, for the remaining part of Case 1, we can WLOG assume that
$n_{j}\neq0$. Let us assume this. Then, $n_{j}\geq1$. Hence, since
$P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$, we have $\dfrac{\partial
}{\partial x_{j}}P=n_{j}x_{1}^{n_{1}^{\prime}}x_{2}^{n_{2}^{\prime}}%
x_{3}^{n_{3}^{\prime}}...$, where the sequence $\left(  n_{1}^{\prime}%
,n_{2}^{\prime},n_{3}^{\prime},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ is defined by%
\[
n_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
n_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
n_{i}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,3,...\right\}
.
\]
From this definition, it is clear that the sequence $\left(  n_{1}^{\prime
},n_{2}^{\prime},n_{3}^{\prime},...\right)  $ differs from the sequence
$\left(  n_{1},n_{2},n_{3},...\right)  $ only in the $j$-th term. Hence, the
product $\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}$ differs from the
product $\prod\limits_{i=1}^{\infty}i^{n_{i}}$ only in the $j$-th factor.
Thus,%
\begin{align*}
\dfrac{\prod\limits_{i=1}^{\infty}i^{n_{i}}}{\prod\limits_{i=1}^{\infty
}i^{n_{i}^{\prime}}}  &  =\dfrac{j^{n_{j}}}{j^{n_{j}^{\prime}}}=\dfrac
{j^{n_{j}}}{j^{n_{j}-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n_{j}%
^{\prime}=n_{j}-1\text{ by the definition of }\left(  n_{1}^{\prime}%
,n_{2}^{\prime},n_{3}^{\prime},...\right)  \right) \\
&  =j,
\end{align*}
so that $\prod\limits_{i=1}^{\infty}i^{n_{i}}=j\prod\limits_{i=1}^{\infty
}i^{n_{i}^{\prime}}$. A similar argument (using the products $\prod
\limits_{i=1}^{\infty}n_{i}^{\prime}$ and $\prod\limits_{i=1}^{\infty}n_{i}$
instead of the products $\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}$ and
$\prod\limits_{i=1}^{\infty}i^{n_{i}}$) shows that $\prod\limits_{i=1}%
^{\infty}n_{i}!=n_{j}\prod\limits_{i=1}^{\infty}n_{i}^{\prime}!$.

As a consequence of the definition of $\left(  n_{1}^{\prime},n_{2}^{\prime
},n_{3}^{\prime},...\right)  $, we have $n_{i}-n_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  $ for every $i\in\left\{  1,2,3,...\right\}  $. Thus, every
$i\in\left\{  1,2,3,...\right\}  $ satisfies%
\[
n_{i}-n_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  =m_{i}^{\prime}-m_{i},
\]
so that $n_{i}-m_{i}^{\prime}=n_{i}^{\prime}-m_{i}$, so that $\delta
_{n_{i}-m_{i}^{\prime},0}=\delta_{n_{i}^{\prime}-m_{i},0}$.

Now, since $P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ and $a_{-j}%
Q=x_{j}Q=x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}%
}...$, we have%
\begin{align*}
\left(  P,a_{-j}Q\right)   &  =\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}%
}...,x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}%
}...\right) \\
&  =\prod\limits_{i=1}^{\infty}\underbrace{\delta_{n_{i},m_{i}^{\prime}}%
}_{=\delta_{n_{i}-m_{i}^{\prime},0}=\delta_{n_{i}^{\prime}-m_{i},0}%
=\delta_{n_{i}^{\prime},m_{i}}}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}%
\cdot\prod\limits_{i=1}^{\infty}n_{i}!\ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\left(  \cdot,\cdot\right)  \right) \\
&  =\prod\limits_{i=1}^{\infty}\delta_{n_{i}^{\prime},m_{i}}\cdot
\underbrace{\prod\limits_{i=1}^{\infty}i^{n_{i}}}_{=j\prod\limits_{i=1}%
^{\infty}i^{n_{i}^{\prime}}}\cdot\underbrace{\prod\limits_{i=1}^{\infty}%
n_{i}!}_{=n_{j}\prod\limits_{i=1}^{\infty}n_{i}^{\prime}!}=jn_{j}\cdot
\prod\limits_{i=1}^{\infty}\delta_{n_{i}^{\prime},m_{i}}\cdot\prod
\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}\cdot\prod\limits_{i=1}^{\infty}%
n_{i}^{\prime}!.
\end{align*}
Compared with%
\begin{align*}
\left(  a_{j}P,Q\right)   &  =\left(  jn_{j}x_{1}^{n_{1}^{\prime}}x_{2}%
^{n_{2}^{\prime}}x_{3}^{n_{3}^{\prime}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}P=j\underbrace{\dfrac
{\partial}{\partial x_{j}}P}_{=n_{j}x_{1}^{n_{1}^{\prime}}x_{2}^{n_{2}%
^{\prime}}x_{3}^{n_{3}^{\prime}}...}=jn_{j}x_{1}^{n_{1}^{\prime}}x_{2}%
^{n_{2}^{\prime}}x_{3}^{n_{3}^{\prime}}...\text{ and }Q=x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right) \\
&  =jn_{j}\underbrace{\left(  x_{1}^{n_{1}^{\prime}}x_{2}^{n_{2}^{\prime}%
}x_{3}^{n_{3}^{\prime}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)
}_{\substack{=\prod\limits_{i=1}^{\infty}\delta_{n_{i}^{\prime},m_{i}}%
\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}\cdot\prod\limits_{i=1}%
^{\infty}n_{i}^{\prime}!\\\text{(by the definition of }\left(  \cdot
,\cdot\right)  \text{)}}}=jn_{j}\cdot\prod\limits_{i=1}^{\infty}\delta
_{n_{i}^{\prime},m_{i}}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}%
\cdot\prod\limits_{i=1}^{\infty}n_{i}^{\prime}!,
\end{align*}
this yields $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $. Thus,
$\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is proven in Case 1. In
other words, we have shown that%
\begin{equation}
\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)
\ \ \ \ \ \ \ \ \ \ \text{for every integer }j\geq1\text{ and any monomials
}P\text{ and }Q\text{.} \label{pf.A.contravariantform.1}%
\end{equation}


In Case 2, proving $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is
trivial (since $a_{0}$ acts on $F_{\mu}$ as $\mu\cdot\operatorname*{id}$).

Now, let us consider Case 3. In this case, $j\leq-1$, so that $-j\geq1$. Thus,
(\ref{pf.A.contravariantform.1}) (applied to $-j$, $Q$ and $P$ instead of $j$,
$P$ and $Q$) yields $\left(  a_{-j}Q,P\right)  =\left(  Q,a_{-\left(
-j\right)  }P\right)  $. Now, since $\left(  \cdot,\cdot\right)  $ is
symmetric, we have $\left(  a_{j}P,Q\right)  =\left(  Q,\underbrace{a_{j}%
}_{=a_{-\left(  -j\right)  }}P\right)  =\left(  Q,a_{-\left(  -j\right)
}P\right)  =\left(  a_{-j}Q,P\right)  =\left(  P,a_{-j}Q\right)  $ (again
since $\left(  \cdot,\cdot\right)  $ is symmetric). Thus, $\left(
a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is proven in Case 3.

We have now proven $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is
each of the cases 1, 2 and 3. Since no other cases can occur, this completes
the proof of $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $. As we
have explained above, this proves Proposition \ref{prop.A.contravariantform}
\textbf{(c)}.

\textbf{(d)} Let $x\in\mathcal{A}$, $P\in F_{\mu}$ and $Q\in F_{\mu}$. Since
the form $\left(  \cdot,\cdot\right)  $ is symmetric, we have $\left(
P,xQ\right)  =\left(  xQ,P\right)  $ and $\left(  \omega\left(  x\right)
P,Q\right)  =\left(  Q,\omega\left(  x\right)  P\right)  $. Proposition
\ref{prop.A.contravariantform} \textbf{(c)} (applied to $P$ and $Q$ instead of
$Q$ and $P$) yields $\left(  xQ,P\right)  =-\left(  Q,\omega\left(  x\right)
P\right)  $. Thus, $\left(  P,xQ\right)  =\left(  xQ,P\right)
=-\underbrace{\left(  Q,\omega\left(  x\right)  P\right)  }_{=\left(
\omega\left(  x\right)  P,Q\right)  }=-\left(  \omega\left(  x\right)
P,Q\right)  $. This proves Proposition \ref{prop.A.contravariantform}
\textbf{(d)}.

\textbf{(e)} and \textbf{(f)} The proofs of Proposition
\ref{prop.A.contravariantform} \textbf{(e)} and \textbf{(f)} are analogous to
those of Proposition \ref{prop.A.contravariantform} \textbf{(c)} and
\textbf{(d)}, respectively, and thus will be omitted.

\subsection{Representations of the Virasoro algebra
\texorpdfstring{$\operatorname*{Vir}$}{Vir}}

We now come to the Virasoro algebra $\operatorname*{Vir}$. First, some notations:

\begin{definition}
\textbf{(a)} The notion ``\textit{Virasoro module}'' will be a synonym for
``$\operatorname*{Vir}$-module''. Similarly, ``Virasoro action'' means
``$\operatorname*{Vir}$-action''.

\textbf{(b)} Let $c\in\mathbb{C}$. A $\operatorname*{Vir}$-module $M$ is said
to have \textit{central charge }$c$ if and only if the element $C$ of
$\operatorname*{Vir}$ acts as $c\cdot\operatorname*{id}$ on $M$.
\end{definition}

Note that not every $\operatorname*{Vir}$-module has a central charge (and the
zero module has infinitely many central charges), but Corollary \ref{cor.dix2}
yields that every irreducible $\operatorname*{Vir}$-module of countable
dimension has a (unique) central charge.

There are lots and lots of Virasoro modules in mathematics, and we will
encounter them as this course progresses; the more complicated among them will
require us to introduce a lot of machinery like Verma modules, semiinfinite
wedges and affine Lie algebras. For now, we define one of the simplest
families of representations of $\operatorname*{Vir}$: the ``chargeless''
$\operatorname*{Vir}$-modules $V_{\alpha,\beta}$ parametrized by pairs of
complex numbers $\left(  \alpha,\beta\right)  $.

\begin{proposition}
\label{prop.Vab.1}Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Let
$V_{\alpha,\beta}$ be the vector space of formal expressions of the form
$gt^{\alpha}\left(  dt\right)  ^{\beta}$ with $g\in\mathbb{C}\left[
t,t^{-1}\right]  $ (where $\mathbb{C}\left[  t,t^{-1}\right]  $ is the ring of
Laurent polynomials in the variable $t$). (Formally, this vector space
$V_{\alpha,\beta}$ is defined to be a copy of the $\mathbb{C}$-vector space
$\mathbb{C}\left[  t,t^{-1}\right]  $, but in which the element corresponding
to any $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ is denoted by $gt^{\alpha
}\left(  dt\right)  ^{\beta}$. For a geometric intuition, the elements of
$V_{\alpha,\beta}$ can be seen as ``tensor fields'' of rank $\beta$ and
branching $\alpha$ on the punctured complex plane $\mathbb{C}^{\times}$.)

\textbf{(a)} The formula
\begin{equation}
f\partial\rightharpoonup\left(  gt^{\alpha}\left(  dt\right)  ^{\beta}\right)
=\left(  fg^{\prime}+\alpha t^{-1}fg+\beta f^{\prime}g\right)  t^{\alpha
}\left(  dt\right)  ^{\beta} \label{ex1.1.1}%
\end{equation}
defines an action of $W$ on $V_{\alpha,\beta}$. Thus, $V_{\alpha,\beta}$
becomes a $\operatorname*{Vir}$-module with $C$ acting as $0$. (In other
words, $V_{\alpha,\beta}$ becomes a $\operatorname*{Vir}$-module with central
charge $0$.)

\textbf{(b)} For every $k\in\mathbb{Z}$, let $v_{k}=t^{-k+\alpha}\left(
dt\right)  ^{\beta}\in V_{\alpha,\beta}$. Here, for any $\ell\in\mathbb{Z}$,
the term $t^{\ell+\alpha}\left(  dt\right)  ^{\beta}$ denotes $t^{\ell
}t^{\alpha}\left(  dt\right)  ^{\beta}$. Then,%
\begin{equation}
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{Z}\text{ and }k\in\mathbb{Z}.
\label{ex1.1.2.var}%
\end{equation}

\end{proposition}

Note that Proposition \ref{prop.Vab.1} was Homework Set 1 exercise 1, but the
notation $v_{k}$ had a slightly different meaning in Homework Set 1 exercise 1
than it has here.

The proof of this proposition consists of straightforward computations. We
give it for the sake of completeness, slightly simplifying the calculation by
introducing auxiliary functions.

\textit{Proof of Proposition \ref{prop.Vab.1}.} \textbf{(a)} In order to prove
Proposition \ref{prop.Vab.1} \textbf{(a)}, we must show that the formula
(\ref{ex1.1.1}) defines an action of $W$ on $V_{\alpha,\beta}$.

It is clear that $\left(  fg^{\prime}+\alpha t^{-1}fg+\beta f^{\prime
}g\right)  t^{\alpha}\left(  dt\right)  ^{\beta}$ depends linearly on each of
$f$ and $g$. Hence, we must only prove that, with the definition
(\ref{ex1.1.1}), we have
\begin{equation}
\left[  f\partial,g\partial\right]  \rightharpoonup\left(  ht^{\alpha}\left(
dt\right)  ^{\beta}\right)  =f\partial\rightharpoonup\left(  g\partial
\rightharpoonup\left(  ht^{\alpha}\left(  dt\right)  ^{\beta}\right)  \right)
-g\partial\rightharpoonup\left(  f\partial\rightharpoonup\left(  ht^{\alpha
}\left(  dt\right)  ^{\beta}\right)  \right)  \label{sol1.1.1}%
\end{equation}
for any Laurent polynomials $f$, $g$ and $h$ in $\mathbb{C}\left[
t,t^{-1}\right]  $.

So let $f$, $g$ and $h$ be any three Laurent polynomials in $\mathbb{C}\left[
t,t^{-1}\right]  $. Denote by $p$ the Laurent polynomial $h^{\prime}+\alpha
t^{-1}h$. Denote by $q$ the Laurent polynomial $fg^{\prime}-gf^{\prime}$.
Then,\footnote{In the following computations, terms like $f\left(  u\right)  $
(where $u$ is a subterm, usually a complicated one) have to be understood as
$f\cdot u$ (the product of $f$ with $u$) and not as $f\left(  u\right)  $ (the
Laurent polynomial $f$ applied to $u$).}%
\begin{equation}
f\left(  g^{\prime}h\right)  ^{\prime}-g\left(  f^{\prime}h\right)  ^{\prime
}=q^{\prime}h+qh^{\prime} \label{sol1.1.fgh}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol1.1.fgh}):} Since $q=fg^{\prime
}-gf^{\prime}$, we have $qh=\left(  fg^{\prime}-gf^{\prime}\right)
h=fg^{\prime}h-gf^{\prime}h=f\left(  g^{\prime}h\right)  -g\left(  f^{\prime
}h\right)  $, so that%
\begin{align*}
\left(  qh\right)  ^{\prime}  &  =\left(  f\left(  g^{\prime}h\right)
-g\left(  f^{\prime}h\right)  \right)  ^{\prime}=\underbrace{\left(  f\left(
g^{\prime}h\right)  \right)  ^{\prime}}_{\substack{=f^{\prime}\left(
g^{\prime}h\right)  +f\left(  g^{\prime}h\right)  ^{\prime}\\\text{(by the
Leibniz rule)}}}-\underbrace{\left(  g\left(  f^{\prime}h\right)  \right)
^{\prime}}_{\substack{=g^{\prime}\left(  f^{\prime}h\right)  +g\left(
f^{\prime}h\right)  ^{\prime}\\\text{(by the Leibniz rule)}}}\\
&  =\underbrace{f^{\prime}\left(  g^{\prime}h\right)  }_{=f^{\prime}g^{\prime
}h}+f\left(  g^{\prime}h\right)  ^{\prime}-\underbrace{g^{\prime}\left(
f^{\prime}h\right)  }_{=f^{\prime}g^{\prime}h}-g\left(  f^{\prime}h\right)
^{\prime}\\
&  =f^{\prime}g^{\prime}h+f\left(  g^{\prime}h\right)  ^{\prime}-f^{\prime
}g^{\prime}h-g\left(  f^{\prime}h\right)  ^{\prime}=f\left(  g^{\prime
}h\right)  ^{\prime}-g\left(  f^{\prime}h\right)  ^{\prime}.
\end{align*}
Since $\left(  qh\right)  ^{\prime}=q^{\prime}h+qh^{\prime}$ (by the Leibniz
rule), this rewrites as $q^{\prime}h+qh^{\prime}=f\left(  g^{\prime}h\right)
^{\prime}-g\left(  f^{\prime}h\right)  ^{\prime}$. This proves
(\ref{sol1.1.fgh}).} and%
\begin{align}
&  f\underbrace{\left(  gp\right)  ^{\prime}}_{\substack{=g^{\prime
}p+gp^{\prime}\\\text{(by the Leibniz rule)}}}-g\underbrace{\left(  fp\right)
^{\prime}}_{\substack{=f^{\prime}p+fp^{\prime}\\\text{(by the Leibniz rule)}%
}}\nonumber\\
&  =\underbrace{f\left(  g^{\prime}p+gp^{\prime}\right)  }_{=fg^{\prime
}p+fgp^{\prime}}-\underbrace{g\left(  f^{\prime}p+fp^{\prime}\right)
}_{=gf^{\prime}p+gfp^{\prime}=gf^{\prime}p+fgp^{\prime}}\nonumber\\
&  =fg^{\prime}p+fgp^{\prime}-gf^{\prime}p-fgp^{\prime}=fg^{\prime
}p-gf^{\prime}p=\underbrace{\left(  fg^{\prime}-gf^{\prime}\right)  }%
_{=q}p=qp. \label{sol1.1.fgh2}%
\end{align}


Also,%
\begin{align*}
\underbrace{\left[  f\partial,g\partial\right]  }_{=\left(  fg^{\prime
}-gf^{\prime}\right)  \partial}\rightharpoonup\left(  ht^{\alpha}\left(
dt\right)  ^{\beta}\right)   &  =\underbrace{\left(  fg^{\prime}-gf^{\prime
}\right)  }_{=q}\partial\rightharpoonup\left(  ht^{\alpha}\left(  dt\right)
^{\beta}\right)  =q\partial\rightharpoonup\left(  ht^{\alpha}\left(
dt\right)  ^{\beta}\right) \\
&  =\left(  \underbrace{qh^{\prime}+\alpha t^{-1}qh}_{\substack{=q\left(
h^{\prime}+\alpha t^{-1}h\right)  =qp\\\text{(since }h^{\prime}+\alpha
t^{-1}h=p\text{)}}}+\beta q^{\prime}h\right)  t^{\alpha}\left(  dt\right)
^{\beta}=\left(  qp+\beta q^{\prime}h\right)  t^{\alpha}\left(  dt\right)
^{\beta}.
\end{align*}
Moreover, $gh^{\prime}+\alpha t^{-1}gh=g\underbrace{\left(  h^{\prime}+\alpha
t^{-1}h\right)  }_{=p}=gp$, and
\[
g\partial\rightharpoonup\left(  ht^{\alpha}\left(  dt\right)  ^{\beta}\right)
=\left(  \underbrace{gh^{\prime}+\alpha t^{-1}gh}_{=gp}+\beta g^{\prime
}h\right)  t^{\alpha}\left(  dt\right)  ^{\beta}=\left(  gp+\beta g^{\prime
}h\right)  t^{\alpha}\left(  dt\right)  ^{\beta},
\]
so that%
\begin{align}
&  f\partial\rightharpoonup\underbrace{\left(  g\partial\rightharpoonup\left(
ht^{\alpha}\left(  dt\right)  ^{\beta}\right)  \right)  }_{=\left(  gp+\beta
g^{\prime}h\right)  t^{\alpha}\left(  dt\right)  ^{\beta}}\nonumber\\
&  =f\partial\rightharpoonup\left(  \left(  gp+\beta g^{\prime}h\right)
t^{\alpha}\left(  dt\right)  ^{\beta}\right) \nonumber\\
&  =\left(  f\underbrace{\left(  gp+\beta g^{\prime}h\right)  ^{\prime}%
}_{=\left(  gp\right)  ^{\prime}+\beta\left(  g^{\prime}h\right)  ^{\prime}%
}+\underbrace{\alpha t^{-1}f\left(  gp+\beta g^{\prime}h\right)  }_{=\alpha
t^{-1}fgp+\alpha\beta t^{-1}fg^{\prime}h}+\underbrace{\beta f^{\prime}\left(
gp+\beta g^{\prime}h\right)  }_{=\beta f^{\prime}gp+\beta^{2}f^{\prime
}g^{\prime}h}\right)  t^{\alpha}\left(  dt\right)  ^{\beta}\nonumber\\
&  =\left(  \underbrace{f\left(  \left(  gp\right)  ^{\prime}+\beta\left(
g^{\prime}h\right)  ^{\prime}\right)  }_{=f\left(  gp\right)  ^{\prime}+\beta
f\left(  g^{\prime}h\right)  ^{\prime}}+\alpha t^{-1}fgp+\alpha\beta
t^{-1}fg^{\prime}h+\beta f^{\prime}gp+\beta^{2}f^{\prime}g^{\prime}h\right)
t^{\alpha}\left(  dt\right)  ^{\beta}\nonumber\\
&  =\left(  f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime}h\right)
^{\prime}+\alpha t^{-1}fgp+\alpha\beta t^{-1}fg^{\prime}h+\beta f^{\prime
}gp+\beta^{2}f^{\prime}g^{\prime}h\right)  t^{\alpha}\left(  dt\right)
^{\beta}. \label{sol1.1.2.var}%
\end{align}


Since the roles of $f$ and $g$ in our situation are symmetric, we can
interchange $f$ and $g$ in (\ref{sol1.1.2.var}), and obtain%
\begin{align}
&  g\partial\rightharpoonup\left(  f\partial\rightharpoonup\left(  ht^{\alpha
}\left(  dt\right)  ^{\beta}\right)  \right) \nonumber\\
&  =\left(  g\left(  fp\right)  ^{\prime}+\beta g\left(  f^{\prime}h\right)
^{\prime}+\alpha t^{-1}gfp+\alpha\beta t^{-1}gf^{\prime}h+\beta g^{\prime
}fp+\beta^{2}g^{\prime}f^{\prime}h\right)  t^{\alpha}\left(  dt\right)
^{\beta}\nonumber\\
&  =\left(  g\left(  fp\right)  ^{\prime}+\beta g\left(  f^{\prime}h\right)
^{\prime}+\alpha t^{-1}fgp+\alpha\beta t^{-1}gf^{\prime}h+\beta g^{\prime
}fp+\beta^{2}f^{\prime}g^{\prime}h\right)  t^{\alpha}\left(  dt\right)
^{\beta}. \label{sol1.1.3}%
\end{align}


Thus,%
\begin{align*}
&  \underbrace{f\partial\rightharpoonup\left(  g\partial\rightharpoonup\left(
ht^{\alpha}\left(  dt\right)  ^{\beta}\right)  \right)  }_{\substack{=\left(
f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime}h\right)  ^{\prime
}+\alpha t^{-1}fgp+\alpha\beta t^{-1}fg^{\prime}h+\beta f^{\prime}gp+\beta
^{2}f^{\prime}g^{\prime}h\right)  t^{\alpha}\left(  dt\right)  ^{\beta
}\\\text{(by (\ref{sol1.1.2.var}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\underbrace{g\partial
\rightharpoonup\left(  f\partial\rightharpoonup\left(  ht^{\alpha}\left(
dt\right)  ^{\beta}\right)  \right)  }_{\substack{=\left(  g\left(  fp\right)
^{\prime}+\beta g\left(  f^{\prime}h\right)  ^{\prime}+\alpha t^{-1}%
fgp+\alpha\beta t^{-1}gf^{\prime}h+\beta g^{\prime}fp+\beta^{2}f^{\prime
}g^{\prime}h\right)  t^{\alpha}\left(  dt\right)  ^{\beta}\\\text{(by
(\ref{sol1.1.3}))}}}\\
&  =\left(  f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime}h\right)
^{\prime}+\alpha t^{-1}fgp+\alpha\beta t^{-1}fg^{\prime}h+\beta f^{\prime
}gp+\beta^{2}f^{\prime}g^{\prime}h\right)  t^{\alpha}\left(  dt\right)
^{\beta}\\
&  \ \ \ \ \ \ \ \ \ \ -\left(  g\left(  fp\right)  ^{\prime}+\beta g\left(
f^{\prime}h\right)  ^{\prime}+\alpha t^{-1}fgp+\alpha\beta t^{-1}gf^{\prime
}h+\beta g^{\prime}fp+\beta^{2}f^{\prime}g^{\prime}h\right)  t^{\alpha}\left(
dt\right)  ^{\beta}\\
&  =\left(  \left(  f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime
}h\right)  ^{\prime}+\alpha t^{-1}fgp+\alpha\beta t^{-1}fg^{\prime}h+\beta
f^{\prime}gp+\beta^{2}f^{\prime}g^{\prime}h\right)  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  -\left(  g\left(
fp\right)  ^{\prime}+\beta g\left(  f^{\prime}h\right)  ^{\prime}+\alpha
t^{-1}fgp+\alpha\beta t^{-1}gf^{\prime}h+\beta g^{\prime}fp+\beta^{2}%
f^{\prime}g^{\prime}h\right)  \right)  t^{\alpha}\left(  dt\right)  ^{\beta}\\
&  =\left(  \underbrace{f\left(  gp\right)  ^{\prime}-g\left(  fp\right)
^{\prime}}_{\substack{=qp\\\text{(by (\ref{sol1.1.fgh2}))}}}+\underbrace{\beta
f\left(  g^{\prime}h\right)  ^{\prime}-\beta g\left(  f^{\prime}h\right)
^{\prime}}_{=\beta\left(  f\left(  g^{\prime}h\right)  ^{\prime}-g\left(
f^{\prime}h\right)  ^{\prime}\right)  }+\underbrace{\alpha\beta t^{-1}%
fg^{\prime}h-\alpha\beta t^{-1}gf^{\prime}h}_{=\alpha\beta t^{-1}\left(
fg^{\prime}-gf^{\prime}\right)  h}+\underbrace{\beta f^{\prime}gp-\beta
g^{\prime}fp}_{=\beta\left(  f^{\prime}g-g^{\prime}f\right)  p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ t^{\alpha}\left(  dt\right)  ^{\beta}\\
&  =\left(  qp+\beta\underbrace{\left(  f\left(  g^{\prime}h\right)  ^{\prime
}-g\left(  f^{\prime}h\right)  ^{\prime}\right)  }_{\substack{=q^{\prime
}h+qh^{\prime}\\\text{(by (\ref{sol1.1.fgh}))}}}+\alpha\beta t^{-1}%
\underbrace{\left(  fg^{\prime}-gf^{\prime}\right)  }_{=q}h+\beta
\underbrace{\left(  f^{\prime}g-g^{\prime}f\right)  }%
_{\substack{=-q\\\text{(since }q=fg^{\prime}-gf^{\prime}=g^{\prime}%
f-f^{\prime}g\text{)}}}p\right)  t^{\alpha}\left(  dt\right)  ^{\beta}\\
&  =\left(  qp+\underbrace{\beta\left(  q^{\prime}h+qh^{\prime}\right)
}_{=\beta q^{\prime}h+\beta qh^{\prime}}+\alpha\beta t^{-1}%
qh+\underbrace{\beta\left(  -q\right)  p}_{=-\beta qp}\right)  t^{\alpha
}\left(  dt\right)  ^{\beta}\\
&  =\left(  qp+\beta q^{\prime}h+\underbrace{\beta qh^{\prime}+\alpha\beta
t^{-1}qh}_{=\beta q\left(  h^{\prime}+\alpha t^{-1}h\right)  }-\beta
qp\right)  t^{\alpha}\left(  dt\right)  ^{\beta}\\
&  =\left(  qp+\beta q^{\prime}h+\beta q\underbrace{\left(  h^{\prime}+\alpha
t^{-1}h\right)  }_{=p}-\beta qp\right)  t^{\alpha}\left(  dt\right)  ^{\beta
}\\
&  =\left(  qp+\beta q^{\prime}h+\underbrace{\beta qp-\beta qp}_{=0}\right)
t^{\alpha}\left(  dt\right)  ^{\beta}=\left(  qp+\beta q^{\prime}h\right)
t^{\alpha}\left(  dt\right)  ^{\beta}=\left[  f\partial,g\partial\right]
\rightharpoonup\left(  ht^{\alpha}\left(  dt\right)  ^{\beta}\right)  .
\end{align*}


Thus, (\ref{sol1.1.1}) is proven for any Laurent polynomials $f$, $g$ and $h$.
This proves that the formula (\ref{ex1.1.1}) defines an action of $W$ on
$V_{\alpha,\beta}$. Hence, $V_{\alpha,\beta}$ becomes a $W$-module, i. e., a
$\operatorname*{Vir}$-module with $C$ acting as $0$. (In other words,
$V_{\alpha,\beta}$ becomes a $\operatorname*{Vir}$-module with central charge
$0$.) This proves Proposition \ref{prop.Vab.1} \textbf{(a)}.

\textbf{(b)} We only need to prove (\ref{ex1.1.2.var}).

Let $m\in\mathbb{Z}$ and $k\in\mathbb{Z}$. Then, $v_{k}=t^{-k+\alpha}\left(
dt\right)  ^{\beta}=t^{-k}t^{\alpha}\left(  dt\right)  ^{\beta}$ and
$v_{k-m}=t^{-\left(  k-m\right)  +\alpha}\left(  dt\right)  ^{\beta}%
=t^{m-k}t^{\alpha}\left(  dt\right)  ^{\beta}$. Thus,%
\begin{align*}
&  \underbrace{L_{m}}_{=-t^{m+1}\partial}\rightharpoonup\underbrace{v_{k}%
}_{=t^{-k}t^{\alpha}\left(  dt\right)  ^{\beta}}\\
&  =\left(  -t^{m+1}\partial\right)  \rightharpoonup\left(  t^{-k}t^{\alpha
}\left(  dt\right)  ^{\beta}\right) \\
&  =\left(  -t^{m+1}\underbrace{\left(  t^{-k}\right)  ^{\prime}}%
_{=-kt^{-k-1}}+\underbrace{\alpha t^{-1}\left(  -t^{m+1}\right)  t^{-k}%
}_{=-\alpha t^{-1}t^{m+1}t^{-k}}+\beta\underbrace{\left(  -t^{m+1}\right)
^{\prime}}_{=-\left(  m+1\right)  t^{m}}t^{-k}\right)  t^{\alpha}\left(
dt\right)  ^{\beta}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{ex1.1.1}), applied to
}f=-t^{m+1}\text{ and }g=t^{-k}\right) \\
&  =\left(  -\left(  -k\right)  \underbrace{t^{m+1}t^{-k-1}}_{=t^{m-k}}%
-\alpha\underbrace{t^{-1}t^{m+1}t^{-k}}_{=t^{\left(  -1\right)  +\left(
m+1\right)  +\left(  -k\right)  }=t^{m-k}}+\beta\left(  -\left(  m+1\right)
\right)  \underbrace{t^{m}t^{-k}}_{=t^{m-k}}\right)  t^{\alpha}\left(
dt\right)  ^{\beta}\\
&  =\left(  kt^{m-k}-\alpha t^{m-k}+\beta\left(  -\left(  m+1\right)  \right)
t^{m-k}\right)  t^{\alpha}\left(  dt\right)  ^{\beta}\\
&  =\underbrace{\left(  k-\alpha+\beta\left(  -\left(  m+1\right)  \right)
\right)  }_{=k-\alpha-\left(  m+1\right)  \beta}\underbrace{t^{m-k}t^{\alpha
}\left(  dt\right)  ^{\beta}}_{=v_{k-m}}=\left(  k-\alpha-\left(  m+1\right)
\beta\right)  v_{k-m}.
\end{align*}
This proves (\ref{ex1.1.2.var}). Proposition \ref{prop.Vab.1} \textbf{(b)} is proven.

The representations $V_{\alpha,\beta}$ are not all pairwise non-isomorphic,
but there are still uncountably many non-isomorphic ones among them. More precisely:

\begin{proposition}
\label{prop.Vab.iso}\textbf{(a)} For every $\ell\in\mathbb{Z}$, $\alpha
\in\mathbb{C}$ and $\beta\in\mathbb{C}$, the $\mathbb{C}$-linear map%
\begin{align*}
V_{\alpha,\beta}  &  \rightarrow V_{\alpha+\ell,\beta},\\
gt^{\alpha}\left(  dt\right)  ^{\beta}  &  \mapsto\left(  gt^{-\ell}\right)
t^{\alpha+\ell}\left(  dt\right)  ^{\beta}%
\end{align*}
is an isomorphism of $\operatorname*{Vir}$-modules. (This map sends $v_{k}$ to
$v_{k+\ell}$ for every $k\in\mathbb{Z}$.)

\textbf{(b)} For every $\alpha\in\mathbb{C}$, the $\mathbb{C}$-linear map%
\begin{align*}
V_{\alpha,0}  &  \rightarrow V_{\alpha-1,1},\\
gt^{\alpha}\left(  dt\right)  ^{0}  &  \mapsto\left(  -g^{\prime}t-\alpha
g\right)  t^{\alpha-1}\left(  dt\right)  ^{1}%
\end{align*}
is a homomorphism of $\operatorname*{Vir}$-modules. (This map sends $v_{k}$ to
$\left(  k-\alpha\right)  v_{k}$ for every $k\in\mathbb{Z}$.) If $\alpha
\notin\mathbb{Z}$, then this map is an isomorphism.

\textbf{(c)} Let $\left(  \alpha,\beta,\alpha^{\prime},\beta^{\prime}\right)
\in\mathbb{C}^{4}$. Then, $V_{\alpha,\beta}\cong V_{\alpha^{\prime}%
,\beta^{\prime}}$ as $\operatorname*{Vir}$-modules if and only if either
$\left(  \beta=\beta^{\prime}\text{ and }\alpha-\alpha^{\prime}\in
\mathbb{Z}\right)  $ or $\left(  \beta=0\text{, }\beta^{\prime}=1\text{,
}\alpha-\alpha^{\prime}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)
$ or $\left(  \beta=1\text{, }\beta^{\prime}=0\text{, }\alpha-\alpha^{\prime
}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)  $.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Vab.iso} (sketched).} \textbf{(a)} and
\textbf{(b)} Very easy and left to the reader.

\textbf{(c)} The $\Longleftarrow$ direction is handled by parts \textbf{(a)}
and \textbf{(b)}.

$\Longrightarrow:$ Assume that $V_{\alpha,\beta}\cong V_{\alpha^{\prime}%
,\beta^{\prime}}$ as $\operatorname*{Vir}$-modules. We must prove that either
$\left(  \beta=\beta^{\prime}\text{ and }\alpha-\alpha^{\prime}\in
\mathbb{Z}\right)  $ or $\left(  \beta=0\text{, }\beta^{\prime}=1\text{,
}\alpha-\alpha^{\prime}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)
$ or $\left(  \beta=1\text{, }\beta^{\prime}=0\text{, }\alpha-\alpha^{\prime
}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)  $.

Let $\Phi$ be the $\operatorname*{Vir}$-module isomorphism $V_{\alpha,\beta
}\rightarrow V_{\alpha^{\prime},\beta^{\prime}}$.

Applying (\ref{ex1.1.2.var}) to $m=0$, we obtain%
\begin{equation}
L_{0}v_{k}=\left(  k-\alpha-\beta\right)  v_{k}\text{ in }V_{\alpha,\beta
}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}. \label{pf.Vab.iso.1}%
\end{equation}
Hence, $L_{0}$ acts on $V_{\alpha,\beta}$ as a diagonal matrix with
eigenvalues $k-\alpha-\beta$ for all $k\in\mathbb{Z}$, each eigenvalue
appearing exactly once. Similarly, applying (\ref{ex1.1.2.var}) to $0$ and
$\left(  \alpha^{\prime},\beta^{\prime}\right)  $ instead of $m$ and $\left(
\alpha,\beta\right)  $, we obtain%
\begin{equation}
L_{0}v_{k}=\left(  k-\alpha^{\prime}-\beta^{\prime}\right)  v_{k}\text{ in
}V_{\alpha^{\prime},\beta^{\prime}}\ \ \ \ \ \ \ \ \ \ \text{for every }%
k\in\mathbb{Z}. \label{pf.Vab.iso.2}%
\end{equation}
Thus, $L_{0}$ acts on $V_{\alpha^{\prime},\beta^{\prime}}$ as a diagonal
matrix with eigenvalues $k-\alpha^{\prime}-\beta^{\prime}$ for all
$k\in\mathbb{Z}$, each eigenvalue appearing exactly once.

But since $V_{\alpha,\beta}\cong V_{\alpha^{\prime},\beta^{\prime}}$ as
$\operatorname*{Vir}$-modules, the eigenvalues of $L_{0}$ acting on
$V_{\alpha,\beta}$ must be the same as the eigenvalues of $L_{0}$ acting on
$V_{\alpha^{\prime},\beta^{\prime}}$. In other words,%
\[
\left\{  k-\alpha-\beta\ \mid\ k\in\mathbb{Z}\right\}  =\left\{
k-\alpha^{\prime}-\beta^{\prime}\ \mid\ k\in\mathbb{Z}\right\}
\]
(because we know that the eigenvalues of $L_{0}$ acting on $V_{\alpha,\beta}$
are $k-\alpha-\beta$ for all $k\in\mathbb{Z}$, while the eigenvalues of
$L_{0}$ acting on $V_{\alpha^{\prime},\beta^{\prime}}$ are $k-\alpha^{\prime
}-\beta^{\prime}$ for all $k\in\mathbb{Z}$). Hence, $\left(  \alpha
+\beta\right)  -\left(  \alpha^{\prime}+\beta^{\prime}\right)  \in\mathbb{Z}$.
Since we can shift $\alpha$ by an arbitrary integer without changing the
isomorphism class of $V_{\alpha,\beta}$ (due to part \textbf{(a)}), we can
thus WLOG assume that $\alpha+\beta=\alpha^{\prime}+\beta^{\prime}$.

Let us once again look at the equality (\ref{pf.Vab.iso.1}). This equality
tells us that, for each $k\in\mathbb{Z}$, the vector $v_{k}$ is the unique (up
to scaling) eigenvector of the operator $L_{0}$ with eigenvalue $k-\alpha
-\beta$ in $V_{\alpha,\beta}$. The isomorphism $\Phi$ (being
$\operatorname*{Vir}$-linear) must map this vector $v_{k}$ to an eigenvector
of the operator $L_{0}$ with eigenvalue $k-\alpha-\beta$ in $V_{\alpha
^{\prime},\beta^{\prime}}$. Since $\alpha+\beta=\alpha^{\prime}+\beta^{\prime
}$, this eigenvalue equals $k-\alpha^{\prime}-\beta^{\prime}$. But (due to
(\ref{pf.Vab.iso.2})) the unique (up to scaling) eigenvector of the operator
$L_{0}$ with eigenvalue $k-\alpha^{\prime}-\beta^{\prime}$ in $V_{\alpha
^{\prime},\beta^{\prime}}$ is $v_{k}$. Hence, $\Phi\left(  v_{k}\right)  $
must equal $v_{k}$ up to scaling, i. e., there exists a nonzero complex number
$\lambda_{k}$ such that $\Phi\left(  v_{k}\right)  =\lambda_{k}v_{k}$.

Now, let $m\in\mathbb{Z}$ and $k\in\mathbb{Z}$. Then, in $V_{\alpha,\beta}$,
we have%
\[
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m},
\]
so that%
\begin{align*}
\Phi\left(  L_{m}v_{k}\right)   &  =\Phi\left(  \left(  k-\alpha-\beta\left(
m+1\right)  \right)  v_{k-m}\right)  =\left(  k-\alpha-\beta\left(
m+1\right)  \right)  \underbrace{\Phi\left(  v_{k-m}\right)  }_{=\lambda
_{k-m}v_{k-m}}\\
&  =\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\end{align*}
in $V_{\alpha^{\prime},\beta^{\prime}}$. Compared with%
\begin{align*}
\Phi\left(  L_{m}v_{k}\right)   &  =L_{m}\underbrace{\Phi\left(  v_{k}\right)
}_{=\lambda_{k}v_{k}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Phi\text{ is
}\operatorname*{Vir}\text{-linear}\right) \\
&  =\lambda_{k}\underbrace{L_{m}v_{k}}_{=\left(  k-\alpha^{\prime}%
-\beta^{\prime}\left(  m+1\right)  \right)  v_{k-m}}=\lambda_{k}\left(
k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)  v_{k-m}%
\end{align*}
in $V_{\alpha^{\prime},\beta^{\prime}}$, this yields%
\[
\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
=\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)
\right)  v_{k-m}.
\]
Since $v_{k-m}\neq0$, this yields%
\begin{equation}
\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)  \right)  =\lambda
_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)  .
\label{pf.Vab.iso.5a}%
\end{equation}


Now, any $m\in\mathbb{Z}$, $k\in\mathbb{Z}$ and $n\in\mathbb{Z}$ satisfy%
\begin{equation}
\lambda_{k-\left(  n+m\right)  }\left(  k-\alpha-\beta\left(  m+1\right)
\right)  =\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
n+m+1\right)  \right)  \label{pf.Vab.iso.5b}%
\end{equation}
(by (\ref{pf.Vab.iso.5a}), applied to $n+m$ instead of $m$) and%
\begin{equation}
\lambda_{k-m-n}\left(  k-m-\alpha-\beta\left(  n+1\right)  \right)
=\lambda_{k-m}\left(  k-m-\alpha^{\prime}-\beta^{\prime}\left(  n+1\right)
\right)  \label{pf.Vab.iso.5c}%
\end{equation}
(by (\ref{pf.Vab.iso.5a}), applied to $k-m$ and $n$ instead of $k$ and $m$).
Hence, any $m\in\mathbb{Z}$, $k\in\mathbb{Z}$ and $n\in\mathbb{Z}$ satisfy
\begin{align*}
&  \lambda_{k}\lambda_{k-m}\lambda_{k-m-n}\cdot\left(  k-\alpha^{\prime}%
-\beta^{\prime}\left(  n+m+1\right)  \right)  \cdot\left(  k-\alpha
-\beta\left(  m+1\right)  \right)  \cdot\left(  k-m-\alpha-\beta\left(
n+1\right)  \right) \\
&  =\underbrace{\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
n+m+1\right)  \right)  }_{\substack{=\lambda_{k-\left(  n+m\right)  }\left(
k-\alpha-\beta\left(  m+1\right)  \right)  \\\text{(by (\ref{pf.Vab.iso.5b}%
))}}}\cdot\underbrace{\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)
\right)  }_{\substack{=\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime
}\left(  m+1\right)  \right)  \\\text{(by (\ref{pf.Vab.iso.5a}))}}%
}\cdot\underbrace{\lambda_{k-m-n}\left(  k-m-\alpha-\beta\left(  n+1\right)
\right)  }_{\substack{=\lambda_{k-m}\left(  k-m-\alpha^{\prime}-\beta^{\prime
}\left(  n+1\right)  \right)  \\\text{(by (\ref{pf.Vab.iso.5c}))}}}\\
&  =\lambda_{k-\left(  n+m\right)  }\left(  k-\alpha-\beta\left(  m+1\right)
\right)  \cdot\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
m+1\right)  \right)  \cdot\lambda_{k-m}\left(  k-m-\alpha^{\prime}%
-\beta^{\prime}\left(  n+1\right)  \right) \\
&  =\lambda_{k}\lambda_{k-m}\underbrace{\lambda_{k-\left(  n+m\right)  }%
}_{=\lambda_{k-m-n}}\cdot\left(  k-\alpha-\beta\left(  n+m+1\right)  \right)
\cdot\left(  k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)
\cdot\left(  k-m-\alpha^{\prime}-\beta^{\prime}\left(  n+1\right)  \right) \\
&  =\lambda_{k}\lambda_{k-m}\lambda_{k-m-n}\cdot\left(  k-\alpha-\beta\left(
n+m+1\right)  \right)  \cdot\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
m+1\right)  \right)  \cdot\left(  k-m-\alpha^{\prime}-\beta^{\prime}\left(
n+1\right)  \right)  .
\end{align*}
We can divide this equality by $\lambda_{k}\lambda_{k-m}\lambda_{k-m-n}$
(since $\lambda_{i}\neq0$ for every $i\in\mathbb{Z}$, and therefore we have
$\lambda_{k}\lambda_{k-m}\lambda_{k-m-n}\neq0$), and thus obtain that any
$m\in\mathbb{Z}$, $k\in\mathbb{Z}$ and $n\in\mathbb{Z}$ satisfy
\begin{align*}
&  \left(  k-\alpha^{\prime}-\beta^{\prime}\left(  n+m+1\right)  \right)
\cdot\left(  k-\alpha-\beta\left(  m+1\right)  \right)  \cdot\left(
k-m-\alpha-\beta\left(  n+1\right)  \right) \\
&  =\left(  k-\alpha-\beta\left(  n+m+1\right)  \right)  \cdot\left(
k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)  \cdot\left(
k-m-\alpha^{\prime}-\beta^{\prime}\left(  n+1\right)  \right)  .
\end{align*}
Since $\mathbb{Z}^{3}$ is Zariski-dense in $\mathbb{C}^{3}$, this yields that
\begin{align*}
&  \left(  X-\alpha^{\prime}-\beta^{\prime}\left(  Y+Z+1\right)  \right)
\cdot\left(  X-\alpha-\beta\left(  Z+1\right)  \right)  \cdot\left(
X-Z-\alpha-\beta\left(  Y+1\right)  \right) \\
&  =\left(  X-\alpha-\beta\left(  Y+Z+1\right)  \right)  \cdot\left(
X-\alpha^{\prime}-\beta^{\prime}\left(  Z+1\right)  \right)  \cdot\left(
X-Z-\alpha^{\prime}-\beta^{\prime}\left(  Y+1\right)  \right)  .
\end{align*}
holds as a polynomial identity in the polynomial ring $\mathbb{C}\left[
X,Y,Z\right]  $.

If we compare coefficients before $XYZ$ in this polynomial identity, we get an
equation which easily simplifies to $\left(  \beta-\beta^{\prime}\right)
\left(  \beta+\beta^{\prime}-1\right)  =0$. If we compare coefficients before
$YZ^{2}$ in the same identity, we similarly obtain $\beta\beta^{\prime}\left(
\beta-\beta^{\prime}\right)  =0$.

If $\beta=\beta^{\prime}$, then $\alpha=\alpha^{\prime}$ (since $\alpha
+\beta=\alpha^{\prime}+\beta^{\prime}$), and thus we are done. Hence, let us
assume that $\beta\neq\beta^{\prime}$ for the rest of this proof. Then,
$\left(  \beta-\beta^{\prime}\right)  \left(  \beta+\beta^{\prime}-1\right)
=0$ simplifies to $\beta+\beta^{\prime}-1=0$, and $\beta\beta^{\prime}\left(
\beta-\beta^{\prime}\right)  =0$ simplifies to $\beta\beta^{\prime}=0$.
Combining these two equations, we see that either $\left(  \beta=0\text{ and
}\beta^{\prime}=1\right)  $ or $\left(  \beta=1\text{ and }\beta^{\prime
}=0\right)  $. Assume WLOG that $\left(  \beta=0\text{ and }\beta^{\prime
}=1\right)  $ (otherwise, just switch $\left(  \alpha,\beta\right)  $ with
$\left(  \alpha^{\prime},\beta^{\prime}\right)  $). From $\alpha+\beta
=\alpha^{\prime}+\beta^{\prime}$, we obtain $\alpha-\alpha^{\prime
}=\underbrace{\beta^{\prime}}_{=1}-\underbrace{\beta}_{=0}=1\in\mathbb{Z}$. If
we are able to prove that $\alpha\notin\mathbb{Z}$, then we can conclude that
$\left(  \beta=0\text{, }\beta^{\prime}=1\text{, }\alpha-\alpha^{\prime}%
\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)  $, and thus we are
done. So let us show that $\alpha\notin\mathbb{Z}$.

In fact, assume the opposite. Then, $\alpha\in\mathbb{Z}$, so that $v_{\alpha
}$ is well-defined in $V_{\alpha,\beta}$ and in $V_{\alpha^{\prime}%
,\beta^{\prime}}$. Then, (\ref{ex1.1.2.var}) yields that every $m\in
\mathbb{Z}$ satisfies
\[
L_{m}v_{\alpha}=\left(  \underbrace{\alpha-\alpha}_{=0}-\underbrace{\beta
}_{=0}\left(  m+1\right)  \right)  v_{\alpha-m}=0\text{ in }V_{\alpha,\beta}.
\]
Thus, every $m\in\mathbb{Z}$ satisfies $\Phi\left(  L_{m}v_{\alpha}\right)
=\Phi\left(  0\right)  =0$, so that $0=\Phi\left(  L_{m}v_{\alpha}\right)
=L_{m}\underbrace{\Phi\left(  v_{\alpha}\right)  }_{=\lambda_{\alpha}%
v_{\alpha}}=\lambda_{\alpha}L_{m}v_{\alpha}$ in $V_{\alpha^{\prime}%
,\beta^{\prime}}$, and thus $0=L_{m}v_{\alpha}$ in $V_{\alpha^{\prime}%
,\beta^{\prime}}$ (since $\lambda_{\alpha}\neq0$). But since
(\ref{ex1.1.2.var}) yields%
\[
L_{m}v_{\alpha}=\left(  \alpha-\alpha^{\prime}-\underbrace{\beta^{\prime}%
}_{=1}\left(  m+1\right)  \right)  \underbrace{v_{\alpha-\alpha}}_{=v_{0}%
}=\left(  \alpha-\alpha^{\prime}-\left(  m+1\right)  \right)  v_{0}\text{ in
}V_{\alpha^{\prime},\beta^{\prime}},
\]
this rewrites as $0=\left(  \alpha-\alpha^{\prime}-\left(  m+1\right)
\right)  v_{0}$, so that $0=\alpha-\alpha^{\prime}-\left(  m+1\right)  $. But
this cannot hold for every $m\in\mathbb{Z}$. This contradiction shows that our
assumption (that $\alpha\in\mathbb{Z}$) was wrong. Thus, $\alpha
\notin\mathbb{Z}$, and our proof of the $\Longrightarrow$ direction is finally
done. Proposition \ref{prop.Vab.iso} \textbf{(c)} is finally proven.

Proving Proposition \ref{prop.Vab.iso} was one part of Homework Set 1 exercise
2; the other was the following:

\begin{proposition}
\label{prop.Vab.irr}Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Then,
the $\operatorname*{Vir}$-module $V_{\alpha,\beta}$ is not irreducible if and
only if $\left(  \alpha\in\mathbb{Z}\text{ and }\beta\in\left\{  0,1\right\}
\right)  $.
\end{proposition}

We will not prove this; the interested reader is referred to Proposition 1.1
in \S 1.2 of Kac-Raina.

\begin{remark}
\label{rmk.Vab.adj}Consider the $\operatorname*{Vir}$-module
$\operatorname*{Vir}$ (with the adjoint action). Since $\left\langle
C\right\rangle $ is a $\operatorname*{Vir}$-submodule of $\operatorname*{Vir}%
$, we obtain a $\operatorname*{Vir}$-module $\operatorname*{Vir}%
\diagup\left\langle C\right\rangle $. This $\operatorname*{Vir}$-module is
isomorphic to $V_{1,-1}$. More precisely, the $\mathbb{C}$-linear map%
\begin{align*}
\operatorname*{Vir}\diagup\left\langle C\right\rangle  &  \rightarrow
V_{1,-1},\\
\overline{L_{n}}  &  \mapsto v_{-n}%
\end{align*}
is a $\operatorname*{Vir}$-module isomorphism. Thus, $\operatorname*{Vir}%
\diagup\left\langle C\right\rangle \cong V_{1,-1}\cong V_{\alpha,-1}$ as
$\operatorname*{Vir}$-modules for every $\alpha\in\mathbb{Z}$ (because of
Proposition \ref{prop.Vab.iso} \textbf{(a)}).
\end{remark}

\subsection{Some consequences of Poincar\'{e}-Birkhoff-Witt}

We will now spend some time with generalities on Lie algebras and their
universal enveloping algebras. These generalities will be applied later, and
while these applications could be substituted by concrete computations, it
appears to me that it is better for the sake of clarity to do them generally
in here.

\begin{proposition}
\label{prop.U(X)U}Let $k$ be a field. Let $\mathfrak{c}$ be a $k$-Lie algebra.
Let $\mathfrak{a}$ and $\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$
such that $\mathfrak{a}+\mathfrak{b}=\mathfrak{c}$. Notice that $\mathfrak{a}%
\cap\mathfrak{b}$ is also a Lie subalgebra of $\mathfrak{c}$.

Let $\rho:U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \rightarrow U\left(
\mathfrak{c}\right)  $ be the $k$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
U\left(  \mathfrak{a}\right)  \text{ and }\beta\in U\left(  \mathfrak{b}%
\right)
\]
(this is clearly well-defined). Then, $\rho$ is an isomorphism of filtered
vector spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and of right
$U\left(  \mathfrak{b}\right)  $-modules.
\end{proposition}

\begin{corollary}
\label{cor.U(X)U}Let $k$ be a field. Let $\mathfrak{c}$ be a $k$-Lie algebra.
Let $\mathfrak{a}$ and $\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$
such that $\mathfrak{a}\oplus\mathfrak{b}=\mathfrak{c}$ (as vector spaces, not
necessarily as Lie algebras). Let $\rho:U\left(  \mathfrak{a}\right)
\otimes_{k}U\left(  \mathfrak{b}\right)  \rightarrow U\left(  \mathfrak{c}%
\right)  $ be the $k$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \mathfrak{a}\right)
\text{ and }\beta\in U\left(  \mathfrak{b}\right)
\]
(this is clearly well-defined). Then, $\rho$ is an isomorphism of filtered
vector spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and of right
$U\left(  \mathfrak{b}\right)  $-modules.
\end{corollary}

We give two proofs of Proposition \ref{prop.U(X)U}. They are very similar
(both use the Poincar\'{e}-Birkhoff-Witt theorem, albeit different versions
thereof). The first is more conceptual (and more general), while the second is
more down-to-earth.

\textit{First proof of Proposition \ref{prop.U(X)U}.} For any Lie algebra
$\mathfrak{u}$, we have a $k$-algebra homomorphism $\operatorname*{PBW}%
\nolimits_{\mathfrak{u}}:S\left(  \mathfrak{u}\right)  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{u}\right)  \right)  $ which sends
$u_{1}u_{2}...u_{\ell}$ to $\overline{u_{1}u_{2}...u_{\ell}}\in
\operatorname*{gr}\nolimits_{\ell}\left(  U\left(  \mathfrak{u}\right)
\right)  $ for every $\ell\in\mathbb{N}$ and every $u_{1},u_{2},...,u_{\ell
}\in\mathfrak{u}$. This homomorphism $\operatorname*{PBW}%
\nolimits_{\mathfrak{u}}$ is an isomorphism due to the
Poincar\'{e}-Birkhoff-Witt theorem.

\begin{verlong}
It is rather clear that $\operatorname*{gr}\left(  U\left(  \mathfrak{a}%
\right)  \right)  $ and $\operatorname*{gr}\left(  U\left(  \mathfrak{b}%
\right)  \right)  $ are $\operatorname*{gr}\left(  U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  \right)  $-modules (since $U\left(  \mathfrak{a}%
\right)  $ and $U\left(  \mathfrak{b}\right)  $ are filtered $U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  $-modules)
\end{verlong}

We can define a $k$-algebra homomorphism $f:\operatorname*{gr}\left(  U\left(
\mathfrak{a}\right)  \right)  \otimes_{\operatorname*{gr}\left(  U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \right)  }\operatorname*{gr}\left(
U\left(  \mathfrak{b}\right)  \right)  \rightarrow\operatorname*{gr}\left(
U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \right)  $ by
\[
f\left(  \overline{u}\otimes_{\operatorname*{gr}\left(  U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \right)  }\overline{v}\right)
=\overline{u\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }v}%
\in\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)
\]
for any $k\in\mathbb{N}$, any $\ell\in\mathbb{N}$, any $u\in U_{\leq k}\left(
\mathfrak{a}\right)  $ and $v\in U_{\leq\ell}\left(  \mathfrak{b}\right)  $.
This $f$ is easily seen to be well-defined. Moreover, $f$ is
surjective\footnote{To show this, either notice that the image of $f$ contains
a generating set of $\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)  $ (because the definition of $f$ easily rewrites
as
\par%
\[
f\left(  \overline{\alpha_{1}\alpha_{2}...\alpha_{k}}\otimes
_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
\right)  }\overline{\beta_{1}\beta_{2}...\beta_{\ell}}\right)  =\overline
{\alpha_{1}\alpha_{2}...\alpha_{k}\otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\beta_{1}\beta_{2}...\beta_{\ell}}\in
\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)
\]
for any $k\in\mathbb{N}$, any $\ell\in\mathbb{N}$, any $\alpha_{1},\alpha
_{2},...,\alpha_{k}\in\mathfrak{a}$ and $\beta_{1},\beta_{2},...,\beta_{\ell
}\in\mathfrak{b}$), or prove the more general fact that for any $\mathbb{Z}%
_{+}$-filtered algebra $A$, any filtered right $A$-module $M$ and any filtered
left $A$-module $N$, the canonical map%
\begin{align*}
\operatorname*{gr}\left(  M\right)  \otimes_{\operatorname*{gr}\left(
A\right)  }\operatorname*{gr}\left(  N\right)   &  \rightarrow
\operatorname*{gr}\left(  M\otimes_{A}N\right)  ,\\
\overline{\mu}\otimes_{\operatorname*{gr}\left(  A\right)  }\overline{\nu}  &
\mapsto\overline{\mu\otimes_{A}\nu}\in\operatorname*{gr}\nolimits_{m+n}\left(
M\otimes_{A}N\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{for all }\mu\in
M_{m}\text{ and }\nu\in N_{n}\text{, for all }m,n\in\mathbb{N}\right)
\end{align*}
is well-defined and surjective (this is easy to prove).}.

It is easy to see that the isomorphisms $\operatorname*{PBW}%
\nolimits_{\mathfrak{a}}:S\left(  \mathfrak{a}\right)  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)  \right)  $,
$\operatorname*{PBW}\nolimits_{\mathfrak{b}}:S\left(  \mathfrak{b}\right)
\rightarrow\operatorname*{gr}\left(  U\left(  \mathfrak{b}\right)  \right)  $
and $\operatorname*{PBW}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}:S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \rightarrow\operatorname*{gr}\left(
U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \right)  $ are ``compatible''
with each other in the sense that the diagrams%
\[%
%TCIMACRO{\TeXButton{X}{\xycs{12pc}
%\xymatrix{
%S\left(\fraka\right) \otimes S\left(\fraka\cap\frakb\right) \ar[r]^-{\text
%{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\fraka\right)}
%\ar[d]_{\PBW_{\fraka}\otimes\PBW_{\fraka\cap\frakb}}^{\cong} & S\left
%(\fraka\right) \ar[d]_{\PBW_{\fraka}}^{\cong} \\
%\gr\left(U\left(\fraka\right)\right) \otimes\gr\left(U\left(\fraka\cap
%\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
%\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\fraka\right)\right)}
%& \gr\left(U\left(\fraka\right)\right)
%}}}%
%BeginExpansion
\xycs{12pc}
\xymatrix{
S\left(\fraka\right) \otimes S\left(\fraka\cap\frakb\right) \ar[r]^-{\text
{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\fraka\right)}
\ar[d]_{\PBW_{\fraka}\otimes\PBW_{\fraka\cap\frakb}}^{\cong} & S\left
(\fraka\right) \ar[d]_{\PBW_{\fraka}}^{\cong} \\
\gr\left(U\left(\fraka\right)\right) \otimes\gr\left(U\left(\fraka\cap
\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\fraka\right)\right)}
& \gr\left(U\left(\fraka\right)\right)
}%
%EndExpansion
\]
and%
\[%
%TCIMACRO{\TeXButton{X}{\xycs{12pc}
%\xymatrix{
%S\left(\fraka\cap\frakb\right) \otimes S\left(\frakb\right) \ar[r]^-{\text
%{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\frakb\right)}
%\ar[d]_{\PBW_{\fraka\cap\frakb}\otimes\PBW_{\frakb}}^{\cong} & S\left
%(\frakb\right) \ar[d]_{\PBW_{\frakb}}^{\cong} \\
%\gr\left(U\left(\fraka\cap\frakb\right)\right) \otimes\gr\left(U\left
%(\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
%\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\frakb\right)\right)}
%& \gr\left(U\left(\frakb\right)\right)
%}}}%
%BeginExpansion
\xycs{12pc}
\xymatrix{
S\left(\fraka\cap\frakb\right) \otimes S\left(\frakb\right) \ar[r]^-{\text
{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\frakb\right)}
\ar[d]_{\PBW_{\fraka\cap\frakb}\otimes\PBW_{\frakb}}^{\cong} & S\left
(\frakb\right) \ar[d]_{\PBW_{\frakb}}^{\cong} \\
\gr\left(U\left(\fraka\cap\frakb\right)\right) \otimes\gr\left(U\left
(\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\frakb\right)\right)}
& \gr\left(U\left(\frakb\right)\right)
}%
%EndExpansion
\]
commute\footnote{This is pretty easy to see from the definition of
$\operatorname*{PBW}\nolimits_{\mathfrak{u}}$.}. Hence, they give rise to an
isomorphism%
\begin{align*}
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)   &  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)  \right)
\otimes_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  \right)  }\operatorname*{gr}\left(  U\left(  \mathfrak{b}\right)
\right)  ,\\
\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }\beta &
\mapsto\left(  \operatorname*{PBW}\nolimits_{\mathfrak{a}}\alpha\right)
\otimes_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  \right)  }\left(  \operatorname*{PBW}\nolimits_{\mathfrak{b}}%
\beta\right)  .
\end{align*}
Denote this isomorphism by $\left(  \operatorname*{PBW}\nolimits_{\mathfrak{a}%
}\right)  \otimes_{\operatorname*{PBW}\nolimits_{\mathfrak{a}\cap\mathfrak{b}%
}}\left(  \operatorname*{PBW}\nolimits_{\mathfrak{b}}\right)  $.

Finally, let $\sigma:S\left(  \mathfrak{a}\right)  \otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)
\rightarrow S\left(  \mathfrak{c}\right)  $ be the vector space homomorphism
defined by%
\[
\sigma\left(  \alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
S\left(  \mathfrak{a}\right)  \text{ and }\beta\in S\left(  \mathfrak{b}%
\right)  .
\]
This $\sigma$ is rather obviously an algebra homomorphism. Now, it is easy to
see that $\sigma$ is an algebra isomorphism\footnote{\textit{First proof that
}$\sigma$\textit{ is an algebra isomorphism:} Since every subspace of a vector
space has a complementary subspace, we can find a $k$-vector subspace
$\mathfrak{d}$ of $\mathfrak{a}$ such that $\mathfrak{a}=\mathfrak{d}%
\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $. Consider such a
$\mathfrak{d}$.
\par
Since $\mathfrak{a}=\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  =\mathfrak{d}+\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $, the
fact that $\mathfrak{c}=\mathfrak{a}+\mathfrak{b}$ rewrites as $\mathfrak{c}%
=\mathfrak{d}+\underbrace{\left(  \mathfrak{a}\cap\mathfrak{b}\right)
+\mathfrak{b}}_{\substack{=\mathfrak{b}\\\text{(since }\mathfrak{a}%
\cap\mathfrak{b}\subseteq\mathfrak{b}\text{)}}}=\mathfrak{d}+\mathfrak{b}$.
Combined with $\underbrace{\mathfrak{d}}_{\substack{=\mathfrak{d}%
\cap\mathfrak{a}\\\text{(since }\mathfrak{d}\subseteq\mathfrak{a}\text{)}%
}}\cap\mathfrak{b}\subseteq\mathfrak{d}\cap\mathfrak{a}\cap\mathfrak{b}=0$
(since $\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $ is a
well-defined internal direct sum), this yields $\mathfrak{c}=\mathfrak{d}%
\oplus\mathfrak{b}$.
\par
Recall a known fact from multilinear algebra: Any two $k$-vector spaces $U$
and $V$ satisfy $S\left(  U\oplus V\right)  \cong S\left(  U\right)
\otimes_{k}S\left(  V\right)  $ by the canonical algebra isomorphism. Hence,
$S\left(  \mathfrak{d}\oplus\mathfrak{b}\right)  \cong S\left(  \mathfrak{d}%
\right)  \otimes_{k}S\left(  \mathfrak{b}\right)  $.
\par
But $\mathfrak{a}=\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  $ yields $S\left(  \mathfrak{a}\right)  =S\left(  \mathfrak{d}%
\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \right)  \cong S\left(
\mathfrak{d}\right)  \otimes_{k}S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
$ (by the above-quoted fact that any two $k$-vector spaces $U$ and $V$ satisfy
$S\left(  U\oplus V\right)  \cong S\left(  U\right)  \otimes_{k}S\left(
V\right)  $ by the canonical algebra isomorphism). Hence,%
\begin{align*}
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)   &  \cong\left(  S\left(
\mathfrak{d}\right)  \otimes_{k}S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
\right)  \otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right) \\
&  \cong S\left(  \mathfrak{d}\right)  \otimes_{k}\underbrace{\left(  S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  \right)  }_{\cong
S\left(  \mathfrak{b}\right)  }\cong S\left(  \mathfrak{d}\right)  \otimes
_{k}S\left(  \mathfrak{b}\right)  \cong S\left(  \underbrace{\mathfrak{d}%
\oplus\mathfrak{b}}_{=\mathfrak{c}}\right)  =S\left(  \mathfrak{c}\right)  .
\end{align*}
Thus we have constructed an algebra isomorphism $S\left(  \mathfrak{a}\right)
\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right)  \rightarrow S\left(  \mathfrak{c}\right)  $. If we track
down what happens to elements of $\mathfrak{d}$, $\mathfrak{a}\cap
\mathfrak{b}$ and $\mathfrak{b}$ under this isomorphism, we notice that they
just get sent to themselves, so this isomorphism must coincide with $\sigma$
(because if two algebra homomorphisms from the same algebra coincide on a set
of generators of said algebra, then these two algebra homomorphisms must be
identical). Thus, $\sigma$ is an algebra isomorphism, qed.
\par
\textit{Second proof that }$\sigma$\textit{ is an algebra isomorphism:} Define
a map $\tau:\mathfrak{c}\rightarrow S\left(  \mathfrak{a}\right)
\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right)  $ as follows: For every $c\in\mathfrak{c}$, let
$\tau\left(  c\right)  $ be $a\otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }b$, where we have written $c$ in the form $c=a+b$ with
$a\in\mathfrak{a}$ and $b\in\mathfrak{b}$ (in fact, we can write $c$ this way,
because $\mathfrak{c}=\mathfrak{a}+\mathfrak{b}$). This map $\tau$ is
well-defined, because the value of $a\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }b$ depends only on $c$ and not on the exact values of $a$ and $b$ in
the decomposition $c=a+b$. (In fact, if $c=a+b$ and $c=a^{\prime}+b^{\prime}$
are two different ways to decompose $c$ into a sum of an element of
$\mathfrak{a}$ with an element of $\mathfrak{b}$, then $a+b=c=a^{\prime
}+b^{\prime}$, so that $a-a^{\prime}=b^{\prime}-b$, thus $a-a^{\prime}%
\in\mathfrak{a}\cap\mathfrak{b}$ (because $a-a^{\prime}\in\mathfrak{a}$ and
$a-a^{\prime}=b^{\prime}-b\in\mathfrak{b}$), so that%
\begin{align*}
&  \underbrace{a}_{=a^{\prime}+\left(  a-a^{\prime}\right)  }\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }b\\
&  =\left(  a^{\prime}+\left(  a-a^{\prime}\right)  \right)  \otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }b\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+\underbrace{\left(  a-a^{\prime}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }1}_{\substack{=1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\left(  a-a^{\prime}\right)  \\\text{(since
}a-a^{\prime}\in\mathfrak{a}\cap\mathfrak{b}\subseteq S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  \text{)}}}+1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }b\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
\underbrace{\left(  a-a^{\prime}\right)  }_{=b^{\prime}-b}+1\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }b\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+\underbrace{1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\left(  b^{\prime}-b\right)  +1\otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }b}_{=1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\left(  \left(  b^{\prime}-b\right)  +b\right)  }\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
\underbrace{\left(  \left(  b^{\prime}-b\right)  +b\right)  }_{=b^{\prime}%
}=a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }b^{\prime}.
\end{align*}
)
\par
It is also easy to see that $\tau$ is a linear map. Thus, by the universal
property of the symmetric algebra, the map $\tau:\mathfrak{c}\rightarrow
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  $ gives rise to a
$k$-algebra homomorphism $\widehat{\tau}:S\left(  \mathfrak{c}\right)
\rightarrow S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  $ that lifts $\tau$.
\par
Any $\alpha\in\mathfrak{a}$ satisfies%
\begin{align*}
\left(  \widehat{\tau}\circ\sigma\right)  \left(  \alpha\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }1\right)   &  =\widehat{\tau}\left(
\underbrace{\sigma\left(  \alpha\otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }1\right)  }_{\substack{=\alpha1\\\text{(by the
definition of }\sigma\text{)}}}\right)  =\widehat{\tau}\left(  \alpha1\right)
=\widehat{\tau}\left(  \alpha\right)  =\tau\left(  \alpha\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widehat{\tau}\text{ lifts }%
\tau\right) \\
&  =\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }0\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\tau\text{, since }\alpha=\alpha+0\text{ is a
decomposition of}\\
\text{ }\alpha\text{ into a sum of an element of }\mathfrak{a}\text{ with an
element of }\mathfrak{b}%
\end{array}
\right) \\
&  =\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }1.
\end{align*}
In other words, the map $\widehat{\tau}\circ\sigma$ fixes all tensors of the
form $\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }1$ with
$\alpha\in\mathfrak{a}$. Similarly, the map $\widehat{\tau}\circ\sigma$ fixes
all tensors of the form $1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\beta$ with $\beta\in\mathfrak{b}$. Combining the previous two
sentences, we conclude that the map map $\widehat{\tau}\circ\sigma$ fixes all
elements of the set $\left\{  \alpha\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }1\ \mid\ \alpha\in\mathfrak{a}\right\}  \cup\left\{
1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }\beta\ \mid\ \beta
\in\mathfrak{b}\right\}  $. Thus, there is a generating set of the $k$-algebra
$S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  $ such that the map
$\widehat{\tau}\circ\sigma$ fixes all elements of this set (because $\left\{
\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }1\ \mid
\ \alpha\in\mathfrak{a}\right\}  \cup\left\{  1\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }\beta\ \mid\ \beta\in\mathfrak{b}%
\right\}  $ is a generating set of the $k$-algebra $S\left(  \mathfrak{a}%
\right)  \otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right)  $). Since this map $\widehat{\tau}\circ\sigma$ is a
$k$-algebra homomorphism (because $\widehat{\tau}$ and $\sigma$ are
$k$-algebra homomorphisms), this yields that the map $\widehat{\tau}%
\circ\sigma$ is the identity (since a $k$-algebra homomorphism which fixes a
generating set of its domain must be the identity). In other words, we have
shown that $\widehat{\tau}\circ\sigma=\operatorname*{id}$. A slightly
different but similarly simple argument shows that $\sigma\circ\widehat{\tau
}=\operatorname*{id}$. Combining $\sigma\circ\widehat{\tau}=\operatorname*{id}%
$ with $\widehat{\tau}\circ\sigma=\operatorname*{id}$, we conclude that
$\widehat{\tau}$ is an inverse to $\sigma$, so that $\sigma$ is an algebra
isomorphism, qed.}.

Now, it is easy to see (by elementwise checking) that the diagram%
\[%
%TCIMACRO{\TeXButton{X}{\xymatrixcolsep{11pc}
%\xymatrix{
%\gr\left(U\left(\fraka\right)\right) \otimes_{\gr\left(U\left(\fraka\cap
%\frakb\right)\right)} \gr\left(U\left(\frakb\right)\right) \ar[d]^{f}
%& S\left(\fraka\right) \otimes_{S\left(\fraka\cap\frakb\right)} S\left
%(\frakb\right) \ar[l]_-{\left(\PBW_{\fraka}\right) \otimes_{\PBW_{\fraka
%\cap\frakb}} \left(\PBW_{\frakb}\right)}^{\cong} \ar[d]_{\cong}^{\sigma} \\
%\gr\left(U\left(\fraka\right) \otimes_{U\left(\fraka\cap\frakb\right)}
%U\left(\frakb\right)\right) \ar[dr]_{\gr\rho} & S\left(\frakc\right
%) \ar[d]^{\PBW_{\frakc}}_{\cong} \\
%& \gr\left(U\left(\frakc\right)\right)
%}}}%
%BeginExpansion
\xymatrixcolsep{11pc}
\xymatrix{
\gr\left(U\left(\fraka\right)\right) \otimes_{\gr\left(U\left(\fraka\cap
\frakb\right)\right)} \gr\left(U\left(\frakb\right)\right) \ar[d]^{f}
& S\left(\fraka\right) \otimes_{S\left(\fraka\cap\frakb\right)} S\left
(\frakb\right) \ar[l]_-{\left(\PBW_{\fraka}\right) \otimes_{\PBW_{\fraka
\cap\frakb}} \left(\PBW_{\frakb}\right)}^{\cong} \ar[d]_{\cong}^{\sigma} \\
\gr\left(U\left(\fraka\right) \otimes_{U\left(\fraka\cap\frakb\right)}
U\left(\frakb\right)\right) \ar[dr]_{\gr\rho} & S\left(\frakc\right
) \ar[d]^{\PBW_{\frakc}}_{\cong} \\
& \gr\left(U\left(\frakc\right)\right)
}%
%EndExpansion
\]
is commutative.\footnote{In fact, if we follow the pure tensor $\alpha
_{1}\alpha_{2}...\alpha_{k}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\beta_{1}\beta_{2}...\beta_{\ell}$ (with $k\in\mathbb{N}$, $\ell
\in\mathbb{N}$, $\alpha_{1},\alpha_{2},...,\alpha_{k}\in\mathfrak{a}$ and
$\beta_{1},\beta_{2},...,\beta_{\ell}\in\mathfrak{b}$) through this diagram,
we get $\overline{\alpha_{1}\alpha_{2}...\alpha_{k}\beta_{1}\beta_{2}%
...\beta_{\ell}}\in\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(
\mathfrak{c}\right)  \right)  $ both ways.} Hence, $\left(  \operatorname*{gr}%
\rho\right)  \circ f$ is an isomorphism, so that $f$ is injective. Since $f$
is also surjective, this yields that $f$ is an isomorphism. Thus,
$\operatorname*{gr}\rho$ is an isomorphism (since $\left(  \operatorname*{gr}%
\rho\right)  \circ f$ is an isomorphism). Since $\rho$ is a filtered map and
$\operatorname*{gr}\rho$ is an isomorphism, it follows that $\rho$ is an
isomorphism of filtered vector spaces. Hence, $\rho$ is an isomorphism of
filtered vector spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and
of right $U\left(  \mathfrak{b}\right)  $-modules (since it is clear that
$\rho$ is a homomorphism of $U\left(  \mathfrak{a}\right)  $-left modules and
of $U\left(  \mathfrak{b}\right)  $-right modules). This proves Proposition
\ref{prop.U(X)U}.

\textit{Second proof of Proposition \ref{prop.U(X)U}.} Let $\left(
z_{i}\right)  _{i\in I}$ be a basis of the $k$-vector space $\mathfrak{a}%
\cap\mathfrak{b}$. We extend this basis to a basis $\left(  z_{i}\right)
_{i\in I}\cup\left(  x_{j}\right)  _{j\in J}$ of the $k$-vector space
$\mathfrak{a}$ and to a basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
y_{\ell}\right)  _{\ell\in L}$ of the $k$-vector space $\mathfrak{b}$. Then,
$\left(  z_{i}\right)  _{i\in I}\cup\left(  x_{j}\right)  _{j\in J}\cup\left(
y_{\ell}\right)  _{\ell\in L}$ is a basis of the $k$-vector space
$\mathfrak{c}$. We endow this basis with a total ordering in such a way that
every $x_{j}$ is smaller than every $z_{i}$, and that every $z_{i}$ is smaller
than every $y_{\ell}$. By the Poincar\'{e}-Birkhoff-Witt theorem, we have a
basis of $U\left(  \mathfrak{c}\right)  $ consisting of increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}\cup\left(  y_{\ell}\right)  _{\ell\in L}$. On the
other hand, again by the Poincar\'{e}-Birkhoff-Witt theorem, we have a basis
of $U\left(  \mathfrak{a}\right)  $ consisting of increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}$. Note that the $z_{i}$ accumulate at the right end of
these products, while the $x_{j}$ accumulate at the left end (because we
defined the total ordering in such a way that every $x_{j}$ is smaller than
every $z_{i}$). Hence, $U\left(  \mathfrak{a}\right)  $ is a free right
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $-module, with a basis (over
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $, not over $k$) consisting of
increasing products of elements of the basis $\left(  x_{j}\right)  _{j\in J}%
$. Combined with the fact that $U\left(  \mathfrak{b}\right)  $ is a free
$k$-vector space with a basis consisting of increasing products of elements of
the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(  y_{\ell}\right)
_{\ell\in L}$ (again by Poincar\'{e}-Birkhoff-Witt), this yields that
$U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  $ is a free $k$-vector
space with a basis consisting of tensors of the form%
\begin{align*}
&  \left(  \text{some increasing product of elements of the basis }\left(
x_{j}\right)  _{j\in J}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\left(  \text{some increasing product of elements of the basis }\left(
z_{i}\right)  _{i\in I}\cup\left(  y_{\ell}\right)  _{\ell\in L}\right)  .
\end{align*}
The map $\rho$ clearly maps such terms bijectively into increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}\cup\left(  y_{\ell}\right)  _{\ell\in L}$. Hence,
$\rho$ maps a basis of $U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  $
bijectively to a basis of $U\left(  \mathfrak{c}\right)  $. Thus, $\rho$ is an
isomorphism of vector spaces. Moreover, since both of our bases were
filtered\footnote{A basis $\mathcal{B}$ of a filtered vector space $V$ is said
to be \textit{filtered} if for every $n\in\mathbb{N}$, the subfamily of
$\mathcal{B}$ consisting of those elements of $\mathcal{B}$ lying in the
$n$-th filtration of $V$ is a basis of the $n$-th filtration of $V$.}, and
$\rho$ respects this filtration on the bases, we can even conclude that $\rho$
is an isomorphism of filtered vector spaces. Since it is clear that $\rho$ is
a homomorphism of $U\left(  \mathfrak{a}\right)  $-left modules and of
$U\left(  \mathfrak{b}\right)  $-right modules, it follows that $\rho$ is an
isomorphism of filtered vector spaces, of left $U\left(  \mathfrak{a}\right)
$-modules and of right $U\left(  \mathfrak{b}\right)  $-modules. This proves
Proposition \ref{prop.U(X)U}.

\textit{Proof of Corollary \ref{cor.U(X)U}.} Corollary \ref{cor.U(X)U}
immediately follows from Proposition \ref{prop.U(X)U} (since $\mathfrak{a}%
\oplus\mathfrak{b}=\mathfrak{c}$ yields $\mathfrak{a}\cap\mathfrak{b}=0$, thus
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  =U\left(  0\right)  =k$).

\begin{remark}
\label{rmk.U(X)U}While we have required $k$ to be a field in Proposition
\ref{prop.U(X)U} and Corollary \ref{cor.U(X)U}, these two results hold in more
general situations as well. For instance, Proposition \ref{prop.U(X)U} holds
whenever $k$ is a commutative ring, as long as $\mathfrak{a}$, $\mathfrak{b}$
and $\mathfrak{a}\cap\mathfrak{b}$ are free $k$-modules, and $\mathfrak{a}%
\cap\mathfrak{b}$ is a direct summand of $\mathfrak{a}$ as a $k$-module. In
fact, the first proof of Proposition \ref{prop.U(X)U} works in this situation
(because the Poincar\'{e}-Birkhoff-Witt theorem holds for free modules). In a
more restrictive situation (namely, when $\mathfrak{a}\cap\mathfrak{b}$ is a
free $k$-module, and a direct summand of each of $\mathfrak{a}$ and
$\mathfrak{b}$, with the other two summands also being free), the second proof
of Proposition \ref{prop.U(X)U} works as well. As for Corollary
\ref{cor.U(X)U}, it holds whenever $k$ is a commutative ring, as long as
$\mathfrak{a}$ and $\mathfrak{b}$ are free $k$-modules.

This generality is more than enough for most applications of Proposition
\ref{prop.U(X)U} and Corollary \ref{cor.U(X)U}. Yet we can go even further
using the appropriate generalizations of the Poincar\'{e}-Birkhoff-Witt
theorem (for these, see, e. g., P. J. Higgins, \textit{Baer Invariants and the
Birkhoff-Witt theorem}, J. of Alg. 11, pp. 469-482, (1969), \newline%
\texttt{\url{http://www.sciencedirect.com/science/article/pii/0021869369900866}
}).
\end{remark}

\subsection{\texorpdfstring{$\mathbb{Z}$}{Z}-graded Lie algebras and Verma
modules}

\subsubsection{\texorpdfstring{$\mathbb{Z}$}{Z}-graded Lie algebras}

Let us show some general results about representations of $\mathbb{Z}$-graded
Lie algebras -- particularly of \textit{nondegenerate} $\mathbb{Z}$-graded Lie
algebras. This is a notion that encompasses many of the concrete Lie algebras
that we want to study (among others, $\mathcal{A}$, $\mathcal{A}_{0}$, $W$ and
$\operatorname*{Vir}$), and thus by proving the properties of nondegenerate
$\mathbb{Z}$-graded Lie algebras now we can avoid proving them separately in
many different cases.

\begin{definition}
\label{def.gradLie}A $\mathbb{Z}$\textit{-graded Lie algebra} is a Lie algebra
$\mathfrak{g}$ with a decomposition $\mathfrak{g}=\bigoplus\limits_{n\in
\mathbb{Z}}\mathfrak{g}_{n}$ (as a vector space) such that $\left[
\mathfrak{g}_{n},\mathfrak{g}_{m}\right]  \subseteq\mathfrak{g}_{n+m}$ for all
$n,m\in\mathbb{Z}$. The family $\left(  \mathfrak{g}_{n}\right)
_{n\in\mathbb{Z}}$ is called the \textit{grading} of this $\mathbb{Z}$-graded
Lie algebra.\footnotemark
\end{definition}

\footnotetext{\textbf{Warning:} Some algebraists use the words ``$\mathbb{Z}%
$-graded Lie algebra'' to denote a $\mathbb{Z}$-graded Lie \textbf{super}%
algebra, where the even homogeneous components constitute the even part and
the odd homogeneous components constitute the odd part. This is \textbf{not}
how we understand the notion of a ``$\mathbb{Z}$-graded Lie algebra'' here. In
particular, for us, a $\mathbb{Z}$-graded Lie algebra $\mathfrak{g}$ should
satisfy $\left[  x,x\right]  =0$ for all $x\in\mathfrak{g}$ (not just for $x$
lying in even homogeneous components).} Of course, every $\mathbb{Z}$-graded
Lie algebra automatically is a $\mathbb{Z}$-graded vector space (by way of
forgetting the Lie bracket and only keeping the grading). Note that if
$\mathfrak{g}=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}$ is a
$\mathbb{Z}$-graded Lie algebra, then $\bigoplus\limits_{n<0}\mathfrak{g}_{n}%
$, $\mathfrak{g}_{0}$ and $\bigoplus\limits_{n>0}\mathfrak{g}_{n}$ are Lie
subalgebras of $\mathfrak{g}$.

\begin{example}
We defined a grading on the Heisenberg algebra $\mathcal{A}$ in Definition
\ref{def.A.grad}. This makes $\mathcal{A}$ into a $\mathbb{Z}$-graded Lie
algebra. Also, $\mathcal{A}_{0}$ is a $\mathbb{Z}$-graded Lie subalgebra of
$\mathcal{A}$.
\end{example}

\begin{example}
We make the Witt algebra $W$ into a $\mathbb{Z}$-graded Lie algebra by using
the grading $\left(  W\left[  n\right]  \right)  _{n\in\mathbb{Z}}$, where
$W\left[  n\right]  =\left\langle L_{n}\right\rangle $ for every
$n\in\mathbb{Z}$.

We make the Virasoro algebra $\operatorname*{Vir}$ into a $\mathbb{Z}$-graded
Lie algebra by using the grading $\left(  \operatorname*{Vir}\left[  n\right]
\right)  _{n\in\mathbb{Z}}$, where $\operatorname*{Vir}\left[  n\right]
=\left\{
\begin{array}
[c]{l}%
\left\langle L_{n}\right\rangle ,\ \ \ \ \ \ \ \ \ \ \text{if }n\neq0;\\
\left\langle L_{0},C\right\rangle ,\ \ \ \ \ \ \ \ \ \ \text{if }n=0
\end{array}
\right.  $ for every $n\in\mathbb{Z}$.
\end{example}

\begin{definition}
\label{def.gradLienondeg}A $\mathbb{Z}$-graded Lie algebra $\mathfrak{g}%
=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}$ is said to be
\textit{nondegenerate} if

\textbf{(1)} the vector space $\mathfrak{g}_{n}$ is finite-dimensional for
every $n\in\mathbb{Z}$;

\textbf{(2)} the Lie algebra $\mathfrak{g}_{0}$ is abelian;

\textbf{(3)} for every positive integer $n$, for generic $\lambda
\in\mathfrak{g}_{0}^{\ast}$, the bilinear form $\mathfrak{g}_{n}%
\times\mathfrak{g}_{-n}\rightarrow\mathbb{C},\ \left(  a,b\right)
\mapsto\lambda\left(  \left[  a,b\right]  \right)  $ is nondegenerate.
(``Generic $\lambda$'' means ``$\lambda$ lying in some dense open subset of
$\mathfrak{g}_{0}^{\ast}$ with respect to the Zariski topology''. This subset
can depend on $n$.)
\end{definition}

Note that condition \textbf{(3)} in Definition \ref{def.gradLienondeg} implies
that $\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}%
_{-n}\right)  $ for all $n\in\mathbb{Z}$.

Here are some examples:

\begin{proposition}
The $\mathbb{Z}$-graded Lie algebras $\mathcal{A}$, $\mathcal{A}_{0}$, $W$ and
$\operatorname*{Vir}$ are nondegenerate (with the gradings defined above).
\end{proposition}

\begin{proposition}
\label{prop.grad.g}Let $\mathfrak{g}$ be a finite-dimensional simple Lie
algebra. The following is a reasonable (although non-canonical) way to define
a grading on $\mathfrak{g}$:

Using a Cartan subalgebra and the roots of $\mathfrak{g}$, we can present the
Lie algebra $\mathfrak{g}$ as a Lie algebra with generators $e_{1}$, $e_{2}$,
$...$, $e_{m}$, $f_{1}$, $f_{2}$, $...$, $f_{m}$, $h_{1}$, $h_{2}$, $...$,
$h_{m}$ (the so-called Chevalley generators) and some relations (among them
the Serre relations). Then, we can define a grading on $\mathfrak{g}$ by
setting
\[
\deg\left(  e_{i}\right)  =1,\ \ \ \ \ \ \ \ \ \ \deg\left(  f_{i}\right)
=-1\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \deg\left(  h_{i}\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,...,m\right\}  ,
\]
and extending this grading in such a way that $\mathfrak{g}$ becomes a graded
Lie algebra. This grading is non-canonical, but it makes $\mathfrak{g}$ into a
nondegenerate graded Lie algebra.
\end{proposition}

\begin{proposition}
\label{prop.grad.ghat.simple}If $\mathfrak{g}$ is a finite-dimensional simple
Lie algebra, then the loop algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $ and
the affine Kac-Moody algebra $\widehat{\mathfrak{g}}=\mathfrak{g}\left[
t,t^{-1}\right]  \oplus\mathbb{C}K$ can be graded as follows:

Fix Chevalley generators for $\mathfrak{g}$ and grade $\mathfrak{g}$ as in
Proposition \ref{prop.grad.g}. Now let $\theta$ be the maximal root of
$\mathfrak{g}$, i. e., the highest weight of the adjoint representation of
$\mathfrak{g}$. Let $e_{\theta}$ and $f_{\theta}$ be the root elements
corresponding to $\theta$. The \textit{Coxeter number} of $\mathfrak{g}$ is
defined as $\deg\left(  e_{\theta}\right)  +1$, and denoted by $h$. Now let us
grade $\widehat{\mathfrak{g}}$ by setting $\deg K=0$ and $\deg\left(
at^{m}\right)  =\deg a+mh$ for every homogeneous $a\in\mathfrak{g}$ and every
$m\in\mathbb{Z}$. This grading satisfies $\deg\left(  f_{\theta}t\right)  =1$
and $\deg\left(  e_{\theta}t^{-1}\right)  =-1$. Moreover, the map
$\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathfrak{g}\left[
t,t^{-1}\right]  ,\ x\mapsto xt$ is homogeneous of degree $h$; this is often
informally stated as ``$\deg t=h$'' (although $t$ itself is not an element of
$\widehat{\mathfrak{g}}$). It is easy to see that the elements of
$\widehat{\mathfrak{g}}$ of positive degree span $\mathfrak{n}_{+}\oplus
t\mathfrak{g}\left[  t\right]  $.

The graded Lie algebra $\widehat{\mathfrak{g}}$ is nondegenerate. The loop
algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $, however, is not (with the
grading defined in the same way).
\end{proposition}

If $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie algebra, we can write%
\[
\mathfrak{g}=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}=\bigoplus
\limits_{n<0}\mathfrak{g}_{n}\oplus\mathfrak{g}_{0}\oplus\bigoplus
\limits_{n>0}\mathfrak{g}_{n}.
\]
We denote $\bigoplus\limits_{n<0}\mathfrak{g}_{n}$ by $\mathfrak{n}_{-}$ and
we denote $\bigoplus\limits_{n>0}\mathfrak{g}_{n}$ by $\mathfrak{n}_{+}$. We
also denote $\mathfrak{g}_{0}$ by $\mathfrak{h}$. Then, $\mathfrak{n}_{-}$,
$\mathfrak{n}_{+}$ and $\mathfrak{h}$ are Lie subalgebras of $\mathfrak{g}$,
and the above decomposition rewrites as $\mathfrak{g}=\mathfrak{n}_{-}%
\oplus\mathfrak{h}\oplus\mathfrak{n}_{+}$ (but this is, of course, not a
direct sum of Lie algebras). This is called the \textit{triangular
decomposition} of $\mathfrak{g}$.

It is easy to see that when $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie
algebra, the universal enveloping algebra $U\left(  \mathfrak{g}\right)  $
canonically becomes a $\mathbb{Z}$-graded algebra.\footnote{In fact, $U\left(
\mathfrak{g}\right)  $ is defined as the quotient of the tensor algebra
$T\left(  \mathfrak{g}\right)  $ by a certain ideal. When $\mathfrak{g}$ is a
$\mathbb{Z}$-graded Lie algebra, this ideal is generated by homogeneous
elements, and thus is a graded ideal.}

\subsubsection{\texorpdfstring{$\mathbb{Z}$}{Z}-graded modules}

\begin{definition}
\label{def.liesubspace}Let $\mathfrak{g}$ be a Lie algebra over a field $k$.
Let $M$ be a $\mathfrak{g}$-module. Let $U$ be a vector subspace of
$\mathfrak{g}$. Let $N$ be a vector subspace of $M$. Then, $U\rightharpoonup
N$ will denote the $k$-linear span of all elements of the form
$u\rightharpoonup n$ with $u\in U$ and $n\in N$. (Notice that this notation is
analogous to the notation $\left[  U,N\right]  $ which is defined if $U$ and
$N$ are both subspaces of $\mathfrak{g}$.)
\end{definition}

\begin{definition}
\label{def.gradLie.mod}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra
with grading $\left(  \mathfrak{g}_{n}\right)  _{n\in\mathbb{Z}}$. A
$\mathbb{Z}$\textit{-graded $\mathfrak{g}$-module} means a $\mathbb{Z}$-graded
vector space $M$ equipped with a $\mathfrak{g}$-module structure such that any
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $\mathfrak{g}_{i}\rightharpoonup
M_{j}\subseteq M_{i+j}$, where $\left(  M_{n}\right)  _{n\in\mathbb{Z}}$
denotes the grading of $M$.
\end{definition}

The reader can easily check that when $\mathfrak{g}$ is a $\mathbb{Z}$-graded
Lie algebra, and $M$ is a $\mathbb{Z}$-graded $\mathfrak{g}$-module, then $M$
canonically becomes a $\mathbb{Z}$-graded $U\left(  \mathfrak{g}\right)
$-module (by taking the canonical $U\left(  \mathfrak{g}\right)  $-module
structure on $M$ and the given $\mathbb{Z}$-grading on $M$).

Examples of $\mathbb{Z}$-graded $\mathfrak{g}$-modules for various Lie
algebras $\mathfrak{g}$ are easy to get by. For example, when $\mathfrak{g}$
is a $\mathbb{Z}$-graded Lie algebra, then the adjoint representation
$\mathfrak{g}$ itself is a $\mathbb{Z}$-graded $\mathfrak{g}$-module. For two
more interesting examples:

\begin{example}
The action of the Heisenberg algebra $\mathcal{A}$ on the $\mu$-Fock
representation $F_{\mu}$ makes $F_{\mu}$ into a $\mathbb{Z}$-graded
$\mathcal{A}$-module (i. e., it maps $\mathcal{A}\left[  i\right]  \otimes
F_{\mu}\left[  j\right]  $ to $F_{\mu}\left[  i+j\right]  $ for all
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$). Here, we are using the $\mathbb{Z}%
$-grading on $F_{\mu}$ defined in Definition \ref{def.fock.grad}. (If we would
use the alternative $\mathbb{Z}$-grading on $F_{\mu}$ defined in Remark
\ref{rmk.fockgrad}, then the action of $\mathcal{A}$ on $F_{\mu}$ would still
make $F_{\mu}$ into a $\mathbb{Z}$-graded $\mathcal{A}$-module.)

The action of $\mathcal{A}_{0}$ on the Fock module $F$ makes $F$ into a
$\mathbb{Z}$-graded $\mathcal{A}_{0}$-module.
\end{example}

\begin{example}
Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. The $\operatorname*{Vir}%
$-module $V_{\alpha,\beta}$ defined in Proposition \ref{prop.Vab.1} becomes a
$\mathbb{Z}$-graded $\operatorname*{Vir}$-module by means of the grading
$\left(  V_{\alpha,\beta}\left[  n\right]  \right)  _{n\in\mathbb{Z}}$, where
$V_{\alpha,\beta}\left[  n\right]  =\left\langle v_{-n}\right\rangle $ for
every $n\in\mathbb{Z}$.
\end{example}

Let us formulate a graded analogue of Lemma \ref{lem.V=F}:

\begin{lemma}
\label{lem.V=F.gr}Let $V$ be a $\mathbb{Z}$-graded $\mathcal{A}_{0}$-module
with grading $\left(  V\left[  n\right]  \right)  _{n\in\mathbb{Z}}$. Let
$u\in V\left[  0\right]  $ be such that $a_{i}u=0$ for all $i>0$, and such
that $Ku=u$. Then, there exists a $\mathbb{Z}$-graded homomorphism
$\eta:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\eta\left(
1\right)  =u$. (This homomorphism $\eta$ is unique, although we won't need this.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.V=F.gr}.} Let $\eta$ be the map $F\rightarrow
V$ which sends every polynomial $P\in F=\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to $P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u\in
V$.\ \ \ \ \footnote{Note that the term $P\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  $ denotes the evaluation of the polynomial $P$ at $\left(
x_{1},x_{2},x_{3},...\right)  =\left(  a_{-1},a_{-2},a_{-3},...\right)  $.
This evaluation is a well-defined element of $U\left(  \mathcal{A}_{0}\right)
$, since the elements $a_{-1}$, $a_{-2}$, $a_{-3}$, $...$ of $U\left(
\mathcal{A}_{0}\right)  $ commute.} Just as in the Second proof of Lemma
\ref{lem.V=F}, we can show that $\eta$ is an $\mathcal{A}_{0}$-module
homomorphism $F\rightarrow V$ such that $\eta\left(  1\right)  =u$. Hence, in
order to finish the proof of Lemma \ref{lem.V=F.gr}, we only need to check
that $\eta$ is a $\mathbb{Z}$-graded map.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

Let $n\in\mathbb{Z}$ and $P\in F\left[  n\right]  $. Then, we can write the
polynomial $P$ in the form
\begin{equation}
P=\sum\limits_{\substack{\left(  i_{1},i_{2},i_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\1i_{1}%
+2i_{2}+3i_{3}+...=-n}}\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }%
x_{1}^{i_{1}}x_{2}^{i_{2}}x_{3}^{i_{3}}... \label{pf.V=F.gr.P}%
\end{equation}
for some scalars $\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }%
\in\mathbb{C}$. Consider these $\lambda_{\left(  i_{1},i_{2},i_{3},...\right)
}$. From (\ref{pf.V=F.gr.P}), it follows that%
\begin{align*}
P\left(  a_{-1},a_{-2},a_{-3},...\right)   &  =\sum\limits_{\substack{\left(
i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  };\\1i_{1}+2i_{2}+3i_{3}+...=-n}}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\underbrace{a_{-1}^{i_{1}}a_{-2}^{i_{2}}%
a_{-3}^{i_{3}}...}_{\substack{\in U\left(  \mathcal{A}_{0}\right)  \left[
i_{1}\left(  -1\right)  +i_{2}\left(  -2\right)  +i_{3}\left(  -3\right)
+...\right]  \\\text{(since every positive integer }k\text{ satisfies}%
\\a_{-k}\in\mathcal{A}_{0}\left[  -k\right]  \subseteq U\left(  \mathcal{A}%
_{0}\right)  \left[  -k\right]  \text{ and thus }a_{-k}^{i_{k}}\in U\left(
\mathcal{A}_{0}\right)  \left[  i_{k}\left(  -k\right)  \right]  \text{)}}}\\
&  \in\sum\limits_{\substack{\left(  i_{1},i_{2},i_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }%
;\\1i_{1}+2i_{2}+3i_{3}+...=-n}}\lambda_{\left(  i_{1},i_{2},i_{3},...\right)
}U\left(  \mathcal{A}_{0}\right)  \left[  \underbrace{i_{1}\left(  -1\right)
+i_{2}\left(  -2\right)  +i_{3}\left(  -3\right)  +...}_{\substack{=-\left(
1i_{1}+2i_{2}+3i_{3}+...\right)  =n\\\text{(since }1i_{1}+2i_{2}%
+3i_{3}+...=-n\text{)}}}\right] \\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},i_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  };\\1i_{1}%
+2i_{2}+3i_{3}+...=-n}}\lambda_{\left(  i_{1},i_{2},i_{3},...\right)
}U\left(  \mathcal{A}_{0}\right)  \left[  n\right]  \subseteq U\left(
\mathcal{A}_{0}\right)  \left[  n\right]
\end{align*}
(since $U\left(  \mathcal{A}_{0}\right)  \left[  n\right]  $ is a vector
space). By the definition of $\eta$, we have%
\[
\eta\left(  P\right)  =\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)
}_{\in U\left(  \mathcal{A}_{0}\right)  \left[  n\right]  }\cdot
\underbrace{u}_{\in V\left[  0\right]  }\in U\left(  \mathcal{A}_{0}\right)
\left[  n\right]  \cdot V\left[  0\right]  \subseteq V\left[  n\right]
\]
(since $V$ is a $\mathbb{Z}$-graded $\mathcal{A}_{0}$-module and thus a
$\mathbb{Z}$-graded $U\left(  \mathcal{A}_{0}\right)  $-module). Now forget
that we fixed $n$ and $P$. We have thus shown that every $n\in\mathbb{Z}$ and
$P\in F\left[  n\right]  $ satisfy $\eta\left(  P\right)  \subseteq V\left[
n\right]  $. In other words, every $n\in\mathbb{Z}$ satisfies $\eta\left(
F\left[  n\right]  \right)  \subseteq V\left[  n\right]  $. In other words,
$\eta$ is $\mathbb{Z}$-graded. This proves Lemma \ref{lem.V=F.gr}.

And here is a graded analogue of Lemma \ref{lem.V=F.A}:

\begin{lemma}
\label{lem.V=F.A.gr}Let $V$ be a graded $\mathcal{A}$-module with grading
$\left(  V\left[  n\right]  \right)  _{n\in\mathbb{Z}}$. Let $\mu\in
\mathbb{C}$. Let $u\in V\left[  0\right]  $ be such that $a_{i}u=0$ for all
$i>0$, such that $a_{0}u=\mu u$, and such that $Ku=u$. Then, there exists a
$\mathbb{Z}$-graded homomorphism $\eta:F_{\mu}\rightarrow V$ of $\mathcal{A}%
$-modules such that $\eta\left(  1\right)  =u$. (This homomorphism $\eta$ is
unique, although we won't need this.)
\end{lemma}

The proof of Lemma \ref{lem.V=F.A.gr} is completely analogous to that of Lemma
\ref{lem.V=F.gr}, but this time using Lemma \ref{lem.V=F.A} instead of Lemma
\ref{lem.V=F}.

\subsubsection{Verma modules}

\begin{definition}
\label{def.verma}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra (not
necessarily nondegenerate). Let us work with the notations introduced above.
Let $\lambda\in\mathfrak{h}^{\ast}$.

Let $\mathbb{C}_{\lambda}$ denote the $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{+}\right)  $-module which, as a $\mathbb{C}$-vector space, is
the free vector space with basis $\left(  v_{\lambda}^{+}\right)  $ (thus, a
$1$-dimensional vector space), and whose $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{+}\right)  $-action is given by%
\begin{align*}
hv_{\lambda}^{+}  &  =\lambda\left(  h\right)  v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h};\\
\mathfrak{n}_{+}v_{\lambda}^{+}  &  =0.
\end{align*}


The \textit{Verma highest-weight module }$M_{\lambda}^{+}$ \textit{of
}$\left(  \mathfrak{g},\lambda\right)  $ is defined by%
\[
M_{\lambda}^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{\lambda}.
\]
The element $1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
}v_{\lambda}^{+}$ of $M_{\lambda}^{+}$ will still be denoted by $v_{\lambda
}^{+}$ by abuse of notation, and will be called the \textit{defining vector}
of $M_{\lambda}^{+}$. Since $U\left(  \mathfrak{g}\right)  $ and
$\mathbb{C}_{\lambda}$ are graded $U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  $-modules, their tensor product $U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{\lambda}=M_{\lambda}^{+}$ becomes graded as well.

Let $\mathbb{C}_{\lambda}$ denote the $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  $-module which, as a $\mathbb{C}$-vector space, is
the free vector space with basis $\left(  v_{\lambda}^{-}\right)  $ (thus, a
$1$-dimensional vector space), and whose $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  $-action is given by%
\begin{align*}
hv_{\lambda}^{-}  &  =\lambda\left(  h\right)  v_{\lambda}^{-}%
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h};\\
\mathfrak{n}_{-}v_{\lambda}^{-}  &  =0.
\end{align*}
(Note that we denote this $\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
$-module by $\mathbb{C}_{\lambda}$, although we already have denoted an
$\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-module by $\mathbb{C}%
_{\lambda}$. This is ambiguous, but misunderstandings are unlikely to occur
since these modules are modules over different Lie algebras, and their
restrictions to $\mathfrak{h}$ are identical.)

The \textit{Verma lowest-weight module }$M_{\lambda}^{-}$ \textit{of }$\left(
\mathfrak{g},\lambda\right)  $ is defined by%
\[
M_{\lambda}^{-}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{\lambda}.
\]
The element $1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
}v_{\lambda}^{-}$ of $M_{\lambda}^{-}$ will still be denoted by $v_{\lambda
}^{-}$ by abuse of notation, and will be called the \textit{defining vector}
of $M_{\lambda}^{-}$. Since $U\left(  \mathfrak{g}\right)  $ and
$\mathbb{C}_{\lambda}$ are graded $U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{-}\right)  $-modules, their tensor product $U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}%
_{\lambda}=M_{\lambda}^{-}$ becomes graded as well.
\end{definition}

We notice some easy facts about these modules:

\begin{proposition}
\label{prop.verma1}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra
(not necessarily nondegenerate). Let us work with the notations introduced
above. Let $\lambda\in\mathfrak{h}^{\ast}$.

\textbf{(a)} As a graded $\mathfrak{n}_{-}$-module, $M_{\lambda}^{+}=U\left(
\mathfrak{n}_{-}\right)  v_{\lambda}^{+}$; more precisely, there exists a
graded $\mathfrak{n}_{-}$-module isomorphism $U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{+}$ which
sends every $x\otimes t\in U\left(  \mathfrak{n}_{-}\right)  \otimes
\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{+}$. The Verma module $M_{\lambda
}^{+}$ is concentrated in nonpositive degrees:%
\[
M_{\lambda}^{+}=\bigoplus\limits_{n\geq0}M_{\lambda}^{+}\left[  -n\right]
;\ \ \ \ \ \ \ \ \ \ M_{\lambda}^{+}\left[  -n\right]  =U\left(
\mathfrak{n}_{-}\right)  \left[  -n\right]  v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\geq0.
\]
Also, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\leq-1$, we have%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]


\textbf{(b)} As a graded $\mathfrak{n}_{+}$-module, $M_{\lambda}^{-}=U\left(
\mathfrak{n}_{+}\right)  v_{\lambda}^{-}$; more precisely, there exists a
graded $\mathfrak{n}_{+}$-module isomorphism $U\left(  \mathfrak{n}%
_{+}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{-}$ which
sends every $x\otimes t\in U\left(  \mathfrak{n}_{+}\right)  \otimes
\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{-}$. The Verma module $M_{\lambda
}^{-}$ is concentrated in nonnegative degrees:%
\[
M_{\lambda}^{-}=\bigoplus\limits_{n\geq0}M_{\lambda}^{-}\left[  n\right]
;\ \ \ \ \ \ \ \ \ \ M_{\lambda}^{-}\left[  n\right]  =U\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  v_{\lambda}^{-}\ \ \ \ \ \ \ \ \ \ \text{for
every }n\geq0.
\]
Also, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\geq1$, we have%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{-}\left[  n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\geq1}\left(  1-q^{j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.verma1}.} \textbf{(a)} Let
$\rho:U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}}U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \rightarrow U\left(  \mathfrak{g}%
\right)  $ be the $\mathbb{C}$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \mathfrak{n}_{-}\right)
\text{ and }\beta\in U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
\]
(this is clearly well-defined). By Corollary \ref{cor.U(X)U} (applied to
$\mathfrak{a}=\mathfrak{n}_{-}$, $\mathfrak{b}=\mathfrak{h}\oplus
\mathfrak{n}_{+}$ and $\mathfrak{c}=\mathfrak{g}$), this $\rho$ is an
isomorphism of filtered\footnote{Filtered by the usual filtration on the
universal enveloping algebra of a Lie algebra. This filtration does not take
into account the grading on $\mathfrak{n}_{-}$, $\mathfrak{h}\oplus
\mathfrak{n}_{+}$ and $\mathfrak{g}$.} vector spaces, of left $U\left(
\mathfrak{n}_{-}\right)  $-modules and of right $U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  $-modules. Also, it is a graded linear
map\footnote{Here we \textit{do} take into account the grading on
$\mathfrak{n}_{-}$, $\mathfrak{h}\oplus\mathfrak{n}_{+}$ and $\mathfrak{g}$.}
(this is clear from its definition), and thus an isomorphism of graded vector
spaces (because if a vector space isomorphism of graded vector spaces is a
graded linear map, then it must be an isomorphism of graded vector
spaces\footnote{If you are wondering why this statement is more than a
blatantly obvious tautology, let me add some clarifications:
\par
A \textit{graded linear map} is a morphism in the category of graded vector
spaces. What I am stating here is that if a vector space isomorphism between
graded vector spaces is at the same time a morphism in the category of graded
vector spaces, then it must be an \textit{isomorphism} in the category of
graded vector spaces. This is very easy to show, but not a self-evident
tautology. In fact, the analogous assertion about filtered vector spaces (i.
e., the assertion that if a vector space isomorphism between filtered vector
spaces is at the same time a morphism in the category of filtered vector
spaces, then it must be an \textit{isomorphism} in the category of filtered
vector spaces) is wrong.}). Altogether, $\rho$ is an isomorphism of graded
filtered vector spaces, of left $U\left(  \mathfrak{n}_{-}\right)  $-modules
and of right $U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-modules.
Hence,%
\begin{align*}
M_{\lambda}^{+}  &  =\underbrace{U\left(  \mathfrak{g}\right)  }%
_{\substack{\cong U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}%
}U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \\\text{(by the
isomorphism }\rho\text{)}}}\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}\cong\left(  U\left(  \mathfrak{n}%
_{-}\right)  \otimes_{\mathbb{C}}U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  \right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}\\
&  \cong U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}}%
\underbrace{\left(  U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{\lambda}\right)  }_{\cong\mathbb{C}_{\lambda}}\cong U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\ \ \ \ \ \ \ \ \ \ \text{as graded
}U\left(  \mathfrak{n}_{-}\right)  \text{-modules.}%
\end{align*}
This gives us a graded $\mathfrak{n}_{-}$-module isomorphism $U\left(
\mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda
}^{+}$ which is easily seen to send every $x\otimes t\in U\left(
\mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{+}$.
Hence, $M_{\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)  v_{\lambda}^{+}$.
Since $\mathfrak{n}_{-}$ is concentrated in negative degrees, it is clear that
$U\left(  \mathfrak{n}_{-}\right)  $ is concentrated in nonpositive degrees.
Hence, $U\left(  \mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ is
concentrated in nonpositive degrees, and thus the same holds for $M_{\lambda
}^{+}$ (since $M_{\lambda}^{+}\cong U\left(  \mathfrak{n}_{-}\right)
\otimes\mathbb{C}_{\lambda}$ as graded $U\left(  \mathfrak{n}_{-}\right)
$-modules). In other words, $M_{\lambda}^{+}=\bigoplus\limits_{n\geq
0}M_{\lambda}^{+}\left[  -n\right]  $.

Since the isomorphism $U\left(  \mathfrak{n}_{-}\right)  \otimes
\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{+}$ which sends every $x\otimes
t\in U\left(  \mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ to
$xtv_{\lambda}^{+}$ is graded, it sends $U\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  \otimes\mathbb{C}_{\lambda}=\left(  U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\right)  \left[  -n\right]  $ to
$M_{\lambda}^{+}\left[  -n\right]  $ for every $n\geq0$. Thus, $M_{\lambda
}^{+}\left[  -n\right]  =U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]
v_{\lambda}^{+}$ for every $n\geq0$. Hence,
\begin{align*}
\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)   &  =\dim\left(
U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  v_{\lambda}^{+}\right)
=\dim\left(  U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \right)
=\dim\left(  S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }U\left(  \mathfrak{n}_{-}\right)  \cong S\left(  \mathfrak{n}%
_{-}\right)  \text{ as graded vector spaces}\\
\text{(by the Poincar\'{e}-Birkhoff-Witt theorem)}%
\end{array}
\right)
\end{align*}
for every $n\geq0$. Hence, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\leq-1$,
then%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\sum\limits_{n\geq0}\dim\left(  S\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  \right)  q^{n}=\dfrac{1}{\prod\limits_{j\leq-1}\left(
1-q^{-j}\right)  ^{\dim\left(  \left(  \mathfrak{n}_{-}\right)  _{j}\right)
}}=\dfrac{1}{\prod\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]
This proves Proposition \ref{prop.verma1} \textbf{(a)}.

\textbf{(b)} The proof of part \textbf{(b)} is analogous to that of
\textbf{(a)}.

This proves Proposition \ref{prop.verma1}.

We have already encountered an example of a Verma highest-weight module:

\begin{proposition}
\label{prop.fockverma}Let $\mathfrak{g}$ be the Lie algebra $\mathcal{A}_{0}$.
Consider the Fock module $F$ over the Lie algebra $\mathcal{A}_{0}$. Then,
there is a canonical isomorphism $M_{1}^{+}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules (where $1$ is the element of $\mathfrak{h}^{\ast}$ which sends $K$
to $1$) which sends $v_{1}^{+}\in M_{1}^{+}$ to $1\in F$.
\end{proposition}

\textit{First proof of Proposition \ref{prop.fockverma}.} As we showed in the
First proof of Lemma \ref{lem.V=F}, there exists a homomorphism $\overline
{\eta}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}%
_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules such that $\overline{\eta}_{F,1}\left(  1\right)  =1$. In the same
proof, we also showed that this $\overline{\eta}_{F,1}$ is an isomorphism. We
thus have an isomorphism $\overline{\eta}_{F,1}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{F,1}\left(  1\right)  =1$. Since%
\begin{align*}
\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}  &  =U\left(  \mathcal{A}_{0}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }%
\mathbb{C}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathcal{A}_{0}=\mathfrak{g}%
\text{, }\mathbb{C}K=\mathfrak{h}\text{, }\mathcal{A}_{0}^{+}=\mathfrak{n}%
_{+}\text{ and }\mathbb{C}=\mathbb{C}_{1}\right) \\
&  =M_{1}^{+},
\end{align*}
and since the element $1$ of $\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$ is exactly the
element $v_{1}^{+}$ of $M_{1}^{+}$, this rewrites as follows: We have an
isomorphism $\overline{\eta}_{F,1}:M_{1}^{+}\rightarrow F$ of $\mathcal{A}%
_{0}$-modules such that $\overline{\eta}_{F,1}\left(  v_{1}^{+}\right)  =1$.
This proves Proposition \ref{prop.fockverma}.

\textit{Second proof of Proposition \ref{prop.fockverma}.} It is clear from
the definition of $v_{1}^{+}$ that $a_{i}v_{1}^{+}=0$ for all $i>0$, and that
$Kv_{1}^{+}=v_{1}^{+}$. Applying Lemma \ref{lem.V=F} to $u=v_{1}^{+}$ and
$V=M_{1}^{+}$, we thus conclude that there exists a homomorphism
$\eta:F\rightarrow M_{1}^{+}$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =v_{1}^{+}$.

On the other hand, since $M_{1}^{+}=U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{1}$
(by the definition of $M_{1}^{+}$), we can define an $U\left(  \mathfrak{g}%
\right)  $-module homomorphism%
\[
M_{1}^{+}\rightarrow F,\ \ \ \ \ \ \ \ \ \ \alpha\otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }z\mapsto\alpha z.
\]
Since $\mathfrak{g}=\mathcal{A}_{0}$, this is an $U\left(  \mathcal{A}%
_{0}\right)  $-module homomorphism, i. e., an $\mathcal{A}_{0}$-module
homomorphism. Denote this homomorphism by $\xi$. We are going to prove that
$\eta$ and $\xi$ are mutually inverse.

Since $v_{1}^{+}=1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
}1$, we have%
\begin{align*}
\xi\left(  v_{1}^{+}\right)   &  =\xi\left(  1\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }1\right)  =1\cdot1\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\xi\right) \\
&  =1.
\end{align*}
Since $v_{1}^{+}=\eta\left(  1\right)  $, this rewrites as $\xi\left(
\eta\left(  1\right)  \right)  =1$. In other words, $\left(  \xi\circ
\eta\right)  \left(  1\right)  =1$. Since the vector $1$ generates the
$\mathcal{A}_{0}$-module $F$ (because Lemma \ref{lem.F.P1=P} yields
$P=\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  }_{\in U\left(
\mathcal{A}_{0}\right)  }\cdot1\in U\left(  \mathcal{A}_{0}\right)  \cdot1$
for every $P\in F$), this yields that the $\mathcal{A}_{0}$-module
homomorphisms $\xi\circ\eta:F\rightarrow F$ and $\operatorname*{id}%
:F\rightarrow F$ are equal on a generating set of the $\mathcal{A}_{0}$-module
$F$. Thus, $\xi\circ\eta=\operatorname*{id}$.

Also, $\left(  \eta\circ\xi\right)  \left(  v_{1}^{+}\right)  =\eta\left(
\underbrace{\xi\left(  v_{1}^{+}\right)  }_{=1}\right)  =\eta\left(  1\right)
=v_{1}^{+}$. Since the vector $v_{1}^{+}$ generates $M_{1}^{+}$ as an
$\mathcal{A}_{0}$-module (because $M_{1}^{+}=U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{1}=U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{1}$), this yields that the
$\mathcal{A}_{0}$-module homomorphisms $\eta\circ\xi:M_{1}^{+}\rightarrow
M_{1}^{+}$ and $\operatorname*{id}:M_{1}^{+}\rightarrow M_{1}^{+}$ are equal
on a generating set of the $\mathcal{A}_{0}$-module $M_{1}^{+}$. Thus,
$\eta\circ\xi=\operatorname*{id}$.

Since $\eta\circ\xi=\operatorname*{id}$ and $\xi\circ\eta=\operatorname*{id}$,
the maps $\xi$ and $\eta$ are mutually inverse, so that $\xi$ is an
isomorphism $M_{1}^{+}\rightarrow F$ of $\mathcal{A}_{0}$-modules. We know
that $\xi$ sends $v_{1}^{+}$ to $\xi\left(  v_{1}^{+}\right)  =1$. Thus, there
is a canonical isomorphism $M_{1}^{+}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules which sends $v_{1}^{+}\in M_{1}^{+}$ to $1\in F$. Proposition
\ref{prop.fockverma} is proven.

In analogy to the Second proof of Proposition \ref{prop.fockverma}, we can show:

\begin{proposition}
\label{prop.fockverma.A}Let $\mathfrak{g}$ be the Lie algebra $\mathcal{A}$.
Let $\mu\in\mathbb{C}$. Consider the $\mu$-Fock module $F_{\mu}$ over the Lie
algebra $\mathcal{A}$. Then, there is a canonical isomorphism $M_{1,\mu}%
^{+}\rightarrow F_{\mu}$ of $\mathcal{A}$-modules (where $\left(
1,\mu\right)  $ is the element of $\mathfrak{h}^{\ast}$ which sends $K$ to $1$
and $a_{0}$ to $\mu$) which sends $v_{1,\mu}^{+}\in M_{1,\mu}^{+}$ to $1\in
F_{\mu}$.
\end{proposition}

\subsubsection{Degree-\texorpdfstring{$0$}{0} forms}

We introduce another simple notion:

\begin{definition}
\label{def.deg0}Let $V$ and $W$ be two $\mathbb{Z}$-graded vector spaces over
a field $k$. Let $\beta:V\times W\rightarrow k$ be a $k$-bilinear form. We say
that the $k$-bilinear form $\beta$ \textit{has degree }$0$ (or, equivalently,
is a \textit{degree-}$0$ bilinear form) if and only if it satisfies%
\[
\left(  \beta\left(  V_{n}\times W_{m}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for
all }\left(  n,m\right)  \in\mathbb{Z}^{2}\text{ satisfying }n+m\neq0\right)
.
\]
(Here, $V_{n}$ denotes the $n$-th homogeneous component of $V$, and $W_{m}$
denotes the $m$-th homogeneous component of $W$.)
\end{definition}

It is straightforward to see the following characterization of degree-$0$
bilinear forms:

\begin{remark}
\label{rmk.deg0}Let $V$ and $W$ be two $\mathbb{Z}$-graded vector spaces over
a field $k$. Let $\beta:V\times W\rightarrow k$ be a $k$-bilinear form. Let
$B$ be the linear map $V\otimes W\rightarrow k$ induced by the $k$-bilinear
map $V\times W\rightarrow k$ using the universal property of the tensor
product. Consider $V\otimes W$ as a $\mathbb{Z}$-graded vector space (in the
usual way in which one defines a grading on the tensor product of two
$\mathbb{Z}$-graded vector spaces), and consider $k$ as a $\mathbb{Z}$-graded
vector space (by letting the whole field $k$ live in degree $0$).

Then, $\beta$ has degree $0$ if and only if $B$ is a graded map.
\end{remark}

\begin{verlong}
\textit{Proof of Remark \ref{rmk.deg0}.} For every $i\in\mathbb{Z}$ and every
$\mathbb{Z}$-graded vector space $U$, we denote by $U_{i}$ the $i$-th
homogeneous component of $U$. This is consistent with the use of the notations
$V_{n}$ and $W_{m}$ in Definition \ref{def.deg0}.

We know that $B$ is the linear map $V\otimes W\rightarrow k$ induced by the
$k$-bilinear map $V\times W\rightarrow k$ using the universal property of the
tensor product. Hence, any $a\in V$ and $b\in W$ satisfy%
\begin{equation}
B\left(  a\otimes b\right)  =\beta\left(  a,b\right)  . \label{pf.deg0.Bb}%
\end{equation}


We are going to prove the following two assertions:

\textit{Assertion \ref{rmk.deg0}.1:} If $\beta$ has degree $0$, then $B$ is a
graded map.

\textit{Assertion \ref{rmk.deg0}.2:} If $B$ is a graded map, then $\beta$ has
degree $0$.

\textit{Proof of Assertion \ref{rmk.deg0}.1:} Assume that $\beta$ has degree
$0$. Therefore,%
\begin{equation}
\left(  \beta\left(  V_{n}\times W_{m}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for
all }\left(  n,m\right)  \in\mathbb{Z}^{2}\text{ satisfying }n+m\neq0\right)
\label{pf.deg0.1.1}%
\end{equation}
(because Definition \ref{def.deg0} states that $\beta$ has degree $0$ if and
only if it satisfies (\ref{pf.deg0.1.1})).

Now, let $\left(  n,m\right)  \in\mathbb{Z}^{2}$ be such that $n+m\neq0$. Let
$u\in V_{n}\otimes W_{m}$.

Since $u$ is a tensor in $V_{n}\otimes W_{m}$, we can write $u$ in the form
$u=\sum\limits_{i=1}^{n}\lambda_{i}a_{i}\otimes b_{i}$ for some $n\in
\mathbb{N}$, some elements $\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$
of $k$, some elements $a_{1}$, $a_{2}$, $...$, $a_{n}$ of $V_{n}$ and some
elements $b_{1}$, $b_{2}$, $...$, $b_{n}$ of $W_{m}$. Consider this $n$, these
$\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$, these $a_{1}$, $a_{2}$,
$...$, $a_{n}$, and these $b_{1}$, $b_{2}$, $...$, $b_{n}$. Since
$u=\sum\limits_{i=1}^{n}\lambda_{i}a_{i}\otimes b_{i}$, we have%
\begin{align*}
B\left(  u\right)   &  =B\left(  \sum\limits_{i=1}^{n}\lambda_{i}a_{i}\otimes
b_{i}\right)  =\sum\limits_{i=1}^{n}\lambda_{i}\underbrace{B\left(
a_{i}\otimes b_{i}\right)  }_{\substack{=\beta\left(  a_{i},b_{i}\right)
\\\text{(by (\ref{pf.deg0.Bb}), applied}\\\text{to }a=a_{i}\text{ and }%
b=b_{i}\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\text{ is
}k\text{-linear}\right) \\
&  =\sum\limits_{i=1}^{n}\lambda_{i}\underbrace{\beta\left(  a_{i}%
,b_{i}\right)  }_{\substack{\in\beta\left(  V_{n}\times W_{m}\right)
\\\text{(since }a_{i}\in V_{n}\text{ and }b_{i}\in W_{m}\text{)}}}\in
\sum\limits_{i=1}^{n}\lambda_{i}\underbrace{\beta\left(  V_{n}\times
W_{m}\right)  }_{\substack{=0\\\text{(by (\ref{pf.deg0.1.1}))}}}=\sum
\limits_{i=1}^{n}\lambda_{i}0=0.
\end{align*}
In other words, $B\left(  u\right)  =0$.

Now forget that we fixed $u$. We thus have shown that every $u\in V_{n}\otimes
W_{m}$ satisfies $B\left(  u\right)  =0$. In other words, $B\left(
V_{n}\otimes W_{m}\right)  =0$.

Now forget that we fixed $\left(  n,m\right)  $. We thus have shown that
\begin{equation}
\text{every }\left(  n,m\right)  \in\mathbb{Z}^{2}\text{ such that }%
n+m\neq0\text{ satisfies }B\left(  V_{n}\otimes W_{m}\right)  =0\text{.}
\label{pf.deg0.1.3}%
\end{equation}


Now, every $N\in\mathbb{Z}$ satisfies%
\begin{align}
\left(  V\otimes W\right)  _{N}  &  =\bigoplus\limits_{i\in\mathbb{Z}}%
V_{i}\otimes W_{N-i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the grading on the
tensor product }V\otimes W\right) \nonumber\\
&  =\sum\limits_{i\in\mathbb{Z}}V_{i}\otimes W_{N-i} \label{pf.deg0.1.4a}%
\end{align}
(since direct sums are sums). Hence, for every nonzero $N\in\mathbb{Z}$, we
have%
\begin{align}
B\left(  \left(  V\otimes W\right)  _{N}\right)   &  =B\left(  \sum
\limits_{i\in\mathbb{Z}}V_{i}\otimes W_{N-i}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.deg0.1.4a})}\right) \nonumber\\
&  =\sum\limits_{i\in\mathbb{Z}}\underbrace{B\left(  V_{i}\otimes
W_{N-i}\right)  }_{\substack{=0\\\text{(by (\ref{pf.deg0.1.3}) (applied to
}\left(  n,m\right)  =\left(  i,N-i\right)  \text{)}\\\text{(since }i+\left(
N-i\right)  =N\neq0\text{))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\text{
is }k\text{-linear}\right) \nonumber\\
&  =\sum\limits_{i\in\mathbb{Z}}0=0. \label{pf.deg0.1.5}%
\end{align}


It is now clear that every $N\in\mathbb{Z}$ satisfies $B\left(  \left(
V\otimes W\right)  _{N}\right)  \subseteq k_{N}$%
\ \ \ \ \footnote{\textit{Proof.} Let $N\in\mathbb{Z}$. We have to prove that
$B\left(  \left(  V\otimes W\right)  _{N}\right)  \subseteq k_{N}$. But
$k_{0}=k$ (by the definition of the grading on $k$) and thus $B\left(  \left(
V\otimes W\right)  _{0}\right)  \subseteq k=k_{0}$. Hence, $B\left(  \left(
V\otimes W\right)  _{N}\right)  \subseteq k_{N}$ is obvious when $N=0$. Hence,
for the rest of this proof, we can WLOG assume that we don't have $N=0$.
Assume this. Thus, $N\neq0$. Hence, (\ref{pf.deg0.1.5}) yields $B\left(
\left(  V\otimes W\right)  _{N}\right)  =0\subseteq k_{N}$, qed.}. In other
words, $B$ is a graded map. This proves Assertion \ref{rmk.deg0}.1.

\textit{Proof of Assertion \ref{rmk.deg0}.2:} Assume that $B$ is a graded map.
Now, let $\left(  n,m\right)  \in\mathbb{Z}^{2}$ be such that $n+m\neq0$. Let
$a\in V_{n}$ and $b\in W_{m}$.

By the definition of the grading on $k$, we have $k_{i}=0$ for all nonzero
$i\in\mathbb{Z}$. Applying this to $i=n+m$, we obtain $k_{n+m}=0$.

By the definition of the grading on the tensor product $V\otimes W$, we have
\begin{align}
\left(  V\otimes W\right)  _{n+m}  &  =\bigoplus\limits_{i\in\mathbb{Z}}%
V_{i}\otimes W_{n+m-i}\supseteq V_{n}\otimes\underbrace{W_{n+m-n}}_{=W_{m}%
}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }V_{n}\otimes W_{n+m-n}\text{ is an addend of the direct sum}\\
\bigoplus\limits_{i\in\mathbb{Z}}V_{i}\otimes W_{n+m-i}\text{ (namely, the
addend for }i=n\text{)}%
\end{array}
\right) \nonumber\\
&  =V_{n}\otimes W_{m}. \label{pf.deg0.2.1}%
\end{align}


But%
\begin{align*}
\beta\left(  a,b\right)   &  =B\left(  \underbrace{a}_{\in V_{n}}%
\otimes\underbrace{b}_{\in W_{m}}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.deg0.Bb})}\right) \\
&  \in B\left(  \underbrace{V_{n}\otimes W_{m}}_{\substack{\subseteq\left(
V\otimes W\right)  _{n+m}\\\text{(by (\ref{pf.deg0.2.1}))}}}\right)  \subseteq
B\left(  \left(  V\otimes W\right)  _{n+m}\right) \\
&  \subseteq k_{n+m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\text{ is a
graded map}\right) \\
&  =0,
\end{align*}
so that $\beta\left(  a,b\right)  =0$.

Now, forget that we fixed $a$ and $b$. We thus have shown that $\beta\left(
a,b\right)  =0$ for every $a\in V_{n}$ and $b\in W_{m}$. In other words,
$\beta\left(  a,b\right)  =0$ for every $\left(  a,b\right)  \in V_{n}\times
W_{m}$. In other words, $\beta\left(  V_{n}\times W_{m}\right)  =0$.

Now, forget that we fixed $n$ and $m$. We thus have shown that%
\begin{equation}
\left(  \beta\left(  V_{n}\times W_{m}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for
all }\left(  n,m\right)  \in\mathbb{Z}^{2}\text{ satisfying }n+m\neq0\right)
. \label{pf.deg0.2.3}%
\end{equation}
In other words, $\beta$ has degree $0$ (because Definition \ref{def.deg0}
states that $\beta$ has degree $0$ if and only if it satisfies
(\ref{pf.deg0.2.3})). This proves Assertion \ref{rmk.deg0}.2. Now, both
Assertion \ref{rmk.deg0}.1 and Assertion \ref{rmk.deg0}.2 are proven.
Combining these two assertions, we conclude that $\beta$ has degree $0$ if and
only if $B$ is a graded map. This proves Remark \ref{rmk.deg0}.
\end{verlong}

\subsection{\label{subsect.invform}The invariant bilinear form on Verma
modules}

\subsubsection{The invariant bilinear form}

The study of the Verma modules rests on a $\mathfrak{g}$-bilinear form which
connects a highest-weight Verma module with a lowest-weight Verma module for
the opposite weight. First, let us prove its existence and basic properties:

\begin{proposition}
\label{prop.invform}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra,
and $\lambda\in\mathfrak{h}^{\ast}$.

\textbf{(a)} There exists a unique $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =1$ (where we denote this
bilinear form by $\left(  \cdot,\cdot\right)  $).

\textbf{(b)} This form has degree $0$. (This means that if we consider this
bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ as
a linear map $M_{\lambda}^{+}\otimes M_{-\lambda}^{-}\rightarrow\mathbb{C}$,
then it is a graded map, where $M_{\lambda}^{+}\otimes M_{-\lambda}^{-}$ is
graded as a tensor product of graded vector spaces, and $\mathbb{C}$ is
concentrated in degree $0$.)

\textbf{(c)} Every $\mathfrak{g}$-invariant bilinear form $M_{\lambda}%
^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ is a scalar multiple of this
form $\left(  \cdot,\cdot\right)  $.
\end{proposition}

\begin{remark}
\label{rmk.invform.1}Proposition \ref{prop.invform} still holds when the
ground field $\mathbb{C}$ is replaced by a commutative ring $k$, as long as
some rather weak conditions hold (for instance, it is enough that
$\mathfrak{n}_{-}$, $\mathfrak{n}_{+}$ and $\mathfrak{h}$ are free $k$-modules).
\end{remark}

\begin{definition}
\label{def.invform}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra,
and $\lambda\in\mathfrak{h}^{\ast}$. According to Proposition
\ref{prop.invform} \textbf{(a)}, there exists a unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$ (where we denote this bilinear form by $\left(  \cdot
,\cdot\right)  $). This form is going to be denoted by $\left(  \cdot
,\cdot\right)  _{\lambda}$ (to stress its dependency on $\lambda$). (Later we
will also denote this form by $\left(  \cdot,\cdot\right)  _{\lambda
}^{\mathfrak{g}}$ to point out its dependency on both $\lambda$ and
$\mathfrak{g}$.)
\end{definition}

To prove Proposition \ref{prop.invform}, we recall two facts about modules
over Lie algebras:

\begin{lemma}
\label{lem.pushpull}Let $\mathfrak{a}$ be a Lie algebra, and let
$\mathfrak{b}$ be a Lie subalgebra of $\mathfrak{a}$. Let $V$ be a
$\mathfrak{b}$-module, and $W$ be an $\mathfrak{a}$-module. Then, $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\cong\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ as $\mathfrak{a}$-modules (where the $W$ on the right
hand side is to be understood as $\operatorname*{Res}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}W$). More precisely, there exists a canonical $\mathfrak{a}%
$-module isomorphism $\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}V\right)  \otimes W\rightarrow\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  $ which maps
$\left(  1\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes w$ to
$1\otimes_{U\left(  \mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all
$v\in V$ and $w\in W$.
\end{lemma}

\begin{lemma}
\label{lem.IndRes}Let $\mathfrak{c}$ be a Lie algebra. Let $\mathfrak{a}$ and
$\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$ such that
$\mathfrak{a}+\mathfrak{b}=\mathfrak{c}$. Notice that $\mathfrak{a}%
\cap\mathfrak{b}$ is also a Lie subalgebra of $\mathfrak{c}$. Let $N$ be a
$\mathfrak{b}$-module. Then, $\operatorname*{Ind}\nolimits_{\mathfrak{a}%
\cap\mathfrak{b}}^{\mathfrak{a}}\left(  \operatorname*{Res}%
\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{b}}N\right)
\cong\operatorname*{Res}\nolimits_{\mathfrak{a}}^{\mathfrak{c}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\right)  $ as
$\mathfrak{a}$-modules.
\end{lemma}

We will give two proofs of Lemma \ref{lem.pushpull}: one which is direct and
uses Hopf algebras; the other which is more elementary but less direct.

\textit{First proof of Lemma \ref{lem.pushpull}.} Remember that $U\left(
\mathfrak{a}\right)  $ is a Hopf algebra (a cocommutative one, actually; but
we won't use this). Let us denote its antipode by $S$ and use sumfree Sweedler notation.

Recalling that $\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V=U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{b}\right)  }V$
and $\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  =U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{b}\right)  }\left(  V\otimes W\right)  $, we define a $\mathbb{C}%
$-linear map $\phi:\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}V\right)  \otimes W\rightarrow\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  $ by
$\left(  \alpha\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes
w\mapsto\alpha_{\left(  1\right)  }\otimes_{U\left(  \mathfrak{b}\right)
}\left(  v\otimes S\left(  \alpha_{\left(  2\right)  }\right)  w\right)  $.
This map is easily checked to be well-defined and $\mathfrak{a}$-linear. Also,
we define a $\mathbb{C}$-linear map $\psi:\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)
\rightarrow\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V\right)  \otimes W$ by $\alpha\otimes_{U\left(  \mathfrak{b}\right)
}\left(  v\otimes w\right)  \mapsto\left(  \alpha_{\left(  1\right)  }%
\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes\alpha_{\left(
2\right)  }w$. This map is easily checked to be well-defined. It is also easy
to see that $\phi\circ\psi=\operatorname*{id}$ and $\psi\circ\phi
=\operatorname*{id}$. Hence, $\phi$ and $\psi$ are mutually inverse
isomorphisms between the $\mathfrak{a}$-modules $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W$ and
$\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $. This proves that $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W\cong%
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $ as $\mathfrak{a}$-modules. Moreover, the isomorphism $\phi:\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\rightarrow\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ is canonical and maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$. In other words, Lemma \ref{lem.pushpull} is proven.

\textit{Second proof of Lemma \ref{lem.pushpull}.} For every $\mathfrak{a}%
$-module $Y$, we have%
\begin{align*}
&  \operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W,Y\right) \\
&  =\left(  \underbrace{\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)
\otimes W,Y\right)  }_{\cong\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
}\right)  ^{\mathfrak{a}}\\
&  \cong\left(  \operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
\right)  ^{\mathfrak{a}}=\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{b}}\left(
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right) \\
&  =\left(  \underbrace{\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
}_{\cong\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  V\otimes W,Y\right)
}\right)  ^{\mathfrak{b}}\cong\left(  \operatorname*{Hom}\nolimits_{\mathbb{C}%
}\left(  V\otimes W,Y\right)  \right)  ^{\mathfrak{b}}\\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{b}}\left(  V\otimes W,Y\right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  ,Y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right)  .
\end{align*}
Since this isomorphism is canonical, it gives us a natural isomorphism between
the functors $\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W,-\right)  $ and $\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  ,-\right)  $. By Yoneda's lemma, this yields that $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\cong\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ as $\mathfrak{a}$-modules. It is also rather clear that
the $\mathfrak{a}$-module isomorphism $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W\rightarrow
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $ we have just obtained is canonical.

In order to check that this isomorphism maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$, we must retrace the proof of Yoneda's lemma. This proof proceeds by
evaluating the natural isomorphism $\operatorname*{Hom}\nolimits_{\mathfrak{a}%
}\left(  \left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V\right)  \otimes W,-\right)  \rightarrow\operatorname*{Hom}%
\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}\left(  V\otimes W\right)  ,-\right)  $ at the object
$\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $, thus obtaining an isomorphism%
\[
\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W,\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  \right)
\rightarrow\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  ,\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  \right)  ,
\]
and taking the preimage of $\operatorname*{id}\in\operatorname*{Hom}%
\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}\left(  V\otimes W\right)  ,\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  \right)  $
under this isomorphism. This preimage is our isomorphism $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\rightarrow\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $. Checking that this maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$ is a matter of routine now, and left to the reader. Lemma
\ref{lem.pushpull} is thus proven.

\textit{Proof of Lemma \ref{lem.IndRes}.} Let $\rho:U\left(  \mathfrak{a}%
\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \rightarrow U\left(  \mathfrak{c}\right)  $ be the
$\mathbb{C}$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
U\left(  \mathfrak{a}\right)  \text{ and }\beta\in U\left(  \mathfrak{b}%
\right)
\]
(this is clearly well-defined). By Proposition \ref{prop.U(X)U}, this map
$\rho$ is an isomorphism of left $U\left(  \mathfrak{a}\right)  $-modules and
of right $U\left(  \mathfrak{b}\right)  $-modules. Hence, $U\left(
\mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}U\left(  \mathfrak{b}\right)  \cong U\left(  \mathfrak{c}\right)  $ as left
$U\left(  \mathfrak{a}\right)  $-modules and simultaneously right $U\left(
\mathfrak{b}\right)  $-modules. Now,%
\begin{align*}
\operatorname*{Ind}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}%
}\left(  \operatorname*{Res}\nolimits_{\mathfrak{a}\cap\mathfrak{b}%
}^{\mathfrak{b}}\underbrace{N}_{\cong U\left(  \mathfrak{b}\right)
\otimes_{U\left(  \mathfrak{b}\right)  }N}\right)   &  \cong%
\operatorname*{Ind}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}%
}\left(  \underbrace{\operatorname*{Res}\nolimits_{\mathfrak{a}\cap
\mathfrak{b}}^{\mathfrak{b}}\left(  U\left(  \mathfrak{b}\right)
\otimes_{U\left(  \mathfrak{b}\right)  }N\right)  }_{\substack{=U\left(
\mathfrak{b}\right)  \otimes_{U\left(  \mathfrak{b}\right)  }N\\\text{(as a
}U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \text{-module)}}}\right) \\
&  =\operatorname*{Ind}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}%
}\left(  U\left(  \mathfrak{b}\right)  \otimes_{U\left(  \mathfrak{b}\right)
}N\right)  =U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\left(  U\left(  \mathfrak{b}\right)  \otimes
_{U\left(  \mathfrak{b}\right)  }N\right) \\
&  \cong\underbrace{\left(  U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \right)
}_{\cong U\left(  \mathfrak{c}\right)  }\otimes_{U\left(  \mathfrak{b}\right)
}N\cong U\left(  \mathfrak{c}\right)  \otimes_{U\left(  \mathfrak{b}\right)
}N\\
&  =\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}%
N=\operatorname*{Res}\nolimits_{\mathfrak{a}}^{\mathfrak{c}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\right)
\ \ \ \ \ \ \ \ \ \ \text{as }\mathfrak{a}\text{-modules.}%
\end{align*}
This proves Lemma \ref{lem.IndRes}.

\textit{Proof of Proposition \ref{prop.invform}.} We have $M_{\lambda}%
^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{\lambda}=\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}%
_{\lambda}$. Thus,%
\begin{align*}
&  \operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+}\otimes
M_{-\lambda}^{-},\mathbb{C}\right) \\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  \underbrace{\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}^{\mathfrak{g}}\mathbb{C}_{\lambda}\right)  \otimes M_{-\lambda}^{-}%
}_{\substack{\cong\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus
\mathfrak{n}_{+}}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes
M_{-\lambda}^{-}\right)  \\\text{(by Lemma \ref{lem.pushpull})}}%
},\mathbb{C}\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes M_{-\lambda}^{-}\right)
,\mathbb{C}\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}\left(  \mathbb{C}_{\lambda}\otimes\underbrace{M_{-\lambda}^{-}%
}_{\substack{=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{-\lambda}\\=\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\mathbb{C}%
_{-\lambda}}},\mathbb{C}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
Frobenius reciprocity}\right) \\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\underbrace{\mathbb{C}_{\lambda}\otimes\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\mathbb{C}%
_{-\lambda}\right)  }_{\substack{\cong\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right)  \\\text{(by Lemma
\ref{lem.pushpull})}}},\mathbb{C}\right)  \cong\operatorname*{Hom}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right)  ,\mathbb{C}\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{h}}^{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}\otimes\mathbb{C}%
_{-\lambda}\right)  ,\mathbb{C}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Lemma \ref{lem.IndRes} (applied to }\mathfrak{c}=\mathfrak{g}%
\text{, }\mathfrak{a}=\mathfrak{h}\oplus\mathfrak{n}_{+}\text{, }%
\mathfrak{b}=\mathfrak{h}\oplus\mathfrak{n}_{-}\text{ and }N=\mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda}\text{)}\\
\text{yields }\operatorname*{Ind}\nolimits_{\mathfrak{h}}^{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \operatorname*{Res}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{-}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \right)  \cong\operatorname*{Res}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}%
}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda
}\right)  \right)  \text{,}\\
\text{which rewrites as }\operatorname*{Ind}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \cong\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right) \\
\text{ (since we are suppressing the }\operatorname*{Res}\text{ functors),}\\
\text{ so that }\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus
\mathfrak{n}_{-}}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes
\mathbb{C}_{-\lambda}\right)  \cong\operatorname*{Ind}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \text{ (as }\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  \text{-modules)}%
\end{array}
\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}}\left(  \mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda},\mathbb{C}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right) \\
&  \cong\mathbb{C}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda}\cong\mathbb{C}\text{ as }%
\mathfrak{h}\text{-modules (this is easy to see)}\right)  .
\end{align*}
This isomorphism $\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(
M_{\lambda}^{+}\otimes M_{-\lambda}^{-},\mathbb{C}\right)  \rightarrow
\mathbb{C}$ is easily seen to map every $\mathfrak{g}$-invariant bilinear form
$\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ (seen as a linear map $M_{\lambda}^{+}\otimes
M_{-\lambda}^{-}\rightarrow\mathbb{C}$) to the value $\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  $. Hence, there exists a unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$ (where we denote this bilinear form by $\left(  \cdot
,\cdot\right)  $), and every other $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ must be a scalar
multiple of this one. This proves Proposition \ref{prop.invform} \textbf{(a)}
and \textbf{(c)}.

Now, for the proof of \textbf{(b)}: Denote by $\left(  \cdot,\cdot\right)  $
the unique $\mathfrak{g}$-invariant bilinear form $M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  =1$. Let us now prove that this bilinear form is
of degree $0$:

Consider the antipode $S:U\left(  \mathfrak{g}\right)  \rightarrow U\left(
\mathfrak{g}\right)  $ of the Hopf algebra $U\left(  \mathfrak{g}\right)  $.
This $S$ is a graded algebra antiautomorphism satisfying $S\left(  x\right)
=-x$ for every $x\in\mathfrak{g}$. It can be explicitly described by
\[
S\left(  x_{1}x_{2}...x_{m}\right)  =\left(  -1\right)  ^{m}x_{m}%
x_{m-1}...x_{1}\ \ \ \ \ \ \ \ \ \ \text{for all }m\in\mathbb{N}\text{ and
}x_{1},x_{2},...,x_{m}\in\mathfrak{g}.
\]


We can easily see by induction (using the $\mathfrak{g}$-invariance of the
bilinear form $\left(  \cdot,\cdot\right)  $) that $\left(  v,aw\right)
=\left(  S\left(  a\right)  v,w\right)  $ for all $v\in M_{\lambda}^{+}$ and
$w\in M_{-\lambda}^{-}$ and $a\in U\left(  \mathfrak{g}\right)  $. In
particular,%
\[
\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(  b\right)
av_{\lambda}^{+},v_{-\lambda}^{-}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a\in U\left(  \mathfrak{g}\right)  \text{ and }b\in U\left(  \mathfrak{g}%
\right)  .
\]
Thus, $\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(
b\right)  av_{\lambda}^{+},v_{-\lambda}^{-}\right)  =0$ whenever $a$ and $b$
are homogeneous elements of $U\left(  \mathfrak{g}\right)  $ satisfying $\deg
b>-\deg a$ (this is because any two homogeneous elements $a$ and $b$ of
$U\left(  \mathfrak{g}\right)  $ satisfying $\deg b>-\deg a$ satisfy $S\left(
b\right)  av_{\lambda}^{+}=0$\ \ \ \ \footnote{\textit{Proof.} Let $a$ and $b$
be homogeneous elements of $U\left(  \mathfrak{g}\right)  $ satisfying $\deg
b>-\deg a$. Then, $\deg b+\deg a>0$, and thus the element $S\left(  b\right)
av_{\lambda}^{+}$ of $M_{\lambda}^{+}$ is a homogeneous element of positive
degree (since $\deg v_{\lambda}^{+}=0$), but the only homogeneous element of
$M_{\lambda}^{+}$ of positive degree is $0$ (since $M_{\lambda}^{+}$ is
concentrated in nonpositive degrees), so that $S\left(  b\right)  av_{\lambda
}^{+}=0$.}). In other words, whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
are integers satisfying $m>-n$, we have $\left(  av_{\lambda}^{+}%
,bv_{-\lambda}^{-}\right)  =0$ for every $a\in U\left(  \mathfrak{g}\right)
\left[  n\right]  $ and $b\in U\left(  \mathfrak{g}\right)  \left[  m\right]
$. Since $M_{\lambda}^{+}\left[  n\right]  =\left\{  av_{\lambda}^{+}%
\ \mid\ a\in U\left(  \mathfrak{g}\right)  \left[  n\right]  \right\}  $ and
$M_{-\lambda}^{-}\left[  m\right]  =\left\{  bv_{-\lambda}^{-}\ \mid\ b\in
U\left(  \mathfrak{g}\right)  \left[  m\right]  \right\}  $, this rewrites as
follows: Whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ are integers
satisfying $m>-n$, we have $\left(  M_{\lambda}^{+}\left[  n\right]
,M_{-\lambda}^{-}\left[  m\right]  \right)  =0$.

Similarly, using the formula $\left(  av,w\right)  =\left(  v,S\left(
a\right)  w\right)  $ (which holds for all $v\in M_{\lambda}^{+}$ and $w\in
M_{-\lambda}^{-}$ and $a\in U\left(  \mathfrak{g}\right)  $), we can show that
whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ are integers satisfying $m<-n$,
we have $\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}^{-}\left[
m\right]  \right)  =0$.

Thus we have $\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}%
^{-}\left[  m\right]  \right)  =0$ whenever $m>-n$ and whenever $m<-n$. Hence,
$\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}^{-}\left[  m\right]
\right)  $ can only be nonzero when $m=-n$. In other words, the form $\left(
\cdot,\cdot\right)  $ has degree $0$. This proves Proposition
\ref{prop.invform}. In this proof, we have not used any properties of
$\mathbb{C}$ other than being a commutative ring over which $\mathfrak{n}_{-}%
$, $\mathfrak{n}_{+}$ and $\mathfrak{h}$ are free modules (the latter was only
used for applying consequences of Poincar\'{e}-Birkhoff-Witt); we thus have
also verified Remark \ref{rmk.invform.1}.

\subsubsection{Generic nondegeneracy: Statement of the fact}

We will later (Theorem \ref{thm.verma}) see that the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ is nondegenerate if and only if the $\mathfrak{g}%
$-module $M_{\lambda}^{+}$ is irreducible. This makes the question of when the
form $\left(  \cdot,\cdot\right)  _{\lambda}$ is nondegenerate an important
question to study. It can, in many concrete cases, be answered by
combinatorial computations. But let us first give a general result about how
it is nondegenerate ``if $\lambda$ is in sufficiently general position'':

\begin{theorem}
\label{thm.invformnondeg}Assume that $\mathfrak{g}$ is a nondegenerate
$\mathbb{Z}$-graded Lie algebra.

Let $\left(  \cdot,\cdot\right)  $ be the form $\left(  \cdot,\cdot\right)
_{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$. (In
other words, let $\left(  \cdot,\cdot\right)  $ be the unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$. Such a form exists and is unique by Proposition
\ref{prop.invform} \textbf{(a)}.)

In every degree, the form $\left(  \cdot,\cdot\right)  $ is nondegenerate for
generic $\lambda$. More precisely: For every $n\in\mathbb{N}$, the restriction
of the form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda
}^{-}\rightarrow\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times
M_{-\lambda}^{-}\left[  n\right]  $ is nondegenerate for generic $\lambda$.

(What ``generic $\lambda$'' means here may depend on the degree. Thus, we
cannot claim that ``for generic $\lambda$, the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate in every degree''!)
\end{theorem}

The proof of this theorem will occupy the rest of Section
\ref{subsect.invform}. While the statement of Theorem \ref{thm.invformnondeg}
itself will never be used in this text, the proof involves several useful
ideas and provides good examples of how to work with Verma modules
computationally; moreover, the main auxiliary result (Proposition
\ref{prop.det.US}) will be used later in the text.

\textbf{[Note: The below proof has been written at nighttime and not been
checked for mistakes. It also has not been checked for redundancies and
readability.]}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Casting bilinear
forms on coinvariant spaces}

Before we start with the proof, a general fact from representation theory:

\begin{lemma}
\label{lem.bilform}Let $k$ be a field, and let $G$ be a finite group. Let
$\Lambda\in k\left[  G\right]  $ be the element $\sum\limits_{g\in G}g$.

Let $V$ and $W$ be representations of $G$ over $k$. Let $B:V\times
W\rightarrow k$ be a $G$-invariant bilinear form.

\textbf{(a)} Then, there exists one and only one bilinear form $B^{\prime
}:V_{G}\times W_{G}\rightarrow k$ satisfying%
\[
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W\text{.}%
\]
(Here, $\overline{v}$ denotes the projection of $v$ onto $V_{G}$, and
$\overline{w}$ denotes the projection of $w$ onto $W_{G}$.)

\textbf{(b)} Assume that $\left\vert G\right\vert $ is invertible in $k$ (in
other words, assume that $\operatorname*{char}k$ is either $0$ or coprime to
$\left\vert G\right\vert $). If the form $B$ is nondegenerate, then the form
$B^{\prime}$ constructed in Lemma \ref{lem.bilform} \textbf{(a)} is
nondegenerate, too.
\end{lemma}

\textit{Proof of Lemma \ref{lem.bilform}.} Every $h\in G$ satisfies%
\begin{align*}
h\Lambda &  =h\sum\limits_{g\in G}g\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\Lambda=\sum\limits_{g\in G}g\right) \\
&  =\sum\limits_{g\in G}hg=\sum\limits_{i\in G}i\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }i\text{ for }hg\text{ in the sum, since the map}\\
G\rightarrow G,\ g\mapsto hg\text{ is a bijection}%
\end{array}
\right) \\
&  =\sum\limits_{g\in G}g=\Lambda
\end{align*}
and similarly $\Lambda h=\Lambda$.

Also,%
\begin{align*}
\sum\limits_{g\in G}g^{-1}  &  =\sum\limits_{g\in G}%
g\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }g\text{ for }g^{-1}\text{ in the sum, since the
map}\\
G\rightarrow G,\ g\mapsto g^{-1}\text{ is a bijection}%
\end{array}
\right) \\
&  =\Lambda.
\end{align*}


We further notice that the group $G$ acts trivially on the $G$-modules $k$ and
$W_{G}$ (this follows from the definitions of these modules), and thus $G$
acts trivially on $\operatorname*{Hom}\left(  W_{G},k\right)  $ as well.

For every $v\in V$, the map%
\[
W\rightarrow k,\ \ \ \ \ \ \ \ \ \ w\mapsto B\left(  \Lambda v,w\right)
\]
is clearly $G$-equivariant (since it maps $hw$ to%
\begin{align*}
B\left(  \underbrace{\Lambda}_{=h\Lambda}v,hw\right)   &  =B\left(  h\Lambda
v,hw\right)  =B\left(  \Lambda v,w\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }B\text{ is }G\text{-invariant}\right) \\
&  =hB\left(  \Lambda v,w\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}G\text{ acts trivially on }k\right)
\end{align*}
for every $h\in G$ and $w\in W$), and thus descends to a map%
\[
W_{G}\rightarrow k_{G},\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto\overline
{B\left(  \Lambda v,w\right)  }.
\]
Hence, we have obtained a map%
\[
V\rightarrow\operatorname*{Hom}\left(  W_{G},k_{G}\right)
,\ \ \ \ \ \ \ \ \ \ v\mapsto\left(  \overline{w}\mapsto\overline{B\left(
\Lambda v,w\right)  }\right)  .
\]
Since $k_{G}=k$ (because $G$ acts trivially on $k$), this rewrites as a map%
\[
V\rightarrow\operatorname*{Hom}\left(  W_{G},k\right)
,\ \ \ \ \ \ \ \ \ \ v\mapsto\left(  \overline{w}\mapsto B\left(  \Lambda
v,w\right)  \right)  .
\]


This map, too, is $G$-equivariant (since it maps $hv$ to the map%
\begin{align*}
&  \left(  W_{G}\rightarrow k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto B\left(
\underbrace{\Lambda h}_{=\Lambda}v,w\right)  \right) \\
&  =\left(  W_{G}\rightarrow k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto
B\left(  \Lambda v,w\right)  \right)  =h\left(  W_{G}\rightarrow
k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto B\left(  \Lambda v,w\right)  \right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }G\text{ acts trivially on
}\operatorname*{Hom}\left(  W_{G},k\right)  \right)
\end{align*}
for every $h\in G$ and $v\in V$). Thus, it descends to a map%
\[
V_{G}\rightarrow\left(  \operatorname*{Hom}\left(  W_{G},k\right)  \right)
_{G},\ \ \ \ \ \ \ \ \ \ \overline{v}\mapsto\overline{\left(  \overline
{w}\mapsto B\left(  \Lambda v,w\right)  \right)  }.
\]
Since $\left(  \operatorname*{Hom}\left(  W_{G},k\right)  \right)
_{G}=\operatorname*{Hom}\left(  W_{G},k\right)  $ (because $G$ acts trivially
on $\operatorname*{Hom}\left(  W_{G},k\right)  $), this rewrites as a map%
\[
V_{G}\rightarrow\operatorname*{Hom}\left(  W_{G},k\right)
,\ \ \ \ \ \ \ \ \ \ \overline{v}\mapsto\left(  \overline{w}\mapsto B\left(
\Lambda v,w\right)  \right)  .
\]


This map can be rewritten as a bilinear form $V_{G}\times W_{G}\rightarrow k$
which maps $\left(  \overline{v},\overline{w}\right)  $ to $B\left(  \Lambda
v,w\right)  $ for all $v\in V$ and $w\in W$. Since
\begin{align*}
B\left(  \Lambda v,w\right)   &  =B\left(  \sum\limits_{g\in G}gv,w\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Lambda=\sum\limits_{g\in G}g\right)
\\
&  =\sum\limits_{g\in G}B\left(  gv,\underbrace{w}_{=gg^{-1}w}\right)
=\sum\limits_{g\in G}\underbrace{B\left(  gv,gg^{-1}w\right)  }%
_{\substack{=B\left(  v,g^{-1}w\right)  \\\text{(since }B\text{ is
}G\text{-invariant)}}}=\sum\limits_{g\in G}B\left(  v,g^{-1}w\right) \\
&  =B\left(  v,\underbrace{\sum\limits_{g\in G}g^{-1}}_{=\Lambda}w\right)
=B\left(  v,\Lambda w\right)
\end{align*}
for all $v\in V$ and $w\in W$, we have thus proven that there exists a
bilinear form $B^{\prime}:V_{G}\times W_{G}\rightarrow k$ satisfying%
\[
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W\text{.}%
\]
The uniqueness of such a form is self-evident. This proves Lemma
\ref{lem.bilform} \textbf{(a)}.

\textbf{(b)} Assume that $\left\vert G\right\vert $ is invertible in $k$.
Assume that the form $B$ is nondegenerate. Consider the form $B^{\prime}$
constructed in Lemma \ref{lem.bilform} \textbf{(a)}.

Let $p\in V_{G}$ be such that $B^{\prime}\left(  p,W_{G}\right)  =0$. Since
$p\in V_{G}$, there exists some $v\in V$ such that $p=\overline{v}$. Consider
this $v$. Then, every $w\in W$ satisfies $B\left(  \Lambda v,w\right)  =0$
(since $B\left(  \Lambda v,w\right)  =B^{\prime}\left(  \underbrace{\overline
{v}}_{=p},\underbrace{\overline{w}}_{\in W_{G}}\right)  \in B^{\prime}\left(
p,W_{G}\right)  =0$). Hence, $\Lambda v=0$ (since $B$ is nondegenerate).

But since the projection of $V$ to $V_{G}$ is a $G$-module map, we have
\begin{align*}
\overline{\Lambda v}  &  =\Lambda\overline{v}=\sum\limits_{g\in G}%
\underbrace{g\overline{v}}_{\substack{=\overline{v}\\\text{(since }G\text{
acts}\\\text{trivially on }V_{G}\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Lambda=\sum\limits_{g\in G}g\right) \\
&  =\sum\limits_{g\in G}\overline{v}=\left\vert G\right\vert \overline{v}.
\end{align*}
Since $\left\vert G\right\vert $ is invertible in $k$, this yields
$\overline{v}=\dfrac{1}{\left\vert G\right\vert }\overline{\Lambda v}=0$
(since $\Lambda v=0$), so that $p=\overline{v}=0$.

We have thus shown that every $p\in V_{G}$ such that $B^{\prime}\left(
p,W_{G}\right)  =0$ must satisfy $p=0$. In other words, the form $B^{\prime}$
is nondegenerate. Lemma \ref{lem.bilform} \textbf{(b)} is proven.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The form
\texorpdfstring{$\left(\cdot,\cdot\right)  _{\lambda}^{\circ}$}{induced
by lambda on the k-th symmetric power}}

Let us formulate some standing assumptions:

\begin{Convention}
From now on until the end of Section \ref{subsect.invform}, we let
$\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra, and let $\lambda
\in\mathfrak{h}^{\ast}$. We also require that $\mathfrak{g}_{0}$ is abelian
(this is condition \textbf{(2)} of Definition \ref{def.gradLienondeg}), but we
do \textit{not} require $\mathfrak{g}$ to be nondegenerate (unless we
explicitly state this).
\end{Convention}

As vector spaces, $M_{\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\lambda}^{+}\cong U\left(  \mathfrak{n}_{-}\right)  $ (where the
isomorphism maps $v_{\lambda}^{+}$ to $1$) and $M_{-\lambda}^{-}=U\left(
\mathfrak{n}_{+}\right)  v_{-\lambda}^{-}\cong U\left(  \mathfrak{n}%
_{+}\right)  $ (where the isomorphism maps $v_{-\lambda}^{-}$ to $1$). Thus,
the bilinear form $\left(  \cdot,\cdot\right)  =\left(  \cdot,\cdot\right)
_{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$
corresponds to a bilinear form $U\left(  \mathfrak{n}_{-}\right)  \times
U\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$.

For every $n\in\mathbb{N}$, let $\left(  \cdot,\cdot\right)  _{\lambda,n}$
denote the restriction of our form $\left(  \cdot,\cdot\right)  =\left(
\cdot,\cdot\right)  _{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times
M_{-\lambda}^{-}\left[  n\right]  $. In order to prove Theorem
\ref{thm.invformnondeg}, it is enough to prove that for every $n\in\mathbb{N}%
$, when $\mathfrak{g}$ is nondegenerate, this form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ is nondegenerate for generic $\lambda$.

We now introduce a $\mathbb{C}$-bilinear form, which will turn out to be, in
some sense, the ``highest term'' of the form $\left(  \cdot,\cdot\right)  $
with respect to $\lambda$ (what this exactly means will be explained in
Proposition \ref{prop.det.US}).

\begin{proposition}
\label{prop.lambda_k}For every $k\in\mathbb{N}$, there exists one and only one
$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ by
\begin{align}
\lambda_{k}\left(  \alpha_{1}\alpha_{2}...\alpha_{k},\beta_{1}\beta
_{2}...\beta_{k}\right)   &  =\sum\limits_{\sigma\in S_{k}}\lambda\left(
\left[  \alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)
\lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  \alpha_{k},\beta_{\sigma\left(  k\right)
}\right]  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }\alpha_{1},\alpha_{2},...,\alpha_{k}%
\in\mathfrak{n}_{-}\text{ and }\beta_{1},\beta_{2},...,\beta_{k}%
\in\mathfrak{n}_{+}. \label{thm.invformnondeg.pf.lambda}%
\end{align}

\end{proposition}

Here, we are using the following convention:

\begin{Convention}
From now on until the end of Section \ref{subsect.invform}, the map
$\lambda:\mathfrak{g}_{0}\rightarrow\mathbb{C}$ is extended to a linear map
$\lambda:\mathfrak{g}\rightarrow\mathbb{C}$ by composing it with the canonical
projection $\mathfrak{g}\rightarrow\mathfrak{g}_{0}$.
\end{Convention}

\textit{First proof of Proposition \ref{prop.lambda_k} (sketched).} Let
$k\in\mathbb{N}$. The value of
\[
\sum\limits_{\sigma\in S_{k}}\lambda\left(  \left[  \alpha_{1},\beta
_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[  \alpha
_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)
\]
depends linearly on each of the $\alpha_{1},\alpha_{2},...,\alpha_{k}$ and
$\beta_{1},\beta_{2},...,\beta_{k}$, and is invariant under any permutation of
the $\alpha_{1},\alpha_{2},...,\alpha_{k}$ and under any permutation of the
$\beta_{1},\beta_{2},...,\beta_{k}$ (as is easily checked). This readily shows
that we can indeed define a $\mathbb{C}$-bilinear form $\lambda_{k}%
:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$ by (\ref{thm.invformnondeg.pf.lambda}).
This proves Proposition \ref{prop.lambda_k}.

\textit{Second proof of Proposition \ref{prop.lambda_k}.} Let $G=S_{k}$. Let
$\Lambda\in\mathbb{C}\left[  G\right]  $ be the element $\sum\limits_{g\in
S_{k}}g=\sum\limits_{\sigma\in S_{k}}\sigma=\sum\limits_{\sigma\in S_{k}%
}\sigma^{-1}$. Let $V$ and $W$ be the canonical representations $\mathfrak{n}%
_{-}^{\otimes k}$ and $\mathfrak{n}_{+}^{\otimes k}$ of $S_{k}$ (where $S_{k}$
acts by permuting the tensorands). Let $B:V\times W\rightarrow\mathbb{C}$ be
the $\mathbb{C}$-bilinear form defined as the $k$-th tensor power of the
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}_{+}%
\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  $. It is easy to see that this form is
$S_{k}$-invariant (in fact, more generally, the $k$-th tensor power of any
bilinear form is $S_{k}$-invariant). Thus, Lemma \ref{lem.bilform}
\textbf{(a)} (applied to $\mathbb{C}$ instead of $k$) yields that there exists
one and only one bilinear form $B^{\prime}:V_{G}\times W_{G}\rightarrow
\mathbb{C}$ satisfying%
\begin{equation}
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W \label{thm.invformnondeg.pf.B'}%
\end{equation}
(where $\overline{v}$ denotes the projection of $v$ onto $V_{G}=V_{S_{k}%
}=S^{k}\left(  \mathfrak{n}_{-}\right)  $, and $\overline{w}$ denotes the
projection of $w$ onto $W_{G}=W_{S_{k}}=S^{k}\left(  \mathfrak{n}_{+}\right)
$). Consider this form $B^{\prime}$. All $\alpha_{1},\alpha_{2},...,\alpha
_{k}\in\mathfrak{n}_{-}$ and $\beta_{1},\beta_{2},...,\beta_{k}\in
\mathfrak{n}_{+}$ satisfy%
\begin{align*}
&  B^{\prime}\left(  \alpha_{1}\alpha_{2}...\alpha_{k},\beta_{1}\beta
_{2}...\beta_{k}\right) \\
&  =B^{\prime}\left(  \overline{\alpha_{1}\otimes\alpha_{2}\otimes
...\otimes\alpha_{k}},\overline{\beta_{1}\otimes\beta_{2}\otimes
...\otimes\beta_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha_{1}\alpha_{2}...\alpha
_{k}=\overline{\alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}}\text{
and }\beta_{1}\beta_{2}...\beta_{k}=\overline{\beta_{1}\otimes\beta_{2}%
\otimes...\otimes\beta_{k}}\right) \\
&  =B\left(  \alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}%
,\Lambda\left(  \beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.invformnondeg.pf.B'}),
applied to }v=\alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}\text{ and
}w=\beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right) \\
&  =B\left(  \alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}%
,\sum\limits_{\sigma\in S_{k}}\beta_{\sigma\left(  1\right)  }\otimes
\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\Lambda=\sum\limits_{\sigma\in S_{k}}\sigma^{-1}\text{ yields
}\Lambda\left(  \beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)
=\sum\limits_{\sigma\in S_{k}}\underbrace{\sigma^{-1}\left(  \beta_{1}%
\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)  }_{=\beta_{\sigma\left(
1\right)  }\otimes\beta_{\sigma\left(  2\right)  }\otimes...\otimes
\beta_{\sigma\left(  k\right)  }}\\
=\sum\limits_{\sigma\in S_{k}}\beta_{\sigma\left(  1\right)  }\otimes
\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }%
\end{array}
\right) \\
&  =\sum\limits_{\sigma\in S_{k}}\underbrace{B\left(  \alpha_{1}\otimes
\alpha_{2}\otimes...\otimes\alpha_{k},\beta_{\sigma\left(  1\right)  }%
\otimes\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }\right)  }_{\substack{=\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)
\\\text{(since }B\text{ is the }k\text{-th tensor power of the }%
\mathbb{C}\text{-bilinear form }\mathfrak{n}_{-}\times\mathfrak{n}%
_{+}\rightarrow\mathbb{C},\ \left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  \text{)}}}\\
&  =\sum\limits_{\sigma\in S_{k}}\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)  .
\end{align*}
Thus, there exists a $\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(
\mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}_{+}\right)
\rightarrow\mathbb{C}$ satisfying (\ref{thm.invformnondeg.pf.lambda}) (namely,
$B^{\prime}$). On the other hand, there exists \textbf{at most one
}$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ satisfying
(\ref{thm.invformnondeg.pf.lambda})\ \ \ \ \footnote{\textit{Proof.} The
vector space $S^{k}\left(  \mathfrak{n}_{-}\right)  $ is spanned by products
of the form $\alpha_{1}\alpha_{2}...\alpha_{k}$ with $\alpha_{1},\alpha
_{2},...,\alpha_{k}\in\mathfrak{n}_{-}$, whereas the vector space
$S^{k}\left(  \mathfrak{n}_{+}\right)  $ is spanned by products of the form
$\beta_{1}\beta_{2}...\beta_{k}$ with $\beta_{1},\beta_{2},...,\beta_{k}%
\in\mathfrak{n}_{+}$. Hence, the equation (\ref{thm.invformnondeg.pf.lambda})
makes it possible to compute the value of $\lambda_{k}\left(  A,B\right)  $
for any $A\in S^{k}\left(  \mathfrak{n}_{-}\right)  $ and $B\in S^{k}\left(
\mathfrak{n}_{+}\right)  $. Thus, the equation
(\ref{thm.invformnondeg.pf.lambda}) uniquely determines $\lambda_{k}$. In
other words, there exists at most one $\mathbb{C}$-bilinear form $\lambda
_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$ satisfying
(\ref{thm.invformnondeg.pf.lambda}).}. Hence, we can indeed define a
$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ by
(\ref{thm.invformnondeg.pf.lambda}). And, moreover,%
\begin{equation}
\text{this form }\lambda_{k}\text{ is the form }B^{\prime}\text{ satisfying
(\ref{thm.invformnondeg.pf.B'}).} \label{thm.invformnondeg.pf.B'=l_k}%
\end{equation}
Proposition \ref{prop.lambda_k} is thus proven.

\begin{definition}
\label{def.lambda_k}For every $k\in\mathbb{N}$, let $\lambda_{k}:S^{k}\left(
\mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}_{+}\right)
\rightarrow\mathbb{C}$ be the $\mathbb{C}$-bilinear form whose existence and
uniqueness is guaranteed by Proposition \ref{prop.lambda_k}. These forms can
be added together, resulting in a bilinear form $\bigoplus\limits_{k\geq
0}\lambda_{k}:S\left(  \mathfrak{n}_{-}\right)  \times S\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$. It is very easy to see that this form is
of degree $0$ (where the grading on $S\left(  \mathfrak{n}_{-}\right)  $ and
$S\left(  \mathfrak{n}_{+}\right)  $ is not the one that gives the $k$-th
symmetric power the degree $k$ for every $k\in\mathbb{N}$, but is the one
induced by the grading on $\mathfrak{n}_{-}$ and $\mathfrak{n}_{+}$). Denote
this form by $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$.
\end{definition}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Generic nondegeneracy
of \texorpdfstring{$\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$}{the
form induced by lambda on the symmetric power}}

\begin{lemma}
\label{lem.lambda_k}Let $\lambda\in\mathfrak{h}^{\ast}$ be such that the
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}_{+}%
\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  $ is nondegenerate. Then, the form
$\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$ is nondegenerate.
\end{lemma}

\textit{Proof of Lemma \ref{lem.lambda_k}.} Let $k\in\mathbb{N}$. Introduce
the same notations as in the Second proof of Proposition \ref{prop.lambda_k}.

The $\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}%
_{+}\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto
\lambda\left(  \left[  \alpha,\beta\right]  \right)  $ is nondegenerate. Thus,
the $k$-th tensor power of this form is also nondegenerate (since all tensor
powers of a nondegenerate form are always nondegenerate). But the $k$-th
tensor power of this form is $B$. Thus, $B$ is nondegenerate. Hence, Lemma
\ref{lem.bilform} \textbf{(b)} yields that the form $B^{\prime}$ is
nondegenerate. Due to (\ref{thm.invformnondeg.pf.B'=l_k}), this yields that
the form $\lambda_{k}$ is nondegenerate.

Forget that we fixed $k$. We thus have shown that for every $k\in\mathbb{N}$,
the form $\lambda_{k}$ is nondegenerate. Thus, the direct sum $\bigoplus
\limits_{k\geq0}\lambda_{k}$ of these forms is also nondegenerate. Since
$\bigoplus\limits_{k\geq0}\lambda_{k}=\left(  \cdot,\cdot\right)  _{\lambda
}^{\circ}$, this yields that $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$
is nondegenerate. This proves Lemma \ref{lem.lambda_k}.

For every $n\in\mathbb{N}$, define $\left(  \cdot,\cdot\right)  _{\lambda
,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \times
S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  \rightarrow\mathbb{C}$ to
be the restriction of this form $\left(  \cdot,\cdot\right)  _{\lambda}%
^{\circ}=\bigoplus\limits_{k\geq0}\lambda_{k}:S\left(  \mathfrak{n}%
_{-}\right)  \times S\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$
to $S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \times S\left(
\mathfrak{n}_{+}\right)  \left[  n\right]  $. We now need the following
strengthening of Lemma \ref{lem.lambda_k}:

\begin{lemma}
\label{lem.lambda_k.2}Let $n\in\mathbb{N}$ and $\lambda\in\mathfrak{h}^{\ast}$
be such that the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $. Then, the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must also be nondegenerate.
\end{lemma}

\textit{Proof of Lemma \ref{lem.lambda_k.2}.} For Lemma \ref{lem.lambda_k} to
hold, we did not need $\mathfrak{g}$ to be a graded Lie algebra; we only
needed that $\mathfrak{g}$ is a graded vector space with a well-defined
bilinear map $\left[  \cdot,\cdot\right]  :\mathfrak{g}_{-k}\times
\mathfrak{g}_{k}\rightarrow\mathfrak{g}_{0}$ for every positive integer $k$.
This is a rather weak condition, and holds not only for $\mathfrak{g}$, but
also for the graded subspace $\mathfrak{g}_{-n}\oplus\mathfrak{g}_{-n+1}%
\oplus...\oplus\mathfrak{g}_{n}$ of $\mathfrak{g}$. Denote this graded
subspace $\mathfrak{g}_{-n}\oplus\mathfrak{g}_{-n+1}\oplus...\oplus
\mathfrak{g}_{n}$ by $\mathfrak{g}^{\prime}$, and let $\mathfrak{n}%
_{-}^{\prime}\oplus\mathfrak{h}^{\prime}\oplus\mathfrak{n}_{+}^{\prime}$ be
its triangular decomposition (thus, $\mathfrak{n}_{-}^{\prime}=\mathfrak{g}%
_{-n}\oplus\mathfrak{g}_{-n+1}\oplus...\oplus\mathfrak{g}_{-1}$,
$\mathfrak{h}^{\prime}=\mathfrak{g}_{0}=\mathfrak{h}$ and $\mathfrak{n}%
_{+}^{\prime}=\mathfrak{g}_{1}\oplus\mathfrak{g}_{2}\oplus...\oplus
\mathfrak{g}_{n}$). The $\mathbb{C}$-bilinear form $\mathfrak{n}_{-}^{\prime
}\times\mathfrak{n}_{+}^{\prime}\rightarrow\mathbb{C},$ $\left(  \alpha
,\beta\right)  \mapsto\lambda\left(  \left[  \alpha,\beta\right]  \right)  $
is nondegenerate (because the bilinear form $\mathfrak{g}_{-k}\times
\mathfrak{g}_{k}\rightarrow\mathbb{C},\ \left(  a,b\right)  \mapsto
\lambda\left(  \left[  a,b\right]  \right)  $ is nondegenerate for every
$k\in\left\{  1,2,...,n\right\}  $). Hence, by Lemma \ref{lem.lambda_k}, the
form $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$ \textit{defined for
}$\mathfrak{g}^{\prime}$ \textit{instead of }$\mathfrak{g}$ is nondegenerate.
Since this form is of degree $0$, the restriction $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}$ of this form to $S\left(  \mathfrak{n}_{-}^{\prime
}\right)  \left[  -n\right]  \times S\left(  \mathfrak{n}_{+}^{\prime}\right)
\left[  n\right]  $ must also be nondegenerate\footnote{This is because if $V$
and $W$ are two graded vector spaces, and $\phi:V\times W\rightarrow
\mathbb{C}$ is a nondegenerate bilinear form of degree $0$, then for every
$n\in\mathbb{Z}$, the restriction of $\phi$ to $V\left[  -n\right]  \times
W\left[  n\right]  $ must also be nondegenerate.}. But since $S\left(
\mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]  =S\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  $\ \ \ \ \footnote{\textit{Proof.} Since
$\mathfrak{n}_{+}=\sum\limits_{i\geq1}\mathfrak{g}_{i}$, we have $S\left(
\mathfrak{n}_{+}\right)  =\sum\limits_{k\in\mathbb{N}}\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{k}\right)  \in\mathbb{N}%
^{k};\\\text{each }i_{j}\geq1}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}%
}...\mathfrak{g}_{i_{k}}$ and thus%
\[
S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  =\sum\limits_{k\in
\mathbb{N}}\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{k}\right)
\in\mathbb{N}^{k};\\\text{each }i_{j}\geq1;\\i_{1}+i_{2}+...+i_{k}%
=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
\]
(since $\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}%
}\subseteq S\left(  \mathfrak{n}_{+}\right)  \left[  i_{1}+i_{2}%
+...+i_{k}\right]  $ for all $\left(  i_{1},i_{2},...,i_{k}\right)
\in\mathbb{N}^{k}$). Similarly,%
\[
S\left(  \mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]  =\sum
\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq1;\\\text{each
}\left\vert i_{j}\right\vert \leq n;\\i_{1}+i_{2}+...+i_{k}=n}}\mathfrak{g}%
_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
\]
(because $\mathfrak{g}^{\prime}$ is obtained from $\mathfrak{g}$ by removing
all $\mathfrak{g}_{i}$ with $\left\vert i\right\vert >n$). Thus,%
\begin{align*}
S\left(  \mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]   &
=\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq
1;\\\text{each }\left\vert i_{j}\right\vert \leq n;\\i_{1}+i_{2}+...+i_{k}%
=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
=\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq
1;\\i_{1}+i_{2}+...+i_{k}=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}%
}...\mathfrak{g}_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we removed the condition }\left(  \text{each }\left\vert
i_{j}\right\vert \leq n\right)  \text{, because it was redundant}\\
\text{(since every }\left(  i_{1},i_{2},...,i_{k}\right)  \in\mathbb{N}%
^{k}\text{ satisfying }i_{1}+i_{2}+...+i_{k}=n\text{ automatically}\\
\text{satisfies }\left(  \text{each }\left\vert i_{j}\right\vert \leq
n\right)  \text{)}%
\end{array}
\right) \\
&  =S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  ,
\end{align*}
qed.} and $S\left(  \mathfrak{n}_{-}^{\prime}\right)  \left[  -n\right]
=S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  $\ \ \ \ \footnote{for
analogous reasons}, this restriction is exactly our form $\left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  \times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
\rightarrow\mathbb{C}$ (in fact, the form is clearly given by the same
formula). Thus we have shown that our form $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]
\times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  \rightarrow
\mathbb{C}$ is nondegenerate. Lemma \ref{lem.lambda_k.2} is proven.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}:
\texorpdfstring{$\left(  \cdot ,\cdot\right)  _{\lambda}^{\circ}$}{the form
induced by lambda} is the ``highest term'' of \texorpdfstring{$\left(
\cdot,\cdot\right)  _{\lambda}$}{the invariant bilinear form}}

Before we go on, let us sketch the direction in which we want to go. We want
to study how, for a fixed $n\in\mathbb{N}$, the form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ changes with $\lambda$. If $V$ and $W$ are two
finite-dimensional vector spaces \textbf{of the same dimension}, and if we
have chosen bases for these two vector spaces $V$ and $W$, then we can
represent every bilinear form $V\times W\rightarrow\mathbb{C}$ as a square
matrix with respect to these two bases, and the bilinear form is nondegenerate
if and only if this matrix has nonzero determinant. This suggests that we
study how the determinant $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ of the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$
with respect to some bases of $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ changes with $\lambda$ (and, in
particular, show that this determinant is nonzero for generic $\lambda$ when
$\mathfrak{g}$ is nondegenerate). Of course, speaking of this determinant
$\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $ only makes
sense when the bases of $M_{\lambda}^{+}\left[  -n\right]  $ and $M_{-\lambda
}^{-}\left[  n\right]  $ have the same size (since only square matrices have
determinants), but this is automatically satisfied if we have $\dim\left(
\mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}_{-n}\right)  $ for every
integer $n>0$ (this condition is automatically satisfied when $\mathfrak{g}$
is a nondegenerate $\mathbb{Z}$-graded Lie algebra, but of course not only then).

Unfortunately, the spaces $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ themselves change with $\lambda$. Thus,
if we want to pick some bases of $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ for all $\lambda\in\mathfrak{h}^{\ast}$,
we have to pick new bases \textbf{for every }$\lambda$. If we just pick these
bases randomly, then the determinant $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ can change very unpredictably (because the determinant
depends on the choice of bases). Thus, if we want to say something interesting
about how $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $
changes with $\lambda$, then we should specify a reasonable choice of bases
for all $\lambda$. Fortunately, this is not difficult: It is enough to choose
Poincar\'{e}-Birkhoff-Witt bases for $U\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  $ and $U\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
$, and thus obtain bases $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ due to the isomorphisms $M_{\lambda}%
^{+}\left[  -n\right]  \cong U\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  \cong U\left(
\mathfrak{n}_{+}\right)  \left[  n\right]  $. (See Convention
\ref{conv.invformnondeg.bases} for details.) With bases chosen this way, the
determinant $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $
will depend on $\lambda$ polynomially, and we will be able to conclude some
useful properties of this polynomial.

So much for our roadmap. Let us first make a convention:

\begin{Convention}
If $V$ and $W$ are two finite-dimensional vector spaces \textbf{of the same
dimension}, and if we have chosen bases for these two vector spaces $V$ and
$W$, then we can represent every bilinear form $B:V\times W\rightarrow
\mathbb{C}$ as a square matrix with respect to these two bases. The
determinant of this matrix will be denoted by $\det B$ and called the
\textit{determinant of the form }$B$. Of course, this determinant $\det B$
depends on the bases chosen. A change of either basis induces a scaling of
$\det B$ by a \textbf{nonzero} scalar. Thus, while the determinant $\det B$
itself depends on the choice of bases, the property of $\det B$ to be zero or
nonzero does \textbf{not} depend on the choice of bases.
\end{Convention}

Let us now look at how the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$ and
its determinant $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
$ depend on $\lambda$. We want to show that this dependence is polynomial. In
order to make sense of this, let us define what we mean by ``polynomial'' here:

\begin{definition}
\label{def.det.US.poly}Let $V$ be a finite-dimensional vector space. A
function $\phi:V\rightarrow\mathbb{C}$ is said to be a \textit{polynomial
function} (or just to be \textit{polynomial} -- but this is not the same as
being \textit{a polynomial}) if one of the following equivalent conditions holds:

\textbf{(1)} There exist a basis $\left(  \beta_{1},\beta_{2},...,\beta
_{m}\right)  $ of the dual space $V^{\ast}$ and a polynomial $P\in
\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]


\textbf{(2)} For every basis $\left(  \beta_{1},\beta_{2},...,\beta
_{m}\right)  $ of the dual space $V^{\ast}$, there exists a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]


\textbf{(3)} There exist finitely many elements $\beta_{1}$, $\beta_{2}$,
$...$, $\beta_{m}$ of the dual space $V^{\ast}$ and a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]

\end{definition}

Note that this is exactly the meaning of the word ``polynomial function'' that
is used in Classical Invariant Theory. In our case (where the field is
$\mathbb{C}$), polynomial functions $V\rightarrow\mathbb{C}$ can be identified
with elements of the symmetric algebra $\operatorname*{S}\left(  V^{\ast
}\right)  $, and in some sense are an ``obsoleted version'' of the
latter.\footnote{The identification of polynomial functions $V\rightarrow
\mathbb{C}$ with elements of the symmetric algebra $\operatorname*{S}\left(
V^{\ast}\right)  $ works similarly over any \textit{infinite} field instead of
$\mathbb{C}$. It breaks down over finite fields, however (because different
elements of $\operatorname*{S}\left(  V^{\ast}\right)  $ may correspond to the
same polynomial function over a finite field).} For our goals, however,
polynomial functions are enough. Let us define the notion of
\textit{homogeneous polynomial functions}:

\begin{definition}
\label{def.det.US.poly.hom}Let $V$ be a finite-dimensional vector space.

\textbf{(a)} Let $n\in\mathbb{N}$. A polynomial function $\phi:V\rightarrow
\mathbb{C}$ is said to be \textit{homogeneous of degree }$n$ if and only if%
\[
\text{every }v\in V\text{ and every }\lambda\in\mathbb{C}\text{ satisfy }%
\phi\left(  \lambda v\right)  =\lambda^{n}\phi\left(  v\right)  .
\]


\textbf{(b)} A polynomial function $\phi:V\rightarrow\mathbb{C}$ is said to be
\textit{homogeneous} if and only if there exists some $n\in\mathbb{N}$ such
that $\phi$ is homogeneous of degree $n$.

\textbf{(c)} It is easy to see that for every polynomial function
$\phi:V\rightarrow\mathbb{C}$, there exists a unique sequence $\left(
\phi_{n}\right)  _{n\in\mathbb{N}}$ of polynomial functions $\phi
_{n}:V\rightarrow\mathbb{C}$ such that all but finitely many $n\in\mathbb{N}$
satisfy $\phi_{n}=0$, such that $\phi_{n}$ is homogeneous of degree $n$ for
every $n\in\mathbb{N}$, and such that $\phi=\sum\limits_{n\in\mathbb{N}}%
\phi_{n}$. This sequence is said to be the \textit{graded decomposition} of
$\phi$. For every $n\in\mathbb{N}$, its member $\phi_{n}$ is called the
$n$\textit{-th homogeneous component} of $\phi$. If $N$ is the highest
$n\in\mathbb{N}$ such that $\phi_{n}\neq0$, then $\phi_{N}$ is said to be the
\textit{leading term} of $\phi$.
\end{definition}

Note that Definition \ref{def.det.US.poly.hom} \textbf{(c)} defines the
``leading term'' of a polynomial as its highest-degree nonzero homogeneous
component. This ``leading term'' may (and usually will) contain more than one
monomial, so this notion of a ``leading term'' is not the same as the notion
of a ``leading term'' commonly used, e. g., in Gr\"{o}bner basis theory.

We now state the following crucial fact:

\begin{proposition}
\label{prop.det.US}Let $n\in\mathbb{N}$. Assume that $\mathfrak{g}$ is a
nondegenerate $\mathbb{Z}$-graded Lie algebra. As a consequence,
$\dim\mathfrak{h}=\dim\left(  \mathfrak{g}_{0}\right)  \neq\infty$, so that
$\dim\left(  \mathfrak{h}^{\ast}\right)  \neq\infty$, and thus the notion of a
polynomial function $\mathfrak{h}^{\ast}\rightarrow\mathbb{C}$ is well-defined.

There is an appropriate way of choosing bases of the vector spaces $S\left(
\mathfrak{n}_{-}\right)  \left[  -n\right]  $ and $S\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  $ and bases of the vector spaces $M_{\lambda
}^{+}\left[  -n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  $ for all
$\lambda\in\mathfrak{h}^{\ast}$ such that the following holds:

\textbf{(a)} The determinants $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ and $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ (these determinants are defined with respect to
the chosen bases of $S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  $,
$S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  $, $M_{\lambda}%
^{+}\left[  -n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  $) depend
polynomially on $\lambda$. By this, we mean that the functions%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
and%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
are polynomial functions.

\textbf{(b)} The leading term of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]

\end{proposition}

\begin{remark}
We can extend Proposition \ref{prop.det.US} to the case when $\mathfrak{g}$ is
no longer nondegenerate. However, this requires the following changes to
Proposition \ref{prop.det.US}:

Replace the requirement that $\mathfrak{g}$ be nondegenerate by the
requirement that $\mathfrak{g}$ satisfy the conditions \textbf{(1)} and
\textbf{(2)} in Definition \ref{def.gradLienondeg} as well as the condition
that $\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}%
_{-n}\right)  $ for every integer $n>0$ (this condition is a weakening of
condition \textbf{(3)} in Definition \ref{def.gradLienondeg}). Replace the
claim that ``The leading term of the polynomial function $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}\right)  $ is $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  $, up to multiplication by a
nonzero scalar'' by the claim that ``There exists some $k\in\mathbb{N}$ such
that the polynomial function $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ is the $k$-th homogeneous component of the
polynomial function $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda
,n}\right)  $, and such that the $\ell$-th homogeneous component of the
polynomial function $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda
,n}\right)  $ is $0$ for all $\ell>k$''. Note that this does not imply that
$\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  $ is not
identically zero, and indeed $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ can be identically zero.
\end{remark}

Before we prove Proposition \ref{prop.det.US}, let us show how it completes
the proof of Theorem \ref{thm.invformnondeg}:

\textit{Proof of Theorem \ref{thm.invformnondeg}.} Fix a positive
$n\in\mathbb{N}$. For generic $\lambda$, the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $ (because
$\mathfrak{g}$ is nondegenerate). Thus, for generic $\lambda$, the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must also be nondegenerate
(by Lemma \ref{lem.lambda_k.2}), so that $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. Since the leading term of
the polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
(by Proposition \ref{prop.det.US}), this yields that $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}\right)  \neq0$ for generic $\lambda$. In
other words, the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$ is
nondegenerate for generic $\lambda$. But this form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ is exactly the restriction of the form $\left(
\cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow
\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times M_{-\lambda}%
^{-}\left[  n\right]  $. Hence, the restriction of the form $\left(
\cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow
\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times M_{-\lambda}%
^{-}\left[  n\right]  $ is nondegenerate for generic $\lambda$. This proves
Theorem \ref{thm.invformnondeg}.

So all that remains to finish the proof of Theorem \ref{thm.invformnondeg} is
verifying Proposition \ref{prop.det.US}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Polynomial maps}

We already defined the notion of a polynomial function in Definition
\ref{def.det.US.poly}. Let us give a definition of a notion of a ``polynomial
map'' which is tailored for our proof of Theorem \ref{thm.invformnondeg}. I
cannot guarantee that it is the same as what other people call ``polynomial
map'', but it should be very close.

\begin{definition}
\label{def.det.US.polymap}Let $V$ be a finite-dimensional vector space. Let
$W$ be a vector space. A map $\phi:V\rightarrow W$ is said to be a
\textit{polynomial map} if and only if there exist:

- some $n\in\mathbb{N}$;

- $n$ vectors $w_{1}$, $w_{2}$, $...$, $w_{n}$ in $W$;

- $n$ polynomial functions $P_{1}$, $P_{2}$, $...$, $P_{n}$ from $V$ to
$\mathbb{C}$

such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =\sum
\limits_{i=1}^{n}P_{i}\left(  v\right)  w_{i}.
\]

\end{definition}

Note that it is clear that:

\begin{itemize}
\item If $V$ is a finite-dimensional vector space and $W$ is a vector space,
then any $\mathbb{C}$-linear combination of polynomial maps $V\rightarrow W$
is a polynomial map.

\item If $V$ is a finite-dimensional vector space and $W$ is a $\mathbb{C}%
$-algebra, then any product of polynomial maps $V\rightarrow W$ is a
polynomial map.

\item If $V$ is a finite-dimensional vector space, then polynomial maps
$V\rightarrow\mathbb{C}$ are exactly the same as polynomial functions
$V\rightarrow\mathbb{C}$ (since $\mathbb{C}$-linear combinations of polynomial
functions are polynomial functions).
\end{itemize}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The deformed Lie
algebra \texorpdfstring{$\mathfrak{g}^{\varepsilon}$}{g superscript epsilon}}

Before we go on, here is a rough plan of how we will attack Proposition
\ref{prop.det.US}:

In order to gain a foothold on $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $, we are going to consider not just one Lie algebra
$\mathfrak{g}$ but a whole family $\left(  \mathfrak{g}^{\varepsilon}\right)
_{\varepsilon\in\mathbb{C}}$ of its ``deformations'' at the same time. Despite
all of these deformations being isomorphic as Lie algebras with one exception,
they will give us useful information: we will show that the bilinear forms
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ they
induce, in some sense, depend ``polynomially'' on $\lambda$ and $\varepsilon$.
We will have to restrain from speaking directly of the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ as depending
polynomially on $\lambda$, since this makes no sense (the domain of the
bilinear form $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}$ changes with $\lambda$), but instead we will sample this form
on particular elements of the Verma modules coming from appropriately chosen
Poincar\'{e}-Birkhoff-Witt bases of $U\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $ and $U\left(  \mathfrak{n}_{+}^{\varepsilon}\right)  $. These
sampled values of the form will turn out to depend polynomially on $\lambda$
and $\varepsilon$, and thus the determinant $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\varepsilon}\right)  $ will be a polynomial
function in $\lambda$ and $\varepsilon$. This polynomial function will turn
out to have some kind of ``homogeneity with respect to $\lambda$ and
$\varepsilon^{2}$'' (this is not a standard notion, but see Corollary
\ref{cor.invformnondeg.polynomiality} for what exactly this means in our
context), so that the leading term of $\lambda$ will be the term with smallest
power of $\varepsilon$ (and, as it will turn out, this will be the power
$\varepsilon^{0}$, so this term will be obtainable by setting $\varepsilon$ to
$0$). Once this all is formalized and proven, we will explicitly show that
(more or less) $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}%
}=\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ (again this does not
literally hold but must be correctly interpreted), and we know the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ to be nondegenerate (by
Lemma \ref{lem.lambda_k.2}), so that the form $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{0}}$ will be nondegenerate, and this will quickly
yield the nondegeneracy of $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\varepsilon}\right)  $ for generic $\lambda$ and $\varepsilon$,
and thus the nondegeneracy of $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ for generic $\lambda$.

Now, to the details. Consider the situation of Proposition \ref{prop.det.US}.
In particular, this means that (from now on until the end of Section
\ref{subsect.invform}) the Lie algebra $\mathfrak{g}$ will be assumed nondegenerate.

First, let us define $\left(  \mathfrak{g}^{\varepsilon}\right)
_{\varepsilon\in\mathbb{C}}$.

For every $\varepsilon\in\mathbb{C}$, let us define a new Lie bracket $\left[
\cdot,\cdot\right]  ^{\varepsilon}$ on the vector space $\mathfrak{g}$ by the
formula%
\begin{align}
\left[  x,y\right]  ^{\varepsilon}  &  =\varepsilon\left[  x,y\right]
+\left(  1-\varepsilon\right)  \pi\left(  \left[  x,y\right]  \right)
-\varepsilon\left(  1-\varepsilon\right)  \left[  x,\pi\left(  y\right)
\right]  -\varepsilon\left(  1-\varepsilon\right)  \left[  \pi\left(
x\right)  ,y\right] \label{pf.invformnondeg.g^epsi.1}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}\text{ and }%
y\in\mathfrak{g},\nonumber
\end{align}
where $\pi$ is the canonical projection $\mathfrak{g}\rightarrow
\mathfrak{g}_{0}$. In other words, let us define a new Lie bracket $\left[
\cdot,\cdot\right]  ^{\varepsilon}$ on the vector space $\mathfrak{g}$ by%
\begin{align}
\left[  x,y\right]  ^{\varepsilon}  &  =\varepsilon^{\delta_{n,0}+\delta
_{m,0}+1-\delta_{n+m,0}}\left[  x,y\right]  \label{pf.invformnondeg.g^epsi.2}%
\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z}\text{, }m\in
\mathbb{Z}\text{, }x\in\mathfrak{g}_{n}\text{ and }y\in\mathfrak{g}%
_{m}\nonumber
\end{align}
(note that the right hand side of this equation makes sense since
$1-\delta_{n+m,0}\geq0$ for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}%
$)\ \ \ \ \footnote{Proving that these two definitions of $\left[  \cdot
,\cdot\right]  ^{\varepsilon}$ are equivalent is completely straightforward:
just assume WLOG that $x$ and $y$ are homogeneous, so that $x\in
\mathfrak{g}_{n}$ and $y\in\mathfrak{g}_{m}$ for $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, and distinguish between the following four cases:
\par
\textit{Case 1:} We have $n=0$ and $m=0$.
\par
\textit{Case 2:} We have $n\neq0$ and $m\neq0$ but $n+m=0$.
\par
\textit{Case 3:} We have $n\neq0$, $m\neq0$ and $n+m\neq0$.
\par
\textit{Case 4:} Exactly one of $n$ and $m$ is $0$.
\par
In Case 1, the assumption that $\mathfrak{g}_{0}$ is abelian must be used.}.
It is easy to prove that this Lie bracket $\left[  \cdot,\cdot\right]
^{\varepsilon}$ is antisymmetric and satisfies the Jacobi
identity\footnote{\textit{Proof.} Antisymmetry is obvious. As for the Jacobi
identity, it can be proven in a straightforward way:
\par
We must show the equality $\left[  x,\left[  y,z\right]  ^{\varepsilon
}\right]  ^{\varepsilon}+\left[  y,\left[  z,x\right]  ^{\varepsilon}\right]
^{\varepsilon}+\left[  z,\left[  x,y\right]  ^{\varepsilon}\right]
^{\varepsilon}=0$ for all $x,y,z\in\mathfrak{g}$. Since this equality is
linear in each of $x$, $y$ and $z$, it is enough to prove it for homogeneous
$x,y,z\in\mathfrak{g}$. So let $x,y,z\in\mathfrak{g}$ be homogeneous. Then,
there exist $n,m,p\in\mathbb{Z}$ such that $x\in\mathfrak{g}_{n}$,
$y\in\mathfrak{g}_{m}$ and $z\in\mathfrak{g}_{p}$. Consider these $n$, $m$ and
$p$. Then, by (\ref{pf.invformnondeg.g^epsi.2}) (applied to $y$, $z$, $m$ and
$p$ instead of $x$, $y$, $n$ and $m$), we have $\left[  y,z\right]
^{\varepsilon}=\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\left[
y,z\right]  $. Thus,%
\begin{align*}
&  \left[  x,\left[  y,z\right]  ^{\varepsilon}\right]  ^{\varepsilon}\\
&  =\left[  x,\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\left[
y,z\right]  \right]  ^{\varepsilon}=\varepsilon^{\delta_{m,0}+\delta
_{p,0}+1-\delta_{m+p,0}}\left[  x,\left[  y,z\right]  \right]  ^{\varepsilon
}\\
&  =\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\varepsilon
^{\delta_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because (\ref{pf.invformnondeg.g^epsi.2}) (applied to }\left[
y,z\right]  \text{ and }m+p\text{ instead of }y\text{ and }m\text{) yields}\\
\left[  x,\left[  y,z\right]  \right]  ^{\varepsilon}=\varepsilon
^{\delta_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right]  \text{ (since }\left[  y,z\right]  \in\mathfrak{g}_{m+p}\text{ (since
}y\in\mathfrak{g}_{m}\text{ and }z\in\mathfrak{g}_{p}\text{))}%
\end{array}
\right) \\
&  =\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}+\delta
_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right]  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta
_{n+m+p,0}}\left[  x,\left[  y,z\right]  \right]  .
\end{align*}
Similarly,
\begin{align*}
\left[  y,\left[  z,x\right]  ^{\varepsilon}\right]  ^{\varepsilon}  &
=\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  y,\left[  z,x\right]  \right]  \ \ \ \ \ \ \ \ \ \ \text{and}\\
\left[  z,\left[  x,y\right]  ^{\varepsilon}\right]  ^{\varepsilon}  &
=\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  z,\left[  x,y\right]  \right]  .
\end{align*}
Adding up these three equations yields%
\begin{align*}
&  \left[  x,\left[  y,z\right]  ^{\varepsilon}\right]  ^{\varepsilon}+\left[
y,\left[  z,x\right]  ^{\varepsilon}\right]  ^{\varepsilon}+\left[  z,\left[
x,y\right]  ^{\varepsilon}\right]  ^{\varepsilon}\\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  x,\left[  y,z\right]  \right]  +\varepsilon^{\delta_{n,0}%
+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}}\left[  y,\left[  z,x\right]
\right]  +\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta
_{n+m+p,0}}\left[  z,\left[  x,y\right]  \right] \\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\underbrace{\left(  \left[  x,\left[  y,z\right]  \right]  +\left[  y,\left[
z,x\right]  \right]  +\left[  z,\left[  x,y\right]  \right]  \right)
}_{=0\text{ (since }\mathfrak{g}\text{ is a Lie algebra)}}=0.
\end{align*}
This proves the Jacobi identity for the Lie bracket $\left[  \cdot
,\cdot\right]  ^{\varepsilon}$, qed.} and is graded. Thus, this Lie bracket
$\left[  \cdot,\cdot\right]  ^{\varepsilon}$ defines a graded Lie algebra
structure on $\mathfrak{g}$. Let us denote this Lie algebra by $\mathfrak{g}%
^{\varepsilon}$. Thus, $\mathfrak{g}^{\varepsilon}$ is identical with
$\mathfrak{g}$ as a vector space, but the Lie bracket on $\mathfrak{g}%
^{\varepsilon}$ is $\left[  \cdot,\cdot\right]  ^{\varepsilon}$ rather than
$\left[  \cdot,\cdot\right]  $.

Trivially, $\mathfrak{g}^{1}=\mathfrak{g}$ (this is an actual equality, not
only an isomorphism) and $\left[  \cdot,\cdot\right]  ^{1}=\left[  \cdot
,\cdot\right]  $.

For every $\varepsilon\in\mathbb{C}$, define a $\mathbb{C}$-linear map
$J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow\mathfrak{g}$ by%
\[
J_{\varepsilon}\left(  x\right)  =\varepsilon^{1+\delta_{n,0}}%
x\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ and }%
x\in\mathfrak{g}_{n}.
\]
Then, $J_{\varepsilon}$ is a Lie algebra homomorphism\footnote{\textit{Proof.}
We must show that $J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon
}\right)  =\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(
y\right)  \right]  $ for all $x,y\in\mathfrak{g}$. In order to show this, it
is enough to prove that $J_{\varepsilon}\left(  \left[  x,y\right]
^{\varepsilon}\right)  =\left[  J_{\varepsilon}\left(  x\right)
,J_{\varepsilon}\left(  y\right)  \right]  $ for all homogeneous
$x,y\in\mathfrak{g}$ (because of linearity). So let $x,y\in\mathfrak{g}$ be
homogeneous. Thus, there exist $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ such that
$x\in\mathfrak{g}_{n}$ and $y\in\mathfrak{g}_{m}$. Consider these $n$ and $m$.
Then, $\left[  x,y\right]  \in\mathfrak{g}_{n+m}$. Now, $J_{\varepsilon
}\left(  x\right)  =\varepsilon^{1+\delta_{n,0}}x$ and $J_{\varepsilon}\left(
y\right)  =\varepsilon^{1+\delta_{m,0}}y$ by the definition of $J_{\varepsilon
}$. Thus,%
\[
\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(  y\right)
\right]  =\left[  \varepsilon^{1+\delta_{n,0}}x,\varepsilon^{1+\delta_{m,0}%
}y\right]  =\varepsilon^{1+\delta_{n,0}}\varepsilon^{1+\delta_{m,0}}\left[
x,y\right]  =\varepsilon^{2+\delta_{n,0}+\delta_{m,0}}\left[  x,y\right]  .
\]
Compared with%
\begin{align*}
J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon}\right)   &
=J_{\varepsilon}\left(  \varepsilon^{\delta_{n,0}+\delta_{m,0}+1-\delta
_{n+m,0}}\left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.invformnondeg.g^epsi.2})}\right) \\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+1-\delta_{n+m,0}}%
\underbrace{J_{\varepsilon}\left(  \left[  x,y\right]  \right)  }%
_{\substack{=\varepsilon^{1+\delta_{n+m,0}}\left[  x,y\right]  \\\text{(by the
definition of }J_{\varepsilon}\text{,}\\\text{since }\left[  x,y\right]
\in\mathfrak{g}_{n+m}\text{)}}}=\varepsilon^{\delta_{n,0}+\delta
_{m,0}+1-\delta_{n+m,0}}\varepsilon^{1+\delta_{n+m,0}}\left[  x,y\right] \\
&  =\varepsilon^{2+\delta_{n,0}+\delta_{m,0}}\left[  x,y\right]  ,
\end{align*}
this yields $J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon}\right)
=\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(  y\right)
\right]  $, qed.}. Also, $J_{\varepsilon}$ is a vector space isomorphism when
$\varepsilon\neq0$. Hence, $J_{\varepsilon}$ is a Lie algebra isomorphism when
$\varepsilon\neq0$. Moreover, $J_{1}=\operatorname*{id}$.

For every $\varepsilon\in\mathbb{C}$, we are going to denote by $\mathfrak{n}%
_{-}^{\varepsilon}$, $\mathfrak{n}_{+}^{\varepsilon}$ and $\mathfrak{h}%
^{\varepsilon}$ the vector spaces $\mathfrak{n}_{-}$, $\mathfrak{n}_{+}$ and
$\mathfrak{h}$ \textbf{as Lie subalgebras of }$\mathfrak{g}^{\varepsilon}$.
Note that $\mathfrak{h}^{\varepsilon}=\mathfrak{h}$ as Lie algebras (because
$\mathfrak{h}$ and $\mathfrak{h}^{\varepsilon}$ are abelian Lie algebras), but
the equalities $\mathfrak{n}_{-}^{\varepsilon}=\mathfrak{n}_{-}$ and
$\mathfrak{n}_{+}^{\varepsilon}=\mathfrak{n}_{+}$ hold only as equalities of
vector spaces (unless we are in some rather special situation). Since the
grading of $\mathfrak{g}^{\varepsilon}$ is the same as the grading of
$\mathfrak{g}$, the triangular decomposition of $\mathfrak{g}^{\varepsilon}$
is $\mathfrak{n}_{-}^{\varepsilon}\oplus\mathfrak{h}^{\varepsilon}%
\oplus\mathfrak{n}_{+}^{\varepsilon}$ for every $\varepsilon\in\mathbb{C}$.

Now, we are dealing with several Lie algebras on the same vector space, and we
are going to be dealing with their Verma modules. In order not to confuse
them, let us introduce a notation:

\begin{Convention}
In the following, whenever $\mathfrak{e}$ is a $\mathbb{Z}$-graded Lie
algebra, and $\lambda\in\mathfrak{e}_{0}^{\ast}$, we are going to denote by
$M_{\lambda}^{+\mathfrak{e}}$ the Verma highest-weight module of $\left(
\mathfrak{e},\lambda\right)  $, and we are going to denote by $M_{\lambda
}^{-\mathfrak{e}}$ the Verma lowest-weight module of $\left(  \mathfrak{e}%
,\lambda\right)  $. We will furthermore denote by $v_{\lambda}^{+\mathfrak{e}%
}$ the defining vector of $M_{\lambda}^{+\mathfrak{e}}$, and we will denote by
$v_{\lambda}^{-\mathfrak{e}}$ the defining vector of $M_{\lambda
}^{-\mathfrak{e}}$.

Further, we denote by $\left(  \cdot,\cdot\right)  _{\lambda}^{\mathfrak{e}}$
and $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{e}}$ the forms
$\left(  \cdot,\cdot\right)  _{\lambda}$ and $\left(  \cdot,\cdot\right)
_{\lambda,n}$ defined for the Lie algebra $\mathfrak{e}$ instead of
$\mathfrak{g}$.
\end{Convention}

Thus, for instance, the Verma highest-weight module of $\left(  \mathfrak{g}%
,\lambda\right)  $ (which we have always denoted by $M_{\lambda}^{+}$) can now
be called $M_{\lambda}^{+\mathfrak{g}}$, and thus can be discerned from the
Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$ of
$\left(  \mathfrak{g}^{\varepsilon},\lambda\right)  $.

\begin{Convention}
\label{conv.invformnondeg.bases}For every $n\in\mathbb{Z}$, let $\left(
e_{n,i}\right)  _{i\in\left\{  1,2,...,m_{n}\right\}  }$ be a basis of the
vector space $\mathfrak{g}_{n}$ (such a basis exists since $\dim\left(
\mathfrak{g}_{n}\right)  <\infty$). Then, $\left(  e_{n,i}\right)  _{\left(
n,i\right)  \in E}$ is a basis of the vector space $\mathfrak{g}$, where
$E=\left\{  \left(  n,i\right)  \ \mid\ n\in\mathbb{Z};\ i\in\left\{
1,2,...,m_{n}\right\}  \right\}  $.

For every integer $n>0$, we have $\dim\left(  \mathfrak{g}_{n}\right)  =m_{n}$
(since $\left(  e_{n,i}\right)  _{i\in\left\{  1,2,...,m_{n}\right\}  }$ is a
basis of the vector space $\mathfrak{g}_{n}$) and $\dim\left(  \mathfrak{g}%
_{-n}\right)  =m_{-n}$ (similarly), so that $m_{n}=\dim\left(  \mathfrak{g}%
_{n}\right)  =\dim\left(  \mathfrak{g}_{-n}\right)  =m_{-n}$. Of course, this
yields that $m_{n}=m_{-n}$ for every integer $n$ (whether positive or not).

We totally order the set $E$ lexicographically. Let $\operatorname*{Seq}E$ be
the set of all finite sequences of elements of $E$. For every $\mathbf{i}%
\in\operatorname*{Seq}E$ and every $\varepsilon\in\mathbb{C}$, we define an
element $e_{\mathbf{i}}^{\varepsilon}$ of $U\left(  \mathfrak{g}^{\varepsilon
}\right)  $ by%
\[
e_{\mathbf{i}}^{\varepsilon}=e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell
},i_{\ell}},\ \ \ \ \ \ \ \ \ \ \text{where we write }\mathbf{i}\text{ in the
form }\left(  \left(  n_{1},i_{1}\right)  ,\left(  n_{2},i_{2}\right)
,...,\left(  n_{\ell},i_{\ell}\right)  \right)  .
\]
For every $\mathbf{i}\in\operatorname*{Seq}E$, we define the \textit{length}
$\operatorname*{len}\mathbf{i}$ of $\mathbf{i}$ to be the number of members of
$\mathbf{i}$ (in other words, we set $\operatorname*{len}\mathbf{i}=\ell$,
where we write $\mathbf{i}$ in the form $\left(  \left(  n_{1},i_{1}\right)
,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
$), and we define the \textit{degree} $\deg\mathbf{i}$ of $\mathbf{i}$ to be
the sum $n_{1}+n_{2}+...+n_{\ell}$, where we write $\mathbf{i}$ in the form
$\left(  \left(  n_{1},i_{1}\right)  ,\left(  n_{2},i_{2}\right)  ,...,\left(
n_{\ell},i_{\ell}\right)  \right)  $. It is clear that $e_{\mathbf{i}%
}^{\varepsilon}\in U\left(  \mathfrak{g}^{\varepsilon}\right)  \left[
\deg\mathbf{i}\right]  $.

Let $\operatorname*{Seq}\nolimits_{+}E$ be the set of all
\textbf{nondecreasing} sequences $\left(  \left(  n_{1},i_{1}\right)  ,\left(
n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
\in\operatorname*{Seq}E$ such that all of $n_{1}$, $n_{2}$, $...$, $n_{\ell}$
are \textbf{positive}. By the Poincar\'{e}-Birkhoff-Witt theorem (applied to
the Lie algebra $\mathfrak{n}_{+}^{\varepsilon}$), the family $\left(
e_{\mathbf{j}}^{\varepsilon}\right)  _{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E}$ is a basis of the vector space $U\left(  \mathfrak{n}%
_{+}^{\varepsilon}\right)  $. Moreover, it is a graded basis, i. e., the
family $\left(  e_{\mathbf{j}}^{\varepsilon}\right)  _{\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ is a basis of the
vector space $U\left(  \mathfrak{n}_{+}^{\varepsilon}\right)  \left[
n\right]  $ for every $n\in\mathbb{Z}$. Hence, $\left(  e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ is a
basis of the vector space $M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[
n\right]  $ for every $n\in\mathbb{Z}$ and $\lambda\in\mathfrak{h}^{\ast}$.

Let $\operatorname*{Seq}\nolimits_{-}E$ be the set of all
\textbf{nonincreasing} sequences $\left(  \left(  n_{1},i_{1}\right)  ,\left(
n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
\in\operatorname*{Seq}E$ such that all of $n_{1}$, $n_{2}$, $...$, $n_{\ell}$
are \textbf{negative}. By the Poincar\'{e}-Birkhoff-Witt theorem (applied to
the Lie algebra $\mathfrak{n}_{-}^{\varepsilon}$), the family $\left(
e_{\mathbf{i}}^{\varepsilon}\right)  _{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E}$ is a basis of the vector space $U\left(  \mathfrak{n}%
_{-}^{\varepsilon}\right)  $. Moreover, it is a graded basis, i. e., the
family $\left(  e_{\mathbf{i}}^{\varepsilon}\right)  _{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ is a basis of the
vector space $U\left(  \mathfrak{n}_{-}^{\varepsilon}\right)  \left[
-n\right]  $ for every $n\in\mathbb{Z}$. Hence, $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ is a
basis of the vector space $M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[
-n\right]  $ for every $n\in\mathbb{Z}$ and $\lambda\in\mathfrak{h}^{\ast}$.

We can define a bijection%
\begin{align*}
E  &  \rightarrow E,\\
\left(  n,i\right)   &  \mapsto\left(  -n,m_{n}+1-i\right)
\end{align*}
(because $m_{n}=m_{-n}$ for every $n\in\mathbb{Z}$). This bijection reverses
the order on $E$. Hence, this bijection canonically induces a bijection
$\operatorname*{Seq}E\rightarrow\operatorname*{Seq}E$, which maps
$\operatorname*{Seq}\nolimits_{+}E$ to $\operatorname*{Seq}\nolimits_{-}E$ and
vice versa, and reverses the degree of every sequence while keeping the length
of every sequence invariant. One consequence of this bijection is that for
every $n\in\mathbb{Z}$, the number of all $\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals the number of all
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ satisfying$\ \deg
\mathbf{i}=-n$. Another consequence is that $\sum\limits_{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}%
\mathbf{i=}\sum\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$.

For every positive integer $n$, we represent the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}:M_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right]  \times M_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]  \rightarrow\mathbb{C}$ by its
matrix with respect to the bases $\left(  e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\right)  _{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ and $\left(
e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ of
$M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right]  $ and
$M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]  $, respectively.
This is the matrix%
\[
\left(  \left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}.
\]
This matrix is a square matrix (since the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$), and its determinant is what we are going to
denote by $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  $.
\end{Convention}

A few words about tensor algebras:

\begin{Convention}
In the following, we let $T$ denote the tensor algebra functor. Hence, for
every vector space $V$, we denote by $T\left(  V\right)  $ the tensor algebra
of $V$.

We notice that $T\left(  V\right)  $ is canonically graded even if $V$ is not.
In fact, $T\left(  V\right)  =\bigoplus\limits_{i\in\mathbb{N}}V^{\otimes i}$,
so that we get a grading on $T\left(  V\right)  $ if we set $V^{\otimes i}$ to
be the $i$-th homogeneous component of $T\left(  V\right)  $. This grading is
called the \textit{tensor length grading} on $T\left(  V\right)  $. It makes
$T\left(  V\right)  $ concentrated in nonnegative degrees.

If $V$ itself is a graded vector space, then we can also grade $T\left(
V\right)  $ by canonically extending the grading on $V$ to $T\left(  V\right)
$ (this means that whenever $v_{1}$, $v_{2}$, $...$, $v_{n}$ are homogeneous
elements of $V$ of degrees $d_{1}$, $d_{2}$, $...$, $d_{n}$, then the pure
tensor $v_{1}\otimes v_{2}\otimes...\otimes v_{n}$ has degree $d_{1}%
+d_{2}+...+d_{n}$). This grading is called the \textit{internal grading} on
$T\left(  V\right)  $. It is different from the tensor length grading (unless
$V$ is concentrated in degree $1$).

Hence, if $V$ is a graded vector space, then $T\left(  V\right)  $ becomes a
bigraded vector space (i. e., a vector space with two gradings). Let us agree
to denote by $T\left(  V\right)  \left[  n,m\right]  $ the intersection of the
$n$-th homogeneous component in the internal grading with the $m$-th
homogeneous component in the tensor length grading (i. e., with $V^{\otimes
m}$).
\end{Convention}

Let us notice that \textbf{as vector spaces}, we have $\mathfrak{g}%
=\mathfrak{g}^{\varepsilon}$, $\mathfrak{n}_{-}=\mathfrak{n}_{-}^{\varepsilon
}$, $\mathfrak{n}_{+}=\mathfrak{n}_{+}^{\varepsilon}$ and $\mathfrak{h}%
=\mathfrak{h}^{\varepsilon}$ for every $\varepsilon\in\mathbb{C}$. Hence,
$T\left(  \mathfrak{g}\right)  =T\left(  \mathfrak{g}^{\varepsilon}\right)  $,
$T\left(  \mathfrak{n}_{-}\right)  =T\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $, $T\left(  \mathfrak{n}_{+}\right)  =T\left(  \mathfrak{n}%
_{+}^{\varepsilon}\right)  $ and $T\left(  \mathfrak{h}\right)  =T\left(
\mathfrak{h}^{\varepsilon}\right)  $.

\begin{definition}
In the following, for every Lie algebra $\mathfrak{a}$ and every element $x\in
T\left(  \mathfrak{a}\right)  $, we denote by $\operatorname*{env}%
\nolimits_{\mathfrak{a}}x$ the projection of $x$ onto the factor algebra
$U\left(  \mathfrak{a}\right)  $ of $T\left(  \mathfrak{a}\right)  $.
\end{definition}

Let us again stress that $T\left(  \mathfrak{g}\right)  =T\left(
\mathfrak{g}^{\varepsilon}\right)  $, so that $T\left(  \mathfrak{g}%
^{\varepsilon}\right)  $ does not depend on $\varepsilon$, whereas $U\left(
\mathfrak{g}^{\varepsilon}\right)  $ does. Hence, if we want to study the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ as it
changes with $\varepsilon$, the easiest thing to do is to study the values of
$\left(  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}a\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}b\right)
v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}$ for fixed $a\in T\left(  \mathfrak{g}\right)  =T\left(
\mathfrak{g}^{\varepsilon}\right)  $ and $b\in T\left(  \mathfrak{g}\right)
=T\left(  \mathfrak{g}^{\varepsilon}\right)  $. Here is the polynomiality
lemma that we want to have:

\begin{lemma}
\label{lem.invformnondeg.polynomiality}Let $\mathbf{i}\in\operatorname*{Seq}E$
and $\mathbf{j}\in\operatorname*{Seq}E$. Then, there exists a polynomial
function $Q_{\mathbf{i},\mathbf{j}}:\mathfrak{h}^{\ast}\times\mathbb{C}%
\rightarrow\mathbb{C}$ such that every $\lambda\in\mathfrak{h}^{\ast}$ and
every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  .
\]

\end{lemma}

To prove this lemma, we show something more general:

\begin{lemma}
\label{lem.invformnondeg.polynomiality2}For every $n\in\mathbb{Z}$ and $c\in
T\left(  \mathfrak{g}\right)  \left[  n\right]  $, there exists a polynomial
map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ such that every $\lambda\in\mathfrak{h}%
^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]

\end{lemma}

To get some intuition about Lemma \ref{lem.invformnondeg.polynomiality2},
recall that the Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ was defined as $U\left(  \mathfrak{g}^{\varepsilon}\right)
\otimes_{U\left(  \mathfrak{h}^{\varepsilon}\oplus\mathfrak{n}_{+}%
^{\varepsilon}\right)  }\mathbb{C}_{\lambda}$, but turned out to be $U\left(
\mathfrak{n}_{-}^{\varepsilon}\right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ (as a vector space), so that every term of the form
$xv_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$ with $x\in U\left(
\mathfrak{g}^{\varepsilon}\right)  $ can be reduced to the form $yv_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}$ with $y\in U\left(  \mathfrak{n}%
_{-}^{\varepsilon}\right)  $. Lemma \ref{lem.invformnondeg.polynomiality2}
says that, if $x$ is given as the projection $\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}c$ of some tensor $c\in T\left(
\mathfrak{g}\right)  \left[  n\right]  $ onto $U\left(  \mathfrak{g}%
^{\varepsilon}\right)  $, then $y$ can be found as the projection of some
tensor $d\left(  \lambda,\varepsilon\right)  \in T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ onto $U\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $ which depends polynomially on $\lambda$ and $\varepsilon$. This is
not particularly surprising, since $y$ is found from $x$ by picking a
tensorial representation\footnote{By a ``tensorial representation'' of $x$, I
mean a tensor $c\in T\left(  \mathfrak{g}\right)  $ such that
$\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c=x$.} of $x$ and
``gradually'' stratifying it\footnote{By ``stratifying'' a tensorial
representation of $x$, I mean writing it as a linear combination of pure
tensors, and whenever such a pure tensor has a negative tensorand (i. e., a
tensorand in $\mathfrak{n}_{-}$) standing directly before a positive tensorand
(i. e., a tensorand in $\mathfrak{n}_{+}$), applying the $xy-yx=\left[
x,y\right]  ^{\varepsilon}$ relations in $U\left(  \mathfrak{g}^{\varepsilon
}\right)  $ to move the negative tensorand past the positive one. As soon as a
positive tensorand hits the right end of the tensor, the tensor can be thrown
away since $\mathfrak{n}_{+}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$. For
instance, in Example \ref{exa.Vir} further below, we compute $L_{1}%
L_{-1}v_{\lambda}^{+}$ by stratifying the tensorial representation
$L_{1}\otimes L_{-1}$ of $L_{1}L_{-1}$, and we compute $L_{1}^{2}L_{-1}%
^{2}v_{\lambda}^{+}$ by stratifying the tensorial representation $L_{1}\otimes
L_{1}\otimes L_{-1}\otimes L_{-1}$ of $L_{1}^{2}L_{-1}^{2}$.}, and the
$\lambda$'s and $\varepsilon$'s which appear during this stratification
process don't appear ``randomly'', but rather appear at foreseeable places.
The following proof of Lemma \ref{lem.invformnondeg.polynomiality2} will
formalize this idea.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality2}.} First some notations:

If $n\in\mathbb{Z}$, then a tensor $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is said to be $n$\textit{-stratifiable} if there exists a
polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ such that every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]


Lemma \ref{lem.invformnondeg.polynomiality2} states that for every
$n\in\mathbb{Z}$, every tensor $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable.

We will now prove that
\begin{equation}
\text{for every }n\in\mathbb{Z}\text{ and every }m\in\mathbb{N}\text{, every
tensor }c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  \text{ is
}n\text{-stratifiable.} \label{lem.invformnondeg.polynomiality2.ind}%
\end{equation}


Before we start proving this, let us formulate two easy observations about
stratifiable tensors:

\textit{Observation 1:} For any fixed $n$, any $\mathbb{C}$-linear combination
of $n$-stratifiable tensors is $n$-stratifiable. (In fact, we can just take
the corresponding $\mathbb{C}$-linear combination of the corresponding
polynomial maps $d$.)

\textit{Observation 2:} If an integer $n$, a negative integer $\nu$, a vector
$x\in\mathfrak{g}_{\nu}$ and a tensor $y\in T\left(  \mathfrak{g}\right)
\left[  n-\nu\right]  $ are such that $y$ is $\left(  n-\nu\right)
$-stratifiable, then $x\otimes y\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable.\footnote{\textit{Proof of Observation 2.} Let
an integer $n$, a negative integer $\nu$, a vector $x\in\mathfrak{g}_{\nu}$
and a tensor $y\in T\left(  \mathfrak{g}\right)  \left[  n-\nu\right]  $ be
such that $y$ is $\left(  n-\nu\right)  $-stratifiable. Then, there exists a
polynomial map $\widetilde{d}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow
T\left(  \mathfrak{n}_{-}\right)  \left[  n-\nu\right]  $ such that every
$\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}y\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}%
\]
(by the definition of ``$\left(  n-\nu\right)  $-stratifiable''). Now, define
a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by
\[
d\left(  \lambda,\varepsilon\right)  =x\otimes\widetilde{d}\left(
\lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(
\lambda,\varepsilon\right)  \in\mathfrak{h}^{\ast}\times\mathbb{C}.
\]
(This is well-defined, since $x\in\mathfrak{g}_{\nu}\subseteq\mathfrak{n}_{-}$
(since $\nu$ is negative).) This map $d$ is clearly polynomial (since
$\widetilde{d}$ is a polynomial map), and every $\lambda\in\mathfrak{h}^{\ast
}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\begin{align*}
\underbrace{\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x\otimes y\right)  \right)  }_{=x\cdot\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}y}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}  &  =x\cdot\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}y\right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}_{=\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}%
=\underbrace{x\cdot\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  }_{=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon
}}\left(  x\otimes\widetilde{d}\left(  \lambda,\varepsilon\right)  \right)
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  x\otimes\widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  }_{=d\left(  \lambda,\varepsilon\right)  }\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\end{align*}
Hence, $x\otimes y$ is $n$-stratifiable (by the definition of ``$n$%
-stratifiable''). This proves Observation 2.}

We are now going to prove (\ref{lem.invformnondeg.polynomiality2.ind}) by
induction on $m$:

\textit{Induction base:} We have $T\left(  \mathfrak{g}\right)  \left[
n,0\right]  =\mathbb{C}\left[  n\right]  $. Hence, every tensor $c\in T\left(
\mathfrak{g}\right)  \left[  n,0\right]  $ is $n$-stratifiable (because we can
define the polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow
T\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =c\ \ \ \ \ \ \ \ \ \ \text{for all
}\left(  \lambda,\varepsilon\right)  \in\mathfrak{h}^{\ast}\times\mathbb{C}%
\]
). In other words, (\ref{lem.invformnondeg.polynomiality2.ind}) is proven for
$m=0$. In other words, the induction base is complete.

\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. We must show that
(\ref{lem.invformnondeg.polynomiality2.ind}) holds for this $m$, using the
assumption that (\ref{lem.invformnondeg.polynomiality2.ind}) holds for $m-1$
instead of $m$.

Let $n\in\mathbb{Z}$. Let $\pi_{n}:T\left(  \mathfrak{g}\right)  \rightarrow
T\left(  \mathfrak{g}\right)  \left[  n\right]  $ denote the canonical
projection of $T\left(  \mathfrak{g}\right)  $ to the $n$-th homogeneous
component with respect to the internal grading.

Let $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $. We must prove
that $c$ is $n$-stratifiable.

We have $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  \subseteq
\mathfrak{g}^{\otimes m}$, and since the $m$-th tensor power is generated by
pure tensors, this yields that $c$ is a $\mathbb{C}$-linear combination of
pure tensors. In other words, $c$ is a $\mathbb{C}$-linear combination of
finitely many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes
x_{m}$ with $x_{1},x_{2},...,x_{m}\in\mathfrak{g}$. We can WLOG assume that,
in each of these pure tensors, the elements $x_{1},x_{2},...,x_{m}$ are
homogeneous (since otherwise we can break each of $x_{1},x_{2},...,x_{m}$ into
homogeneous components, and thus the pure tensors $x_{1}\otimes x_{2}%
\otimes...\otimes x_{m}$ break into smaller pieces which are still pure
tensors). So we can write $c$ as a $\mathbb{C}$-linear combination of finitely
many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
\textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$. If we apply the
projection $\pi_{n}$ to this, then $c$ remains invariant (since $c\in T\left(
\mathfrak{g}\right)  \left[  n,m\right]  \subseteq T\left(  \mathfrak{g}%
\right)  \left[  n\right]  $), and the terms of the form $x_{1}\otimes
x_{2}\otimes...\otimes x_{m}$ with \textbf{homogeneous }$x_{1},x_{2}%
,...,x_{m}\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$ remain invariant as well
(since they also lie in $T\left(  \mathfrak{g}\right)  \left[  n\right]  $),
whereas the terms of the form $x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
\textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ satisfying
$\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)  +...+\deg\left(
x_{m}\right)  \neq n$ are mapped to $0$ (since they lie in homogeneous
components of $T\left(  \mathfrak{g}\right)  $ other than $T\left(
\mathfrak{g}\right)  \left[  n\right]  $). Hence, we write $c$ as a
$\mathbb{C}$-linear combination of finitely many pure tensors of the form
$x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with \textbf{homogeneous }%
$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ \textbf{satisfying} $\deg\left(
x_{1}\right)  +\deg\left(  x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$.

Therefore, in proving (\ref{lem.invformnondeg.polynomiality2.ind}), we can
WLOG assume that $c$ \textbf{is} a pure tensor of the form $x_{1}\otimes
x_{2}\otimes...\otimes x_{m}$ with homogeneous $x_{1},x_{2},...,x_{m}%
\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$ (because, clearly, once Lemma
\ref{lem.invformnondeg.polynomiality2} is proven for certain values of $c\in
T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $, it must clearly also hold
for all their $\mathbb{C}$-linear combinations\footnote{due to Observation
1}). Let us now assume this.

So we have $c=x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with homogeneous
$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)
+\deg\left(  x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$. We must now
prove that $c$ is $n$-stratifiable.

For every $i\in\left\{  1,2,...,m\right\}  $, let $n_{i}$ be the degree of
$x_{i}$ (this is well-defined since $x_{i}$ is homogeneous). Thus, $x_{i}%
\in\mathfrak{g}_{n_{i}}$.

We have%
\[
\deg\left(  x_{2}\right)  +\deg\left(  x_{3}\right)  +...+\deg\left(
x_{m}\right)  =\underbrace{\left(  \deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  \right)  }_{=n}-\underbrace{\deg
\left(  x_{1}\right)  }_{=n_{1}}=n-n_{1},
\]
so that $x_{2}\otimes x_{3}\otimes...\otimes x_{m}\in T\left(  \mathfrak{g}%
\right)  \left[  n-n_{1}\right]  $ and thus $x_{2}\otimes x_{3}\otimes
...\otimes x_{m}\in T\left(  \mathfrak{g}\right)  \left[  n-n_{1},m-1\right]
$. Since we have assumed that (\ref{lem.invformnondeg.polynomiality2.ind})
holds for $m-1$ instead of $m$, we can thus apply
(\ref{lem.invformnondeg.polynomiality2.ind}) to $n-n_{1}$, $m-1$ and
$x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ instead of $n$, $m$ and $c$. We
conclude that $x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ is $\left(
n-n_{1}\right)  $-stratifiable. In other words, there exists a polynomial map
$\widetilde{d}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n-n_{1}\right]  $ such that every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\]


We notice that $c=x_{1}\otimes x_{2}\otimes...\otimes x_{m}$, so that%
\begin{align}
&  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\nonumber\\
&  =x_{1}x_{2}...x_{m}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\underbrace{\left(  x_{2}x_{3}...x_{i-1}x_{i}\cdot
x_{1}\cdot x_{i+1}x_{i+2}...x_{m}-x_{2}x_{3}...x_{i}x_{i+1}\cdot x_{1}\cdot
x_{i+2}x_{i+3}...x_{m}\right)  }_{=x_{2}x_{3}...x_{i-1}x_{i}\left(
x_{1}x_{i+1}-x_{i+1}x_{1}\right)  x_{i+2}x_{i+3}...x_{m}}+x_{2}x_{3}%
...x_{m}\cdot x_{1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the sum }\sum\limits_{i=1}^{m-1}\left(  x_{2}x_{3}...x_{i-1}%
x_{i}\cdot x_{1}\cdot x_{i+1}x_{i+2}...x_{m}-x_{2}x_{3}...x_{i}x_{i+1}\cdot
x_{1}\cdot x_{i+2}x_{i+3}...x_{m}\right) \\
\text{telescopes to }x_{1}x_{2}...x_{m}-x_{2}x_{3}...x_{m}\cdot x_{1}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{i=1}^{m-1}x_{2}x_{3}...x_{i-1}x_{i}\underbrace{\left(
x_{1}x_{i+1}-x_{i+1}x_{1}\right)  }_{\substack{=\left[  x_{1},x_{i+1}\right]
^{\varepsilon}\\\text{(since we are in }U\left(  \mathfrak{g}^{\varepsilon
}\right)  \text{)}}}x_{i+2}x_{i+3}...x_{m}+x_{2}x_{3}...x_{m}\cdot
x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}x_{2}x_{3}...x_{i-1}x_{i}\underbrace{\left[
x_{1},x_{i+1}\right]  ^{\varepsilon}}_{\substack{=\varepsilon^{\delta
_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\left[  x_{1}%
,x_{i+1}\right]  \\\text{(by (\ref{pf.invformnondeg.g^epsi.2}) (applied to
}x_{1}\text{ and }x_{i+1}\text{ instead of }x\text{ and }y\text{),}%
\\\text{since }x_{1}\in\mathfrak{g}_{n_{1}}\text{ and }x_{i+1}\in
\mathfrak{g}_{n_{i+1}}\text{)}}}x_{i+2}x_{i+3}...x_{m}+x_{2}x_{3}...x_{m}\cdot
x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{x_{2}x_{3}...x_{i-1}x_{i}\left[
x_{1},x_{i+1}\right]  x_{i+2}x_{i+3}...x_{m}}_{=\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[  x_{1},x_{i+1}\right]
\otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{x_{2}x_{3}...x_{m}}_{=\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{m}\right)  }\cdot x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{i-1}\otimes
x_{i}\otimes\left[  x_{1},x_{i+1}\right]  \otimes x_{i+2}\otimes
x_{i+3}\otimes...\otimes x_{m}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
x_{1}. \label{pf.invformnondeg.polynomiality2.big}%
\end{align}


Now, for every $i\in\left\{  1,2,...,m-1\right\}  $, denote the element
$x_{2}\otimes x_{3}\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[
x_{1},x_{i+1}\right]  \otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}$
by $c_{i}$. It is easily seen that $c_{i}\in T\left(  \mathfrak{g}\right)
\left[  n,m-1\right]  $. Since \newline$c_{i}=x_{2}\otimes x_{3}%
\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[  x_{1},x_{i+1}\right]
\otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}$, the equality
(\ref{pf.invformnondeg.polynomiality2.big}) rewrites as%
\begin{align}
&  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  c_{i}\right)  +\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{m}\right)  \cdot x_{1}.
\label{pf.invformnondeg.polynomiality2.small}%
\end{align}


For every $i\in\left\{  1,2,...,m-1\right\}  $, we can apply
(\ref{lem.invformnondeg.polynomiality2.ind}) to $m-1$ and $c_{i}$ instead of
$m$ and $c$ (since $c_{i}\in T\left(  \mathfrak{g}\right)  \left[
n,m-1\right]  $, and since we have assumed that
(\ref{lem.invformnondeg.polynomiality2.ind}) holds for $m-1$ instead of $m$).
We conclude that $c_{i}$ is $n$-stratifiable for every $i\in\left\{
1,2,...,m-1\right\}  $. In other words, for every $i\in\left\{
1,2,...,m-1\right\}  $, there exists a polynomial map $\widetilde{d_{i}%
}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ such that every $\lambda\in\mathfrak{h}%
^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
c_{i}\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)  \right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]


We now distinguish between three cases:

\textit{Case 1:} We have $n_{1}>0$.

\textit{Case 2:} We have $n_{1}=0$.

\textit{Case 3:} We have $n_{1}<0$.

First, let us consider Case 1. In this case, $n_{1}>0$. Thus, $x_{1}%
\in\mathfrak{n}_{+}$ (since $x_{1}\in\mathfrak{g}_{n_{1}}$), so that
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in\mathfrak{n}_{+}%
^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$ and thus
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$. Now,
(\ref{pf.invformnondeg.polynomiality2.small}) yields%
\begin{align}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}%
+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)
+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \cdot x_{1}\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
\underbrace{x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=0}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality2.small1}%
\end{align}
If we define a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =\sum\limits_{i=1}^{m-1}\varepsilon
^{\delta_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}%
}\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \lambda,\varepsilon\right)
\in\mathfrak{h}^{\ast}\times\mathbb{C},
\]
then this map $d$ is polynomial (since $\widetilde{d_{i}}$ are polynomial maps
for all $i$), and (\ref{pf.invformnondeg.polynomiality2.small1}) becomes%
\begin{align*}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1}%
,0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  }_{=d\left(  \lambda,\varepsilon\right)
}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\end{align*}
Hence, $c$ is $n$-stratifiable (by the definition of ``$n$-stratifiable'').

Next, let us consider Case 2. In this case, $n_{1}=0$. Thus, $x_{1}%
\in\mathfrak{h}$ (since $x_{1}\in\mathfrak{g}_{n_{1}}$), so that
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\lambda\left(  x_{1}\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$. Now,
(\ref{pf.invformnondeg.polynomiality2.small}) yields%
\begin{align}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}%
+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)
+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \cdot x_{1}\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
\underbrace{x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\lambda\left(
x_{1}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\lambda\left(  x_{1}\right)  \underbrace{\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}_{=\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}+\lambda\left(  x_{1}\right)  \left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(  \lambda,\varepsilon
\right)  +\lambda\left(  x_{1}\right)  \widetilde{d}\left(  \lambda
,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}. \label{pf.invformnondeg.polynomiality2.small2}%
\end{align}
If we define a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =\sum\limits_{i=1}^{m-1}\varepsilon
^{\delta_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}%
}\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)  +\lambda\left(
x_{1}\right)  \widetilde{d}\left(  \lambda,\varepsilon\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \lambda,\varepsilon\right)
\in\mathfrak{h}^{\ast}\times\mathbb{C}%
\]
(this map is well-defined, since $\widetilde{d}\left(  \lambda,\varepsilon
\right)  \in T\left(  \mathfrak{n}_{-}\right)  \left[  n-n_{1}\right]
=T\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $ (due to $n_{1}=0$)),
then this map $d$ is polynomial (since $\widetilde{d_{i}}$ are polynomial maps
for all $i$, and since $\widetilde{d}$ is polynomial), and
(\ref{pf.invformnondeg.polynomiality2.small2}) becomes%
\begin{align*}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1}%
,0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  +\lambda\left(  x_{1}\right)  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  }_{=d\left(  \lambda,\varepsilon\right)
}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\end{align*}
Hence, $c$ is $n$-stratifiable (by the definition of ``$n$-stratifiable'').

Now, let us consider Case 3. In this case, $n_{1}<0$. Thus, we can apply
Observation 2 to $x_{1}$, $x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ and
$n_{1}$ instead of $x$, $y$ and $\nu$, and conclude that $x_{1}\otimes\left(
x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  $ is $n$-stratifiable (since
$x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ is $\left(  n-n_{1}\right)
$-stratifiable). Since $x_{1}\otimes\left(  x_{2}\otimes x_{3}\otimes
...\otimes x_{m}\right)  =x_{1}\otimes x_{2}\otimes...\otimes x_{m}=c$, this
shows that $c$ is $n$-stratifiable.

Hence, in each of the cases 1, 2 and 3, we have shown that $c$ is
$n$-stratifiable. Thus, $c$ is always $n$-stratifiable.

Forget that we fixed $c$. We thus have shown that $c$ is $n$-stratifiable for
every tensor $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $. In
other words, we have proven (\ref{lem.invformnondeg.polynomiality2.ind}) for
our $m$. This completes the induction step.

Thus, (\ref{lem.invformnondeg.polynomiality2.ind}) is proven by induction.

Now, let $n\in\mathbb{Z}$. Then, every $c\in T\left(  \mathfrak{g}\right)
\left[  n\right]  $ is a $\mathbb{C}$-linear combination of elements of
$T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $ for varying
$m\in\mathbb{N}$ (since $T\left(  \mathfrak{g}\right)  \left[  n\right]
=\bigoplus\limits_{m\in\mathbb{N}}T\left(  \mathfrak{g}\right)  \left[
n,m\right]  $), and thus every $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable (since
(\ref{lem.invformnondeg.polynomiality2.ind}) shows that every element of
$T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $ is $n$-stratifiable, and
due to Observation 1).

Now forget that we fixed $n$. We have thus proven that for every
$n\in\mathbb{Z}$, every $c\in T\left(  \mathfrak{g}\right)  \left[  n\right]
$ is $n$-stratifiable. In other words, we have proved Lemma
\ref{lem.invformnondeg.polynomiality2}.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality}.} We have
$e_{\mathbf{i}}^{\varepsilon}\in U\left(  \mathfrak{g}^{\varepsilon}\right)
\left[  \deg\mathbf{i}\right]  $ and thus $e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{i}\right]  $. Similarly, $e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\in M_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\left[  \deg\mathbf{j}\right]  $. Hence, if
$\deg\mathbf{i}+\deg\mathbf{j}\neq0$, then $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}\in\left(  M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{i}\right]  ,M_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{j}\right]  \right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}=0$ (because the form $\left(  \cdot
,\cdot\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}$ is of degree $0$,
while $\deg\mathbf{i}+\deg\mathbf{j}\neq0$) and thus $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}=0$. Thus, if $\deg\mathbf{i}+\deg\mathbf{j}%
\neq0$, then Lemma \ref{lem.invformnondeg.polynomiality} trivially holds
(because we can then just take $Q_{\mathbf{i},\mathbf{j}}=0$). Thus, for the
rest of the proof of Lemma \ref{lem.invformnondeg.polynomiality}, we can WLOG
assume that we \textit{don't} have $\deg\mathbf{i}+\deg\mathbf{j}\neq0$.
Hence, we have $\deg\mathbf{i}+\deg\mathbf{j}=0$.

Write the sequence $\mathbf{j}$ in the form $\left(  \left(  m_{1}%
,j_{1}\right)  ,\left(  m_{2},j_{2}\right)  ,...,\left(  m_{k},j_{k}\right)
\right)  $. Then, $e_{\mathbf{j}}^{\varepsilon}=e_{m_{1},j_{1}}e_{m_{2},j_{2}%
}...e_{m_{k},j_{k}}$ and $\deg\mathbf{j}=m_{1}+m_{2}+...+m_{k}=m_{k}%
+m_{k-1}+...+m_{1}$.

Since $e_{\mathbf{j}}^{\varepsilon}=e_{m_{1},j_{1}}e_{m_{2},j_{2}}%
...e_{m_{k},j_{k}}$, we have%
\begin{align}
&  \left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{m_{1},j_{1}}e_{m_{2},j_{2}}...e_{m_{k},j_{k}}v_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon
}}=\left(  -1\right)  ^{k}\left(  e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}v_{\lambda}%
^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we applied the }\mathfrak{g}%
^{\varepsilon}\text{-invariance of the form }\left(  \cdot,\cdot\right)
_{\lambda}^{\mathfrak{g}^{\varepsilon}}\text{ for a total of }k\text{
times}\right) \nonumber\\
&  =\left(  \left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}v_{\lambda}%
^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.1}%
\end{align}


Write the sequence $\mathbf{i}$ in the form $\left(  \left(  n_{1}%
,i_{1}\right)  ,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell
}\right)  \right)  $. Then, $e_{\mathbf{i}}^{\varepsilon}=e_{n_{1},i_{1}%
}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ and $\deg\mathbf{i}=n_{1}%
+n_{2}+...+n_{\ell}$. Now,%
\begin{align}
&  \left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1},j_{1}%
}\cdot\underbrace{e_{\mathbf{i}}^{\varepsilon}}_{=e_{n_{1},i_{1}}%
e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}}\nonumber\\
&  =\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1}%
,j_{1}}\cdot e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\nonumber\\
&  =\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \left(
-1\right)  ^{k}e_{m_{k},j_{k}}\otimes e_{m_{k-1},j_{k-1}}\otimes...\otimes
e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}}\otimes e_{n_{2},i_{2}}\otimes...\otimes
e_{n_{\ell},i_{\ell}}\right)  . \label{pf.invformnondeg.polynomiality.2}%
\end{align}
Denote the tensor $\left(  -1\right)  ^{k}e_{m_{k},j_{k}}\otimes
e_{m_{k-1},j_{k-1}}\otimes...\otimes e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}%
}\otimes e_{n_{2},i_{2}}\otimes...\otimes e_{n_{\ell},i_{\ell}}$ by $c$. Then,
(\ref{pf.invformnondeg.polynomiality.2}) rewrites as%
\begin{equation}
\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1},j_{1}%
}\cdot e_{\mathbf{i}}^{\varepsilon}=\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}c. \label{pf.invformnondeg.polynomiality.3}%
\end{equation}


Since
\begin{align*}
c  &  =\left(  -1\right)  ^{k}e_{m_{k},j_{k}}\otimes e_{m_{k-1},j_{k-1}%
}\otimes...\otimes e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}}\otimes
e_{n_{2},i_{2}}\otimes...\otimes e_{n_{\ell},i_{\ell}}\\
&  \in T\left(  \mathfrak{g}\right)  \left[  m_{k}+m_{k-1}+...+m_{1}%
+n_{1}+n_{2}+...+n_{\ell}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }e_{m_{k},j_{k}}\in\mathfrak{g}_{m_{k}}\text{, }e_{m_{k-1}%
,j_{k-1}}\in\mathfrak{g}_{m_{k-1}}\text{, }...\text{, }e_{m_{1},j_{1}}%
\in\mathfrak{g}_{m_{1}}\\
\text{and }e_{n_{1},i_{1}}\in\mathfrak{g}_{n_{1}}\text{, }e_{n_{2},i_{2}}%
\in\mathfrak{g}_{n_{2}}\text{, }...\text{, }e_{n_{\ell},i_{\ell}}%
\in\mathfrak{g}_{n_{\ell}}%
\end{array}
\right) \\
&  =T\left(  \mathfrak{g}\right)  \left[  0\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{m_{k}+m_{k-1}%
+...+m_{1}}_{=\deg\mathbf{j}}+\underbrace{n_{1}+n_{2}+...+n_{\ell}}%
_{=\deg\mathbf{i}}=\deg\mathbf{j}+\deg\mathbf{i}=\deg\mathbf{i}+\deg
\mathbf{j}=0\right)  ,
\end{align*}
we can apply Lemma \ref{lem.invformnondeg.polynomiality2} to $n=0$. We
conclude that there exists a polynomial map $d:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}_{-}\right)  \left[
0\right]  $ such that every $\lambda\in\mathfrak{h}^{\ast}$ and every
$\varepsilon\in\mathbb{C}$ satisfy%
\begin{equation}
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.4}%
\end{equation}
Since $T\left(  \mathfrak{n}_{-}\right)  \left[  0\right]  =\mathbb{C}$
(because $\mathfrak{n}_{-}$ is concentrated in negative degrees), this
polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  0\right]  $ is a polynomial function
$d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$. Denote this
function $d$ by $Q_{\mathbf{i},\mathbf{j}}$. Then, every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy $d\left(
\lambda,\varepsilon\right)  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda
,\varepsilon\right)  $ and thus $\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon\right)  \right)
=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  $ (since
$Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \in\mathbb{C}$).
Thus, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon
\in\mathbb{C}$ satisfy%
\begin{align}
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}  &  =\underbrace{\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  }_{=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  }v_{\lambda}^{+\mathfrak{g}^{\varepsilon
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.invformnondeg.polynomiality.4}%
)}\right) \nonumber\\
&  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \cdot
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.5}%
\end{align}


Now, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in
\mathbb{C}$ satisfy%
\begin{align*}
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}  &  =\left(
\underbrace{\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}}%
_{\substack{=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}c\\\text{(by (\ref{pf.invformnondeg.polynomiality.3}))}}}v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.invformnondeg.polynomiality.1})}\right) \\
&  =\left(  \underbrace{\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}c\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}%
}_{\substack{=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)
\cdot v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\\text{(by
(\ref{pf.invformnondeg.polynomiality.5}))}}},v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=\left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \cdot v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\\
&  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)
\cdot\underbrace{\left(  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}%
,v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}%
^{\varepsilon}}}_{=1}=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon
\right)  .
\end{align*}
This proves Lemma \ref{lem.invformnondeg.polynomiality}.

We shall now take a closer look at the polynomial function $Q_{\mathbf{i}%
,\mathbf{j}}$ of Lemma \ref{lem.invformnondeg.polynomiality}:

\begin{lemma}
\label{lem.invformnondeg.polynomiality3}Let $\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E$ and $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$. Consider
the polynomial function $Q_{\mathbf{i},\mathbf{j}}:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow\mathbb{C}$ of Lemma
\ref{lem.invformnondeg.polynomiality}. Then, every $\lambda\in\mathfrak{h}%
^{\ast}$ and every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  .
\]

\end{lemma}

Note that Lemma \ref{lem.invformnondeg.polynomiality3} does not really need
the conditions $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ and
$\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$. It is sufficient that
$\mathbf{i}\in\operatorname*{Seq}E$ is such that no element $\left(
n,i\right)  $ of the sequence $\mathbf{i}$ satisfies $n=0$, and that a similar
condition holds for $\mathbf{j}$. But since we will only use Lemma
\ref{lem.invformnondeg.polynomiality3} in the case when $\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E$ and $\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E$, we would not gain much from thus generalizing it.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality3}.} We recall that
the definition of $Q_{\mathbf{i},\mathbf{j}}$ said that%
\begin{equation}
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}\lambda\in\mathfrak{h}^{\ast}\text{ and }\varepsilon\in\mathbb{C}.
\label{pf.invformnondeg.polynomiality3.1}%
\end{equation}


Let $\lambda\in\mathfrak{h}^{\ast}$ be arbitrary, and let $\varepsilon
\in\mathbb{C}$ be nonzero. Since $\varepsilon\neq0$, the Lie algebra
isomorphism $J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow
\mathfrak{g}$ exists and satisfies $\left(  \lambda/\varepsilon^{2}\right)
\circ J_{\varepsilon}=\lambda$. Hence, we have an isomorphism $J_{\varepsilon
}:\left(  \mathfrak{g}^{\varepsilon},\lambda\right)  \rightarrow\left(
\mathfrak{g},\lambda/\varepsilon^{2}\right)  $ in the category of pairs of a
$\mathbb{Z}$-graded Lie algebra and a linear form on its $0$-th homogeneous
component (where the morphisms in this category are defined in the obvious
way). This isomorphism induces a corresponding isomorphism $M_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\rightarrow M_{\lambda/\varepsilon^{2}%
}^{+\mathfrak{g}}$ of Verma modules which sends $xv_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ to $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
x\right)  v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}}$ for every $x\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  $ (where $U\left(  J_{\varepsilon
}\right)  $ is the isomorphism $U\left(  \mathfrak{g}^{\varepsilon}\right)
\rightarrow U\left(  \mathfrak{g}\right)  $ canonically induced by the Lie
algebra isomorphism $J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow
\mathfrak{g}$). Similarly, we get an isomorphism $M_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\rightarrow M_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}$ of
Verma modules which sends $yv_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}$ to
$\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  y\right)
v_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}$ for every $y\in U\left(
\mathfrak{g}^{\varepsilon}\right)  $. Since the bilinear form $\left(
\cdot,\cdot\right)  _{\mu}^{\mathfrak{e}}$ depends functorially on a
$\mathbb{Z}$-graded Lie algebra $\mathfrak{e}$ and a linear form
$\mu:\mathfrak{e}_{0}\rightarrow\mathbb{C}$, these isomorphisms leave the
bilinear form unchanged, i. e., we have%
\[
\left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  x\right)
v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon
}\right)  \right)  \left(  y\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}=\left(
xv_{\lambda}^{+\mathfrak{g}^{\varepsilon}},yv_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}%
\]
for every $x\in U\left(  \mathfrak{g}^{\varepsilon}\right)  $ and $y\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  $. Applied to $x=e_{\mathbf{i}%
}^{\varepsilon}$ and $y=e_{\mathbf{j}}^{\varepsilon}$, this yields%
\begin{equation}
\left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  v_{\lambda/\varepsilon^{2}}%
^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}=\left(
e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  \label{pf.invformnondeg.polynomiality3.2}%
\end{equation}
(by the definition of $Q_{\mathbf{i},\mathbf{j}}$).

But we have $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  =\varepsilon^{\operatorname*{len}%
\mathbf{i}}e_{\mathbf{i}}^{1}$\ \ \ \ \footnote{\textit{Proof.} Write the
sequence $\mathbf{i}$ in the form $\left(  \left(  n_{1},i_{1}\right)
,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
$. Since $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$, all of the numbers
$n_{1}$, $n_{2}$, $...$, $n_{\ell}$ are negative, so that none of them is $0$.
As a consequence, $\delta_{n_{u},0}=0$ for every $u\in\left\{  1,2,...,\ell
\right\}  $. By the definition of $J_{\varepsilon}$, we have%
\begin{align*}
J_{\varepsilon}\left(  e_{n_{u},i_{u}}\right)   &  =\underbrace{\varepsilon
^{1+\delta_{n_{u},0}}}_{\substack{=\varepsilon\\\text{(since }\delta_{n_{u}%
,0}=0\text{)}}}e_{n_{u},i_{u}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}e_{n_{u},i_{u}}\in\mathfrak{g}_{n_{u}}\right) \\
&  =\varepsilon e_{n_{u},i_{u}}%
\end{align*}
for every $u\in\left\{  1,2,...,\ell\right\}  $.
\par
Now, $e_{\mathbf{i}}^{\varepsilon}$ is defined as the product $e_{n_{1},i_{1}%
}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ in $U\left(  \mathfrak{g}%
^{\varepsilon}\right)  $, and $e_{\mathbf{i}}^{1}$ is defined as the product
$e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ in $U\left(
\mathfrak{g}^{1}\right)  $. Hence,%
\begin{align*}
\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  e_{\mathbf{i}%
}^{\varepsilon}\right)   &  =\left(  U\left(  J_{\varepsilon}\right)  \right)
\left(  e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }e_{\mathbf{i}}^{\varepsilon}%
=e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\right) \\
&  =J_{\varepsilon}\left(  e_{n_{1},i_{1}}\right)  J_{\varepsilon}\left(
e_{n_{2},i_{2}}\right)  ...J_{\varepsilon}\left(  e_{n_{\ell},i_{\ell}}\right)
\\
&  =\varepsilon e_{n_{1},i_{1}}\cdot\varepsilon e_{n_{2},i_{2}}\cdot
...\cdot\varepsilon e_{n_{\ell},i_{\ell}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }J_{\varepsilon}\left(  e_{n_{u},i_{u}}\right)  =\varepsilon
e_{n_{u},i_{u}}\text{ for every }u\in\left\{  1,2,...,\ell\right\}  \right) \\
&  =\varepsilon^{\ell}\underbrace{e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell
},i_{\ell}}}_{=e_{\mathbf{i}}^{1}}=\varepsilon^{\ell}e_{\mathbf{i}}%
^{1}=\varepsilon^{\operatorname*{len}\mathbf{i}}e_{\mathbf{i}}^{1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell=\operatorname*{len}%
\mathbf{i}\text{ by the definition of }\operatorname*{len}\mathbf{i}\right)  ,
\end{align*}
qed.} and similarly $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  =\varepsilon^{\operatorname*{len}%
\mathbf{j}}e_{\mathbf{j}}^{1}$. Hence,%
\begin{align*}
&  \left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  v_{\lambda/\varepsilon^{2}}%
^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}\\
&  =\left(  \varepsilon^{\operatorname*{len}\mathbf{i}}e_{\mathbf{i}}%
^{1}v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}},\varepsilon
^{\operatorname*{len}\mathbf{j}}e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}%
}=\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}\left(  e_{\mathbf{i}}^{1}v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}%
},e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}\right)
_{\lambda/\varepsilon^{2}}^{\mathfrak{g}}\\
&  =\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}\underbrace{\left(  e_{\mathbf{i}}^{1}v_{\lambda/\varepsilon^{2}%
}^{+\mathfrak{g}^{1}},e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}^{1}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}^{1}}%
}_{\substack{=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon
^{2},1\right)  \\\text{(by (\ref{pf.invformnondeg.polynomiality3.1}), applied
to }\lambda/\varepsilon^{1}\text{ and }1\text{ instead of }\lambda\text{ and
}\varepsilon\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}%
=\mathfrak{g}^{1}\right) \\
&  =\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  .
\end{align*}
Compared to (\ref{pf.invformnondeg.polynomiality3.2}), this yields
$Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  $. This proves Lemma
\ref{lem.invformnondeg.polynomiality3}.

Here is the consequence of Lemmas \ref{lem.invformnondeg.polynomiality} and
\ref{lem.invformnondeg.polynomiality3} that we will actually use:

\begin{corollary}
\label{cor.invformnondeg.polynomiality}Let $n\in\mathbb{N}$. Let
$\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}%
\sum\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$ (we are using the fact
that $\sum\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}\sum
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$, which we proved above).

Then, there exists a polynomial function $Q_{n}:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow\mathbb{C}$ such that every $\lambda\in
\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\begin{equation}
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  =Q_{n}\left(  \lambda,\varepsilon\right)  .
\label{cor.invformnondeg.polynomiality.1}%
\end{equation}
This function $Q_{n}$ satisfies%
\[
Q_{n}\left(  \lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}%
n}Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }\lambda\in\mathfrak{h}^{\ast}\text{ and every nonzero }\varepsilon
\in\mathbb{C}.
\]

\end{corollary}

\textit{Proof of Corollary \ref{cor.invformnondeg.polynomiality}.} For any
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ satisfying $\deg
\mathbf{i}=-n$, and any $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$
satisfying $\deg\mathbf{j}=n$, consider the polynomial function $Q_{\mathbf{i}%
,\mathbf{j}}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$ of
Lemma \ref{lem.invformnondeg.polynomiality}. Define a polynomial function
$Q_{n}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$ by%
\[
Q_{n}=\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  .
\]
Then, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon
\in\mathbb{C}$ satisfy%
\begin{align*}
Q_{n}\left(  \lambda,\varepsilon\right)   &  =\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  =\det\left(  \left(  \left(  e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon
}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda,n}%
^{\mathfrak{g}^{\varepsilon}}\right)  _{\substack{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Lemma \ref{lem.invformnondeg.polynomiality} yields}\\
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\left(
e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
,n}^{\mathfrak{g}^{\varepsilon}}\\
\text{(since }\deg\mathbf{i}=-n\text{ yields }e_{\mathbf{i}}^{\varepsilon}\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  \left[  -n\right]  \text{ and thus
}e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in
M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right] \\
\text{and similarly }e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\in M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]
\text{)}%
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  .
\end{align*}
We have thus proven that every $\lambda\in\mathfrak{h}^{\ast}$ and every
$\varepsilon\in\mathbb{C}$ satisfy $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{\varepsilon}}\right)  =Q_{n}\left(  \lambda
,\varepsilon\right)  $.

Now, it remains to show that this function $Q_{n}$ satisfies $Q_{n}\left(
\lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}n}Q_{n}\left(
\lambda/\varepsilon^{2},1\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$
and every nonzero $\varepsilon\in\mathbb{C}$. In order to do this, we let
$\lambda\in\mathfrak{h}^{\ast}$ be arbitrary and $\varepsilon\in\mathbb{C}$ be
nonzero. Then,
\begin{equation}
Q_{n}\left(  \lambda,\varepsilon\right)  =\det\left(  \left(  Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  =\det\left(  \left(  \varepsilon^{\operatorname*{len}\mathbf{i}%
}\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)
\label{pf.invformnondeg.polynomiality5.1}%
\end{equation}
(since Lemma \ref{lem.invformnondeg.polynomiality3} yields $Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  $ for all
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ and$\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$).

Now, recall that if we multiply a row of a square matrix by some scalar, then
the determinant of the matrix is also multiplied by the same scalar. A similar
fact holds for the columns. Thus,
\begin{align*}
&  \det\left(  \left(  \varepsilon^{\operatorname*{len}\mathbf{i}}%
\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  =\left(  \prod\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}%
\mathbf{i}}\right)  \cdot\left(  \prod\limits_{\substack{\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon
^{\operatorname*{len}\mathbf{j}}\right)  \cdot\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)
\end{align*}
(because the matrix $\left(  \varepsilon^{\operatorname*{len}\mathbf{i}%
}\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}$ is obtained from the
matrix $\left(  Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon
^{2},1\right)  \right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}$ by multiplying every row $\mathbf{i}$ by
the scalar $\varepsilon^{\operatorname*{len}\mathbf{i}}$ and multiplying every
column $\mathbf{j}$ by the scalar $\varepsilon^{\operatorname*{len}\mathbf{j}%
}$). Hence, (\ref{pf.invformnondeg.polynomiality5.1}) becomes%
\begin{equation}
Q_{n}\left(  \lambda,\varepsilon\right)  =\left(  \prod
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}\mathbf{i}}\right)
\cdot\left(  \prod\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}\right)  \cdot\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  .
\label{pf.invformnondeg.polynomiality5.3}%
\end{equation}


Now, since $\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}%
\mathbf{i}$, we have $\varepsilon^{\operatorname*{LEN}n}=\prod
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}\mathbf{i}}$. Also,
since $\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{j}\in
\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{j}=n}}\operatorname*{len}%
\mathbf{j}$, we have $\varepsilon^{\operatorname*{LEN}n}=\prod
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}}$. Thus,%
\begin{equation}
\underbrace{\left(  \prod\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}%
\mathbf{i}}\right)  }_{=\varepsilon^{\operatorname*{LEN}n}}\cdot
\underbrace{\left(  \prod\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}\right)  }_{=\varepsilon^{\operatorname*{LEN}n}}=\varepsilon
^{\operatorname*{LEN}n}\varepsilon^{\operatorname*{LEN}n}=\varepsilon
^{2\operatorname*{LEN}n}. \label{pf.invformnondeg.polynomiality5.6}%
\end{equation}


On the other hand, since $Q_{n}=\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}%
}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}%
=-n;\ \deg\mathbf{j}=n}}\right)  $, we have%
\begin{equation}
Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  =\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  . \label{pf.invformnondeg.polynomiality5.5}%
\end{equation}
Hence, (\ref{pf.invformnondeg.polynomiality5.3}) becomes%
\begin{align*}
&  Q_{n}\left(  \lambda,\varepsilon\right) \\
&  =\underbrace{\left(  \prod\limits_{\substack{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon
^{\operatorname*{len}\mathbf{i}}\right)  \cdot\left(  \prod
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}}\right)
}_{\substack{=\varepsilon^{2\operatorname*{LEN}n}\\\text{(by
(\ref{pf.invformnondeg.polynomiality5.6}))}}}\cdot\underbrace{\det\left(
\left(  Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)
\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}%
=-n;\ \deg\mathbf{j}=n}}\right)  }_{\substack{=Q_{n}\left(  \lambda
/\varepsilon^{2},1\right)  \\\text{(by
(\ref{pf.invformnondeg.polynomiality5.5}))}}}\\
&  =\varepsilon^{2\operatorname*{LEN}n}\cdot Q_{n}\left(  \lambda
/\varepsilon^{2},1\right)  .
\end{align*}
We have thus proven that $Q_{n}\left(  \lambda,\varepsilon\right)
=\varepsilon^{2\operatorname*{LEN}n}Q_{n}\left(  \lambda/\varepsilon
^{2},1\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$ and every nonzero
$\varepsilon\in\mathbb{C}$. This concludes the proof of Corollary
\ref{cor.invformnondeg.polynomiality}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: On leading terms of
pseudo-homogeneous polynomial maps}

The following lemma about polynomial maps could be an easy exercise in any
algebra text. Unfortunately I do not see a quick way to prove it, so the proof
is going to take a few pages. Reading it will probably waste more of the
reader's time than proving it on her own.

\begin{lemma}
\label{lem.invformnondeg.elemen}Let $V$ be a finite-dimensional $\mathbb{C}%
$-vector space. Let $k\in\mathbb{N}$. Let $\phi:V\times\mathbb{C}%
\rightarrow\mathbb{C}$ be a polynomial function such that every $\lambda\in V$
and every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
\phi\left(  \lambda,\varepsilon\right)  =\varepsilon^{2k}\phi\left(
\lambda/\varepsilon^{2},1\right)  .
\]
Then:

\textbf{(a)} The polynomial function
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,0\right)
\]
is homogeneous of degree $k$.

\textbf{(b)} For every integer $N>k$, the $N$-th homogeneous component of the
polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,1\right)
\]
is zero.

\textbf{(c)} The $k$-th homogeneous component of the polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,1\right)
\]
is the polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,0\right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.invformnondeg.elemen}.} \textbf{(a)} Let
$\left(  v_{1},v_{2},...,v_{n}\right)  $ be a basis of the vector space
$V^{\ast}$. Let $\pi_{V}:V\times\mathbb{C}\rightarrow V$ and $\pi_{\mathbb{C}%
}:V\times\mathbb{C}\rightarrow\mathbb{C}$ be the canonical projections. Then,
$\left(  v_{1}\circ\pi_{V},v_{2}\circ\pi_{V},...,v_{n}\circ\pi_{V}%
,\pi_{\mathbb{C}}\right)  $ is a basis of the vector space $\left(
V\times\mathbb{C}\right)  ^{\ast}$.

Therefore, since $\phi$ is a polynomial function, there exists a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{n},X_{n+1}\right]  $ such that every
$w\in V\times\mathbb{C}$ satisfies%
\[
\phi\left(  w\right)  =P\left(  \left(  v_{1}\circ\pi_{V}\right)  \left(
w\right)  ,\left(  v_{2}\circ\pi_{V}\right)  \left(  w\right)  ,...,\left(
v_{n}\circ\pi_{V}\right)  \left(  w\right)  ,\pi_{\mathbb{C}}\left(  w\right)
\right)  .
\]
In other words, every $\left(  \lambda,\varepsilon\right)  \in V\times
\mathbb{C}$ satisfies%
\begin{equation}
\phi\left(  \lambda,\varepsilon\right)  =P\left(  v_{1}\left(  \lambda\right)
,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda\right)  ,\varepsilon
\right)  . \label{pf.invformnondeg.elemen.1}%
\end{equation}


Now, it is easy to see that for every $\left(  x_{1},x_{2},...,x_{n}\right)
\in\mathbb{C}^{n}$ and nonzero $\varepsilon\in\mathbb{C}$, we have
\begin{equation}
P\left(  x_{1},x_{2},...,x_{n},\varepsilon\right)  =\varepsilon^{2k}P\left(
x_{1}/\varepsilon^{2},x_{2}/\varepsilon^{2},...,x_{n}/\varepsilon
^{2},1\right)  . \label{pf.invformnondeg.elemen.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.invformnondeg.elemen.2}).} Let $\left(
x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ be arbitrary, and let
$\varepsilon\in\mathbb{C}$ be nonzero.
\par
Let $\lambda\in V$ be a vector satisfying%
\[
v_{i}\left(  \lambda\right)  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\]
(such a vector $\lambda$ exists since $\left(  v_{1},v_{2},...,v_{n}\right)  $
is a basis of $V^{\ast}$). Then,%
\begin{align*}
P\left(  x_{1},x_{2},...,x_{n},\varepsilon\right)   &  =P\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  ,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
x_{i}=v_{i}\left(  \lambda\right)  \text{ for every }i\in\left\{
1,2,...,n\right\}  \right) \\
&  =\phi\left(  \lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.invformnondeg.elemen.1})}\right) \\
&  =\varepsilon^{2k}\underbrace{\phi\left(  \lambda/\varepsilon^{2},1\right)
}_{\substack{=P\left(  v_{1}\left(  \lambda/\varepsilon^{2}\right)
,v_{2}\left(  \lambda/\varepsilon^{2}\right)  ,...,v_{n}\left(  \lambda
/\varepsilon^{2}\right)  ,1\right)  \\\text{(by
(\ref{pf.invformnondeg.elemen.1}), applied to }\left(  \lambda/\varepsilon
^{2},1\right)  \text{ instead of }\left(  \lambda,\varepsilon\right)
\text{)}}}\\
&  =\varepsilon^{2k}P\left(  v_{1}\left(  \lambda/\varepsilon^{2}\right)
,v_{2}\left(  \lambda/\varepsilon^{2}\right)  ,...,v_{n}\left(  \lambda
/\varepsilon^{2}\right)  ,1\right)  =\varepsilon^{2k}P\left(  x_{1}%
/\varepsilon^{2},x_{2}/\varepsilon^{2},...,x_{n}/\varepsilon^{2},1\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v_{i}\left(  \lambda
/\varepsilon^{2}\right)  =\underbrace{v_{i}\left(  \lambda\right)  }_{=x_{i}%
}/\varepsilon^{2}=x_{i}/\varepsilon^{2}\text{ for every }i\in\left\{
1,2,...,n\right\}  \right)  .
\end{align*}
This proves (\ref{pf.invformnondeg.elemen.2}).}

Now, since $P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{n},X_{n+1}\right]
\cong\left(  \mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  \right)  \left[
X_{n+1}\right]  $, we can write the polynomial $P$ as a polynomial in the
variable $X_{n+1}$ over the ring $\mathbb{C}\left[  X_{1},X_{2},...,X_{n}%
\right]  $. In other words, we can write the polynomial $P$ in the form
$P=\sum\limits_{i\in\mathbb{N}}P_{i}\cdot X_{n+1}^{i}$ for some polynomials
$P_{0}$, $P_{1}$, $P_{2}$, $...$ in $\mathbb{C}\left[  X_{1},X_{2}%
,...,X_{n}\right]  $ such that all but finitely many $i\in\mathbb{N}$ satisfy
$P_{i}=0$. Consider these $P_{0}$, $P_{1}$, $P_{2}$, $...$.

Since all but finitely many $i\in\mathbb{N}$ satisfy $P_{i}=0$, there exists a
$d\in\mathbb{N}$ such that every integer $i>d$ satisfies $P_{i}=0$. Consider
this $d$. Then, $P=\sum\limits_{i\in\mathbb{N}}P_{i}\cdot X_{n+1}^{i}%
=\sum\limits_{i=0}^{d}P_{i}\cdot X_{n+1}^{i}$ (here, we have removed all the
terms with $i>d$ from the sum, because every integer $i>d$ satisfies $P_{i}=0$
and thus $P_{i}\cdot X_{n+1}^{i}=0$).

For every $i\in\mathbb{N}$ and every $j\in\mathbb{N}$, let $Q_{i,j}$ be the
$j$-th homogeneous component of the polynomial $P_{i}$. Then, $P_{i}%
=\sum\limits_{j\in\mathbb{N}}Q_{i,j}$ for every $i\in\mathbb{N}$, and each
$Q_{i,j}$ is homogeneous of degree $j$.

Hence,%
\begin{equation}
P=\sum\limits_{i\in\mathbb{N}}\underbrace{P_{i}}_{=\sum\limits_{j\in
\mathbb{N}}Q_{i,j}}\cdot X_{n+1}^{i}=\sum\limits_{i\in\mathbb{N}}%
\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}.
\label{pf.invformnondeg.elemen.3a}%
\end{equation}


Now, we are going to show the following fact: We have%
\begin{equation}
Q_{u,v}=0\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  u,v\right)  \in
\mathbb{N}\times\mathbb{N}\text{ which don't satisfy }u+2v=2k.
\label{pf.invformnondeg.elemen.4}%
\end{equation}


\textit{Proof of (\ref{pf.invformnondeg.elemen.4}).} Let $\left(  u,v\right)
\in\mathbb{N}\times\mathbb{N}$ be such that $u+2v\neq2k$. We must prove that
$Q_{u,v}=0$.

If $u>d$, then $Q_{u,v}=0$ is clear (because $Q_{u,v}$ is the $v$-th
homogeneous component of $P_{u}$, but we have $P_{u}=0$ since $u>d$). Hence,
for the rest of the proof of $Q_{u,v}=0$, we can WLOG assume that $u\leq d$.

We have%
\[
P=\sum\limits_{i=0}^{d}\underbrace{P_{i}}_{=\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}}\cdot X_{n+1}^{i}=\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}X_{n+1}^{i}.
\]


Let $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ and
$\varepsilon\in\mathbb{C}\diagdown\left\{  0\right\}  $. Then, $\varepsilon$
is nonzero, and we have%
\begin{align*}
P\left(  x_{1},x_{2},...,x_{n},1/\varepsilon\right)   &  =\sum\limits_{i=0}%
^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)
\underbrace{\left(  1/\varepsilon\right)  ^{i}}_{=\varepsilon^{d-i}%
/\varepsilon^{d}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum
\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}\right) \\
&  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}\left(
x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}/\varepsilon^{d}=\dfrac
{1}{\varepsilon^{d}}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}%
\end{align*}
and%
\begin{align*}
P\left(  \varepsilon^{2}x_{1},\varepsilon^{2}x_{2},...,\varepsilon^{2}%
x_{n},1\right)   &  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\underbrace{Q_{i,j}\left(  \varepsilon^{2}x_{1},\varepsilon^{2}%
x_{2},...,\varepsilon^{2}x_{n}\right)  }_{\substack{=\left(  \varepsilon
^{2}\right)  ^{j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \\\text{(since
}Q_{i,j}\text{ is homogeneous of degree }j\text{)}}}\underbrace{1^{i}}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{i=0}^{d}%
\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}\right) \\
&  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}\underbrace{\left(
\varepsilon^{2}\right)  ^{j}}_{=\varepsilon^{2j}}Q_{i,j}\left(  x_{1}%
,x_{2},...,x_{n}\right)  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  .
\end{align*}
Now,%
\begin{align*}
&  \dfrac{1}{\varepsilon^{d}}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}\\
&  =P\left(  x_{1},x_{2},...,x_{n},1/\varepsilon\right)  =\left(
1/\varepsilon\right)  ^{2k}\underbrace{P\left(  x_{1}/\left(  \dfrac
{1}{\varepsilon}\right)  ^{2},x_{2}/\left(  \dfrac{1}{\varepsilon}\right)
^{2},...,x_{n}/\left(  \dfrac{1}{\varepsilon}\right)  ^{2},1\right)
}_{=P\left(  \varepsilon^{2}x_{1},\varepsilon^{2}x_{2},...,\varepsilon
^{2}x_{n},1\right)  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.invformnondeg.elemen.2}),
applied to }1/\varepsilon\text{ instead of }\varepsilon\right) \\
&  =\left(  1/\varepsilon\right)  ^{2k}\sum\limits_{i=0}^{d}\sum
\limits_{j\in\mathbb{N}}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2}%
,...,x_{n}\right)  ,
\end{align*}
so that%
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}=\varepsilon
^{d}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}\varepsilon^{2j}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  .
\]
For fixed $\varepsilon$, this is a polynomial identity in $\left(  x_{1}%
,x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$. Since it holds for all $\left(
x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ (as we just have shown), it
thus must hold as a formal identity, i. e., we must have%
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\varepsilon^{d-i}=\varepsilon^{d}\sum\limits_{i=0}^{d}\sum
\limits_{j\in\mathbb{N}}\varepsilon^{2j}Q_{i,j}\ \ \ \ \ \ \ \ \ \ \text{in
}\mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  .
\]
Let us take the $v$-th homogeneous components of both sides of this equation.
Since each $Q_{i,j}$ is homogeneous of degree $j$, this amounts to removing
all $Q_{i,j}$ with $j\neq v$, and leaving the $Q_{i,j}$ with $j=v$ unchanged.
Thus, we obtain%
\begin{equation}
\varepsilon^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\varepsilon^{d-i}=\varepsilon
^{d}\sum\limits_{i=0}^{d}\varepsilon^{2v}Q_{i,v}\ \ \ \ \ \ \ \ \ \ \text{in
}\mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  .
\label{pf.invformnondeg.elemen.6}%
\end{equation}


Now, let $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ be
arbitrary again. Then, evaluating the identity
(\ref{pf.invformnondeg.elemen.6}) at $\left(  X_{1},X_{2},...,X_{n}\right)
=\left(  x_{1},x_{2},...,x_{n}\right)  $, we obtain
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}%
\right)  \varepsilon^{d-i}=\varepsilon^{d}\sum\limits_{i=0}^{d}\varepsilon
^{2v}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}\right)  .
\]
For fixed $\left(  x_{1},x_{2},...,x_{n}\right)  $, this is a polynomial
identity in $\varepsilon$ (since $d-i\geq0$ for all $i\in\left\{
0,1,...,d\right\}  $). Since it holds for all nonzero $\varepsilon
\in\mathbb{C}$ (as we just have shown), it thus must hold as a formal identity
(since any polynomial in one variable which evaluates to zero at all nonzero
complex numbers must be the zero polynomial). In other words, we must have%
\[
E^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}\right)
E^{d-i}=E^{d}\sum\limits_{i=0}^{d}E^{2v}Q_{i,v}\left(  x_{1},x_{2}%
,...,x_{n}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }\mathbb{C}\left[  E\right]
\]
(where $\mathbb{C}\left[  E\right]  $ denotes the polynomial ring over
$\mathbb{C}$ in one variable $E$). Let us compare the coefficients of
$E^{2k+d-u}$ on both sides of this equation: The coefficient of $E^{2k+d-u}$
on the left hand side of this equation is clearly $Q_{u,v}\left(  x_{1}%
,x_{2},...,x_{n}\right)  $, while the coefficient of $E^{2k+d-u}$ on the right
hand side is $0$ (in fact, the only coefficient on the right hand side of the
equation which is not trivially zero is the coefficient of $E^{d+2v}$, but
$d+2v\neq2k+d-u$ (since $u+2v\neq2k$ and thus $2v\neq2k-u$)). Hence,
comparison yields $Q_{u,v}\left(  x_{1},x_{2},...,x_{n}\right)  =0$. Since
this holds for all $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$,
we thus obtain $Q_{u,v}=0$ (because any polynomial which vanishes on the whole
$\mathbb{C}^{n}$ must be the zero polynomial). This proves
(\ref{pf.invformnondeg.elemen.4}).

Now, (\ref{pf.invformnondeg.elemen.3a}) rewrites as%
\begin{align*}
P  &  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}Q_{i,j}%
X_{n+1}^{i}=\sum\limits_{u\in\mathbb{N}}\sum\limits_{v\in\mathbb{N}}%
Q_{u,v}X_{n+1}^{u}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the
indices }i\text{ and }j\text{ as }u\text{ and }v\right) \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{N}\times\mathbb{N}}%
Q_{u,v}X_{n+1}^{u}=\sum\limits_{\substack{\left(  u,v\right)  \in
\mathbb{N}\times\mathbb{N};\\u+2v=2k}}Q_{u,v}X_{n+1}^{u}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we removed from our sum all terms for }\left(  u,v\right)
\in\mathbb{N}\times\mathbb{N}\text{ which}\\
\text{don't satisfy }u+2v=2k\text{ (because (\ref{pf.invformnondeg.elemen.4})
shows that these terms}\\
\text{don't contribute anything to the sum)}%
\end{array}
\right) \\
&  =\sum\limits_{v=0}^{k}Q_{2k-2v,v}X_{n+1}^{2k-2v}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we substituted }\left(  2k-2v,v\right)  \text{ for }\left(
u,v\right)  \text{ in the sum}\right)  .
\end{align*}


Now, for every $v\in\left\{  0,1,...,k\right\}  $, let $\psi_{v}%
:V\rightarrow\mathbb{C}$ be the polynomial map defined by%
\[
\psi_{v}\left(  \lambda\right)  =Q_{2k-2v,v}\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in V.
\]
Then, $\psi_{v}$ is homogeneous of degree $v$ (since $Q_{2k-2v,v}$ is
homogeneous of degree $v$). In particular, this yields that $\psi_{k}$ is
homogeneous of degree $k$.

Every $\left(  \lambda,\varepsilon\right)  \in V\times\mathbb{C}$ satisfies%
\begin{align}
\phi\left(  \lambda,\varepsilon\right)   &  =P\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  ,\varepsilon\right) \nonumber\\
&  =\sum\limits_{v=0}^{k}\underbrace{Q_{2k-2v,v}\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  \right)  }_{=\psi_{v}\left(  \lambda\right)  }\varepsilon
^{2k-2v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{v=0}%
^{k}Q_{2k-2v,v}X_{n+1}^{2k-2v}\right) \nonumber\\
&  =\sum\limits_{v=0}^{k}\psi_{v}\left(  \lambda\right)  \varepsilon^{2k-2v}.
\label{pf.invformnondeg.elemen.10}%
\end{align}
Applied to $\varepsilon=0$, this yields%
\[
\phi\left(  \lambda,0\right)  =\sum\limits_{v=0}^{k}\psi_{v}\left(
\lambda\right)  0^{2k-2v}=\psi_{k}\left(  \lambda\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0^{2k-2v}=0\text{ for all
}v<k\right)
\]
for every $\lambda\in V$. Hence, the polynomial function $V\rightarrow
\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,0\right)  $ equals the
polynomial function $\psi_{k}$, and thus is homogeneous of degree $k$ (since
$\psi_{k}$ is homogeneous of degree $k$). This proves Lemma
\ref{lem.invformnondeg.elemen} \textbf{(a)}.

Applying (\ref{pf.invformnondeg.elemen.10}) to $\varepsilon=1$, we obtain%
\[
\phi\left(  \lambda,1\right)  =\sum\limits_{v=0}^{k}\psi_{v}\left(
\lambda\right)  \underbrace{1^{2k-2v}}_{=1}=\sum\limits_{v=0}^{k}\psi
_{v}\left(  \lambda\right)  .
\]
Hence, the polynomial function $V\rightarrow\mathbb{C},\ \lambda\mapsto
\phi\left(  \lambda,1\right)  $ equals the sum $\sum\limits_{v=0}^{k}\psi_{v}%
$. Since we know that the polynomial function $\psi_{v}$ is homogeneous of
degree $v$ for every $v\in\left\{  0,1,...,k\right\}  $, this yields that, for
every integer $N>k$, the $N$-th homogeneous component of the polynomial
function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,1\right)
$ is zero. This proves Lemma \ref{lem.invformnondeg.elemen} \textbf{(b)}.

Finally, recall that the polynomial function $V\rightarrow\mathbb{C}%
,\ \lambda\mapsto\phi\left(  \lambda,1\right)  $ equals the sum $\sum
\limits_{v=0}^{k}\psi_{v}$, and the polynomial function $\psi_{v}$ is
homogeneous of degree $v$ for every $v\in\left\{  0,1,...,k\right\}  $. Hence,
for every $v\in\left\{  0,1,...,k\right\}  $, the $v$-th homogeneous component
of the polynomial function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(
\lambda,1\right)  $ is $\psi_{v}$. In particular, the $k$-th homogeneous
component of the polynomial function $V\rightarrow\mathbb{C},\ \lambda
\mapsto\phi\left(  \lambda,1\right)  $ is $\psi_{k}$. Since $\psi_{k}$ equals
the function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(
\lambda,0\right)  $, this rewrites as follows: The $k$-th homogeneous
component of the polynomial function $V\rightarrow\mathbb{C},\ \lambda
\mapsto\phi\left(  \lambda,1\right)  $ is the function $V\rightarrow
\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,0\right)  $. This proves Lemma
\ref{lem.invformnondeg.elemen} \textbf{(c)}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The Lie algebra
\texorpdfstring{$\mathfrak{g}^{0}$}{g superscript 0}}

Consider the polynomial function $Q_{n}$ of Corollary
\ref{cor.invformnondeg.polynomiality}. Due to Corollary
\ref{cor.invformnondeg.polynomiality}, it satisfies the condition of Lemma
\ref{lem.invformnondeg.elemen} for $k=\operatorname*{LEN}n$. Hence, Lemma
\ref{lem.invformnondeg.elemen} suggests that we study the Lie algebra
$\mathfrak{g}^{0}$, since this will show us what the function $\mathfrak{h}%
^{\ast}\rightarrow\mathbb{C},$ $\lambda\mapsto Q_{n}\left(  \lambda,0\right)
$ looks like.

First, let us reformulate the definition of $\mathfrak{g}^{0}$ as follows: As
a vector space, $\mathfrak{g}^{0}=\mathfrak{g}$, but the bracket on
$\mathfrak{g}^{0}$ is given by%
\begin{equation}
\left[  \cdot,\cdot\right]  ^{0}:\mathfrak{g}_{i}\otimes\mathfrak{g}%
_{j}\rightarrow\mathfrak{g}_{i+j}\text{ }\text{is }\left\{
\begin{array}
[c]{c}%
\text{zero if }i+j\neq0\text{;}\\
\text{the Lie bracket }\left[  \cdot,\cdot\right]  \text{ of }\mathfrak{g}%
\text{ if }i+j=0
\end{array}
\right.  . \label{prop.det.US.pf.-1}%
\end{equation}


It is very easy to see (from this) that $\left[  \mathfrak{n}_{-}%
,\mathfrak{n}_{-}\right]  ^{0}=0$, $\left[  \mathfrak{n}_{+},\mathfrak{n}%
_{+}\right]  ^{0}=0$, $\left[  \mathfrak{n}_{-},\mathfrak{n}_{+}\right]
^{0}=\left[  \mathfrak{n}_{+},\mathfrak{n}_{-}\right]  ^{0}\subseteq
\mathfrak{h}$ and that $\mathfrak{h}\subseteq Z\left(  \mathfrak{g}%
^{0}\right)  $.

We notice that $\mathfrak{n}_{-}^{0}=\mathfrak{n}_{-}$, $\mathfrak{n}_{+}%
^{0}=\mathfrak{n}_{+}$ and $\mathfrak{h}^{0}=\mathfrak{h}$ as vector spaces.

Since $\left[  \mathfrak{n}_{-}^{0},\mathfrak{n}_{-}^{0}\right]  ^{0}=\left[
\mathfrak{n}_{-},\mathfrak{n}_{-}\right]  ^{0}=0$, the Lie algebra
$\mathfrak{n}_{-}^{0}$ is abelian, so that $U\left(  \mathfrak{n}_{-}%
^{0}\right)  =S\left(  \mathfrak{n}_{-}^{0}\right)  =S\left(  \mathfrak{n}%
_{-}\right)  $. Similarly, $U\left(  \mathfrak{n}_{+}^{0}\right)  =S\left(
\mathfrak{n}_{+}^{0}\right)  =S\left(  \mathfrak{n}_{+}\right)  $.

We notice that%
\begin{equation}
\lambda\left(  \left[  x,y\right]  ^{0}\right)  =\lambda\left(  \left[
x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for any }x\in\mathfrak{g}\text{
and }y\in\mathfrak{g}. \label{prop.det.US.pf.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{prop.det.US.pf.0}).} Let $x\in\mathfrak{g}$
and $y\in\mathfrak{g}$. Since the equation (\ref{prop.det.US.pf.0}) is linear
in each of $x$ and $y$, we can WLOG assume that $x$ and $y$ are homogeneous
(since every element of $\mathfrak{g}$ is a sum of homogeneous elements). So
we can assume that $x\in\mathfrak{g}_{i}$ and $y\in\mathfrak{g}_{j}$ for some
$i\in\mathbb{N}$ and $j\in\mathbb{N}$. Consider these $i$ and $j$. If
$i+j\neq0$, then $\left[  x,y\right]  ^{0}=0$ (by (\ref{prop.det.US.pf.-1}))
and $\lambda\left(  \left[  x,y\right]  \right)  =0$ (since $x\in
\mathfrak{g}_{i}$ and $y\in\mathfrak{g}_{j}$ yield $\left[  x,y\right]
\in\mathfrak{g}_{i+j}$, and due to $i+j\neq0$ the form $\lambda$ annihilates
$\mathfrak{g}_{i+j}$), so that (\ref{prop.det.US.pf.0}) trivially holds in
this case. If $i+j=0$, then $\left[  x,y\right]  ^{0}=\left[  x,y\right]  $
(again by (\ref{prop.det.US.pf.-1})), and thus (\ref{prop.det.US.pf.0}) holds
in this case as well. We have thus proven (\ref{prop.det.US.pf.0}) both in the
case $i+j\neq0$ and in the case $i+j=0$. These cases cover all possibilities,
and thus (\ref{prop.det.US.pf.0}) is proven.}

In the following, we will use the form $\left(  \cdot,\cdot\right)  _{\lambda
}^{\circ}$ defined in Definition \ref{def.lambda_k}. We will only consider
this form for the Lie algebra $\mathfrak{g}$, not for the Lie algebras
$\mathfrak{g}^{\varepsilon}$ and $\mathfrak{g}^{0}$; thus we don't have any
reason to rename it as $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ
\mathfrak{g}}$.

\begin{lemma}
\label{lem.invform.g^0.1}We have%
\begin{equation}
\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  a,b\right)  _{\lambda}^{\circ
}\ \ \ \ \ \ \ \ \ \ \text{for all }a\in S\left(  \mathfrak{n}_{-}\right)
\text{ and }b\in S\left(  \mathfrak{n}_{+}\right)  . \label{prop.det.US.pf.1}%
\end{equation}
Here, $av_{\lambda}^{+\mathfrak{g}^{0}}$ and $bv_{-\lambda}^{-\mathfrak{g}%
^{0}}$ are elements of $M_{\lambda}^{+\mathfrak{g}^{0}}$ and $M_{-\lambda
}^{-\mathfrak{g}^{0}}$, respectively (because $a\in S\left(  \mathfrak{n}%
_{-}\right)  =U\left(  \mathfrak{n}_{-}^{0}\right)  $ and $b\in S\left(
\mathfrak{n}_{+}\right)  =U\left(  \mathfrak{n}_{+}^{0}\right)  $).
\end{lemma}

\textit{Proof of Lemma \ref{lem.invform.g^0.1}.} Let $a\in S\left(
\mathfrak{n}_{-}\right)  $ and $b\in S\left(  \mathfrak{n}_{+}\right)  $ be
arbitrary. Since the claim that $\left(  av_{\lambda}^{+\mathfrak{g}^{0}%
},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  a,b\right)  _{\lambda}^{\circ}$ is linear in each of $a$ and $b$, we
can WLOG assume that $a=a_{1}a_{2}...a_{u}$ for some homogeneous $a_{1}%
,a_{2},...,a_{u}\in\mathfrak{n}_{-}$ and that $b=b_{1}b_{2}...b_{v}$ for some
homogeneous $b_{1},b_{2},...,b_{v}\in\mathfrak{n}_{+}$ (because every element
of $S\left(  \mathfrak{n}_{-}\right)  $ is a $\mathbb{C}$-linear combination
of products of the form $a_{1}a_{2}...a_{u}$ with homogeneous $a_{1}%
,a_{2},...,a_{u}\in\mathfrak{n}_{-}$, and because every element of $S\left(
\mathfrak{n}_{+}\right)  $ is a $\mathbb{C}$-linear combination of products of
the form $b_{1}b_{2}...b_{v}$ with homogeneous $b_{1},b_{2},...,b_{v}%
\in\mathfrak{n}_{+}$).

WLOG assume that $v\geq u$. (Else, the proof is analogous.)

Recall the equality $\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)
=\left(  S\left(  b\right)  av_{\lambda}^{+},v_{-\lambda}^{-}\right)  $ shown
during the proof of Proposition \ref{prop.invform}. Applied to $\mathfrak{g}%
^{0}$ instead of $\mathfrak{g}$, this yields $\left(  av_{\lambda
}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
}^{\mathfrak{g}^{0}}=\left(  S\left(  b\right)  av_{\lambda}^{+\mathfrak{g}%
^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}$.

Since $\mathfrak{h}\subseteq Z\left(  \mathfrak{g}^{0}\right)  $, we have
$\mathfrak{h}\subseteq Z\left(  U\left(  \mathfrak{g}^{0}\right)  \right)  $
(because the center of a Lie algebra always lies in the center of its
universal enveloping algebra).

Since $b=b_{1}b_{2}...b_{v}$, we have $S\left(  b\right)  =\left(  -1\right)
^{v}b_{v}b_{v-1}...b_{1}$. Combined with $a=a_{1}a_{2}...a_{u}$, this yields%
\[
S\left(  b\right)  a=\left(  -1\right)  ^{v}b_{v}b_{v-1}...b_{1}a_{1}%
a_{2}...a_{u},
\]
so that%
\begin{equation}
\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  \underbrace{S\left(  b\right)
a}_{=\left(  -1\right)  ^{v}b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}}v_{\lambda
}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
}^{\mathfrak{g}^{0}}=\left(  -1\right)  ^{v}\left(  b_{v}b_{v-1}...b_{1}%
a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}. \label{prop.det.US.pf.4}%
\end{equation}


We will now prove some identities in order to simplify the $b_{v}%
b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}$ term here.

First: In the Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}^{0}}$ of
$\left(  \mathfrak{g}^{0},\lambda\right)  $, we have%
\begin{align}
\beta\alpha_{1}\alpha_{2}...\alpha_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\sum\limits_{p=1}^{\ell}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}\label{prop.det.US.pf.3}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{N}\text{, }\alpha
_{1},\alpha_{2},...,\alpha_{\ell}\in\mathfrak{n}_{-}\text{ and }\beta
\in\mathfrak{n}_{+}\text{.}\nonumber
\end{align}
\footnote{\textit{Proof of (\ref{prop.det.US.pf.3}).} We will prove
(\ref{prop.det.US.pf.3}) by induction over $\ell$:
\par
\textit{Induction base:} For $\ell=0$, the left hand side of
(\ref{prop.det.US.pf.3}) is $\beta v_{\lambda}^{+\mathfrak{g}^{0}}=0$ (since
$\beta\in\mathfrak{n}_{+}=\mathfrak{n}_{+}^{0}$), and the right hand side of
(\ref{prop.det.US.pf.3}) is $\left(  \text{empty sum}\right)  =0$. Thus, for
$\ell=0$, the equality (\ref{prop.det.US.pf.3}) holds. This completes the
induction base.
\par
\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. Assume that
(\ref{prop.det.US.pf.3}) holds for $\ell=m-1$. We now must show that
(\ref{prop.det.US.pf.3}) holds for $\ell=m$.
\par
Let $\alpha_{1},\alpha_{2},...,\alpha_{m}\in\mathfrak{n}_{-}$ and $\beta
\in\mathfrak{n}_{+}$.
\par
Since (\ref{prop.det.US.pf.3}) holds for $\ell=m-1$, we can apply
(\ref{prop.det.US.pf.3}) to $m-1$ and $\left(  \alpha_{2},\alpha
_{3},...,\alpha_{m}\right)  $ instead of $\ell$ and $\left(  \alpha_{1}%
,\alpha_{2},...,\alpha_{\ell}\right)  $, and thus obtain%
\begin{align*}
\beta\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\sum\limits_{p=1}^{m-1}\lambda\left(  \left[  \beta,\alpha_{p+1}\right]
\right)  \alpha_{2}\alpha_{3}...\alpha_{p-1+1}\alpha_{p+1+1}\alpha
_{p+2+1}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }p\text{ for
}p+1\text{ in the sum}\right)  .
\end{align*}
\par
Now, we notice that $\beta\in\mathfrak{n}_{+}$ and $\alpha_{1}\in
\mathfrak{n}_{-}$, so that $\left[  \beta,\alpha_{1}\right]  ^{0}\in\left[
\mathfrak{n}_{+},\mathfrak{n}_{-}\right]  ^{0}\subseteq\mathfrak{h}\subseteq
Z\left(  U\left(  \mathfrak{g}^{0}\right)  \right)  $. Thus, $\left[
\beta,\alpha_{1}\right]  ^{0}\alpha_{2}\alpha_{3}...\alpha_{m}=\alpha
_{2}\alpha_{3}...\alpha_{m}\left[  \beta,\alpha_{1}\right]  ^{0}$. But since
$\left[  \beta,\alpha_{1}\right]  ^{0}\in\mathfrak{h}=\mathfrak{h}^{0}$, we
also have $\left[  \beta,\alpha_{1}\right]  ^{0}v_{\lambda}^{+\mathfrak{g}%
^{0}}=\lambda\left(  \left[  \beta,\alpha_{1}\right]  ^{0}\right)  v_{\lambda
}^{+\mathfrak{g}^{0}}=\lambda\left(  \left[  \beta,\alpha_{1}\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}$ (since $\lambda\left(  \left[  \beta
,\alpha_{1}\right]  ^{0}\right)  =\lambda\left(  \left[  \beta,\alpha
_{1}\right]  \right)  $ by (\ref{prop.det.US.pf.0})).
\par
We now compute:
\begin{align*}
\beta\alpha_{1}\alpha_{2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\underbrace{\beta\alpha_{1}}_{\substack{=\alpha_{1}\beta+\left[  \beta
,\alpha_{1}\right]  ^{0}\\\text{(since we are in }U\left(  \mathfrak{g}%
^{0}\right)  \text{)}}}\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}=\left(  \alpha_{1}\beta+\left[  \beta,\alpha_{1}\right]
^{0}\right)  \alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}%
}\\
&  =\alpha_{1}\underbrace{\beta\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}}_{\substack{=\sum\limits_{p=2}^{m}\lambda\left(  \left[
\beta,\alpha_{p}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}%
\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}%
}}+\underbrace{\left[  \beta,\alpha_{1}\right]  ^{0}\alpha_{2}\alpha
_{3}...\alpha_{m}}_{\substack{=\alpha_{2}\alpha_{3}...\alpha_{m}\left[
\beta,\alpha_{1}\right]  ^{0}}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\underbrace{\alpha_{1}\sum\limits_{p=2}^{m}\lambda\left(  \left[
\beta,\alpha_{p}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}%
\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}%
_{=\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}+\alpha_{2}\alpha
_{3}...\alpha_{m}\underbrace{\left[  \beta,\alpha_{1}\right]  ^{0}v_{\lambda
}^{+\mathfrak{g}^{0}}}_{\substack{=\lambda\left(  \left[  \beta,\alpha
_{1}\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}}}\\
&  =\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}+\lambda\left(  \left[
\beta,\alpha_{1}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=1}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=1}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}.
\end{align*}
Thus, (\ref{prop.det.US.pf.3}) holds for $\ell=m$. This completes the
induction step. Thus, (\ref{prop.det.US.pf.3}) is proven.}

Next we will show that in the Verma highest-weight module $M_{\lambda
}^{+\mathfrak{g}^{0}}$ of $\left(  \mathfrak{g}^{0},\lambda\right)  $, we have%
\begin{align}
\beta_{\ell}\beta_{\ell-1}...\beta_{1}\alpha_{1}\alpha_{2}...\alpha_{\ell
}v_{\lambda}^{+\mathfrak{g}^{0}}  &  =\left(  -1\right)  ^{\ell}%
\sum\limits_{\sigma\in S_{\ell}}\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{\ell},\beta_{\sigma\left(  \ell\right)  }\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}\label{prop.det.US.pf.2}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{N}\text{, }\alpha
_{1},\alpha_{2},...,\alpha_{\ell}\in\mathfrak{n}_{-}\text{ and }\beta
_{1},\beta_{2},...,\beta_{\ell}\in\mathfrak{n}_{+}\text{.}\nonumber
\end{align}


\textit{Proof of (\ref{prop.det.US.pf.2}).} We will prove
(\ref{prop.det.US.pf.2}) by induction over $\ell$:

\textit{Induction base:} For $\ell=0$, we have $\underbrace{\beta_{\ell}%
\beta_{\ell-1}...\beta_{1}}_{\text{empty product}}\underbrace{\alpha_{1}%
\alpha_{2}...\alpha_{\ell}}_{\text{empty product}}v_{\lambda}^{+\mathfrak{g}%
^{0}}=v_{\lambda}^{+\mathfrak{g}^{0}}$ and \newline$\underbrace{\left(
-1\right)  ^{\ell}}_{=1}\underbrace{\sum\limits_{\sigma\in S_{\ell}}%
}_{\text{sum over }1\text{ element}}\underbrace{\lambda\left(  \left[
\alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(
\left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  \alpha_{\ell},\beta_{\sigma\left(  \ell\right)
}\right]  \right)  }_{\text{empty product}}v_{\lambda}^{+\mathfrak{g}^{0}%
}=v_{\lambda}^{+\mathfrak{g}^{0}}$. Thus, for $\ell=0$, the equality
(\ref{prop.det.US.pf.2}) holds. This completes the induction base.

\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. Assume that
(\ref{prop.det.US.pf.2}) holds for $\ell=m-1$. We now must show that
(\ref{prop.det.US.pf.2}) holds for $\ell=m$.

Let $\alpha_{1},\alpha_{2},...,\alpha_{m}\in\mathfrak{n}_{-}$ and $\beta
_{1},\beta_{2},...,\beta_{m}\in\mathfrak{n}_{+}$.

For every $p\in\left\{  1,2,...,m\right\}  $, let $c_{p}$ denote the
permutation in $S_{m}$ which is written in row form as $\left(
1,2,...,p-1,p+1,p+2,...,m,p\right)  $. (This is the permutation with cycle
decomposition $\left(  1\right)  \left(  2\right)  ...\left(  p-1\right)
\left(  p,p+1,...,m\right)  $.) Since (\ref{prop.det.US.pf.2}) holds for
$\ell=m-1$, we can apply (\ref{prop.det.US.pf.2}) to $m-1$ and $\left(
\alpha_{c_{p}\left(  1\right)  },\alpha_{c_{p}\left(  2\right)  }%
,...,\alpha_{c_{p}\left(  m-1\right)  }\right)  $ instead of $\ell$ and
$\left(  \alpha_{1},\alpha_{2},...,\alpha_{\ell}\right)  $. This results in%
\begin{align*}
&  \beta_{m-1}\beta_{m-2}...\beta_{1}\alpha_{c_{p}\left(  1\right)  }%
\alpha_{c_{p}\left(  2\right)  }...\alpha_{c_{p}\left(  m-1\right)
}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}%
\underbrace{\lambda\left(  \left[  \alpha_{c_{p}\left(  1\right)  }%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{c_{p}\left(  2\right)  },\beta_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  \alpha_{c_{p}\left(  m-1\right)  }%
,\beta_{\sigma\left(  m-1\right)  }\right]  \right)  }_{\substack{=\prod
\limits_{i\in\left\{  1,2,...,m-1\right\}  }\lambda\left(  \left[
\alpha_{c_{p}\left(  i\right)  },\beta_{\sigma\left(  i\right)  }\right]
\right)  =\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  c_{p}%
^{-1}\left(  i\right)  \right)  }\right]  \right)  \\\text{(here, we
substituted }i\text{ for }c_{p}\left(  i\right)  \text{ in the product)}%
}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}\prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\underbrace{\beta_{\sigma\left(  c_{p}%
^{-1}\left(  i\right)  \right)  }}_{=\beta_{\left(  \sigma\circ c_{p}%
^{-1}\right)  \left(  i\right)  }}\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}\prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\left(  \sigma\circ c_{p}%
^{-1}\right)  \left(  i\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m};\ \sigma\left(
m\right)  =m}\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\left(  \sigma\circ
c_{p}^{-1}\right)  \left(  i\right)  }\right]  \right)  v_{\lambda
}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we identified the permutations in }S_{m-1}\text{ with the
permutations}\\
\sigma\in S_{m}\text{ satisfying }\sigma\left(  m\right)  =m
\end{array}
\right) \\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m};\ \sigma\left(
p\right)  =m}\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)
}\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\sigma\text{ for
}\sigma\circ c_{p}^{-1}\text{ in the sum}\right)  .
\end{align*}


The elements $\beta_{m}$, $\beta_{m-1}$, $...$, $\beta_{1}$ all lie in
$\mathfrak{n}_{+}$ and thus commute in $U\left(  \mathfrak{g}^{0}\right)  $
(since $\left[  \mathfrak{n}_{+},\mathfrak{n}_{+}\right]  ^{0}=0$). Thus,
$\beta_{m}\beta_{m-1}...\beta_{1}=\beta_{m-1}\beta_{m-2}...\beta_{1}\beta_{m}$
in $U\left(  \mathfrak{g}^{0}\right)  $, so that%
\begin{align*}
&  \beta_{m}\beta_{m-1}...\beta_{1}\alpha_{1}\alpha_{2}...\alpha_{m}%
v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\beta_{m-1}\beta_{m-2}...\beta_{1}\underbrace{\beta_{m}\alpha_{1}%
\alpha_{2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}_{\substack{=\sum
\limits_{p=1}^{m}\lambda\left(  \left[  \beta_{m},\alpha_{p}\right]  \right)
\alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha
_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\\text{(by (\ref{prop.det.US.pf.3}),
applied to }\beta=\beta_{m}\text{ and }\ell=m\text{)}}}\\
&  =\beta_{m-1}\beta_{m-2}...\beta_{1}\sum\limits_{p=1}^{m}\lambda\left(
\left[  \beta_{m},\alpha_{p}\right]  \right)  \alpha_{1}\alpha_{2}%
...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\sum\limits_{p=1}^{m}\underbrace{\lambda\left(  \left[  \beta_{m}%
,\alpha_{p}\right]  \right)  }_{=\lambda\left(  -\left[  \alpha_{p},\beta
_{m}\right]  \right)  =-\lambda\left(  \left[  \alpha_{p},\beta_{m}\right]
\right)  }\beta_{m-1}\beta_{m-2}...\beta_{1}\underbrace{\alpha_{1}\alpha
_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha_{m}}_{\substack{=\alpha
_{c_{p}\left(  1\right)  }\alpha_{c_{p}\left(  2\right)  }...\alpha
_{c_{p}\left(  m-1\right)  }\\\text{(by the definition of }c_{p}\text{)}%
}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =-\sum\limits_{p=1}^{m}\lambda\left(  \left[  \alpha_{p},\beta_{m}\right]
\right)  \underbrace{\beta_{m-1}\beta_{m-2}...\beta_{1}\alpha_{c_{p}\left(
1\right)  }\alpha_{c_{p}\left(  2\right)  }...\alpha_{c_{p}\left(  m-1\right)
}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)  ^{m-1}\sum
\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)  =m}\prod\limits_{i\in
\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }\lambda\left(
\left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}}\\
&  =\underbrace{-\left(  -1\right)  ^{m-1}}_{=\left(  -1\right)  ^{m}}%
\sum\limits_{p=1}^{m}\sum\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)
=m}\lambda\left(  \left[  \alpha_{p},\underbrace{\beta_{m}}_{\substack{=\beta
_{\sigma\left(  p\right)  }\\\text{(since }\sigma\left(  p\right)  =m\text{)}%
}}\right]  \right)  \prod\limits_{i\in\left\{  1,2,...,m\right\}
\diagdown\left\{  p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta
_{\sigma\left(  i\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m}\sum\limits_{p=1}^{m}\sum\limits_{\sigma\in
S_{m};\ \sigma\left(  p\right)  =m}\underbrace{\lambda\left(  \left[
\alpha_{p},\beta_{\sigma\left(  p\right)  }\right]  \right)  \prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]
\right)  }_{\substack{=\prod\limits_{i\in\left\{  1,2,...,m\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]
\right)  \\=\lambda\left(  \left[  \alpha_{1},\beta_{\sigma\left(  1\right)
}\right]  \right)  \lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  \alpha_{m}%
,\beta_{\sigma\left(  m\right)  }\right]  \right)  }}v_{\lambda}%
^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m}\underbrace{\sum\limits_{p=1}^{m}\sum
\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)  =m}}_{=\sum\limits_{\sigma
\in S_{m}}}\lambda\left(  \left[  \alpha_{1},\beta_{\sigma\left(  1\right)
}\right]  \right)  \lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  \alpha_{m}%
,\beta_{\sigma\left(  m\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m}\sum\limits_{\sigma\in S_{m}}\lambda\left(  \left[
\alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(
\left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  \alpha_{m},\beta_{\sigma\left(  m\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}}.
\end{align*}
In other words, (\ref{prop.det.US.pf.2}) is proven for $\ell=m$. This
completes the induction step. Thus, the induction proof of
(\ref{prop.det.US.pf.2}) is done.

Now, back to proving $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$. Applying (\ref{prop.det.US.pf.2}) to $\ell
=u$, $\alpha_{i}=a_{i}$ and $\beta_{i}=b_{i}$, we obtain%
\[
b_{u}b_{u-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}%
= \left(-1\right)^u
\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}.
\]
Hence, if $v>u$, then%
\begin{align*}
&  b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =b_{v}b_{v-1}...b_{u+2}b_{u+1}\underbrace{b_{u}b_{u-1}...b_{1}a_{1}%
a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)  ^{u}%
\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}}\\
&  =b_{v}b_{v-1}...b_{u+2}b_{u+1}\left(  -1\right)  ^{u}\sum\limits_{\sigma\in
S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]
\right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[
a_{1},b_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
a_{2},b_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(  \left[
a_{u},b_{\sigma\left(  u\right)  }\right]  \right)  b_{v}b_{v-1}%
...b_{u+2}\underbrace{b_{u+1}v_{\lambda}^{+\mathfrak{g}^{0}}}%
_{\substack{=0\\\text{(since }b_{u+1}\in\mathfrak{n}_{+}=\mathfrak{n}_{+}%
^{0}\text{)}}}\\
&  =0,
\end{align*}
and thus%
\begin{align*}
&  \left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{v}\left(  \underbrace{b_{v}b_{v-1}...b_{1}a_{1}%
a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=0},v_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{prop.det.US.pf.4})}\right) \\
&  =0=\left(  a,b\right)  _{\lambda}^{\circ}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the form }\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}\text{
was defined as a restriction of a sum}\\
\bigoplus\limits_{k\geq0}\lambda_{k}:S\left(  \mathfrak{n}_{-}\right)  \times
S\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}\text{ of bilinear
forms }\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(
\mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}\text{,}\\
\text{and thus }\left(  S^{u}\left(  \mathfrak{n}_{-}\right)  ,S^{v}\left(
\mathfrak{n}_{+}\right)  \right)  _{\lambda}^{\circ}=0\text{ for }u\neq
v\text{, so that }\left(  a,b\right)  _{\lambda}^{\circ}=0\\
\text{(since }a\in S^{u}\left(  \mathfrak{n}_{-}\right)  \text{ and }b\in
S^{v}\left(  \mathfrak{n}_{+}\right)  \text{ and }u\neq v\text{)}%
\end{array}
\right)  .
\end{align*}
We thus have proven $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$ in the case when $v>u$. It remains to prove
that $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  a,b\right)  _{\lambda
}^{\circ}$ in the case when $v=u$. So let us assume that $v=u$. In this case,%
\begin{align*}
b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=b_{u}b_{u-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[
a_{1},b_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
a_{2},b_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(  \left[
a_{u},b_{\sigma\left(  u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}},
\end{align*}
so that%
\begin{align*}
&  \left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\underbrace{\left(  -1\right)  ^{v}}_{\substack{=\left(  -1\right)
^{u}\\\text{(since }v=u\text{)}}}\left(  \underbrace{b_{v}b_{v-1}...b_{1}%
a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)
^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}},v_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\left(  \left(  -1\right)  ^{u}\sum\limits_{\sigma
\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]
\right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)  ^{u}}%
_{\substack{=\left(  -1\right)  ^{u+u}=\left(  -1\right)  ^{2u}%
=1\\\text{(since }2u\text{ is even)}}}\sum\limits_{\sigma\in S_{u}}%
\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]  \right)
\lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]  \right)
\underbrace{\left(  v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}%
^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}}_{=1}\\
&  =\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  .
\end{align*}
Compared to%
\begin{align*}
\left(  \underbrace{a}_{=a_{1}a_{2}...a_{u}},\underbrace{b}_{\substack{=b_{1}%
b_{2}...b_{v}=b_{1}b_{2}...b_{u}\\\text{(since }v=u\text{)}}}\right)
_{\lambda}^{\circ}  &  =\left(  a_{1}a_{2}...a_{u},b_{1}b_{2}...b_{u}\right)
_{\lambda}^{\circ}=\lambda_{u}\left(  a_{1}a_{2}...a_{u},b_{1}b_{2}%
...b_{u}\right) \\
&  =\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  ,
\end{align*}
this yields $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$. Now that $\left(  av_{\lambda}^{+\mathfrak{g}%
^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  a,b\right)  _{\lambda}^{\circ}$ is proven in each of the cases $v>u$
and $v=u$ (and the case $v<u$ is analogous), we are done with proving
(\ref{prop.det.US.pf.1}).

This proves Proposition \ref{lem.invform.g^0.1}.

\begin{corollary}
\label{cor.invform.g^0.1}Let $n\in\mathbb{N}$. Recall that the family $\left(
e_{\mathbf{i}}^{0}\right)  _{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \deg\mathbf{i}=-n}$ is a basis of the vector space $U\left(
\mathfrak{n}_{-}^{0}\right)  \left[  -n\right]  =S\left(  \mathfrak{n}%
_{-}\right)  \left[  -n\right]  $, and that the family $\left(  e_{\mathbf{j}%
}^{0}\right)  _{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg
\mathbf{j}=n}$ is a basis of the vector space $U\left(  \mathfrak{n}_{+}%
^{0}\right)  \left[  n\right]  =S\left(  \mathfrak{n}_{+}\right)  \left[
n\right]  $. Thus, let us represent the bilinear form $\left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  \times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  $ by its
matrix with respect to the bases $\left(  e_{\mathbf{i}}^{0}\right)
_{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ and
$\left(  e_{\mathbf{j}}^{0}\right)  _{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\ \deg\mathbf{j}=n}$ of $S\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  $ and $S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
$, respectively. This is the matrix%
\[
\left(  \left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda
,n}^{\circ}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}.
\]
This matrix is a square matrix (since the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$), and its determinant is what we are going to
denote by $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ
}\right)  $.

Then,%
\[
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}%
}\right)  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ
}\right)  .
\]

\end{corollary}

\textit{Proof of Corollary \ref{cor.invform.g^0.1}.} For every $\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E$ satisfying $\deg\mathbf{i}=-n$, and
every $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$ satisfying
$\deg\mathbf{j}=n$, we have%
\begin{align*}
\left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}},e_{\mathbf{j}}%
^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda,n}^{\mathfrak{g}^{0}}
&  =\left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}},e_{\mathbf{j}%
}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda}^{\circ}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.invform.g^0.1}, applied
to }a=e_{\mathbf{i}}^{0}\text{ and }b=e_{\mathbf{j}}^{0}\right) \\
&  =\left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda,n}^{\circ
}.
\end{align*}
Thus,%
\[
\det\left(  \left(  \left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}%
},e_{\mathbf{j}}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
,n}^{\mathfrak{g}^{0}}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  =\det\left(  \left(  \left(
e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda,n}^{\circ}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  .
\]


Now,%
\begin{align*}
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)   &
=\det\left(  \left(  \left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)
_{\lambda,n}^{\circ}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  =\det\left(  \left(  \left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}%
^{0}},e_{\mathbf{j}}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
,n}^{\mathfrak{g}^{0}}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  =\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}}\right)  .
\end{align*}
This proves Corollary \ref{cor.invform.g^0.1}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Joining the threads}

\textit{Proof of Proposition \ref{prop.det.US}.} Consider the polynomial
function $Q_{n}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$
introduced in Corollary \ref{cor.invformnondeg.polynomiality}. Due to
Corollary \ref{cor.invformnondeg.polynomiality}, every $\lambda\in V$ and
every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
Q_{n}\left(  \lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}%
n}Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  .
\]
Hence, we can apply Lemma \ref{lem.invformnondeg.elemen} to $V=\mathfrak{h}%
^{\ast}$, $\phi=Q_{n}$ and $k=\operatorname*{LEN}n$. Thus, we obtain the
following three observations:

\textit{Observation 1:} The polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)
\]
is homogeneous of degree $k$. (This follows from Lemma
\ref{lem.invformnondeg.elemen} \textbf{(a)}.)

\textit{Observation 2:} For every integer $N>k$, the $N$-th homogeneous
component of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is zero. (This follows from Lemma \ref{lem.invformnondeg.elemen} \textbf{(b)}.)

\textit{Observation 3:} The $k$-th homogeneous component of the polynomial
function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)  .
\]
(This follows from Lemma \ref{lem.invformnondeg.elemen} \textbf{(c)}.)

Since every $\lambda\in\mathfrak{h}^{\ast}$ satisfies%
\begin{align*}
Q_{n}\left(  \lambda,1\right)   &  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{1}}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{cor.invformnondeg.polynomiality.1}) (applied to }%
\varepsilon=1\text{)}\\
\text{yields }\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}%
^{\mathfrak{g}^{1}}\right)  =Q_{n}\left(  \lambda,1\right)
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}^{1}=\mathfrak{g}\text{
and thus }\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{1}}=\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}}=\left(  \cdot,\cdot\right)
_{\lambda,n}\right)  ,
\end{align*}
the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  .
\]
This yields that%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is a polynomial function.

Since every $\lambda\in\mathfrak{h}^{\ast}$ satisfies%
\begin{align*}
Q_{n}\left(  \lambda,0\right)   &  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{0}}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{cor.invformnondeg.polynomiality.1}) (applied to }%
\varepsilon=0\text{)}\\
\text{yields }\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}%
^{\mathfrak{g}^{0}}\right)  =Q_{n}\left(  \lambda,0\right)
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Corollary \ref{cor.invform.g^0.1}%
}\right)  ,
\end{align*}
the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]
This yields that%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is a polynomial function. This polynomial function is not identically
zero\footnote{\textit{Proof.} Since $\mathfrak{g}$ is nondegenerate, there
exists $\lambda\in\mathfrak{h}^{\ast}$ such that the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $. For such
$\lambda$, the form $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must be
nondegenerate (by Lemma \ref{lem.lambda_k.2}), so that $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. Hence, there exists
$\lambda\in\mathfrak{h}^{\ast}$ such that $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. In other words, the
polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is not identically zero, qed.}.

Since $Q_{n}\left(  \lambda,1\right)  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$, Observation
2 rewrites as follows:

\textit{Observation 2':} For every integer $n>k$, the $n$-th homogeneous
component of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is zero.

Since $Q_{n}\left(  \lambda,1\right)  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ and $Q_{n}\left(  \lambda,0\right)  =\det\left(
\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  $ for every
$\lambda\in\mathfrak{h}^{\ast}$, Observation 3 rewrites as follows:

\textit{Observation 3':} The $k$-th homogeneous component of the polynomial
function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]


Combining Observations 2' and 3' and the fact that the polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is not identically zero, we conclude that the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is the leading term of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  .
\]


This proves Proposition \ref{prop.det.US}.

Now that Proposition \ref{prop.det.US} is proven, the proof of Theorem
\ref{thm.invformnondeg} is also complete (because we have already proven
Theorem \ref{thm.invformnondeg} using Proposition \ref{prop.det.US}).

\subsection{The irreducible quotients of the Verma modules}

We will now use the form $\left(  \cdot,\cdot\right)  _{\lambda}$ to develop
the representation theory of $\mathfrak{g}$. In the following, we assume that
$\mathfrak{g}$ is nondegenerate.

\begin{definition}
Let $\left(  \cdot,\cdot\right)  $ denote the form $\left(  \cdot
,\cdot\right)  _{\lambda}$. Let $J_{\lambda}^{\pm}$ be the kernel of $\left(
\cdot,\cdot\right)  $ on $M_{\lambda}^{\pm}$. This is a graded $\mathfrak{g}%
$-submodule of $M_{\lambda}^{\pm}$ (since the form $\left(  \cdot
,\cdot\right)  $ is $\mathfrak{g}$-invariant). Let $L_{\lambda}^{\pm}$ be the
quotient module $M_{\lambda}^{\pm}\diagup J_{\lambda}^{\pm}$. Then, $\left(
\cdot,\cdot\right)  $ descends to a nondegenerate pairing $L_{\lambda}%
^{+}\times L_{-\lambda}^{-}\rightarrow\mathbb{C}$.
\end{definition}

\begin{remark}
For Weil-generic $\lambda$ (away from a countable union of hypersurfaces), we
have $J_{\lambda}^{\pm}=0$ (by Theorem \ref{thm.invformnondeg}) and thus
$L_{\lambda}^{\pm}=M_{\lambda}^{\pm}$.
\end{remark}

\begin{theorem}
\label{thm.verma}\textbf{(i)} The $\mathfrak{g}$-module $L_{\lambda}^{\pm}$ is irreducible.

\textbf{(ii)} The $\mathfrak{g}$-module $J_{\lambda}^{\pm}$ is the maximal
proper graded submodule of $M_{\lambda}^{\pm}$. (This means that $J_{\lambda
}^{\pm}$ contains all proper graded submodules in $M_{\lambda}^{\pm}$.)

\textbf{(iii)} Assume that there exists some $L\in\mathfrak{g}_{0}$ such that
every $n\in\mathbb{Z}$ satisfies
\[
\left(  \operatorname*{ad}L\right)  \mid_{\mathfrak{g}_{n}}=n\cdot
\operatorname*{id}\mid_{\mathfrak{g}_{n}}.
\]
(In this case it is said that \textit{the grading on }$\mathfrak{g}$
\textit{is internal}, i. e., comes from bracketing with some $L\in
\mathfrak{g}_{0}$.) Then $J_{\lambda}^{\pm}$ is the maximal proper submodule
of $M_{\lambda}^{\pm}$.
\end{theorem}

\begin{remark}
Here are two examples of cases when the grading on $\mathfrak{g}$ is internal:

\textbf{(a)} If $\mathfrak{g}$ is a simple finite-dimensional Lie algebra,
then we know (from Proposition \ref{prop.grad.g}) that choosing a Cartan
subalgebra $\mathfrak{h}$ and corresponding Chevalley generators $e_{1}$,
$e_{2}$, $...$, $e_{m}$, $f_{1}$, $f_{2}$, $...$, $f_{m}$, $h_{1}$, $h_{2}$,
$...$, $h_{m}$ of $\mathfrak{g}$ endows $\mathfrak{g}$ with a grading. This
grading is internal. In fact, in this case, we can take $L=\rho^{\vee}$, where
$\rho^{\vee}$ is defined as the element of $\mathfrak{h}$ satisfying
$\alpha_{i}\left(  \rho^{\vee}\right)  =1$ for all $i$ (where $\alpha_{i}$ are
the simple roots of $\mathfrak{g}$). Since the actions of the $\alpha_{i}$ on
$\mathfrak{h}$ are a basis of $\mathfrak{h}^{\ast}$, this $\rho^{\vee}$ is
well-defined and unique. (But it depends on the choice of $\mathfrak{h}$ and
the Chevalley generators, of course.)

\textbf{(b)} If $\mathfrak{g}=\operatorname*{Vir}$, then the grading on
$\mathfrak{g}$ is internal. In fact, in this case, we can take $L=-L_{0}$.

On the other hand, if $\mathfrak{g}$ is the affine Kac-Moody algebra
$\widehat{\mathfrak{g}}_{\omega}$ of Definition \ref{def.kac}, then the
grading on $\mathfrak{g}$ is not internal.
\end{remark}

\textit{Proof of Theorem \ref{thm.verma}.} \textbf{(i)} Let us show that
$L_{\lambda}^{-}$ is irreducible (the proof for $L_{\lambda}^{+}$ will be similar).

In fact, assume the contrary. Then, there exists a nonzero $w\in L_{\lambda
}^{-}$ such that $U\left(  \mathfrak{g}\right)  \cdot w\neq L_{\lambda}^{-}$.
Since $L_{\lambda}^{-}$ is graded by \textit{nonnegative} integers, we can
choose $w$ to have the smallest possible degree $m$ (without necessarily being
homogeneous). Clearly, $m>0$. Thus we can write $w=w_{0}+w_{1}+...+w_{m}$,
where each $w_{i}$ is homogeneous of degree $\deg w_{i}=i$ and $w_{m}\neq0$.

Let $a\in\mathfrak{g}_{j}$ for some $j<0$. Then $aw=0$ (since $\deg\left(
aw\right)  <\deg w$, but still $U\left(  \mathfrak{g}\right)  \cdot aw\neq
L_{\lambda}^{-}$ (since $U\left(  \mathfrak{g}\right)  \cdot aw\subseteq
U\left(  \mathfrak{g}\right)  \cdot w$ and $U\left(  \mathfrak{g}\right)
\cdot w\neq L_{\lambda}^{-}$), and we have chosen $w$ to have the smallest
possible degree). By homogeneity, this yields $aw_{m}=0$ (since $aw_{m}$ is
the $\left(  m+j\right)  $-th homogeneous component of $aw$).

For every $u\in L_{-\lambda}^{+}\left[  -m-j\right]  $, the term $\left(
au,w_{m}\right)  $ is well-defined (since $au\in L_{-\lambda}^{+}$ and
$w_{m}\in L_{\lambda}^{-}$). Since the form $\left(  \cdot,\cdot\right)  $ is
$\mathfrak{g}$-invariant, it satisfies $\left(  au,w_{m}\right)  =-\left(
u,\underbrace{aw_{m}}_{=0}\right)  =0$. But since $m>0$, we have $L_{-\lambda
}^{+}\left[  -m\right]  =\sum\limits_{j<0}\mathfrak{g}_{j}\cdot L_{-\lambda
}^{+}\left[  -m-j\right]  $ (because Proposition \ref{prop.verma1}
\textbf{(a)} yields $M_{-\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\lambda}^{+}$, so that $L_{-\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
\overline{v_{\lambda}^{+}}$, thus%
\[
L_{-\lambda}^{+}\left[  -m\right]  =\underbrace{U\left(  \mathfrak{n}%
_{-}\right)  \left[  -m\right]  }_{=\sum\limits_{j<0}\left(  \mathfrak{n}%
_{-}\right)  \left[  j\right]  \cdot U\left(  \mathfrak{n}_{-}\right)  \left[
-m-j\right]  }\overline{v_{\lambda}^{+}}=\sum\limits_{j<0}\underbrace{\left(
\mathfrak{n}_{-}\right)  \left[  j\right]  }_{=\mathfrak{g}\left[  j\right]
=\mathfrak{g}_{j}}\cdot\underbrace{U\left(  \mathfrak{n}_{-}\right)  \left[
-m-j\right]  \overline{v_{\lambda}^{+}}}_{\substack{=L_{-\lambda}^{+}\left[
-m-j\right]  \\\text{(since }U\left(  \mathfrak{n}_{-}\right)  \overline
{v_{\lambda}^{+}}=L_{-\lambda}^{+}\text{)}}}=\sum\limits_{j<0}\mathfrak{g}%
_{j}\cdot L_{-\lambda}^{+}\left[  -m-j\right]
\]
). Hence, any element of $L_{-\lambda}^{+}\left[  -m\right]  $ is a linear
combination of elements of the form $au$ with $a\in\mathfrak{g}_{j}$ (for
$j<0$) and $u\in L_{-\lambda}^{+}\left[  -m-j\right]  $. Thus, since we know
that $\left(  au,w_{m}\right)  =0$ for every $a\in\mathfrak{g}_{j}$ and $u\in
L_{-\lambda}^{+}\left[  -m-j\right]  $, we conclude that $\left(  L_{-\lambda
}^{+}\left[  -m\right]  ,w_{m}\right)  =0$. As a consequence, $\left(
L_{-\lambda}^{+},w_{m}\right)  =0$ (because the form $\left(  \cdot
,\cdot\right)  :L_{-\lambda}^{+}\times L_{\lambda}^{-}\rightarrow\mathbb{C}$
is of degree $0$, and thus $\left(  L_{-\lambda}^{+}\left[  j\right]
,w_{m}\right)  =0$ for all $j\neq-m$). Since the form $\left(  \cdot
,\cdot\right)  :L_{-\lambda}^{+}\times L_{\lambda}^{-}\rightarrow\mathbb{C}$
is nondegenerate, this yields $w_{m}=0$. This is a contradiction to $w_{m}%
\neq0$. This contradiction shows that our assumption was wrong. Thus,
$L_{\lambda}^{-}$ is irreducible. Similarly, $L_{\lambda}^{+}$ is irreducible.

\textbf{(ii)} First let us prove that the $\mathfrak{g}$-module $J_{\lambda
}^{+}$ is the maximal proper graded submodule of $M_{\lambda}^{+}$.

Let $K\subseteq M_{\lambda}^{+}$ be a proper graded submodule, and let
$\overline{K}$ be its image in $L_{\lambda}^{+}$. Then, $K$ lives in strictly
negative degrees (because it is graded, so if it would have a component in
degrees $\geq0$, it would contain $v_{\lambda}^{+}$ and thus contain
everything, and thus not be proper). Hence, $\overline{K}$ also lives in
strictly negative degrees, and thus is proper. Hence, by \textbf{(i)}, we have
$\overline{K}=0$, thus $K\subseteq J_{\lambda}^{+}$. This shows that
$J_{\lambda}^{+}$ is the maximal proper graded submodule of $M_{\lambda}^{+}$.
The proof of the corresponding statement for $J_{\lambda}^{-}$ and
$M_{\lambda}^{-}$ is similar.

\textbf{(iii)} Assume that there exists some $L\in\mathfrak{g}_{0}$ such that
every $n\in\mathbb{Z}$ satisfies
\[
\left(  \operatorname*{ad}L\right)  \mid_{\mathfrak{g}_{n}}=n\cdot
\operatorname*{id}\mid_{\mathfrak{g}_{n}}.
\]
Consider this $L$. It is easy to prove (by induction) that $\left[
L,a\right]  =na$ for every $a\in U\left(  \mathfrak{g}\right)  \left[
n\right]  $.

We are now going to show that all $\mathfrak{g}$-submodules of $M_{\lambda
}^{+}$ are automatically graded.

In fact, it is easy to see that $M_{\lambda}^{+}\left[  n\right]
\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}}-\left(
\lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $ for every
$n\in\mathbb{Z}$.\ \ \ \ \footnote{\textit{Proof.} Let $n\in\mathbb{Z}$. Let
$a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $. Then, $a\in
U\left(  \mathfrak{g}\right)  \left[  n\right]  $, so that $\left[
L,a\right]  =na$ and thus $La=aL+\underbrace{\left[  L,a\right]  }%
_{=na}=aL+na$. Thus,%
\begin{align*}
\left(  L\mid_{M_{\lambda}^{+}}\right)  \left(  av_{\lambda}^{+}\right)   &
=\underbrace{La}_{=aL+na}v_{\lambda}^{+}=\left(  aL+na\right)  v_{\lambda}%
^{+}=a\underbrace{Lv_{\lambda}^{+}}_{=\lambda\left(  L\right)  v_{\lambda}%
^{+}}+nav_{\lambda}^{+}=\lambda\left(  L\right)  av_{\lambda}^{+}%
+nav_{\lambda}^{+}\\
&  =\left(  \lambda\left(  L\right)  +n\right)  av_{\lambda}^{+},
\end{align*}
so that $av_{\lambda}^{+}\in\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}%
}-\left(  \lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $.
Forget that we fixed $a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]
$. Thus we have showed that every $a\in U\left(  \mathfrak{n}_{-}\right)
\left[  n\right]  $ satisfies $av_{\lambda}^{+}\in\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right)  $. In other words, $\left\{  av_{\lambda}^{+}%
\ \mid\ a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \right\}
\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}}-\left(
\lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $. Since
$\left\{  av_{\lambda}^{+}\ \mid\ a\in U\left(  \mathfrak{n}_{-}\right)
\left[  n\right]  \right\}  =U\left(  \mathfrak{n}_{-}\right)  \left[
n\right]  \cdot v_{\lambda}^{+}=M_{\lambda}^{+}\left[  n\right]  $, this
becomes $M_{\lambda}^{+}\left[  n\right]  \subseteq\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right)  $, qed.} In other words, for every $n\in\mathbb{Z}%
$, the $n$-th homogeneous component $M_{\lambda}^{+}\left[  n\right]  $ of
$M_{\lambda}^{+}$ is contained in the eigenspace of the operator
$L\mid_{M_{\lambda}^{+}}$ for the eigenvalue $\lambda\left(  L\right)  +n$.
Now,%
\begin{align*}
M_{\lambda}^{+}  &  =\bigoplus\limits_{n\in\mathbb{Z}}M_{\lambda}^{+}\left[
n\right]  =\sum\limits_{n\in\mathbb{Z}}\underbrace{M_{\lambda}^{+}\left[
n\right]  }_{\substack{\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda
}^{+}}-\left(  \lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)
\\=\left(  \text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for
the eigenvalue }\lambda\left(  L\right)  +n\right)  }}\\
&  \subseteq\sum\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  .
\end{align*}
Since all eigenspaces of $L\mid_{M_{\lambda}^{+}}$ are clearly contained in
$M_{\lambda}^{+}$, this rewrites as%
\[
M_{\lambda}^{+}=\sum\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  .
\]
Since eigenspaces of an operator corresponding to distinct eigenvalues are
linearly disjoint, the sum $\sum\limits_{n\in\mathbb{Z}}\left(
\text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for the
eigenvalue }\lambda\left(  L\right)  +n\right)  $ must be a direct sum, so
this becomes%
\begin{equation}
M_{\lambda}^{+}=\bigoplus\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of
the operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  . \label{thm.verma.pf.5}%
\end{equation}
As a consequence of this, the map $L\mid_{M_{\lambda}^{+}}$ is diagonalizable,
and all of its eigenvalues belong to the set $\left\{  \lambda\left(
L\right)  +n\ \mid\ n\in\mathbb{Z}\right\}  $.

So for every $n\in\mathbb{Z}$, we have the inclusion%
\begin{align*}
M_{\lambda}^{+}\left[  n\right]   &  \subseteq\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right) \\
&  =\left(  \text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{
for the eigenvalue }\lambda\left(  L\right)  +n\right)  ,
\end{align*}
but the direct sum of these inclusions over all $n\in\mathbb{Z}$ is an
equality (since%
\[
\bigoplus\limits_{n\in\mathbb{Z}}M_{\lambda}^{+}\left[  n\right]  =M_{\lambda
}^{+}=\bigoplus\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)
\]
by (\ref{thm.verma.pf.5})). Hence, each of these inclusions must be an
equality. In other words,
\begin{equation}
M_{\lambda}^{+}\left[  n\right]  =\left(  \text{eigenspace of the operator
}L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}.
\label{thm.verma.pf.6}%
\end{equation}


Now, let $K$ be a $\mathfrak{g}$-submodule of $M_{\lambda}^{+}$. Then,
$L\mid_{K}$ is a restriction of $L\mid_{M_{\lambda}^{+}}$ to $K$. Hence, map
$L\mid_{K}$ is diagonalizable, and all of its eigenvalues belong to the set
$\left\{  \lambda\left(  L\right)  +n\ \mid\ n\in\mathbb{Z}\right\}  $
(because we know that the map $L\mid_{M_{\lambda}^{+}}$ is diagonalizable, and
all of its eigenvalues belong to the set $\left\{  \lambda\left(  L\right)
+n\ \mid\ n\in\mathbb{Z}\right\}  $). In other words,%
\begin{align*}
K  &  =\bigoplus\limits_{n\in\mathbb{Z}}\underbrace{\left(  \text{eigenspace
of the operator }L\mid_{K}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  }_{=K\cap\left(  \text{eigenspace of the operator }L\mid
_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  }\\
&  =\bigoplus\limits_{n\in\mathbb{Z}}\left(  K\cap\underbrace{\left(
\text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for the
eigenvalue }\lambda\left(  L\right)  +n\right)  }_{=M_{\lambda}^{+}\left[
n\right]  }\right) \\
&  =\bigoplus\limits_{n\in\mathbb{Z}}\left(  K\cap M_{\lambda}^{+}\left[
n\right]  \right)  .
\end{align*}
Hence, $K$ is graded. We thus have shown that every $\mathfrak{g}$-submodule
of $M_{\lambda}^{+}$ is graded. Similarly, every $\mathfrak{g}$-submodule of
$M_{\lambda}^{-}$ is graded. Thus, Theorem \ref{thm.verma} \textbf{(iii)}
follows from Theorem \ref{thm.verma} \textbf{(ii)}.

\begin{remark}
Theorem \ref{thm.verma} \textbf{(ii)} does not hold if the word ``graded'' is
removed. In fact, here is a counterexample: Let $\mathfrak{g}$ be the
3-dimensional Heisenberg algebra. (This is the Lie algebra with vector-space
basis $\left(  x,K,y\right)  $ and with Lie bracket given by $\left[
y,x\right]  =K$, $\left[  x,K\right]  =0$ and $\left[  y,K\right]  =0$. It can
be considered as a Lie subalgebra of the oscillator algebra $\mathcal{A}$
defined in Definition \ref{def.osc}.) It is easy to see that $\mathfrak{g}$
becomes a nondegenerate $\mathbb{Z}$-graded Lie algebra by setting
$\mathfrak{g}_{-1}=\left\langle x\right\rangle $, $\mathfrak{g}_{0}%
=\left\langle K\right\rangle $, $\mathfrak{g}_{1}=\left\langle y\right\rangle
$ and $\mathfrak{g}_{i}=0$ for every $i\in\mathbb{Z}\diagdown\left\{
-1,0,1\right\}  $. Then, on the Verma highest-weight module $M_{0}%
^{+}=\mathbb{C}\left[  x\right]  v_{0}^{+}$, both $K$ and $y$ act as $0$ (and
$x$ acts as multiplication with $x$), so that $Iv_{0}^{+}$ is a $\mathfrak{g}%
$-submodule of $M_{0}^{+}$ for every ideal $I\subseteq\mathbb{C}\left[
x\right]  $, but not all of these ideals are graded, and not all of them are
contained in $J_{0}^{+}$ (as can be easily checked).
\end{remark}

\begin{corollary}
\label{cor.verma.irred}For Weil-generic $\lambda$ (this means a $\lambda$
outside of countably many hypersurfaces in $\mathfrak{h}^{\ast}$), the
$\mathfrak{g}$-modules $M_{\lambda}^{+}$ and $M_{\lambda}^{-}$ are irreducible.
\end{corollary}

\begin{definition}
Let $Y$ be a $\mathfrak{g}$-module. A vector $w\in Y$ is called a
\textit{singular vector of weight }$\mu\in\mathfrak{h}^{\ast}$ (here, recall
that $\mathfrak{h}=\mathfrak{g}_{0}$) if it satisfies%
\[
hw=\mu\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h}%
\]
and%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0\text{.}%
\]
We denote by $\operatorname*{Sing}\nolimits_{\mu}\left(  Y\right)  $ the space
of singular vectors of $Y$ of weight $\mu$.
\end{definition}

When people talk about ``singular vectors'', they usually mean nonzero
singular vectors in negative degrees. We are not going to adhere to this
convention, though.

\begin{lemma}
\label{lem.singvec}Let $Y$ be a $\mathfrak{g}$-module. Then there is a
canonical isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
&  \rightarrow\operatorname*{Sing}\nolimits_{\lambda}Y,\\
\phi &  \mapsto\phi\left(  v_{\lambda}^{+}\right)  .
\end{align*}

\end{lemma}

\textit{Proof of Lemma \ref{lem.singvec}.} We have $M_{\lambda}^{+}=U\left(
\mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}=\operatorname*{Ind}\nolimits_{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}_{\lambda}$, so that
\[
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
=\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}%
_{\lambda},Y\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda},Y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right)  .
\]
But $\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\mathbb{C}_{\lambda},Y\right)  \cong\operatorname*{Sing}\nolimits_{\lambda}Y$
(because every $\mathbb{C}$-linear map $\mathbb{C}_{\lambda}\rightarrow Y$ is
uniquely determined by the image of $v_{\lambda}^{+}$, and this map is a
$\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-module map if and only
if this image is a singular vector of $Y$ of weight $\lambda$). Thus,
$\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\mathbb{C}_{\lambda},Y\right)  \cong\operatorname*{Sing}\nolimits_{\lambda}Y$.
If we make this isomorphism explicit, we notice that it sends every $\phi$ to
$\phi\left(  v_{\lambda}^{+}\right)  $, so that Lemma \ref{lem.singvec} is proven.

\begin{corollary}
\label{cor.singvec}The representation $M_{\lambda}^{+}$ is irreducible if and
only if it does not have nonzero singular vectors in negative degrees. Here, a
vector in $M_{\lambda}^{+}$ is said to be ``in negative degrees'' if its
projection on the $0$-th homogeneous component $M_{\lambda}^{+}\left[
0\right]  $ is zero.
\end{corollary}

\textit{Proof of Corollary \ref{cor.singvec}.} $\Longleftarrow:$ Assume that
$M_{\lambda}^{+}$ does not have nonzero singular vectors in negative degrees.

We must then show that $M_{\lambda}^{+}$ is irreducible.

In fact, assume the contrary. Then, $M_{\lambda}^{+}$ is not irreducible.
Hence, there exists a nonzero \textit{homogeneous} $v\in M_{\lambda}^{+}$ such
that $U\left(  \mathfrak{g}\right)  \cdot v\neq M_{\lambda}^{+}$%
.\ \ \ \ \footnote{\textit{Proof.} Notice that $M_{\lambda}^{+}$ is a graded
$U\left(  \mathfrak{g}\right)  $-module (since $M_{\lambda}^{+}$ is a graded
$\mathfrak{g}$-module).
\par
Since $M_{\lambda}^{+}$ is not irreducible, there exists a nonzero $w\in
M_{\lambda}^{+}$ such that $U\left(  \mathfrak{g}\right)  \cdot w\neq
M_{\lambda}^{+}$. Since $M_{\lambda}^{+}$ is graded by \textit{nonpositive}
integers, we can write $w$ in the form $w=\sum\limits_{j=0}^{m}w_{j}$, where
each $w_{i}$ is homogeneous of degree $\deg w_{i}=-i$ and $m\in\mathbb{Z}$.
Now,
\begin{align*}
\underbrace{U\left(  \mathfrak{g}\right)  }_{=\sum\limits_{i\in\mathbb{Z}%
}U\left(  \mathfrak{g}\right)  \left[  i\right]  }\cdot\underbrace{w}%
_{=\sum\limits_{j=0}^{m}w_{j}}  &  =\left(  \sum\limits_{i\in\mathbb{Z}%
}U\left(  \mathfrak{g}\right)  \left[  i\right]  \right)  \cdot\left(
\sum\limits_{j=0}^{m}w_{j}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}%
\right)  \left[  i\right]  \cdot w_{j}.
\end{align*}
Hence, for every $n\in\mathbb{Z}$, we have
\begin{align*}
\left(  U\left(  \mathfrak{g}\right)  \cdot w\right)  \left[  n\right]   &
=\left(  \sum\limits_{i\in\mathbb{Z}}\sum\limits_{j=0}^{m}U\left(
\mathfrak{g}\right)  \left[  i\right]  \cdot w_{j}\right)  \left[  n\right]
=\sum\limits_{j=0}^{m}\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}U\left(
\mathfrak{g}\right)  \left[  i\right]  \cdot w_{j}\right)  }%
_{\substack{\subseteq U\left(  \mathfrak{g}\right)  \left[  i-j\right]
\\\text{(since }\deg w_{j}=-j\text{ and since}\\M_{\lambda}^{+}\text{ is a
graded }U\left(  \mathfrak{g}\right)  \text{-module)}}}\left[  n\right] \\
&  =\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}\right)  \left[  n+j\right]
\cdot w_{j}.
\end{align*}
Now, since $U\left(  \mathfrak{g}\right)  \cdot w\neq M_{\lambda}^{+}$, there
exists at least one $n\in\mathbb{Z}$ such that $\left(  U\left(
\mathfrak{g}\right)  \cdot w\right)  \left[  n\right]  \neq M_{\lambda}%
^{+}\left[  n\right]  $. Consider such an $n$. Then, $M_{\lambda}^{+}\left[
n\right]  \neq\left(  U\left(  \mathfrak{g}\right)  \cdot w\right)  \left[
n\right]  =\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}\right)  \left[
n+j\right]  \cdot w_{j}$. Thus, $U\left(  \mathfrak{g}\right)  \left[
n+j\right]  \cdot w_{j}\neq M_{\lambda}^{+}\left[  n\right]  $ for all
$j\in\left\{  0,1,...,m\right\}  $. But some $j\in\left\{  0,1,...,m\right\}
$ satisfies $w_{j}\neq0$ (since $\sum\limits_{j=0}^{m}w_{j}=w\neq0$). Consider
this $j$. Then, $w_{j}$ is a nonzero homogeneous element of $M_{\lambda}^{+}$
satisfying $U\left(  \mathfrak{g}\right)  \cdot w_{j}\neq M_{\lambda}^{+}$
(because $\left(  U\left(  \mathfrak{g}\right)  \cdot w_{j}\right)  \left[
n\right]  =U\left(  \mathfrak{g}\right)  \left[  n+j\right]  \cdot w_{j}\neq
M_{\lambda}^{+}\left[  n\right]  $). This proves that there exists a nonzero
\textit{homogeneous} $v\in M_{\lambda}^{+}$ such that $U\left(  \mathfrak{g}%
\right)  \cdot v\neq M_{\lambda}^{+}$. Qed.} Consider this $v$. Then,
$U\left(  \mathfrak{g}\right)  \cdot v$ is a proper graded submodule of
$M_{\lambda}^{+}$, and thus is contained in $J_{\lambda}^{+}$. Hence,
$J_{\lambda}^{+}\neq0$.

There exist some $d\in\mathbb{Z}$ such that $J_{\lambda}^{+}\left[  d\right]
\neq0$ (since $J_{\lambda}^{+}\neq0$ and since $J_{\lambda}^{+}$ is graded).
All such $d$ are nonpositive (since $J_{\lambda}^{+}$ is nonpositively
graded). Thus, there exists a highest integer $d$ such that $J_{\lambda}%
^{+}\left[  d\right]  \neq0$. Consider this $d$. Clearly, $d<0$ (since the
bilinear form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda
}^{-}$ is obviously nondegenerate on $M_{\lambda}^{+}\left[  0\right]  \times
M_{-\lambda}^{-}\left[  0\right]  $, so that $J_{\lambda}^{+}\left[  0\right]
=0$).

Every $i>0$ satisfies
\begin{align*}
\mathfrak{g}_{i}\cdot\left(  J_{\lambda}^{+}\left[  d\right]  \right)   &
\subseteq J_{\lambda}^{+}\left[  i+d\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }J_{\lambda}^{+}\text{ is a graded }\mathfrak{g}\text{-module}%
\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i+d>d\text{, but }d\text{ was
the highest integer such that }J_{\lambda}^{+}\left[  d\right]  \neq0\right)
.
\end{align*}


By Conditions \textbf{(1)} and \textbf{(2)} of Definition
\ref{def.gradLienondeg}, the Lie algebra $\mathfrak{g}_{0}$ is abelian and
finite-dimensional. Hence, every nonzero $\mathfrak{g}_{0}$-module has a
one-dimensional submodule\footnote{\textit{Proof.} This is because of the
following fact:
\par
Every nonzero finite-dimensional module over an abelian finite-dimensional Lie
algebra has a one-dimensional submodule. (This is just a restatement of the
fact that a finite set of pairwise commuting matrices on a finite-dimensional
nonzero $\mathbb{C}$-vector space has a common nonzero eigenvector.)}. Thus,
the nonzero $\mathfrak{g}_{0}$-module $J_{\lambda}^{+}\left[  d\right]  $ has
a one-dimensional submodule. Let $w$ be the generator of this submodule. Then,
this submodule is $\left\langle w\right\rangle $.

For every $h\in\mathfrak{h}$, the vector $hw$ is a scalar multiple of $w$
(since $h\in\mathfrak{h}=\mathfrak{g}_{0}$, so that $hw$ lies in the
$\mathfrak{g}_{0}$-submodule of $J_{\lambda}^{+}\left[  d\right]  $ generated
by $w$, but this submodule is $\left\langle w\right\rangle $). Thus, we can
write $hw=\lambda_{h}w$ for some $\lambda_{h}\in\mathbb{C}$. This $\lambda
_{h}$ is uniquely determined (since $w\neq0$), so we can define a map
$\mu:\mathfrak{h}\rightarrow\mathbb{C}$ such that $\mu\left(  h\right)
=\lambda_{h}$ for every $h\in\mathfrak{h}$. This map $\mu$ is easily seen to
be $\mathbb{C}$-linear, so that we have found a $\mu\in\mathfrak{h}^{\ast}$
such that%
\[
hw=\mu\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }h\in
\mathfrak{h}.
\]
Also,%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0
\]
(since $\underbrace{a}_{\in\mathfrak{g}_{i}}\underbrace{w}_{\in J_{\lambda
}^{+}\left[  d\right]  }\in\mathfrak{g}_{i}\cdot\left(  J_{\lambda}^{+}\left[
d\right]  \right)  \subseteq0$). Thus, $w$ is a nonzero singular vector. Since
$w\in J_{\lambda}^{+}\left[  d\right]  $ and $d<0$, this vector $w$ is in
negative degrees. This contradicts to the assumption that $M_{\lambda}^{+}$
does not have nonzero singular vectors in negative degrees. This contradiction
shows that our assumption was wrong, so that $M_{\lambda}^{+}$ is irreducible.
This proves the $\Longleftarrow$ direction of Corollary \ref{cor.singvec}.

$\Longrightarrow:$ Assume that $M_{\lambda}^{+}$ is irreducible.

We must then show that $M_{\lambda}^{+}$ does not have nonzero singular
vectors in negative degrees.

Let $v$ be a singular vector of $M_{\lambda}^{+}$ in negative degrees. Let it
be a singular vector of weight $\mu$ for some $\mu\in\mathfrak{h}^{\ast}$.

By Lemma \ref{lem.singvec} (applied to $\mu$ and $M_{\lambda}^{+}$ instead of
$\lambda$ and $Y$), we have an isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\mu}^{+},M_{\lambda}%
^{+}\right)   &  \rightarrow\operatorname*{Sing}\nolimits_{\mu}\left(
M_{\lambda}^{+}\right)  ,\\
\phi &  \mapsto\phi\left(  v_{\mu}^{+}\right)  .
\end{align*}
Let $\phi$ be the preimage of $v$ under this isomorphism. Then, $v=\phi\left(
v_{\mu}^{+}\right)  $.

Since $v$ is in negative degrees, we have $v\in\sum\limits_{n<0}M_{\lambda
}^{+}\left[  n\right]  $. Now, $M_{\mu}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\mu}^{+}=\sum\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[
m\right]  v_{\mu}^{+}$ (since $M_{\mu}^{+}$ is nonpositively graded), so that%
\begin{align*}
\phi\left(  M_{\mu}^{+}\right)   &  =\phi\left(  \sum\limits_{m\leq0}U\left(
\mathfrak{n}_{-}\right)  \left[  m\right]  v_{\mu}^{+}\right)  =\sum
\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[  m\right]
\underbrace{\phi\left(  v_{\mu}^{+}\right)  }_{=v\in\sum\limits_{n<0}%
M_{\lambda}^{+}\left[  n\right]  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\phi\in\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\mu}%
^{+},M_{\lambda}^{+}\right)  \right) \\
&  \in\sum\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[  m\right]
\sum\limits_{n<0}M_{\lambda}^{+}\left[  n\right]  =\sum\limits_{m\leq0}%
\sum\limits_{n<0}\underbrace{U\left(  \mathfrak{n}_{-}\right)  \left[
m\right]  \cdot M_{\lambda}^{+}\left[  n\right]  }_{\substack{\subseteq
M_{\lambda}^{+}\left[  m+n\right]  \\\text{(since }M_{\lambda}^{+}\text{ is a
graded }\mathfrak{g}\text{-module)}}}\\
&  \subseteq\sum\limits_{m\leq0}\sum\limits_{n<0}M_{\lambda}^{+}\left[
m+n\right]  \subseteq\sum\limits_{r<0}M_{\lambda}^{+}\left[  r\right]  .
\end{align*}
Thus, the projection of $\phi\left(  M_{\mu}^{+}\right)  $ onto the $0$-th
degree of $M_{\lambda}^{+}$ is $0$. Hence, $\phi\left(  M_{\mu}^{+}\right)  $
is a proper $\mathfrak{g}$-submodule of $M_{\lambda}^{+}$. Therefore,
$\phi\left(  M_{\mu}^{+}\right)  =0$ (since $M_{\lambda}^{+}$ is irreducible).
Thus, $v=\phi\left(  v_{\mu}^{+}\right)  \in\phi\left(  M_{\mu}^{+}\right)
=0$, so that $v=0$.

We have thus proven: Whenever $v$ is a singular vector of $M_{\lambda}^{+}$ in
negative degrees, we have $v=0$. In other words, $M_{\lambda}^{+}$ does not
have nonzero singular vectors in negative degrees. This proves the
$\Longrightarrow$ direction of Corollary \ref{cor.singvec}.

Here is a variation on Corollary \ref{cor.singvec}:

\begin{corollary}
\label{cor.singvec.2}The representation $M_{\lambda}^{+}$ is irreducible if
and only if it does not have nonzero homogeneous singular vectors in negative degrees.
\end{corollary}

\textit{Proof of Corollary \ref{cor.singvec.2}.} $\Longrightarrow:$ This
follows from the $\Longrightarrow$ direction of Corollary \ref{cor.singvec}.

$\Longleftarrow:$ Repeat the proof of the $\Longleftarrow$ direction of
Corollary \ref{cor.singvec}, noticing that $w$ is homogeneous (since $w\in
J_{\lambda}^{+}\left[  d\right]  $).

Corollary \ref{cor.singvec.2} is thus proven.

\subsection{Highest/lowest-weight modules}

\begin{definition}
A \textit{highest-weight module} with highest weight $\lambda\in
\mathfrak{h}^{\ast}$ means a quotient $V$ of the graded $\mathfrak{g}$-module
$M_{\lambda}^{+}$ by a proper graded submodule. The projection of $v_{\lambda
}^{+}\in M_{\lambda}^{+}$ onto this quotient will be called a
\textit{highest-weight vector} of $V$. (Note that a highest-weight module may
have several highest-weight vectors: in fact, every nonzero vector in its
$0$-th homogeneous component is a highest-weight vector.) The notion
``highest-weight representation'' is also used as a synonym for
``highest-weight module''.

A \textit{lowest-weight module} with lowest weight $\lambda\in\mathfrak{h}%
^{\ast}$ means a quotient $V$ of the graded $\mathfrak{g}$-module $M_{\lambda
}^{-}$ by a proper graded submodule. The projection of $v_{\lambda}^{-}\in
M_{\lambda}^{-}$ onto this quotient will be called a \textit{lowest-weight
vector} of $V$. (Note that a lowest-weight module may have several
lowest-weight vectors: in fact, every nonzero vector in its $0$-th homogeneous
component is a lowest-weight vector.) The notion ``lowest-weight
representation'' is also used as a synonym for ``lowest-weight module''.

If $Y$ is a highest-weight module with highest weight $\lambda$, then we have
an exact sequence $%
%TCIMACRO{\TeXButton{M surj Y surj L}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{+}_{\lambda}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{+}_{\lambda}
}%
%EndExpansion
$ (by Theorem \ref{thm.verma} \textbf{(ii)}).

If $Y$ is a lowest-weight module with lowest weight $\lambda$, then we have an
exact sequence $%
%TCIMACRO{\TeXButton{M surj Y surj L}{\xymatrix{
%M^{-}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{-}_{\lambda}
%}}}%
%BeginExpansion
\xymatrix{
M^{-}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{-}_{\lambda}
}%
%EndExpansion
$ (by Theorem \ref{thm.verma} \textbf{(ii)}).
\end{definition}

\subsection{Categories \texorpdfstring{$\mathcal{O}^{+}$}{O-plus} and
\texorpdfstring{$\mathcal{O}^{-}$}{O-minus}}

The category of all $\mathfrak{g}$-modules for a graded Lie algebra is
normally not particularly well-behaved: modules can be too big. One could
restrict one's attention to finite-dimensional modules, but this is often too
much of a sacrifice (e. g., the Heisenberg algebra $\mathcal{A}$ has no
finite-dimensional modules which are not direct sums of $1$-dimensional ones).
A balance between nontriviality and tamability is achieved by considering the
so-called \textit{Category }$\mathcal{O}$. Actually, there are two of these
categories, $\mathcal{O}^{+}$ and $\mathcal{O}^{-}$, which are antiequivalent
to each other (in general) and equivalent to each other (in some more
restrictive cases). There are several definitions for each of these
categories, and some of them are not even equivalent to each other, although
they mostly differ in minor technicalities. Here are the definitions that we
are going to use:

\begin{definition}
\label{def.O+}The objects of \textit{category }$\mathcal{O}^{+}$ will be
$\mathbb{C}$-graded $\mathfrak{g}$-modules $M$ such that:

\textbf{(1)} all degrees lie in a halfplane $\operatorname{Re}z<a$ and fall
into finitely many arithmetic progressions with step $1$;

\textbf{(2)} for every $d\in\mathbb{C}$, the space $M\left[  d\right]  $ is finite-dimensional.

The \textit{morphisms of category }$\mathcal{O}^{+}$ will be graded
$\mathfrak{g}$-module homomorphisms.
\end{definition}

\begin{definition}
\label{def.O-}The objects of \textit{category }$\mathcal{O}^{-}$ will be
$\mathbb{C}$-graded $\mathfrak{g}$-modules $M$ such that:

\textbf{(1)} all degrees lie in a halfplane $\operatorname{Re}z>a$ and fall
into finitely many arithmetic progressions with step $1$;

\textbf{(2)} for every $d\in\mathbb{C}$, the space $M\left[  d\right]  $ is finite-dimensional.

The \textit{morphisms of category }$\mathcal{O}^{-}$ will be graded
$\mathfrak{g}$-module homomorphisms.
\end{definition}

It is rather clear that for a nondegenerate $\mathbb{Z}$-graded Lie algebra
(or, more generally, for a $\mathbb{Z}$-graded Lie algebra satisfying
conditions \textbf{(1)} and \textbf{(2)} of Definition \ref{def.gradLienondeg}%
), the Verma highest-weight module $M_{\lambda}^{+}$ lies in category
$\mathcal{O}^{+}$ for every $\lambda\in\mathfrak{h}^{\ast}$, and the Verma
lowest-weight module $M_{\lambda}^{-}$ lies in category $\mathcal{O}^{-}$ for
every $\lambda\in\mathfrak{h}^{\ast}$.

\begin{definition}
Let $V$ and $W$ be two $\mathbb{C}$-graded vector spaces, and $x\in\mathbb{C}%
$. A map $f:V\rightarrow W$ is said to be \textit{homogeneous of degree }$x$
if and only if every $z\in\mathbb{C}$ satisfies $f\left(  V\left[  z\right]
\right)  \subseteq W\left[  z+x\right]  $. (For example, this yields that a
map is homogeneous of degree $0$ if and only if it is graded.)
\end{definition}

\begin{proposition}
\label{prop.O.irred}The irreducible modules in category $\mathcal{O}^{\pm}$
(up to homogeneous isomorphism) are $L_{\lambda}^{\pm}$ for varying
$\lambda\in\mathbb{C}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.O.irred}.} First of all, for every
$\lambda\in\mathfrak{h}^{\ast}$, the $\mathfrak{g}$-module $L_{\lambda}^{+}$
has a unique singular vector (up to scaling), and this vector is a singular
vector of weight $\lambda$.\ \ \ \ \footnote{\textit{Proof.} It is clear that
$\overline{v_{\lambda}^{+}}\in L_{\lambda}^{+}$ is a singular vector of weight
$\lambda$. Now we must prove that it is the only singular vector (up to
scaling).
\par
In fact, assume the opposite. Then, there exists a singular vector in
$L_{\lambda}^{+}$ which is not a scalar multiple of $\overline{v_{\lambda}%
^{+}}$. This singular vector must have a nonzero $d$-th homogeneous component
for some $d<0$ (because it is not a scalar multiple of $\overline{v_{\lambda
}^{+}}$), and this component itself must be a singular vector (since any
homogeneous component of a singular vector must itself be a singular vector).
So the module $L_{\lambda}^{+}$ has a nonzero homogeneous singular vector $w$
of degree $d$.
\par
Now, repeat the proof of the $\Longrightarrow$ part of Corollary
\ref{cor.singvec}, with $M_{\lambda}^{+}$ replaced by $L_{\lambda}^{+}$ (using
the fact that $L_{\lambda}^{+}$ is irreducible). As a consequence, it follows
that $L_{\lambda}^{+}$ does not have nonzero singular vectors in negative
degrees. This contradicts the fact that the module $L_{\lambda}^{+}$ has a
nonzero homogeneous singular vector $w$ of degree $d<0$. This contradiction
shows that our assumption was wrong, so that indeed, $\overline{v_{\lambda
}^{+}}$ is the only singular vector of $L_{\lambda}^{+}$ (up to scaling),
qed.} Thus, the $\mathfrak{g}$-modules $L_{\lambda}^{+}$ are pairwise
nonisomorphic for varying $\lambda$. Similarly, the $\mathfrak{g}$-modules
$L_{\lambda}^{-}$ are pairwise nonisomorphic for varying $\lambda$.

Let $Y$ be any irreducible module in category $\mathcal{O}^{+}$. We are now
going to prove that $Y\cong L_{\lambda}^{+}$ for some $\lambda\in
\mathfrak{h}^{\ast}$.

Let $d$ be a complex number such that $Y\left[  d\right]  \neq0$ and $Y\left[
d+j\right]  =0$ for all $j\geq1$. (Such a complex number exists due to
condition \textbf{(1)} in Definition \ref{def.O+}.) For every $v\in Y\left[
d\right]  $, we have $av=0$ for every $a\in\mathfrak{g}_{i}$ for every
$i>0$\ \ \ \ \footnote{\textit{Proof.} Let $i>0$ and $a\in\mathfrak{g}_{i}$.
Then, $i\geq1$. Now, $a\in\mathfrak{g}_{i}$ and $v\in Y\left[  d\right]  $
yield $av\in\mathfrak{g}_{i}\cdot Y\left[  d\right]  \subseteq Y\left[
d+i\right]  =0$ (since $Y\left[  d+j\right]  =0$ for all $j\geq1$), so that
$av=0$, qed.}.

By Conditions \textbf{(1)} and \textbf{(2)} of Definition
\ref{def.gradLienondeg}, the Lie algebra $\mathfrak{g}_{0}$ is abelian and
finite-dimensional. Hence, every nonzero $\mathfrak{g}_{0}$-module has a
one-dimensional submodule\footnote{\textit{Proof.} This is because of the
following fact:
\par
Every nonzero finite-dimensional module over an abelian finite-dimensional Lie
algebra has a one-dimensional submodule. (This is just a restatement of the
fact that a finite set of pairwise commuting matrices on a finite-dimensional
nonzero $\mathbb{C}$-vector space has a common nonzero eigenvector.)}. Thus,
the nonzero $\mathfrak{g}_{0}$-module $Y\left[  d\right]  $ has a
one-dimensional submodule. Let $w$ be the generator of this submodule. Then,
this submodule is $\left\langle w\right\rangle $.

For every $h\in\mathfrak{h}$, the vector $hw$ is a scalar multiple of $w$
(since $h\in\mathfrak{h}=\mathfrak{g}_{0}$, so that $hw$ lies in the
$\mathfrak{g}_{0}$-submodule of $Y\left[  d\right]  $ generated by $w$, but
this submodule is $\left\langle w\right\rangle $). Thus, we can write
$hw=\lambda_{h}w$ for some $\lambda_{h}\in\mathbb{C}$. This $\lambda_{h}$ is
uniquely determined by $h$ (since $w\neq0$), so we can define a map
$\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ such that $\lambda\left(
h\right)  =\lambda_{h}$ for every $h\in\mathfrak{h}$. This map $\lambda$ is
easily seen to be $\mathbb{C}$-linear, so that we have found a $\lambda
\in\mathfrak{h}^{\ast}$ such that%
\[
hw=\lambda\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }%
h\in\mathfrak{h}.
\]
Also,%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0
\]
(since $av=0$ for every $v\in Y\left[  d\right]  $ and every $a\in
\mathfrak{g}_{i}$ for every $i>0$). Thus, $w$ is a nonzero singular vector of
weight $\lambda$.

By Lemma \ref{lem.singvec}, we have an isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
&  \rightarrow\operatorname*{Sing}\nolimits_{\lambda}Y,\\
\phi &  \mapsto\phi\left(  v_{\lambda}^{+}\right)  .
\end{align*}
Let $\phi$ be the preimage of $w$ under this isomorphism. Then, $w=\phi\left(
v_{\lambda}^{+}\right)  $. Since $w\in Y\left[  d\right]  $, it is easy to see
that $\phi$ is a homogeneous homomorphism of degree $d$ (in fact, every
$n\in\mathbb{Z}$ satisfies $M_{\lambda}^{+}\left[  n\right]  =U\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  \cdot v_{\lambda}^{+}$, so that%
\begin{align*}
\phi\left(  M_{\lambda}^{+}\left[  n\right]  \right)   &  =\phi\left(
U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \cdot v_{\lambda}%
^{+}\right)  =U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]
\cdot\underbrace{\phi\left(  v_{\lambda}^{+}\right)  }_{=w\in Y\left[
d\right]  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\phi\text{ is
}\mathfrak{g}\text{-linear}\right) \\
&  \subseteq U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \cdot
Y\left[  d\right]  \subseteq Y\left[  n+d\right]
\end{align*}
). This homomorphism $\phi$ must be surjective, since $Y$ is irreducible.
Thus, we have a homogeneous isomorphism $M_{\lambda}^{+}\diagup\left(
\operatorname*{Ker}\phi\right)  \cong Y$. Also, $\operatorname*{Ker}\phi$ is a
proper graded submodule of $M_{\lambda}^{+}$, thus a submodule of $J_{\lambda
}^{+}$ (by Theorem \ref{thm.verma} \textbf{(ii)}). Hence, we have a projection
$M_{\lambda}^{+}\diagup\left(  \operatorname*{Ker}\phi\right)  \rightarrow
M_{\lambda}^{+}\diagup J_{\lambda}^{+}$. Since $M_{\lambda}^{+}\diagup\left(
\operatorname*{Ker}\phi\right)  \cong Y$ is irreducible, this projection must
either be an isomorphism or the zero map. It cannot be the zero map (since it
is a projection onto the nonzero module $M_{\lambda}^{+}\diagup J_{\lambda
}^{+}$), so it therefore is an isomorphism. Thus, $M_{\lambda}^{+}\diagup
J_{\lambda}^{+}\cong M_{\lambda}^{+}\diagup\left(  \operatorname*{Ker}%
\phi\right)  \cong Y$, so we have a homogeneous isomorphism $Y\cong
M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$.

We thus have showed that any irreducible module in category $\mathcal{O}^{+}$
is isomorphic to $L_{\lambda}^{+}$ for some $\lambda\in\mathfrak{h}^{\ast}$.
Similarly, the analogous assertion holds for $\mathcal{O}^{-}$. Proposition
\ref{prop.O.irred} is thus proven.

\begin{definition}
Let $M$ be a module in category $\mathcal{O}^{+}$. We define the
\textit{character} $\operatorname*{ch}M$ of $M$ as follows:

Write $M=\bigoplus\limits_{d}M\left[  d\right]  $. Then, define
$\operatorname*{ch}M$ by%
\[
\operatorname*{ch}M=\sum\limits_{d}q^{-d}\operatorname*{tr}\nolimits_{M\left[
d\right]  }\left(  e^{x}\right)  \ \ \ \ \ \ \ \ \ \ \text{as a power series
in }q
\]
for every $x\in\mathfrak{h}$. We also write $\left(  \operatorname*{ch}%
M\right)  \left(  q,x\right)  $ for this, so it becomes a formal power series
in both $q$ and $x$. (Note that this power series can contain noninteger
powers of $q$, but due to $M\in\mathcal{O}^{+}$, the exponents in these powers
are bounded from above in their real part, and fall into infinitely many
arithmetic progressions with step $1$.)
\end{definition}

\begin{proposition}
\label{prop.chVerma}Here is an example:%
\[
\left(  \operatorname*{ch}M_{\lambda}^{+}\right)  \left(  x\right)  =\dfrac
{1}{\prod\limits_{j>0}\det\nolimits_{\mathfrak{g}\left[  -j\right]  }\left(
1-q^{j}e^{\operatorname*{ad}\left(  x\right)  }\right)  }.
\]
(To prove this, use Molien's identity which states that, for every linear map
$A:V\rightarrow V$, we have%
\[
\sum\limits_{n\in\mathbb{N}}q^{n}\operatorname*{Tr}\nolimits_{S^{n}V}\left(
S^{n}A\right)  =\dfrac{1}{\det\left(  1-qA\right)  },
\]
where $S^{n}A$ denotes the $n$-th symmetric power of the operator $A$.)
\end{proposition}

Let us consider some examples:

\begin{example}
\label{exa.sl2}Let $\mathfrak{g}=\mathfrak{sl}_{2}$. We can write this Lie
algebra in terms of Chevalley generators and their relations (this is a
particular case of what we did in Proposition \ref{prop.grad.g}). The most
traditional way to do this is by setting $e=\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $, $f=\left(
\begin{array}
[c]{cc}%
0 & 0\\
1 & 0
\end{array}
\right)  $ and $h=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & -1
\end{array}
\right)  $; then, $\mathfrak{g}$ is generated by $e$, $f$ and $h$ as a Lie
algebra, and these generators satisfy $\left[  h,e\right]  =2e$, $\left[
h,f\right]  =-2f$ and $\left[  e,f\right]  =h$. Also, $\left(  e,f,h\right)  $
is a basis of the vector space $\mathfrak{g}$. In accordance with Proposition
\ref{prop.grad.g}, we grade $\mathfrak{g}$ by setting $\deg e=1$, $\deg f=-1$
and $\deg h=0$. Then, $\mathfrak{n}_{+}=\left\langle e\right\rangle $,
$\mathfrak{n}_{-}=\left\langle f\right\rangle $ and $\mathfrak{h}=\left\langle
h\right\rangle $. Hence, linear maps $\lambda:\mathfrak{h}\rightarrow
\mathbb{C}$ are in 1-to-1 correspondence with complex numbers (namely, the
images $\lambda\left(  h\right)  $ of $h$ under these maps). Thus, we can
identify any linear map $\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ with the
image $\lambda\left(  h\right)  \in\mathbb{C}$.

Consider any $\lambda\in\mathfrak{h}^{\ast}$. Since $\mathfrak{n}%
_{-}=\left\langle f\right\rangle $, the universal enveloping algebra $U\left(
\mathfrak{n}_{-}\right)  $ is the polynomial algebra $\mathbb{C}\left[
f\right]  $, and Proposition \ref{prop.verma1} \textbf{(a)} yields
$M_{\lambda}^{+}=\underbrace{U\left(  \mathfrak{n}_{-}\right)  }%
_{=\mathbb{C}\left[  f\right]  }v_{\lambda}^{+}=\mathbb{C}\left[  f\right]
v_{\lambda}^{+}$. Similarly, $M_{-\lambda}^{-}=\mathbb{C}\left[  e\right]
v_{-\lambda}^{-}$. In order to compute the bilinear form $\left(  \cdot
,\cdot\right)  $ on $M_{\lambda}^{+}\times M_{-\lambda}^{-}$, it is thus
enough to compute $\left(  f^{n}v_{\lambda}^{+},e^{n}v_{-\lambda}^{-}\right)
$ for all $n\in\mathbb{N}$. (The values $\left(  f^{n}v_{\lambda}^{+}%
,e^{m}v_{-\lambda}^{-}\right)  $ for $n\neq m$ are zero since the form has
degree $0$.) In order to do this, we notice that $e^{n}f^{n}v_{\lambda}%
^{+}=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
v_{\lambda}^{+}$\ \ \ \ \footnote{\textit{Proof.} Here is a sketch of the
proof. (If you want to see it in details, read the proof of Lemma
\ref{lem.serre-gen.sl2} \textbf{(a)} below; this lemma yields the equality
$e^{n}f^{n}v_{\lambda}^{+}=n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  v_{\lambda}^{+}$ by substituting $x=v_{\lambda}^{+}$.)
\par
First show that $hf^{m}v_{\lambda}^{+}=\left(  \lambda-2m\right)
f^{m}v_{\lambda}^{+}$ for every $m\in\mathbb{N}$. (This follows easily by
induction over $m$, using $hf-fh=\left[  h,f\right]  =-2f$.)
\par
Next show that $ef^{n}v_{\lambda}^{+}=n\left(  \lambda-n+1\right)
f^{n-1}v_{\lambda}^{+}$ for every positive $n\in\mathbb{N}$. (This is again an
easy induction proof using the equalities $ef-fe=\left[  e,f\right]  =h$,
$hv_{\lambda}^{+}=\underbrace{\lambda\left(  h\right)  }_{=\lambda}v_{\lambda
}^{+}=\lambda v_{\lambda}^{+}$ and $ev_{\lambda}^{+}=0$, and using the
equality $hf^{m}v_{\lambda}^{+}=\left(  \lambda-2m\right)  f^{m}v_{\lambda
}^{+}$ applied to $m=n-1$.)
\par
Now show that $e^{n}f^{n}v_{\lambda}^{+}=n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  v_{\lambda}^{+}$ for every $n\in\mathbb{N}$.
(For this, again use induction.)} and thus%
\begin{align}
\left(  f^{n}v_{\lambda}^{+},e^{n}v_{-\lambda}^{-}\right)   &  =\left(
\underbrace{S\left(  e^{n}\right)  }_{=\left(  -1\right)  ^{n}e^{n}}%
f^{n}v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =\left(  \left(  -1\right)
^{n}\underbrace{e^{n}f^{n}v_{\lambda}^{+}}_{\substack{=n!\lambda\left(
\lambda-1\right)  ...\left(  \lambda-n+1\right)  v_{\lambda}^{+}}%
},v_{-\lambda}^{-}\right) \nonumber\\
&  =\left(  \left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right)
\nonumber\\
&  =\left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  \underbrace{\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  }_{=1}\label{exa.sl2.bilinform}\\
&  =\left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  .\nonumber
\end{align}
So $M_{\lambda}^{+}$ is irreducible if $\lambda\notin\mathbb{Z}_{+}$. If
$\lambda\in\mathbb{Z}_{+}$, then $J_{\lambda}^{+}=\left\langle f^{n}%
v_{\lambda}^{+}\ \mid\ n\geq\lambda+1\right\rangle =\mathbb{C}\left[
f\right]  \cdot\left(  f^{\lambda+1}v_{\lambda}^{+}\right)  $, and the
irreducible $\mathfrak{g}$-module $L_{\lambda}^{+}=\left\langle \overline
{v_{\lambda}^{+}},f\overline{v_{\lambda}^{+}},...,f^{\lambda}\overline
{v_{\lambda}^{+}}\right\rangle $ has dimension $\dim\lambda+1$%
.\ \ \ \ \footnote{If you know the representation theory of $\mathfrak{sl}%
_{2}$, you probably recognize this module $L_{\lambda}^{+}$ as the $\left(
\dim\lambda\right)  $-th symmetric power of the vector module $\mathbb{C}^{2}$
(as there is only one irreducible $\mathfrak{sl}_{2}$-module of every
dimension).}
\end{example}

\begin{example}
\label{exa.Vir}Let $\mathfrak{g}=\operatorname*{Vir}$. With the grading that
we have defined on $\operatorname*{Vir}$, we have $\mathfrak{h}=\mathfrak{g}%
_{0}=\left\langle L_{0},C\right\rangle $. Thus, linear maps $\lambda
:\mathfrak{h}\rightarrow\mathbb{C}$ can be uniquely described by the images of
$L_{0}$ and $C$ under these maps. We thus identify every linear map
$\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ with the pair $\left(
\lambda\left(  L_{0}\right)  ,\lambda\left(  C\right)  \right)  $.

For every $\lambda=\left(  \lambda\left(  L_{0}\right)  ,\lambda\left(
C\right)  \right)  $, the number $\lambda\left(  L_{0}\right)  $ is denoted by
$h$ and called the \textit{conformal weight} of $\lambda$, and the number
$\lambda\left(  C\right)  $ is denoted by $c$ and called the \textit{central
charge} of $\lambda$. Thus, $\lambda$ is identified with the pair $\left(
h,c\right)  $. As a consequence, the Verma modules $M_{\lambda}^{+}$ and
$M_{\lambda}^{-}$ are often denoted by $M_{h,c}^{+}$ and $M_{h,c}^{-}$,
respectively, and the modules $L_{\lambda}^{+}$ and $L_{\lambda}^{-}$ are
often denoted by $L_{h,c}^{+}$ and $L_{h,c}^{-}$, respectively.

(Note, of course, that the central charge of $\lambda$ is the central charge
of each of the $\operatorname*{Vir}$-modules $M_{\lambda}^{+}$, $M_{\lambda
}^{-}$, $L_{\lambda}^{+}$ and $L_{\lambda}^{-}$.)

Consider any $\lambda\in\mathfrak{h}^{\ast}$. Let us compute the bilinear form
$\left(  \cdot,\cdot\right)  $ on $M_{\lambda}^{+}\times M_{-\lambda}^{-}$.
Note first that $L_{0}v_{\lambda}^{+}=\underbrace{\lambda\left(  L_{0}\right)
}_{=h}v_{\lambda}^{+}=hv_{\lambda}^{+}$ and $Cv_{\lambda}^{+}%
=\underbrace{\lambda\left(  C\right)  }_{=c}v_{\lambda}^{+}=cv_{\lambda}^{+}$.

In order to compute $\left(  L_{-1}v_{\lambda}^{+},L_{1}v_{-\lambda}%
^{-}\right)  $, we notice that
\[
\underbrace{L_{1}L_{-1}}_{=L_{-1}L_{1}+\left[  L_{1},L_{-1}\right]
}v_{\lambda}^{+}=L_{-1}\underbrace{L_{1}v_{\lambda}^{+}}_{=0}%
+\underbrace{\left[  L_{1},L_{-1}\right]  }_{=2L_{0}}v_{\lambda}%
^{+}=2\underbrace{L_{0}v_{\lambda}^{+}}_{=hv_{\lambda}^{+}}=2hv_{\lambda}%
^{+},
\]
so that%
\[
\left(  L_{-1}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)  =\left(
-\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}},v_{-\lambda}%
^{-}\right)  =\left(  -2hv_{\lambda}^{+},v_{-\lambda}^{-}\right)
=-2h\underbrace{\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  }_{=1}=-2h.
\]
Since $\left(  L_{-1}v_{\lambda}^{+}\right)  $ is a basis of $M_{\lambda}%
^{+}\left[  -1\right]  $ and $\left(  L_{1}v_{-\lambda}^{-}\right)  $ is a
basis of $M_{-\lambda}^{-}\left[  1\right]  $, this yields $\det\left(
\left(  \cdot,\cdot\right)  _{1}\right)  =2h$ (where $\left(  \cdot
,\cdot\right)  _{1}$ denotes the restriction of the form $\left(  \cdot
,\cdot\right)  $ to $M_{\lambda}^{+}\left[  -1\right]  \times M_{-\lambda}%
^{-}\left[  1\right]  $). This vanishes for $h=0$.

In degree $2$, the form is somewhat more complicated: With respect to the
basis $\left(  L_{-1}^{2}v_{\lambda}^{+},L_{-2}v_{\lambda}^{+}\right)  $ of
$M_{\lambda}^{+}\left[  -2\right]  $, and the basis $\left(  L_{1}%
^{2}v_{-\lambda}^{-},L_{2}v_{-\lambda}^{-}\right)  $ of $M_{-\lambda}%
^{-}\left[  2\right]  $, the restriction $\left(  \cdot,\cdot\right)  _{2}$ of
the form $\left(  \cdot,\cdot\right)  $ to $M_{\lambda}^{+}\left[  -2\right]
\times M_{-\lambda}^{-}\left[  2\right]  $ is given by the matrix%
\[
\left(
\begin{array}
[c]{cc}%
\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)  & \left(
L_{-1}^{2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right) \\
\left(  L_{-2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)  & \left(
L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)
\end{array}
\right)  .
\]


Let us compute, as an example, the lower right entry of this matrix, that is,
the entry $\left(  L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)  $. We
have%
\begin{align*}
\underbrace{L_{2}L_{-2}}_{=L_{-2}L_{2}+\left[  L_{2},L_{-2}\right]
}v_{\lambda}^{+}  &  =L_{-2}\underbrace{L_{2}v_{\lambda}^{+}}_{=0}%
+\underbrace{\left[  L_{2},L_{-2}\right]  }_{=4L_{0}+\dfrac{1}{2}C}v_{\lambda
}^{+}=\left(  4L_{0}+\dfrac{1}{2}C\right)  v_{\lambda}^{+}=4\underbrace{L_{0}%
v_{\lambda}^{+}}_{=hv_{\lambda}^{+}}+\dfrac{1}{2}\underbrace{Cv_{\lambda}^{+}%
}_{=cv_{\lambda}^{+}}\\
&  =4hv_{\lambda}^{+}+\dfrac{1}{2}cv_{\lambda}^{+}=\left(  4h+\dfrac{1}%
{2}c\right)  v_{\lambda}^{+},
\end{align*}
so that%
\begin{align*}
\left(  L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)   &  =\left(
-\underbrace{L_{2}L_{-2}v_{\lambda}^{+}}_{=\left(  4h+\dfrac{1}{2}c\right)
v_{\lambda}^{+}},v_{-\lambda}^{-}\right)  =\left(  -\left(  4h+\dfrac{1}%
{2}c\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =-\left(  4h+\dfrac{1}{2}c\right)  \underbrace{\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  }_{=1}=-\left(  4h+\dfrac{1}{2}c\right)  .
\end{align*}
As a further (more complicated) example, let us compute the upper left entry
of the matrix, namely $\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda
}^{-}\right)  $. We have%
\begin{align*}
L_{1}^{2}L_{-1}^{2}v_{\lambda}^{+}  &  =L_{1}\underbrace{L_{1}L_{-1}}%
_{=L_{-1}L_{1}+\left[  L_{1},L_{-1}\right]  }L_{-1}v_{\lambda}^{+}=L_{1}%
L_{-1}\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}%
+L_{1}\underbrace{\left[  L_{1},L_{-1}\right]  }_{=2L_{0}}L_{-1}v_{\lambda
}^{+}\\
&  =2h\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}%
+2L_{1}\underbrace{L_{0}L_{-1}}_{\substack{=L_{-1}L_{0}+\left[  L_{0}%
,L_{-1}\right]  \\=L_{-1}L_{0}+L_{-1}\\\text{(since }\left[  L_{0}%
,L_{-1}\right]  =L_{-1}\text{)}}}v_{\lambda}^{+}=4h^{2}v_{\lambda}^{+}%
+2L_{1}L_{-1}\underbrace{L_{0}v_{\lambda}^{+}}_{=hv_{\lambda}^{+}%
}+2\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}\\
&  =4h^{2}v_{\lambda}^{+}+2h\underbrace{L_{1}L_{-1}v_{\lambda}^{+}%
}_{=2hv_{\lambda}^{+}}+4hv_{\lambda}^{+}=4h^{2}v_{\lambda}^{+}+4h^{2}%
v_{\lambda}^{+}+4hv_{\lambda}^{+}=\left(  8h^{2}+4h\right)  v_{\lambda}^{+}%
\end{align*}
and thus%
\begin{align*}
\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)   &
=\left(  -L_{1}L_{-1}^{2}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)
=\left(  \underbrace{L_{1}^{2}L_{-1}^{2}v_{\lambda}^{+}}_{=\left(
8h^{2}+4h\right)  v_{\lambda}^{+}},v_{-\lambda}^{-}\right)  =\left(  \left(
8h^{2}+4h\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =\left(  8h^{2}+4h\right)  \underbrace{\left(  v_{\lambda}^{+},v_{-\lambda
}^{-}\right)  }_{=1}=8h^{2}+4h.
\end{align*}


Similarly, we compute the other two entries of the matrix. The matrix thus
becomes%
\[
\left(
\begin{array}
[c]{cc}%
8h^{2}+4h & 6h\\
-6h & -\left(  4h+\dfrac{1}{2}c\right)
\end{array}
\right)  .
\]
The determinant of this matrix is%
\[
\det\left(  \left(  \cdot,\cdot\right)  _{2}\right)  =\left(  8h^{2}%
+4h\right)  \left(  -\left(  4h+\dfrac{1}{2}c\right)  \right)  -6h\left(
-6h\right)  =-4h\left(  \left(  2h+1\right)  \left(  4h+\dfrac{1}{2}c\right)
-9h\right)  .
\]
Notice the term $\left(  2h+1\right)  \left(  4h+\dfrac{1}{2}c\right)  -9h$:
The set of zeroes of this term is a hyperbola\footnote{Here, a
\textit{hyperbola} means an affine conic over $\mathbb{C}$ which is defined
over $\mathbb{R}$ and whose restriction to $\mathbb{R}$ is a hyperbola.}. The
determinant of $\left(  \cdot,\cdot\right)  _{2}$ thus vanishes on the union
of a line and a hyperbola. For every point $\left(  h,c\right)  $ lying on
this hyperbola, the highest-weight module $M_{h,c}^{+}$ has a nonzero singular
vector in degree $-2$ (this means a nonzero singular vector of the form
$\alpha L_{-2}v_{\lambda}^{+}+\beta L_{-1}^{2}v_{\lambda}^{+}$ for some
$\alpha,\beta\in\mathbb{C}$).

We will later discuss $\det\left(  \left(  \cdot,\cdot\right)  _{n}\right)  $
for generic $n$. In fact, there is an explicit formula for this determinant,
namely the so-called Kac determinant formula.
\end{example}

\subsubsection{Restricted dual modules}

\begin{definition}
Let $V=\bigoplus\limits_{i\in I}V\left[  i\right]  $ be an $I$-graded vector
space, where $I$ is some set (for example, $I$ can be $\mathbb{Z}$,
$\mathbb{N}$ or $\mathbb{C}$). The \textit{restricted dual} $V^{\vee}$ of $V$
is defined to be the direct sum $\bigoplus\limits_{i\in I}V\left[  i\right]
^{\ast}$. This is a vector subspace of the dual $V^{\ast}$ of $V$, but (in
general) not the same as $V^{\ast}$ unless the direct sum is finite.

One can make the restricted dual $V^{\vee}$ into an $I$-graded vector space by
defining $V^{\vee}\left[  i\right]  =V\left[  i\right]  ^{\ast}$ for every
$i\in I$. But when $I$ is an abelian group, one can also make the restricted
dual $V^{\vee}$ into an $I$-graded vector space by defining $V^{\vee}\left[
i\right]  =V\left[  -i\right]  ^{\ast}$ for every $i\in I$. These two
constructions result in two (generally) \textbf{different} gradings on
$V^{\vee}$; both of these gradings are used in algebra.

Using either of these two gradings on $V^{\vee}$, we can make sense of the
restricted dual $V^{\vee\vee}$ of $V^{\vee}$. This restricted dual
$V^{\vee\vee}$ does not depend on which of the two gradings on $V^{\vee}$ has
been chosen. There is a canonical injection $V\rightarrow V^{\vee\vee}$. If
$V\left[  i\right]  $ is finite-dimensional for every $i\in I$, then this
injection $V\rightarrow V^{\vee\vee}$ is an isomorphism (so that $V^{\vee\vee
}\cong V$ canonically).

If $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie algebra, and $V$ is a
$\mathbb{C}$-graded $\mathfrak{g}$-module, then $V^{\vee}$ canonically becomes
a $\mathbb{C}$-graded $\mathfrak{g}$-module if the grading on $V^{\vee}$ is
defined by $V^{\vee}\left[  i\right]  =V\left[  -i\right]  ^{\ast}$ for every
$i\in\mathbb{C}$. (Note that the grading defined by $V^{\vee}\left[  i\right]
=V\left[  i\right]  ^{\ast}$ for every $i\in\mathbb{C}$ would \textbf{not} (in
general) make $V^{\vee}$ into a $\mathbb{C}$-graded $\mathfrak{g}$-module.)
\end{definition}

It is clear that:

\begin{proposition}
We have two mutually inverse antiequivalences of categories $\mathcal{O}%
^{+}\overset{\vee}{\rightarrow}\mathcal{O}^{-}$ and $\mathcal{O}%
^{-}\overset{\vee}{\rightarrow}\mathcal{O}^{+}$, each defined by mapping every
$\mathfrak{g}$-module in one category to its restricted dual.
\end{proposition}

We can view the form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ as a linear map $M_{\lambda}%
^{+}\rightarrow\left(  M_{-\lambda}^{-}\right)  ^{\vee}$. The kernel of this
map is $J_{\lambda}^{+}$, and therefore, when $\mathfrak{g}$ is nondegenerate,
this map is an isomorphism for Weil-generic $\lambda$ (by Theorem
\ref{thm.invformnondeg}). In general, this map factors as $%
%TCIMACRO{\TeXButton{diag}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong} &
%\left(L^{-}_{-\lambda}\right)^{\vee} \arinj[r] & \left(M^{-}_{-\lambda}%
%\right)^{\vee}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong} &
\left(L^{-}_{-\lambda}\right)^{\vee} \arinj[r] & \left(M^{-}_{-\lambda}%
\right)^{\vee}
}%
%EndExpansion
$.

\subsubsection{\label{subsect.invol}Involutions}

In many applications, we are not just working with a graded Lie algebra
$\mathfrak{g}$. Very often we additionally have a degree-reversing involution:

\begin{definition}
\label{def.invol}Let $\mathfrak{g}$ be a graded Lie algebra. Let
$\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be an involutive automorphism of
the Lie algebra $\mathfrak{g}$ (``involutive'' means $\omega^{2}%
=\operatorname*{id}$) such that $\omega\left(  \mathfrak{g}_{i}\right)
=\mathfrak{g}_{-i}$ for all $i\in\mathbb{Z}$ and such that $\omega
\mid_{\mathfrak{g}_{0}}=-\operatorname*{id}$. Then, for every graded
$\mathfrak{g}$-module $M$, we can define a graded $\mathfrak{g}$-module
$M^{c}$ as being the $\mathfrak{g}$-module $M^{\omega}$ with opposite grading
(i. e., the grading on $M^{c}$ is defined by $M^{c}\left[  i\right]
=M^{\omega}\left[  -i\right]  $ for every $i$). Then, we have an equivalence
of categories $\mathcal{O}^{+}\overset{\omega}{\rightarrow}\mathcal{O}^{-}$
which sends every $\mathfrak{g}$-module $M\in\mathcal{O}^{+}$ to the
$\mathfrak{g}$-module $M^{c}\in\mathcal{O}^{-}$, and the quasiinverse
equivalence of categories $\mathcal{O}^{-}\overset{\omega}{\rightarrow
}\mathcal{O}^{+}$ which does the same thing.

So the functor $\mathcal{O}^{+}\overset{\vee}{\rightarrow}\mathcal{O}%
^{-}\overset{\omega}{\rightarrow}\mathcal{O}^{+}$ is an antiequivalence,
called the \textit{functor of contragredient module}. This functor allows us
to identify $\left(  M_{-\lambda}^{-}\right)  ^{\omega}$ with $M_{\lambda}%
^{+}$ (via the isomorphism $M_{\lambda}^{+}\rightarrow\left(  M_{-\lambda}%
^{-}\right)  ^{\omega}$ which sends $x\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }v_{\lambda}^{+}$ to $\left(  U\left(
\omega\right)  \right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }v_{-\lambda}^{-}$ for every $x\in U\left(
\mathfrak{g}\right)  $), and thus to view the form $\left(  \cdot
,\cdot\right)  $ as a form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$. But this form is not $\mathfrak{g}%
$-invariant; it is contravariant; this means that any $a\in\mathfrak{g}$,
$v\in M_{\lambda}^{+}$ and $w\in M_{\lambda}^{+}$ satisfy $\left(
av,w\right)  =-\left(  v,\omega\left(  a\right)  w\right)  $ and $\left(
v,aw\right)  =-\left(  \omega\left(  a\right)  v,w\right)  $.

This form can be viewed as a linear map $M_{\lambda}^{+}\rightarrow\left(
M_{\lambda}^{+}\right)  ^{c}$, which factors into $%
%TCIMACRO{\TeXButton{diag}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong}
%& \left(L^{+}_{\lambda}\right)^{c} \arinj[r] & \left(M^{+}_{\lambda}%
%\right)^{c}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong}
& \left(L^{+}_{\lambda}\right)^{c} \arinj[r] & \left(M^{+}_{\lambda}%
\right)^{c}
}%
%EndExpansion
$.

Notice that this form $\left(  \cdot,\cdot\right)  $ is a contravariant form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$. Of course, this yields
that the transpose of $\left(  \cdot,\cdot\right)  $ is also such a form.
Since there exists a \textbf{unique} contravariant form $M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{\lambda}^{+}\right)  =1$ (because contravariant forms $M_{\lambda}%
^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ are in 1-to-1 correspondence
with $\mathfrak{g}$-invariant bilinear forms $M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$, and for the latter we have Proposition
\ref{prop.invform} \textbf{(a)}), this yields that the form $\left(
\cdot,\cdot\right)  $ and its transpose must be identical. In other words, the
form $\left(  \cdot,\cdot\right)  $ is symmetric.
\end{definition}

Involutive automorphisms of $\mathfrak{g}$ satisfying the conditions of
Definition \ref{def.invol} are not uncommon; here are four examples:

\begin{proposition}
\label{prop.invol.A}The $\mathbb{C}$-linear map $\omega:\mathcal{A}%
\rightarrow\mathcal{A}$ defined by $\omega\left(  K\right)  =-K$ and
$\omega\left(  a_{i}\right)  =-a_{-i}$ for every $i\in\mathbb{Z}$ is an
involutive automorphism of the Lie algebra $\mathcal{A}$. This automorphism
$\omega$ satisfies the conditions of Definition \ref{def.invol} (for
$\mathfrak{g}=\mathcal{A}$). We already know this from Proposition
\ref{prop.A.omega}. Moreover, if we let $\lambda=\left(  1,\mu\right)  $ for a
complex number $\mu$, then $M_{\lambda}^{+}\cong F_{\mu}$ (by Proposition
\ref{prop.fockverma.A}), and thus we can regard the contravariant form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ from Definition
\ref{def.invol} as a contravariant form $F_{\mu}\times F_{\mu}\rightarrow
\mathbb{C}$. This contravariant form $F_{\mu}\times F_{\mu}\rightarrow
\mathbb{C}$ is exactly the form $\left(  \cdot,\cdot\right)  $ of Proposition
\ref{prop.A.contravariantform}. (This is because the form $\left(  \cdot
,\cdot\right)  $ of Proposition \ref{prop.A.contravariantform} is
contravariant (due to Proposition \ref{prop.A.contravariantform} \textbf{(c)}
and \textbf{(d)}) and satisfies $\left(  1,1\right)  =1$.)
\end{proposition}

\begin{proposition}
The $\mathbb{C}$-linear map $\omega:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ defined by $\omega\left(  C\right)  =-C$ and
$\omega\left(  L_{i}\right)  =-L_{-i}$ for every $i\in\mathbb{Z}$ is an
involutive automorphism of the Lie algebra $\operatorname*{Vir}$. This
automorphism $\omega$ satisfies the conditions of Definition \ref{def.invol}
(for $\mathfrak{g}=\operatorname*{Vir}$).
\end{proposition}

\begin{proposition}
\label{prop.simple.omega}Let $\mathfrak{g}$ be a simple Lie algebra, graded
and presented as in Proposition \ref{prop.grad.g}. Then, there exists a unique
Lie algebra homomorphism $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$
satisfying $\omega\left(  e_{i}\right)  =-f_{i}$, $\omega\left(  h_{i}\right)
=-h_{i}$ and $\omega\left(  f_{i}\right)  =-e_{i}$ for every $i\in\left\{
1,2,...,m\right\}  $. This automorphism $\omega$ satisfies the conditions of
Definition \ref{def.invol}.
\end{proposition}

\begin{proposition}
Let $\mathfrak{g}$ be a simple finite-dimensional Lie algebra, graded and
presented as in Proposition \ref{prop.grad.g}. Let $\widehat{\mathfrak{g}}$ be
the Kac-Moody Lie algebra defined in Definition \ref{def.kac}. Let $K$ denote
the element $\left(  0,1\right)  $ of $\mathfrak{g}\left[  t,t^{-1}\right]
\oplus\mathbb{C}=\widehat{\mathfrak{g}}$. Consider the $\mathbb{Z}$-grading on
$\widehat{\mathfrak{g}}$ defined in Proposition \ref{prop.grad.ghat.simple}.

Let $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be defined as in Proposition
\ref{prop.simple.omega}. Then, the $\mathbb{C}$-linear map $\widehat{\omega
}:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$ defined by
$\widehat{\omega}\left(  a\cdot t^{j}\right)  =\omega\left(  a\right)  t^{-j}$
for every $a\in\mathfrak{g}$ and $j\in\mathbb{Z}$, and $\widehat{\omega
}\left(  K\right)  =-K$, is an involutive automorphism of the Lie algebra
$\widehat{\mathfrak{g}}$. This automorphism $\widehat{\omega}$ satisfies the
conditions of Definition \ref{def.invol} (for $\widehat{\mathfrak{g}}$ and
$\widehat{\omega}$ instead of $\mathfrak{g}$ and $\omega$).
\end{proposition}

More generally:

\begin{proposition}
Let $\mathfrak{g}$ be a Lie algebra equipped with a $\mathfrak{g}$-invariant
symmetric bilinear form $\left(  \cdot,\cdot\right)  $ of degree $0$. Let
$\widehat{\mathfrak{g}}$ be the Lie algebra defined in Definition
\ref{def.loop}. Let $K$ denote the element $\left(  0,1\right)  $ of
$\mathfrak{g}\left[  t,t^{-1}\right]  \oplus\mathbb{C}=\widehat{\mathfrak{g}}$.

Let $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be an involutive automorphism
of the Lie algebra $\mathfrak{g}$ (not to be confused with the $2$-cocycle
$\omega$ of Definition \ref{def.loop}). Then, the $\mathbb{C}$-linear map
$\widehat{\omega}:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$
defined by $\widehat{\omega}\left(  a\cdot t^{j}\right)  =\omega\left(
a\right)  t^{-j}$ for every $a\in\mathfrak{g}$ and $j\in\mathbb{Z}$, and
$\widehat{\omega}\left(  K\right)  =-K$, is an involutive automorphism of the
Lie algebra $\widehat{\mathfrak{g}}$.

Assume now that the Lie algebra $\mathfrak{g}$ is graded and that the
automorphism $\omega$ satisfies the conditions of Definition \ref{def.invol}.
Assume further that we extend the grading of $\mathfrak{g}$ to a grading on
$\widehat{\mathfrak{g}}$ in such a way that $K$ is homogeneous of degree $0$,
and that the multiplications by $t$ and $t^{-1}$ are homogeneous linear maps
(that is, linear maps which shift the degree by a fixed integer). Then, the
automorphism $\widehat{\omega}$ of $\widehat{\mathfrak{g}}$ satisfies
$\widehat{\omega}\left(  \widehat{\mathfrak{g}}_{i}\right)
=\widehat{\mathfrak{g}}_{-i}$ for all $i\in\mathbb{Z}$. (But in general,
$\widehat{\omega}$ does not necessarily satisfy $\widehat{\omega}%
\mid_{\widehat{\mathfrak{g}}_{0}}=-\operatorname*{id}$.)
\end{proposition}

\subsubsection{\textbf{[unfinished]} Unitary structures}

\begin{impnot}
\textbf{The parts of these notes concerned with unitary/Hermitian/real
structures are in an unfinished state and contain mistakes which I don't know
how to fix.}

For instance, if we define $\mathfrak{g}_{\mathbb{R}}$ by $\mathfrak{g}%
_{\mathbb{R}}=\left\{  a\in\mathfrak{g}\ \mid\ a^{\dag}=-a\right\}  $, and
define $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ by $\mathfrak{g}_{0\mathbb{R}%
}^{\ast}=\left\{  f\in\mathfrak{g}_{0}^{\ast}\ \mid\ f\left(  \mathfrak{g}%
_{0\mathbb{R}}\right)  \subseteq\mathbb{R}\right\}  $ (as I do below), and
define the antilinear $\mathbb{R}$-antiinvolution $\dag:\operatorname*{Vir}%
\rightarrow\operatorname*{Vir}$ on $\operatorname*{Vir}$ by $L_{i}^{\dag
}=L_{-i}$ for all $i\in\mathbb{Z}$, and $C^{\dag}=C$, then
$\operatorname*{Vir}\nolimits_{0\mathbb{R}}^{\ast}$ is \textbf{not} the set of
all weights $\left(  h,c\right)  $ satisfying $h,c\in\mathbb{R}$, but it is
the set of all weights $\left(  h,c\right)  $ satisfying $ih,ic\in\mathbb{R}$
(because the definition of $\dag$ that we gave leads to $\operatorname*{Vir}%
\nolimits_{0\mathbb{R}}=\left\langle iC,iL_{0}\right\rangle _{\mathbb{R}}$).
This is not what we want later. Probably it is possible to fix these issues by
correcting some signs, but I do not know how. If you know a consistent way to
correct these definitions and results, please drop me a mail
(AB\texttt{@gmail.com} where A=\texttt{darij} and B=\texttt{grinberg}).
\end{impnot}

Over $\mathbb{C}$, it makes sense to study not only linear but also antilinear
maps. Sometimes, the latter actually enjoy even better properties of the
former (e. g., Hermitian forms are better behaved than complex-symmetric forms).

\begin{definition}
If $\mathfrak{g}$ and $\mathfrak{h}$ are two Lie algebras over a field $k$,
then a $k$-\textit{antihomomorphism} from $\mathfrak{g}$ to $\mathfrak{h}$
means a $k$-linear map $f:\mathfrak{g}\rightarrow\mathfrak{h}$ such that
$f\left(  \left[  x,y\right]  \right)  =-\left[  f\left(  x\right)  ,f\left(
y\right)  \right]  $ for all $x,y\in\mathfrak{g}$.
\end{definition}

\begin{definition}
In the following, an \textit{$k$-antiinvolution} of a Lie algebra
$\mathfrak{g}$ over a field $k$ means a $k$-antihomomorphism from
$\mathfrak{g}$ to $\mathfrak{g}$ which is simultaneously an involution.
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra. Let $\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ be an antilinear $\mathbb{R}$-antiinvolution. This
means that $\dag$ is an $\mathbb{R}$-linear map and satisfies the relations%
\begin{align*}
\dag^{2}  &  =\operatorname*{id};\\
\left(  za\right)  ^{\dag}  &  =\overline{z}a^{\dag}%
\ \ \ \ \ \ \ \ \ \ \text{for all }z\in\mathbb{C}\text{ and }a\in
\mathfrak{g};\\
\left[  a,b\right]  ^{\dag}  &  =-\left[  a^{\dag},b^{\dag}\right]
\ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in\mathfrak{g}.
\end{align*}
(Here and in the following, we write $c^{\dag}$ for the image of an element
$c\in\mathfrak{g}$ under $\dag$.) Such a map $\dag$ is called a \textit{real
structure}, for the following reason: If $\dag$ is such a map, then we can
define an $\mathbb{R}$-vector subspace $\mathfrak{g}_{\mathbb{R}}=\left\{
a\in\mathfrak{g}\ \mid\ a^{\dag}=-a\right\}  $ of $\mathfrak{g}$, and this
$\mathfrak{g}_{\mathbb{R}}$ is a real Lie algebra such that $\mathfrak{g}%
\cong\mathfrak{g}_{\mathbb{R}}\otimes_{\mathbb{R}}\mathbb{C}$ as complex Lie
algebras. (It is said that $\mathfrak{g}_{\mathbb{R}}$ is a \textit{real form}
of $\mathfrak{g}$.)
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. If
$V$ is a $\mathfrak{g}$-module, we say that $V$ is \textit{Hermitian} if $V$
is equipped with a nondegenerate Hermitian form $\left(  \cdot,\cdot\right)  $
satisfying%
\[
\left(  av,w\right)  =\left(  v,a^{\dag}w\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }v\in V\text{ and
}w\in V.
\]
The $\mathfrak{g}$-module $V$ is said to be \textit{unitary} if this form is
positive definite.
\end{definition}

The real Lie algebra $\mathfrak{g}_{\mathbb{R}}$ acts on a Hermitian module by
skew-Hermitian operators.

\begin{remark}
While we will not be studying Lie groups in this course, here are some facts
about them that explain why unitary $\mathfrak{g}$-modules are called ``unitary'':

If $\mathfrak{g}$ is a finite-dimensional Lie algebra, and $V$ is a unitary
$\mathfrak{g}$-module, then the Hilbert space completion of $V$ is a unitary
representation of the Lie group $G_{\mathbb{R}}=\exp\left(  \mathfrak{g}%
_{\mathbb{R}}\right)  $ corresponding to $\mathfrak{g}_{\mathbb{R}}$ by Lie's
Third Theorem. (Note that this Hilbert space completion of $V$ is $V$ itself
if $\dim V<\infty$.) This even holds for some infinite-dimensional
$\mathfrak{g}$ under sufficiently restrictive conditions.
\end{remark}

So let us consider this situation. Two definitions:

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. Let
$V$ be a $\mathfrak{g}$-module. A Hermitian form $\left(  \cdot,\cdot\right)
$ on $V$ is said to be $\dag$\textit{-invariant} if and only if%
\[
\left(  av,w\right)  =\left(  v,a^{\dag}w\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }v\in V\text{ and
}w\in V.
\]

\end{definition}

\begin{definition}
\label{def.gRstar}Let $\mathfrak{g}$ be a complex Lie algebra with a real
structure $\dag$. For every $f\in\mathfrak{g}^{\ast}$, we denote by $f^{\dag}$
the map $\mathfrak{g}_{0}\rightarrow\mathbb{C},$ $x\mapsto\overline{f\left(
x^{\dag}\right)  }$ (this map $f^{\dag}$ is easily seen to be $\mathbb{C}%
$-linear). Let $\mathfrak{g}_{\mathbb{R}}^{\star}$ be the subset $\left\{
f\in\mathfrak{g}^{\ast}\ \mid\ f^{\dag}=-f\right\}  $ of $\mathfrak{g}^{\star
}$. Then, it is easily seen that%
\[
\mathfrak{g}_{\mathbb{R}}^{\star}=\left\{  f\in\mathfrak{g}^{\ast}%
\ \mid\ f\left(  \mathfrak{g}_{\mathbb{R}}\right)  \subseteq\mathbb{R}%
\right\}  .
\]
Hence, we get an $\mathbb{R}$-bilinear form $\mathfrak{g}_{\mathbb{R}}^{\star
}\times\mathfrak{g}_{\mathbb{R}}\rightarrow\mathbb{R},$ $\left(  f,a\right)
\mapsto f\left(  a\right)  $. This form is nondegenerate and thus enables us
to identify $\mathfrak{g}_{\mathbb{R}}^{\star}$ with the dual space of the
$\mathbb{R}$-vector space $\mathfrak{g}_{\mathbb{R}}$. (More precisely, we
have an isomorphism from $\mathfrak{g}_{\mathbb{R}}^{\star}$ to the dual space
of the $\mathbb{R}$-vector space $\mathfrak{g}_{\mathbb{R}}$. This isomorphism
sends every $f\in\mathfrak{g}_{\mathbb{R}}^{\star}$ to the map $f\mid
_{\mathfrak{g}_{\mathbb{R}}}$ (with target restricted to $\mathbb{R}$), and
conversely, the preimage of any $\mathbb{R}$-linear map $F:\mathfrak{g}%
_{\mathbb{R}}\rightarrow\mathbb{R}$ is the $\mathbb{C}$-linear map
$f\in\mathfrak{g}_{\mathbb{R}}^{\star}$ given by%
\[
f\left(  a\right)  =F\left(  \dfrac{a-a^{\dag}}{2}\right)  +iF\left(
\dfrac{a+a^{\dag}}{2i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathfrak{g}.
\]
) We can thus write $\mathfrak{g}_{\mathbb{R}}^{\ast}$ for $\mathfrak{g}%
_{\mathbb{R}}^{\star}$.

The elements of $\mathfrak{g}_{\mathbb{R}}^{\ast}$ are said to be the
\textit{real} elements of $\mathfrak{g}^{\ast}$.
\end{definition}

\begin{proposition}
\label{prop.M+l.unitary}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie
algebra with real structure $\dag$. Assume that the map $\dag$ reverses the
degree (i. e., every $j\in\mathbb{Z}$ satisfies $\dag\left(  \mathfrak{g}%
_{j}\right)  \subseteq\mathfrak{g}_{-j}$). In particular, $\dag\left(
\mathfrak{g}_{0}\right)  \subseteq\mathfrak{g}_{0}$. Also, assume that
$\mathfrak{g}_{0}$ is an abelian Lie algebra (but let us not require
$\mathfrak{g}$ to be nondegenerate). Note that $\mathfrak{g}_{0}$ itself is a
Lie algebra, and thus Definition \ref{def.gRstar} can be applied to
$\mathfrak{g}_{0}$ in lieu of $\mathfrak{g}$.

If $\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$, then the $\mathfrak{g}%
$-module $M_{\lambda}^{+}$ carries a $\dag$-invariant Hermitian form $\left(
\cdot,\cdot\right)  $ satisfying $\left(  v_{\lambda}^{+},v_{\lambda}%
^{+}\right)  =1$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.M+l.unitary}.} In the following,
whenever $U$ is a $\mathbb{C}$-vector space, we will denote by $\overline{U}$
the $\mathbb{C}$-vector space which is identical to $U$ as a set, but with the
$\mathbb{C}$-vector space structure twisted by complex conjugation.

The antilinear $\mathbb{R}$-Lie algebra homomorphism $-\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ can be viewed as a $\mathbb{C}$-Lie algebra
homomorphism $-\dag:\mathfrak{g}\rightarrow\overline{\mathfrak{g}}$, and thus
induces a $\mathbb{C}$-algebra homomorphism $U\left(  -\dag\right)  :U\left(
\mathfrak{g}\right)  \rightarrow U\left(  \overline{\mathfrak{g}}\right)  $.
Since $U\left(  \overline{\mathfrak{g}}\right)  \cong\overline{U\left(
\mathfrak{g}\right)  }$ canonically as $\mathbb{C}$-algebras (because taking
the universal enveloping algebra commutes with base change)\footnote{Warning:
This isomorphism $U\left(  \overline{\mathfrak{g}}\right)  \rightarrow
\overline{U\left(  \mathfrak{g}\right)  }$ sends $i\cdot1_{U\left(
\overline{\mathfrak{g}}\right)  }$ to $-i\cdot1_{U\left(  \mathfrak{g}\right)
}$.}, we can thus consider this $U\left(  -\dag\right)  $ as a $\mathbb{C}%
$-algebra homomorphism $U\left(  \mathfrak{g}\right)  \rightarrow
\overline{U\left(  \mathfrak{g}\right)  }$. This, in turn, can be viewed as an
antilinear $\mathbb{R}$-algebra homomorphism $U\left(  -\dag\right)  :U\left(
\mathfrak{g}\right)  \rightarrow U\left(  \mathfrak{g}\right)  $.

Let $\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$. Let $\left(  M_{-\lambda
}^{-}\right)  ^{-\dag}$ be the $\mathfrak{g}$-module $M_{-\lambda}^{-}$
twisted by the isomorphism $-\dag:\mathfrak{g}\rightarrow\mathfrak{g}$ of
$\mathbb{R}$-Lie algebras. Then, $\left(  M_{-\lambda}^{-}\right)  ^{-\dag}$
is a module over the $\mathbb{R}$-Lie algebra $\mathfrak{g}$, but not a module
over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$, since it satisfies $\left(
za\right)  \rightharpoonup v=\overline{z}\left(  a\rightharpoonup v\right)  $
(rather than $\left(  za\right)  \rightharpoonup v=z\left(  a\rightharpoonup
v\right)  $) for all $z\in\mathbb{C}$, $a\in\mathfrak{g}$ and $v\in
M_{-\lambda}^{-}$ (where $\rightharpoonup$ denotes the action of
$\mathfrak{g}$). However, this can be easily transformed into a $\mathbb{C}%
$-Lie algebra action: Namely, $\overline{\left(  M_{-\lambda}^{-}\right)
^{-\dag}}$ is a module over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$.

We have an isomorphism%
\begin{align*}
\overline{\left(  M_{-\lambda}^{-}\right)  ^{-\dag}}  &  \rightarrow
M_{\lambda}^{+},\\
x\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda
}^{-}  &  \mapsto U\left(  -\dag\right)  \left(  x\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}%
\end{align*}
of modules over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$%
.\ \ \ \ \footnote{Here are some details on the definition of this
isomorphism:
\par
As $\mathbb{R}$-vector spaces, $\overline{\left(  M_{-\lambda}^{-}\right)
^{-\dag}}=M_{-\lambda}^{-}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{-\lambda}$ and
$M_{\lambda}^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{\lambda}$. Hence, we
can define an $\mathbb{R}$-linear map $\overline{\left(  M_{-\lambda}%
^{-}\right)  ^{-\dag}}\rightarrow M_{\lambda}^{+}$ that sends $x\otimes
_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda}^{-}$ to
$U\left(  -\dag\right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}$ for every $x\in
U\left(  \mathfrak{g}\right)  $ and $z\in\mathbb{C}$ if we are able to show
that
\[
U\left(  -\dag\right)  \left(  xw\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}=U\left(
-\dag\right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  }\overline{wz}v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for all }x\in U\left(  \mathfrak{g}\right)  \text{,
}w\in U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \text{ and }%
z\in\mathbb{C}.
\]
But showing this is rather easy (left to the reader), and thus we get an
$\mathbb{R}$-linear map $\overline{\left(  M_{-\lambda}^{-}\right)  ^{-\dag}%
}\rightarrow M_{\lambda}^{+}$ that sends $x\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda}^{-}$ to $U\left(  -\dag\right)
\left(  x\right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
}\overline{z}v_{\lambda}^{+}$ for every $x\in U\left(  \mathfrak{g}\right)  $
and $z\in\mathbb{C}$. This map is easily seen to be $\mathfrak{g}$-linear and
$\mathbb{C}$-linear, so it is a homomorphism of modules over $\mathbb{C}$-Lie
algebra $\mathfrak{g}$. Showing that it is an isomorphism is easy as well (one
just has to construct its inverse).} Hence, $M_{-\lambda}^{-}\cong%
\overline{\left(  M_{\lambda}^{+}\right)  ^{-\dag}}$.

Hence, our bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ can be viewed as a bilinear form $M_{\lambda}^{+}%
\times\overline{M_{\lambda}^{+}}\rightarrow\mathbb{C}$, id est, as a
sesquilinear form $M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}%
$. This sesquilinear form is the unique sesquilinear Hermitian form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$\ \ \ \ \footnote{This can
be easily derived from Proposition \ref{prop.invform} \textbf{(a)}, which
claims that our form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ is the unique $\mathfrak{g}$-invariant
bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$
satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =1$.}. As a
consequence, this sesquilinear form can be easily seen to be Hermitian
symmetric, i. e., to satisfy%
\[
\left(  v,w\right)  =\overline{\left(  w,v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for all }v\in M_{\lambda}^{+}\text{ and }w\in
M_{\lambda}^{+}.
\]
\footnote{In fact, the form which sends $v\times w$ to $\overline{\left(
w,v\right)  }$ is also a sesquilinear Hermitian form $M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{\lambda}^{+}\right)  =1$, so that by uniqueness, it must be identical
with the form which sends $v\times w$ to $\left(  v,w\right)  $.}

However, this form can be degenerate. Its kernel is $J_{\lambda}^{+}$, so it
descends to a nondegenerate Hermitian form on $L_{\lambda}^{+}$. Thus, we get:

\begin{proposition}
\label{prop.hermitian.lambdareal}If $\lambda$ is real (this means that
$\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$), then $L_{\lambda}^{+}$ carries
a $\dag$-invariant nondegenerate Hermitian form. Different degrees in
$L_{\lambda}^{+}$ are orthogonal with respect to this form.
\end{proposition}

A reasonable (and, in most cases, difficult and interesting) question to ask
is the following: For which $\lambda$ is $L_{\lambda}^{+}$ unitary?

We are going to address this question in some cases and give hints in some
others, leaving many more unanswered.

First, let us give several examples of complex Lie algebras $\mathfrak{g}$
with antilinear $\mathbb{R}$-antiinvolutions $\dag:\mathfrak{g}\rightarrow
\mathfrak{g}$:

\begin{proposition}
We can define an antilinear map $\dag:\mathcal{A}\rightarrow\mathcal{A}$ by
$K^{\dag}=K$ and\ $a_{i}^{\dag}=a_{-i}$ for all $i\in\mathbb{Z}$. This map is
an antilinear $\mathbb{R}$-antiinvolution of the Heisenberg algebra
$\mathcal{A}$.
\end{proposition}

\begin{proposition}
One can define an antilinear map $\dag:\mathfrak{sl}_{2}\rightarrow
\mathfrak{sl}_{2}$ by$\ e^{\dag}=f,\ f^{\dag}=e,\ h^{\dag}=h$. This map is an
antilinear $\mathbb{R}$-antiinvolution of the Lie algebra $\mathfrak{sl}_{2}$.
\end{proposition}

More generally:

\begin{proposition}
Let $\mathfrak{g}$ be a simple finite-dimensional Lie algebra. Using the
Chevalley generators $e_{1}$, $e_{2}$, $...$, $e_{m}$, $f_{1}$, $f_{2}$,
$...$, $f_{m}$, $h_{1}$, $h_{2}$, $...$, $h_{m}$ of Proposition
\ref{prop.grad.g}, we can define an antilinear map $\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ by $e_{i}^{\dag}=f_{i},$ $f_{i}^{\dag}=e_{i}%
,\ h_{i}^{\dag}=h_{i}$ for all $i\in\left\{  1,2,...,m\right\}  $. This map is
an antilinear $\mathbb{R}$-antiinvolution of the Lie algebra $\mathfrak{g}$.
\end{proposition}

\begin{proposition}
We can define an antilinear map $\dag:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ by $L_{i}^{\dag}=L_{-i}$ for all $i\in\mathbb{Z}$, and
$C^{\dag}=C$. This map is an antilinear $\mathbb{R}$-antiinvolution of the
Virasoro algebra $\operatorname*{Vir}$.
\end{proposition}

\begin{proposition}
If $\mathfrak{g}$ is a Lie algebra with an antilinear $\mathbb{R}%
$-antiinvolution $\dag:\mathfrak{g}\rightarrow\mathfrak{g}$ and with a
symmetric $\mathfrak{g}$-invariant bilinear form $\left(  \cdot,\cdot\right)
$ of degree $0$, then we can define an antilinear map $\dag
:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$ (where
$\widehat{\mathfrak{g}}$ is the Lie algebra defined in Definition
\ref{def.loop}) by $\left(  at^{n}\right)  ^{\dag}=a^{\dag}\cdot t^{-n}$ for
every $a\in\mathfrak{g}$ and $n\in\mathbb{Z}$, and by $K^{\dag}=K$ (where $K$
denotes the element $\left(  0,1\right)  $ of $\mathfrak{g}\left[
t,t^{-1}\right]  \oplus\mathbb{C}=\widehat{\mathfrak{g}}$). This map $\dag$ is
an antilinear involution of the Lie algebra $\widehat{\mathfrak{g}}$.
\end{proposition}

As for examples of Hermitian modules: The $\operatorname*{Vir}$-module
$L_{h,c}^{+}$ (see Example \ref{exa.Vir} for the definition of this module)
for $h,c\in\mathbb{R}$ has a $\dag$-invariant nondegenerate Hermitian form.
(This is because the requirement $h,c\in\mathbb{R}$ forces the form
$\lambda\in\mathfrak{g}_{0}^{\ast}$ which corresponds to the pair $\left(
h,c\right)  $ to lie in $\mathfrak{g}_{0\mathbb{R}}^{\ast}$, and thus we can
apply Proposition \ref{prop.hermitian.lambdareal}.)

But now, back to the general case:

\begin{proposition}
\label{prop.unitrick}Let $V$ be a unitary representation in Category
$\mathcal{O}^{+}$. Then, $V$ is completely reducible (i. e., the
representation $V$ is a direct sum of irreducible representations).
\end{proposition}

To prove this, we will use a lemma:

\begin{lemma}
\label{lem.unitrick}If $V$ is a highest-weight representation, and $V$ has a
nondegenerate $\dag$-invariant Hermitian form, then $V$ is irreducible. (We
recall that a ``highest-weight representation'' means a quotient of
$M_{\lambda}^{+}$ by a proper graded submodule for some $\lambda$.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.unitrick}.} Let $V$ be a highest-weight
representation having a nondegenerate $\dag$-invariant Hermitian form. Since
$V$ is a highest-weight representation, $V$ is a quotient of $M_{\lambda}^{+}$
by a proper graded submodule $P$ for some $\lambda$. The nondegenerate $\dag
$-invariant Hermitian form on $V$ thus induces a $\dag$-invariant Hermitian
form on $M_{\lambda}^{+}$ whose kernel is $P$. It is easy to see that
$\lambda$ is real. Thus, this $\dag$-invariant Hermitian form on $M_{\lambda
}^{+}$ can be rewritten as a $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$, which still has
kernel $P$. Such a form is unique up to scaling (by Proposition
\ref{prop.invform} \textbf{(c)}), and thus must be the form defined in
Proposition \ref{prop.invform} \textbf{(a)}. But the kernel of this form is
$J_{\lambda}^{+}$. Thus, the kernel of this form is, at the same time, $P$ and
$J_{\lambda}^{+}$. Hence, $P=J_{\lambda}^{+}$, so that $V=L_{\lambda}^{+}$
(since $V$ is the quotient of $M_{\lambda}^{+}$ by $P$), and thus $V$ is
irreducible. Lemma \ref{lem.unitrick} is proven.

\textit{Proof of Proposition \ref{prop.unitrick}.} Take a nonzero homogeneous
vector $v\in V$ of maximal degree. (``Maximal'' means ``maximal in real
part''. Such a maximal degree exists by the definition of Category
$\mathcal{O}^{+}$.) Let $v$ be an eigenvector of $\mathfrak{g}_{0}$ with
eigenvalue $\lambda$. Consider the submodule of $V$ generated by $v$. This
submodule is highest-weight (since $\mathfrak{g}_{j}v=0$ for $j>0$). Hence, by
Lemma \ref{lem.unitrick}, this submodule is irreducible and therefore $\cong
L_{\lambda_{1}}^{+}$ for some $\lambda_{1}\in\mathfrak{h}^{\ast}$. Let $V_{1}$
be the orthogonal complement of $L_{\lambda_{1}}^{+}$. Then, $V=L_{\lambda
_{1}}^{+}\oplus V_{1}$. Now take a vector in $V_{1}$, and so on. Since the
degrees of $V$ lie in finitely many arithmetic progressions, and homogeneous
subspaces have finite dimension, this process is exhaustive, so we obtain
$V=L_{\lambda_{1}}^{+}\oplus L_{\lambda_{2}}^{+}\oplus...$.

\begin{remark}
In this decomposition, every irreducible object of Category $\mathcal{O}^{+}$
occurs finitely many times.
\end{remark}

\section{Representation theory: concrete examples}

\subsection{Some lemmata about exponentials and commutators}

This section is devoted to some elementary lemmata about power series and
iterated commutators over noncommutative rings. These lemmata are well-known
in geometrical contexts (in these contexts they tend to appear in Lie groups
textbooks), but here we will formulate and prove them purely algebraically. We
will not use these lemmata until Theorem \ref{thm.euler}, but I prefer to put
them here in order not to interrupt the flow of representation-theoretical
arguments later.

We start with easy things:

\begin{lemma}
\label{lem.powerseries1}Let $K$ be a commutative ring. If $\alpha$ and $\beta$
are two elements of a topological $K$-algebra $R$ such that $\left[
\alpha,\beta\right]  $ commutes with $\beta$, then $\left[  \alpha,P\left(
\beta\right)  \right]  =\left[  \alpha,\beta\right]  \cdot P^{\prime}\left(
\beta\right)  $ for every power series $P\in K\left[  \left[  X\right]
\right]  $ for which the series $P\left(  \beta\right)  $ and $P^{\prime
}\left(  \beta\right)  $ converge.
\end{lemma}

\textit{Proof of Lemma \ref{lem.powerseries1}.} Let $\gamma=\left[
\alpha,\beta\right]  $. Then, $\gamma$ commutes with $\beta$ (since we know
that $\left[  \alpha,\beta\right]  $ commutes with $\beta$), so that
$\gamma\beta=\beta\gamma$.

Write $P$ in the form $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ for some
$\left(  u_{0},u_{1},u_{2},...\right)  \in K^{\mathbb{N}}$. Then, $P^{\prime
}=\sum\limits_{i=1}^{\infty}iu_{i}X^{i-1}$, so that $P^{\prime}\left(
\beta\right)  =\sum\limits_{i=1}^{\infty}iu_{i}\beta^{i-1}$. On the other
hand, $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ shows that $P\left(
\beta\right)  =\sum\limits_{i=0}^{\infty}u_{i}\beta^{i}$ and thus
\[
\left[  \alpha,P\left(  \beta\right)  \right]  =\left[  \alpha,\sum
\limits_{i=0}^{\infty}u_{i}\beta^{i}\right]  =\sum\limits_{i=0}^{\infty}%
u_{i}\left[  \alpha,\beta^{i}\right]  =u_{0}\underbrace{\left[  \alpha
,\beta^{0}\right]  }_{\substack{=0\\\text{(since }\beta^{0}=1\in Z\left(
R\right)  \text{)}}}+\sum\limits_{i=1}^{\infty}u_{i}\left[  \alpha,\beta
^{i}\right]  =\sum\limits_{i=1}^{\infty}u_{i}\left[  \alpha,\beta^{i}\right]
.
\]


Now, it is easy to prove that every positive $i\in\mathbb{N}$ satisfies
$\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$%
\ \ \ \ \footnote{\textit{Proof.} We will prove this by induction over $i$:
\par
\textit{Induction base:} For $i=1$, we have $\left[  \alpha,\beta^{i}\right]
=\left[  \alpha,\beta^{1}\right]  =\left[  \alpha,\beta\right]  =\gamma$ and
$\underbrace{i}_{=1}\gamma\underbrace{\beta^{i-1}}_{=\beta^{1-1}=1}=\gamma$,
so that $\left[  \alpha,\beta^{i}\right]  =\gamma=i\gamma\beta^{i-1}$. This
proves $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for $i=1$, and
thus the induction base is complete.
\par
\textit{Induction step:} Let $j\in\mathbb{N}$ be positive. Assume that
$\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ is proven for $i=j$. We
must then prove $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for
$i=j+1$.
\par
Since $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ is proven for
$i=j$, we have $\left[  \alpha,\beta^{j}\right]  =j\gamma\beta^{j-1}$.
\par
Now,
\begin{align*}
\left[  \alpha,\underbrace{\beta^{j+1}}_{=\beta\beta^{j}}\right]   &  =\left[
\alpha,\beta\beta^{j}\right]  =\alpha\beta\beta^{j}-\beta\beta^{j}%
\alpha=\underbrace{\left(  \alpha\beta\beta^{j}-\beta\alpha\beta^{j}\right)
}_{=\left(  \alpha\beta-\beta\alpha\right)  \beta^{j}}+\underbrace{\left(
\beta\alpha\beta^{j}-\beta\beta^{j}\alpha\right)  }_{=\beta\left(  \alpha
\beta^{j}-\beta^{j}\alpha\right)  }\\
&  =\underbrace{\left(  \alpha\beta-\beta\alpha\right)  }_{=\left[
\alpha,\beta\right]  =\gamma}\beta^{j}+\beta\underbrace{\left(  \alpha
\beta^{j}-\beta^{j}\alpha\right)  }_{=\left[  \alpha,\beta^{j}\right]
=j\gamma\beta^{j-1}}=\gamma\beta^{j}+\beta j\gamma\beta^{j-1}=\gamma\beta
^{j}+j\underbrace{\beta\gamma}_{=\gamma\beta}\beta^{j-1}\\
&  =\gamma\beta^{j}+j\gamma\underbrace{\beta\beta^{j-1}}_{=\beta^{j}}%
=\gamma\beta^{j}+j\gamma\beta^{j}=\left(  j+1\right)  \gamma\beta^{j}=\left(
j+1\right)  \gamma\beta^{\left(  j+1\right)  -1}.
\end{align*}
In other words, $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ holds
for $i=j+1$. This completes the induction step, and thus by induction we have
proven that $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for every
positive $i\in\mathbb{N}$.}. Hence,%
\[
\left[  \alpha,P\left(  \beta\right)  \right]  =\sum\limits_{i=1}^{\infty
}u_{i}\underbrace{\left[  \alpha,\beta^{i}\right]  }_{=i\gamma\beta^{i-1}%
}=\sum\limits_{i=1}^{\infty}u_{i}i\gamma\beta^{i-1}=\underbrace{\gamma
}_{=\left[  \alpha,\beta\right]  }\underbrace{\sum\limits_{i=1}^{\infty}%
iu_{i}\beta^{i-1}}_{=P^{\prime}\left(  \beta\right)  }=\left[  \alpha
,\beta\right]  \cdot P^{\prime}\left(  \beta\right)  .
\]
Lemma \ref{lem.powerseries1} is proven.

\begin{corollary}
\label{cor.powerseries2}If $\alpha$ and $\beta$ are two elements of a
topological $\mathbb{Q}$-algebra $R$ such that $\left[  \alpha,\beta\right]  $
commutes with $\beta$, then $\left[  \alpha,\exp\beta\right]  =\left[
\alpha,\beta\right]  \cdot\exp\beta$ whenever the power series $\exp\beta$ converges.
\end{corollary}

\textit{Proof of Corollary \ref{cor.powerseries2}.} Applying Lemma
\ref{lem.powerseries1} to $P=\exp X$ and $K=\mathbb{Q}$, and recalling that
$\exp^{\prime}=\exp$, we obtain $\left[  \alpha,\exp\beta\right]  =\left[
\alpha,\beta\right]  \cdot\exp\beta$. This proves Corollary
\ref{cor.powerseries2}.

In Lemma \ref{lem.powerseries1} and Corollary \ref{cor.powerseries2}, we had
to require convergence of certain power series in order for the results to
make sense. In the following, we will prove some results for which such
requirements are not sufficient anymore\footnote{At least they are not
sufficient for my proofs...}; instead we need more global conditions. A
standard condition to require in such cases is that all the elements to which
we apply power series lie in some ideal $I$ of $R$ such that $R$ is complete
and Hausdorff with respect to the $I$-adic topology. Under this condition,
things work nicely, due to the following fact (which is one part of the
universal property of the power series ring $K\left[  \left[  X\right]
\right]  $):

\begin{proposition}
\label{prop.K[[X]].univ}Let $K$ be a commutative ring. Let $R$ be a
$K$-algebra, and $I$ be an ideal of $R$ such that $R$ is complete and
Hausdorff with respect to the $I$-adic topology. Then, for every power series
$P\in K\left[  \left[  X\right]  \right]  $ and every $\alpha\in I$, there is
a well-defined element $P\left(  \alpha\right)  \in R$ (which is defined as
the limit $\lim\limits_{n\rightarrow\infty}\sum\limits_{i=0}^{n}u_{i}%
\alpha^{i}$ (with respect to the $I$-adic topology), where the power series
$P$ is written in the form $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ for some
$\left(  u_{0},u_{1},u_{2},...\right)  \in K^{\mathbb{N}}$). For every
$\alpha\in I$, the map $K\left[  \left[  X\right]  \right]  \rightarrow R$
which sends every $P\in K\left[  \left[  X\right]  \right]  $ to $P\left(
\alpha\right)  $ is a continuous $K$-algebra homomorphism (where the topology
on $K\left[  \left[  X\right]  \right]  $ is the standard one, and the
topology on $R$ is the $I$-adic one).
\end{proposition}

\begin{theorem}
\label{thm.exp(u+v)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an ideal
of $R$ such that $R$ is complete and Hausdorff with respect to the $I$-adic
topology. Let $\alpha\in I$ and $\beta\in I$ be such that $\alpha\beta
=\beta\alpha$. Then, $\exp\alpha$, $\exp\beta$ and $\exp\left(  \alpha
+\beta\right)  $ are well-defined (by Proposition \ref{prop.K[[X]].univ}) and
satisfy $\exp\left(  \alpha+\beta\right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  $.
\end{theorem}

\textit{Proof of Theorem \ref{thm.exp(u+v)}.} We know that $\alpha\beta
=\beta\alpha$. That is, $\alpha$ and $\beta$ commute, so that we can apply the
binomial formula to $\alpha$ and $\beta$.

Comparing%
\[
\exp\left(  \alpha+\beta\right)  =\sum\limits_{n=0}^{\infty}\dfrac{\left(
\alpha+\beta\right)  ^{n}}{n!}=\sum\limits_{n=0}^{\infty}\dfrac{1}%
{n!}\underbrace{\left(  \alpha+\beta\right)  ^{n}}_{\substack{=\sum
\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i}\\\text{(by the binomial
formula,}\\\text{since }\alpha\text{ and }\beta\text{ commute)}}%
}=\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}%
{i}\alpha^{i}\beta^{n-i}%
\]
with%
\begin{align*}
\underbrace{\left(  \exp\alpha\right)  }_{=\sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}}\cdot\underbrace{\left(  \exp\beta\right)  }%
_{=\sum\limits_{j=0}^{\infty}\dfrac{\beta^{j}}{j!}}  &  =\left(
\sum\limits_{i=0}^{\infty}\dfrac{\alpha^{i}}{i!}\right)  \cdot\left(
\sum\limits_{j=0}^{\infty}\dfrac{\beta^{j}}{j!}\right)  =\sum\limits_{i=0}%
^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{\alpha^{i}\beta^{j}}{i!j!}%
=\sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{1}{i!j!}\alpha
^{i}\beta^{j}\\
&  =\underbrace{\sum\limits_{i=0}^{\infty}\sum\limits_{n=i}^{\infty}}%
_{=\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}}\underbrace{\dfrac
{1}{i!\left(  n-i\right)  !}}_{\substack{=\dfrac{1}{n!}\dbinom{n}%
{i}\\\text{(since }\dbinom{n}{i}=\dfrac{n!}{i!\left(  n-i\right)  !}\text{)}%
}}\alpha^{i}\beta^{n-i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n\text{ for
}i+j\text{ in the second sum}\right) \\
&  =\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}\dfrac{1}{n!}\dbinom{n}%
{i}\alpha^{i}\beta^{n-i}=\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}%
\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i},
\end{align*}
we obtain $\exp\left(  \alpha+\beta\right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  $. This proves Theorem \ref{thm.exp(u+v)}.

\begin{corollary}
\label{cor.exp(-w)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an ideal
of $R$ such that $R$ is complete and Hausdorff with respect to the $I$-adic
topology. Let $\gamma\in I$. Then, $\exp\gamma$ and $\exp\left(
-\gamma\right)  $ are well-defined (by Proposition \ref{prop.K[[X]].univ}) and
satisfy $\left(  \exp\gamma\right)  \cdot\left(  \exp\left(  -\gamma\right)
\right)  =1$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.exp(-w)}.} By Theorem \ref{thm.exp(u+v)}
(applied to $\alpha=\gamma$ and $\beta=-\gamma$), we have $\exp\left(
\gamma+\left(  -\gamma\right)  \right)  =\left(  \exp\gamma\right)
\cdot\left(  \exp\left(  -\gamma\right)  \right)  $, thus%
\[
\left(  \exp\gamma\right)  \cdot\left(  \exp\left(  -\gamma\right)  \right)
=\exp\underbrace{\left(  \gamma+\left(  -\gamma\right)  \right)  }_{=0}%
=\exp0=1.
\]
This proves Corollary \ref{cor.exp(-w)}.

\begin{theorem}
\label{thm.exp(a)bexp(-a)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an
ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$. Denote by $\operatorname*{ad}\alpha$ the
map $R\rightarrow R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[
\alpha,x\right]  $ denotes the commutator $\alpha x-x\alpha$).

\textbf{(a)} Then, the infinite series $\sum\limits_{n=0}^{\infty}%
\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}$ converges pointwise
(i. e., for every $x\in R$, the infinite series $\sum\limits_{n=0}^{\infty
}\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}\left(  x\right)  $
converges). Denote the value of this series by $\exp\left(  \operatorname*{ad}%
\alpha\right)  $.

\textbf{(b)} We have $\left(  \exp\alpha\right)  \cdot\beta\cdot\left(
\exp\left(  -\alpha\right)  \right)  =\left(  \exp\left(  \operatorname*{ad}%
\alpha\right)  \right)  \left(  \beta\right)  $ for every $\beta\in R$.
\end{theorem}

To prove this, we will use a lemma:

\begin{lemma}
\label{lem.exp(a)bexp(-a)}Let $R$ be a ring. Let $\alpha$ and $\beta$ be
elements of $R$. Denote by $\operatorname*{ad}\alpha$ the map $R\rightarrow
R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[  \alpha,x\right]  $
denotes the commutator $\alpha x-x\alpha$). Let $n\in\mathbb{N}$. Then,
\[
\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)
=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(  -\alpha\right)
^{n-i}.
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.exp(a)bexp(-a)}.} Let $L_{\alpha}$ denote the
map $R\rightarrow R,$ $x\mapsto\alpha x$. Let $R_{\alpha}$ denote the map
$R\rightarrow R,\ x\mapsto x\alpha$. Then, every $x\in R$ satisfies%
\[
\left(  L_{\alpha}-R_{\alpha}\right)  \left(  x\right)  =\underbrace{L_{\alpha
}\left(  x\right)  }_{\substack{=\alpha x\\\text{(by the definition of
}L_{\alpha}\text{)}}}-\underbrace{R_{\alpha}\left(  x\right)  }%
_{\substack{=x\alpha\\\text{(by the definition of }R_{\alpha}\text{)}}}=\alpha
x-x\alpha=\left[  \alpha,x\right]  =\left(  \operatorname*{ad}\alpha\right)
\left(  x\right)  .
\]
Hence, $L_{\alpha}-R_{\alpha}=\operatorname*{ad}\alpha$.

Also, every $x\in R$ satisfies%
\[
\left(  L_{\alpha}\circ R_{\alpha}\right)  \left(  x\right)  =L_{\alpha
}\underbrace{\left(  R_{\alpha}\left(  x\right)  \right)  }%
_{\substack{=x\alpha\\\text{(by the definition of }R_{\alpha}\text{)}%
}}=L_{\alpha}\left(  x\alpha\right)  =\alpha x\alpha
\]
(by the definition of $L_{\alpha}$) and%
\[
\left(  R_{\alpha}\circ L_{\alpha}\right)  \left(  x\right)  =R_{\alpha
}\underbrace{\left(  L_{\alpha}\left(  x\right)  \right)  }_{\substack{=\alpha
x\\\text{(by the definition of }L_{\alpha}\text{)}}}=R_{\alpha}\left(  \alpha
x\right)  =\alpha x\alpha
\]
(by the definition of $R_{\alpha}$), so that $\left(  L_{\alpha}\circ
R_{\alpha}\right)  \left(  x\right)  =\left(  R_{\alpha}\circ L_{\alpha
}\right)  \left(  x\right)  $. Hence, $L_{\alpha}\circ R_{\alpha}=R_{\alpha
}\circ L_{\alpha}$. In other words, the maps $L_{\alpha}$ and $R_{\alpha}$
commute. Thus, we can apply the binomial formula to $L_{\alpha}$ and
$R_{\alpha}$, and conclude that $\left(  L_{\alpha}-R_{\alpha}\right)
^{n}=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}%
^{i}\circ R_{\alpha}^{n-i}$. Since $L_{\alpha}-R_{\alpha}=\operatorname*{ad}%
\alpha$, this rewrites as $\left(  \operatorname*{ad}\alpha\right)  ^{n}%
=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}%
^{i}\circ R_{\alpha}^{n-i}$.

Now, it is easy to see (by induction over $j$) that
\begin{equation}
L_{\alpha}^{j}y=\alpha^{j}y\ \ \ \ \ \ \ \ \ \ \text{for every }j\in
\mathbb{N}\text{ and }y\in R. \label{pf.exp(a)bexp(-a).1}%
\end{equation}
Also, it is easy to see (by induction over $j$) that
\begin{equation}
R_{\alpha}^{j}y=y\alpha^{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in
\mathbb{N}\text{ and }y\in R. \label{pf.exp(a)bexp(-a).2}%
\end{equation}


Now, since $\left(  \operatorname*{ad}\alpha\right)  ^{n}=\sum\limits_{i=0}%
^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}^{i}\circ R_{\alpha}%
^{n-i}$, we have%
\begin{align*}
\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)   &
=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}%
\underbrace{\left(  L_{\alpha}^{i}\circ R_{\alpha}^{n-i}\right)  \left(
\beta\right)  }_{\substack{=L_{\alpha}^{i}\left(  R_{\alpha}^{n-i}%
\beta\right)  =\alpha^{i}R_{\alpha}^{n-i}\beta\\\text{(by
(\ref{pf.exp(a)bexp(-a).1}), applied to }j=i\text{ and }y=R_{\alpha}%
^{n-i}\beta\text{)}}}\\
&  =\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}\alpha
^{i}\underbrace{R_{\alpha}^{n-i}\beta}_{\substack{=\beta\alpha^{n-i}%
\\\text{(by (\ref{pf.exp(a)bexp(-a).2}), applied to }j=n-i\text{ and }%
y=\beta\text{)}}}\\
&  =\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}\alpha^{i}%
\beta\alpha^{n-i}=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(
-\alpha\right)  ^{n-i}.
\end{align*}
This proves Lemma \ref{lem.exp(a)bexp(-a)}.

\textit{Proof of Theorem \ref{thm.exp(a)bexp(-a)}.} \textbf{(a)} For every
$x\in R$ and every $n\in\mathbb{N}$, we have $\left(  \operatorname*{ad}%
\alpha\right)  ^{n}\left(  x\right)  \in I^{n}$ (this can be easily proven by
induction over $n$, using the fact that $I$ is an ideal) and thus
$\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}\left(  x\right)
=\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
x\right)  }_{\in I^{n}}\in I^{n}$. Hence, for every $x\in R$, the infinite
series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  x\right)  $ converges (because $R$ is complete
and Hausdorff with respect to the $I$-adic topology). In other words, the
infinite series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}$ converges pointwise. Theorem
\ref{thm.exp(a)bexp(-a)} \textbf{(a)} is proven.

\textbf{(b)} Let $\beta\in R$. By the definition of of $\exp\left(
\operatorname*{ad}\alpha\right)  $, we have%
\begin{align*}
\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)   &  =\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  \beta\right)  =\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
\beta\right)  }_{\substack{=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}%
\beta\left(  -\alpha\right)  ^{n-i}\\\text{(by Lemma \ref{lem.exp(a)bexp(-a)}%
)}}}\\
&  =\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}%
{i}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}.
\end{align*}
Compared with%
\begin{align*}
\underbrace{\left(  \exp\alpha\right)  }_{=\sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}}\cdot\beta\cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  }_{=\sum\limits_{j=0}^{\infty}\dfrac{\left(
-\alpha\right)  ^{j}}{j!}}  &  =\left(  \sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}\right)  \cdot\beta\cdot\left(  \sum\limits_{j=0}%
^{\infty}\dfrac{\left(  -\alpha\right)  ^{j}}{j!}\right) \\
&  =\sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{\alpha^{i}%
\beta\left(  -\alpha\right)  ^{j}}{i!j!}=\sum\limits_{i=0}^{\infty}%
\sum\limits_{j=0}^{\infty}\dfrac{1}{i!j!}\alpha^{i}\beta\left(  -\alpha
\right)  ^{j}\\
&  =\underbrace{\sum\limits_{i=0}^{\infty}\sum\limits_{n=i}^{\infty}}%
_{=\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}}\underbrace{\dfrac
{1}{i!\left(  n-i\right)  !}}_{\substack{=\dfrac{1}{n!}\dbinom{n}%
{i}\\\text{(since }\dbinom{n}{i}=\dfrac{n!}{i!\left(  n-i\right)  !}\text{)}%
}}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n\text{ for
}i+j\text{ in the second sum}\right) \\
&  =\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}\dfrac{1}{n!}\dbinom{n}%
{i}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}=\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(
-\alpha\right)  ^{n-i},
\end{align*}
this yields $\left(  \exp\alpha\right)  \cdot\beta\cdot\left(  \exp\left(
-\alpha\right)  \right)  =\left(  \exp\left(  \operatorname*{ad}\alpha\right)
\right)  \left(  \beta\right)  $. This proves Theorem \ref{thm.exp(a)bexp(-a)}
\textbf{(b)}.

\begin{corollary}
\label{cor.exp(a)exp(b)exp(-a)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$
be an ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$. Denote by $\operatorname*{ad}\alpha$ the
map $R\rightarrow R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[
\alpha,x\right]  $ denotes the commutator $\alpha x-x\alpha$).

As we know from Theorem \ref{thm.exp(a)bexp(-a)} \textbf{(a)}, the infinite
series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}$ converges pointwise. Denote the value of this series
by $\exp\left(  \operatorname*{ad}\alpha\right)  $.

We have $\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  =\exp\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  $ for every $\beta\in I$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.exp(a)exp(b)exp(-a)}.} Corollary
\ref{cor.exp(-w)} (applied to $\gamma=-\alpha$) yields $\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\left(  -\left(  -\alpha\right)
\right)  \right)  =1$. Since $-\left(  -\alpha\right)  =\alpha$, this rewrites
as $\left(  \exp\left(  -\alpha\right)  \right)  \cdot\left(  \exp
\alpha\right)  =1$.

Let $\beta\in I$. Let $T$ denote the map $R\rightarrow R,\ x\mapsto\left(
\exp\alpha\right)  \cdot x\cdot\left(  \exp\left(  -\alpha\right)  \right)  $.
Clearly, this map $T$ is $\mathbb{Q}$-linear. It also satisfies%
\begin{align*}
T\left(  1\right)   &  =\left(  \exp\alpha\right)  \cdot1\cdot\left(
\exp\left(  -\alpha\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }T\right) \\
&  =\left(  \exp\alpha\right)  \cdot\left(  \exp\left(  -\alpha\right)
\right)  =1,
\end{align*}
and any $x\in R$ and $y\in R$ satisfy%
\begin{align*}
\underbrace{T\left(  x\right)  }_{\substack{=\left(  \exp\alpha\right)  \cdot
x\cdot\left(  \exp\left(  -\alpha\right)  \right)  \\\left(  \text{by the
definition of }T\right)  }}\cdot\underbrace{T\left(  y\right)  }%
_{\substack{=\left(  \exp\alpha\right)  \cdot y\cdot\left(  \exp\left(
-\alpha\right)  \right)  \\\left(  \text{by the definition of }T\right)  }}
&  =\left(  \exp\alpha\right)  \cdot x\cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\alpha\right)  }_{=1}\cdot
y\cdot\left(  \exp\left(  -\alpha\right)  \right) \\
&  =\left(  \exp\alpha\right)  \cdot xy\cdot\left(  \exp\left(  -\alpha
\right)  \right)  =T\left(  xy\right)
\end{align*}
(since $T\left(  xy\right)  =\left(  \exp\alpha\right)  \cdot xy\cdot\left(
\exp\left(  -\alpha\right)  \right)  $ by the definition of $T$). Hence, $T$
is a $\mathbb{Q}$-algebra homomorphism. Also, $T$ is continuous (with respect
to the $I$-adic topology). Thus, $T$ is a continuous $\mathbb{Q}$-algebra
homomorphism, and hence commutes with the application of power series. Thus,
$T\left(  \exp\beta\right)  =\exp\left(  T\left(  \beta\right)  \right)  $.
But since $T\left(  \exp\beta\right)  =\left(  \exp\alpha\right)  \cdot\left(
\exp\beta\right)  \cdot\left(  \exp\left(  -\alpha\right)  \right)  $ (by the
definition of $T$) and%
\begin{align*}
T\left(  \beta\right)   &  =\left(  \exp\alpha\right)  \cdot\beta\cdot\left(
\exp\left(  -\alpha\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }T\right) \\
&  =\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem
\ref{thm.exp(a)bexp(-a)} \textbf{(b)}}\right)  ,
\end{align*}
this rewrites as $\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  =\exp\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  $. This proves Corollary \ref{cor.exp(a)exp(b)exp(-a)}.

\begin{lemma}
\label{lem.powerseries3}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an
ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$ and $\beta\in I$. Assume that $\left[
\alpha,\beta\right]  $ commutes with each of $\alpha$ and $\beta$. Then,
$\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  $.
\end{lemma}

First we give two short proofs of this lemma.

\textit{First proof of Lemma \ref{lem.powerseries3}.} Define the map
$\operatorname*{ad}\alpha$ as in Corollary \ref{cor.exp(a)exp(b)exp(-a)}.
Then, $\left(  \operatorname*{ad}\alpha\right)  ^{2}\left(  \beta\right)
=\left[  \alpha,\left[  \alpha,\beta\right]  \right]  =0$ (since $\left[
\alpha,\beta\right]  $ commutes with $\alpha$). Hence, $\left(
\operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)  =0$ for every
integer $n\geq2$. Now, by the definition of $\exp\left(  \operatorname*{ad}%
\alpha\right)  $, we have%
\begin{align*}
\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)   &  =\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  \beta\right)  =\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
\beta\right) \\
&  =\underbrace{\dfrac{1}{0!}}_{=1}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{0}}_{=\operatorname*{id}}\left(  \beta\right)
+\underbrace{\dfrac{1}{1!}}_{=1}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{1}}_{=\operatorname*{ad}\alpha}\left(  \beta\right)
+\sum\limits_{n=2}^{\infty}\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}\left(  \beta\right)  }_{\substack{=0\\\text{(since }%
n\geq2\text{)}}}\\
&  =\underbrace{\operatorname*{id}\left(  \beta\right)  }_{=\beta
}+\underbrace{\left(  \operatorname*{ad}\alpha\right)  \left(  \beta\right)
}_{=\left[  \alpha,\beta\right]  }+\underbrace{\sum\limits_{n=2}^{\infty
}\dfrac{1}{n!}0}_{=0}=\beta+\left[  \alpha,\beta\right]  .
\end{align*}
By Corollary \ref{cor.exp(a)exp(b)exp(-a)}, we now have%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \cdot\left(
\exp\left(  -\alpha\right)  \right)  =\exp\underbrace{\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  }_{=\beta+\left[  \alpha,\beta\right]  }=\exp\left(  \beta+\left[
\alpha,\beta\right]  \right)  .
\]
But $\beta$ and $\left[  \alpha,\beta\right]  $ commute, so that $\beta\left[
\alpha,\beta\right]  =\left[  \alpha,\beta\right]  \beta$. Hence, Theorem
\ref{thm.exp(u+v)} (applied to $\beta$ and $\left[  \alpha,\beta\right]  $
instead of $\alpha$ and $\beta$) yields $\exp\left(  \beta+\left[
\alpha,\beta\right]  \right)  =\left(  \exp\beta\right)  \cdot\left(
\exp\left[  \alpha,\beta\right]  \right)  $.

On the other hand,%
\begin{align*}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \cdot\left(
\exp\left(  -\alpha\right)  \right)  \cdot\left(  \exp\underbrace{\alpha
}_{=-\left(  -\alpha\right)  }\right)   &  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  \cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\left(  -\left(  -\alpha\right)
\right)  \right)  }_{\substack{=1\\\text{(by Corollary \ref{cor.exp(-w)},
applied to }\gamma=-\alpha\text{)}}}\\
&  =\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  .
\end{align*}
Compared with%
\[
\underbrace{\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  }_{=\exp\left(
\beta+\left[  \alpha,\beta\right]  \right)  =\left(  \exp\beta\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }\cdot\left(
\exp\alpha\right)  =\left(  \exp\beta\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  \cdot\left(  \exp\alpha\right)  ,
\]
this yields%
\begin{equation}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)
\cdot\left(  \exp\alpha\right)  . \label{pf.powerseries3.4}%
\end{equation}


Besides, $\alpha$ and $\left[  \alpha,\beta\right]  $ commute, so that
$\alpha\left[  \alpha,\beta\right]  =\left[  \alpha,\beta\right]  \alpha$.
Hence, Theorem \ref{thm.exp(u+v)} (applied to $\left[  \alpha,\beta\right]  $
instead of $\beta$) yields $\exp\left(  \alpha+\left[  \alpha,\beta\right]
\right)  =\left(  \exp\alpha\right)  \cdot\left(  \exp\left[  \alpha
,\beta\right]  \right)  $.

On the other hand, $\alpha$ and $\left[  \alpha,\beta\right]  $ commute, so
that $\left[  \alpha,\beta\right]  \alpha=\alpha\left[  \alpha,\beta\right]
$. Hence, Theorem \ref{thm.exp(u+v)} (applied to $\left[  \alpha,\beta\right]
$ and $\alpha$ instead of $\alpha$ and $\beta$) yields $\exp\left(  \left[
\alpha,\beta\right]  +\alpha\right)  =\left(  \exp\left[  \alpha,\beta\right]
\right)  \cdot\left(  \exp\alpha\right)  $.

Thus, $\left(  \exp\left[  \alpha,\beta\right]  \right)  \cdot\left(
\exp\alpha\right)  =\exp\underbrace{\left(  \left[  \alpha,\beta\right]
+\alpha\right)  }_{=\alpha+\left[  \alpha,\beta\right]  }=\exp\left(
\alpha+\left[  \alpha,\beta\right]  \right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $. Now,
(\ref{pf.powerseries3.4}) becomes%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\underbrace{\left(  \exp\left[  \alpha,\beta\right]
\right)  \cdot\left(  \exp\alpha\right)  }_{=\left(  \exp\alpha\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }=\left(  \exp
\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  .
\]
This proves Lemma \ref{lem.powerseries3}.

\textit{Second proof of Lemma \ref{lem.powerseries3}.} Clearly, $\left[
\beta,\alpha\right]  =-\left[  \alpha,\beta\right]  $ commutes with each of
$\alpha$ and $\beta$ (since $\left[  \alpha,\beta\right]  $ commutes with each
of $\alpha$ and $\beta$).

The Baker-Campbell-Hausdorff formula has the form%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\exp\left(
\alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  +\left(  \text{higher
terms}\right)  \right)  ,
\]
where the ``higher terms'' on the right hand side mean $\mathbb{Q}$-linear
combinations of nested Lie brackets of three or more $\alpha$'s and $\beta$'s.
Since $\left[  \alpha,\beta\right]  $ commutes with each of $\alpha$ and
$\beta$, all of these higher terms are zero, and thus the
Baker-Campbell-Hausdorff formula simplifies to%
\begin{equation}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\exp\left(
\alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  .
\label{pf.powerseries3.6}%
\end{equation}
Applying this to $\beta$ and $\alpha$ instead of $\alpha$ and $\beta$, we
obtain%
\[
\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  =\exp\left(
\beta+\alpha+\dfrac{1}{2}\left[  \beta,\alpha\right]  \right)  .
\]
Since $\left[  \beta,\alpha\right]  =-\left[  \alpha,\beta\right]  $, this
becomes%
\begin{equation}
\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  =\exp\left(
\beta+\alpha+\dfrac{1}{2}\underbrace{\left[  \beta,\alpha\right]  }_{=-\left[
\alpha,\beta\right]  }\right)  =\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[
\alpha,\beta\right]  \right)  . \label{pf.powerseries3.8}%
\end{equation}


Now, $\left[  \alpha,\beta\right]  $ commutes with each of $\alpha$ and
$\beta$ (by the assumptions of the lemma) and also with $\left[  \alpha
,\beta\right]  $ itself (clearly). Hence, $\left[  \alpha,\beta\right]  $
commutes with $\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  $. In
other words, $\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]
\right)  \left[  \alpha,\beta\right]  =\left[  \alpha,\beta\right]  \left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  $. Hence,
Theorem \ref{thm.exp(u+v)} (applied to $\beta+\alpha-\dfrac{1}{2}\left[
\alpha,\beta\right]  $ and $\left[  \alpha,\beta\right]  $ instead of $\alpha$
and $\beta$) yields $\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  +\left[  \alpha,\beta\right]  \right)  =\left(  \exp\left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  \right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $. Now,%
\begin{align*}
\underbrace{\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)
}_{\substack{=\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  \right)  \\\text{(by (\ref{pf.powerseries3.8}))}}}\cdot\left(
\exp\left[  \alpha,\beta\right]  \right)   &  =\left(  \exp\left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  \right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right) \\
&  =\exp\underbrace{\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  +\left[  \alpha,\beta\right]  \right)  }_{=\alpha+\beta
+\dfrac{1}{2}\left[  \alpha,\beta\right]  }\\
&  =\exp\left(  \alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)
=\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\end{align*}
(by (\ref{pf.powerseries3.6})). Lemma \ref{lem.powerseries3} is proven.

We are going to also present a third, very elementary (term-by-term) proof of
Lemma \ref{lem.powerseries3}. It relies on the following proposition, which
can also be applied in some other contexts (e. g., computing in universal
enveloping algebras):

\begin{proposition}
\label{prop.powerseries3.fin}Let $R$ be a ring. Let $\alpha\in R$ and
$\beta\in R$. Assume that $\left[  \alpha,\beta\right]  $ commutes with each
of $\alpha$ and $\beta$. Then, for every $i\in\mathbb{N}$ and $j\in\mathbb{N}%
$, we have%
\[
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha
,\beta\right]  ^{k}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.powerseries3.fin}.} Let $\gamma$ denote
$\left[  \alpha,\beta\right]  $. Then, $\gamma$ commutes with each of $\alpha$
and $\beta$ (since $\left[  \alpha,\beta\right]  $ commutes with each of
$\alpha$ and $\beta$). In other words, $\gamma\alpha=\alpha\gamma$ and
$\gamma\beta=\beta\gamma$.

As we showed in the proof of Lemma \ref{lem.powerseries1}, every positive
$i\in\mathbb{N}$ satisfies $\left[  \alpha,\beta^{i}\right]  =i\gamma
\beta^{i-1}$. Since $\gamma=\left[  \alpha,\beta\right]  $, this rewrites as
follows:
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\left[  \alpha,\beta
^{i}\right]  =i\left[  \alpha,\beta\right]  \beta^{i-1}.
\label{pf.powerseries3.fin.1}%
\end{equation}


Since $\left[  \beta,\alpha\right]  =-\underbrace{\left[  \alpha,\beta\right]
}_{=\gamma}=-\gamma$, we see that $\underbrace{\left[  \beta,\alpha\right]
}_{=-\gamma}\alpha=-\underbrace{\gamma\alpha}_{=\alpha\gamma}=-\alpha
\gamma=\alpha\underbrace{\left(  -\gamma\right)  }_{=\left[  \beta
,\alpha\right]  }=\alpha\left[  \beta,\alpha\right]  $ and
$\underbrace{\left[  \beta,\alpha\right]  }_{=-\gamma}\beta
=-\underbrace{\gamma\beta}_{=\beta\gamma}=-\beta\gamma=\beta
\underbrace{\left(  -\gamma\right)  }_{=\left[  \beta,\alpha\right]  }%
=\beta\left[  \beta,\alpha\right]  $. In other words, $\left[  \beta
,\alpha\right]  $ commutes with each of $\alpha$ and $\beta$. Therefore, the
roles of $\alpha$ and $\beta$ are symmetric, and thus we can apply
(\ref{pf.powerseries3.fin.1}) to $\beta$ and $\alpha$ instead of $\alpha$ and
$\beta$, and conclude that%
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\left[  \beta,\alpha
^{i}\right]  =i\left[  \beta,\alpha\right]  \alpha^{i-1}.
\label{pf.powerseries3.fin.1a}%
\end{equation}
Thus, every positive $i\in\mathbb{N}$ satisfies $\beta\alpha^{i}-\alpha
^{i}\beta=\left[  \beta,\alpha^{i}\right]  =i\underbrace{\left[  \beta
,\alpha\right]  }_{=-\gamma}\alpha^{i-1}=-i\gamma\alpha^{i-1}$, so that
$\beta\alpha^{i}=\alpha^{i}\beta-i\gamma\alpha^{i-1}$ and thus $\alpha
^{i}\beta=\beta\alpha^{i}+i\gamma\alpha^{i-1}$. We have thus proven that%
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\alpha^{i}\beta
=\beta\alpha^{i}+i\gamma\alpha^{i-1}. \label{pf.powerseries3.fin.1b}%
\end{equation}


Now, we are going to prove that every $i\in\mathbb{N}$ and $j\in\mathbb{N}$
satisfy%
\begin{equation}
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}.
\label{pf.powerseries3.fin.2}%
\end{equation}


\textit{Proof of (\ref{pf.powerseries3.fin.2}):} We will prove
(\ref{pf.powerseries3.fin.2}) by induction over $i$:

\textit{Induction base:} Let $j\in\mathbb{N}$ be arbitrary. For $i=0$, we have
$\alpha^{j}\beta^{i}=\alpha^{j}\underbrace{\beta^{0}}_{=1}=\alpha^{j}$ and
\begin{align*}
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}%
{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}  &  =\underbrace{\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq0;\ k\leq j}}}_{=\sum\limits_{k\in
\left\{  0\right\}  }}k!\dbinom{0}{k}\dbinom{j}{k}\beta^{0-k}\alpha
^{j-k}\gamma^{k}=\sum\limits_{k\in\left\{  0\right\}  }k!\dbinom{0}{k}%
\dbinom{j}{k}\beta^{0-k}\alpha^{j-k}\gamma^{k}\\
&  =\underbrace{0!}_{=1}\underbrace{\dbinom{0}{0}}_{=1}\underbrace{\dbinom
{j}{0}}_{=1}\underbrace{\beta^{0-0}}_{=1}\underbrace{\alpha^{j-0}}%
_{=\alpha^{j}}\underbrace{\gamma^{0}}_{=1}=\alpha^{j}.
\end{align*}
Hence, for $i=0$, we have $\alpha^{j}\beta^{i}=\alpha^{j}=\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}{k}%
\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}$. Thus,
(\ref{pf.powerseries3.fin.2}) holds for $i=0$, so that the induction base is complete.

\textit{Induction step:} Let $u\in\mathbb{N}$. Assume that
(\ref{pf.powerseries3.fin.2}) holds for $i=u$. We must now prove that
(\ref{pf.powerseries3.fin.2}) holds for $i=u+1$.

Since (\ref{pf.powerseries3.fin.2}) holds for $i=u$, we have%
\begin{equation}
\alpha^{j}\beta^{u}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\mathbb{N}.
\label{pf.powerseries3.fin.3}%
\end{equation}


Now, let $j\in\mathbb{N}$ be positive. Then, $j-1\in\mathbb{N}$. Now,%
\begin{align}
&  \alpha^{j}\underbrace{\beta^{u+1}}_{=\beta\beta^{u}}\nonumber\\
&  =\underbrace{\alpha^{j}\beta}_{\substack{=\beta\alpha^{j}+j\gamma
\alpha^{j-1}\\\text{(by (\ref{pf.powerseries3.fin.1b}),}\\\text{applied to
}j\text{ instead of }i\text{)}}}\beta^{u}=\left(  \beta\alpha^{j}%
+j\gamma\alpha^{j-1}\right)  \beta^{u}=\beta\alpha^{j}\beta^{u}%
+j\underbrace{\gamma\alpha^{j-1}\beta^{u}}_{\substack{=\alpha^{j-1}\beta
^{u}\gamma\\\text{(since }\gamma\text{ commutes with}\\\text{each of }%
\beta\text{ and }\alpha\text{)}}}\nonumber\\
&  =\beta\underbrace{\alpha^{j}\beta^{u}}_{\substack{=\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}{k}%
\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}\\\text{(by
(\ref{pf.powerseries3.fin.3}))}}}+j\underbrace{\alpha^{j-1}\beta^{u}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma
^{k}\\\text{(by (\ref{pf.powerseries3.fin.3}), applied to }j-1\\\text{instead
of }j\text{ (since }j-1\in\mathbb{N}\text{))}}}\gamma\nonumber\\
&  =\beta\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}%
}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}+j\left(
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom{u}%
{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k}\right)  \gamma
\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\underbrace{\beta\beta^{u-k}}_{=\beta^{u+1-k}}\alpha
^{j-k}\gamma^{k}+j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\underbrace{\gamma
^{k}\gamma}_{=\gamma^{k+1}}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}+j\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom{u}%
{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}.
\label{pf.powerseries3.fin.twosums}%
\end{align}
Let us separately simplify the two addends on the right hand side of this equation.

First of all, every $k\in\mathbb{N}$ which satisfies $k\leq u+1$ and $k\leq j$
but does \textbf{not} satisfy $k\leq u$ must satisfy \newline$k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=0$ (because this $k$ does
not satisfy $k\leq u$, so that we have $k>u$, and thus $\dbinom{u}{k}=0$).
Thus, $\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ \left(  \text{not
}k\leq u\right)  ;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq
u+1;\ \left(  \text{not }k\leq u\right)  ;\ k\leq j}}0=0$. Hence,%
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\nonumber\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq
u;\ k\leq j}}}_{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}%
}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
+\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ \left(
\text{not }k\leq u\right)  ;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}}_{=0}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.5}%
\end{align}


On the other hand,
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\underbrace{\beta
^{u-\left(  k-1\right)  }}_{=\beta^{u+1-k}}\underbrace{\alpha^{j-1-\left(
k-1\right)  }}_{=\alpha^{j-k}}\underbrace{\gamma^{\left(  k-1\right)  +1}%
}_{=\gamma^{k}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k-1\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}. \label{pf.powerseries3.fin.6a}%
\end{align}
But every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\[
\dbinom{j-1}{k-1}=\dfrac{\left(  j-1\right)  !}{\left(  k-1\right)  !\left(
\left(  j-1\right)  -\left(  k-1\right)  \right)  !}=\dfrac{\left(
j-1\right)  !}{\left(  k-1\right)  !\left(  j-k\right)  !}.
\]
Hence, every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\begin{align}
\left(  k-1\right)  !\dbinom{u}{k-1}\underbrace{\dbinom{j-1}{k-1}}%
_{=\dfrac{\left(  j-1\right)  !}{\left(  k-1\right)  !\left(  j-k\right)  !}}
&  =\left(  k-1\right)  !\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(
k-1\right)  !\left(  j-k\right)  !}\nonumber\\
&  =\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}.
\label{pf.powerseries3.fin.6b}%
\end{align}
But multiplying both sides of (\ref{pf.powerseries3.fin.6a}) with $j$, we
obtain%
\begin{align}
&  j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =j\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\underbrace{\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}%
}_{\substack{=\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)
!}\\\text{(by (\ref{pf.powerseries3.fin.6b}))}}}\beta^{u+1-k}\alpha
^{j-k}\gamma^{j}\nonumber\\
&  =j\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N}%
;\ k\geq1;\\k\leq u+1;\ k\leq j}}\dbinom{u}{k-1}j\dfrac{\left(  j-1\right)
!}{\left(  j-k\right)  !}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.6c}%
\end{align}
But every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\begin{align*}
\dbinom{j}{k}  &  =\dfrac{j!}{k!\left(  j-k\right)  !}=\dfrac{j\left(
j-1\right)  !}{k!\left(  j-k\right)  !}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}j!=j\left(  j-1\right)  !\right) \\
&  =\dfrac{1}{k!}\cdot j\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}.
\end{align*}
Hence, every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\begin{equation}
k!\dbinom{j}{k}=j\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}.
\label{pf.powerseries3.fin.6d}%
\end{equation}
Thus, (\ref{pf.powerseries3.fin.6c}) becomes%
\begin{align}
&  j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\dbinom{u}{k-1}\underbrace{j\dfrac{\left(  j-1\right)  !}{\left(
j-k\right)  !}}_{\substack{=k!\dbinom{j}{k}\\\text{(by
(\ref{pf.powerseries3.fin.6d}))}}}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\dbinom{u}{k-1}k!\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
=\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.6e}%
\end{align}


But every $k\in\mathbb{N}$ which satisfies $k\leq u+1$ and $k\leq j$ but does
\textbf{not} satisfy $k\geq1$ must satisfy \newline$k!\dbinom{u}{k-1}%
\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=0$ (because this $k$ does not
satisfy $k\geq1$, so that we have $k<1$, and thus $\dbinom{u}{k-1}=0$). Thus,
\newline$\sum\limits_{\substack{k\in\mathbb{N};\ \left(  \text{not }%
k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k-1}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N}%
;\ \left(  \text{not }k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}0=0$. Hence,%
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N}\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}+\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\ \left(  \text{not
}k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k-1}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}}_{=0}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.6f}%
\end{align}
Thus, (\ref{pf.powerseries3.fin.6e}) becomes%
\begin{align}
&  j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}
\label{pf.powerseries3.fin.6g}%
\end{align}
(by (\ref{pf.powerseries3.fin.6f})).

Also, notice that every $k\in\mathbb{N}$ satisfies%
\begin{equation}
k!\dbinom{u}{k}+k!\dbinom{u}{k-1}=k!\underbrace{\left(  \dbinom{u}{k}%
+\dbinom{u}{k-1}\right)  }_{\substack{=\dbinom{u+1}{k}\\\text{(by the
recurrence equation}\\\text{of the binomial coefficients)}}}=k!\dbinom{u+1}%
{k}. \label{pf.powerseries3.fin.binom}%
\end{equation}


Now, (\ref{pf.powerseries3.fin.twosums}) becomes%
\begin{align*}
\alpha^{j}\beta^{u+1}  &  =\underbrace{\sum\limits_{\substack{k\in
\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}}_{\substack{=\sum\limits_{\substack{k\in
\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}\\\text{(by (\ref{pf.powerseries3.fin.5}))}%
}}+\underbrace{j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}\\\text{(by (\ref{pf.powerseries3.fin.6g}))}}}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}+\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom{u}%
{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}%
}\underbrace{\left(  k!\dbinom{u}{k}+k!\dbinom{u}{k-1}\right)  }%
_{\substack{=k!\dbinom{u+1}{k}\\\text{(by (\ref{pf.powerseries3.fin.binom}))}%
}}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}%
}k!\dbinom{u+1}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\end{align*}


Now, forget that we fixed $j$. We thus have shown that%
\begin{equation}
\alpha^{j}\beta^{u+1}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq
u+1;\ k\leq j}}k!\dbinom{u+1}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j} \label{pf.powerseries3.fin.9}%
\end{equation}
holds for every positive $j\in\mathbb{N}$. Since it is easy to see that
(\ref{pf.powerseries3.fin.9}) also holds for $j=0$ (the proof is similar to
our induction base above), this yields that (\ref{pf.powerseries3.fin.9})
holds for every $j\in\mathbb{N}$. In other words, (\ref{pf.powerseries3.fin.2}%
) holds for $i=u+1$. Thus, the induction step is complete. Hence, we have
proven (\ref{pf.powerseries3.fin.2}) by induction over $i$.

Since $\gamma=\left[  \alpha,\beta\right]  $, the (now proven) identity
(\ref{pf.powerseries3.fin.2}) rewrites as%
\[
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left.
\underbrace{\gamma}_{=\left[  \alpha,\beta\right]  }\right.  ^{k}%
=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}%
{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}.
\]
Proposition \ref{prop.powerseries3.fin} is thus proven.

\textit{Third proof of Lemma \ref{lem.powerseries3}.} By the definition of the
exponential, we have $\exp\left[  \alpha,\beta\right]  =\sum\limits_{k\in
\mathbb{N}}\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}$, $\exp\alpha
=\sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}{j!}$ and $\exp\beta
=\sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}$. Multiplying the last two
of these three equalities, we obtain%
\begin{align*}
&  \left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right) \\
&  =\left(  \sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}{j!}\right)
\cdot\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\right)
=\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}%
}{j!}\cdot\dfrac{\beta^{i}}{i!}=\sum\limits_{i\in\mathbb{N}}\sum
\limits_{j\in\mathbb{N}}\dfrac{1}{i!j!}\underbrace{\alpha^{j}\beta^{i}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha
,\beta\right]  ^{k}\\\text{(by Proposition \ref{prop.powerseries3.fin})}}}\\
&  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{1}%
{i!j!}\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom
{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}}_{=\sum
\limits_{k\in\mathbb{N}}\sum\limits_{\substack{i\in\mathbb{N};\\k\leq i}%
}\sum\limits_{\substack{j\in\mathbb{N};\\k\leq j}}}\underbrace{\dfrac{1}%
{i!j!}k!\dbinom{i}{k}\dbinom{j}{k}}_{\substack{=\dfrac{1}{\left(  i-k\right)
!\left(  j-k\right)  !k!}\\\text{(by easy computations)}}}\beta^{i-k}%
\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}\\
&  =\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{i\in\mathbb{N};\\k\leq
i}}\sum\limits_{\substack{j\in\mathbb{N};\\k\leq j}}\dfrac{1}{\left(
i-k\right)  !\left(  j-k\right)  !k!}\beta^{i-k}\alpha^{j-k}\left[
\alpha,\beta\right]  ^{k}=\sum\limits_{k\in\mathbb{N}}\sum\limits_{i\in
\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{1}{i!j!k!}\beta^{i}\alpha
^{j}\left[  \alpha,\beta\right]  ^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }i\text{ for }i-k\text{ in the second sum,}\\
\text{and we substituted }j\text{ for }j-k\text{ in the third sum}%
\end{array}
\right) \\
&  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\sum
\limits_{k\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\cdot\dfrac{\alpha^{j}}{j!}%
\cdot\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}=\underbrace{\left(
\sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\right)  }_{=\exp\beta}%
\cdot\underbrace{\left(  \sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}%
{j!}\right)  }_{=\exp\alpha}\cdot\underbrace{\left(  \sum\limits_{k\in
\mathbb{N}}\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}\right)  }%
_{=\exp\left[  \alpha,\beta\right]  }\\
&  =\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(
\exp\left[  \alpha,\beta\right]  \right)  .
\end{align*}
This proves Lemma \ref{lem.powerseries3} once again.

\subsection{\label{subsect.fockvir}Representations of
\texorpdfstring{$\operatorname*{Vir}$}{Vir} on
\texorpdfstring{$F_{\mu}$}{the Fock module}}

\subsubsection{The Lie-algebraic semidirect product: the general case}

Let us define the ``full-fledged'' version of the Lie-algebraic semidirect
product, although it will not be central to what we will later do:

\begin{definition}
\label{def.semidir.lielie}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a vector space equipped with both a Lie algebra structure
and a $\mathfrak{g}$-module structure.

\textbf{(a)} Let $\rho:\mathfrak{g}\rightarrow\operatorname*{End}\mathfrak{h}$
be the map representing the action of $\mathfrak{g}$ on $\mathfrak{h}$. We say
that $\mathfrak{g}$ \textit{acts on }$\mathfrak{h}$\textit{ by derivations} if
$\rho\left(  \mathfrak{g}\right)  \subseteq\operatorname*{Der}\mathfrak{h}$,
or, equivalently, if the map%
\[
\mathfrak{h}\rightarrow\mathfrak{h},\ \ \ \ \ \ \ \ \ \ x\mapsto
a\rightharpoonup x
\]
is a derivation for every $a\in\mathfrak{g}$. (Here and in the following, the
symbol $\rightharpoonup$ means action; i. e., a term like $c\rightharpoonup h$
(with $c\in\mathfrak{g}$ and $h\in\mathfrak{h}$) means the action of $c$ on
$h$.)

\textbf{(b)} Assume that $\mathfrak{g}$ acts on $\mathfrak{h}$ by derivations.
Then, we define the \textit{semidirect product} $\mathfrak{g}\ltimes
\mathfrak{h}$ to be the Lie algebra which, as a vector space, is
$\mathfrak{g}\oplus\mathfrak{h}$, but whose Lie bracket is defined by%
\begin{align*}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]   &
=\left(  \left[  a,b\right]  ,\left[  \alpha,\beta\right]  +a\rightharpoonup
\beta-b\rightharpoonup\alpha\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }a\in\mathfrak{g}\text{, }%
\alpha\in\mathfrak{h}\text{, }b\in\mathfrak{g}\text{ and }\beta\in
\mathfrak{h}\right.  .
\end{align*}


Thus, the canonical injection $\mathfrak{g}\rightarrow\mathfrak{g}%
\ltimes\mathfrak{h},$ $a\mapsto\left(  a,0\right)  $ is a Lie algebra
homomorphism, and so is the canonical projection $\mathfrak{g}\ltimes
\mathfrak{h}\rightarrow\mathfrak{g},$ $\left(  a,\alpha\right)  \mapsto a$.
Also, the canonical injection $\mathfrak{h}\rightarrow\mathfrak{g}%
\ltimes\mathfrak{h},$ $\alpha\mapsto\left(  0,\alpha\right)  $ is a Lie
algebra homomorphism.
\end{definition}

All statements made in Definition \ref{def.semidir.lielie} (including the
tacit statement that the Lie bracket on $\mathfrak{g}\ltimes\mathfrak{h}$
defined in Definition \ref{def.semidir.lielie} satisfies antisymmetry and the
Jacobi identity) are easy to verify by computation.

\begin{remark}
If $\mathfrak{g}$ is a Lie algebra, and $\mathfrak{h}$ is an \textbf{abelian}
Lie algebra with any $\mathfrak{g}$-module structure, then $\mathfrak{g}$
automatically acts on $\mathfrak{h}$ by derivations (because any endomorphism
of the vector space $\mathfrak{h}$ is a derivation), and thus Definition
\ref{def.semidir.lielie} \textbf{(b)} defines a semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$. In this case, this semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$ coincides with the semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$ defined in Definition \ref{def.semidir}
(applied to $M=\mathfrak{h}$). However, when $\mathfrak{h}$ is not abelian,
the semidirect product $\mathfrak{g}\ltimes\mathfrak{h}$ defined in Definition
\ref{def.semidir.lielie} (in general) differs from that defined in Definition
\ref{def.semidir} (since the former depends on the Lie algebra structure on
$\mathfrak{h}$, while the latter does not). Care must therefore be taken when
speaking of semidirect products.
\end{remark}

An example for the semidirect product construction given in Definition
\ref{def.semidir.lielie} \textbf{(b)} is given by the following proposition:

\begin{proposition}
\label{prop.VirtoDerA}Consider the Witt algebra $W$, the Virasoro algebra
$\operatorname*{Vir}$ and the Heisenberg algebra $\mathcal{A}$.

\textbf{(a)} In Lemma \ref{lem.WtoDerA}, we constructed a homomorphism
$\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ of Lie algebras. This
homomorphism $\eta$ makes $\mathcal{A}$ into a $W$-module, and $W$ acts on
$\mathcal{A}$ by derivations. Therefore, a Lie algebra $W\ltimes\mathcal{A}$
is defined (according to Definition \ref{def.semidir.lielie} \textbf{(b)}).

\textbf{(b)} There is a natural homomorphism $\widetilde{\eta}%
:\operatorname*{Vir}\rightarrow\operatorname*{Der}\mathcal{A}$ of Lie algebras
given by
\[
\left(  \widetilde{\eta}\left(  f\partial+\lambda K\right)  \right)  \left(
g,\alpha\right)  =\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for
all }f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{, }\lambda\in\mathbb{C}\text{ and }\alpha\in
\mathbb{C}.
\]
This homomorphism $\widetilde{\eta}$ is simply the extension of the
homomorphism $\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ (defined in
Lemma \ref{lem.WtoDerA}) to $\operatorname*{Vir}$ by means of requiring that
$\widetilde{\eta}\left(  K\right)  =0$.

This homomorphism $\widetilde{\eta}$ makes $\mathcal{A}$ a
$\operatorname*{Vir}$-module, and $\operatorname*{Vir}$ acts on $\mathcal{A}$
by derivations. Therefore, a Lie algebra $\operatorname*{Vir}\ltimes
\mathcal{A}$ is defined (according to Definition \ref{def.semidir.lielie}
\textbf{(b)}).
\end{proposition}

The proof of Proposition \ref{prop.VirtoDerA} is straightforward and left to
the reader.

\subsubsection{The action of \texorpdfstring{$\operatorname*{Vir}$}{Vir} on
\texorpdfstring{$F_{\mu}$}{the Fock module}}

Let us now return to considering the Witt and Heisenberg algebras.

According to Proposition \ref{prop.VirtoDerA} \textbf{(a)}, we have a Lie
algebra $W\ltimes\mathcal{A}$, of which $\mathcal{A}$ is a Lie subalgebra.
Now, recall (from Definition \ref{def.fock}) that, for every $\mu\in
\mathbb{C}$, we have a representation $F_{\mu}$ of the Lie algebra
$\mathcal{A}$ on the Fock space $F$.

Can we extend this representation $F_{\mu}$ of $\mathcal{A}$ to a
representation of the semidirect product $W\ltimes\mathcal{A}$ ?

This question splits into two questions:

\textbf{Question 1:} Can we find linear operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ for all $n\in\mathbb{Z}$ such that $\left[  L_{n},a_{m}\right]
=-ma_{n+m}$ ? (Note that there are several abuses of notation in this
question. First, we denote the sought operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ by the same letters as the elements $L_{n}$ of $W$ because our
intuition for the $L_{n}$ is as if they would form a representation of $W$,
although we do not actually require them to form a representation of $W$ in
Question 1. Second, in the equation $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$,
we use $a_{m}$ and $a_{n+m}$ as abbreviations for $a_{m}\mid_{F_{\mu}}$ and
$a_{n+m}\mid_{F_{\mu}}$, respectively (so that this equation actually means
$\left[  L_{n},a_{m}\mid_{F_{\mu}}\right]  =-ma_{n+m}\mid_{F_{\mu}}$).)

\textbf{Question 2:} Do the operators $L_{n}:F_{\mu}\rightarrow F_{\mu}$ that
answer Question 1 also satisfy $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$? (In other words, do they really form a representation of
$W$ ?)

The answers to these questions are the following:

\textbf{Answer to Question 1:} Yes, and moreover, these operators are unique
up to adding a constant (a new constant for each operator). (The uniqueness is
rather easy to prove: If we have two families $\left(  L_{n}^{\prime}\right)
_{n\in\mathbb{Z}}$ and $\left(  L_{n}^{\prime\prime}\right)  _{n\in\mathbb{Z}%
}$ of linear maps $F_{\mu}\rightarrow F_{\mu}$ satisfying $\left[
L_{n}^{\prime},a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n}^{\prime\prime
},a_{m}\right]  =-ma_{n+m}$, then every $L_{n}^{\prime}-L_{n}^{\prime\prime}$
commutes with all $a_{m}$, and thus is constant by Dixmier's lemma.)

\textbf{Answer to Question 2:} No, but almost. Our operators $L_{n}$ satisfy
$\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}$ whenever $n+m\neq
0$, but the $n+m=0$ case requires a correction term. This correction term (as
a function of $\left(  L_{n},L_{m}\right)  $) happens to be the $2$-cocycle
$\omega$ of Theorem \ref{thm.H^2(W)}. So the $\mathcal{A}$-module $F_{\mu}$
does not extend to a $W\ltimes\mathcal{A}$-module, but extends to a
$\operatorname*{Vir}\ltimes\mathcal{A}$-module, where $\operatorname*{Vir}%
\ltimes\mathcal{A}$ is defined as in Proposition \ref{prop.VirtoDerA}
\textbf{(b)}.

Now we are going to prove the answers to Questions 1 and 2 formulated above.
First, we must define our operators $L_{n}$. ``Formally'' (in the sense of
``not caring about divergence of sums''), one could try to define $L_{n}$ by
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z} \label{def.fockvir.wrong}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$), and this would ``formally'' make $F_{\mu}$ into a
$W\ltimes\mathcal{A}$-module (in the sense that if the sums were not
divergent, one could manipulate them to ``prove'' that $\left[  L_{n}%
,a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$ for all $n$ and $m$). But the problem with this
``formal'' approach is that the sum $\sum\limits_{m\in\mathbb{Z}}a_{-m}%
a_{n+m}$ does not make sense for $n=0$: it is an infinite sum, and infinitely
many of its terms yield nonzero values when applied to a given
vector.\footnote{In fact, assume that this sum would make sense for $n=0$.
Thus we would have $L_{0}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{m}%
$. Applied to the vector $1\in F_{0}$, this would give $L_{0}1=\dfrac{1}%
{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{m}1$. The terms for $m>0$ will get
killed (since $a_{m}1=0$ for $m>0$), but the terms for $m\leq0$ will survive.
The sum would become
\begin{align*}
L_{0}1  &  =\dfrac{1}{2}\left(  a_{0}a_{-0}1+a_{1}a_{-1}1+a_{2}a_{-2}%
1+a_{3}a_{-3}1+...\right) \\
&  =\dfrac{1}{2}\left(  \mu^{2}1+1\dfrac{\partial}{\partial x_{1}}%
x_{1}+2\dfrac{\partial}{\partial x_{2}}x_{2}+3\dfrac{\partial}{\partial x_{3}%
}x_{3}+...\right)  =\dfrac{1}{2}\left(  \mu^{2}+1+2+3+...\right)  .
\end{align*}
Unless we interpret $1+2+3+...$ as $-\dfrac{1}{12}$ (which we are going to do
in some sense: the modified formulae further below include $-\dfrac{1}{12}$
factors), this makes no sense.} So we are not allowed to make the definition
(\ref{def.fockvir.wrong}), and we cannot rescue it just by defining a more
liberal notion of convergence. Instead, we must modify this ``definition''.

In order to modify it, we define the so-called \textit{normal ordering}:

\begin{definition}
\label{def.fockvir.normal}For any two integers $m$ and $n$, define the
\textit{normal ordered product }$\left.  :a_{m}a_{n}:\right.  $ in the
universal enveloping algebra $U\left(  \mathcal{A}\right)  $ by
\[
\left.  :a_{m}a_{n}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]


More generally, for any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$, define the
\textit{normal ordered product }$\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}%
}:\right.  $ in the universal enveloping algebra $U\left(  \mathcal{A}\right)
$ by%
\[
\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.  \ =\left(
\begin{array}
[c]{c}%
\text{the product of the elements }a_{n_{1}}\text{, }a_{n_{2}}\text{,
}...\text{, }a_{n_{k}}\text{ of }U\left(  \mathcal{A}\right)  \text{,}\\
\text{rearranged in such a way that the subscripts are in increasing order}%
\end{array}
\right)  .
\]
(More formally, this normal ordered product $\left.  :a_{n_{1}}a_{n_{2}%
}...a_{n_{k}}:\right.  $ is defined as the product $a_{m_{1}}a_{m_{2}%
}...a_{m_{k}}$, where $\left(  m_{1},m_{2},...,m_{k}\right)  $ is the
permutation of the list $\left(  n_{1},n_{2},...,n_{k}\right)  $ satisfying
$m_{1}\leq m_{2}\leq...\leq m_{k}$.)
\end{definition}

Note that we have thus defined only normal ordered products of elements of the
form $a_{n}$ for $n\in\mathbb{Z}$. Normal ordered products of basis elements
of other Lie algebras are not always defined by the same formulas (although
sometimes they are).

\begin{remark}
\label{rmk.fockvir.normal.mn}If $m$ and $n$ are integers such that $m\neq-n$,
then $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}$. (This is because $\left[
a_{m},a_{n}\right]  =0$ in $\mathcal{A}$ when $m\neq-n$.)
\end{remark}

Normal ordered products have the property of being commutative:

\begin{remark}
\label{rmk.fockvir.normal.comm}\textbf{(a)} Any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$ satisfy $\left.  :a_{m}a_{n}:\right.  =\left.  :a_{n}%
a_{m}:\right.  $.

\textbf{(b)} Any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$ and any permutation
$\pi\in S_{k}$ satisfy $\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.
=\left.  :a_{n_{\pi\left(  1\right)  }}a_{n_{\pi\left(  2\right)  }%
}...a_{n_{\pi\left(  k\right)  }}:\right.  $.
\end{remark}

The proof of this is trivial.

By Remark \ref{rmk.fockvir.normal.mn} (and by the rather straightforward
generalization of this fact to many integers), normal ordered products are
rarely different from the usual products. But even when they are different,
they don't differ much:

\begin{remark}
\label{rmk.fockvir.normal.K}Let $m$ and $n$ be integers.

\textbf{(a)} Then, $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K$. Here, when $\mathfrak{A}$ is an assertion, we
denote by $\left[  \mathfrak{A}\right]  $ the truth value of $\mathfrak{A}$
(that is, the number $\left\{
\begin{array}
[c]{c}%
1\text{, if }\mathfrak{A}\text{ is true;}\\
0\text{, if }\mathfrak{A}\text{ is false }%
\end{array}
\right.  $).

\textbf{(b)} For any $x\in U\left(  \mathcal{A}\right)  $, we have $\left[
x,\left.  :a_{m}a_{n}:\right.  \right]  =\left[  x,a_{m}a_{n}\right]  $ (where
$\left[  \cdot,\cdot\right]  $ denotes the commutator in $U\left(
\mathcal{A}\right)  $).
\end{remark}

Note that when we denote by $\left[  \cdot,\cdot\right]  $ the commutator in
$U\left(  \mathcal{A}\right)  $, we are seemingly risking a confusion with the
notation $\left[  \cdot,\cdot\right]  $ for the Lie bracket of $\mathcal{A}$
(because we embed $\mathcal{A}$ in $U\left(  \mathcal{A}\right)  $). However,
this confusion is harmless, because the very definition of $U\left(
\mathcal{A}\right)  $ ensures that the commutator of two elements of
$\mathcal{A}$, taken in $U\left(  \mathcal{A}\right)  $, equals to their Lie
bracket in $\mathcal{A}$.

\textit{Proof of Remark \ref{rmk.fockvir.normal.K}.} \textbf{(a)} We
distinguish between three cases:

\textit{Case 1:} We have $m\neq-n$.

\textit{Case 2:} We have $m=-n$ and $m>0$.

\textit{Case 3:} We have $m=-n$ and $m\leq0$.

In Case 1, we have $m\neq-n$, so that $\delta_{m,-n}=0$ and thus%
\[
a_{m}a_{n}+n\left[  m>0\right]  \underbrace{\delta_{m,-n}}_{=0}K=a_{m}%
a_{n}=\left.  :a_{m}a_{n}:\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark
\ref{rmk.fockvir.normal.mn}}\right)  .
\]
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 1.

In Case 2, we have $m=-n$ and $m>0$, so that $m>n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{n}a_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m>n\right) \\
&  =a_{m}a_{n}+\underbrace{\left[  a_{n},a_{m}\right]  }_{=n\delta
_{n,-m}K=n1\delta_{m,-n}K}=a_{m}a_{n}+n\underbrace{1}_{\substack{=\left[
m>0\right]  \\\text{(since }m>0\text{)}}}\delta_{m,-n}K=a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K.
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 2.

In Case 3, we have $m=-n$ and $m\leq0$, so that $m\leq n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{m}a_{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\leq n\right) \\
&  =a_{m}a_{n}+\underbrace{0}_{\substack{=n\left[  m>0\right]  \delta
_{m,-n}K\\\text{(since }m\leq0\text{, so that }\left(  \text{not }m>0\right)
\text{, thus}\\\left[  m>0\right]  =0\text{ and hence }n\left[  m>0\right]
\delta_{m,-n}K=0\text{)}}}=a_{m}a_{n}+n\left[  m>0\right]  \delta_{m,-n}K.
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 3.

Thus, we have proven Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} in all
three possible cases. This completes the proof of Remark
\ref{rmk.fockvir.normal.K} \textbf{(a)}.

\textbf{(b)} We have $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(
U\left(  \mathcal{A}\right)  \right)  $ (since the center of a Lie algebra is
contained in the center of its universal enveloping algebra). Hence, $\left[
x,K\right]  =0$ for any $x\in U\left(  \mathcal{A}\right)  $.

Since $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[  m>0\right]
\delta_{m,-n}K$, we have%
\begin{align*}
\left[  x,\left.  :a_{m}a_{n}:\right.  \right]   &  =\left[  x,a_{m}%
a_{n}+n\left[  m>0\right]  \delta_{m,-n}K\right] \\
&  =\left[  x,a_{m}a_{n}\right]  +n\left[  m>0\right]  \delta_{m,-n}%
\underbrace{\left[  x,K\right]  }_{=0}=\left[  x,a_{m}a_{n}\right]
\end{align*}
for every $x\in U\left(  \mathcal{A}\right)  $. This proves Remark
\ref{rmk.fockvir.normal.K} \textbf{(b)}.

Now, the true definition of our maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ will
be the following:

\begin{definition}
\label{def.fockvir}For every $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$, define a
linear map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ by%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
\label{def.fockvir.def}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$). This sum $\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  $ is an infinite sum, but it is well-defined in the
following sense: For any vector $v\in F_{\mu}$, applying $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ to the vector $v$ gives the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  v$, which has
only finitely many nonzero addends (because of Lemma \ref{lem.fockvir.welldef}
\textbf{(c)} below) and thus has a well-defined value.
\end{definition}

Note that we have not defined the meaning of the sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ in the universal enveloping
algebra $U\left(  \mathcal{A}\right)  $ itself, but only its meaning as an
endomorphism of $F_{\mu}$. However, if we wanted, we could also define the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ as an element
of a suitable completion of the universal enveloping algebra $U\left(
\mathcal{A}\right)  $ (although not in $U\left(  \mathcal{A}\right)  $
itself). We don't really have a reason to do so here, however.

\begin{Convention}
\label{conv.fockvir.L}During the rest of Section \ref{subsect.fockvir}, we are
going to use the labels $L_{n}$ for the maps $L_{n}:F_{\mu}\rightarrow F_{\mu
}$ introduced in Definition \ref{def.fockvir}, and \textbf{not} for the
eponymous elements of the Virasoro algebra $\operatorname*{Vir}$ or of the
Witt algebra $W$, unless we explicitly refer to ``the element $L_{n}$ of
$\operatorname*{Vir}$'' or ``the element $L_{n}$ of $W$'' or something
similarly unambiguous.

(While it is correct that the maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ satisfy
the same relations as the eponymous elements $L_{n}$ of $\operatorname*{Vir}$
(but not the eponymous elements $L_{n}$ of $W$), this is a nontrivial fact
that needs to be proven, and until it is proven we must avoid any confusion
between these different meanings of $L_{n}$.)
\end{Convention}

Let us first show that Definition \ref{def.fockvir} makes sense:

\begin{lemma}
\label{lem.fockvir.welldef}Let $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$. Let
$v\in F_{\mu}$. Then:

\textbf{(a)} If $m\in\mathbb{Z}$ is sufficiently high, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(b)} If $m\in\mathbb{Z}$ is sufficiently low, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(c)} All but finitely many $m\in\mathbb{Z}$ satisfy $\left.
:a_{-m}a_{n+m}:\right.  v=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.fockvir.welldef}.} \textbf{(a)} Since $v\in
F_{\mu}\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $, the vector $v$ is
a polynomial in infinitely many variables. Since every polynomial contains
only finitely many variables, there exists an integer $N\in\mathbb{N}$ such
that no variable $x_{r}$ with $r>N$ occurs in $v$. Consider this $N$. Then,
\begin{equation}
\dfrac{\partial}{\partial x_{r}}v=0\ \ \ \ \ \ \ \ \ \ \text{for every integer
}r>N. \label{pf.fockvir.welldef.1}%
\end{equation}


Now, let $m\geq\max\left\{  -n+N+1,-\dfrac{1}{2}n\right\}  $. Then,
$m\geq-n+N+1$ and $m\geq-\dfrac{1}{2}n$.

Since $m\geq-\dfrac{1}{2}n$, we have $2m\geq-n$, so that $-m\leq n+m$.

From $m\geq-n+N+1$, we get $n+m\geq N+1$, so that $n+m>0$. Hence, $a_{n+m}%
\mid_{F_{\mu}}=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}$, so
that $a_{n+m}v=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}v$. Since
$\dfrac{\partial}{\partial x_{n+m}}v=0$ (by (\ref{pf.fockvir.welldef.1}),
applied to $r=n+m$ (since $n+m\geq N+1>N$)), we thus have $a_{n+m}v=0$.

By Definition \ref{def.fockvir.normal}, we have
\[
\left.  :a_{-m}a_{n+m}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{-m}a_{n+m},\ \ \ \ \ \ \ \ \ \ \text{if }-m\leq n+m;\\
a_{n+m}a_{-m},\ \ \ \ \ \ \ \ \ \ \text{if }-m>n+m
\end{array}
\right.  .
\]
Since $-m\leq n+m$, this rewrites as $\left.  :a_{-m}a_{n+m}:\right.
=a_{-m}a_{n+m}$. Thus, $\left.  :a_{-m}a_{n+m}:\right.  v=a_{-m}%
\underbrace{a_{n+m}v}_{=0}=0$, and Lemma \ref{lem.fockvir.welldef}
\textbf{(a)} is proven.

\textbf{(b)} Applying Lemma \ref{lem.fockvir.welldef} \textbf{(a)} to $-n-m$
instead of $m$, we see that, if $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.  v=0$.
Since%
\[
\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.
=\left.  :a_{n+m}a_{-m}:\right.  =\left.  :a_{-m}a_{n+m}:\right.
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.fockvir.normal.comm}
\textbf{(a)}}\right)  ,
\]
this rewrites as follows: If $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-m}a_{n+m}:\right.  v=0$. This proves Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\textbf{(c)} Lemma \ref{lem.fockvir.welldef} \textbf{(c)} follows immediately
by combining Lemma \ref{lem.fockvir.welldef} \textbf{(a)} and Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\begin{remark}
\label{rmk.fockvir.explicit}\textbf{(a)} If $n\neq0$, then the operator
$L_{n}$ defined in Definition \ref{def.fockvir} can be rewritten as%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}.
\]
In other words, for $n\neq0$, our old definition (\ref{def.fockvir.wrong}) of
$L_{n}$ makes sense and is equivalent to the new definition (Definition
\ref{def.fockvir}).

\textbf{(b)} But when $n=0$, the formula (\ref{def.fockvir.wrong}) is devoid
of sense, whereas Definition \ref{def.fockvir} is legit. However, we can
rewrite the definition of $L_{0}$ without using normal ordered products:
Namely, we have%
\[
L_{0}=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}}{2}=\sum\limits_{m>0}%
a_{-m}a_{m}+\dfrac{\mu^{2}}{2}.
\]


\textbf{(c)} Let us grade the space $F_{\mu}$ as in Definition
\ref{def.fock.grad}. (Recall that this is the grading which gives every
variable $x_{i}$ the degree $-i$ and makes $F_{\mu}=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ into a graded $\mathbb{C}$-algebra. This is
\textbf{not} the modified grading that we gave to the space $F_{\mu}$ in
Remark \ref{rmk.fockgrad}.) Let $d\in\mathbb{N}$. Then, every homogeneous
polynomial $f\in F_{\mu}$ of degree $d$ (with respect to this grading)
satisfies $L_{0}f=\left(  \dfrac{\mu^{2}}{2}-d\right)  f$.

\textbf{(d)} Consider the grading on $F_{\mu}$ defined in part \textbf{(c)}.
For every $n\in\mathbb{Z}$, the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ is
homogeneous of degree $n$. (The notion ``homogeneous of degree $n$'' we are
using here is that defined in Definition \ref{def.hg} \textbf{(a)}, not the
one defined in Definition \ref{def.det.US.poly.hom} \textbf{(a)}.)
\end{remark}

\textit{Proof of Remark \ref{rmk.fockvir.explicit}.} \textbf{(a)} Let $n\neq
0$. Then, every $m\in\mathbb{Z}$ satisfies $-m\neq-\left(  n+m\right)  $ and
thus $\left.  :a_{-m}a_{n+m}:\right.  =a_{-m}a_{n+m}$ (by Remark
\ref{rmk.fockvir.normal.mn}, applied to $-m$ and $n+m$ instead of $m$ and
$n$). Hence, the formula $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  $ (which is how we defined $L_{n}$) rewrites
as $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}$. This proves
Remark \ref{rmk.fockvir.explicit} \textbf{(a)}.

\textbf{(b)} By the definition of $L_{0}$ (in Definition \ref{def.fockvir}),
we have%
\begin{align*}
L_{0}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{0+m}:\right.  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{m}:\right. \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m<0}\underbrace{\left.  :a_{-m}%
a_{m}:\right.  }_{\substack{=a_{m}a_{-m}\\\text{(by the definition of }\left.
:a_{-m}a_{m}:\right.  \\\text{(since }m<0\text{ and thus }-m>m\text{))}%
}}+\underbrace{\left.  :a_{-0}a_{0}:\right.  }_{\substack{=\left.  :a_{0}%
a_{0}:\right.  =a_{0}a_{0}\\\text{(by the definition of }\left.  :a_{0}%
a_{0}:\right.  \\\text{(since }0\leq0\text{))}}}+\sum\limits_{m>0}%
\underbrace{\left.  :a_{-m}a_{m}:\right.  }_{\substack{=a_{-m}a_{m}\\\text{(by
the definition of }\left.  :a_{-m}a_{m}:\right.  \\\text{(since }m>0\text{ and
thus }-m\leq m\text{))}}}\right) \\
&  =\dfrac{1}{2}\left(  \underbrace{\sum\limits_{m<0}a_{m}a_{-m}%
}_{\substack{=\sum\limits_{m>0}a_{-m}a_{m}\\\text{(here, we substituted
}m\text{ for }-m\text{ in the sum)}}}+\underbrace{a_{0}a_{0}}_{=a_{0}^{2}%
}+\sum\limits_{m>0}a_{-m}a_{m}\right) \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m>0}a_{-m}a_{m}+a_{0}^{2}+\sum
\limits_{m>0}a_{-m}a_{m}\right)  =\dfrac{1}{2}\left(  2\sum\limits_{m>0}%
a_{-m}a_{m}+a_{0}^{2}\right)  =\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}%
}{2}\\
&  =\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{\mu^{2}}{2}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }a_{0}\text{ acts as multiplication with }\mu\text{ on }F_{\mu
}\right)
\end{align*}
on $F_{\mu}$. This proves Remark \ref{rmk.fockvir.explicit} \textbf{(b)}.

\textbf{(c)} We must prove the equation $L_{0}f=\left(  \dfrac{\mu^{2}}%
{2}-d\right)  f$ for every homogeneous polynomial $f\in F_{\mu}$ of degree
$d$. Since this equation is linear in $f$, it is clearly enough to prove this
for the case of $f$ being a monomial\footnote{Here, ``monomial'' means
``monomial without coefficient''.} of degree $d$. So let $f$ be a monomial of
degree $d$. Then, $f$ can be written in the form $f=x_{1}^{\alpha_{1}}%
x_{2}^{\alpha_{2}}x_{3}^{\alpha_{3}}...$ for a sequence $\left(  \alpha
_{1},\alpha_{2},\alpha_{3},...\right)  $ of nonnegative integers such that
$\sum\limits_{m>0}\left(  -m\right)  \alpha_{m}=d$ (the $-m$ coefficient comes
from $\deg\left(  x_{m}\right)  =-m$) and such that all but finitely many
$i\in\left\{  1,2,3,...\right\}  $ satisfy $\alpha_{i}=0$. Consider this
sequence. Clearly, $\sum\limits_{m>0}\left(  -m\right)  \alpha_{m}=d$ yields
$\sum\limits_{m>0}m\alpha_{m}=-d$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(b)}, we have $L_{0}%
=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{\mu^{2}}{2}$. Since $a_{m}=m\dfrac
{\partial}{\partial x_{m}}$ and $a_{-m}=x_{m}$ for every integer $m>0$ (by the
definition of the action of $a_{m}$ on $F_{\mu}$), this rewrites as
$L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}}+\dfrac{\mu^{2}%
}{2}$. Now, since $f=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}^{\alpha_{3}%
}...$, every $m>0$ satisfies%
\begin{align*}
x_{m}m\dfrac{\partial}{\partial x_{m}}f  &  =x_{m}m\underbrace{\dfrac
{\partial}{\partial x_{m}}\left(  x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
x_{3}^{\alpha_{3}}...\right)  }_{\substack{=\alpha_{m}x_{1}^{\alpha_{1}}%
x_{2}^{\alpha_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}%
^{\alpha_{m+1}}x_{m+2}^{\alpha_{m+2}}...\\\text{(this term should be
understood as }0\text{ if }\alpha_{m}=0\text{)}}}\\
&  =x_{m}m\alpha_{m}x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}...x_{m-1}%
^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}}x_{m+2}^{\alpha
_{m+2}}...\\
&  =m\alpha_{m}\cdot\underbrace{x_{m}\cdot x_{1}^{\alpha_{1}}x_{2}^{\alpha
_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}%
}x_{m+2}^{\alpha_{m+2}}...}_{=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}}x_{m+1}^{\alpha_{m+1}}%
x_{m+2}^{\alpha_{m+2}}...=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}%
^{\alpha_{3}}...=f}=m\alpha_{m}f.
\end{align*}
Hence,
\begin{align*}
L_{0}f  &  =\sum\limits_{m>0}\underbrace{x_{m}m\dfrac{\partial}{\partial
x_{m}}f}_{=m\alpha_{m}f}+\dfrac{\mu^{2}}{2}f\ \ \ \ \ \ \ \ \ \ \left(
\text{since }L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}%
}+\dfrac{\mu^{2}}{2}\right) \\
&  =\underbrace{\sum\limits_{m>0}m\alpha_{m}}_{=-d}f+\dfrac{\mu^{2}}%
{2}f=-df+\dfrac{\mu^{2}}{2}f=\left(  \dfrac{\mu^{2}}{2}-d\right)  f.
\end{align*}
We thus have proven the equation $L_{0}f=\left(  \dfrac{\mu^{2}}{2}-d\right)
f$ for every monomial $f$ of degree $d$. As we said above, this completes the
proof of Remark \ref{rmk.fockvir.explicit} \textbf{(c)}.

\textbf{(d)} For every $m\in\mathbb{Z}$,%
\begin{equation}
\text{the map }a_{m}:F_{\mu}\rightarrow F_{\mu}\text{ is homogeneous of degree
}m\text{.} \label{pf.fockvir.explicit.5}%
\end{equation}
(In fact, this is easily seen from the definition of how $a_{m}$ acts on
$F_{\mu}$.)

Thus, for every $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$, the map $\left.
:a_{u}a_{v}:\right.  $ is homogeneous of degree $u+v$%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$.
By (\ref{pf.fockvir.explicit.5}) (applied to $m=u$), the map $a_{u}$ is
homogeneous of degree $u$. Similarly, the map $a_{v}$ is homogeneous of degree
$v$. Thus, the map $a_{u}a_{v}$ is homogeneous of degree $u+v$. Similarly, the
map $a_{v}a_{u}$ is homogeneous of degree $v+u=u+v$.
\par
Since $\left.  :a_{u}a_{v}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{u}a_{v},\ \ \ \ \ \ \ \ \ \ \text{if }u\leq v;\\
a_{v}a_{u},\ \ \ \ \ \ \ \ \ \ \text{if }u>v
\end{array}
\right.  $ (by the definition of normal ordered products), the map $\left.
:a_{u}a_{v}:\right.  $ equals one of the maps $a_{u}a_{v}$ and $a_{v}a_{u}$.
Since both of these maps $a_{u}a_{v}$ and $a_{v}a_{u}$ are homogeneous of
degree $u+v$, this yields that $\left.  :a_{u}a_{v}:\right.  $ is homogeneous
of degree $u+v$, qed.}. Applied to $u=-m$ and $v=n+m$, this yields: For every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the map $\left.  :a_{-m}a_{n+m}%
:\right.  $ is homogeneous of degree $\left(  -m\right)  +\left(  n+m\right)
=n$. Now, the map%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\underbrace{\left.
:a_{-m}a_{n+m}:\right.  }_{\text{this map is homogeneous of degree }n}%
\]
must be homogeneous of degree $n$. This proves Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}.

Now it turns out that the operators $L_{n}$ that we have defined give a
positive answer to question \textbf{1)}:

\begin{proposition}
\label{prop.fockvir.answer1}Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and $\mu
\in\mathbb{C}$. Then, $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$ (where $L_{n}$
is defined as in Definition \ref{def.fockvir}, and $a_{\ell}$ is shorthand
notation for $a_{\ell}\mid_{F_{\mu}}$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.fockvir.answer1}.} Since%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
=\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.  ,
\]
we have%
\begin{align}
\left[  L_{n},a_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{j\in
\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.  ,a_{m}\right]  =\dfrac{1}{2}%
\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-j}a_{n+j}%
:\right.  ,a_{m}\right]  }_{=-\left[  a_{m},\left.  :a_{-j}a_{n+j}:\right.
\right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  a_{m},\left.
:a_{-j}a_{n+j}:\right.  \right]  }_{\substack{=\left[  a_{m},a_{-j}%
a_{n+j}\right]  \\\text{(by Remark \ref{rmk.fockvir.normal.K} \textbf{(b)},
applied}\\\text{to }a_{m}\text{, }-j\text{ and }n+j\text{ instead of }x\text{,
}m\text{ and }n\text{)}}}=-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}%
}\underbrace{\left[  a_{m},a_{-j}a_{n+j}\right]  }_{=\left[  a_{m}%
,a_{-j}\right]  a_{n+j}+a_{-j}\left[  a_{m},a_{n+j}\right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  \underbrace{\left[
a_{m},a_{-j}\right]  }_{=m\delta_{m,-\left(  -j\right)  }K}a_{n+j}%
+a_{-j}\underbrace{\left[  a_{m},a_{n+j}\right]  }_{=m\delta_{m,-\left(
n+j\right)  }K}\right) \nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\underbrace{\delta
_{m,-\left(  -j\right)  }}_{=\delta_{m,j}}Ka_{n+j}+a_{-j}m\underbrace{\delta
_{m,-\left(  n+j\right)  }}_{=\delta_{-m,n+j}=\delta_{-m-n,j}}K\right)
\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\delta_{m,j}%
Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  . \label{pf.fockvir.answer1.2}%
\end{align}


But each of the two sums $\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}$
and $\sum\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K$ is
convergent\footnote{In fact, due to the factors $\delta_{m,j}$ and
$\delta_{-m-n,j}$ in the addends, it is clear that in each of these two sums,
only at most one addend can be nonzero. Concretely:%
\[
\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}=mKa_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K=a_{-\left(  -m-n\right)  }mK.
\]
}. Hence, we can split the sum $\sum\limits_{j\in\mathbb{Z}}\left(
m\delta_{m,j}Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  $ into $\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}+\sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K$. Thus, (\ref{pf.fockvir.answer1.2}) becomes%
\begin{align*}
\left[  L_{n},a_{m}\right]   &  =-\dfrac{1}{2}\left(  \underbrace{\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}}_{=mKa_{n+m}}+\underbrace{\sum
\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K}_{=a_{-\left(  -m-n\right)
}mK}\right)  =-\dfrac{1}{2}\left(  mKa_{n+m}+a_{-\left(  -m-n\right)
}mK\right) \\
&  =-\dfrac{1}{2}\left(  ma_{n+m}+a_{-\left(  -m-n\right)  }m\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }\operatorname*{id}%
\text{ on }F_{\mu}\right) \\
&  =-\dfrac{1}{2}m\left(  a_{n+m}+\underbrace{a_{-\left(  -m-n\right)  }%
}_{=a_{m+n}=a_{n+m}}\right)  =-\dfrac{1}{2}m\left(  a_{n+m}+a_{n+m}\right)
=-ma_{n+m}.
\end{align*}
This proves Proposition \ref{prop.fockvir.answer1}.

Now let us check whether our operators $L_{n}$ answer Question \textbf{2)}, or
at least try to do so. We are going to make some ``dirty'' arguments; cleaner
ones can be found in the proof of Proposition \ref{prop.fockvir.answer2} that
we give below.

First, it is easy to see that any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
satisfy%
\[
\left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m},a_{k}\right]
=0\ \ \ \ \ \ \ \ \ \ \text{for any }k\in\mathbb{Z}%
\]
\footnote{\textit{Proof.} Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and
$k\in\mathbb{Z}$. Then,%
\begin{align*}
&  \left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}%
,a_{k}\right] \\
&  =\underbrace{\left[  \left[  L_{n},L_{m}\right]  ,a_{k}\right]
}_{\substack{=\left[  \left[  L_{n},a_{k}\right]  ,L_{m}\right]  +\left[
L_{n},\left[  L_{m},a_{k}\right]  \right]  \\\text{(by the Leibniz identity
for commutators)}}}-\left(  n-m\right)  \left[  L_{n+m},a_{k}\right] \\
&  =\left[  \underbrace{\left[  L_{n},a_{k}\right]  }_{\substack{=-ka_{n+k}%
\\\text{(by Proposition \ref{prop.fockvir.answer1},}\\\text{applied to
}k\text{ instead of }m\text{)}}},L_{m}\right]  +\left[  L_{n}%
,\underbrace{\left[  L_{m},a_{k}\right]  }_{\substack{=-ka_{m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }m\text{ and
}k\\\text{instead of }n\text{ and }m\text{)}}}\right]  -\left(  n-m\right)
\underbrace{\left[  L_{n+m},a_{k}\right]  }_{\substack{=-ka_{n+m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }n+m\text{ and
}k\\\text{instead of }n\text{ and }m\text{)}}}\\
&  =-k\underbrace{\left[  a_{n+k},L_{m}\right]  }_{=-\left[  L_{m}%
,a_{n+k}\right]  }-k\left[  L_{n},a_{m+k}\right]  +\left(  n-m\right)
ka_{n+m+k}\\
&  =k\underbrace{\left[  L_{m},a_{n+k}\right]  }_{\substack{=-\left(
n+k\right)  a_{m+n+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m\text{ and }n+k\text{ instead of }n\text{ and
}m\text{)}}}-k\underbrace{\left[  L_{n},a_{m+k}\right]  }_{\substack{=-\left(
m+k\right)  a_{n+m+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m+k\text{ instead of }m\text{)}}}+\left(  n-m\right)
ka_{n+m+k}\\
&  =-k\left(  n+k\right)  \underbrace{a_{m+n+k}}_{=a_{n+m+k}}+k\left(
m+k\right)  a_{n+m+k}+\left(  n-m\right)  ka_{n+m+k}\\
&  =-k\left(  n+k\right)  a_{n+m+k}+k\left(  m+k\right)  a_{n+m+k}+\left(
n-m\right)  ka_{n+m+k}\\
&  =\underbrace{\left(  -k\left(  n+k\right)  +k\left(  m+k\right)  +\left(
n-m\right)  k\right)  }_{=0}a_{n+m+k}=0.
\end{align*}
Qed.}. Hence, for any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the endomorphism
$\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ of $F_{\mu}$ is an
$\mathcal{A}$-module homomorphism (since $\left[  \left[  L_{n},L_{m}\right]
-\left(  n-m\right)  L_{n+m},K\right]  =0$ also holds, for obvious reasons).
Since $F_{\mu}$ is an irreducible $\mathcal{A}$-module of countable dimension,
this yields (by Lemma \ref{lem.dix}) that, for any $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, the map $\left[  L_{n},L_{m}\right]  -\left(  n-m\right)
L_{n+m}:F_{\mu}\rightarrow F_{\mu}$ is a scalar multiple of the identity. But
since this map $\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ must
also be homogeneous of degree $n+m$ (by an application of Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}), this yields that $\left[
L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=0$ whenever $n+m\neq0$
(because any homogeneous map of degree $\neq0$ which is, at the same time, a
scalar multiple of the identity, must be the $0$ map). Thus, for every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we can write%
\begin{equation}
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\gamma_{n}%
\delta_{n,-m}\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for some }\gamma
_{n}\in\mathbb{C}\text{ depending on }n\text{.} \label{pf.fockvir.answer2.1}%
\end{equation}
We can get some more information about these $\gamma_{n}$ if we consider the
Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}\cup\left(
\operatorname*{id}\right)  $\ \ \ \ \footnote{This is well-defined because (as
the reader can easily check) the family $\left(  L_{n}\right)  _{n\in
\mathbb{Z}}\cup\left(  \operatorname*{id}\right)  $ of operators on $F_{\mu}$
is linearly independent.}. (Note that, according to Convention
\ref{conv.fockvir.L}, these $L_{n}$ still denote maps from $F_{\mu}$ to
$F_{\mu}$, rather than elements of $\operatorname*{Vir}$ or $W$. Of course,
this Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}%
\cup\left(  \operatorname*{id}\right)  $ \textbf{will} turn out to be
isomorphic to $\operatorname*{Vir}$, but we have not yet proven this.) This
Lie algebra, due to the formula (\ref{pf.fockvir.answer2.1}) and to the fact
that $\operatorname*{id}$ commutes with everything, must be a $1$-dimensional
central extension of the Witt algebra. Hence, the map
\[
W\times W\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \left(  L_{n},L_{m}\right)
\mapsto\gamma_{n}\delta_{n,-m}%
\]
(where $L_{n}$ and $L_{m}$ really mean the elements $L_{n}$ and $L_{m}$ of $W$
this time) must be a $2$-cocycle on $W$. But since we know (from Theorem
\ref{thm.H^2(W)}) that every $2$-cocycle on $W$ is a scalar multiple of the
$2$-cocycle $\omega$ defined in Theorem \ref{thm.H^2(W)} modulo the
$2$-coboundaries, this yields that this $2$-cocycle is a scalar multiple of
$\omega$ modulo the $2$-coboundaries. In other words, there exist
$c\in\mathbb{C}$ and $\xi\in W^{\ast}$ such that%
\[
\gamma_{n}\delta_{n,-m}=c\omega\left(  L_{n},L_{m}\right)  +\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Since $\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}$,
this rewrites as%
\[
\gamma_{n}\delta_{n,-m}=c\dfrac{n^{3}-n}{6}\delta_{n,-m}+\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Applied to $m=-n$, this yields%
\begin{equation}
\gamma_{n}=c\dfrac{n^{3}-n}{6}+\xi\left(  \underbrace{\left[  L_{n}%
,L_{-n}\right]  }_{=2nL_{0}}\right)  =c\dfrac{n^{3}-n}{6}+2n\xi\left(
L_{0}\right)  . \label{pf.fockvir.answer2.2}%
\end{equation}


All that remains now, in order to get the values of $\left[  L_{n}%
,L_{m}\right]  -\left(  n-m\right)  L_{n+m}$, is to compute the scalars $c$
and $\xi\left(  L_{0}\right)  $. For this, we only need to compute $\gamma
_{1}$ and $\gamma_{2}$ (because this will give $2$ linear equations for $c$
and $L_{0}$). In order to do this, we will evaluate the endomorphisms $\left[
L_{1},L_{-1}\right]  -2L_{0}$ and $\left[  L_{2},L_{-2}\right]  -4L_{0}$ at
the element $1$ of $F_{\mu}$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(c)} (applied to $d=0$ and
$f=1$), we get $L_{0}1=\left(  \dfrac{\mu^{2}}{2}-0\right)  1=\dfrac{\mu^{2}%
}{2}$.

Since $L_{1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{1+m}:\right.  $, we have $L_{1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{1+m}:\right.  1=0$ (because, as it is easily seen, $\left.
:a_{-m}a_{1+m}:\right.  1=0$ for every $m\in\mathbb{Z}$). Similarly,
$L_{2}1=0$.

Since $L_{-1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{-1+m}:\right.  $, we have $L_{-1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{-1+m}:\right.  1$. It is easy to see that the only
$m\in\mathbb{Z}$ for which $\left.  :a_{-m}a_{-1+m}:\right.  1$ is nonzero are
$m=0$ and $m=1$. Hence,
\[
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-1+m}:\right.
1=\underbrace{\left.  :a_{-0}a_{-1+0}:\right.  1}_{=\left.  :a_{0}%
a_{-1}:\right.  1=a_{-1}a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}+\underbrace{\left.
:a_{-1}a_{-1+1}:\right.  1}_{=\left.  :a_{-1}a_{0}:\right.  1=a_{-1}%
a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}=\mu x_{1}+\mu x_{1}=2\mu x_{1},
\]
so that $L_{-1}1=\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-1+m}:\right.  1}_{=2\mu x_{1}}=\mu x_{1}$. Thus,%
\begin{align*}
L_{1}L_{-1}1  &  =L_{1}\mu x_{1}=\mu\underbrace{L_{1}}_{=\dfrac{1}{2}%
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{1+m}:\right.  }x_{1}=\mu
\cdot\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{1+m}:\right.  x_{1}}_{\substack{=\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}+\left.  :a_{-0}a_{1+0}:\right.
x_{1}\\\text{(in fact, it is easy to see that the only}\\m\in\mathbb{Z}\text{
for which }\left.  :a_{-m}a_{1+m}:\right.  x_{1}\neq0\text{ are }m=-1\text{
and }m=0\text{)}}}\\
&  =\mu\cdot\dfrac{1}{2}\left(  \underbrace{\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}}_{=\left.  :a_{1}a_{0}:\right.
x_{1}=a_{0}a_{1}x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}+\underbrace{\left.  :a_{-0}a_{1+0}:\right.  x_{1}}_{=\left.  :a_{0}%
a_{1}:\right.  x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}\right) \\
&  =\mu\cdot\dfrac{1}{2}\left(  \mu+\mu\right)  =\mu^{2}.
\end{align*}


A similar (but messier) computation works for $L_{2}L_{-2}1$: Since
$L_{-2}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-2+m}%
:\right.  $, we have $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-2+m}:\right.  1$. It is easy to see that the only $m\in\mathbb{Z}$
for which $\left.  :a_{-m}a_{-2+m}:\right.  1$ is nonzero are $m=0$, $m=1$ and
$m=2$. This allows us to simplify $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{-2+m}:\right.  1$ to $L_{-2}1=\mu x_{2}+\dfrac
{1}{2}x_{1}^{2}$ (the details are left to the reader). Thus,%
\[
L_{2}L_{-2}1=L_{2}\left(  \mu x_{2}+\dfrac{1}{2}x_{1}^{2}\right)  =\mu
L_{2}x_{2}+\dfrac{1}{2}L_{2}x_{1}^{2}.
\]
Straightforward computations, which I omit, show that $L_{2}x_{2}=2\mu$ and
$L_{2}x_{1}^{2}=1$. Hence,%
\[
L_{2}L_{-2}1=\mu\underbrace{L_{2}x_{2}}_{=2\mu}+\dfrac{1}{2}\underbrace{L_{2}%
x_{1}^{2}}_{=1}=2\mu^{2}+\dfrac{1}{2}.
\]


Now,%
\[
\left(  \left[  L_{1},L_{-1}\right]  -2L_{0}\right)  1=\underbrace{L_{1}%
L_{-1}1}_{=\mu^{2}}-L_{-1}\underbrace{L_{1}1}_{=0}-2\underbrace{L_{0}%
1}_{=\dfrac{\mu^{2}}{2}}=\mu^{2}-0-2\cdot\dfrac{\mu^{2}}{2}=0.
\]
Since%
\begin{align*}
\left[  L_{1},L_{-1}\right]  -2L_{0}  &  =\gamma_{1}\underbrace{\delta
_{1,-\left(  -1\right)  }}_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.fockvir.answer2.1}), applied to }n=1\text{ and }m=-1\right) \\
&  =\gamma_{1}=c\underbrace{\dfrac{1^{3}-1}{6}}_{=0}+2\cdot1\cdot\xi\left(
L_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.2}%
), applied to }n=1\right) \\
&  =0+2\cdot1\cdot\xi\left(  L_{0}\right)  =2\xi\left(  L_{0}\right)  ,
\end{align*}
this rewrites as $2\xi\left(  L_{0}\right)  \cdot1=0$, so that $\xi\left(
L_{0}\right)  =0$.

On the other hand,%
\[
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1=\underbrace{L_{2}%
L_{-2}1}_{=2\mu^{2}+\dfrac{1}{2}}-L_{-2}\underbrace{L_{2}1}_{=0}%
-4\underbrace{L_{0}1}_{=\dfrac{\mu^{2}}{2}}=\left(  2\mu^{2}+\dfrac{1}%
{2}\right)  -0-4\cdot\dfrac{\mu^{2}}{2}=\dfrac{1}{2}.
\]
Since%
\begin{align*}
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1  &  =\gamma
_{2}\underbrace{\delta_{2,-\left(  -2\right)  }}_{=1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.1}), applied to
}n=2\text{ and }m=-2\right) \\
&  =\gamma_{2}=c\underbrace{\dfrac{2^{3}-2}{6}}_{=1}+2\cdot2\cdot
\underbrace{\xi\left(  L_{0}\right)  }_{=0}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.fockvir.answer2.2}), applied to }n=2\right) \\
&  =c+0=c,
\end{align*}
this rewrites as $c=\dfrac{1}{2}$.

Due to $\xi\left(  L_{0}\right)  =0$ and $c=\dfrac{1}{2}$, we can rewrite
(\ref{pf.fockvir.answer2.2}) as
\[
\gamma_{n}=\dfrac{1}{2}\cdot\dfrac{n^{3}-n}{6}+2n0=\dfrac{n^{3}-n}{12}.
\]
Hence, (\ref{pf.fockvir.answer2.1}) becomes%
\[
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\dfrac{n^{3}-n}%
{12}\delta_{n,-m}\operatorname*{id}.
\]
We have thus proven:

\begin{proposition}
\label{prop.fockvir.answer2}For any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we
have%
\begin{equation}
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}-n}%
{12}\delta_{n,-m}\operatorname*{id} \label{prop.fockvir.answer2.form}%
\end{equation}
(where $L_{n}$ and $L_{m}$ are maps $F_{\mu}\rightarrow F_{\mu}$ as explained
in Convention \ref{conv.fockvir.L}). Thus, we can make $F_{\mu}$ a
representation of $\operatorname*{Vir}$ by letting the element $L_{n}$ of
$\operatorname*{Vir}$ act as the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$, and letting the element $C$ of $\operatorname*{Vir}$
act as $\operatorname*{id}$.
\end{proposition}

Due to Proposition \ref{prop.fockvir.answer1}, this $\operatorname*{Vir}%
$-action harmonizes with the $\mathcal{A}$-action on $F_{\mu}$:

\begin{proposition}
The $\mathcal{A}$-action on $F_{\mu}$ extends (essentially uniquely) to an
action of $\operatorname*{Vir}\ltimes\mathcal{A}$ on $F_{\mu}$ with $C$ acting
as $1$.
\end{proposition}

This is the reason why the construction of the Virasoro algebra involved the
$2$-cocycle $\dfrac{1}{2}\omega$ rather than $\omega$ (or, actually, rather
than simpler-looking $2$-cocycles like $\left(  L_{n},L_{m}\right)  \mapsto
n^{3}\delta_{n,-m}$).

Our proof of Proposition \ref{prop.fockvir.answer2} above was rather insidious
and nonconstructive: We used the Dixmier theorem to prove (what boils down to)
an algebraic identity, and later we used Theorem \ref{thm.H^2(W)} (which is
constructive but was applied in a rather unexpected way) to reduce our
computations to two concrete cases. We will now show a different, more direct
proof of Proposition \ref{prop.fockvir.answer2}:\footnote{The following proof
is a slight variation of the proof given in the Kac-Raina book (where our
Proposition \ref{prop.fockvir.answer2} is Proposition 2.3).}

\textit{Second proof of Proposition \ref{prop.fockvir.answer2}.} Let
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. By (\ref{def.fockvir.def}) (with the
index $m$ renamed as $\ell$), we have $L_{n}=\dfrac{1}{2}\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{n+\ell}:\right.  $. Hence,%
\begin{align}
\left[  L_{n},L_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{n+\ell}:\right.  ,L_{m}\right]  =\dfrac
{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-\ell
}a_{n+\ell}:\right.  ,L_{m}\right]  }_{=-\left[  L_{m},\left.  :a_{-\ell
}a_{n+\ell}:\right.  \right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  . \label{pf.fockvir.answer2.pf0}%
\end{align}


Now, let $\ell\in\mathbb{Z}$. Then, we obtain $\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  =\left[  L_{m},a_{-\ell}a_{n+\ell
}\right]  $ (more or less by applying Remark \ref{rmk.fockvir.normal.K}
\textbf{(b)} to $L_{m}$, $-\ell$ and $n+\ell$ instead of $x$, $m$ and
$n$\ \ \ \ \footnote{I am saying ``more or less'' because this is not
completely correct: We cannot apply Remark \ref{rmk.fockvir.normal.K}
\textbf{(b)} to $L_{m}$, $-\ell$ and $n+\ell$ instead of $x$, $m$ and $n$
(since $L_{m}$ does not lie in $U\left(  \mathcal{A}\right)  $). However,
there are two ways to get around this obstruction:
\par
One way is to generalize Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to a
suitable completion of $U\left(  \mathcal{A}\right)  $. We will not do this
here.
\par
Another way is to notice that we can replace $U\left(  \mathcal{A}\right)  $
by $\operatorname*{End}\left(  F_{\mu}\right)  $ throughout Remark
\ref{rmk.fockvir.normal.K}. (This, of course, means that $a_{n}$ and $a_{m}$
have to be reinterpreted as endomorphisms of $F_{\mu}$ rather than elements of
$\mathcal{A}$; but since the action of $\mathcal{A}$ on $F_{\mu}$ is a Lie
algebra representation, all equalities that hold in $U\left(  \mathcal{A}%
\right)  $ remain valid in $\operatorname*{End}\left(  F_{\mu}\right)  $.) The
proof of Remark \ref{rmk.fockvir.normal.K} still works after this replacement
(except that $\left[  x,K\right]  =0$ should no longer be proven using the
argument $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(  U\left(
\mathcal{A}\right)  \right)  $, but simply follows from the fact that $K$ acts
as the identity on $F_{\mu}$). Now, after this replacement, we \textbf{can}
apply Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to $L_{m}$, $-\ell$ and
$n+\ell$ instead of $x$, $m$ and $n$, and we obtain $\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  =\left[  L_{m},a_{-\ell}a_{n+\ell
}\right]  $.}), so that%
\begin{align*}
\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right]   &  =\left[
L_{m},a_{-\ell}a_{n+\ell}\right] \\
&  =\underbrace{\left[  L_{m},a_{-\ell}\right]  }_{\substack{=-\left(
-\ell\right)  a_{m+\left(  -\ell\right)  }\\\text{(by Proposition
\ref{prop.fockvir.answer1}}\\\text{(applied to }m\text{ and }-\ell\text{
instead of }n\text{ and }m\text{))}}}a_{n+\ell}+a_{-\ell}\underbrace{\left[
L_{m},a_{n+\ell}\right]  }_{\substack{=-\left(  n+\ell\right)  a_{m+\left(
n+\ell\right)  }\\\text{(by Proposition \ref{prop.fockvir.answer1}%
}\\\text{(applied to }m\text{ and }n+\ell\text{ instead of }n\text{ and
}m\text{))}}}\\
&  =\underbrace{-\left(  -\ell\right)  }_{=\ell}\underbrace{a_{m+\left(
-\ell\right)  }}_{=a_{m-\ell}}a_{n+\ell}+\underbrace{a_{-\ell}\left(  -\left(
n+\ell\right)  a_{m+\left(  n+\ell\right)  }\right)  }_{=-\left(
n+\ell\right)  a_{-\ell}a_{m+n+\ell}}\\
&  =\ell a_{m-\ell}a_{n+\ell}-\left(  n+\ell\right)  a_{-\ell}a_{m+n+\ell}.
\end{align*}
Since $a_{m-\ell}a_{n+\ell}=\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(
n+\ell\right)  \left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}%
$\ \ \ \ \footnote{because Remark \ref{rmk.fockvir.normal.K} \textbf{(a)}
(applied to $m-\ell$ and $n+\ell$ instead of $m$ and $n$) yields
\begin{align*}
\left.  :a_{m-\ell}a_{n+\ell}:\right.   &  =a_{m-\ell}a_{n+\ell}+\left(
n+\ell\right)  \underbrace{\left[  m-\ell>0\right]  }_{=\left[  \ell<m\right]
}\underbrace{\delta_{m-\ell,-\left(  n+\ell\right)  }}_{=\delta_{m-\ell
,-n-\ell}=\delta_{m,-n}}\underbrace{K}_{\substack{=\operatorname*{id}%
\\\text{(since }K\text{ acts as }\operatorname*{id}\text{ on }F_{\mu}\text{)}%
}}\\
&  =a_{m-\ell}a_{n+\ell}+\left(  n+\ell\right)  \left[  \ell<m\right]
\delta_{m,-n}\operatorname*{id}%
\end{align*}
} and $a_{-\ell}a_{m+n+\ell}=\left.  :a_{-\ell}a_{m+n+\ell}:\right.
-\ell\left[  \ell<0\right]  \delta_{m,-n}\operatorname*{id}$%
\ \ \ \ \footnote{because Remark \ref{rmk.fockvir.normal.K} \textbf{(a)}
(applied to $\ell$ and $n+m+\ell$ instead of $m$ and $n$) yields
\begin{align*}
\left.  :a_{-\ell}a_{m+n+\ell}:\right.   &  =a_{-\ell}a_{m+n+\ell}+\left(
m+n+\ell\right)  \underbrace{\left[  -\ell>0\right]  }_{=\left[
\ell<0\right]  }\underbrace{\delta_{-\ell,-\left(  m+n+\ell\right)  }%
}_{=\delta_{-\ell,-m-n-\ell}=\delta_{m,-n}}\underbrace{K}%
_{\substack{=\operatorname*{id}\\\text{(since }K\text{ acts as }%
\operatorname*{id}\text{ on }F_{\mu}\text{)}}}\\
&  =a_{-\ell}a_{m+n+\ell}+\underbrace{\left(  m+n+\ell\right)  \left[
\ell<0\right]  }_{=\left[  \ell<0\right]  \left(  m+n+\ell\right)  }%
\delta_{m,-n}\operatorname*{id}\\
&  =a_{-\ell}a_{m+n+\ell}+\left[  \ell<0\right]  \underbrace{\left(
m+n+\ell\right)  \delta_{m,-n}}_{\substack{=\ell\delta_{m,-n}\\\text{(this can
be easily proven by treating}\\\text{the cases of }m=-n\text{ and of }%
m\neq-n\text{ separately)}}}\operatorname*{id}\\
&  =a_{-\ell}a_{m+n+\ell}+\underbrace{\left[  \ell<0\right]  \ell}%
_{=\ell\left[  \ell<0\right]  }\delta_{m,-n}\operatorname*{id}=a_{-\ell
}a_{m+n+\ell}+\ell\left[  \ell<0\right]  \delta_{m,-n}\operatorname*{id}%
\end{align*}
}, this equation rewrites as%
\begin{align}
&  \left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right] \nonumber\\
&  =\ell\underbrace{a_{m-\ell}a_{n+\ell}}_{=\left.  :a_{m-\ell}a_{n+\ell
}:\right.  -\left(  n+\ell\right)  \left[  \ell<m\right]  \delta
_{m,-n}\operatorname*{id}}-\left(  n+\ell\right)  \underbrace{a_{-\ell
}a_{m+n+\ell}}_{=\left.  :a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}}\nonumber\\
&  =\ell\left(  \left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}\right)  -\left(
n+\ell\right)  \left(  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}\right) \nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\ell\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}-\left(  n+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right.  +\left(  n+\ell\right)  \ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\underbrace{\left(
n+\ell\right)  }_{=\left(  n-m\right)  +\left(  m+\ell\right)  }\left.
:a_{-\ell}a_{m+n+\ell}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  n+\ell\right)  \ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}-\ell\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}}_{=\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\underbrace{\left(  \left(
n-m\right)  +\left(  m+\ell\right)  \right)  \left.  :a_{-\ell}a_{m+n+\ell
}:\right.  }_{=\left(  n-m\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.
+\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n-m\right)  \left.
:a_{-\ell}a_{m+n+\ell}:\right.  -\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id} \label{pf.fockvir.answer2.pf2}%
\end{align}


Now forget that we fixed $\ell$. We want to use the equality
(\ref{pf.fockvir.answer2.pf2}) in order to split the infinite sum
$\sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell
}:\right.  \right]  $ on the right hand side of (\ref{pf.fockvir.answer2.pf0})
into
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  -\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right. \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}.
\end{align*}
But before we can do this, we must check that this splitting is allowed (since
infinite sums cannot always be split: e. g., the sum $\sum\limits_{\ell
\in\mathbb{Z}}\left(  1-1\right)  $ is well-defined (and has value $0$), but
splitting it into $\sum\limits_{\ell\in\mathbb{Z}}1-\sum\limits_{\ell
\in\mathbb{Z}}1$ is not allowed). Clearly, in order to check this, it is
enough to check that the four infinite sums $\sum\limits_{\ell\in\mathbb{Z}%
}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $, $\sum\limits_{\ell\in
\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $, $\sum\limits_{\ell
\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  $
and $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}$ converge.

Before we do this, let us formalize what we mean by ``converge'': We consider
the product topology on the set $\left(  F_{\mu}\right)  ^{F_{\mu}}$ (the set
of all maps $F_{\mu}\rightarrow F_{\mu}$) by viewing this set as
$\prod\limits_{v\in F_{\mu}}F_{\mu}$, where each $F_{\mu}$ is endowed with the
discrete topology. With respect to this topology, a net $\left(  f_{i}\right)
_{i\in I}$ of maps $f_{i}:F_{\mu}\rightarrow F_{\mu}$ converges to a map
$f:F_{\mu}\rightarrow F_{\mu}$ if and only if%
\[
\left(
\begin{array}
[c]{c}%
\text{for every }v\in F_{\mu}\text{, the net of values }\left(  f_{i}\left(
v\right)  \right)  _{i\in I}\text{ converges to }f\left(  v\right)  \in
F_{\mu}\\
\text{with respect to the discrete topology on }F_{\mu}%
\end{array}
\right)  .
\]
Hence, with respect to this topology, an infinite sum $\sum\limits_{\ell
\in\mathbb{Z}}f_{\ell}$ of maps $f_{\ell}:F_{\mu}\rightarrow F_{\mu}$
converges if and only if%
\[
\left(  \text{for every }v\in F_{\mu}\text{, all but finitely many }\ell
\in\mathbb{Z}\text{ satisfy }f_{\ell}\left(  v\right)  =0\right)  .
\]
Hence, this is exactly the notion of convergence which we used in Definition
\ref{def.fockvir} to make sense of the infinite sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $.

Now, we are going to show that the infinite sums $\sum\limits_{\ell
\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $, $\sum
\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $,
$\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ and $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}$ converge with respect to this topology.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{m+n+\ell}:\right.  $\textit{:} For every $v\in F_{\mu}$, all but
finitely many $\ell\in\mathbb{Z}$ satisfy $\left.  :a_{-\ell}a_{m+n+\ell
}:\right.  v=0$ (by Lemma \ref{lem.fockvir.welldef} \textbf{(c)}, applied to
$m+n$ and $\ell$ instead of $n$ and $m$). Hence, the sum $\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}\left(
m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  $\textit{:} For every
$v\in F_{\mu}$, all but finitely many $\ell\in\mathbb{Z}$ satisfy $\left.
:a_{-\ell}a_{m+n+\ell}:\right.  v=0$ (by Lemma \ref{lem.fockvir.welldef}
\textbf{(c)}, applied to $m+n$ and $\ell$ instead of $n$ and $m$). Hence, for
every $v\in F_{\mu}$, all but finitely many $\ell\in\mathbb{Z}$ satisfy
$\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  =0$. Thus, the
sum $\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}%
\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $\textit{:} We know that the sum
$\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ converges. Thus, we have%
\begin{align}
\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.   &  =\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left(
m+\left(  \ell-m\right)  \right)  }_{=\ell}\left.  :\underbrace{a_{-\left(
\ell-m\right)  }}_{=a_{m-\ell}}\underbrace{a_{m+n+\left(  \ell-m\right)  }%
}_{=a_{n+\ell}}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\ell-m\text{ for
}\ell\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
. \label{pf.fockvir.answer2.pf5}%
\end{align}
Hence, the sum $\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell
}a_{n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}%
\ell\left(  n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[
\ell<m\right]  \right)  \delta_{m,-n}\operatorname*{id}$\textit{:} It is easy
to see that:

\begin{itemize}
\item Every sufficiently small $\ell\in\mathbb{Z}$ satisfies $\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}=0$.\ \ \ \ \footnote{\textit{Proof.} Every
sufficiently small $\ell\in\mathbb{Z}$ satisfies $\ell<0$ and $\ell<m$ and
thus%
\[
\ell\left(  n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]
}_{=1\text{ (since }\ell<0\text{)}}-\underbrace{\left[  \ell<m\right]
}_{=1\text{ (since }\ell<m\text{)}}\right)  \delta_{m,-n}\operatorname*{id}%
=\ell\left(  n+\ell\right)  \underbrace{\left(  1-1\right)  }_{=0}%
\delta_{m,-n}\operatorname*{id}=0.
\]
}

\item Every sufficiently high $\ell\in\mathbb{Z}$ satisfies $\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}=0$.\ \ \ \ \footnote{\textit{Proof.} Every
sufficiently high $\ell\in\mathbb{Z}$ satisfies $\ell\geq0$ and $\ell\geq m$
and thus%
\[
\ell\left(  n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]
}_{=0\text{ (since }\ell\geq0\text{)}}-\underbrace{\left[  \ell<m\right]
}_{=0\text{ (since }\ell\geq m\text{)}}\right)  \delta_{m,-n}%
\operatorname*{id}=\ell\left(  n+\ell\right)  \underbrace{\left(  0-0\right)
}_{=0}\delta_{m,-n}\operatorname*{id}=0.
\]
}
\end{itemize}

Combining these two results, we conclude that all but finitely many $\ell
\in\mathbb{Z}$ satisfy $\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}=0$. The sum $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}$ therefore converges.

We now know that all four sums that we care about converge, and that two of
them have the same value (by (\ref{pf.fockvir.answer2.pf5})). Let us compute
the other two of the sums:

First of all, by (\ref{def.fockvir.def}) (with the index $m$ renamed as $\ell
$), we have $L_{n}=\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{n+\ell}:\right.  $. Applying this to $m+n$ instead of $n$, we get%
\begin{equation}
L_{m+n}=\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  . \label{pf.fockvir.answer2.pf7}%
\end{equation}
This gives us the value of one of the sums we need.

Finally, let us notice that
\begin{equation}
\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}=-\dfrac{n^{3}-n}{6}\delta_{m,-n}\operatorname*{id}.
\label{pf.fockvir.answer2.pf6}%
\end{equation}


\begin{vershort}
In fact, proving this is a completely elementary computation
exercise\footnote{Indeed, both sides of this equation are $0$ when $m\neq-n$,
so the only nontrivial case is the case when $m=-n$. This case splits further
into two subcases: $m\geq0$ and $m<0$. In the first of these two subcases, the
left hand side of (\ref{pf.fockvir.answer2.pf6}) simplifies as $-\sum
\limits_{\ell=0}^{m-1}\ell\left(  n+\ell\right)  \operatorname*{id}$; in the
second, it simplifies as $\sum\limits_{\ell=m}^{-1}\ell\left(  n+\ell\right)
\operatorname*{id}$. The rest is straightforward computation.}.
\end{vershort}

\begin{verlong}
Indeed, proving this is a completely straightforward
exercise\footnote{\textit{Proof of (\ref{pf.fockvir.answer2.pf6}).} We must be
in one of the following three cases:
\par
\textit{Case 1:} We have $m\neq-n$.
\par
\textit{Case 2:} We have $m=-n$ and $m\geq0$.
\par
\textit{Case 3:} We have $m=-n$ and $m<0$.
\par
First, let us consider Case 1. In this case, $m\neq-n$, so that $\delta
_{m,-n}=0$. Hence, $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)
\left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\underbrace{\delta_{m,-n}}_{=0}\operatorname*{id}=\sum\limits_{\ell
\in\mathbb{Z}}0=0$ and $-\dfrac{n^{3}-n}{6}\underbrace{\delta_{m,-n}}%
_{=0}\operatorname*{id}=0$. This shows that $\sum\limits_{\ell\in\mathbb{Z}%
}\ell\left(  n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[
\ell<m\right]  \right)  \delta_{m,-n}\operatorname*{id}=-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}$. Thus, (\ref{pf.fockvir.answer2.pf6}) is
proven in Case 1.
\par
Next, let us consider Case 2. In this case, $m=-n$ and $m\geq0$. Since $m=-n$,
we have $\delta_{m,-n}=1$. Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \underbrace{\delta_{m,-n}%
}_{=1}\operatorname*{id}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \operatorname*{id}\\
&  =\sum\limits_{\ell=-\infty}^{-1}\ell\left(  n+\ell\right)  \left(
\underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }\ell<0\leq m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=0}^{m-1}\ell\left(  n+\ell\right)
\left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since }\ell
\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }%
\ell<m\text{)}}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=m}^{\infty}\ell\left(
n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since
}\ell\geq m\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=0\text{
(since }\ell\geq m\text{)}}\right)  \operatorname*{id}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\geq0\right) \\
&  =\sum\limits_{\ell=-\infty}^{-1}\ell\left(  n+\ell\right)
\underbrace{\left(  1-1\right)  }_{=0}\operatorname*{id}+\sum\limits_{\ell
=0}^{m-1}\ell\left(  \underbrace{n}_{\substack{=-m\\\text{(since }%
m=-n\text{)}}}+\ell\right)  \underbrace{\left(  0-1\right)  }_{=-1}%
\operatorname*{id}+\sum\limits_{\ell=m}^{\infty}\ell\left(  n+\ell\right)
\underbrace{\left(  0-0\right)  }_{=0}\operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=-\infty}^{-1}0}_{=0}+\sum\limits_{\ell
=0}^{m-1}\underbrace{\ell\left(  -m+\ell\right)  \left(  -1\right)  }%
_{=m\ell-\ell^{2}}\operatorname*{id}+\underbrace{\sum\limits_{\ell=m}^{\infty
}0}_{=0}=\underbrace{\sum\limits_{\ell=0}^{m-1}\left(  m\ell-\ell^{2}\right)
}_{=m\sum\limits_{\ell=0}^{m-1}\ell-\sum\limits_{\ell=0}^{m-1}\ell^{2}%
}\operatorname*{id}\\
&  =\left(  m\underbrace{\sum\limits_{\ell=0}^{m-1}\ell}_{\substack{=\dfrac
{\left(  m-1\right)  m}{2}\\\text{(by standard formulas)}}}-\underbrace{\sum
\limits_{\ell=0}^{m-1}\ell^{2}}_{\substack{=\dfrac{\left(  m-1\right)
m\left(  2m-1\right)  }{6}\\\text{(by standard formulas)}}}\right)
\operatorname*{id}=\underbrace{\left(  m\cdot\dfrac{\left(  m-1\right)  m}%
{2}-\dfrac{\left(  m-1\right)  m\left(  2m-1\right)  }{6}\right)
}_{\substack{=\dfrac{m^{3}-m}{6}=\dfrac{\left(  -n\right)  ^{3}-\left(
-n\right)  }{6}\\\text{(since }m=-n\text{)}}}\operatorname*{id}\\
&  =\underbrace{\dfrac{\left(  -n\right)  ^{3}-\left(  -n\right)  }{6}%
}_{=-\dfrac{n^{3}-n}{6}\cdot1}\operatorname*{id}=-\dfrac{n^{3}-n}{6}%
\cdot\underbrace{1}_{=\delta_{m,-n}}\operatorname*{id}=-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}.
\end{align*}
\par
Thus, (\ref{pf.fockvir.answer2.pf6}) is proven in Case 2.
\par
Finally, let us consider Case 3. In this case, $m=-n$ and $m<0$. Since $m=-n$,
we have $\delta_{m,-n}=1$ and $-m=n$. Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \underbrace{\delta_{m,-n}%
}_{=1}\operatorname*{id}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \operatorname*{id}\\
&  =\sum\limits_{\ell=-\infty}^{m-1}\ell\left(  n+\ell\right)  \left(
\underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<m<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }\ell<m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=m}^{-1}\ell\left(  n+\ell\right)
\left(  \underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=0\text{ (since }\ell\geq m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=0}^{\infty}\ell\left(
n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since
}\ell\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=0\text{ (since
}\ell\geq0>m\text{)}}\right)  \operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }m<0\right) \\
&  =\sum\limits_{\ell=-\infty}^{m-1}\ell\left(  n+\ell\right)
\underbrace{\left(  1-1\right)  }_{=0}\operatorname*{id}+\sum\limits_{\ell
=m}^{-1}\ell\left(  n+\ell\right)  \underbrace{\left(  1-0\right)  }%
_{=1}\operatorname*{id}+\sum\limits_{\ell=0}^{\infty}\ell\left(
n+\ell\right)  \underbrace{\left(  0-0\right)  }_{=0}\operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=-\infty}^{m-1}0}_{=0}+\sum\limits_{\ell
=m}^{-1}\ell\left(  n+\ell\right)  1\operatorname*{id}+\underbrace{\sum
\limits_{\ell=0}^{\infty}0}_{=0}=\sum\limits_{\ell=m}^{-1}\ell\left(
n+\ell\right)  1\operatorname*{id}=\sum\limits_{\ell=m}^{-1}\ell\left(
n+\ell\right)  \operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=1}^{-m}}_{\substack{=\sum\limits_{\ell
=1}^{n}\\\text{(since }-m=n\text{)}}}\underbrace{\left(  -\ell\right)  \left(
n+\left(  -\ell\right)  \right)  }_{=\ell^{2}-n\ell}\operatorname*{id}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\ell\text{ for }%
-\ell\text{ in the sum}\right) \\
&  =\underbrace{\sum\limits_{\ell=1}^{n}\left(  \ell^{2}-n\ell\right)
}_{=\sum\limits_{\ell=1}^{n}\ell^{2}-n\sum\limits_{\ell=1}^{n}\ell
}\operatorname*{id}=\left(  \underbrace{\sum\limits_{\ell=1}^{n}\ell^{2}%
}_{\substack{=\dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }{6}\\\text{(by
standard formulas)}}}-n\underbrace{\sum\limits_{\ell=1}^{n}\ell}%
_{\substack{=\dfrac{n\left(  n+1\right)  }{2}\\\text{(by standard formulas)}%
}}\right)  \operatorname*{id}\\
&  =\underbrace{\left(  \dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }%
{6}-n\cdot\dfrac{n\left(  n+1\right)  }{2}\right)  }_{=-\dfrac{n^{3}-n}{6}%
}\underbrace{\operatorname*{id}}_{=1\operatorname*{id}}=-\dfrac{n^{3}-n}%
{6}\underbrace{1}_{=\delta_{m,-n}}\operatorname*{id}=-\dfrac{n^{3}-n}{6}%
\delta_{m,-n}\operatorname*{id}.
\end{align*}
\par
Thus, (\ref{pf.fockvir.answer2.pf6}) is proven in Case 3.
\par
We have therefore proven (\ref{pf.fockvir.answer2.pf6}) in each of the three
cases 1, 2 and 3. Since these three cases cover all possibilities, this
completes the proof of (\ref{pf.fockvir.answer2.pf6}).}.
\end{verlong}

Now, since (\ref{pf.fockvir.answer2.pf2}) holds for every $\ell\in\mathbb{Z}$,
we have
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell
}:\right.  \right] \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\left(  \ell\left.  :a_{m-\ell}a_{n+\ell
}:\right.  -\left(  n-m\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.
-\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  +\ell\left(  n+\ell\right)
\left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)  \delta
_{m,-n}\operatorname*{id}\right) \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \underbrace{\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{m+n+\ell}:\right.  }_{\substack{=2L_{m+n}\\\text{(by
(\ref{pf.fockvir.answer2.pf7}))}}}-\underbrace{\sum\limits_{\ell\in\mathbb{Z}%
}\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  }%
_{\substack{=\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell
}:\right.  \\\text{(by (\ref{pf.fockvir.answer2.pf5}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}}_{\substack{=-\dfrac{n^{3}-n}{6}\delta
_{m,-n}\operatorname*{id}\\\text{(by (\ref{pf.fockvir.answer2.pf6}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split the sum; this was allowed, since the infinite
sums}\\
\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
\text{, }\sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell
}:\right.  \text{, }\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right. \\
\text{and }\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(
\left[  \ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}\text{ converge}%
\end{array}
\right) \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \cdot2L_{m+n}-\sum\limits_{\ell\in\mathbb{Z}}\ell\left.
:a_{m-\ell}a_{n+\ell}:\right.  -\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}\\
&  =-\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}.
\end{align*}
Hence, (\ref{pf.fockvir.answer2.pf0}) becomes%
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =-\dfrac{1}{2}\underbrace{\sum\limits_{\ell
\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right]
}_{=-\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}}\\
&  =-\dfrac{1}{2}\left(  -\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}\right) \\
&  =\left(  n-m\right)  \underbrace{L_{m+n}}_{=L_{n+m}}-\dfrac{n^{3}-n}%
{12}\underbrace{\delta_{m,-n}}_{=\delta_{n,-m}}\operatorname*{id}=\left(
n-m\right)  L_{n+m}-\dfrac{n^{3}-n}{12}\delta_{n,-m}\operatorname*{id}.
\end{align*}


This proves Proposition \ref{prop.fockvir.answer2}.

We can generalize our family $\left(  L_{n}\right)  _{n\in\mathbb{Z}}$ of
operators on $F_{\mu}$ as follows (the so-called \textit{Fairlie construction}):

\begin{theorem}
\label{thm.fockvir.hw2ex1}Let $\mu\in\mathbb{C}$ and $\lambda\in\mathbb{C}$.
We can define a linear map $\widetilde{L}_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$ as follows: For $n\neq0$, define the map $\widetilde{L}%
_{n}$ by%
\[
\widetilde{L}_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{m+n}:\right.  +i\lambda na_{n}%
\]
(where $i$ stands for the complex number $\sqrt{-1}$). Define the map
$\widetilde{L}_{0}$ by%
\[
\widetilde{L}_{0}=\dfrac{\mu^{2}}{2}+\dfrac{\lambda^{2}}{2}+\sum
\limits_{j>0}a_{-j}a_{j}.
\]
Then, this defines an action of $\operatorname*{Vir}$ on $F_{\mu}$ with
$c=1+12\lambda^{2}$ (by letting $L_{n}\in\operatorname*{Vir}$ act as the
operator $\widetilde{L}_{n}$, and by letting $C\in\operatorname*{Vir}$ acting
as $\left(  1+12\lambda^{2}\right)  \operatorname*{id}$). Moreover, it
satisfies $\left[  \widetilde{L}_{n},a_{m}\right]  =-ma_{n+m}+i\lambda
n^{2}\delta_{n,-m}\operatorname*{id}$ for all $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$.
\end{theorem}

Proving this proposition was exercise 1 in homework problem set 2. It is
rather easy now that we have proven Propositions \ref{prop.fockvir.answer1}
and \ref{prop.fockvir.answer2} and thus left to the reader.

\subsubsection{\textbf{[unfinished]} Unitarity properties of the Fock module}

\begin{proposition}
\label{prop.fockvir.unitary}Let $\mu\in\mathbb{R}$. Consider the
representation $F_{\mu}$ of $\mathcal{A}$. Let $\left\langle \cdot
,\cdot\right\rangle :F_{\mu}\times F_{\mu}\rightarrow\mathbb{C}$ be the unique
Hermitian form satisfying $\left\langle 1,1\right\rangle =1$ and%
\begin{equation}
\left\langle av,w\right\rangle =\left\langle v,a^{\dag}w\right\rangle
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathcal{A}\text{, }v\in F_{\mu}\text{
and }w\in F_{\mu} \label{pf.fockvir.unitary.1}%
\end{equation}
(this is the usual Hermitian form on $F_{\mu}$). Then, equipped with this
form, $F_{\mu}$ is a unitary representation of $\mathcal{A}$.
\end{proposition}

\textit{Proof.} We must prove that the form $\left\langle \cdot,\cdot
\right\rangle $ is positive definite.

Let $\overrightarrow{n}=\left(  n_{1},n_{2},n_{3},...\right)  $ and
$\overrightarrow{m}=\left(  m_{1},m_{2},m_{3},...\right)  $ be two sequences
of nonnegative integers, each of them containing only finitely many nonzero
entries. We are going to compute the value $\left\langle x_{1}^{n_{1}}%
x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...\right\rangle $. This will give us the matrix that represents the
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ with respect to the
monomial basis of $F_{\mu}$.

If $n_{1}+n_{2}+n_{3}+...\neq m_{1}+m_{2}+m_{3}+...$, then this value is
clearly zero, because the Hermitian form $\left\langle \cdot,\cdot
\right\rangle $ is of degree $0$ (as can be easily seen). Thus, we can WLOG
assume that $n_{1}+n_{2}+n_{3}+...=m_{1}+m_{2}+m_{3}+...$.

Let $k$ be a positive integer such that every $i>k$ satisfies $n_{i}=0$ and
$m_{i}=0$. (Such a $k$ clearly exists.) Then, $n_{1}+n_{2}+...+n_{k}%
=n_{1}+n_{2}+n_{3}+...$ and $m_{1}+m_{2}+...+m_{k}=m_{1}+m_{2}+m_{3}+...$.
Hence, the equality $n_{1}+n_{2}+n_{3}+...=m_{1}+m_{2}+m_{3}+...$ (which we
know to hold) rewrites as $n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}+...+m_{k}$. Now,
since every $i>k$ satisfies $n_{i}=0$ and $m_{i}=0$, we have%
\begin{align*}
&  \left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right\rangle \\
&  =\left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}},\underbrace{x_{1}%
^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}}_{\substack{=a_{-1}^{m_{1}}a_{-2}%
^{m_{2}}...a_{-k}^{m_{k}}1\\=\left(  a_{1}^{\dag}\right)  ^{m_{1}}\left(
a_{2}^{\dag}\right)  ^{m_{2}}...\left(  a_{k}^{\dag}\right)  ^{m_{k}}%
1}}\right\rangle =\left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}%
},\left(  a_{1}^{\dag}\right)  ^{m_{1}}\left(  a_{2}^{\dag}\right)  ^{m_{2}%
}...\left(  a_{k}^{\dag}\right)  ^{m_{k}}1\right\rangle \\
&  =\left\langle a_{k}^{m_{k}}a_{k-1}^{m_{k-1}}...a_{1}^{m_{1}}x_{1}^{n_{1}%
}x_{2}^{n_{2}}...x_{k}^{n_{k}},1\right\rangle \ \ \ \ \ \ \ \ \ \ \left(
\text{due to (\ref{pf.fockvir.unitary.1}), applied several times}\right) \\
&  =\left\langle \underbrace{\left(  k\dfrac{\partial}{\partial x_{k}}\right)
^{m_{k}}\left(  \left(  k-1\right)  \dfrac{\partial}{\partial x_{k-1}}\right)
^{m_{k-1}}...\left(  1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}%
x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}}_{\substack{\text{this is a
constant polynomial,}\\\text{since }n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}%
+...+m_{k}}},1\right\rangle \\
&  =\left(  k\dfrac{\partial}{\partial x_{k}}\right)  ^{m_{k}}\left(  \left(
k-1\right)  \dfrac{\partial}{\partial x_{k-1}}\right)  ^{m_{k-1}}...\left(
1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}x_{1}^{n_{1}}x_{2}^{n_{2}%
}...x_{k}^{n_{k}}\\
&  =\prod\limits_{j=1}^{k}j^{m_{j}}\cdot\underbrace{\left(  \dfrac{\partial
}{\partial x_{k}}\right)  ^{m_{k}}\left(  \dfrac{\partial}{\partial x_{k-1}%
}\right)  ^{m_{k-1}}...\left(  \dfrac{\partial}{\partial x_{1}}\right)
^{m_{1}}x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}}_{\substack{=\delta
_{\overrightarrow{n},\overrightarrow{m}}\cdot\prod\limits_{j=1}^{k}%
m_{j}!\\\text{(since }n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}+...+m_{k}\text{)}%
}}=\delta_{\overrightarrow{n},\overrightarrow{m}}\cdot\prod\limits_{j=1}%
^{k}j^{m_{j}}\prod\limits_{j=1}^{k}m_{j}!.
\end{align*}
This term is $0$ when $\overrightarrow{n}\neq\overrightarrow{m}$, and a
positive integer when $\overrightarrow{n}=\overrightarrow{m}$. Thus, the
matrix which represents the form $\left\langle \cdot,\cdot\right\rangle $ with
respect to the monomial basis of $F_{\mu}$ is diagonal with positive diagonal
entries. This form is therefore positive definite. Proposition
\ref{prop.fockvir.unitary} is proven.

\begin{corollary}
If $\mu,\lambda\in\mathbb{R}$, then the $\operatorname*{Vir}$-representation
on $F_{\mu}$ given by $\widetilde{L}_{n}$ is unitary.
\end{corollary}

\textit{Proof.} For $n\neq0$, we have%
\begin{align*}
\widetilde{L}_{n}^{\dag}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  ^{\dag}+\left(  i\lambda na_{n}\right)  ^{\dag}\\
&  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{-n-m}:\right.
-i\lambda na_{-n}=\widetilde{L}_{-n}.
\end{align*}


\begin{corollary}
The $\operatorname*{Vir}$-representation $F_{\mu}$ is completely reducible for
$\mu\in\mathbb{R}$.
\end{corollary}

Now, $L_{0}1=\dfrac{\mu^{2}+\lambda^{2}}{2}1$ and $C1=\left(  1+12\lambda
^{2}\right)  1$. Thus, the Verma module $M_{h,c}:=M_{h,c}^{+}$ of the Virasoro
algebra $\operatorname*{Vir}$ for $h=\dfrac{\mu^{2}+\lambda^{2}}{2}$ and
$c=1+12\lambda^{2}$ maps to $F_{\mu}$ with $v_{h,c}\mapsto1$.

\begin{proposition}
For Weil generic $\mu$ and $\lambda$, this is an isomorphism.
\end{proposition}

\textit{Proof.} The dimension of the degree-$n$ part of both modules is
$p\left(  n\right)  $. The map has degree $0$. Hence, if it is injective, it
is surjective. But for Weil generic $\mu$ and $\lambda$, the
$\operatorname*{Vir}$-module $M_{h,c}$ is irreducible, so the map is injective.

\begin{corollary}
For Weil generic $\mu$ and $\lambda$ in $\mathbb{R}$, the representation
$M_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

For any $\mu$ and $\lambda$ in $\mathbb{R}$, the representation $L_{\dfrac
{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

In other words, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq\dfrac{c-1}{24}$.
\end{corollary}

\subsection{\label{subsect.quantumfields}Power series and quantum fields}

In this section, we are going to study different kinds of power series:
polynomials, formal power series, Laurent polynomials, Laurent series and,
finally, a notion of ``formal power series'' which can be infinite ``in both
directions''. Each of these kinds of power series will later be used in our
work; it is important to know the properties and the shortcomings of each of them.

\subsubsection{Definitions}

Parts of the following definition should sound familiar to the reader (indeed,
we have already been working with polynomials, formal power series and Laurent
polynomials), although maybe not in this generality.

\begin{definition}
\label{def.qf.powerseries}For every vector space $B$ and symbol $z$, we make
the following definitions:

\textbf{(a)} We denote by $B\left[  z\right]  $ the vector space of all
sequences $\left(  b_{n}\right)  _{n\in\mathbb{N}}\in B^{\mathbb{N}}$ such
that only finitely many $n\in\mathbb{N}$ satisfy $b_{n}\neq0$. Such a sequence
$\left(  b_{n}\right)  _{n\in\mathbb{N}}$ is denoted by $\sum\limits_{n\in
\mathbb{N}}b_{n}z^{n}$. The elements of $B\left[  z\right]  $ are called
\textit{polynomials in the indeterminate }$z$ \textit{over }$B$ (even when $B$
is not a ring).

\textbf{(b)} We denote by $B\left[  \left[  z\right]  \right]  $ the vector
space of all sequences $\left(  b_{n}\right)  _{n\in\mathbb{N}}\in
B^{\mathbb{N}}$. Such a sequence $\left(  b_{n}\right)  _{n\in\mathbb{N}}$ is
denoted by $\sum\limits_{n\in\mathbb{N}}b_{n}z^{n}$. The elements of $B\left[
\left[  z\right]  \right]  $ are called \textit{formal power series in the
indeterminate }$z$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(c)} We denote by $B\left[  z,z^{-1}\right]  $ the vector space of all
two-sided sequences $\left(  b_{n}\right)  _{n\in\mathbb{Z}}\in B^{\mathbb{Z}%
}$ such that only finitely many $n\in\mathbb{Z}$ satisfy $b_{n}\neq0$. (A
\textit{two-sided sequence} means a sequence indexed by integers, not just
nonnegative integers.) Such a sequence $\left(  b_{n}\right)  _{n\in
\mathbb{Z}}$ is denoted by $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$. The
elements of $B\left[  z,z^{-1}\right]  $ are called\textit{ Laurent
polynomials in the indeterminate }$z$ \textit{over }$B$ (even when $B$ is not
a ring).

\textbf{(d)} We denote by $B\left(  \left(  z\right)  \right)  $ the vector
space of all two-sided sequences $\left(  b_{n}\right)  _{n\in\mathbb{Z}}\in
B^{\mathbb{Z}}$ such that only finitely many among the negative $n\in
\mathbb{Z}$ satisfy $b_{n}\neq0$. (A \textit{two-sided sequence} means a
sequence indexed by integers, not just nonnegative integers.) Such a sequence
$\left(  b_{n}\right)  _{n\in\mathbb{Z}}$ is denoted by $\sum\limits_{n\in
\mathbb{Z}}b_{n}z^{n}$. Sometimes, $B\left(  \left(  z\right)  \right)  $ is
also denoted by $B\left[  \left[  z,z^{-1}\right.  \right]  $. The elements of
$B\left(  \left(  z\right)  \right)  $ are called \textit{formal Laurent
series in the indeterminate }$z$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(e)} We denote by $B\left[  \left[  z,z^{-1}\right]  \right]  $ the
vector space of all two-sided sequences $\left(  b_{n}\right)  _{n\in
\mathbb{Z}}\in B^{\mathbb{Z}}$. Such a sequence $\left(  b_{n}\right)
_{n\in\mathbb{Z}}$ is denoted by $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$.

All five of these spaces $B\left[  z\right]  $, $B\left[  \left[  z\right]
\right]  $, $B\left[  z,z^{-1}\right]  $, $B\left(  \left(  z\right)  \right)
$ and $B\left[  \left[  z,z^{-1}\right]  \right]  $ are $\mathbb{C}\left[
z\right]  $-modules. (Here, the $\mathbb{C}\left[  z\right]  $-module
structure on $B\left[  \left[  z,z^{-1}\right]  \right]  $ is given by%
\begin{equation}
\left(  \sum\limits_{n\in\mathbb{N}}c_{n}z^{n}\right)  \cdot\left(
\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}\right)  =\sum\limits_{n\in\mathbb{Z}%
}\left(  \sum\limits_{m\in\mathbb{N}}c_{m}\cdot b_{n-m}\right)  z^{n}
\label{def.qf.powerseries.prod.prototype}%
\end{equation}
for all $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}\in B\left[  \left[
z,z^{-1}\right]  \right]  $ and $\sum\limits_{n\in\mathbb{N}}c_{n}z^{n}%
\in\mathbb{C}\left[  z\right]  $, and the $\mathbb{C}\left[  z\right]
$-module structures on the other four spaces are defined similarly.) Besides,
$B\left[  \left[  z\right]  \right]  $ and $B\left(  \left(  z\right)
\right)  $ are $\mathbb{C}\left[  \left[  z\right]  \right]  $-modules
(defined in a similar way to (\ref{def.qf.powerseries.prod.prototype})). Also,
$B\left(  \left(  z\right)  \right)  $ is a $\mathbb{C}\left(  \left(
z\right)  \right)  $-module (in a similar way). Besides, $B\left[
z,z^{-1}\right]  $, $B\left(  \left(  z\right)  \right)  $ and $B\left[
\left[  z,z^{-1}\right]  \right]  $ are $\mathbb{C}\left[  z,z^{-1}\right]
$-modules (defined analogously to (\ref{def.qf.powerseries.prod.prototype})).

Of course, if $B$ is a $\mathbb{C}$-algebra, then the above-defined spaces
$B\left[  z\right]  $, $B\left[  z,z^{-1}\right]  $, $B\left[  \left[
z\right]  \right]  $ and $B\left(  \left(  z\right)  \right)  $ are
$\mathbb{C}$-algebras themselves (with the multiplication defined similarly to
(\ref{def.qf.powerseries.prod.prototype})), and in fact $B\left[  z\right]  $
is the algebra of polynomials in the variable $z$ over $B$, and $B\left[
z,z^{-1}\right]  $ is the algebra of Laurent polynomials in the variable $z$
over $B$, and $B\left[  \left[  z\right]  \right]  $ is the algebra of formal
power series in the variable $z$ over $B$.

It should be noticed that $B\left[  z\right]  \cong B\otimes\mathbb{C}\left[
z\right]  $ and $B\left[  z,z^{-1}\right]  \cong B\otimes\mathbb{C}\left[
z,z^{-1}\right]  $ canonically, but such isomorphisms do \textbf{not} hold for
$B\left[  \left[  z\right]  \right]  $, $B\left(  \left(  z\right)  \right)  $
and $B\left[  \left[  z,z^{-1}\right]  \right]  $ unless $B$ is finite-dimensional.

We regard the obvious injections $B\left[  z\right]  \rightarrow B\left[
z,z^{-1}\right]  $, $B\left[  z^{-1}\right]  \rightarrow B\left[
z,z^{-1}\right]  $ (this is the map sending $z^{-1}\in B\left[  z^{-1}\right]
$ to $z^{-1}\in B\left[  z,z^{-1}\right]  $), $B\left[  z\right]  \rightarrow
B\left[  \left[  z\right]  \right]  $, $B\left[  z^{-1}\right]  \rightarrow
B\left[  \left[  z^{-1}\right]  \right]  $, $B\left[  \left[  z\right]
\right]  \rightarrow B\left(  \left(  z\right)  \right)  $, $B\left[  \left[
z^{-1}\right]  \right]  \rightarrow B\left(  \left(  z^{-1}\right)  \right)
$, $B\left[  z,z^{-1}\right]  \rightarrow B\left(  \left(  z\right)  \right)
$, $B\left[  z,z^{-1}\right]  \rightarrow B\left(  \left(  z^{-1}\right)
\right)  $, $B\left(  \left(  z\right)  \right)  \rightarrow B\left[  \left[
z,z^{-1}\right]  \right]  $ and $B\left(  \left(  z^{-1}\right)  \right)
\rightarrow B\left[  \left[  z,z^{-1}\right]  \right]  $ as inclusions.

Clearly, all five spaces $B\left[  z\right]  $, $B\left[  \left[  z\right]
\right]  $, $B\left[  z,z^{-1}\right]  $, $B\left(  \left(  z\right)  \right)
$ and $B\left[  \left[  z,z^{-1}\right]  \right]  $ depend functorially on $B$.
\end{definition}

Before we do anything further with these notions, let us give three warnings:

\textbf{1)} Given Definition \ref{def.qf.powerseries}, one might expect
$B\left[  \left[  z,z^{-1}\right]  \right]  $ to canonically become a
$\mathbb{C}\left[  \left[  z,z^{-1}\right]  \right]  $-algebra. But this is
not true even for $B=\mathbb{C}$ (because there is no reasonable way to define
a product of two elements of $\mathbb{C}\left[  \left[  z,z^{-1}\right]
\right]  $\ \ \ \ \footnote{If we would try the natural way, we would get
nonsense results. For instance, if we tried to compute the coefficient of
$\left(  \sum\limits_{n\in\mathbb{Z}}1z^{n}\right)  \cdot\left(
\sum\limits_{n\in\mathbb{Z}}1z^{n}\right)  $ before $z^{0}$, we would get
$\sum\limits_{\substack{\left(  n,m\right)  \in\mathbb{Z}^{2};\\n+m=0}%
}1\cdot1$, which is not a convergent series.}). This also answers why
$B\left[  \left[  z,z^{-1}\right]  \right]  $ does not become a ring when $B$
is a $\mathbb{C}$-algebra. Nor is $B\left[  \left[  z,z^{-1}\right]  \right]
$, in general, a $B\left[  \left[  z\right]  \right]  $-module.

\textbf{2)} The $\mathbb{C}\left[  z,z^{-1}\right]  $-module $B\left[  \left[
z,z^{-1}\right]  \right]  $ usually has torsion. For example, $\left(
1-z\right)  \cdot\sum\limits_{n\in\mathbb{Z}}z^{n}=0$ in $\mathbb{C}\left[
\left[  z,z^{-1}\right]  \right]  $ despite $\sum\limits_{n\in\mathbb{Z}}%
z^{n}\neq0$. As a consequence, working in $B\left[  \left[  z,z^{-1}\right]
\right]  $ requires extra care.

\textbf{3)} Despite the suggestive notation $B\left(  \left(  z\right)
\right)  $, it is of course not true that $B\left(  \left(  z\right)  \right)
$ is a field whenever $B$ is a commutative ring. However, $B\left(  \left(
z\right)  \right)  $ is a field whenever $B$ is a field.

\begin{Convention}
Let $B$ be a vector space, and $z$ a symbol. By analogy with the notations
$B\left[  z\right]  $, $B\left[  \left[  z\right]  \right]  $ and $B\left(
\left(  z\right)  \right)  $ introduced in Definition \ref{def.qf.powerseries}%
, we will occasionally also use the notations $B\left[  z^{-1}\right]  $,
$B\left[  \left[  z^{-1}\right]  \right]  $ and $B\left(  \left(
z^{-1}\right)  \right)  $. For example, $B\left[  z^{-1}\right]  $ will mean
the vector space of all ``reverse sequences'' $\left(  b_{n}\right)
_{n\in-\mathbb{N}}$ such that only finitely many $n\in-\mathbb{N}$ satisfy
$b_{n}\neq0$\ \ \ \ \footnotemark. Of course, $B\left[  z\right]  \cong
B\left[  z^{-1}\right]  $ as vector spaces, but $B\left[  z\right]  $ and
$B\left[  z^{-1}\right]  $ are two different subspaces of $B\left[
z,z^{-1}\right]  $, so it is useful to distinguish between $B\left[  z\right]
$ and $B\left[  z^{-1}\right]  $.
\end{Convention}

\footnotetext{Here, $-\mathbb{N}$ denotes the set $\left\{
0,-1,-2,-3,...\right\}  $, and a ``reverse sequence'' is a family indexed by
elements of $-\mathbb{N}$.}

Now, let us extend Definition \ref{def.qf.powerseries} to several variables.
The reader is advised to only skim through the following definition, as there
is nothing unexpected in it:

\begin{definition}
\label{def.qf.powerseries.mvars}Let $m\in\mathbb{N}$. Let $z_{1}%
,z_{2},...,z_{m}$ be $m$ symbols. For every vector space $B$, we make the
following definitions:

\textbf{(a)} We denote by $B\left[  z_{1},z_{2},...,z_{m}\right]  $ the vector
space of all families $\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)
}\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}\in
B^{\mathbb{N}^{m}}$ such that only finitely many $\left(  n_{1},n_{2}%
,...,n_{m}\right)  \in\mathbb{N}^{m}$ satisfy $b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }\neq0$. Such a family $\left(  b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{N}^{m}}$ is denoted by $\sum\limits_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2},...,n_{m}\right)
}z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$. The elements of $B\left[
z_{1},z_{2},...,z_{m}\right]  $ are called \textit{polynomials in the
indeterminates }$z_{1},z_{2},...,z_{m}$ \textit{over }$B$ (even when $B$ is
not a ring).

\textbf{(b)} We denote by $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]
\right]  $ the vector space of all families $\left(  b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{N}^{m}}\in B^{\mathbb{N}^{m}}$. Such a family $\left(  b_{\left(
n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}%
\right)  \in\mathbb{N}^{m}}$ is denoted by $\sum\limits_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$. The elements
of $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]  \right]  $ are called
\textit{formal power series in the indeterminates }$z_{1},z_{2},...,z_{m}$
\textit{over }$B$ (even when $B$ is not a ring).

\textbf{(c)} We denote by $B\left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  $ the vector space of all families $\left(
b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}\in B^{\mathbb{Z}^{m}}$ such that
only finitely many $\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}$
satisfy $b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\neq0$. Such a family
$\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}$ is denoted by $\sum
\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(
n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$.
The elements of $B\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m}%
,z_{m}^{-1}\right]  $ are called\textit{ Laurent polynomials in the
indeterminates }$z_{1},z_{2},...,z_{m}$ \textit{over }$B$ (even when $B$ is
not a ring).

\textbf{(d)} We denote by $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)
\right)  $ the vector space of all families $\left(  b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{Z}^{m}}\in B^{\mathbb{Z}^{m}}$ for which there exists an
$N\in\mathbb{Z}$ such that every $\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{Z}^{m}\setminus\left\{  N,N+1,N+2,\ldots\right\}  ^{m}$ satisfies
$b_{\left(  n_{1},n_{2},...,n_{m}\right)  }=0$. Such a family $\left(
b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}$ is denoted by $\sum
\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(
n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$.
The elements of $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ are
called\textit{ formal Laurent series in the indeterminates }$z_{1}%
,z_{2},...,z_{m}$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(e)} We denote by $B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ the vector space of all families
$\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}\in B^{\mathbb{Z}^{m}}$. Such
a family $\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}$ is denoted by $\sum
\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(
n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$.

All five of these spaces $B\left[  z_{1},z_{2},...,z_{m}\right]  $, $B\left[
\left[  z_{1},z_{2},...,z_{m}\right]  \right]  $, $B\left[  z_{1},z_{1}%
^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[  z_{1}%
,z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ are
$\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $-modules. (Here, the
$\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $-module structure on
$B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  \right]  $ is given by%
\begin{align}
&  \left(  \sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in
\mathbb{N}^{m}}c_{\left(  n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}%
z_{2}^{n_{2}}...z_{m}^{n_{m}}\right)  \cdot\left(  \sum\limits_{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}\right)
\nonumber\\
&  =\sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}%
}\left(  \sum\limits_{\left(  m_{1},m_{2},...,m_{m}\right)  \in\mathbb{N}^{m}%
}c_{\left(  m_{1},m_{2},...,m_{m}\right)  }\cdot b_{\left(  n_{1}-m_{1}%
,n_{2}-m_{2},...,n_{m}-m_{m}\right)  }\right)  z_{1}^{n_{1}}z_{2}^{n_{2}%
}...z_{m}^{n_{m}} \label{def.qf.powerseries.mvars.prod.prototype}%
\end{align}
for all $\sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}%
}b_{\left(  n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}%
...z_{m}^{n_{m}}\in B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ and $\sum\limits_{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}c_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}%
\in\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $, and the $\mathbb{C}%
\left[  z_{1},z_{2},...,z_{m}\right]  $-module structures on the other four
spaces are defined similarly.) Besides, $B\left[  \left[  z_{1},z_{2}%
,...,z_{m}\right]  \right]  $ and $B\left(  \left(  z_{1},z_{2},...,z_{m}%
\right)  \right)  $ are $\mathbb{C}\left[  \left[  z_{1},z_{2},...,z_{m}%
\right]  \right]  $-modules (defined in a similar fashion to
(\ref{def.qf.powerseries.mvars.prod.prototype})). Also, $B\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $ is a $\mathbb{C}\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $-module (defined in analogy to
(\ref{def.qf.powerseries.mvars.prod.prototype})). Besides, $B\left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ are
$\mathbb{C}\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  $-modules (in a similar way).

Of course, if $B$ is a $\mathbb{C}$-algebra, then the above-defined spaces
$B\left[  z_{1},z_{2},...,z_{m}\right]  $, $B\left[  z_{1},z_{1}^{-1}%
,z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  $ and $B\left(  \left(  z_{1}%
,z_{2},...,z_{m}\right)  \right)  $ are $\mathbb{C}$-algebras themselves (with
multiplication defined by a formula analogous to
(\ref{def.qf.powerseries.mvars.prod.prototype}) again), and in fact $B\left[
z_{1},z_{2},...,z_{m}\right]  $ is the algebra of polynomials in the variables
$z_{1},z_{2},...,z_{m}$ over $B$, and $B\left[  z_{1},z_{1}^{-1},z_{2}%
,z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $ is the algebra of Laurent
polynomials in the variables $z_{1},z_{2},...,z_{m}$ over $B$, and $B\left[
\left[  z_{1},z_{2},...,z_{m}\right]  \right]  $ is the algebra of formal
power series in the variables $z_{1},z_{2},...,z_{m}$ over $B$.

It should be noticed that $B\left[  z_{1},z_{2},...,z_{m}\right]  \cong
B\otimes\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $ and $B\left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \cong
B\otimes\mathbb{C}\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m}%
,z_{m}^{-1}\right]  $ canonically, but such isomorphisms do \textbf{not} hold
for $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]  \right]  $, $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $
unless $B$ is finite-dimensional or $m=0$.

There are several obvious injections (analogous to the ones listed in
Definition \ref{def.qf.powerseries}) which we regard as inclusions. For
example, one of these is the injection $B\left[  z_{1},z_{2},...,z_{m}\right]
\rightarrow B\left[  \left[  z_{1},z_{2},...,z_{m}\right]  \right]  $; we
won't list the others here.

Clearly, all five spaces $B\left[  z_{1},z_{2},...,z_{m}\right]  $, $B\left[
\left[  z_{1},z_{2},...,z_{m}\right]  \right]  $, $B\left[  z_{1},z_{1}%
^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[  z_{1}%
,z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ depend
functorially on $B$.
\end{definition}

Clearly, when $m=1$, Definition \ref{def.qf.powerseries.mvars} is equivalent
to Definition \ref{def.qf.powerseries}.

Definition \ref{def.qf.powerseries.mvars} can be extended to infinitely many
indeterminates; this is left to the reader.

Our definition of $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $
is rather intricate. The reader might gain a better understanding from the
following equivalent definition: The set $B\left(  \left(  z_{1}%
,z_{2},...,z_{m}\right)  \right)  $ is the subset of $B\left[  \left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $
consisting of those $p\in B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ for which there exists an
$\left(  a_{1},a_{2},\ldots,a_{m}\right)  \in\mathbb{Z}^{m}$ such that
$z_{1}^{a_{1}}z_{2}^{a_{2}}...z_{m}^{a_{m}}\cdot p\in B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  $. It is easy to show that $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ is isomorphic to the
localization of the ring $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]
\right]  $ at the multiplicatively closed subset consisting of all monomials.

The reader should be warned that if $B$ is a field, $m$ is an integer $>1$,
and $z_{1}$, $z_{2}$, $...$, $z_{m}$ are $m$ symbols, then the ring $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ is \textbf{not} a field
(unlike in the case $m=1$); for example, it does not contain an inverse to
$z_{1}-z_{2}$. This is potentially confusing and I would not be surprised if
some texts define $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $
to mean a different ring which actually is a field.

When $B$ is a vector space and $z$ is a symbol, there is an operator we can
define on each of the five spaces $B\left[  z\right]  $, $B\left[  \left[
z\right]  \right]  $, $B\left[  z,z^{-1}\right]  $, $B\left(  \left(
z\right)  \right)  $ and $B\left[  \left[  z,z^{-1}\right]  \right]  $:
derivation with respect to $z$:

\begin{definition}
\label{def.qf.powerseries.d}For every vector space $B$ and symbol $z$, we make
the following definitions:

Define a linear map $\dfrac{d}{dz}:B\left[  z\right]  \rightarrow B\left[
z\right]  $ by the formula%
\begin{align}
\dfrac{d}{dz}\left(  \sum\limits_{n\in\mathbb{N}}b_{n}z^{n}\right)   &
=\sum\limits_{n\in\mathbb{N}}\left(  n+1\right)  b_{n+1}z^{n}%
\label{def.qf.powerseries.d.formula}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\sum\limits_{n\in\mathbb{N}}b_{n}%
z^{n}\in B\left[  z\right]  .\nonumber
\end{align}
Define a linear map $\dfrac{d}{dz}:B\left[  \left[  z\right]  \right]
\rightarrow B\left[  \left[  z\right]  \right]  $ by the very same formula,
and define linear maps $\dfrac{d}{dz}:B\left[  z,z^{-1}\right]  \rightarrow
B\left[  z,z^{-1}\right]  $, $\dfrac{d}{dz}:B\left(  \left(  z\right)
\right)  \rightarrow B\left(  \left(  z\right)  \right)  $ and $\dfrac{d}%
{dz}:B\left[  \left[  z,z^{-1}\right]  \right]  \rightarrow B\left[  \left[
z,z^{-1}\right]  \right]  $ by analogous formulas (more precisely, by formulas
which differ from (\ref{def.qf.powerseries.d.formula}) only in that the sums
range over $\mathbb{Z}$ instead of over $\mathbb{N}$).

For every $f\in B\left[  \left[  z,z^{-1}\right]  \right]  $, the image
$\dfrac{d}{dz}f$ of $f$ under the linear map $\dfrac{d}{dz}$ will be denoted
by $\dfrac{df}{dz}$ or by $f^{\prime}$ and called the $z$\textit{-derivative}
of $f$ (or, briefly, the \textit{derivative} of $f$). The operator $\dfrac
{d}{dz}$ itself (on any of the five vector spaces $B\left[  z\right]  $,
$B\left[  \left[  z\right]  \right]  $, $B\left[  z,z^{-1}\right]  $,
$B\left(  \left(  z\right)  \right)  $ and $B\left[  \left[  z,z^{-1}\right]
\right]  $) will be called the \textit{differentiation with respect to }$z$.
\end{definition}

An analogous definition can be made for several variables:

\begin{definition}
\label{def.qf.powerseries.mvars.d}Let $m\in\mathbb{N}$. Let $z_{1}%
,z_{2},...,z_{m}$ be $m$ symbols. Let $i\in\left\{  1,2,...,m\right\}  $. For
every vector space $B$, we make the following definitions:

Define a linear map $\dfrac{\partial}{\partial z_{i}}:B\left[  z_{1}%
,z_{2},...,z_{m}\right]  \rightarrow B\left[  z_{1},z_{2},...,z_{m}\right]  $
by the formula%
\begin{align}
&  \dfrac{\partial}{\partial z_{i}}\left(  \sum\limits_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}\right)
\nonumber\\
&  =\sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}%
}\left(  n_{i}+1\right)  b_{\left(  n_{1},n_{2},...,n_{i-1},n_{i}%
+1,n_{i+1},n_{i+2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}%
...z_{m}^{n_{m}}\label{def.qf.powerseries.mvars.d.formula}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\sum\limits_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}\in B\left[
z_{1},z_{2},...,z_{m}\right]  .\nonumber
\end{align}
Define a linear map $\dfrac{\partial}{\partial z_{i}}:B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  \rightarrow B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  $ by the very same formula, and define
linear maps $\dfrac{\partial}{\partial z_{i}}:B\left[  z_{1},z_{1}^{-1}%
,z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \rightarrow B\left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $,
$\dfrac{\partial}{\partial z_{i}}:B\left(  \left(  z_{1},z_{2},...,z_{m}%
\right)  \right)  \rightarrow B\left(  \left(  z_{1},z_{2},...,z_{m}\right)
\right)  $ and $\dfrac{\partial}{\partial z_{i}}:B\left[  \left[  z_{1}%
,z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  \rightarrow
B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  \right]  $ by analogous formulas (more precisely, by formulas
which differ from (\ref{def.qf.powerseries.mvars.d.formula}) only in that the
sums range over $\mathbb{Z}^{m}$ instead of over $\mathbb{N}^{m}$).

For every $f\in B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1}%
,...,z_{m},z_{m}^{-1}\right]  \right]  $, the image $\dfrac{\partial}{\partial
z_{i}}f$ of $f$ under the linear map $\dfrac{\partial}{\partial z_{i}}$ will
be denoted by $\dfrac{\partial f}{\partial z_{i}}$ and called the $z_{i}%
$\textit{-derivative} of $f$ (or the \textit{partial derivative of }$f$
\textit{with respect to }$z_{i}$). The operator $\dfrac{\partial}{\partial
z_{i}}$ itself (on any of the five vector spaces $B\left[  z_{1}%
,z_{2},...,z_{m}\right]  $, $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]
\right]  $, $B\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  $, $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $
and $B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  \right]  $) will be called the \textit{differentiation with
respect to }$z_{i}$.
\end{definition}

Again, it is straightforward (and left to the reader) to extend this
definition to infinitely many indeterminates.

\subsubsection{Quantum fields}

Formal power series which are infinite ``in both directions'' might seem like
a perverse and artificial notion; their failure to form a ring certainly does
not suggest them to be useful. Nevertheless, they prove very suitable when
studying infinite-dimensional Lie algebras. Let us explain how.

For us, when we study Lie algebras, we are mainly concerned with their
elements, usually basis elements (e. g., the $a_{n}$ in $\mathcal{A}$). For
physicists, instead, certain generating functions built of these objects are
objects of primary concern, since they are closer to what they observe. They
are called \textit{quantum fields}.

Now, what are quantum fields?

For example, in $\mathcal{A}$, let us set $a\left(  z\right)  =\sum
\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$, where $z$ is a formal variable. This
sum $\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is a formal sum which is
infinite in both directions, so it is not an element of any of the rings
$U\left(  \mathcal{A}\right)  \left[  \left[  z\right]  \right]  $ or
$U\left(  \mathcal{A}\right)  \left(  z \right)  $, but
only an element of $U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  $.

As we said, the vector space $U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  $ is \textbf{not} a ring (even though $U\left(
\mathcal{A}\right)  $ is a $\mathbb{C}$-algebra), so we cannot multiply two
``sums'' like $a\left(  z\right)  $ in general. \textbf{However}, in the
following, we are going to learn about several things that we \textbf{can} do
with such ``sums''. One first thing that we notice about our concrete ``sum''
$a\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is that if we
apply $a\left(  z\right)  $ to some vector $v$ in $F_{\mu}$ (by evaluating the
term $\left(  a\left(  z\right)  \right)  v$ componentwise\footnote{By
``evaluating'' a term like $\left(  a\left(  z\right)  \right)  v$ at a vector
$v$ ``componentwise'', we mean evaluating $\sum\limits_{n\in\mathbb{Z}}\left(
a_{n}z^{-n-1}\right)  \left(  v\right)  $. Here, the variable $z$ is decreed
to commute with everything else, so that $\left(  a_{n}z^{-n-1}\right)
\left(  v\right)  $ means $z^{-n-1}a_{n}v$.}), then we get a sum
$\sum\limits_{n\in\mathbb{Z}}z^{-n-1}a_{n}v$ which evaluates to an element of
$F_{\mu}\left( \left (  z \right) \right)  $ (because every
sufficiently large $n\in\mathbb{Z}$ satisfies $z^{-n-1}\underbrace{a_{n}%
v}_{=0}=0$). As a consequence, $a\left(  z\right)  $ ``acts'' on $F_{\mu}$. I
am saying ``acts'' in quotation marks, since this ``action'' is not a map
$F_{\mu}\rightarrow F_{\mu}$ but a map $F_{\mu}\rightarrow F_{\mu}\left( \left(
  z \right) \right)  $, and since $a\left(  z\right)  $ does not
lie in a ring (as I said, $U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  $ is \textbf{not} a ring).

Physicists call $a\left(  z\right)  $ a \textit{quantum field} (more
precisely, a free bosonic field).

While we cannot take the square $\left(  a\left(  z\right)  \right)  ^{2}$ of
our ``sum'' $a\left(  z\right)  $ (since $U\left(  \mathcal{A}\right)  \left[
\left[  z,z^{-1}\right]  \right]  $ is not a ring), we can multiply two sums
``with different variables''; e. g., we can multiply $a\left(  z\right)  $ and
$a\left(  w\right)  $, where $z$ and $w$ are two distinct formal variables.
The product $a\left(  z\right)  a\left(  w\right)  $ is defined as the formal
sum $\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}a_{n}a_{m}%
z^{-n-1}w^{-m-1}\in U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  $. Note
that elements of $U\left(  \mathcal{A}\right)  \left[  \left[  z,z^{-1}%
\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  $ are two-sided
sequences of two-sided sequences of elements of $U\left(  \mathcal{A}\right)
$; of course, we can interpret them as maps $\mathbb{Z}^{2}\rightarrow
U\left(  \mathcal{A}\right)  $.

It is easy to see that $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$. This identity, in the
first place, holds on the level of formal sums (where $\sum\limits_{n\in
\mathbb{Z}}nz^{-n-1}w^{n-1}$ is a shorthand notation for a particular sequence
of sequences: namely, the one whose $j$-th element is the sequence whose
$i$-th element is $\delta_{i+j+2,0}\left(  j+1\right)  $), but if we evaluate
it on an element $v$ of $F_{\mu}$, then we get an identity $\left[  a\left(
z\right)  ,a\left(  w\right)  \right]  v=\sum\limits_{n\in\mathbb{Z}}%
nz^{-n-1}w^{n-1}v$ which holds in the space $F_{\mu}\left(  \left(  z\right)
\right)  \left(  \left(  w\right)  \right)  $.

We can obtain the ``series'' $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$ by differentiating a
more basic ``series'':%
\[
\delta\left(  w-z\right)  :=\sum_{n\in\mathbb{Z}}z^{-n-1}w^{n}.
\]
This, again, is a formal series infinite in both directions. Why do we call it
$\delta\left(  w-z\right)  $ ? Because in analysis, the delta-``function''
(actually a distribution) satisfies the formula $\int\delta\left(  x-y\right)
f\left(  y\right)  dy=f\left(  x\right)  $ for every function $f$, whereas our
series $\delta\left(  w-z\right)  $ satisfies a remarkably similar
property\footnote{Namely, if we define the ``formal residue'' $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}q\left(  z\right)  dz$ of an
element $q\left(  z\right)  \in B\left(  \left(  z\right)  \right)  $ (for $B$
being some vector space) to be the coefficient of $q\left(  z\right)  $ before
$z^{-1}$, then every $f=\sum\limits_{n\in\mathbb{Z}}f_{n}z^{n}$ (with
$f_{n}\in B$) satisfies $\dfrac{1}{2\pi i}\oint\limits_{\left\vert
z\right\vert =1}z^{-n-1}f\left(  z\right)  dz=f_{n}$, and thus $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}\delta\left(  w-z\right)  f\left(
z\right)  dz=f\left(  w\right)  $.}. And now, $\left[  a\left(  z\right)
,a\left(  w\right)  \right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$
becomes $\left[  a\left(  z\right)  ,a\left(  w\right)  \right]  =\partial
_{w}\delta\left(  w-z\right)  =:\delta^{\prime}\left(  w-z\right)  $.

Something more interesting comes out for the Witt algebra: Set $T\left(
z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$\textbf{ in the Witt
algebra}. Then, we have%
\begin{align*}
&  \left[  T\left(  z\right)  ,T\left(  w\right)  \right] \\
&  =\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left(  n-m\right)
L_{n+m}z^{-n-2}w^{-m-2}=\sum\limits_{\left(  k,m\right)  \in\mathbb{Z}^{2}%
}L_{k}\underbrace{\left(  k-2m\right)  }_{=\left(  k+2\right)  +2\left(
-m-1\right)  }z^{m-k-2}w^{-m-2}\\
&  =\underbrace{\left(  \sum\limits_{k\in\mathbb{Z}}L_{k}\left(  k+2\right)
z^{-k-3}\right)  }_{=-T^{\prime}\left(  z\right)  }\underbrace{\left(
\sum\limits_{m\in\mathbb{Z}}z^{m+1}w^{-m-2}\right)  }_{=\delta\left(
w-z\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +2\underbrace{\left(  \sum\limits_{k\in\mathbb{Z}}%
L_{k}z^{-k-2}\right)  }_{=T\left(  z\right)  }\underbrace{\left(
\sum\limits_{m\in\mathbb{Z}}\left(  -m-1\right)  z^{m}w^{-m-2}\right)
}_{=\delta^{\prime}\left(  w-z\right)  }\\
&  =-T^{\prime}\left(  z\right)  \delta\left(  w-z\right)  +2T\left(
z\right)  \delta^{\prime}\left(  w-z\right)  .
\end{align*}
Note that this formula uniquely determines the Lie bracket of the Witt
algebra. This is how physicists would define the Witt algebra.

Now, let us set $T\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}%
z^{-n-2}$\textbf{ in the Virasoro algebra}. (This power series $T$ looks
exactly like the one before, but note that the $L_{n}$ now mean elements of
the Virasoro algebra rather than the Witt algebra.) Then, our previous
computation of $\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  $ must
be modified by adding a term of $\sum\limits_{n\in\mathbb{Z}}\dfrac{n^{3}%
-n}{12}Cz^{-n-2}w^{n-2}=\dfrac{C}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  $. So we get%
\[
\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  =-T^{\prime}\left(
z\right)  \delta\left(  w-z\right)  +2T\left(  z\right)  \delta^{\prime
}\left(  w-z\right)  +\dfrac{C}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  .
\]


\textbf{Exercise:} Check that, if we interpret $L_{n}$ and $a_{m}$ as the
actions of $L_{n}\in\operatorname*{Vir}$ and $a_{m}\in\mathcal{A}$ on the
$\operatorname*{Vir}\ltimes\mathcal{A}$-module $F_{\mu}$, then the following
identity between maps $F_{\mu}\rightarrow F_{\mu}\left(  \left(  z\right)
\right)  \left(  \left(  w\right)  \right)  $ holds:
\[
\left[  T\left(  z\right)  ,a\left(  w\right)  \right]  =a\left(  z\right)
\delta^{\prime}\left(  w-z\right)  .
\]


Recall%
\[
:a_{m}a_{n}:\ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
So we can reasonably define the ``normal ordered'' product $\left.  :a\left(
z\right)  a\left(  w\right)  :\right.  $ to be
\[
\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left.  :a_{n}%
a_{m}:\right.  z^{-n-1}w^{-m-1}\in U\left(  \mathcal{A}\right)  \left[
\left[  z,z^{-1}\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  .
\]
This definition of $\left.  :a\left(  z\right)  a\left(  w\right)  :\right.  $
is equivalent to the definition given in Problem 2 of Problem Set 3.

That $\left.  :a\left(  z\right)  a\left(  w\right)  :\right.  $ is
well-defined is not a surprise: the variables $z$ and $w$ are distinct, so
there are no terms to collect in the sum $\sum\limits_{\left(  n,m\right)
\in\mathbb{Z}^{2}}\left.  :a_{n}a_{m}:\right.  z^{-n-1}w^{-m-1}$, and thus
there is no danger of obtaining an infinite sum which makes no sense (like
what we would get if we would try to define $a\left(  z\right)  ^{2}%
$).\ \ \ \ \footnote{For the same reason, the product $a\left(  z\right)
a\left(  w\right)  $ (without normal ordering) is well-defined.} But it is
more interesting that (although we cannot define $a\left(  z\right)  ^{2}$) we
can define a ``normal ordered'' square $\left.  :a\left(  z\right)
^{2}:\right.  $ (or, what is the same, $\left.  :a\left(  z\right)  a\left(
z\right)  :\right.  $), although it will not be an element of $U\left(
\mathcal{A}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $ but rather of
a suitable completion. We are not going to do elaborate on how to choose this
completion here; but for us it will be enough to notice that, if we
reinterpret the $a_{n}$ as endomorphisms of $F_{\mu}$ (using the action of
$\mathcal{A}$ on $F_{\mu}$) rather than elements of $U\left(  \mathcal{A}%
\right)  $, then the ``normal ordered'' square $\left.  :a\left(  z\right)
^{2}:\right.  $ is a well-defined element of $\left(  \operatorname*{End}%
F_{\mu}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $. Namely:%
\begin{align*}
&  \left.  :a\left(  z\right)  ^{2}:\right. \\
&  =\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left.  :a_{n}%
a_{m}:\right.  z^{-n-1}z^{-m-1}=\sum\limits_{k\in\mathbb{Z}}\left(
\sum\limits_{\substack{\left(  n,m\right)  \in\mathbb{Z}^{2};\\n+m=k}}\left.
:a_{n}a_{m}:\right.  \right)  z^{-k-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{this is how power series are always multiplied; but we don't yet}\\
\text{know that the sum }\sum\limits_{\substack{\left(  n,m\right)
\in\mathbb{Z}^{2};\\n+m=k}}\left.  :a_{n}a_{m}:\right.  \text{ makes sense for
all }k\\
\text{(although we will see in a few lines that it does)}%
\end{array}
\right) \\
&  =\sum\limits_{k\in\mathbb{Z}}\left(  \sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{k-m}:\right.  \right)  z^{-k-2}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we substituted }\left(  m,k-m\right)  \text{ for }\left(
n,m\right)  \right) \\
&  =\sum\limits_{n\in\mathbb{Z}}\left(  \sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }k\text{ by }n\text{ in the first sum,}\\
\text{and we substituted }m\text{ by }-m\text{ in the second sum}%
\end{array}
\right)  ,
\end{align*}
and the sums $\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $
are well-defined for all $n\in\mathbb{Z}$ (by Lemma \ref{lem.fockvir.welldef}
\textbf{(c)}). We can simplify this result if we also reinterpret the
$L_{n}\in\operatorname*{Vir}$ as endomorphisms of $F_{\mu}$ (using the action
of $\operatorname*{Vir}$ on $F_{\mu}$ that was introduced in Proposition
\ref{prop.fockvir.answer2}) rather than elements of $U\left(
\operatorname*{Vir}\right)  $. In fact, the ``series'' $T\left(  z\right)
=\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$ then becomes%
\begin{align*}
T\left(  z\right)   &  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}%
=\sum\limits_{n\in\mathbb{Z}}\dfrac{1}{2}\left(  \sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{def.fockvir.def})}\right) \\
&  =\dfrac{1}{2}\underbrace{\sum\limits_{n\in\mathbb{Z}}\left(  \sum
\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}%
}_{=\left.  :a\left(  z\right)  ^{2}:\right.  }=\dfrac{1}{2}\left.  :a\left(
z\right)  ^{2}:\right.  .
\end{align*}


\begin{remark}
In Definition \ref{def.fockvir.normal}, we have defined the normal ordered
product $\left.  :a_{m}a_{n}:\right.  $ in the universal enveloping algebra of
the Heisenberg algebra. This is not the only situation in which we can define
a normal ordered product, but in other situations the definition can happen to
be different. For example, in Proposition \ref{prop.ramond.rep}, we will
define a normal ordered product (on a different algebra) which will not be
commutative, and not even ``super-commutative''. There is no general rule to
define normal ordered products; it is done on a case-by-case basis.

\textbf{However}, the definition of the normal ordered product of two
\textbf{quantum fields} given in Problem 2 of Problem Set 3 is general, i. e.,
it is defined not only for quantum fields over $U\left(  \mathcal{A}\right)  $.
\end{remark}

\textbf{Exercise 1.} For any $\beta\in\mathbb{C}$, the formula $T\left(
z\right)  =\dfrac{1}{2}\left.  :a\left(  z\right)  ^{2}:\right.  +\beta
a^{\prime}\left(  z\right)  $ defines a representation of $\operatorname*{Vir}%
$ on $F_{\mu}$ with $c=1-12\beta^{2}$.

\textbf{Exercise 2.} For any $\beta\in\mathbb{C}$, there is a homomorphism
$\varphi_{\beta}:\operatorname*{Vir}\rightarrow\operatorname*{Vir}%
\ltimes\mathcal{A}$ (a splitting of the projection $\operatorname*{Vir}%
\ltimes\mathcal{A}\rightarrow\operatorname*{Vir}$) given by%
\begin{align*}
\varphi_{\beta}\left(  L_{n}\right)   &  =L_{n}+\beta a_{n}%
,\ \ \ \ \ \ \ \ \ \ n\neq0;\\
\varphi_{\beta}\left(  L_{0}\right)   &  =L_{0}+\beta a_{0}+\dfrac{\beta^{2}%
}{2}K,\\
\varphi_{\beta}\left(  C\right)   &  =C.
\end{align*}


\textbf{Exercise 3.} If we twist the action of Exercise 1 by this map, we
recover the action of problem 1 of Homework 2 for $\beta=i\lambda$.

\subsubsection{Recognizing exponential series}

Here is a simple property of power series (actually, an algebraic analogue of
the well-known fact from analysis that the solutions of the differential
equation $f^{\prime}=\alpha f$ are scalar multiples of the function
$x\mapsto\exp\left(  \alpha x\right)  $):

\begin{proposition}
\label{prop.euler.recognizing-exp}Let $R$ be a commutative $\mathbb{Q}%
$-algebra. Let $U$ be an $R$-module. Let $\left(  \alpha_{1},\alpha_{2}%
,\alpha_{3},...\right)  $ be a sequence of elements of $R$. Let $P\in U\left[
\left[  x_{1},x_{2},x_{3},...\right]  \right]  $ is a formal power series with
coefficients in $U$ (where $x_{1},x_{2},x_{3},...$ are symbols) such that
every $i>0$ satisfies $\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P$. Then,
there exists some $f\in U$ such that $P=f\cdot\exp\left(  \sum\limits_{j>0}%
x_{j}\alpha_{j}\right)  $.
\end{proposition}

\begin{vershort}
The proof of Proposition \ref{prop.euler.recognizing-exp} is easy (just let
$f$ be the constant term of the power series $P$, and prove by induction that
every monomial of $P$ equals the corresponding monomial of $f\cdot\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  $).
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.euler.recognizing-exp}.} Recall
Convention \ref{conv.fin}. For any power series $S\in U\left[  \left[
x_{1},x_{2},x_{3},...\right]  \right]  $ and every $\left(  m_{1},m_{2}%
,m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, define an element $\operatorname*{Coeff}%
\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }S\in U$ by setting
$\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}S=c_{\left(  m_{1},m_{2},m_{3},...\right)  }$, where the power series $S$ is
written in the form $S=\sum\limits_{\left(  n_{1},n_{2},n_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}c_{\left(
n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ for
some $c_{\left(  n_{1},n_{2},n_{3},...\right)  }\in U$. This element
$\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }S$ is
called the \textit{coefficient of }$S$ \textit{before the monomial }%
$x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$.

Write the power series $P$ in the form $P=\sum\limits_{\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }}b_{\left(  n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ for some $b_{\left(  n_{1},n_{2}%
,n_{3},...\right)  }\in U$. Consider these $b_{\left(  n_{1},n_{2}%
,n_{3},...\right)  }$. Then, $\operatorname*{Coeff}\nolimits_{\left(
m_{1},m_{2},m_{3},...\right)  }P=b_{\left(  m_{1},m_{2},m_{3},...\right)  }$
(by the definition of $\operatorname*{Coeff}\nolimits_{\left(  m_{1}%
,m_{2},m_{3},...\right)  }P$).

We will now prove that for every $K\in\mathbb{N}$, the following holds:%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\text{ such that }%
m_{1}+m_{2}+m_{3}+...=K\\
\text{satisfies }\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2}%
,m_{3},...\right)  }P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}%
\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P
\end{array}
\right)  \label{pf.euler.recognizing-exp.1}%
\end{equation}
\footnote{Here, the product $m_{1}!m_{2}!m_{3}!...$ is well-defined for every
$\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ (because for every $\left(  m_{1}%
,m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, only finitely many $i>0$ satisfy $m_{i}\neq0$, and thus
only finitely many $i>0$ satisfy $m_{i}!\neq1$).}

\textit{Proof of (\ref{pf.euler.recognizing-exp.1}):} We will prove
(\ref{pf.euler.recognizing-exp.1}) by induction over $K$:

\textit{Induction base:} Every $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$m_{1}+m_{2}+m_{3}+...=0$ must satisfy $\left(  m_{1},m_{2},m_{3},...\right)
=\left(  0,0,0,...\right)  $. Hence, every $\left(  m_{1},m_{2},m_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}$ such that $m_{1}+m_{2}+m_{3}+...=0$ must satisfy%
\begin{align*}
m_{1}!m_{2}!m_{3}!...  &  =0!0!0!...\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  m_{1},m_{2},m_{3},...\right)  =\left(  0,0,0,...\right)  \right) \\
&  =1\cdot1\cdot1\cdot...=1.
\end{align*}
Also, every $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that $m_{1}%
+m_{2}+m_{3}+...=0$ must satisfy%
\begin{align*}
\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...  &  =\alpha_{1}%
^{0}\alpha_{2}^{0}\alpha_{3}^{0}...\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  m_{1},m_{2},m_{3},...\right)  =\left(  0,0,0,...\right)  \right) \\
&  =1\cdot1\cdot1\cdot...=1.
\end{align*}


Thus, every $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that $m_{1}%
+m_{2}+m_{3}+...=0$ must also satisfy%
\begin{align*}
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }P  &
=\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }%
P\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  m_{1},m_{2},m_{3}%
,...\right)  =\left(  0,0,0,...\right)  \right) \\
&  =\underbrace{1}_{\substack{=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\\\text{(since
}m_{1}!m_{2}!m_{3}!...=1\text{)}}}\cdot\underbrace{1}_{=\alpha_{1}^{m_{1}%
}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...}\operatorname*{Coeff}%
\nolimits_{\left(  0,0,0,...\right)  }P\\
&  =\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P.
\end{align*}
In other words, (\ref{pf.euler.recognizing-exp.1}) holds for $K=0$. This
completes the induction base.

\textit{Induction step:} Let $\kappa\in\mathbb{N}$. Assume that
(\ref{pf.euler.recognizing-exp.1}) holds for $K=\kappa$. We now must prove
that (\ref{pf.euler.recognizing-exp.1}) also holds for $K=\kappa+1$.

Let $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ be such that $m_{1}+m_{2}+m_{3}%
+...=\kappa+1$. Then, $m_{1}+m_{2}+m_{3}+...=\kappa+1\geq1>0$, so that there
exists at least one positive integer $i$ such that $m_{i}>0$. Consider this
$i$. Since $m_{i}>0$, we have $m_{i}-1\in\mathbb{N}$, so that $\left(
m_{1},m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$.

We have%
\begin{align*}
&  \dfrac{\partial P}{\partial x_{i}}=\dfrac{\partial}{\partial x_{i}}%
P=\dfrac{\partial}{\partial x_{i}}\left(  \sum\limits_{\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }}b_{\left(  n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }}b_{\left(  n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right) \\
&  =\sum\limits_{\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\left(  n_{i}+1\right)
b_{\left(  n_{1},n_{2},...,n_{i-1},n_{i}+1,n_{i+1},n_{i+2},...\right)  }%
x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...
\end{align*}
(by the definition of $\dfrac{\partial}{\partial x_{i}}$), so that%
\begin{equation}
\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)  }%
\dfrac{\partial P}{\partial x_{i}}=\left(  k_{i}+1\right)  b_{\left(
k_{1},k_{2},...,k_{i-1},k_{i}+1,k_{i+1},k_{i+2},...\right)  }
\label{pf.euler.recognizing-exp.14}%
\end{equation}
for every $\left(  k_{1},k_{2},k_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$.

Now, define $\left(  k_{1},k_{2},k_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ to be the sequence
$\left(  m_{1},m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $. Then,
\begin{align*}
\left(  k_{1},k_{2},...,k_{i-1},k_{i}+1,k_{i+1},k_{i+2},...\right)   &
=\left(  m_{1},m_{2},...,m_{i-1},\underbrace{\left(  m_{i}-1\right)
+1}_{=m_{i}},m_{i+1},m_{i+2},...\right) \\
&  =\left(  m_{1},m_{2},...,m_{i-1},m_{i},m_{i+1},m_{i+2},...\right)  =\left(
m_{1},m_{2},m_{3},...\right)  .
\end{align*}
Hence, (\ref{pf.euler.recognizing-exp.14}) rewrites as%
\begin{equation}
\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)  }%
\dfrac{\partial P}{\partial x_{i}}=\left(  k_{i}+1\right)  b_{\left(
m_{1},m_{2},m_{3},...\right)  }. \label{pf.euler.recognizing-exp.15}%
\end{equation}


Also, since $\left(  k_{1},k_{2},k_{3},...\right)  =\left(  m_{1}%
,m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $, we have
\begin{align*}
k_{1}+k_{2}+k_{3}+...  &  =m_{1}+m_{2}+...+m_{i-1}+\left(  m_{i}-1\right)
+m_{i+1}+m_{i+2}+...\\
&  =\underbrace{\left(  m_{1}+m_{2}+...+m_{i-1}+m_{i}+m_{i+1}+m_{i+2}%
+...\right)  }_{=m_{1}+m_{2}+m_{3}+...=\kappa+1}-1=\kappa+1-1=\kappa.
\end{align*}
Hence, we can apply (\ref{pf.euler.recognizing-exp.1}) to $\kappa$ and
$\left(  k_{1},k_{2},k_{3},...\right)  $ instead of $K$ and $\left(
m_{1},m_{2},m_{3},...\right)  $ (since we have assumed that
(\ref{pf.euler.recognizing-exp.1}) holds for $K=\kappa$). As a consequence, we
obtain%
\begin{equation}
\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)
}P=\dfrac{1}{k_{1}!k_{2}!k_{3}!...}\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}%
\alpha_{3}^{k_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P. \label{pf.euler.recognizing-exp.16}%
\end{equation}


Recall that $\left(  k_{1},k_{2},k_{3},...\right)  =\left(  m_{1}%
,m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $. Thus, every positive
integer $j$ such that $j\neq i$ satisfies $k_{j}=m_{j}$. Hence, $\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{k_{j}}=\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}}$ and $\prod
\limits_{\substack{j>0;\\j\neq i}}k_{j}!=\prod\limits_{\substack{j>0;\\j\neq
i}}m_{j}!$.

On the other hand, from $\left(  k_{1},k_{2},k_{3},...\right)  =\left(
m_{1},m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $, we obtain
$k_{i}=m_{i}-1$, so that $k_{i}+1=m_{i}$. But%
\[
\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}}...=\prod\limits_{j>0}%
\alpha_{j}^{k_{j}}=\underbrace{\alpha_{i}^{k_{i}}}_{\substack{=\alpha
_{i}^{m_{i}-1}\\\text{(since }k_{i}=m_{i}-1\text{)}}}\cdot\underbrace{\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{k_{j}}}_{=\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}}}=\alpha_{i}^{m_{i}%
-1}\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}},
\]
so that%
\begin{equation}
\alpha_{i}\cdot\left(  \alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}%
}...\right)  =\underbrace{\alpha_{i}\cdot\alpha_{i}^{m_{i}-1}}_{=\alpha
_{i}^{m_{i}}}\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}%
}=\alpha_{i}^{m_{i}}\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\alpha
_{j}^{m_{j}}=\prod\limits_{j>0}\alpha_{j}^{m_{j}}=\alpha_{1}^{m_{1}}\alpha
_{2}^{m_{2}}\alpha_{3}^{m_{3}}.... \label{pf.euler.recognizing-exp.18}%
\end{equation}
Besides,%
\begin{align}
k_{1}!k_{2}!k_{3}!...  &  =\prod\limits_{j>0}k_{j}!=\underbrace{k_{i}}%
_{=m_{i}-1}!\cdot\underbrace{\prod\limits_{\substack{j>0;\\j\neq i}}k_{j}%
!}_{=\prod\limits_{\substack{j>0;\\j\neq i}}m_{j}!}=\underbrace{\left(
m_{i}-1\right)  !}_{=\dfrac{m_{i}!}{m_{i}}}\cdot\prod
\limits_{\substack{j>0;\\j\neq i}}m_{j}!\nonumber\\
&  =\dfrac{1}{m_{i}}\cdot\underbrace{m_{i}!\cdot\prod
\limits_{\substack{j>0;\\j\neq i}}m_{j}!}_{=\prod\limits_{j>0}m_{j}%
!=m_{1}!m_{2}!m_{3}!...}=\dfrac{1}{m_{i}}m_{1}!m_{2}!m_{3}!....
\label{pf.euler.recognizing-exp.19}%
\end{align}
Now, $\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=b_{\left(  m_{1},m_{2},m_{3},...\right)  }$, so that%
\begin{align*}
m_{i}\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }P
&  =\underbrace{m_{i}}_{=k_{i}+1}b_{\left(  m_{1},m_{2},m_{3},...\right)
}=\left(  k_{i}+1\right)  b_{\left(  m_{1},m_{2},m_{3},...\right)  }\\
&  =\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)
}\dfrac{\partial P}{\partial x_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.euler.recognizing-exp.15})}\right) \\
&  =\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)
}\left(  \alpha_{i}P\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P\right) \\
&  =\alpha_{i}\underbrace{\operatorname*{Coeff}\nolimits_{\left(  k_{1}%
,k_{2},k_{3},...\right)  }P}_{\substack{=\dfrac{1}{k_{1}!k_{2}!k_{3}%
!...}\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}}%
...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }P\\\text{(by
(\ref{pf.euler.recognizing-exp.16}))}}}\\
&  =\alpha_{i}\dfrac{1}{k_{1}!k_{2}!k_{3}!...}\alpha_{1}^{k_{1}}\alpha
_{2}^{k_{2}}\alpha_{3}^{k_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P\\
&  =\underbrace{\dfrac{1}{k_{1}!k_{2}!k_{3}!...}}_{\substack{=\dfrac{1}%
{\dfrac{1}{m_{i}}m_{1}!m_{2}!m_{3}!...}\\\text{(by
(\ref{pf.euler.recognizing-exp.19}))}}}\cdot\underbrace{\alpha_{i}\cdot\left(
\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}}...\right)
}_{\substack{=\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}%
}...\\\text{(by (\ref{pf.euler.recognizing-exp.18}))}}}\operatorname*{Coeff}%
\nolimits_{\left(  0,0,0,...\right)  }P\\
&  =\dfrac{1}{\dfrac{1}{m_{i}}m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}%
\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P\\
&  =m_{i}\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}%
}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P.
\end{align*}
We can divide this equality by $m_{i}$ (since $m_{i}>0$). As a result, we
obtain%
\[
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P.
\]


Now, forget that we fixed $\left(  m_{1},m_{2},m_{3},...\right)  $. We thus
have proven that every $\left(  m_{1},m_{2},m_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$m_{1}+m_{2}+m_{3}+...=\kappa+1$ satisfies%
\[
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P.
\]
In other words, (\ref{pf.euler.recognizing-exp.1}) holds for $K=\kappa+1$.
This completes the induction step. The induction proof of
(\ref{pf.euler.recognizing-exp.1}) is thus complete.

Now, we see that
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\text{ satisfies}\\
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P
\end{array}
\right)  \label{pf.euler.recognizing-exp.28}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.euler.recognizing-exp.28}):} Let $\left(
m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$. Then, $m_{1}+m_{2}+m_{3}+...=m_{1}+m_{2}+m_{3}+...$.
Hence, applying (\ref{pf.euler.recognizing-exp.1}) to $K=m_{1}+m_{2}%
+m_{3}+...$, we obtain $\operatorname*{Coeff}\nolimits_{\left(  m_{1}%
,m_{2},m_{3},...\right)  }P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}%
}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}%
\nolimits_{\left(  0,0,0,...\right)  }P$. This proves
(\ref{pf.euler.recognizing-exp.28}).}

Now, define an element $f\in U$ by $f=\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P$. Define a power series $Q\in U\left[  \left[
x_{1},x_{2},x_{3},...\right]  \right]  $ by $Q=f\cdot\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  $. Then,%
\begin{align*}
\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }Q  &
=\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }\left(  f\cdot
\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  \right) \\
&  =f\cdot\underbrace{\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }\left(  \exp\left(  \sum\limits_{j>0}x_{j}\alpha
_{j}\right)  \right)  }_{\substack{=1\\\text{(since the constant term of the
exponential series is }1\text{)}}}=f.
\end{align*}


It is easy to see that every integer $i>0$ satisfies%
\begin{equation}
\dfrac{\partial Q}{\partial x_{i}}=\alpha_{i}Q.
\label{pf.euler.recognizing-exp.35}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.euler.recognizing-exp.35}):} Let $i>0$ be
an integer. Since $R\left[  \left[  x_{1},x_{2},x_{3},...\right]  \right]  $
is a commutative $\mathbb{Q}$-algebra, we have%
\[
\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  =\prod\limits_{j>0}%
\exp\left(  x_{j}\alpha_{j}\right)  =\exp\left(  x_{i}\alpha_{i}\right)
\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(  x_{j}\alpha
_{j}\right)  .
\]
Thus,%
\begin{align*}
&  \dfrac{\partial}{\partial x_{i}}\exp\left(  \sum\limits_{j>0}x_{j}%
\alpha_{j}\right) \\
&  =\dfrac{\partial}{\partial x_{i}}\left(  \exp\left(  x_{i}\alpha
_{i}\right)  \cdot\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(
x_{j}\alpha_{j}\right)  \right) \\
&  =\underbrace{\left(  \dfrac{\partial}{\partial x_{i}}\exp\left(
x_{i}\alpha_{i}\right)  \right)  }_{=\alpha_{i}\exp\left(  x_{i}\alpha
_{i}\right)  }\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(
x_{j}\alpha_{j}\right)  +\exp\left(  x_{i}\alpha_{i}\right)  \cdot
\underbrace{\dfrac{\partial}{\partial x_{i}}\left(  \prod
\limits_{\substack{j>0;\\j\neq i}}\exp\left(  x_{j}\alpha_{j}\right)  \right)
}_{\substack{=0\\\text{(since the variable }x_{i}\text{ never appears in
the}\\\text{power series }\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(
x_{j}\alpha_{j}\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the Leibniz rule}\right) \\
&  =\alpha_{i}\underbrace{\exp\left(  x_{i}\alpha_{i}\right)  \cdot
\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(  x_{j}\alpha_{j}\right)
}_{=\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  }+\underbrace{\exp
\left(  x_{i}\alpha_{i}\right)  \cdot0}_{=0}=\alpha_{i}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  .
\end{align*}
Now,%
\begin{align*}
\dfrac{\partial Q}{\partial x_{i}}  &  =\dfrac{\partial}{\partial x_{i}%
}Q=\dfrac{\partial}{\partial x_{i}}\left(  f\cdot\exp\left(  \sum
\limits_{j>0}x_{j}\alpha_{j}\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }Q=f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)
\right) \\
&  =f\cdot\underbrace{\dfrac{\partial}{\partial x_{i}}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  }_{=\alpha_{i}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  }=f\cdot\alpha_{i}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  =\alpha_{i}\cdot\underbrace{\left(
f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  \right)  }%
_{=Q}=\alpha_{i}Q.
\end{align*}
This proves (\ref{pf.euler.recognizing-exp.35}).}

Now, let us recollect what we have done. We have proven the property
(\ref{pf.euler.recognizing-exp.28}) for every power series $P\in U\left[
\left[  x_{1},x_{2},x_{3},...\right]  \right]  $ such that every $i>0$
satisfies $\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P$. Since we know that
$Q\in U\left[  \left[  x_{1},x_{2},x_{3},...\right]  \right]  $ also is a
power series such that every $i>0$ satisfies $\dfrac{\partial Q}{\partial
x_{i}}=\alpha_{i}Q$ (due to (\ref{pf.euler.recognizing-exp.28})), we can
therefore apply (\ref{pf.euler.recognizing-exp.28}) to $Q$ instead of $P$. As
a result, we obtain:%
\[
\left(
\begin{array}
[c]{c}%
\text{every }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\text{ satisfies}\\
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}Q=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}Q
\end{array}
\right)  .
\]
Hence, for every $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$, we have%
\begin{align*}
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }Q  &
=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\underbrace{\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }Q}_{=f=\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P}\\
&  =\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P=\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }P
\end{align*}
(by (\ref{pf.euler.recognizing-exp.28})). In other words, for every $\left(
m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, the coefficient of the power series $Q$ before the
monomial $x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ equals the coefficient
of the power series $P$ before the monomial $x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...$. In other words, the coefficients of the power series $Q$
equal the corresponding coefficients of the power series $P$. Thus, the power
series $Q$ itself equals $P$, so that we have%
\[
P=Q=f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  .
\]
This proves Proposition \ref{prop.euler.recognizing-exp}.
\end{verlong}

\subsubsection{Homogeneous maps and equigraded series}

The discussion we will be doing now is only vaguely related to power series
(let alone quantum fields); it is meant as a preparation for a later proof
(namely, that of Theorem \ref{thm.euler}), where it will provide
``convergence'' assertions (in a certain sense).

A well-known nuisance in the theory of $\mathbb{Z}$-graded vector spaces is
the fact that the endomorphism ring of a $\mathbb{Z}$-graded vector space is
not (in general) $\mathbb{Z}$-graded. It does, however, contain a $\mathbb{Z}%
$-graded subring, which we will introduce now:

\begin{definition}
\label{def.hg}\textbf{(a)} Let $V$ and $W$ be two $\mathbb{Z}$-graded vector
spaces, with gradings $\left(  V\left[  n\right]  \right)  _{n\in\mathbb{Z}}$
and $\left(  W\left[  n\right]  \right)  _{n\in\mathbb{Z}}$, respectively. Let
$f:V\rightarrow W$ be a linear map. Let $m\in\mathbb{Z}$. Then, $f$ is said to
be a \textit{homogeneous linear map of degree }$m$ if every $n\in\mathbb{Z}$
satisfies $f\left(  V\left[  n\right]  \right)  \subseteq W\left[  n+m\right]
$.

(It is important not to confuse this notion of ``homogeneous linear maps of
degree $m$'' with the notion of ``homogeneous polynomial maps of degree $n$''
defined in Definition \ref{def.det.US.poly.hom} \textbf{(a)}; the former of
these notions is not a particular case of the latter.)

Note that the homogeneous linear maps of degree $0$ are exactly the graded
linear maps.

\textbf{(b)} Let $V$ and $W$ be two $\mathbb{Z}$-graded vector spaces. For
every $m\in\mathbb{Z}$, let $\operatorname*{Hom}\nolimits_{\operatorname{hg}%
=m}\left(  V,W\right)  $ denote the vector space of all homogeneous linear
maps $V\rightarrow W$ of degree $m$. This $\operatorname*{Hom}%
\nolimits_{\operatorname{hg}=m}\left(  V,W\right)  $ is a vector subspace of
$\operatorname*{Hom}\left(  V,W\right)  $ for every $m\in\mathbb{Z}$.
Moreover, $\bigoplus\limits_{m\in\mathbb{Z}}\operatorname*{Hom}%
\nolimits_{\operatorname{hg}=m}\left(  V,W\right)  $ is a well-defined
internal direct sum, and will be denoted by $\operatorname*{Hom}%
\nolimits_{\operatorname{hg}}\left(  V,W\right)  $. This $\operatorname*{Hom}%
\nolimits_{\operatorname{hg}}\left(  V,W\right)  $ is a vector subspace of
$\operatorname*{Hom}\left(  V,W\right)  $, and is canonically a $\mathbb{Z}%
$-graded vector space, with its $m$-th graded component being
$\operatorname*{Hom}\nolimits_{\operatorname{hg}=m}\left(  V,W\right)  $.

\textbf{(c)} Let $V$ be a $\mathbb{Z}$-graded vector space. Then, let
$\operatorname*{End}\nolimits_{\operatorname{hg}}V$ denote the $\mathbb{Z}%
$-graded vector subspace $\operatorname*{Hom}\nolimits_{\operatorname{hg}%
}\left(  V,V\right)  $ of $\operatorname*{Hom}\left(  V,V\right)
=\operatorname*{End}V$. Then, $\operatorname*{End}\nolimits_{\operatorname{hg}%
}V$ is a subalgebra of $\operatorname*{End}V$, and a $\mathbb{Z}$-graded
algebra. Moreover, the canonical action of $\operatorname*{End}%
\nolimits_{\operatorname{hg}}V$ on $V$ (obtained by restricting the action of
$\operatorname*{End}V$ on $V$ to $\operatorname*{End}%
\nolimits_{\operatorname{hg}}V$) makes $V$ into a $\mathbb{Z}$-graded
$\operatorname*{End}\nolimits_{\operatorname{hg}}V$-module.
\end{definition}

We next need a relatively simple notion for a special kind of power series. I
(Darij) call them ``equigraded power series'', though noone else seems to use
this nomenclature.

\begin{definition}
Let $B$ be a $\mathbb{Z}$-graded vector space, and $z$ a symbol. An element
$\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$ of $B\left[  \left[  z,z^{-1}\right]
\right]  $ (with $b_{n}\in B$ for every $n\in\mathbb{Z}$) is said to be
\textit{equigraded} if every $n\in\mathbb{Z}$ satisfies $b_{n}\in B\left[
n\right]  $ (where $\left(  B\left[  m\right]  \right)  _{m\in\mathbb{Z}}$
denotes the grading on $B$). Since $B\left[  \left[  z\right]  \right]  $ and
$B\left(  \left(  z\right)  \right)  $ are vector subspaces of $B\left[
\left[  z,z^{-1}\right]  \right]  $, it clearly makes sense to speak of
equigraded elements of $B\left[  \left[  z\right]  \right]  $ or of $B\left(
\left(  z\right)  \right)  $. We will denote by $B\left[  \left[
z,z^{-1}\right]  \right]  _{\operatorname*{equi}}$ the set of all equigraded
elements of $B\left[  \left[  z,z^{-1}\right]  \right]  $. It is easy to see
that $B\left[  \left[  z,z^{-1}\right]  \right]  _{\operatorname*{equi}}$ is a
vector subspace of $B\left[  \left[  z,z^{-1}\right]  \right]  $.
\end{definition}

Elementary properties of equigraded elements are:

\begin{proposition}
\label{prop.equigraded.basics}\textbf{(a)} Let $B$ be a $\mathbb{Z}$-graded
vector space, and $z$ a symbol. Then,%
\begin{align*}
&  \left\{  f\in B\left[  z\right]  \ \mid\ f\text{ is equigraded}\right\}
,\ \ \ \ \ \ \ \ \ \ \left\{  f\in B\left[  z,z^{-1}\right]  \ \mid\ f\text{
is equigraded}\right\}  ,\\
&  \left\{  f\in B\left[  \left[  z\right]  \right]  \ \mid\ f\text{ is
equigraded}\right\}  ,\ \ \ \ \ \ \ \ \ \ \left\{  f\in B\left(  \left(
z\right)  \right)  \ \mid\ f\text{ is equigraded}\right\}  ,\\
&  \left\{  f\in B\left[  \left[  z,z^{-1}\right]  \right]  \ \mid\ f\text{ is
equigraded}\right\}  =B\left[  \left[  z,z^{-1}\right]  \right]
_{\operatorname*{equi}}%
\end{align*}
are vector spaces.

\textbf{(b)} Let $B$ be a $\mathbb{Z}$-graded algebra. Then, $\left\{  f\in
B\left[  \left[  z\right]  \right]  \ \mid\ f\text{ is equigraded}\right\}  $
is a subalgebra of $B\left[  \left[  z\right]  \right]  $ and closed with
respect to the usual topology on $B\left[  \left[  z\right]  \right]  $.

\textbf{(c)} Let $B$ be a $\mathbb{Z}$-graded algebra. If $f\in B\left[
\left[  z\right]  \right]  $ is an equigraded power series and invertible in
the ring $B\left[  \left[  z\right]  \right]  $, then $f^{-1}$ also is an
equigraded power series.
\end{proposition}

We will only use parts \textbf{(a)} and \textbf{(b)} of this proposition, and
these are completely straightforward to prove. (Part \textbf{(c)} is less
straightforward but still an easy exercise.)

Equigradedness of power series sometimes makes their actions on modules more
manageable. Here is an example:

\begin{proposition}
\label{prop.equigraded.fx}Let $A$ be a $\mathbb{Z}$-graded algebra, and let
$M$ be a $\mathbb{Z}$-graded $A$-module. Assume that $M$ is concentrated in
nonnegative degrees. Let $u$ be a symbol.

\textbf{(a)} It is clear that for any $f\in A\left[  \left[  u,u^{-1}\right]
\right]  $ and any $x\in M\left[  u,u^{-1}\right]  $, the product $fx$ is a
well-defined element of $M\left[  \left[  u,u^{-1}\right]  \right]  $.

\textbf{(b)} For any \textbf{equigraded} $f\in A\left[  \left[  u,u^{-1}%
\right]  \right]  $ and any $x\in M\left[  u,u^{-1}\right]  $, the product
$fx$ is a well-defined element of $M\left(  \left(  u\right)  \right)  $ (and
not only of $M\left[  \left[  u,u^{-1}\right]  \right]  $).

\textbf{(c)} For any \textbf{equigraded} $f\in A\left[  \left[  u^{-1}\right]
\right]  $ and any $x\in M\left[  u^{-1}\right]  $, the product $fx$ is a
well-defined element of $M\left[  u^{-1}\right]  $ (and not only of $M\left[
\left[  u^{-1}\right]  \right]  $).
\end{proposition}

\begin{vershort}
The proof of this proposition is quick and straightforward. (The only idea is
that for any fixed $x\in M\left[  u,u^{-1}\right]  $, any sufficiently
low-degree element of $A$ annihilates $x$ due to the \textquotedblleft
concentrated in nonnegative degrees\textquotedblright\ assumption, but
sufficiently low-degree monomials in $f$ come with sufficiently low-degree
coefficients due to $f$ being equigraded.)
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.equigraded.fx}.} Denote by $\left(
A\left[  m\right]  \right)  _{m\in\mathbb{Z}}$ the grading on $A$. Denote by
$\left(  M\left[  m\right]  \right)  _{m\in\mathbb{Z}}$ the $\mathbb{Z}%
$-grading on $M$.

\textbf{(a)} Part \textbf{(a)} of Proposition \ref{prop.equigraded.fx} is obvious.

\textbf{(b)} Let $f\in A\left[  \left[  u,u^{-1}\right]  \right]  $ be
equigraded. Let $x\in M\left[  u,u^{-1}\right]  $. We must show that $fx\in
M\left(  \left(  u\right)  \right)  $.

Since $M$ is concentrated in nonnegative degrees, we have $M=\bigoplus
\limits_{m\geq0}M\left[  m\right]  =\sum\limits_{m\geq0}M\left[  m\right]  $
(since direct sums are sums). Hence, $M\cdot u^{n}=\left(  \sum\limits_{m\geq
0}M\left[  m\right]  \right)  \cdot u^{n}=\sum\limits_{m\geq0}M\left[
m\right]  \cdot u^{n}$ for every $n\in\mathbb{Z}$. But
\begin{align*}
x  &  \in M\left[  u,u^{-1}\right]  =\sum\limits_{n\in\mathbb{Z}%
}\underbrace{M\cdot u^{n}}_{=\sum\limits_{m\geq0}M\left[  m\right]  \cdot
u^{n}}=\sum\limits_{n\in\mathbb{Z}}\sum\limits_{m\geq0}M\left[  m\right]
\cdot u^{n}=\sum\limits_{m\geq0}\underbrace{\sum\limits_{n\in\mathbb{Z}%
}M\left[  m\right]  \cdot u^{n}}_{=\left(  M\left[  m\right]  \right)  \left[
u,u^{-1}\right]  }\\
&  =\sum\limits_{m\geq0}\left(  M\left[  m\right]  \right)  \left[
u,u^{-1}\right]  .
\end{align*}
Hence, we can write the element $x$ in the form $x=\sum\limits_{m\geq0}x_{m}%
$\text{ for some family }$\left(  x_{m}\right)  _{m\in\mathbb{N}}$ of elements
of $M\left[  u,u^{-1}\right]  $ satisfying $\left(  x_{m}\in\left(  M\left[
m\right]  \right)  \left[  u,u^{-1}\right]  \text{ for every }m\in
\mathbb{N}\right)  $ and \newline$\left(  x_{m}=0\text{ for all but finitely
many }m\in\mathbb{N}\right)  $.

Since $\left(  x_{m}=0\text{ for all but finitely many }m\in\mathbb{N}\right)
$, there exists a finite set $S\subseteq\mathbb{N}$ such that every
$m\in\mathbb{N}\setminus S$ satisfies $x_{m}=0$. Consider this $S$.

Since $S$ is a finite set, the set $S$ has an upper bound. That is, there
exists a $t\in\mathbb{N}$ such that every $m\in S$ satisfies $m\leq t$.
Consider this $t$. Then, every $m\in\mathbb{N}$ such that $m>t$ satisfies
$m\in\mathbb{N}\setminus S$ (because otherwise, it would satisfy
$m\notin\mathbb{N}\setminus S$, thus $m\in S$, thus $m\leq t$, contradicting
$m>t$). Consequently, every $m\in\mathbb{N}$ such that $m>t$ satisfies
$x_{m}=0$ (since we know that every $m\in\mathbb{N}\setminus S$ satisfies
$x_{m}=0$). Thus, $\sum\limits_{\substack{m\geq0;\\m>t}}\underbrace{x_{m}%
}_{\substack{=0\\\text{(since }m>t\text{)}}}=\sum\limits_{\substack{m\geq
0;\\m>t}}0=0$. But%
\[
x=\sum\limits_{m\geq0}x_{m}=\sum\limits_{\substack{m\geq0;\\m\leq t}%
}x_{m}+\underbrace{\sum\limits_{\substack{m\geq0;\\m>t}}x_{m}}_{=0}%
=\sum\limits_{\substack{m\geq0;\\m\leq t}}x_{m}.
\]


Write $f\in A\left[  \left[  u,u^{-1}\right]  \right]  $ in the form
$f=\sum\limits_{n\in\mathbb{Z}}b_{n}u^{n}$. Then, every $n\in\mathbb{Z}$
satisfies $b_{n}\in A\left[  n\right]  $ (since $f$ is equigraded). Hence,
every $n\in\mathbb{Z}$ such that $n<-t$ satisfies%
\begin{align*}
b_{n}x  &  =b_{n}\sum\limits_{\substack{m\geq0;\\m\leq t}}x_{m}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }x=\sum\limits_{\substack{m\geq
0;\\m\leq t}}x_{m}\right) \\
&  =\sum\limits_{\substack{m\geq0;\\m\leq t}}\underbrace{b_{n}}_{\in A\left[
n\right]  }\underbrace{x_{m}}_{\in\left(  M\left[  m\right]  \right)  \left[
u,u^{-1}\right]  =\sum\limits_{k\in\mathbb{Z}}M\left[  m\right]  \cdot u^{k}%
}\in\sum\limits_{\substack{m\geq0;\\m\leq t}}\underbrace{\left(  A\left[
n\right]  \right)  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}M\left[  m\right]
\cdot u^{k}\right)  }_{=\sum\limits_{k\in\mathbb{Z}}A\left[  n\right]  \cdot
M\left[  m\right]  \cdot u^{k}}\\
&  =\sum\limits_{\substack{m\geq0;\\m\leq t}}\sum\limits_{k\in\mathbb{Z}%
}\underbrace{A\left[  n\right]  \cdot M\left[  m\right]  }%
_{\substack{\subseteq M\left[  n+m\right]  \\\text{(since }M\text{ is a
}\mathbb{Z}\text{-graded }A\text{-module)}}}\cdot u^{k}\subseteq
\sum\limits_{\substack{m\geq0;\\m\leq t}}\sum\limits_{k\in\mathbb{Z}%
}\underbrace{M\left[  n+m\right]  }_{\substack{=0\\\text{(because from
}n<-t\text{ and }m\leq t\text{, we obtain}\\n+m<\left(  -t\right)  +t=0\text{,
so that }M\left[  n+m\right]  =0\\\text{(because }M\text{ is concentrated in
nonnegative}\\\text{degrees))}}}\cdot u^{k}\\
&  =\sum\limits_{\substack{m\geq0;\\m\leq t}}\sum\limits_{k\in\mathbb{Z}%
}0\cdot u^{k}=0.
\end{align*}
In other words, every $n\in\mathbb{Z}$ such that $n<-t$ satisfies $b_{n}x=0$.
Hence, from $f=\sum\limits_{n\in\mathbb{Z}}b_{n}u^{n}$, we conclude that
\begin{align*}
fx  &  =\sum\limits_{n\in\mathbb{Z}}b_{n}u^{n}x=\sum\limits_{n\in\mathbb{Z}%
}b_{n}xu^{n}=\sum\limits_{\substack{n\in\mathbb{Z};\\n<-t}}\underbrace{b_{n}%
x}_{\substack{=0\\\text{(since }n<-t\text{)}}}u^{n}+\sum
\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}}b_{n}xu^{n}\\
&  =\underbrace{\sum\limits_{\substack{n\in\mathbb{Z};\\n<-t}}0u^{n}}%
_{=0}+\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}}b_{n}xu^{n}%
=\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}}b_{n}xu^{n}%
=\underbrace{\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}%
}b_{n}u^{n}\right)  }_{\in A\left(  \left(  u\right)  \right)  }%
\cdot\underbrace{x}_{\in M\left[  u,u^{-1}\right]  \subseteq M\left(  \left(
u\right)  \right)  }\\
&  \in A\left(  \left(  u\right)  \right)  \cdot M\left(  \left(  u\right)
\right)  \subseteq M\left(  \left(  u\right)  \right)  .
\end{align*}
This proves Proposition \ref{prop.equigraded.fx} \textbf{(b)}.

\textbf{(c)} Let $f\in A\left[  \left[  u^{-1}\right]  \right]  $ be
equigraded. Let $x\in M\left[  u^{-1}\right]  $. We must show that $fx\in
M\left[  u^{-1}\right]  $.

Since $f\in A\left[  \left[  u^{-1}\right]  \right]  \subseteq A\left[
\left[  u,u^{-1}\right]  \right]  $ and $x\in M\left[  u^{-1}\right]
\subseteq M\left[  u,u^{-1}\right]  $, we can apply Proposition
\ref{prop.equigraded.fx} \textbf{(b)} and obtain $fx\in M\left(  \left(
u\right)  \right)  $. Thus, we can write the Laurent series $fx$ in the form
$fx=\sum\limits_{n\in\mathbb{Z}}g_{n}u^{n}$ with $g_{n}$ being elements of $M$
such that $\left(  \text{only finitely many among the negative }n\in
\mathbb{Z}\text{ satisfy }g_{n}\neq0\right)  $. Consider these $g_{n}$. We
have%
\[
\sum\limits_{n\in\mathbb{Z}}g_{n}u^{n}=\underbrace{f}_{\in A\left[  \left[
u^{-1}\right]  \right]  }\underbrace{x}_{\in M\left[  u^{-1}\right]  \subseteq
M\left[  \left[  u^{-1}\right]  \right]  }\in A\left[  \left[  u^{-1}\right]
\right]  \cdot M\left[  \left[  u^{-1}\right]  \right]  \subseteq M\left[
\left[  u^{-1}\right]  \right]  .
\]
Hence, every positive $n\in\mathbb{Z}$ satisfies $g_{n}=0$ (because the
coefficient of any power series in $M\left[  \left[  u^{-1}\right]  \right]  $
before $u^{n}$ must be $0$ for any positive $n\in\mathbb{Z}$). Now,%
\begin{align*}
fx  &  =\sum\limits_{n\in\mathbb{Z}}g_{n}u^{n}=\sum\limits_{\substack{n\in
\mathbb{Z};\\n<0}}g_{n}u^{n}+g_{0}u^{0}+\sum\limits_{\substack{n\in
\mathbb{Z};\\n>0}}\underbrace{g_{n}}_{\substack{=0\\\text{(since }n\text{ is
positive)}}}u^{n}\\
&  =\sum\limits_{\substack{n\in\mathbb{Z};\\n<0}}g_{n}u^{n}+g_{0}%
u^{0}+\underbrace{\sum\limits_{\substack{n\in\mathbb{Z};\\n>0}}0u^{n}}%
_{=0}=\underbrace{\sum\limits_{\substack{n\in\mathbb{Z};\\n<0}}g_{n}u^{n}%
}_{\substack{\in M\left[  u^{-1}\right]  \\\text{(because only finitely
many}\\\text{among the negative }n\in\mathbb{Z}\\\text{satisfy }g_{n}%
\neq0\text{)}}}+\underbrace{g_{0}u^{0}}_{\in M\left[  u^{-1}\right]  }\in
M\left[  u^{-1}\right]  +M\left[  u^{-1}\right]  \subseteq M\left[
u^{-1}\right]  .
\end{align*}
This proves Proposition \ref{prop.equigraded.fx} \textbf{(c)}.
\end{verlong}

\subsection{\textbf{[unfinished]} More on unitary representations}

Let us consider the Verma modules of the Virasoro algebra.

\textbf{Last time:} $L_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is
unitary (for $\lambda,\mu\in\mathbb{R}$), so the $\operatorname*{Vir}$-module
$L_{h,c}$ is unitary if $c\geq1$ and $h\geq\dfrac{c-1}{24}$.

We can extend this as follows: $L_{0,1}^{\otimes m-1}\otimes L_{h,c}$ is
unitary and has a highest-weight vector $v_{0,1}^{\otimes m-1}\otimes v_{h,c}$
which has weight $\left(  h,c+m-1\right)  $. Hence, the representation
$L_{h,c+m-1}$ is unitary [why? use irreducibility of unitary modules and stuff].

Hence, $L_{h,c}$ is unitary if $c\geq m$ and $h\geq\dfrac{c-m}{24}$.

\begin{theorem}
In fact, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq0$.
\end{theorem}

But this is harder to show.

This is still not an only-if. For example, $L_{0,0}$ is unitary (and $1$-dimensional).

\begin{proposition}
\label{prop.Lhc.unitary.triv}If $L_{h,c}$ is unitary, then $h\geq0$ and
$c\geq0$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Lhc.unitary.triv}.} Assume that
$L_{h,c}$ is unitary. Then, $\left(  L_{-n}v_{h,c},L_{-n}v_{h,c}\right)
\geq0$ for every $n\in\mathbb{Z}$. But every positive $n\in\mathbb{Z}$
satisfies
\begin{align*}
\left(  L_{-n}v_{h,c},L_{-n}v_{h,c}\right)   &  =\left(  \underbrace{L_{n}%
L_{-n}}_{=\left[  L_{n},L_{-n}\right]  +L_{-n}L_{n}}v_{h,c},v_{h,c}\right)
=\left(  \underbrace{\left(  \left[  L_{n},L_{-n}\right]  +L_{-n}L_{n}\right)
v_{h,c}}_{\substack{=\left[  L_{n},L_{-n}\right]  v_{h,c}\\\text{(since
}L_{-n}L_{n}v_{h,c}=0\text{)}}},v_{h,c}\right) \\
&  =\left(  \underbrace{\left[  L_{n},L_{-n}\right]  }_{=2nL_{0}+\dfrac
{n^{3}-n}{12}C}v_{h,c},v_{h,c}\right)  =2nh+\dfrac{n^{3}-n}{12}c.
\end{align*}
Thus, $2nh+\dfrac{n^{3}-n}{12}c\geq0$ for every positive $n\in\mathbb{Z}$.
From this, by taking $n\rightarrow\infty$, we obtain $c\geq0$. By taking
$n=1$, we get $h\geq0$. This proves Proposition \ref{prop.Lhc.unitary.triv}.

\begin{definition}
\label{def.Cdelta}Let $\delta\in\left\{  0,\dfrac{1}{2}\right\}  $. Let
$C_{\delta}$ be the $\mathbb{C}$-algebra with generators $\left\{  \psi
_{j}\ \mid\ j\in\delta+\mathbb{Z}\right\}  $ and relations%
\[
\psi_{j}\psi_{k}+\psi_{k}\psi_{j}=\delta_{k,-j}\ \ \ \ \ \ \ \ \ \ \text{for
all }j,k\in\delta+\mathbb{Z}.
\]
This $\mathbb{C}$-algebra $C_{\delta}$ is an infinite-dimensional Clifford
algebra (namely, the Clifford algebra of the free vector space with basis
$\left\{  \psi_{j}\ \mid\ j\in\delta+\mathbb{Z}\right\}  $ and bilinear form
$\left(  \psi_{j},\psi_{k}\right)  \mapsto\dfrac{1}{2}\delta_{k,-j}$). The
algebra $C_{\delta}$ is called an \textit{algebra of free fermions}. For
$\delta=0$, it is called the \textit{Ramond sector}; for $\delta=\dfrac{1}{2}$
it is called \textit{Neveu-Schwarz sector}.

Let us now construct a representation $V_{\delta}$ of $C_{\delta}$: Let
$V_{\delta}$ be the $\mathbb{C}$-algebra $\wedge\left(  \xi_{n}\ \mid
\ n\in\left(  \delta+\mathbb{Z}\right)  _{\geq0}\right)  $. For any
$i\in\delta+\mathbb{Z}$, define an operator $\dfrac{\partial}{\partial\xi_{i}%
}:V_{\delta}\rightarrow V_{\delta}$ by%
\begin{align*}
&  \dfrac{\partial}{\partial\xi_{i}}\left(  \xi_{j_{1}}\wedge\xi_{j_{2}}%
\wedge...\wedge\xi_{j_{k}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{1},j_{2},...,j_{k}\right\}
;\\
\left(  -1\right)  ^{\ell-1}\xi_{j_{1}}\wedge\xi_{j_{2}}\wedge...\wedge
\xi_{j_{\ell-1}}\wedge\xi_{j_{\ell+1}}\wedge\xi_{j_{\ell+2}}\wedge...\wedge
\xi_{j_{k}},\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{1},j_{2}%
,...,j_{k}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }j_{1}<j_{2}<...<j_{k}\text{ in
}\delta+\mathbb{Z}\right.  ,
\end{align*}
where, in the case when $i\in\left\{  j_{1},j_{2},...,j_{k}\right\}  $, we
denote by $\ell$ the element $u$ of $\left\{  1,2,...,k\right\}  $ satisfying
$j_{\ell}=u$. (Note the $\left(  -1\right)  ^{\ell-1}$ sign, which
distinguishes this ``differentiation'' from differentiation in the commutative
case. This is a particular case of the Koszul sign rule.)

Define an action of $C_{\delta}$ on $V_{\delta}$ by%
\begin{align*}
\psi_{-n}  &  \mapsto\xi_{n}\ \ \ \ \ \ \ \ \ \ \text{for }n<0;\\
\psi_{n}  &  \mapsto\dfrac{\partial}{\partial\xi_{n}}%
\ \ \ \ \ \ \ \ \ \ \text{for }n>0;\\
\psi_{0}  &  \mapsto\dfrac{1}{\sqrt{2}}\left(  \dfrac{\partial}{\partial
\xi_{0}}+\xi_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{this is only
relevant if }\delta=0\right)  .
\end{align*}


This indeed defines a representation of $C_{\delta}$ (exercise!). This is an
infinite-dimensional analogue of the well-known spinor representation of
Clifford algebras.
\end{definition}

From Homework Set 2 problem 2, we know:

\begin{proposition}
\label{prop.ramond.rep}Let $\delta\in\left\{  0,\dfrac{1}{2}\right\}  $. For
every $k\in\mathbb{Z}$, define an endomorphism $L_{k}$ of $V_{\delta}$ by%
\[
L_{k}=\delta_{k,0}\dfrac{1-2\delta}{16}+\dfrac{1}{2}\sum\limits_{j\in
\delta+\mathbb{Z}}j\left.  :\psi_{-j}\psi_{j+k}:\right.  ,
\]
where the normal ordered product is defined as follows:%
\[
\left.  :\psi_{n}\psi_{m}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
-\psi_{m}\psi_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
\psi_{n}\psi_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
Then:

\textbf{(a)} Every $m\in\delta+\mathbb{Z}$ and $k\in\mathbb{Z}$ satisfy
$\left[  \psi_{m},L_{k}\right]  =\left(  m+\dfrac{k}{2}\right)  \psi_{m+k}$.

\textbf{(b)} Every $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfy $\left[
L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\delta_{n,-m}\dfrac{m^{3}%
-m}{24}$. (Hence, $V_{\delta}$ is a representation of $\operatorname*{Vir}$
with central charge $c=\dfrac{1}{2}$).
\end{proposition}

Now this representation $V_{\delta}$ of $\operatorname*{Vir}$ is unitary. In
fact, consider the Hermitian form under which all monomials in $\psi_{i}$ are
orthonormal (positive definite). Then it is easy to see that $\psi_{j}^{\dag
}=\psi_{-j}$. Thus, $L_{n}^{\dag}=L_{-n}$.

But these representations $V_{\delta}$ are reducible. In fact, we can define a
$\left(  \mathbb{Z}\diagup2\mathbb{Z}\right)  $-grading on $V_{\delta}$ by
giving each $\xi_{n}$ the degree $\overline{1}$, and then the operators
$L_{n}$ preserve parity (i. e., degree under this grading), so that the
representation $V_{\delta}$ can be decomposed as a direct sum $V_{\delta
}=V_{\delta}^{+}\oplus V_{\delta}^{-}$, where $V_{\delta}^{+}$ is the set of
the even elements of $V_{\delta}$, and $V_{\delta}^{-}$ is the set of the odd
elements of $V_{\delta}$.

\begin{theorem}
These subrepresentations $V_{\delta}^{+}$ and $V_{\delta}^{-}$ are irreducible
Virasoro modules.
\end{theorem}

We will not prove this.

What are the highest weights of $V_{\delta}^{+}$ and $V_{\delta}^{-}$ ?

First consider the case $\delta=0$. The highest-weight vector of $V_{\delta
}^{+}$ is $1$, with weight $\left(  \dfrac{1}{16},\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{0}$, with weight $\left(  \dfrac{1}{16},\dfrac
{1}{2}\right)  $. Thus, $V_{\delta}^{+}\cong V_{\delta}^{-}$ by action of
$\psi_{0}$ (since $\psi_{0}^{2}=\dfrac{1}{2}$).

Now consider the case $\delta=\dfrac{1}{2}$. The highest-weight vector of
$V_{\delta}^{+}$ is $1$, with weight $\left(  0,\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{1/2}$, with weight $\left(  \dfrac{1}{2}%
,\dfrac{1}{2}\right)  $.

\begin{corollary}
The representation $L_{h,\dfrac{1}{2}}$ is unitary if $h=0$, $h=\dfrac{1}{16}$
or $h=\dfrac{1}{2}$. (In physics: Ising model.)
\end{corollary}

We will not prove:

\begin{proposition}
This is an only-if as well.
\end{proposition}

General answer for $c<1$: for $c=1-\dfrac{6}{\left(  m+2\right)  \left(
m+3\right)  }$ for $m\in\mathbb{N}$, there are finitely many $h$ where
$L_{h,c}$ is unitary. For other values of $c$, there are no such values.

\begin{definition}
The \textit{character }$\operatorname*{ch}\nolimits_{V}\left(  q\right)  $ of
a $\operatorname*{Vir}$-module $V$ from category $\mathcal{O}^{+}$ is
$\operatorname*{Tr}\nolimits_{V}\left(  q^{L_{0}}\right)  =\sum\left(  \dim
V_{\lambda}\right)  q^{\lambda}$ for $V_{\lambda}=$generalized eigenspace of
$L_{0}$ with eigenvalue $\lambda$.
\end{definition}

This is related to the old definition of character [how?]

What are the characters of the above modules? Since $V_{\delta}^{+}%
=\wedge\left(  \xi_{1},\xi_{2},\xi_{3},...\right)  ^{+}$, we have%
\[
\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
=q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}\right)
...=q^{1/16}\prod\limits_{n\geq1}\left(  1+q^{n}\right)
\]
(because
\begin{align*}
2\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{0}}\left(  q\right)  =q^{1/16}\left(
1+1\right)  \left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}%
\right)  ...\\
&  =2q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(
1+q^{3}\right)  ...
\end{align*}
).

Now%
\begin{align*}
\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)
+\operatorname*{ch}\nolimits_{L_{\dfrac{1}{2},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{\dfrac{1}{2}}}\left(  q\right)  =\left(
1+q^{1/2}\right)  \left(  1+q^{3/2}\right)  \left(  1+q^{5/2}\right)  ...\\
&  =\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  .
\end{align*}
Thus, $\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)  $ is
the integer part of the product $\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}%
}\left(  1+q^{n}\right)  $, and $\operatorname*{ch}\nolimits_{L_{\dfrac{1}%
{2},\dfrac{1}{2}}}\left(  q\right)  $ is the half-integer part of the product
$\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  $.

With this, we conclude our study of $V_{\delta}$.

\begin{Convention}
The notation $\psi_{j}$ for the generators of $C_{\delta}$ introduced in
Definition \ref{def.Cdelta} will not be used in the following. (Instead, we
will use the notation $\psi_{j}$ for some completely different objects.)
\end{Convention}

\subsection{The Lie algebra \texorpdfstring{$\mathfrak{gl}_{\infty}$}
{gl-infinity} and its representations}

For every $n\in\mathbb{N}$, we can define a Lie algebra $\mathfrak{gl}_{n}$ of
$n\times n$-matrices over $\mathbb{C}$. One can wonder how this can be
generalized to the ``$n=\infty$ case'', i. e., to infinite matrices.
Obviously, not every pair of infinite matrices has a reasonable commutator
(because not any such pair can be multiplied), but there are certain
restrictions on infinite matrices which allow us to multiply them and form
their commutators. These restrictions can be used to define various Lie
algebras consisting of infinite matrices. We will be concerned with some such
Lie algebras; the first of them is $\mathfrak{gl}_{\infty}$:

\begin{definition}
\label{def.glinf.glinf}We define $\mathfrak{gl}_{\infty}$ to be the vector
space of infinite matrices whose rows and columns are labeled by integers (not
only positive integers!) such that only finitely many entries of the matrix
are nonzero. This vector space $\mathfrak{gl}_{\infty}$ is an associative
algebra \textit{without unit} (by matrix multiplication); we can thus make
$\mathfrak{gl}_{\infty}$ into a Lie algebra by the commutator in this
associative algebra.
\end{definition}

We will study the representations of this $\mathfrak{gl}_{\infty}$. The theory
of these representations will extend the well-known (Schur-Weyl) theory of
representations of $\mathfrak{gl}_{n}$.

\begin{definition}
\label{def.glinf.V}The \textit{vector representation} $V$ of $\mathfrak{gl}%
_{\infty}$ is defined as the vector space $\mathbb{C}^{\left(  \mathbb{Z}%
\right)  }=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }%
\mid\ x_{i}\in\mathbb{C}\text{; only finitely many }x_{i}\text{ are
nonzero}\right\}  $. The Lie algebra $\mathfrak{gl}_{\infty}$ acts on the
vector representation $V$ in the obvious way: namely, for any $a\in
\mathfrak{gl}_{\infty}$ and $v\in V$, we let $a\rightharpoonup v$ be the
product of the matrix $a$ with the column vector $v$. Here, every element
$\left(  x_{i}\right)  _{i\in\mathbb{Z}}$ of $V$ is identified with the column
vector $\left(
\begin{array}
[c]{c}%
...\\
x_{-2}\\
x_{-1}\\
x_{0}\\
x_{1}\\
x_{2}\\
...
\end{array}
\right)  $.

For every $j\in\mathbb{Z}$, let $v_{j}$ be the vector $\left(  \delta
_{i,j}\right)  _{i\in\mathbb{Z}}\in V$. Then, $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ is a basis of the vector space $V$.
\end{definition}

\begin{Convention}
When we draw infinite matrices whose rows and columns are labeled by integers,
the index of the rows is supposed to increase as we go from left to right, and
the index of the columns is supposed to increase as we go from top to bottom.
\end{Convention}

\begin{remark}
In Definition \ref{def.glinf.V}, we used the following (very simple) fact: For
every $a\in\mathfrak{gl}_{\infty}$ and every $v\in V$, the product $av$ of the
matrix $a$ with the column vector $v$ is a well-defined element of $V$. This
fact can be generalized: If $a$ is an infinite matrix (whose rows and columns
are labeled by integers) such that every column of $a$ has only finitely many
nonzero entries, and $v$ is an element of $V$, then the product $av$ is a
well-defined element of $V$. However, this does \textbf{no longer} hold if we
drop the condition that every column of $a$ have only finitely many nonzero
entries. (For example, if $a$ would be the matrix whose all entries equal $1$,
then the product $av_{0}$ would \textbf{not} be an element of $V$, but rather
the element $\left(
\begin{array}
[c]{c}%
...\\
1\\
1\\
1\\
1\\
1\\
...
\end{array}
\right)  $ of the \textbf{larger} vector space $\mathbb{C}^{\mathbb{Z}%
}=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }\mid\ x_{i}%
\in\mathbb{C}\right\}  $. Besides, the product $a\left(
\begin{array}
[c]{c}%
...\\
1\\
1\\
1\\
1\\
1\\
...
\end{array}
\right)  $ would not make any sense at all, not even in $\mathbb{C}%
^{\mathbb{Z}}$.)
\end{remark}

We can consider the representation $\wedge^{i}V$ of $\mathfrak{gl}_{\infty}$
for every $i\in\mathbb{N}$. More generally, we have the so-called
\textit{Schur modules}:

\begin{definition}
If $\pi\in\operatorname*{Irr}S_{n}$, then we can define a representation
$S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ by $S_{\pi}\left(
V\right)  =\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes
n}\right)  $ (where $S_{n}$ acts on $V^{\otimes n}$ by permuting the
tensorands). This $S_{\pi}\left(  V\right)  $ is called the $\pi$\textit{-th
Schur module} of $V$.
\end{definition}

This definition mimics the well-known definition (or, more precisely, one of
the definitions) of the Schur modules of a finite-dimensional vector space.

\begin{proposition}
\label{prop.glinf.schur.irred}For every $\pi\in\operatorname*{Irr}S_{n}$, the
representation $S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ is irreducible.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.schur.irred}.} The following is
not a self-contained proof; it is just a way to reduce Proposition
\ref{prop.glinf.schur.irred} to the similar fact about finite-dimensional
vector spaces (which is a well-known fact in the representation theory of
$\mathfrak{gl}_{m}$).

For every vector subspace $W\subseteq V$, we can canonically identify $S_{\pi
}\left(  W\right)  $ with a vector subspace of $S_{\pi}\left(  V\right)  $.

For every subset $I$ of $\mathbb{Z}$, let $W_{I}$ be the subset of $V$
generated by all $v_{i}$ with $i\in I$. Clearly, whenever two subsets $I$ and
$J$ of $\mathbb{Z}$ satisfy $I\subseteq J$, we have $W_{I}\subseteq W_{J}$.
Also, whenever $I$ is a finite subset of $\mathbb{Z}$, the vector space
$W_{I}$ is finite-dimensional.

For every tensor $u\in V^{\otimes n}$, there exists a finite subset $I$ of
$\mathbb{Z}$ such that $u\in\left(  W_{I}\right)  ^{\otimes n}$%
.\ \ \ \ \footnote{\textit{Proof.} The family $\left(  v_{i_{1}}\otimes
v_{i_{2}}\otimes...\otimes v_{i_{n}}\right)  _{\left(  i_{1},i_{2}%
,...,i_{n}\right)  \in\mathbb{Z}^{n}}$ is a basis of $V^{\otimes n}$ (since
$\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ is a basis of $V$). Thus, we can
write the tensor $u\in V^{\otimes n}$ as a $\mathbb{C}$-linear combination of
finitely many tensors of the form $v_{i_{1}}\otimes v_{i_{2}}\otimes...\otimes
v_{i_{n}}$ with $\left(  i_{1},i_{2},...,i_{n}\right)  \in\mathbb{Z}^{n}$. Let
$I$ be the union of the sets $\left\{  i_{1},i_{2},...,i_{n}\right\}  $ over
all the tensors which appear in this linear combination. Since only finitely
many tensors appear in this linear combination, the set $I$ is finite. Every
tensor $v_{i_{1}}\otimes v_{i_{2}}\otimes...\otimes v_{i_{n}}$ which appears
in this linear combination satisfies $\left\{  i_{1},i_{2},...,i_{n}\right\}
\subseteq I$ (by the construction of $I$) and thus $v_{i_{1}}\otimes v_{i_{2}%
}\otimes...\otimes v_{i_{n}}\in\left(  W_{I}\right)  ^{\otimes n}$. Thus, $u$
must lie in $\left(  W_{I}\right)  ^{\otimes n}$, too (because $u$ is the
value of this linear combination). Hence, we have found a finite subset $I$ of
$\mathbb{Z}$ such that $u\in\left(  W_{I}\right)  ^{\otimes n}$. Qed.} Denote
this subset $I$ by $I\left(  u\right)  $. Thus, $u\in\left(  W_{I\left(
u\right)  }\right)  ^{\otimes n}$ for every $u\in V^{\otimes n}$.

For every $w\in S_{\pi}\left(  V\right)  $, there exists some finite subset
$I$ of $\mathbb{Z}$ such that $w\in S_{\pi}\left(  W_{I}\right)
$.\ \ \ \ \footnote{\textit{Proof.} Let $w\in S_{\pi}\left(  V\right)  $.
Then, $w\in S_{\pi}\left(  V\right)  =\operatorname*{Hom}\nolimits_{S_{n}%
}\left(  \pi,V^{\otimes n}\right)  $. But since $\pi$ is a finite-dimensional
vector space, the image $w\left(  \pi\right)  $ must be finite-dimensional.
Hence, $w\left(  \pi\right)  $ is a finite-dimensional vector subspace of
$V^{\otimes n}$. Thus, $w\left(  \pi\right)  $ is generated by some elements
$u_{1},u_{2},...,u_{k}\in V^{\otimes n}$. Let $I$ be the union $\bigcup
\limits_{j=1}^{k}I\left(  u_{j}\right)  $. Then, $I$ is finite (because for
every $j\in\left\{  1,2,...,k\right\}  $, the set $I\left(  u_{j}\right)  $ is
finite) and satisfies $I\left(  u_{j}\right)  \subseteq I$ for every
$j\in\left\{  1,2,...,k\right\}  $.
\par
Recall that every $u\in V^{\otimes n}$ satisfies $u\in\left(  W_{I\left(
u\right)  }\right)  ^{\otimes n}$. Thus, every $j\in\left\{
1,2,...,k\right\}  $ satisfies $u_{j}\in\left(  W_{I\left(  u_{j}\right)
}\right)  ^{\otimes n}\subseteq\left(  W_{I}\right)  ^{\otimes n}$ (since
$I\left(  u_{j}\right)  \subseteq I$ and thus $W_{I\left(  u_{j}\right)
}\subseteq W_{I}$). In other words, all $k$ elements $u_{1},u_{2},...,u_{k}$
lie in the vector space $\left(  W_{I}\right)  ^{\otimes n}$. Since the
elements $u_{1},u_{2},...,u_{k}$ generate the subspace $w\left(  \pi\right)
$, this yields that $w\left(  \pi\right)  \subseteq\left(  W_{I}\right)
^{\otimes n}$. Hence, the map $w:\pi\rightarrow V^{\otimes n}$ factors through
a map $\pi\rightarrow\left(  W_{I}\right)  ^{\otimes n}$. In other words,
$w\in\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes n}\right)  $
is contained in $\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,\left(
W_{I}\right)  ^{\otimes n}\right)  =S_{\pi}\left(  W_{I}\right)  $, qed.}
Denote this subset $I$ by $I\left(  w\right)  $. Thus, $w\in S_{\pi}\left(
W_{I\left(  w\right)  }\right)  $ for every $w\in S_{\pi}\left(  V\right)  $.

Let $w$ and $w^{\prime}$ be two vectors in $S_{\pi}\left(  V\right)  $ such
that $w\neq0$. We are going to prove that $w^{\prime}\in U\left(
\mathfrak{gl}_{\infty}\right)  w$. Once this is proven, it will be obvious
that $S_{\pi}\left(  V\right)  $ is irreducible, and we will be done.

There exists a finite subset $I$ of $\mathbb{Z}$ such that $w\in S_{\pi
}\left(  W_{I}\right)  $ and $w^{\prime}\in S_{\pi}\left(  W_{I}\right)
$.\ \ \ \ \footnote{\textit{Proof.} Let $I=I\left(  w\right)  \cup I\left(
w^{\prime}\right)  $. Then, $I$ is a finite subset of $\mathbb{Z}$ (since
$I\left(  w\right)  $ and $I\left(  w^{\prime}\right)  $ are finite subsets of
$\mathbb{Z}$), and $I\left(  w\right)  \subseteq I$ and $I\left(  w^{\prime
}\right)  \subseteq I$. We have $w\in S_{\pi}\left(  W_{I\left(  w\right)
}\right)  \subseteq S_{\pi}\left(  W_{I}\right)  $ (since $I\left(  w\right)
\subseteq I$ and thus $W_{I\left(  w\right)  }\subseteq W_{I}$) and similarly
$w^{\prime}\in S_{\pi}\left(  W_{I}\right)  $. Thus, there exists a finite
subset $I$ of $\mathbb{Z}$ such that $w\in S_{\pi}\left(  W_{I}\right)  $ and
$w^{\prime}\in S_{\pi}\left(  W_{I}\right)  $, qed.} Consider this $I$.

Since $I$ is finite, the vector space $W_{I}$ is finite-dimensional. Thus, by
the analogue of Proposition \ref{prop.glinf.schur.irred} for representations
of $\mathfrak{gl}_{m}$, the representation $S_{\pi}\left(  W_{I}\right)  $ of
the Lie algebra $\mathfrak{gl}\left(  W_{I}\right)  $ is irreducible. Hence,
$w^{\prime}\in U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w$.

Now, we have a canonical injective Lie algebra homomorphism $\mathfrak{gl}%
\left(  W_{I}\right)  \rightarrow\mathfrak{gl}_{\infty}$\ \ \ \ \footnote{Here
is how it is defined: For every linear map $A\in\mathfrak{gl}\left(
W_{I}\right)  $, we define a linear map $A^{\prime}\in\mathfrak{gl}\left(
V\right)  $ by setting%
\[
A^{\prime}v_{i}=\left\{
\begin{array}
[c]{c}%
Av_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\in I;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin I
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\mathbb{Z}.
\]
This linear map $A^{\prime}$ is represented (with respect to the basis
$\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ of $V$) by an infinite matrix whose
rows and columns are labeled by integers. This matrix lies in $\mathfrak{gl}%
_{\infty}$.
\par
Thus, we have assigned to every $A\in\mathfrak{gl}\left(  W_{I}\right)  $ a
matrix in $\mathfrak{gl}_{\infty}$. This defines an injective Lie algebra
homomorphism $\mathfrak{gl}\left(  W_{I}\right)  \rightarrow\mathfrak{gl}%
_{\infty}$.}. Thus, we can view $\mathfrak{gl}\left(  W_{I}\right)  $ as a Lie
subalgebra of $\mathfrak{gl}_{\infty}$ in a canonical way. Moreover, the
classical action $\mathfrak{gl}\left(  W_{I}\right)  \times S_{\pi}\left(
W_{I}\right)  \rightarrow S_{\pi}\left(  W_{I}\right)  $ of the Lie algebra
$\mathfrak{gl}\left(  W_{I}\right)  $ on the Schur module $S_{\pi}\left(
W_{I}\right)  $ can be viewed as the restriction of the action $\mathfrak{gl}%
_{\infty}\times S_{\pi}\left(  V\right)  \rightarrow S_{\pi}\left(  V\right)
$ to $\mathfrak{gl}\left(  W_{I}\right)  \times S_{\pi}\left(  W_{I}\right)
$. Hence, $U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w\subseteq
U\left(  \mathfrak{gl}_{\infty}\right)  w$. Since we know that $w^{\prime}\in
U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w$, we thus conclude
$w^{\prime}\in U\left(  \mathfrak{gl}_{\infty}\right)  w$. This completes the
proof of Proposition \ref{prop.glinf.schur.irred}.

On the other hand, we can define so-called \textit{highest-weight
representations}. Before we do so, let us make $\mathfrak{gl}_{\infty}$ into a
graded Lie algebra:

\begin{definition}
\label{def.glinf.grade}For every $i\in\mathbb{Z}$, let $\mathfrak{gl}_{\infty
}^{i}$ be the subspace of $\mathfrak{gl}_{\infty}$ which consists of matrices
which have nonzero entries only on the $i$-th diagonal. (The $i$\textit{-th
diagonal} consists of the entries in the $\left(  \alpha,\beta\right)  $-th
places with $\beta-\alpha=i$.)

Then, $\mathfrak{gl}_{\infty}=\bigoplus\limits_{i\in\mathbb{Z}}\mathfrak{gl}%
_{\infty}^{i}$, and this makes $\mathfrak{gl}_{\infty}$ into a $\mathbb{Z}%
$-graded Lie algebra. Note that $\mathfrak{gl}_{\infty}^{0}$ is abelian. Let
$\mathfrak{gl}_{\infty}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\mathfrak{gl}_{\infty}$,
so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}\mathfrak{gl}%
_{\infty}^{i}$ is the space of all strictly lower-triangular matrices in
$\mathfrak{gl}_{\infty}$, the subspace $\mathfrak{h}=\mathfrak{gl}_{\infty
}^{0}$ is the space of all diagonal matrices in $\mathfrak{gl}_{\infty}$, and
the subspace $\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\mathfrak{gl}_{\infty
}^{i}$ is the space of all strictly upper-triangular matrices in
$\mathfrak{gl}_{\infty}$.
\end{definition}

\begin{definition}
For every $i,j\in\mathbb{Z}$, let $E_{i,j}$ be the matrix (with rows and
columns labeled by integers) whose $\left(  i,j\right)  $-th entry is $1$ and
whose all other entries are $0$. Then, $\left(  E_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$ is a basis of the vector space $\mathfrak{gl}%
_{\infty}$.
\end{definition}

\begin{definition}
For every $\lambda\in\mathfrak{h}^{\ast}$, let $M_{\lambda}$ be the
highest-weight Verma module $M_{\lambda}^{+}$ (as defined in Definition
\ref{def.verma}). Let $J_{\lambda}=\operatorname*{Ker}\left(  \cdot
,\cdot\right)  \subseteq M_{\lambda}$ be the maximal proper graded submodule.
Let $L_{\lambda}$ be the quotient module $M_{\lambda}\diagup J_{\lambda
}=M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$; then, $L_{\lambda}$
is irreducible (as we know).
\end{definition}

\begin{definition}
We can define an antilinear $\mathbb{R}$-antiinvolution $\dag:\mathfrak{gl}%
_{\infty}\rightarrow\mathfrak{gl}_{\infty}$ on $\mathfrak{gl}_{\infty}$ by
setting%
\[
E_{i,j}^{\dag}=E_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  i,j\right)
\in\mathbb{Z}^{2}.
\]
(Thus, $\dag:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}_{\infty}$ is the
operator which transposes a matrix and then applies complex conjugation to
each of its entries.) Thus we can speak of Hermitian and unitary
$\mathfrak{gl}_{\infty}$-modules.
\end{definition}

A very important remark:

For the Lie algebra $\mathfrak{gl}_{n}$, the highest-weight modules are the
Schur modules up to tensoring with a power of the determinant module. (More
precisely: For $\mathfrak{gl}_{n}$, every finite-dimensional irreducible
representation and any unitary irreducible representation is of the form
$S_{\pi}\left(  V_{n}\right)  \otimes\left(  \wedge^{n}\left(  V_{n}^{\ast
}\right)  \right)  ^{\otimes j}$ for some partition $\pi$ and some
$j\in\mathbb{N}$, where $V_{n}$ is the $\mathfrak{gl}_{n}$-module
$\mathbb{C}^{n}$.)

Nothing like this is true for $\mathfrak{gl}_{\infty}$. Instead, exterior
powers of $V$ and highest-weight representations live ``in different worlds''.
This is because $V$ is composed of infinite-dimensional vectors which have
``no top or bottom''; $V$ has no highest or lowest weight and does not lie in
category $\mathcal{O}^{+}$ or $\mathcal{O}^{-}$.

This is important, because many beautiful properties of representations of
$\mathfrak{gl}_{n}$ come from the equality of the highest-weight and Schur
module representations.

A way to marry these two worlds is by considering so-called
\textit{semiinfinite wedges}.

\subsubsection{Semiinfinite wedges}

Let us first give an informal definition of semiinfinite wedges and the
semiinfinite wedge space $\wedge^{\dfrac{\infty}{2}}V$ (we will later define
these things formally):

An \textit{elementary semiinfinite wedge} will mean a formal infinite ``wedge
product'' $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with $\left(
i_{0},i_{1},i_{2},...\right)  $ being a sequence of integers satisfying
$i_{0}>i_{1}>i_{2}>...$ and $i_{k+1}=i_{k}-1$ for all sufficiently large $k$.
(At the moment, we consider this wedge product $v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...$ just as a fancy symbol for the sequence $\left(
i_{0},i_{1},i_{2},...\right)  $.)

The \textit{semiinfinite wedge space} $\wedge^{\dfrac{\infty}{2}}V$ is defined
as the free vector space with basis given by elementary semiinfinite wedges.

Note that, despite the notation $\wedge^{\dfrac{\infty}{2}}V$, the
semiinfinite wedge space is not a functor in the vector space $V$. We could
replace our definition of $\wedge^{\dfrac{\infty}{2}}V$ by a somewhat more
functorial one, which doesn't use the basis $\left(  v_{i}\right)
_{i\in\mathbb{Z}}$ of $V$ anymore. But it would still need a topology on $V$
(which makes $V$ locally linearly compact), and some working with formal
Laurent series. It proceeds through the semiinfinite Grassmannian, and will
not be done in these lectures.\footnote{Some pointers to the more functorial
definition:
\par
Consider the field $\mathbb{C}\left(  \left(  t\right)  \right)  $ of formal
Laurent series over $\mathbb{C}$ as a $\mathbb{C}$-vector space.
\par
Let $\operatorname*{Gr}=\left\{  U\text{ vector subspace of }\mathbb{C}\left(
\left(  t\right)  \right)  \ \mid\ \left(
\begin{array}
[c]{c}%
U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ and}\\
\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[  \left[  t\right]  \right]
\right)  \right)  <\infty
\end{array}
\right)  \text{ for some sufficiently high }n\right\}  $.
\par
For every $U\in\operatorname*{Gr}$, define an integer $\operatorname*{sdim}U$
by $\operatorname*{sdim}U=\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[
\left[  t\right]  \right]  \right)  \right)  -n$ for any $n\in\mathbb{Z}$
satisfying $U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]  $.
Note that this integer does not depend on $n$ as long as $n$ is sufficiently
high to satisfy $U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]
$.
\par
This Grassmannian $\operatorname*{Gr}$ is the disjoint union $\coprod
\operatorname*{Gr}\nolimits_{n}$.
\par
There is something called a determinant line bundle on $\operatorname*{Gr}$.
The space of semiinfinite wedges is then defined as the space of regular
sections of this line bundle (in the sense of algebraic geometry).
\par
See the book by Pressley and Segal about loop groups for explanations of these
matters.} For us, the definition using the basis will be enough.

The space $\wedge^{\dfrac{\infty}{2}}V$ is countably dimensional. More
precisely, we can write $\wedge^{\dfrac{\infty}{2}}V$ as
\begin{align*}
\wedge^{\dfrac{\infty}{2}}V  &  =\bigoplus\limits_{m\in\mathbb{Z}}%
\wedge^{\dfrac{\infty}{2},m}V,\ \ \ \ \ \ \ \ \ \ \text{where}\\
\wedge^{\dfrac{\infty}{2},m}V  &  =\operatorname*{span}\left\{  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for
sufficiently large }k\right\}  .
\end{align*}
The space $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for sufficiently
large }k\right\}  $, which is easily seen to be countable. We will see later
that this basis can be naturally labeled by partitions (of all integers, not
just of $m$).

\subsubsection{The action of \texorpdfstring{$\mathfrak{gl}_{\infty}$}
{gl-infinity} on \texorpdfstring{$\wedge^{\dfrac{\infty}{2}}V$}{the
semi-infinite wedge space}}

For every $m\in\mathbb{Z}$, we want to define an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the space $\wedge^{\dfrac{\infty}{2},m}V$ which is
given ``by the usual Leibniz rule'', i. e., satisfies the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ (where, of course,
$a\rightharpoonup v_{i_{k}}$ is the same as $av_{i_{k}}$ due to our definition
of the action of $\mathfrak{gl}_{\infty}$ on $V$). Of course, it is not
immediately clear how to interpret the infinite wedge products $v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...$ on the right
hand side of this equation, since they are (in general) not elementary
semiinfinite wedges anymore. We must find a reasonable definition for such
wedge products. What properties should a wedge product (infinite as it is)
satisfy? It should be multilinear\footnote{i. e., it should satisfy
\begin{align*}
&  b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  \lambda
b+\lambda^{\prime}b^{\prime}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  =\lambda b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge b\wedge
b_{k+1}\wedge b_{k+2}\wedge...+\lambda^{\prime}b_{0}\wedge b_{1}%
\wedge...\wedge b_{k-1}\wedge b^{\prime}\wedge b_{k+1}\wedge b_{k+2}\wedge...
\end{align*}
for all $k\in\mathbb{N}$, $b_{0},b_{1},b_{2},...\in V$, $b,b^{\prime}\in V$
and $\lambda,\lambda^{\prime}\in\mathbb{C}$ for which the right hand side is
well-defined} and antisymmetric\footnote{i. e., a well-defined wedge product
$b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ should be $0$ whenever two of the
$b_{k}$ are equal}. These properties make it possible to compute any wedge
product of the form $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ with $b_{0}%
,b_{1},b_{2},...$ being vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
In fact, whenever we are given such vectors $b_{0},b_{1},b_{2},...$, we can
compute the wedge product $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ by the
following procedure:

\begin{itemize}
\item Find an integer $M\in\mathbb{N}$ such that every $i\geq M$ satisfies
$b_{i}=v_{m-i}$. (This $M$ exists by the condition that $b_{i}=v_{m-i}$ for
sufficiently large $i$.)

\item Expand each of the vectors $b_{0},b_{1},...,b_{M-1}$ as a $\mathbb{C}%
$-linear combination of the basis vectors $v_{\ell}$.

\item Using these expansions and the multilinearity of the wedge product,
reduce the computation of $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ to the
computation of finitely many wedge products of basis vectors.

\item Each wedge product of basis vectors can now be computed as follows: If
two of the basis vectors are equal, then it must be $0$ (by antisymmetry of
the wedge product). If not, reorder the basis vectors in such a way that their
indices decrease (this is possible, because ``most'' of these basis vectors
are already in order, and only the first few must be reordered). Due to the
antisymmetry of the wedge product, the wedge product of the basis vectors
before reordering must be $\left(  -1\right)  ^{\pi}$ times the wedge product
of the basis vectors after reordering, where $\pi$ is the permutation which
corresponds to our reordering. But the wedge product of the basis vectors
after reordering is an elementary semiinfinite wedge, and thus we know how to
compute it.
\end{itemize}

This procedure is not exactly a formal definition, and it is not immediately
clear that the value of $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ that it
computes is independent of, e. g., the choice of $M$. In the following
subsection (Subsection \ref{subsect.degress}), we will give a formal version
of this definition.

\subsubsection{\label{subsect.degress}The
\texorpdfstring{$\mathfrak{gl}_{\infty}$}{gl-infinity}-module
\texorpdfstring{$\wedge^{\dfrac{\infty}{2}}V$}{structure on the semi-infinite
wedge space}: a formal definition}

Before we formally define the value of $b_{0}\wedge b_{1}\wedge b_{2}%
\wedge...$, let us start from scratch and repeat the definitions of
$\wedge^{\dfrac{\infty}{2}}V$ and $\wedge^{\dfrac{\infty}{2},m}V$ in a cleaner
fashion than how we defined them above.

\begin{Warning}
Some of the nomenclature defined in the following (particularly, the notions
of ``$m$-degression'' and ``straying $m$-degression'') is mine (=Darij's). I
don't know whether there are established names for these things.
\end{Warning}

First, we introduce the notion of $m$\textit{-degressions} and formalize the
definitions of $\wedge^{\dfrac{\infty}{2}}V$ and $\wedge^{\dfrac{\infty}{2}%
,m}V$.

\begin{definition}
\label{def.glinf.m-deg}Let $m\in\mathbb{Z}$. An $m$\textit{-degression} will
mean a strictly decreasing sequence $\left(  i_{0},i_{1},i_{2},...\right)  $
of integers such that every sufficiently high $k\in\mathbb{N}$ satisfies
$i_{k}+k=m$. It is clear that any $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ automatically satisfies $i_{k}-i_{k+1}=1$ for all
sufficiently high $k$.

For any $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, we introduce
a new symbol $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$. This symbol
is, for the time being, devoid of any meaning. The symbol $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ will be called an \textit{elementary
semiinfinite wedge}.
\end{definition}

\begin{definition}
\textbf{(a)} Let $\wedge^{\dfrac{\infty}{2}}V$ denote the free $\mathbb{C}%
$-vector space with basis $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  _{m\in\mathbb{Z};\ \left(  i_{0},i_{1},i_{2},...\right)
\text{ is an }m\text{-degression}}$. We will refer to $\wedge^{\dfrac{\infty
}{2}}V$ as the \textit{semiinfinite wedge space}.

\textbf{(b)} For every $m\in\mathbb{Z}$, define a $\mathbb{C}$-vector subspace
$\wedge^{\dfrac{\infty}{2},m}V$ of $\wedge^{\dfrac{\infty}{2}}V$ by%
\[
\wedge^{\dfrac{\infty}{2},m}V=\operatorname*{span}\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ \left(  i_{0},i_{1},i_{2}%
,...\right)  \text{ is an }m\text{-degression}\right\}  .
\]
Clearly, $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$.
\end{definition}

Obviously, $\wedge^{\dfrac{\infty}{2}}V=\bigoplus\limits_{m\in\mathbb{Z}%
}\wedge^{\dfrac{\infty}{2},m}V$.

Now, let us introduce the (more flexible) notion of \textit{straying }%
$m$\textit{-degressions}. This notion is obtained from the notion of
$m$-degressions by dropping the ``strictly decreasing'' condition:

\begin{definition}
\label{def.glinf.straym-deg}Let $m\in\mathbb{Z}$. A \textit{straying }%
$m$\textit{-degression} will mean a sequence $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ of integers such that every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=m$.
\end{definition}

As a consequence, a straying $m$-degression is strictly decreasing from some
point onwards, but needs not be strictly decreasing from the beginning (it can
``stray'', whence the name). A strictly decreasing straying $m$-degression is
exactly the same as an $m$-degression. Thus, every $m$-degression is a
straying $m$-degression.

\begin{definition}
Let $S$ be a (possibly infinite) set. Recall that a \textit{permutation} of
$S$ means a bijection from $S$ to $S$.

A \textit{finitary permutation} of $S$ means a bijection from $S$ to $S$ which
fixes all but finitely many elements of $S$. (Thus, all permutations of $S$
are finitary permutations if $S$ is finite.)
\end{definition}

Notice that the finitary permutations of a given set $S$ form a group (under composition).

\begin{definition}
Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a
straying $m$-degression. If no two elements of this sequence $\left(
i_{0},i_{1},i_{2},...\right)  $ are equal, then there exists a unique finitary
permutation $\pi$ of $\mathbb{N}$ such that $\left(  i_{\pi^{-1}\left(
0\right)  },i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)
},...\right)  $ is an $m$-degression. This finitary permutation $\pi$ is
called the \textit{straightening permutation} of $\left(  i_{0},i_{1}%
,i_{2},...\right)  $.
\end{definition}

\begin{definition}
\label{def.semiinfwedge.stray}Let $m\in\mathbb{Z}$. Let $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ be a straying $m$-degression. We define the meaning
of the term $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ as follows:

- If some two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)
$ are equal, then $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ is
defined to mean the element $0$ of $\wedge^{\dfrac{\infty}{2},m}V$.

- If no two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $
are equal, then $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ is
defined to mean the element $\left(  -1\right)  ^{\pi}v_{i_{\pi^{-1}\left(
0\right)  }}\wedge v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi
^{-1}\left(  2\right)  }}\wedge...$ of $\wedge^{\dfrac{\infty}{2},m}V$, where
$\pi$ is the straightening permutation of $\left(  i_{0},i_{1},i_{2}%
,...\right)  $.
\end{definition}

Note that whenever $\left(  i_{0},i_{1},i_{2},...\right)  $ is an
$m$-degression (not just a straying one), then the value of $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ defined according to Definition
\ref{def.semiinfwedge.stray} is exactly the symbol $v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...$ of Definition \ref{def.glinf.m-deg} (because no
two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ are
equal, and the straightening permutation of $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is $\operatorname*{id}$). Hence, Definition
\ref{def.semiinfwedge.stray} does not conflict with Definition
\ref{def.glinf.m-deg}.

\begin{definition}
\label{def.semiinfwedge}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
Then, let us define the wedge product $b_{0}\wedge b_{1}\wedge b_{2}%
\wedge...\in\wedge^{\dfrac{\infty}{2},m}V$ as follows:

Find an integer $M\in\mathbb{N}$ such that every $i\geq M$ satisfies
$b_{i}=v_{m-i}$. (This $M$ exists by the condition that $b_{i}=v_{m-i}$ for
sufficiently large $i$.)

For every $i\in\left\{  0,1,...,M-1\right\}  $, write the vector $b_{i}$ as a
$\mathbb{C}$-linear combination $\sum\limits_{j\in\mathbb{Z}}\lambda
_{i,j}v_{j}$ (with $\lambda_{i,j}\in\mathbb{C}$ for all $j$).

Now, define $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ to be the element%
\[
\sum\limits_{\left(  j_{0},j_{1},...,j_{M-1}\right)  \in\mathbb{Z}^{M}}%
\lambda_{0,j_{0}}\lambda_{1,j_{1}}...\lambda_{M-1,j_{M-1}}v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{M-1}}\wedge v_{m-M}\wedge v_{m-M-1}\wedge
v_{m-M-2}\wedge...
\]
of $\wedge^{\dfrac{\infty}{2},m}V$. Here, $v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{M-1}}\wedge v_{m-M}\wedge v_{m-M-1}\wedge v_{m-M-2}%
\wedge...$ is well-defined, since $\left(  j_{0},j_{1},...,j_{M-1}%
,m-M,m-M-1,m-M-2,...\right)  $ is a straying $m$-degression.

Note that this element $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ is
well-defined (according to Proposition \ref{prop.semiinfwedge.welldef}
\textbf{(a)} below).

We refer to $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ as the \textit{(infinite)
wedge product} of the vectors $b_{0}$, $b_{1}$, $b_{2}$, $...$.
\end{definition}

Note that, for any straying $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $, the value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ defined according to Definition \ref{def.semiinfwedge} equals the
value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ defined
according to Definition \ref{def.semiinfwedge.stray}. Hence, Definition
\ref{def.semiinfwedge} does not conflict with Definition
\ref{def.semiinfwedge.stray}.

We have the following easily verified properties of the infinite wedge product:

\begin{proposition}
\label{prop.semiinfwedge.welldef}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1}%
,b_{2},...$ be vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]


\textbf{(a)} The wedge product $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ as
defined in Definition \ref{def.semiinfwedge} is well-defined (i. e., does not
depend on the choice of $M$).

\textbf{(b)} For any straying $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, the value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...$ defined according to Definition \ref{def.semiinfwedge} equals the
value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ defined
according to Definition \ref{def.semiinfwedge.stray}.

\textbf{(c)} The infinite wedge product is multilinear. That is, we have%
\begin{align}
&  b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  \lambda
b+\lambda^{\prime}b^{\prime}\right)  \wedge b_{k+1}\wedge b_{k+2}%
\wedge...\nonumber\\
&  =\lambda b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge b\wedge
b_{k+1}\wedge b_{k+2}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\lambda^{\prime}b_{0}\wedge b_{1}\wedge...\wedge
b_{k-1}\wedge b^{\prime}\wedge b_{k+1}\wedge b_{k+2}\wedge...
\label{glinf.semiinfwedge.multilinear}%
\end{align}
for all $k\in\mathbb{N}$, $b_{0},b_{1},b_{2},...\in V$, $b,b^{\prime}\in V$
and $\lambda,\lambda^{\prime}\in\mathbb{C}$ which satisfy $\left(
b_{i}=v_{m-i}\text{ for sufficiently large }i\right)  $.

\textbf{(d)} The infinite wedge product is antisymmetric. This means that if
$b_{0},b_{1},b_{2},...\in V$ are such that $\left(  b_{i}=v_{m-i}\text{ for
sufficiently large }i\right)  $ and $\left(  \text{two of the vectors }%
b_{0},b_{1},b_{2},...\text{ are equal}\right)  $, then%
\begin{equation}
b_{0}\wedge b_{1}\wedge b_{2}\wedge...=0. \label{glinf.semiinfwedge.antisym}%
\end{equation}
In other words, when (at least) two of the vectors forming a well-defined
infinite wedge product are equal, then this wedge product is $0$.

\textbf{(e)} As a consequence, the wedge product $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ gets multiplied by $-1$ when we switch $b_{i}$ with $b_{j}$
for any two distinct $i\in\mathbb{N}$ and $j\in\mathbb{N}$.

\textbf{(f)} If $\pi$ is a finitary permutation of $\mathbb{N}$ and
$b_{0},b_{1},b_{2},...\in V$ are vectors such that $\left(  b_{i}%
=v_{m-i}\text{ for sufficiently large }i\right)  $, then the infinite wedge
product $b_{\pi\left(  0\right)  }\wedge b_{\pi\left(  1\right)  }\wedge
b_{\pi\left(  2\right)  }\wedge...$ is well-defined and satisfies
\begin{equation}
b_{\pi\left(  0\right)  }\wedge b_{\pi\left(  1\right)  }\wedge b_{\pi\left(
2\right)  }\wedge...=\left(  -1\right)  ^{\pi}\cdot b_{0}\wedge b_{1}\wedge
b_{2}\wedge.... \label{glinf.semiinfwedge.permut}%
\end{equation}

\end{proposition}

Now, we can define the action of $\mathfrak{gl}_{\infty}$ on $\wedge
^{\dfrac{\infty}{2},m}V$ just as we wanted to:

\begin{definition}
\label{def.glinf.semiinfwedge}Let $m\in\mathbb{Z}$. Define an action of the
Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge
^{\dfrac{\infty}{2},m}V$ by the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ (and by linear extension). (Recall that
$a\rightharpoonup v=av$ for every $a\in\mathfrak{gl}_{\infty}$ and $v\in V$,
due to how we defined the $\mathfrak{gl}_{\infty}$-module $V$.)
\end{definition}

Of course, this definition is only justified after showing that this indeed is
an action. But this is rather easy. Let us state this as a proposition:

\begin{proposition}
\label{prop.glinf.glinfact.welldef}Let $m\in\mathbb{Z}$. Then, Definition
\ref{def.glinf.semiinfwedge} really defines a representation of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$. In other words, there exists one and only one action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\]

\end{proposition}

The proof of this proposition (using the multilinearity and the antisymmetry
of our wedge product) is rather straightforward and devoid of surprises. I
will show it nevertheless, if only because I assume every other text leaves it
to the reader. Due to its length, it is postponed until Subsection
\ref{subsect.degress.proofs}.

Proposition \ref{prop.glinf.glinfact.welldef} shows that the action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ in Definition \ref{def.glinf.semiinfwedge} is well-defined. This
makes $\wedge^{\dfrac{\infty}{2},m}V$ into a $\mathfrak{gl}_{\infty}$-module.
Computations in this module can be somewhat simplified by the following
``comparably basis-free'' formula\footnote{I'm saying ``comparably'' because
the condition that $b_{i}=v_{m-i}$ for all sufficiently large $i$ is not
basis-free. But this should not come as a surprise, as the definition of
$\wedge^{\dfrac{\infty}{2},m}V$ itself is not basis-free to begin with.}:

\begin{proposition}
\label{prop.glinf.glinfact}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$
be vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]
Then, every $a\in\mathfrak{gl}_{\infty}$ satisfies%
\[
a\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)
=\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]

\end{proposition}

We can also explicitly describe this action on elementary matrices and
semiinfinite wedges:

\begin{proposition}
\label{prop.glinf.explicit}Let $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$. Let
$m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a straying
$m$-degression (so that $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\in\wedge^{\dfrac{\infty}{2},m}V$).

\textbf{(a)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
$E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =0$.

\textbf{(b)} If there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that
$j=i_{\ell}$, then for this $\ell$ we have%
\[
E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}%
}\wedge v_{i}\wedge v_{i_{\ell+1}}\wedge v_{i_{\ell+2}}\wedge...
\]
(In words: If $v_{j}$ appears exactly once as a factor in the wedge product
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, then $E_{i,j}%
\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ is the wedge product which is obtained from $v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ by replacing this factor by
$v_{i}$.)
\end{proposition}

Since we have given $\wedge^{\dfrac{\infty}{2},m}V$ a $\mathfrak{gl}_{\infty}%
$-module structure for every $m\in\mathbb{Z}$, it is clear that $\wedge
^{\dfrac{\infty}{2}}V=\bigoplus\limits_{m\in\mathbb{Z}}\wedge^{\dfrac{\infty
}{2},m}V$ also becomes a $\mathfrak{gl}_{\infty}$-module.

\subsubsection{\label{subsect.degress.proofs}Proofs}

Here are proofs of some of the unproven statements made in Subsection
\ref{subsect.degress}:

\textit{Proof of Proposition \ref{prop.glinf.glinfact.welldef}.} The first
thing we need to check is the following:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.0:} Let $a\in
\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be vectors in $V$ which
satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]


\textbf{(a)} For every $k\in\mathbb{N}$, the infinite wedge product
$b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...$ is well-defined.

\textbf{(b)} All but finitely many $k\in\mathbb{N}$ satisfy $b_{0}\wedge
b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)
\wedge b_{k+1}\wedge b_{k+2}\wedge...=0$. (In other words, the sum%
\[
\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...
\]
converges in the discrete topology.)
\end{quote}

\begin{vershort}
The proof of Assertion \ref{prop.glinf.glinfact.welldef}.0 can easily be
supplied by the reader. (Part \textbf{(a)} is clear, since the property of the
sequence $\left(  b_{0},b_{1},b_{2},...\right)  $ to satisfy $\left(
b_{i}=v_{m-i}\text{ for all sufficiently large }i\right)  $ does not change if
we modify one entry of the sequence. Part \textbf{(b)} requires showing that
$b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...=0$ for all sufficiently
large $k$; but this follows from $a\in\mathfrak{gl}_{\infty}$ being a matrix
with only finitely many nonzero entries, and from the condition that
$b_{i}=v_{m-i}$ for all sufficiently large $i$.)
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.0:} We know that
$b_{i}=v_{m-i}$ for all sufficiently large $i$. In other words, there exists
an $I\in\mathbb{N}$ such that%
\begin{equation}
\text{every integer }i\geq I\text{ satisfies }b_{i}=v_{m-i}.
\label{pf.glinf.glinfact.welldef.ass0.pf.2}%
\end{equation}
Fix such an $I$.

\textbf{(a)} Let $k\in\mathbb{N}$. Define a sequence $\left(  c_{0}%
,c_{1},c_{2},...\right)  $ of elements of $V$ by
\[
\left(  c_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, $\left(  c_{0},c_{1},c_{2},...\right)  =\left(  b_{0},b_{1}%
,...,b_{k-1},a\rightharpoonup b_{k},b_{k+1},b_{k+2},...\right)  $. Now, we
have%
\begin{equation}
c_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass0.pf.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass0.pf.0}):} For
every $i\in\mathbb{N}$ satisfying $i\geq\max\left\{  I,k+1\right\}  $, we have%
\begin{align*}
c_{i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq k\text{ (because
}i\geq\max\left\{  I,k+1\right\}  \geq k+1>k\text{)}\right) \\
&  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass0.pf.2}) (since }i\geq\max\left\{
I,k+1\right\}  \geq I\text{)}\right)  .
\end{align*}
Thus, for every sufficiently large $i$, we have $c_{i}=v_{m-i}$. This proves
(\ref{pf.glinf.glinfact.welldef.ass0.pf.0}).}

\textbf{(b)} We know that $\mathfrak{gl}_{\infty}$ is the vector space of all
infinite matrices whose rows and columns are labeled by integers such that
only finitely many entries of the matrix are nonzero. Since $a\in
\mathfrak{gl}_{\infty}$, this shows that only finitely many entries of the
matrix $a$ are nonzero. Hence, only finitely many columns of $a$ are nonzero.
Thus, for every sufficiently low integer $\ell$, the $\ell$-th column of $a$
is $0$. In other words, there exists an $L\in\mathbb{Z}$ such that%
\begin{equation}
\text{every integer }\ell\text{ such that }\ell\leq L\text{ satisfies }\left(
\text{the }\ell\text{-th column of }a\right)  =0.
\label{pf.glinf.glinfact.welldef.ass0.pf.1}%
\end{equation}
Consider such an $L$.

Now, let $k$ be any element of $\mathbb{N}$ satisfying $k\geq\max\left\{
I,m-L\right\}  $. Then, $k\geq\max\left\{  I,m-L\right\}  \geq I$, so that
$b_{k}=v_{m-k}$ (by (\ref{pf.glinf.glinfact.welldef.ass0.pf.2}), applied to
$i=k$). Moreover, $k\geq\max\left\{  I,m-L\right\}  \geq m-L$, so that
$m-k\leq L$. Hence, (\ref{pf.glinf.glinfact.welldef.ass0.pf.1}) (applied to
$\ell=m-k$) yields $\left(  \text{the }\left(  m-k\right)  \text{-th column of
}a\right)  =0$. Now,%
\[
a\rightharpoonup\underbrace{b_{k}}_{=v_{m-k}}=a\rightharpoonup v_{m-k}%
=av_{m-k}=\left(  \text{the }\left(  m-k\right)  \text{-th column of
}a\right)  =0,
\]
and thus%
\[
b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\underbrace{\left(
a\rightharpoonup b_{k}\right)  }_{=0}\wedge b_{k+1}\wedge b_{k+2}%
\wedge...=b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge0\wedge b_{k+1}\wedge
b_{k+2}\wedge...=0
\]
(due to the multilinearity of the infinite wedge product).

Now, forget that we fixed $k$. We thus have shown that every element $k$ of
$\mathbb{N}$ satisfying $k\geq\max\left\{  I,m-L\right\}  $ satisfies
$b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...=0$. Since all but finitely
many $k\in\mathbb{N}$ satisfy $k\geq\max\left\{  I,m-L\right\}  $, this shows
that all but finitely many $k\in\mathbb{N}$ satisfy $b_{0}\wedge b_{1}%
\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge
b_{k+1}\wedge b_{k+2}\wedge...=0$. In other words, the sum%
\[
\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...
\]
converges in the discrete topology. This proves Assertion
\ref{prop.glinf.glinfact.welldef}.0.
\end{verlong}

\begin{vershort}
Now that Assertion \ref{prop.glinf.glinfact.welldef}.0 is proven, we can make
the following definition:

For every $a\in\mathfrak{gl}_{\infty}$, let us define a $\mathbb{C}$-linear
map $F_{a}:\wedge^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}%
{2},m}V$ as follows: For every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, set%
\begin{equation}
F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge... \label{pf.glinf.glinfact.welldef.pfshort.defFa}%
\end{equation}
\footnote{The right hand side of
(\ref{pf.glinf.glinfact.welldef.pfshort.defFa}) is indeed well-defined. This
follows from applying Assertion \ref{prop.glinf.glinfact.welldef}.0
\textbf{(b)} to $v_{i_{i}}$ instead of $b_{i}$.}. Thus, we have specified the
values of the map $F_{a}$ on the basis \newline$\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$ of $\wedge
^{\dfrac{\infty}{2},m}V$. Therefore, the map $F_{a}$ is uniquely determined
(and exists) by linearity.

We are going to prove various properties of this map now. First, we will prove
that the formula (\ref{pf.glinf.glinfact.welldef.pfshort.defFa}) which we used
to define $F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ for $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)
$ can also be applied when $\left(  i_{0},i_{1},i_{2},...\right)  $ is just a
straying $m$-degression:
\end{vershort}

\begin{verlong}
Now that Assertion \ref{prop.glinf.glinfact.welldef}.0 is proven, we can make
the following definition:

For every $a\in\mathfrak{gl}_{\infty}$, let us define a $\mathbb{C}$-linear
map $F_{a}:\wedge^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}%
{2},m}V$ as follows: For every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, set%
\begin{equation}
F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge... \label{pf.glinf.glinfact.welldef.defFa}%
\end{equation}
\footnote{The right hand side of (\ref{pf.glinf.glinfact.welldef.defFa}) is
indeed well-defined. Here is why:
\par
Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an $m$-degression. Due to the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, every sufficiently high $k\in
\mathbb{N}$ satisfies $i_{k}=m-k$. Hence, every sufficiently high
$k\in\mathbb{N}$ satisfies $v_{i_{k}}=v_{m-k}$. If we rename $k$ as $i$ in
this result, we obtain the following: Every sufficiently high $i\in\mathbb{N}$
satisfies $v_{i_{i}}=v_{m-i}$. In other words, $v_{i_{i}}=v_{m-i}$ for all
sufficiently large $i$. Hence, we can apply Assertion
\ref{prop.glinf.glinfact.welldef}.0 \textbf{(b)} to $v_{i_{i}}$ instead of
$b_{i}$. As a result, we conclude that all but finitely many $k\in\mathbb{N}$
satisfy $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...=0$. In other words, the sum $\sum\limits_{k\geq0}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...$ converges in the
discrete topology. In other words, the right hand side of
(\ref{pf.glinf.glinfact.welldef.defFa}) is indeed well-defined, qed.}. Thus,
we have specified the values of the map $F_{a}$ on the basis \newline$\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(
i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ of
$\wedge^{\dfrac{\infty}{2},m}V$. Therefore, of course, the map $F_{a}$ is
uniquely determined (and exists) by linearity.

We will now explore, step by step, the properties of this map. First, we will
prove an assertion which extends the formula
(\ref{pf.glinf.glinfact.welldef.defFa}) to straying $m$-degressions:
\end{verlong}

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.1:} Let $a\in
\mathfrak{gl}_{\infty}$. Then, every \textbf{straying} $m$-degression $\left(
j_{0},j_{1},j_{2},...\right)  $ satisfies%
\begin{equation}
F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge.... \label{pf.glinf.glinfact.welldef.ass1}%
\end{equation}



\end{quote}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.1 (sketched):}
Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be a straying $m$-degression.
Thus, every sufficiently large $i\in\mathbb{N}$ satisfies $j_{i}+i=m$. We must
prove that (\ref{pf.glinf.glinfact.welldef.ass1}) holds.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.1:} Let $\left(
j_{0},j_{1},j_{2},...\right)  $ be a straying $m$-degression. Thus, every
sufficiently large $i\in\mathbb{N}$ satisfies $j_{i}+i=m$. We must prove that
(\ref{pf.glinf.glinfact.welldef.ass1}) holds.
\end{verlong}

Now, we distinguish between two cases:

\textit{Case 1:} Some two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal.

\textit{Case 2:} No two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal.

\begin{vershort}
Let us first consider Case 1. In this case, some two elements of the sequence
$\left(  j_{0},j_{1},j_{2},...\right)  $ are equal. Hence, $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...=0$ (by the definition of $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...$), and thus the left hand side of
(\ref{pf.glinf.glinfact.welldef.ass1}) vanishes. We now need to show that so
does the right hand side.

We know that some two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal. Let $j_{p}$ and $j_{q}$ be two such elements,
with $p\neq q$. So we have $p\neq q$ and $j_{p}=j_{q}$.

The right hand side of (\ref{pf.glinf.glinfact.welldef.ass1}) is a sum over
all $k\geq0$. Each of its addends with $k\notin\left\{  p,q\right\}  $ is $0$
(because it is an infinite wedge product with two equal factors $v_{j_{p}}$
and $v_{j_{q}}$). So we need to check that the addend with $k=p$ and the
addend with $k=q$ cancel each other. In other words, we need to prove that%
\begin{align}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\nonumber\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+1}}%
\wedge.... \label{pf.glinf.glinfact.welldef.ass1.pfshort.0}%
\end{align}
We recall that an infinite wedge product of the form $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ (where $b_{0},b_{1},b_{2},...$ are vectors in $V$ such that
$\left(  b_{i}=v_{m-i}\text{ for all sufficiently large }i\right)  $) gets
multiplied by $-1$ when we switch $b_{i}$ with $b_{j}$ for any two distinct
$i\in\mathbb{N}$ and $j\in\mathbb{N}$\ \ \ \ \footnote{This is a particular
case of Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)} (namely, the
case when $\pi$ is the transposition $\left(  i,j\right)  $).}. Thus, the
infinite wedge product
\[
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...
\]
gets multiplied by $-1$ when we switch $a\rightharpoonup v_{j_{p}}$ with
$v_{j_{q}}$ (since $p\in\mathbb{N}$ and $q\in\mathbb{N}$ are distinct). In
other words,%
\begin{align*}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge
v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+1}}%
\wedge...\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge....
\end{align*}
Thus,%
\begin{align}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...\nonumber\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}%
}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge.... \label{pf.glinf.glinfact.welldef.ass1.pfshort.1}%
\end{align}
Now,%
\begin{align*}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}%
}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass1.pfshort.1})}\right) \\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{p}%
}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{q}=j_{p}\text{ and }j_{p}%
=j_{q}\right) \\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge....
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.0}). The proof of
(\ref{pf.glinf.glinfact.welldef.ass1}) in Case 1 is thus complete.
\end{vershort}

\begin{verlong}
Let us consider Case 1 first. In this case, some two elements of the sequence
$\left(  j_{0},j_{1},j_{2},...\right)  $ are equal. Hence, $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...=0$ (by the definition of $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...$), and thus $F_{a}\left(  v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  =F_{a}\left(  0\right)  =0$ (since
$F_{a}$ is linear).

We know that some two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal. Let $j_{p}$ and $j_{q}$ be two such elements,
with $p\neq q$. So we have $p\neq q$ and $j_{p}=j_{q}$. WLOG assume that $p<q$
(otherwise, just switch $p$ with $q$).

We recall that an infinite wedge product of the form $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ (where $b_{0},b_{1},b_{2},...$ are vectors in $V$ such that
$\left(  b_{i}=v_{m-i}\text{ for all sufficiently large }i\right)  $) gets
multiplied by $-1$ when we switch $b_{i}$ with $b_{j}$ for any two distinct
$i\in\mathbb{N}$ and $j\in\mathbb{N}$\ \ \ \ \footnote{This is a particular
case of Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)} (namely, the
case when $\pi$ is the transposition $\left(  i,j\right)  $).}. Thus, the
infinite wedge product
\[
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...
\]
gets multiplied by $-1$ when we switch $a\rightharpoonup v_{j_{p}}$ with
$v_{j_{q}}$ (since $p\in\mathbb{N}$ and $q\in\mathbb{N}$ are distinct). In
other words,%
\begin{align}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge
v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge...\nonumber\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge.... \label{pf.glinf.glinfact.welldef.ass1.pf.1}%
\end{align}


On the other hand, for every $k\in\mathbb{N}$ satisfying $k\neq p$ and $k\neq
q$, we have
\begin{equation}
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(
a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}%
\wedge...=0 \label{pf.glinf.glinfact.welldef.ass1.pf.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass1.pf.2}):} Let
$k\in\mathbb{N}$ satisfy $k\neq p$ and $k\neq q$. Since $k\neq p$ and $k\neq
q$, both vectors $v_{j_{p}}$ and $v_{j_{q}}$ appear as factors in the wedge
product $v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(
a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}%
\wedge...$. These two vectors $v_{j_{p}}$ and $v_{j_{q}}$ are equal (since
$j_{p}=j_{q}$).
\par
We know that when (at least) two of the vectors forming a well-defined
infinite wedge product are equal, then this wedge product is $0$. Since two of
the vectors forming the well-defined infinite wedge product $v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(  a\rightharpoonup v_{j_{k}%
}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}\wedge...$ are equal (namely,
the vectors $v_{j_{p}}$ and $v_{j_{q}}$), this yields that the wedge product
$v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(
a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}%
\wedge...$ is $0$. This proves (\ref{pf.glinf.glinfact.welldef.ass1.pf.2}).}.

On the other hand, $p$ and $q$ are two distinct elements of $\mathbb{N}$.
Hence,
\begin{align*}
&  \sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k\neq p;\ k\neq q}}\underbrace{v_{j_{0}%
}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(  a\rightharpoonup
v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}\wedge...}%
_{\substack{=0\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass1.pf.2}) (since
}k\neq p\text{ and }k\neq q\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{p-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}%
}\wedge v_{j_{p+2}}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{p-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)
\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge
v_{j_{q}}\wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\=-v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge
v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}%
}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass1.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}%
}\wedge v_{j_{q+2}}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{p}}\wedge v_{j_{p+1}}\wedge
v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{q}%
}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\=v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge
v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}%
}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\\text{(since }%
j_{p}=j_{q}\text{ and }j_{q}=j_{p}\text{)}}}\\
&  =\underbrace{\sum\limits_{\substack{k\geq0;\\k\neq p;\ k\neq q}}0}_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ -v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}%
}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge
v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}%
}\wedge v_{j_{q+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}%
}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge
v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}%
}\wedge v_{j_{q+2}}\wedge...\\
&  =0=F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
\end{align*}
(since we have shown that $F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  =0$). Thus, (\ref{pf.glinf.glinfact.welldef.ass1})
is proven in Case 1.
\end{verlong}

Now, let us consider Case 2. In this case, no two elements of the sequence
$\left(  j_{0},j_{1},j_{2},...\right)  $ are equal. Thus, the straightening
permutation of the straying $m$-degression $\left(  j_{0},j_{1},j_{2}%
,...\right)  $ is well-defined. Let $\pi$ be this straightening permutation.
Then, $\left(  j_{\pi^{-1}\left(  0\right)  },j_{\pi^{-1}\left(  1\right)
},j_{\pi^{-1}\left(  2\right)  },...\right)  $ is an $m$-degression.

Let $\sigma=\pi^{-1}$. Then, $\sigma$ is a finitary permutation of
$\mathbb{N}$, thus a bijective map $\mathbb{N}\rightarrow\mathbb{N}$. From
$\sigma=\pi^{-1}$, we obtain $\sigma\pi=\operatorname*{id}$, thus $\left(
-1\right)  ^{\sigma\pi}=1$.

We know that $\left(  j_{\pi^{-1}\left(  0\right)  },j_{\pi^{-1}\left(
1\right)  },j_{\pi^{-1}\left(  2\right)  },...\right)  $ is an $m$-degression.
Since $\pi^{-1}=\sigma$, this rewrites as follows: The sequence $\left(
j_{\sigma\left(  0\right)  },j_{\sigma\left(  1\right)  },j_{\sigma\left(
2\right)  },...\right)  $ is an $m$-degression.

By the definition of $v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$ (in
Definition \ref{def.semiinfwedge.stray}), we have%
\[
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...=\left(  -1\right)  ^{\pi
}v_{j_{\pi^{-1}\left(  0\right)  }}\wedge v_{j_{\pi^{-1}\left(  1\right)  }%
}\wedge v_{j_{\pi^{-1}\left(  2\right)  }}\wedge...=\left(  -1\right)  ^{\pi
}v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge
v_{j_{\sigma\left(  2\right)  }}\wedge...
\]
(since $\pi^{-1}=\sigma$). Thus,%
\begin{align*}
&  F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =F_{a}\left(  \left(  -1\right)  ^{\pi}v_{j_{\sigma\left(  0\right)  }%
}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(  2\right)
}}\wedge...\right)  =\left(  -1\right)  ^{\pi}\cdot F_{a}\left(
v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge
v_{j_{\sigma\left(  2\right)  }}\wedge...\right)
\end{align*}
(since $F_{a}$ is linear). Multiplying this equality with $\left(  -1\right)
^{\sigma}$, we obtain%
\begin{align}
&  \left(  -1\right)  ^{\sigma}\cdot F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\pi}%
}_{=\left(  -1\right)  ^{\sigma\pi}=1}\cdot F_{a}\left(  v_{j_{\sigma\left(
0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(
2\right)  }}\wedge...\right)  =F_{a}\left(  v_{j_{\sigma\left(  0\right)  }%
}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(  2\right)
}}\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma
\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}%
\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\label{pf.glinf.glinfact.welldef.ass1.pf.4}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }F_{a}\left(  v_{j_{\sigma\left(  0\right)  }%
}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(  2\right)
}}\wedge...\right)  \text{,}\\
\text{since }\left(  j_{\sigma\left(  0\right)  },j_{\sigma\left(  1\right)
},j_{\sigma\left(  2\right)  },...\right)  \text{ is an }m\text{-degression}%
\end{array}
\right)  .\nonumber
\end{align}


\begin{vershort}
On the other hand, for every $k\in\mathbb{N}$, we have%
\begin{align}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\label{pf.glinf.glinfact.welldef.ass1.pfshort.5}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.5}):}
Let $k\in\mathbb{N}$. Define a sequence $\left(  c_{0},c_{1},c_{2},...\right)
$ of elements of $V$ by%
\[
\left(  c_{0},c_{1},c_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}%
},...,v_{j_{\sigma\left(  k\right)  -1}},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k\right)  +1}},v_{j_{\sigma\left(  k\right)
+2}},...\right)  .
\]
Then,%
\[
c_{0}\wedge c_{1}\wedge c_{2}\wedge...=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\]
\par
But according to Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)}
(applied to $\left(  c_{0},c_{1},c_{2},...\right)  $ instead of $\left(
b_{0},b_{1},b_{2},...\right)  $), the infinite wedge product $c_{\sigma\left(
0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge c_{\sigma\left(
2\right)  }\wedge...$ is well-defined and satisfies
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}\cdot
c_{0}\wedge c_{1}\wedge c_{2}\wedge....
\]
\par
But it is easy to see that
\[
\left(  c_{\sigma\left(  0\right)  },c_{\sigma\left(  1\right)  }%
,c_{\sigma\left(  2\right)  },...\right)  =\left(  v_{j_{\sigma\left(
0\right)  }},v_{j_{\sigma\left(  1\right)  }},...,v_{j_{\sigma\left(
k-1\right)  }},a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}%
,v_{j_{\sigma\left(  k+1\right)  }},v_{j_{\sigma\left(  k+2\right)  }%
},...\right)  ,
\]
so that%
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge....
\]
Hence,%
\begin{align*}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\\
&  =c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}%
\cdot\underbrace{c_{0}\wedge c_{1}\wedge c_{2}\wedge...}_{=v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge...}\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.5}).} Hence,
(\ref{pf.glinf.glinfact.welldef.ass1.pf.4}) becomes%
\begin{align*}
&  \left(  -1\right)  ^{\sigma}\cdot F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge...}_{\substack{=\left(  -1\right)  ^{\sigma}\cdot
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}%
}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)
\wedge v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)
+2}}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.5}))}%
}}\\
&  =\sum\limits_{k\geq0}\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\end{align*}
Dividing this equality by $\left(  -1\right)  ^{\sigma}$, we obtain%
\begin{align*}
&  F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k\text{ for }%
\sigma\left(  k\right)  \text{ in the sum (since }\sigma\text{ is
bijective)}\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass1}) is proven in Case 2.
\end{vershort}

\begin{verlong}
On the other hand, for every $k\in\mathbb{N}$, we have%
\begin{align}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\label{pf.glinf.glinfact.welldef.ass1.pf.5}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass1.pf.5}):} Let
$k\in\mathbb{N}$. Define a sequence $\left(  c_{0},c_{1},c_{2},...\right)  $
of elements of $V$ by%
\[
\left(  c_{i}=\left\{
\begin{array}
[c]{l}%
v_{j_{i}},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\sigma\left(  k\right)  ;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=\sigma\left(  k\right)
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, $\left(  c_{0},c_{1},c_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}%
},...,v_{j_{\sigma\left(  k\right)  -1}},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k\right)  +1}},v_{j_{\sigma\left(  k\right)
+2}},...\right)  $.
\par
It is easy to see that $c_{i}=v_{m-i}$ for sufficiently large $i$.
\par
(\textit{Proof:} We know that every sufficiently large $i\in\mathbb{N}$
satisfies $j_{i}+i=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every $i\geq K$ satisfies $j_{i}+i=m$. Consider this $K$.
\par
Let $i\in\mathbb{N}$ be such that $i\geq\max\left\{  K,\sigma\left(  k\right)
+1\right\}  $. Then, $i\geq\max\left\{  K,\sigma\left(  k\right)  +1\right\}
\geq K$. Hence, $j_{i}+i=m$ (since we know that every $i\geq K$ satisfies
$j_{i}+i=m$), so that $j_{i}=m-i$. But also, $i\geq\max\left\{  K,\sigma
\left(  k\right)  +1\right\}  \geq\sigma\left(  k\right)  +1>\sigma\left(
k\right)  $, so that $i\neq\sigma\left(  k\right)  $. Now, by the definition
of $c_{i}$, we have%
\begin{align*}
c_{i}  &  =\left\{
\begin{array}
[c]{l}%
v_{j_{i}},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\sigma\left(  k\right)  ;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=\sigma\left(  k\right)
\end{array}
\right.  =v_{j_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq\sigma\left(
k\right)  \right) \\
&  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{i}=m-i\right)  .
\end{align*}
\par
Now, forget that we fixed $i$. We thus have shown that $c_{i}=v_{m-i}$ for
every $i\in\mathbb{N}$ such that $i\geq\max\left\{  K,\sigma\left(  k\right)
+1\right\}  $. Hence, $c_{i}=v_{m-i}$ for sufficiently large $i$, qed.)
\par
We have $c_{i}=v_{m-i}$ for sufficiently large $i$. Hence, the infinite wedge
product $c_{0}\wedge c_{1}\wedge c_{2}\wedge...$ is well-defined. Since
$\left(  c_{0},c_{1},c_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}%
},...,v_{j_{\sigma\left(  k\right)  -1}},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k\right)  +1}},v_{j_{\sigma\left(  k\right)
+2}},...\right)  $, we have%
\[
c_{0}\wedge c_{1}\wedge c_{2}\wedge...=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\]
\par
But according to Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)}
(applied to $\left(  c_{0},c_{1},c_{2},...\right)  $ instead of $\left(
b_{0},b_{1},b_{2},...\right)  $), the infinite wedge product $c_{\sigma\left(
0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge c_{\sigma\left(
2\right)  }\wedge...$ is well-defined and satisfies
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}\cdot
c_{0}\wedge c_{1}\wedge c_{2}\wedge....
\]
\par
On the other hand, define a sequence $\left(  d_{0},d_{1},d_{2},...\right)  $
of elements of $V$ by%
\begin{equation}
\left(  d_{i}=\left\{
\begin{array}
[c]{l}%
v_{j_{\sigma\left(  i\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\label{pf.glinf.glinfact.welldef.ass1.pf.5.pf.d}%
\end{equation}
Then, $\left(  d_{0},d_{1},d_{2},...\right)  =\left(  v_{j_{\sigma\left(
0\right)  }},v_{j_{\sigma\left(  1\right)  }},...,v_{j_{\sigma\left(
k-1\right)  }},a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}%
,v_{j_{\sigma\left(  k+1\right)  }},v_{j_{\sigma\left(  k+2\right)  }%
},...\right)  $.
\par
But every $i\in\mathbb{N}$ satisfies%
\begin{align*}
c_{\sigma\left(  i\right)  }  &  =\left\{
\begin{array}
[c]{l}%
v_{j_{\sigma\left(  i\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
i\right)  \neq\sigma\left(  k\right)  ;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}\sigma\left(  i\right)  =\sigma\left(  k\right)
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
c_{\sigma\left(  i\right)  }\right) \\
&  =\left\{
\begin{array}
[c]{l}%
v_{j_{\sigma\left(  i\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=k
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sigma\left(  i\right)  \neq\sigma\left(  k\right)  \text{ is
equivalent to }i\neq k\text{ (since }\sigma\text{ is bijective), and since}\\
\sigma\left(  i\right)  =\sigma\left(  k\right)  \text{ is equivalent to
}i=k\text{ (since }\sigma\text{ is bijective)}%
\end{array}
\right) \\
&  =d_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass1.pf.5.pf.d})}\right)  .
\end{align*}
Thus,
\begin{align*}
\left(  c_{\sigma\left(  0\right)  },c_{\sigma\left(  1\right)  }%
,c_{\sigma\left(  2\right)  },...\right)   &  =\left(  d_{0},d_{1}%
,d_{2},...\right) \\
&  =\left(  v_{j_{\sigma\left(  0\right)  }},v_{j_{\sigma\left(  1\right)  }%
},...,v_{j_{\sigma\left(  k-1\right)  }},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k+1\right)  }},v_{j_{\sigma\left(
k+2\right)  }},...\right)  ,
\end{align*}
so that%
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge....
\]
Hence,%
\begin{align*}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\\
&  =c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}%
\cdot\underbrace{c_{0}\wedge c_{1}\wedge c_{2}\wedge...}_{=v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge...}\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass1.pf.5}).} Hence,
(\ref{pf.glinf.glinfact.welldef.ass1.pf.4}) becomes%
\begin{align*}
&  \left(  -1\right)  ^{\sigma}\cdot F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge...}_{\substack{=\left(  -1\right)  ^{\sigma}\cdot
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}%
}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)
\wedge v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)
+2}}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass1.pf.5}))}}}\\
&  =\sum\limits_{k\geq0}\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge...\\
&  =\left(  -1\right)  ^{\sigma}\cdot\sum\limits_{k\geq0}v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\end{align*}
Dividing this equality by $\left(  -1\right)  ^{\sigma}$, we obtain%
\begin{align*}
&  F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k\text{ for }%
\sigma\left(  k\right)  \text{ in the sum (since }\sigma\text{ is
bijective)}\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass1}) is proven in Case 2.
\end{verlong}

\begin{vershort}
We have now proven (\ref{pf.glinf.glinfact.welldef.ass1}) in each of the two
Cases 1 and 2, hence in all situations. In other words, Assertion
\ref{prop.glinf.glinfact.welldef}.1 is proven.
\end{vershort}

\begin{verlong}
Hence, (\ref{pf.glinf.glinfact.welldef.ass1}) is proven in each of the two
Cases 1 and 2. Since these two cases cover all possibilities, this shows that
(\ref{pf.glinf.glinfact.welldef.ass1}) always holds. This completes the proof
of (\ref{pf.glinf.glinfact.welldef.ass1}). In other words, Assertion
\ref{prop.glinf.glinfact.welldef}.1 is proven.
\end{verlong}

Our next goal is the following assertion:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.2:} Let $a\in
\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be vectors in $V$ which
satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]
Then,%
\[
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]



\end{quote}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.2 (sketched):} We
have $b_{i}=v_{m-i}$ for all sufficiently large $i$. In other words, there
exists a $K\in\mathbb{N}$ such that every $i\geq K$ satisfies $b_{i}=v_{m-i}$.
Fix such a $K$.

We have to prove the equality%
\begin{equation}
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass2.pfshort}%
\end{equation}
This equality is clearly linear in \textbf{each} of the variables $b_{0}$,
$b_{1}$, $...$, $b_{K-1}$ (and also in each of the variables $b_{K}$,
$b_{K+1}$, $b_{K+2}$, $...$, but we don't care about them). Hence, in proving
it, we can WLOG assume that each of the vectors $b_{0}$, $b_{1}$, $...$,
$b_{K-1}$ belongs to the basis $\left(  v_{j}\right)  _{j\in\mathbb{Z}}$ of
$V$.\ \ \ \ \footnote{Note that this assumption is allowed because $b_{0}$,
$b_{1}$, $...$, $b_{K-1}$ are \textbf{finitely many} vectors. In contrast, if
we wanted to WLOG assume that each of the (infinitely many) vectors $b_{0}$,
$b_{1}$, $b_{2}$, $...$ belongs to the basis $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ of $V$, then we would have to need more justification for
such an assumption.} Assume this. Of course, the remaining vectors $b_{K}$,
$b_{K+1}$, $b_{K+2}$, $...$ also belong to the basis $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ of $V$ (because every $i\geq K$ satisfies $b_{i}=v_{m-i}$).
Hence, all the vectors $b_{0}$, $b_{1}$, $b_{2}$, $...$ belong to the basis
$\left(  v_{j}\right)  _{j\in\mathbb{Z}}$ of $V$. Hence, there exists a
sequence $\left(  j_{0},j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}$
such that every $i\in\mathbb{N}$ satisfies $b_{i}=v_{j_{i}}$. Therefore, the
equality that we need to prove, (\ref{pf.glinf.glinfact.welldef.ass2.pfshort}%
), will immediately follow from Assertion \ref{prop.glinf.glinfact.welldef}.1
once we can show that $\left(  j_{0},j_{1},j_{2},...\right)  $ is a straying
$m$-degression. But the latter is obvious (since every $i\geq K$ satisfies
$v_{j_{i}}=b_{i}=v_{m-i}$ and thus $j_{i}=m-i$, so that $j_{i}+i=m$). Hence,
(\ref{pf.glinf.glinfact.welldef.ass2.pfshort}) is proven. That is, Assertion
\ref{prop.glinf.glinfact.welldef}.2 is proven.
\end{vershort}

\begin{verlong}
Rather than prove this directly, we will show the following assertion first:

\textit{Assertion \ref{prop.glinf.glinfact.welldef}.3:} Let $K$ be a
nonnegative integer. Let $a\in\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1}%
,b_{2},...$ be vectors in $V$ which satisfy%
\begin{equation}
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass3.stand}%
\end{equation}
Assume also that%
\begin{equation}
b_{i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all integers }i\geq K.
\label{pf.glinf.glinfact.welldef.ass3.hook}%
\end{equation}
Then,%
\begin{equation}
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.state}%
\end{equation}


Notice that Assertion \ref{prop.glinf.glinfact.welldef}.3 differs from
Assertion \ref{prop.glinf.glinfact.welldef}.2 in the presence of an additional
condition (namely, (\ref{pf.glinf.glinfact.welldef.ass3.hook})). This
condition will turn out to be harmless\footnote{In fact, it is easy to see
that for every fixed sequence $\left(  b_{0},b_{1},b_{2},...\right)  $ of
vectors in $V$ which satisfy $\left(  b_{i}=v_{m-i}\text{ for all sufficiently
large }i\right)  $, there exists a $K\in\mathbb{N}$ for which this condition
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) is satisfied. We will explain this
in more detail later.}, but it allows us to induct over the variable $K$.

\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.3:} We will prove
Assertion \ref{prop.glinf.glinfact.welldef}.3 by induction over $K$:

\textit{Induction base:} To complete the induction base, we need to show that
Assertion \ref{prop.glinf.glinfact.welldef}.3 holds for $K=0$. So, assume that
$K=0$. Let $a\in\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $V$ which satisfy (\ref{pf.glinf.glinfact.welldef.ass3.stand}) and
(\ref{pf.glinf.glinfact.welldef.ass3.hook}). We need to prove that
(\ref{pf.glinf.glinfact.welldef.ass3.state}) holds.

We know that (\ref{pf.glinf.glinfact.welldef.ass3.stand}) holds. In other
words, there exists an $N\in\mathbb{N}$ such that every $i\geq N$ satisfies
$b_{i}=v_{m-i}$. Fix such an $N$.

Notice that%
\begin{equation}
b_{i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.ass3.pf.indbase}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase}):}
Let $i\in\mathbb{N}$. Then, $i\geq0=K$. Thus (according to
(\ref{pf.glinf.glinfact.welldef.ass3.hook})), we have $b_{i}\in\left\{
v_{z}\ \mid\ z\in\mathbb{Z}\right\}  $. This proves
(\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase}).}

For every $i\in\mathbb{N}$, let us define an integer $j_{i}$ as follows: If
$i\geq N$, set $j_{i}=m-i$. Otherwise, set $j_{i}$ to be an integer
$z\in\mathbb{Z}$ such that $b_{i}=v_{z}$ (such an integer $z$ exists, because
(\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase}) shows that $b_{i}\in\left\{
v_{z}\ \mid\ z\in\mathbb{Z}\right\}  $). Thus, we have defined an integer
$j_{i}$ for every $i\in\mathbb{N}$.

It is now rather clear that%
\begin{equation}
b_{i}=v_{j_{i}}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}%
):} Let $i\in\mathbb{N}$. We need to then show that $b_{i}=v_{j_{i}}$. We
distinguish between two cases:
\par
\textit{Case 1:} We have $i\geq N$.
\par
\textit{Case 2:} We don't have $i\geq N$.
\par
Let us first consider Case 1. In this case, $i\geq N$. Hence, $j_{i}=m-i$ (by
the definition of $j_{i}$). Thus, $m-i=j_{i}$, so that $v_{m-i}=v_{j_{i}}$.
But on the other hand, $b_{i}=v_{m-i}$ (since we know that every $i\geq N$
satisfies $b_{i}=v_{m-i}$). Thus, $b_{i}=v_{m-i}=v_{j_{i}}$. This proves
$b_{i}=v_{j_{i}}$ in Case 1.
\par
Now let us consider Case 2. In this case, we don't have $i\geq N$. Hence,
$j_{i}$ is an integer $z\in\mathbb{Z}$ such that $b_{i}=v_{z}$ (by the
definition of $j_{i}$). Hence, $b_{i}=v_{j_{i}}$. Thus, $b_{i}=v_{j_{i}}$ is
proven in Case 2.
\par
We have now proven $b_{i}=v_{j_{i}}$ in each of the Cases 1 and 2. Hence,
$b_{i}=v_{j_{i}}$ always holds (since Cases 1 and 2 cover all possibilities).
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}) is proven.}

Also,%
\begin{equation}
\text{every sufficiently high }k\in\mathbb{N}\text{ satisfies }j_{k}+k=m.
\label{pf.glinf.glinfact.welldef.ass3.pf.indbase.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.3}%
):} For every $k\in\mathbb{N}$ such that $k\geq N$, we have $j_{k}=m-k$ (by
the definition of $j_{k}$). In other words, for every $k\in\mathbb{N}$ such
that $k\geq N$, we have $j_{k}+k=m$. Thus, for every sufficiently high
$k\in\mathbb{N}$, we have $j_{k}+k=m$. This proves
(\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.3}).} Hence, $\left(
j_{0},j_{1},j_{2},...\right)  $ is a straying $m$-degression. Thus, Assertion
\ref{prop.glinf.glinfact.welldef}.1 yields%
\[
F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge....
\]


But due to (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}), we have
$\left(  b_{0},b_{1},b_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}},v_{j_{2}%
},...\right)  $. Therefore, $b_{0}\wedge b_{1}\wedge b_{2}\wedge...=v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$. Hence,%
\begin{align*}
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)   &  =F_{a}\left(
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{k-1}}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}%
}\wedge v_{j_{k+2}}\wedge...}_{\substack{=b_{0}\wedge b_{1}\wedge...\wedge
b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...\\\text{(since }\left(  v_{j_{0}},v_{j_{1}},v_{j_{2}%
},...\right)  =\left(  b_{0},b_{1},b_{2},...\right)  \text{)}}}\\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\end{align*}
In other words, (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds. We have
thus shown that Assertion \ref{prop.glinf.glinfact.welldef}.3 holds for $K=0$.
This completes the induction base.

\textit{Induction step:} Let $\kappa\in\mathbb{N}$. Assume that Assertion
\ref{prop.glinf.glinfact.welldef}.3 holds for $K=\kappa$. We now need to prove
that Assertion \ref{prop.glinf.glinfact.welldef}.3 holds for $K=\kappa+1$.

Let $a\in\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be vectors in
$V$ which satisfy (\ref{pf.glinf.glinfact.welldef.ass3.stand}), and satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) for $K=\kappa+1$. We need to prove
that (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds.

We know that the vectors $b_{0},b_{1},b_{2},...$ satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) for $K=\kappa+1$. In other words,%
\begin{equation}
b_{i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all integers }i\geq\kappa+1.
\label{pf.glinf.glinfact.welldef.ass3.indstep.hook}%
\end{equation}


We have $b_{\kappa}\in V$, while $\left(  v_{j}\right)  _{j\in\mathbb{Z}}$ is
a basis of the vector space $V$. Thus, we can write the vector $b_{\kappa}$ in
the form%
\[
b_{\kappa}=\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}%
\]
for some family $\left(  \beta_{\ell}\right)  _{\ell\in\mathbb{Z}}%
\in\mathbb{C}^{\mathbb{Z}}$ such that all but finitely many $\ell\in
\mathbb{Z}$ satisfy $\beta_{\ell}=0$. Consider this family $\left(
\beta_{\ell}\right)  _{\ell\in\mathbb{Z}}$. Since all but finitely many
$\ell\in\mathbb{Z}$ satisfy $\beta_{\ell}=0$, the sum $\sum\limits_{\ell
\in\mathbb{Z}}\beta_{\ell}v_{\ell}$ has only finitely many nonzero addends,
and thus can be treated as a finite sum in regard to algebraic
transformations. In particular, the following computation is allowed:%
\begin{align}
&  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\nonumber\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{\kappa-1}\wedge b_{\kappa}\wedge
b_{\kappa+1}\wedge b_{\kappa+2}\wedge...\nonumber\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{\kappa-1}\wedge\left(  \sum
\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}\right)  \wedge b_{\kappa
+1}\wedge b_{\kappa+2}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}b_{\kappa}=\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}\right)
\nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{0}\wedge b_{1}%
\wedge...\wedge b_{\kappa-1}\wedge v_{\ell}\wedge b_{\kappa+1}\wedge
b_{\kappa+2}\wedge...\label{pf.glinf.glinfact.welldef.ass3.indstep.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{due to the multilinearity of the infinite
wedge product}\right)  .\nonumber
\end{align}


Now, for every $\ell\in\mathbb{Z}$, let us define a sequence $\left(
b_{\ell,0},b_{\ell,1},b_{\ell,2},...\right)  $ of vectors in $V$ by%
\[
\left(  b_{\ell,i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, for every $\ell\in\mathbb{Z}$, we have $\left(  b_{\ell,0},b_{\ell
,1},b_{\ell,2},...\right)  =\left(  b_{0},b_{1},...,b_{\kappa-1},v_{\ell
},b_{\kappa+1},b_{\kappa+2},...\right)  $. Hence, for every $\ell\in
\mathbb{Z}$, we have%
\[
b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}\wedge...=b_{0}\wedge b_{1}%
\wedge...\wedge b_{\kappa-1}\wedge v_{\ell}\wedge b_{\kappa+1}\wedge
b_{\kappa+2}\wedge....
\]
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.1}) becomes%
\begin{align*}
&  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\underbrace{b_{0}\wedge
b_{1}\wedge...\wedge b_{\kappa-1}\wedge v_{\ell}\wedge b_{\kappa+1}\wedge
b_{\kappa+2}\wedge...}_{=b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}%
\wedge...}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge b_{\ell
,1}\wedge b_{\ell,2}\wedge....
\end{align*}


Applying the map $F_{a}$ to this equality, we obtain%
\begin{align}
&  F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \nonumber\\
&  =F_{a}\left(  \sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge
b_{\ell,1}\wedge b_{\ell,2}\wedge...\right)  =\sum\limits_{\ell\in\mathbb{Z}%
}\beta_{\ell}F_{a}\left(  b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}%
\wedge...\right) \label{pf.glinf.glinfact.welldef.ass3.indstep.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the map }F_{a}\text{ is linear, and since the sum }\sum
\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge b_{\ell,1}\wedge
b_{\ell,2}\wedge...\\
\text{has only finitely many nonzero addends}\\
\text{(because all but finitely many }\ell\in\mathbb{Z}\text{ satisfy }%
\beta_{\ell}=0\text{)}%
\end{array}
\right)  .\nonumber
\end{align}


Now, fix an $\ell\in\mathbb{N}$. The sequence $\left(  b_{\ell,0},b_{\ell
,1},b_{\ell,2},...\right)  \in V^{\mathbb{N}}$ satisfies%
\begin{equation}
b_{\ell,i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}):} We know that the
vectors $b_{0},b_{1},b_{2},...$ satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.stand}). In other words, there exists an
$I\in\mathbb{N}$ such that every integer $i>I$ satisfies $b_{i}=v_{m-i}$.
Consider such an $I$.
\par
Let $i\in\mathbb{N}$ be such that $i>\max\left\{  \kappa,I\right\}  $. Then,
$i>\max\left\{  \kappa,I\right\}  \geq I$, so that $b_{i}=v_{m-i}$ (since we
know that every integer $i>I$ satisfies $b_{i}=v_{m-i}$). Also, $i>\max
\left\{  \kappa,I\right\}  \geq\kappa$, and thus $i\neq\kappa$. By the
definition of $b_{\ell,i}$, we have%
\begin{align*}
b_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq\kappa\right) \\
&  =v_{m-i}.
\end{align*}
\par
Now, forget that we fixed $i$. We thus have shown that every $i\in\mathbb{N}$
such that $i>\max\left\{  \kappa,I\right\}  $ satisfies $b_{\ell,i}=v_{m-i}$.
Thus, every sufficiently large $i$ satisfies $b_{\ell,i}=v_{m-i}$. This proves
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}).} Moreover, the
sequence $\left(  b_{\ell,0},b_{\ell,1},b_{\ell,2},...\right)  \in
V^{\mathbb{N}}$ satisfies
\begin{equation}
b_{\ell,i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all integers }i\geq\kappa.
\label{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}):} Let $i$ be an integer
such that $i\geq\kappa$.
\par
If $i=\kappa$, then%
\begin{align*}
b_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{\ell
,i}\right) \\
&  =v_{\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i=\kappa\right) \\
&  \in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\in\mathbb{Z}\right)  .
\end{align*}
Hence, if $i=\kappa$, then
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}) is true. Thus, for the
rest of the proof of (\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}),
we can WLOG assume that $i\neq\kappa$. Assume this.
\par
Since $i\geq\kappa$ and $i\neq\kappa$, we have $i>\kappa$. Since $i$ and
$\kappa$ are integers, this shows that $i\geq\kappa+1$. By the definition of
$b_{\ell,i}$, we have%
\begin{align*}
b_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq\kappa\right) \\
&  \in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hook}), since }i\geq
\kappa+1\right)  .
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}).}

But we have assumed (as the induction hypothesis) that Assertion
\ref{prop.glinf.glinfact.welldef}.3 holds for $K=\kappa$. Hence, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.3 to $\left(  b_{\ell,0}%
,b_{\ell,1},b_{\ell,2},...\right)  $ and $\kappa$ instead of $\left(
b_{0},b_{1},b_{2},...\right)  $ and $K$ (because we know that the sequence
$\left(  b_{\ell,0},b_{\ell,1},b_{\ell,2},...\right)  $ satisfies
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}) and
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1})). As a result, we
conclude that%
\begin{align}
&  F_{a}\left(  b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}\wedge...\right)
\nonumber\\
&  =\sum\limits_{k\geq0}b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge
b_{\ell,k-1}\wedge\left(  a\rightharpoonup b_{\ell,k}\right)  \wedge
b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.indstep.7}%
\end{align}


Now, forget that we have fixed $\ell$. We have thus proven
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.7}) for every $\ell\in\mathbb{N}%
$. Now, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.2}) becomes%
\begin{align}
&  F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\underbrace{F_{a}\left(
b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}\wedge...\right)  }%
_{\substack{=\sum\limits_{k\geq0}b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge
b_{\ell,k-1}\wedge\left(  a\rightharpoonup b_{\ell,k}\right)  \wedge
b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.7}))}}}\nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\sum\limits_{k\geq0}b_{\ell
,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup
b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...\nonumber\\
&  =\sum\limits_{k\geq0}\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell
,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup
b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.indstep.8}%
\end{align}
It remains to prove that the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.7}) equals the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass3.state}).

So our next goal is to show that every $k\in\mathbb{N}$ satisfies%
\begin{align}
&  \sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge b_{\ell
,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup b_{\ell
,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...\nonumber\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.indstep.13}%
\end{align}


\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}):} Fix
$k\in\mathbb{N}$. We need to prove that
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}) holds.

Let us define a sequence $\left(  \widetilde{b}_{0},\widetilde{b}%
_{1},\widetilde{b}_{2},...\right)  $ of vectors in $V$ by%
\[
\left(  \widetilde{b}_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, $\left(  \widetilde{b}_{0},\widetilde{b}_{1},\widetilde{b}%
_{2},...\right)  =\left(  b_{0},b_{1},...,b_{k-1},a\rightharpoonup
b_{k},b_{k+1},b_{k+2},...\right)  $.

For every $\ell\in\mathbb{Z}$, let us define a sequence $\left(
\widetilde{b}_{\ell,0},\widetilde{b}_{\ell,1},\widetilde{b}_{\ell
,2},...\right)  $ of vectors in $V$ by%
\[
\left(  \widetilde{b}_{\ell,i}=\left\{
\begin{array}
[c]{c}%
b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, for every $\ell\in\mathbb{Z}$, we have $\left(  \widetilde{b}_{\ell
,0},\widetilde{b}_{\ell,1},\widetilde{b}_{\ell,2},...\right)  =\left(
b_{\ell,0},b_{\ell,1},...,b_{\ell,k-1},a\rightharpoonup b_{\ell,k}%
,b_{\ell,k+1},b_{\ell,k+2},...\right)  $. Hence, for every $\ell\in\mathbb{Z}%
$, we have%
\begin{align}
&  \widetilde{b}_{\ell,0}\wedge\widetilde{b}_{\ell,1}\wedge\widetilde{b}%
_{\ell,2}\wedge...\nonumber\\
&  =b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(
a\rightharpoonup b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell
,k+2}\wedge.... \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.0}%
\end{align}


Furthermore, for every $\ell\in\mathbb{Z}$, define an element $w_{\ell}$ of
$V$ by $w_{\ell}=\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  $. Since the sum $\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell
}$ has only finitely many nonzero addends, we have%
\[
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\left(  a\rightharpoonup v_{\ell
}\right)  =a\rightharpoonup\underbrace{\left(  \sum\limits_{\ell\in\mathbb{Z}%
}\beta_{\ell}v_{\ell}\right)  }_{=b_{\kappa}}=a\rightharpoonup b_{\kappa}.
\]
Now, the sum $\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}w_{\ell}$ has only
finitely many nonzero addends (since all but finitely many $\ell\in\mathbb{Z}$
satisfy $\beta_{\ell}=0$), and equals%
\begin{align}
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}w_{\ell}  &  =\sum\limits_{\ell
\in\mathbb{Z}}\beta_{\ell}\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }w_{\ell}=\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \text{ for every }\ell\in\mathbb{Z}\right) \nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}%
,\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\left(  a\rightharpoonup v_{\ell
}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}=b_{\kappa
}\text{ if }\kappa\neq k\text{,}\\
\text{and since }\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\left(
a\rightharpoonup v_{\ell}\right)  =a\rightharpoonup b_{\kappa}\text{ if
}\kappa=k
\end{array}
\right) \nonumber\\
&  =\widetilde{b}_{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the
definition of }\widetilde{b}_{\kappa}\text{ yields }\widetilde{b}_{\kappa
}=\left\{
\begin{array}
[c]{c}%
b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \right)  . \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.1}%
\end{align}


Now, fix $\ell\in\mathbb{Z}$. It is easy to see that
\begin{equation}
\widetilde{b}_{\ell,i}=\left\{
\begin{array}
[c]{c}%
\widetilde{b}_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
w_{\ell}\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}):} Let $i\in\mathbb{N}$.
We distinguish between two cases:
\par
\textit{Case 1:} We have $i\neq k$.
\par
\textit{Case 2:} We have $i=k$.
\par
Let us consider Case 1 first. In this case, $i\neq k$. By the definition of
$\widetilde{b}_{\ell,i}$, we have%
\begin{align}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =b_{\ell,i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq k\right)
\nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{\ell
,i}\right)  . \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.1}%
\end{align}
But the definition of $\widetilde{b}_{i}$ yields $\widetilde{b}_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =b_{i}$ (since $i\neq k$). Thus, $b_{i}=\widetilde{b}_{i}$. Also, if
$i=\kappa$, then $\kappa\neq k$ (since $i\neq k$). Hence, if $i=\kappa$, then%
\begin{align*}
w_{\ell}  &  =\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }w_{\ell
}\right) \\
&  =v_{\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\kappa\neq k\right)  .
\end{align*}
Hence, if $i=\kappa$, then $v_{\ell}=w_{\ell}$. Now,
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.1}) becomes%
\begin{align*}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
\widetilde{b}_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
w_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because we have }b_{i}=\widetilde{b}_{i}\text{ if }i\neq\kappa\text{
(this was proven above),}\\
\text{and because we have }v_{\ell}=w_{\ell}\text{ if }i=\kappa\text{ (this
was proven above)}%
\end{array}
\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) is proven in Case
1.
\par
Let us now consider Case 2. In this case, $i=k$. By the definition
$\widetilde{b}_{\ell,i}$, we have%
\begin{align}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =a\rightharpoonup b_{\ell,i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i=k\right) \nonumber\\
&  =a\rightharpoonup\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }b_{\ell,i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \text{ (by the definition of }b_{\ell,i}\text{)}\right) \nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  . \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.2}%
\end{align}
But the definition of $\widetilde{b}_{i}$ yields $\widetilde{b}_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =a\rightharpoonup b_{i}$ (since $i=k$). Thus, $a\rightharpoonup
b_{i}=\widetilde{b}_{i}$. Also, if $i=\kappa$, then $\kappa=k$ (since $i=k$).
Hence, if $i=\kappa$, then%
\begin{align*}
w_{\ell}  &  =\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }w_{\ell
}\right) \\
&  =a\rightharpoonup v_{\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
\kappa=k\right)  .
\end{align*}
Hence, if $i=\kappa$, then $a\rightharpoonup v_{\ell}=w_{\ell}$. Now,
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.2}) becomes%
\begin{align*}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
\widetilde{b}_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
w_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because we have }a\rightharpoonup b_{i}=\widetilde{b}_{i}\text{ if
}i\neq\kappa\text{ (this was proven above),}\\
\text{and because we have }a\rightharpoonup v_{\ell}=w_{\ell}\text{ if
}i=\kappa\text{ (this was proven above)}%
\end{array}
\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) is proven in Case
2.
\par
We have thus proven (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) in
each of the Cases 1 and 2. Since these two Cases cover all possibilities, this
yields that (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) always
holds, qed.} In other words, $\left(  \widetilde{b}_{\ell,0},\widetilde{b}%
_{\ell,1},\widetilde{b}_{\ell,2},...\right)  =\left(  \widetilde{b}%
_{0},\widetilde{b}_{1},...,\widetilde{b}_{\kappa-1},w_{\ell},\widetilde{b}%
_{\kappa+1},\widetilde{b}_{\kappa+2},...\right)  $. Thus,%
\[
\widetilde{b}_{\ell,0}\wedge\widetilde{b}_{\ell,1}\wedge\widetilde{b}_{\ell
,2}\wedge...=\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge
\widetilde{b}_{\kappa-1}\wedge w_{\ell}\wedge\widetilde{b}_{\kappa+1}%
\wedge\widetilde{b}_{\kappa+2}\wedge....
\]
Compared with (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.0}), this
yields%
\begin{align}
&  b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(
a\rightharpoonup b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell
,k+2}\wedge...\nonumber\\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}%
_{\kappa-1}\wedge w_{\ell}\wedge\widetilde{b}_{\kappa+1}\wedge\widetilde{b}%
_{\kappa+2}\wedge.... \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.4}%
\end{align}


Now, forget that we fixed $\ell$. We thus have proven that
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.4}) holds for every
$\ell\in\mathbb{Z}$.

Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\underbrace{b_{\ell,0}\wedge
b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup
b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...}%
_{\substack{=\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge
\widetilde{b}_{\kappa-1}\wedge w_{\ell}\wedge\widetilde{b}_{\kappa+1}%
\wedge\widetilde{b}_{\kappa+2}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.4}))}}}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\widetilde{b}_{0}%
\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}_{\kappa-1}\wedge w_{\ell
}\wedge\widetilde{b}_{\kappa+1}\wedge\widetilde{b}_{\kappa+2}\wedge...\\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}%
_{\kappa-1}\wedge\underbrace{\left(  \sum\limits_{\ell\in\mathbb{Z}}%
\beta_{\ell}w_{\ell}\right)  }_{\substack{=\widetilde{b}_{\kappa}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.1}))}}}\wedge
\widetilde{b}_{\kappa+1}\wedge\widetilde{b}_{\kappa+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the infinite wedge product is multilinear, and since the sum}\\
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}w_{\ell}\text{ has only finitely
many nonzero addends}%
\end{array}
\right) \\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}%
_{\kappa-1}\wedge\widetilde{b}_{\kappa}\wedge\widetilde{b}_{\kappa+1}%
\wedge\widetilde{b}_{\kappa+2}\wedge...\\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge\widetilde{b}_{2}\wedge...\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  \widetilde{b}%
_{0},\widetilde{b}_{1},\widetilde{b}_{2},...\right)  =\left(  b_{0}%
,b_{1},...,b_{k-1},a\rightharpoonup b_{k},b_{k+1},b_{k+2},...\right)  \right)
.
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}).

Now, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.8}) becomes%
\begin{align*}
&  F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{\sum\limits_{\ell\in\mathbb{Z}}\beta
_{\ell}b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(
a\rightharpoonup b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell
,k+2}\wedge...}_{\substack{=b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}%
\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}%
))}}}\\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\end{align*}
In other words, (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds.

Now, forget that we fixed $a$ and $b_{0},b_{1},b_{2},...$. We thus have shown
the following result:
\[
\left(
\begin{array}
[c]{c}%
\text{If }a\text{ is an element of }\mathfrak{gl}_{\infty}\text{, and }%
b_{0},b_{1},b_{2},...\text{ are vectors in }V\text{ which satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.stand}),}\\
\text{and satisfy (\ref{pf.glinf.glinfact.welldef.ass3.hook}) for }%
K=\kappa+1\text{, then (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds}%
\end{array}
\right)  .
\]
In other words, we have shown that Assertion \ref{prop.glinf.glinfact.welldef}%
.3 holds for $K=\kappa+1$. This completes the inductive proof of Assertion
\ref{prop.glinf.glinfact.welldef}.3.

\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.2:} By the
assumptions, we know that $b_{i}=v_{m-i}$ for all sufficiently large $i$. In
other words, there exists a $K\in\mathbb{N}$ such that every $i\geq K$
satisfies $b_{i}=v_{m-i}$. Fix such a $K$. Then, every integer $i\geq K$
satisfies $b_{i}=v_{m-i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}  $
(since $m-i\in\mathbb{Z}$). In other words,
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) is satisfied. Thus, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.3, and conclude that
\[
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]
This proves Assertion \ref{prop.glinf.glinfact.welldef}.2.
\end{verlong}

Next, here's something obvious that we are going to use a few times in the proof:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.4:} Let $f$ and $g$ be two
endomorphisms of the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}%
V$. If every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies
$f\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =g\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $, then $f=g$.
\end{quote}

\begin{vershort}
This follows from the fact that $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is
an }m\text{-degression}}$ is a basis of the $\mathbb{C}$-vector space
$\wedge^{\dfrac{\infty}{2},m}V$.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.4:} Assume that
every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies
$f\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =g\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. In other words,
the two maps $f$ and $g$ are equal to each other on every element of the
family $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
_{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$.

Let us recall the following fact from linear algebra: If $\mathfrak{A}$ and
$\mathfrak{B}$ are two $\mathbb{C}$-vector spaces, and $S$ is a basis of
$\mathfrak{A}$, and $\mathfrak{f}:\mathfrak{A}\rightarrow\mathfrak{B}$ and
$\mathfrak{g}:\mathfrak{A}\rightarrow\mathfrak{B}$ are two $\mathbb{C}$-linear
maps such that $f$ and $g$ are equal to each other on every element of $S$,
then $\mathfrak{f}=\mathfrak{g}$. Applying this fact to $\mathfrak{A}%
=\wedge^{\dfrac{\infty}{2},m}V$, $\mathfrak{B}=\wedge^{\dfrac{\infty}{2},m}V$,
$S=\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
_{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$,
$\mathfrak{f}=f$ and $\mathfrak{g}=g$, we conclude that $f=g$. This proves
Assertion \ref{prop.glinf.glinfact.welldef}.4.
\end{verlong}

Next, we notice the following easy fact:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.5:} Let $a\in
\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$. Let $\lambda
\in\mathbb{C}$ and $\mu\in\mathbb{C}$. Then, $\lambda F_{a}+\mu F_{b}%
=F_{\lambda a+\mu b}$ in the Lie algebra $\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  $.
\end{quote}

\begin{vershort}
This follows very quickly from the linearity of the definition of $F_{a}$ with
respect to $a$ (the details are left to the reader).
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.5:} Both maps
$F_{a}$ and $F_{b}$ are $\mathbb{C}$-linear (by their definitions). Hence, the
map $\lambda F_{a}+\mu F_{b}$ is $\mathbb{C}$-linear. On the other hand, the
map $F_{\lambda a+\mu b}$ is $\mathbb{C}$-linear (by its definition). Now,
every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align}
&  \left(  \lambda F_{a}+\mu F_{b}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\lambda\underbrace{F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  }_{\substack{=\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\\text{(by the definition of
}F_{a}\text{)}}}+\mu\underbrace{F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  }_{\substack{=\sum\limits_{k\geq0}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\\text{(by the
definition of }F_{b}\text{)}}}\nonumber\\
&  =\lambda\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\mu\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\label{pf.glinf.glinfact.welldef.ass4.1}%
\end{align}
and%
\begin{align}
&  F_{\lambda a+\mu b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \underbrace{\left(  \lambda a+\mu b\right)  \rightharpoonup
v_{i_{k}}}_{=\lambda a\rightharpoonup v_{i_{k}}+\mu b\rightharpoonup v_{i_{k}%
}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }F_{\lambda a+\mu
b}\right) \nonumber\\
&  =\sum\limits_{k\geq0}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \lambda a\rightharpoonup v_{i_{k}}+\mu
b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...}_{\substack{=\lambda v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...+\mu v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\\\text{(by Proposition
\ref{prop.semiinfwedge.welldef} \textbf{(c)})}}}\nonumber\\
&  =\sum\limits_{k\geq0}\left(  \lambda v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  +\mu v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right) \nonumber\\
&  =\lambda\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\mu\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glinf.glinfact.welldef.ass4.2}%
\end{align}
Hence, every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
&  \left(  \lambda F_{a}+\mu F_{b}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =\lambda\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\mu\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass4.1})}\right) \\
&  =F_{\lambda a+\mu b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass4.2})}\right)  .
\end{align*}
Hence, Assertion \ref{prop.glinf.glinfact.welldef}.4 (applied to $f=\lambda
F_{a}+\mu F_{b}$ and $g=F_{\lambda a+\mu b}$) yields that $\lambda F_{a}+\mu
F_{b}=F_{\lambda a+\mu b}$. This proves Assertion
\ref{prop.glinf.glinfact.welldef}.5.
\end{verlong}

Here is something rather simple:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.6:} Let $i\in\mathbb{Z}$
and $j\in\mathbb{Z}$. Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ be a straying $m$-degression.

\textbf{(a)} For every $\ell\in\mathbb{N}$, the sequence $\left(  i_{0}%
,i_{1},...,i_{\ell-1},i,i_{\ell+1},i_{\ell+2},...\right)  $ is a straying $m$-degression.

\textbf{(b)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
$F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=0$.

\textbf{(c)} If there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that
$j=i_{\ell}$, then we have%
\[
F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{i}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...,
\]
where $L$ is the unique $\ell\in\mathbb{N}$ such that $j=i_{\ell}$.


\end{quote}

The proof of Assertion \ref{prop.glinf.glinfact.welldef}.6 is as
straightforward as one would expect: it is a matter of substituting
$a=E_{i,j}$ and $b_{k}=v_{i_{k}}$ into Assertion
\ref{prop.glinf.glinfact.welldef}.2 and taking care of the few addends which
are not $0$.

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.6:} By the
definition of $E_{i,j}$, we have%
\begin{equation}
E_{i,j}v_{u}=\delta_{j,u}v_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
u\in\mathbb{Z}. \label{pf.glinf.glinfact.welldef.ass6.pf.Eij}%
\end{equation}


We have%
\begin{equation}
v_{i_{i}}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass6.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass6.pf.1}):} We
know that $\left(  i_{0},i_{1},i_{2},...\right)  $ is a straying
$m$-degression. By the definition of a straying $m$-degression, this rewrites
as follows: Every sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}+k=m$. In
other words, every sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}=m-k$.
Hence, every sufficiently high $k\in\mathbb{N}$ satisfies $v_{i_{k}}=v_{m-k}$.
Renaming the variable $k$ as $i$ in this result, we conclude: Every
sufficiently high $i\in\mathbb{N}$ satisfies $v_{i_{i}}=v_{m-i}$. This proves
(\ref{pf.glinf.glinfact.welldef.ass6.pf.1}).}. Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.2 (applied to $a=E_{i,j}$ and $b_{k}%
=v_{i_{k}}$) yields%
\begin{align}
&  F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  E_{i,j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=E_{i,j}v_{i_{k}}=\delta_{j,i_{k}}v_{i}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass6.pf.Eij}), applied to }u=i_{k}\text{)}%
}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{k\geq0}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \delta_{j,i_{k}}v_{i}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...}_{\substack{=\delta_{j,i_{k}}\cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\\\text{(since the infinite wedge product is
multilinear)}}}\nonumber\\
&  =\sum\limits_{k\geq0}\delta_{j,i_{k}}\cdot v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge.... \label{pf.glinf.glinfact.welldef.ass6.pf.2}%
\end{align}


\textbf{(a)} Let $\ell\in\mathbb{N}$. We know that $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $m$-degression. By the definition of a
straying $m$-degression, this rewrites as follows: Every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=m$. In other words, there exists a
$K\in\mathbb{N}$ such that every integer $k\geq K$ satisfies $i_{k}+k=m$.
Consider this $k$.

Define a sequence $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime
},...\right)  $ of integers by%
\[
\left(  i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq\ell;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }k=\ell
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
=\left(  i_{0},i_{1},...,i_{\ell-1},i,i_{\ell+1},i_{\ell+2},...\right)  $.

Now, let $k$ be any integer such that $k\geq\max\left\{  \ell+1,K\right\}  $.
Then, $k\geq\max\left\{  \ell+1,K\right\}  \geq K$, so that $i_{k}+k=m$. And
since $k\geq\max\left\{  \ell+1,K\right\}  \geq\ell+1$, we have $k\in
\mathbb{N}$ (since $\ell\in\mathbb{N}$). Moreover, since $k\geq\ell+1>\ell$,
we have $k\neq\ell$. Now, by the definition of $i_{k}^{\prime}$, we have%
\[
i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq\ell;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }k=\ell
\end{array}
\right.  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq\ell\right)  ,
\]
so that $i_{k}^{\prime}+k=i_{k}+k=m$. Now, forget that we fixed $k$. We thus
have proven that every integer $k$ satisfying $k\geq\max\left\{
\ell+1,K\right\}  $ satisfies $i_{k}^{\prime}+k=m$. In other words, the
sequence $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)  $
is a straying $m$-degression. Since $\left(  i_{0}^{\prime},i_{1}^{\prime
},i_{2}^{\prime},...\right)  =\left(  i_{0},i_{1},...,i_{\ell-1},i,i_{\ell
+1},i_{\ell+2},...\right)  $, this rewrites as follows: The sequence $\left(
i_{0},i_{1},...,i_{\ell-1},i,i_{\ell+1},i_{\ell+2},...\right)  $ is a straying
$m$-degression. This proves Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(a)}.

\textbf{(b)} Assume that $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $.
Then,
\begin{equation}
\text{every }k\in\mathbb{N}\text{ satisfies }\delta_{j,i_{k}}=0
\label{pf.glinf.glinfact.welldef.ass6.pf.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass6.pf.3}):} Let
$k\in\mathbb{N}$. Then, $j\neq i_{k}$ (because otherwise, we would have
$j=i_{k}\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, which contradicts
$j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $). Thus, $\delta_{j,i_{k}}%
=0$. This proves (\ref{pf.glinf.glinfact.welldef.ass6.pf.3}).}. Now,
(\ref{pf.glinf.glinfact.welldef.ass6.pf.2}) becomes%
\begin{align*}
&  F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{\delta_{j,i_{k}}}_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass6.pf.3}))}}}\cdot v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\
&  =\sum\limits_{k\geq0}0\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0.
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)}.

\textbf{(c)} Assume that there exists a \textbf{unique} $\ell\in\mathbb{N}$
such that $j=i_{\ell}$. Denote this $\ell$ by $L$.

Recall that $L$ is an $\ell\in\mathbb{N}$ such that $j=i_{\ell}$. Hence,
$j=i_{L}$.

Recall that $L$ is a \textbf{unique} $\ell\in\mathbb{N}$ such that $j=i_{\ell
}$. From the uniqueness in this statement, we conclude that there exists no
$k\in\mathbb{N}$ satisfying $j=i_{k}$ and $k\neq L$. In other words, no
$k\in\mathbb{N}$ satisfying $k\neq L$ can satisfy $j=i_{k}$. In other words,
every $k\in\mathbb{N}$ satisfying $k\neq L$ satisfies $j\neq i_{k}$. Hence,
\begin{equation}
\text{every }k\in\mathbb{N}\text{ satisfying }k\neq L\text{ satisfies }%
\delta_{j,i_{k}}=0. \label{pf.glinf.glinfact.welldef.ass6.pf.4}%
\end{equation}


Now, (\ref{pf.glinf.glinfact.welldef.ass6.pf.2}) becomes%
\begin{align*}
&  F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right) \\
&  =\sum\limits_{k\geq0}\delta_{j,i_{k}}\cdot v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\
&  =\underbrace{\delta_{j,i_{L}}}_{\substack{=1\\\text{(since }j=i_{L}%
\text{)}}}\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge
v_{i}\wedge v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k\neq L}%
}\underbrace{\delta_{j,i_{k}}}_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass6.pf.4}))}}}\cdot v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\
&  =\underbrace{1\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}%
}\wedge v_{i}\wedge v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...}_{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{i}\wedge v_{i_{L+1}}\wedge
v_{i_{L+2}}\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{k\geq0;\\k\neq
L}}0\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{=0}\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{i}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge....
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)}.
\end{verlong}

Now here is something less obvious:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.7:} Every $a\in
\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ satisfy $\left[
F_{a},F_{b}\right]  =F_{\left[  a,b\right]  }$ in the Lie algebra
$\mathfrak{gl}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $.
\end{quote}

There are two possible approaches to proving Assertion
\ref{prop.glinf.glinfact.welldef}.7.

\begin{landscape}
\textit{First proof of Assertion \ref{prop.glinf.glinfact.welldef}.7
(sketched):} In order to prove Assertion \ref{prop.glinf.glinfact.welldef}.7,
it is enough to show that%
\begin{equation}
\left[  F_{a},F_{b}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =F_{\left[  a,b\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)
\label{pf.glinf.glinfact.welldef.ass7.pf.short.2.goal}%
\end{equation}
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. (Indeed,
once this is done, $\left[  F_{a},F_{b}\right]  =F_{\left[  a,b\right]  }$
will follow from Assertion \ref{prop.glinf.glinfact.welldef}.4.) So let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be any $m$-degression. Then,%
\begin{align*}
&  F_{a}\left(  F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  \\
&  =F_{a}\left(  \sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\right)  \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)\text{ is defined as } \sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge... \right)  \\
&  =\sum\limits_{k\geq0}F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\right)  \\
&  =\sum\limits_{q\geq0}\underbrace{F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{q-1}}\wedge\left(  b\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge...\right)  }_{\substack{=\sum
\limits_{k\geq0}\left\{
\begin{array}
[c]{l}%
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}},\ \ \ \ \ \ \ \ \ \ \text{if }k<q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(  \left(
ab\right)  \rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }k=q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}},\ \ \ \ \ \ \ \ \ \ \text{if }k>q
\end{array}
\right.  \\\text{(by an application of Assertion
\ref{prop.glinf.glinfact.welldef}.2)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }q\right)  \\
&  =\sum\limits_{q\geq0}\sum\limits_{k\geq0}\left\{
\begin{array}
[c]{l}%
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if
}k<q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(  \left(
ab\right)  \rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }k=q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if
}k>q
\end{array}
\right.
\end{align*}%
\begin{align}
&  =\sum\limits_{q\geq0}\sum\limits_{\substack{k\geq0;\\k<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{q\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  \left(  ab\right)  \rightharpoonup
v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{q\geq0}\sum\limits_{\substack{k\geq
0;\\k>q}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(  b\rightharpoonup v_{i_{p}%
}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  ab\right)  \rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{p\geq0}\sum\limits_{\substack{q\geq
0;\\q>p}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(
a\rightharpoonup v_{i_{p}}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  b\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge
...\label{pf.glinf.glinfact.welldef.ass7.pf.short.1}%
\end{align}
\footnote{In the last step of this computation, we did the following
substitutions:
\par
-- We renamed the index $k$ as $p$ in the second sum.
\par
-- We renamed the index $q$ as $k$ in the third sum.
\par
-- We switched the meanings of the indices $p$ and $q$ in the fourth and fifth
sums.}. Similarly,%
\begin{align}
& F_{b}\left(  F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  \nonumber\\
& =\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(  a\rightharpoonup v_{i_{p}%
}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  b\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...\nonumber\\
& \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  ba\right)  \rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
& \ \ \ \ \ \ \ \ \ \ +\sum\limits_{p\geq0}\sum\limits_{\substack{q\geq
0;\\q>p}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(
b\rightharpoonup v_{i_{p}}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge
....\label{pf.glinf.glinfact.welldef.ass7.pf.short.2}%
\end{align}
\end{landscape}


Now, let us subtract (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) from
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). I am claiming that the
first term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) cancels against the third
term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). Indeed, in order to see
this, one needs to check that one can interchange the order of summation in
the sum%
\[
\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(  b\rightharpoonup v_{i_{p}%
}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...,
\]
i. e., replace $\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}$ by
$\sum\limits_{p\geq0}\sum\limits_{\substack{q\geq0;\\q>p}}$. This is easy to
see (indeed, one must show that \newline$v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{p-1}}\wedge\left(  b\rightharpoonup v_{i_{p}}\right)
\wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...=0$ for all but finitely many \textbf{pairs} $\left(  i,j\right)
\in\mathbb{N}^{2}$), but not trivial a priori\footnote{Here is a cautionary
tale on why one cannot always interchange summation in infinite sums. Define a
family $\left(  \alpha_{p,q}\right)  _{\left(  p,q\right)  \in\mathbb{N}^{2}}$
of integers by $\alpha_{p,q}=\left\{
\begin{array}
[c]{c}%
1,\text{ if }p=q;\\
-1,\text{ if }p=q+1
\end{array}
\right.  $. Then, every $q\in\mathbb{N}$ satisfies $\sum\limits_{p\geq0}%
\alpha_{p,q}=0$. Hence, $\sum\limits_{q\geq0}\sum\limits_{p\geq0}\alpha
_{p,q}=0$. On the other hand, every $p\in\mathbb{N}$ satisfies $\sum
\limits_{q\geq0}\alpha_{p,q}=\delta_{p,0}$. Hence, $\sum\limits_{p\geq0}%
\sum\limits_{q\geq0}\alpha_{p,q}=1\neq0=\sum\limits_{q\geq0}\sum
\limits_{p\geq0}\alpha_{p,q}$. So the two summation signs in this situation
cannot be interchanged, even though all sums (both inner and outer) converge
in the discrete topology. Generally, for a family $\left(  \lambda
_{p,q}\right)  _{\left(  p,q\right)  \in\mathbb{N}^{2}}$ of elements of an
additive group, we are guaranteed to have $\sum\limits_{p\geq0}\sum
\limits_{q\geq0}\lambda_{p,q}=\sum\limits_{q\geq0}\sum\limits_{p\geq0}%
\lambda_{p,q}$ if the \textbf{double sum }$\sum\limits_{\left(  p,q\right)
\in\mathbb{N}^{2}}\lambda_{p,q}$ still converges in the discrete topology
(this is analogous to Fubini's theorem). But the double sum $\sum
\limits_{\left(  p,q\right)  \in\mathbb{N}^{2}}\alpha_{p,q}$ does not converge
in the discrete topology, so $\sum\limits_{p\geq0}\sum\limits_{q\geq0}%
\alpha_{p,q}\neq\sum\limits_{q\geq0}\sum\limits_{p\geq0}\alpha_{p,q}$ should
not come as a surprise.}. So we know that the first term on the right hand
side of (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) cancels against the
third term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). Similarly, the third term
on the right hand side of (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2})
cancels against the first term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). Thus, when we subtract
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) from
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}), on the right hand side only
the second terms of both equations remain, and we obtain%
\begin{align*}
&  F_{a}\left(  F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  -F_{b}\left(  F_{a}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  ab\right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ -\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  ba\right)  \rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  ab-ba\right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the multilinearity of the infinite
wedge product}\right) \\
&  =F_{ab-ba}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right)  =F_{\left[  a,b\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  .
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2.goal}), and thus
Assertion \ref{prop.glinf.glinfact.welldef}.7. Filling the details of this
proof is left to the reader.

\begin{vershort}
\textit{Second proof of Assertion \ref{prop.glinf.glinfact.welldef}.7
(sketched):} Due to Assertion \ref{prop.glinf.glinfact.welldef}.5, the value
of $F_{c}$ for $c\in\mathfrak{gl}_{\infty}$ depends $\mathbb{C}$-linearly on
$c$.

But we must prove the equality $\left[  F_{a},F_{b}\right]  =F_{\left[
a,b\right]  }$ for all $a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}%
_{\infty}$. This equality is $\mathbb{C}$-linear in $a$ and $b$ (since the
value of $F_{c}$ for $c\in\mathfrak{gl}_{\infty}$ depends $\mathbb{C}%
$-linearly on $c$), so it is enough to show it only when $a$ and $b$ belong to
the basis $\left(  E_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$
of $\mathfrak{gl}_{\infty}$. But in this case, one can check this equality by
verifying that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies%
\[
\left[  F_{a},F_{b}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =F_{\left[  a,b\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  .
\]
This can be done (using Assertion \ref{prop.glinf.glinfact.welldef}.6) by a
straightforward distinction of cases (the cases depend on whether some indices
belong to $\left\{  i_{0},i_{1},i_{2},...\right\}  $ or not, and whether some
indices are equal or not). The reader should not have much of a trouble
supplying these arguments, but they are as unenlightening as one would expect.
There \textbf{is} a somewhat better way to do this verification (better in the
sense that less cases have to be considered) by means of exploiting some
symmetry; this relies on checking the following assertion:
\end{vershort}

\begin{verlong}
We will soon show a second proof of Assertion
\ref{prop.glinf.glinfact.welldef}.7 in more detail. This proof relies
substantially on the following assertion:
\end{verlong}

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.8:} Let $r$, $s$, $u$ and
$v$ be integers. Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ be an $m$-degression. Let $I$ denote the set $\left\{
i_{0},i_{1},i_{2},...\right\}  $.

\textbf{(a)} If $v\notin I$, then
\[
\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0.
\]


\textbf{(b)} If $s=v$, then
\[
\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0.
\]


\textbf{(c)} Assume that $s\neq v$. Let $\mathbf{w}:\mathbb{Z}\rightarrow
\mathbb{Z}$ be the function defined by
\[
\left(  \mathbf{w}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\right)  .
\]
\footnote{Here, the term $\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ makes sense, since $s\neq v$.} Then, $\left(  \mathbf{w}\left(
i_{0}\right)  ,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(
i_{2}\right)  ,...\right)  $ is a straying $m$-degression, and satisfies
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}


Here, whenever $\mathcal{A}$ is an assertion, we denote by $\left[
\mathcal{A}\right]  $ the integer $\left\{
\begin{array}
[c]{l}%
1,\text{ if }\mathcal{A}\text{ is true;}\\
0,\text{ if }\mathcal{A}\text{ is wrong}%
\end{array}
\right.  $.
\end{quote}

\begin{vershort}
The proof of this assertion, as well as the derivation of Assertion
\ref{prop.glinf.glinfact.welldef}.7 from it (Assertion
\ref{prop.glinf.glinfact.welldef}.8 must be applied twice), is left to the reader.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.8:} We know that
$\left(  i_{0},i_{1},i_{2},...\right)  $ is an $m$-degression, hence a
strictly decreasing straying $m$-degression (since the $m$-degressions are
exactly the strictly decreasing straying $m$-degressions). Since $\left(
i_{0},i_{1},i_{2},...\right)  $ is strictly decreasing, the numbers $i_{0}$,
$i_{1}$, $i_{2}$, $...$ are pairwise distinct. Hence,%
\begin{equation}
\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  =I\setminus
\left\{  i_{w}\right\}  \ \ \ \ \ \ \ \ \ \ \text{for every }w\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.8.pf.allbutone}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}):}
Let $w\in\mathbb{N}$. We have $i_{w}\neq i_{p}$ for every $p\in\mathbb{N}$
satisfying $w\neq p$ (since the numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are
pairwise distinct). Thus,%
\begin{align*}
i_{w}  &  \notin\left\{  i_{p}\ \mid\ \underbrace{p\in\mathbb{N};\ w\neq
p}_{\substack{\text{this is equivalent to}\\p\in\left\{  q\in\mathbb{N}%
\ \mid\ w\neq q\right\}  }}\right\}  =\left\{  i_{p}\ \mid\ p\in
\underbrace{\left\{  q\in\mathbb{N}\ \mid\ w\neq q\right\}  }_{=\left\{
0,1,...,w-1,w+1,w+2,...\right\}  }\right\} \\
&  =\left\{  i_{p}\ \mid\ p\in\left\{  0,1,...,w-1,w+1,w+2,...\right\}
\right\}  =\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  .
\end{align*}
Combined with $\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
\subseteq\left\{  i_{0},i_{1},i_{2},...\right\}  $, this yields that
\[
\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  \subseteq
\left\{  i_{0},i_{1},i_{2},...\right\}  \setminus\left\{  i_{w}\right\}  .
\]
Combined with%
\begin{align*}
&  \underbrace{\left\{  i_{0},i_{1},i_{2},...\right\}  }_{\substack{=\left\{
i_{0},i_{1},...,i_{w-1},i_{w},i_{w+1},i_{w+2},...\right\}  \\=\left\{
i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  \cup\left\{
i_{w}\right\}  }}\setminus\left\{  i_{w}\right\} \\
&  =\left(  \left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
\cup\left\{  i_{w}\right\}  \right)  \setminus\left\{  i_{w}\right\} \\
&  =\underbrace{\left(  \left\{  i_{0},i_{1},...,i_{w-1},i_{w+1}%
,i_{w+2},...\right\}  \setminus\left\{  i_{w}\right\}  \right)  }%
_{\subseteq\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  }%
\cup\underbrace{\left(  \left\{  i_{w}\right\}  \setminus\left\{
i_{w}\right\}  \right)  }_{=\varnothing}\\
&  \subseteq\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
\cup\varnothing=\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
,
\end{align*}
this yields $\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
=\left\{  i_{0},i_{1},i_{2},...\right\}  \setminus\left\{  i_{w}\right\}  $.
Since $\left\{  i_{0},i_{1},i_{2},...\right\}  =I$, this rewrites as $\left\{
i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  =I\setminus\left\{
i_{w}\right\}  $. Thus, (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}) is
proven.}

\textbf{(a)} Assume that $v\notin I$. Thus, $v\notin I=\left\{  i_{0}%
,i_{1},i_{2},...\right\}  $. Thus, $F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0$ (by Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)}, applied to $i=u$ and $j=v$)
and $F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =0$ (by Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(b)}, applied to $i=r$ and $j=v$). Hence,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=0}\right)  -\delta
_{s,u}\underbrace{F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  }_{=0}\\
&  =\underbrace{F_{E_{r,s}}\left(  0\right)  }_{\substack{=0\\\text{(since
}F_{E_{r,s}}\text{ is linear)}}}-\underbrace{\delta_{s,u}0}_{=0}=0-0=0.
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(a)}.

\textbf{(b)} Assume that $s=v$. If $v\notin I$, then Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is obviously
true\footnote{\textit{Proof.} Assume that $v\notin I$. Then, due to Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(a)}, we have $\left(  F_{E_{r,s}%
}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =0$. In other words, Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is true, qed.}. Hence, for
the rest of the proof of Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(b)}, we can WLOG assume that we don't have $v\notin I$. Assume this.

So we have $v\in I$ (since we don't have $v\notin I$). Thus, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$%
.\ \ \ \ \footnote{\textit{Proof:} Since $v\in I=\left\{  i_{0},i_{1}%
,i_{2},...\right\}  $, there exists at least one $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$.
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $v=i_{\ell}$. Then, $v=i_{\ell_{1}}$ (since $\ell_{1}$ is an element
$\ell$ of $\mathbb{N}$ such that $v=i_{\ell}$) and $v=i_{\ell_{2}}$
(similarly). Hence, $i_{\ell_{1}}=v=i_{\ell_{2}}$. If we had $\ell_{1}\neq
\ell_{2}$, then we would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the
numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise distinct), which would
contradict $i_{\ell_{1}}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq
\ell_{2}$. Hence, $\ell_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$v=i_{\ell}$, then $\ell_{1}=\ell_{2}$. In other words, there exists at most
one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$. Hence, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$ (since we also know
that there exists at least one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$),
qed.} Denote this $\ell$ by $L$. Then, $v=i_{L}$ (since $L$ is an $\ell
\in\mathbb{N}$ such that $v=i_{\ell}$). Now, define a sequence $\left(
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
=\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $. Thus,
\begin{align}
\left\{  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}   &
=\left\{  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right\} \nonumber\\
&  =\left\{  u\right\}  \cup\underbrace{\left\{  i_{0},i_{1},...,i_{L-1}%
,i_{L+1},i_{L+2},...\right\}  }_{\substack{=I\setminus\left\{  i_{L}\right\}
\\\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}), applied to
}w=L\text{)}}}\nonumber\\
&  =\left\{  u\right\}  \cup\left(  I\setminus\left\{  \underbrace{i_{L}}%
_{=v}\right\}  \right)  =\left\{  u\right\}  \cup\left(  I\setminus\left\{
v\right\}  \right)  . \label{pf.glinf.glinfact.welldef.8.pf.b.1}%
\end{align}


By the definition of $i_{L}^{\prime}$, we have $i_{L}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =u$ (since $L=L$).

On the other hand, $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} (applied to $u$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{u}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}%
\wedge...\label{pf.glinf.glinfact.welldef.8.pf.b.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,u,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime}%
,i_{2}^{\prime},...\right)  \right)  .\nonumber
\end{align}


Moreover, $\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $ is
a straying $m$-degression (by Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(a)}, applied to $u$, $u$ and $L$ instead of $i$, $j$ and $\ell$). In
other words, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
$ is a straying $m$-degression (since $\left(  i_{0}^{\prime},i_{1}^{\prime
},i_{2}^{\prime},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1}%
,i_{L+2},...\right)  $).

We now distinguish between two cases:

\textit{Case 1:} We have $s=u$.

\textit{Case 2:} We don't have $s=u$.

Let us first consider Case 1. In this case, $s=u$. Hence, $s=u=i_{L}^{\prime}%
$. Thus,
\begin{equation}
\text{there exists at least one }\ell\in\mathbb{N}\text{ satisfying }%
s=i_{\ell}^{\prime} \label{pf.glinf.glinfact.welldef.8.pf.b.case1.1}%
\end{equation}
(namely, $\ell=L$). But there exists at most one $\ell\in\mathbb{N}$
satisfying $s=i_{\ell}^{\prime}$\ \ \ \ \footnote{\textit{Proof.} Let $\ell
\in\mathbb{N}$ satisfy $s=i_{\ell}^{\prime}$. We will prove that $\ell=L$.
\par
Assume (for the sake of contradiction) that $\ell\neq L$. Then, by the
definition of $i_{\ell}^{\prime}$, we have $i_{\ell}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\ell\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }\ell=L
\end{array}
\right.  =i_{\ell}$ (since $\ell\neq L$). But since $\ell\in\mathbb{N}$ and
$\ell\neq L$, we have $\ell\in\mathbb{N}\setminus\left\{  L\right\}  =\left\{
0,1,...,L-1,L+1,L+2,...\right\}  $, so that $i_{L}\in\left\{  i_{0}%
,i_{1},...,i_{L-1},i_{L+1},i_{L+2},...\right\}  =I\setminus\left\{
i_{L}\right\}  $ (by (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}), applied
to $w=L$). Thus, $s=i_{\ell}^{\prime}=i_{\ell}\in I\setminus\left\{
i_{L}\right\}  =I\setminus\left\{  v\right\}  $ (since $i_{L}=v$).
Consequently, $s\neq v$. This contradicts $s=v$. This contradiction shows that
our assumption (that $\ell\neq L$) was wrong. Hence, $\ell=L$.
\par
Now forget that we fixed $\ell$. We thus have shown that every $\ell
\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must satisfy $\ell=L$. In
other words, every $\ell\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must
equal $L$. Hence, there exists at most one $\ell\in\mathbb{N}$ satisfying
$s=i_{\ell}^{\prime}$, qed.}. Combined with
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.1}), this yields that there
exists a \textbf{unique }$\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$.
This $\ell$ is $L$ (because $s=i_{L}^{\prime}$). Therefore, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} to $i_{k}^{\prime}%
$, $r$ and $s$ instead of $i_{k}$, $i$ and $j$. As a result, we obtain%
\[
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)  =v_{i_{0}^{\prime}}\wedge v_{i_{1}%
^{\prime}}\wedge...\wedge v_{i_{L-1}^{\prime}}\wedge v_{r}\wedge
v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}}\wedge....
\]
Thus,%
\begin{align}
&  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{L-1}%
^{\prime}}\wedge v_{r}\wedge v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}%
}\wedge...\nonumber\\
&  =F_{E_{r,s}}\left(  \underbrace{v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...}_{\substack{=F_{E_{u,v}}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.2}))}}}\right) \nonumber\\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  .
\label{pf.glinf.glinfact.welldef.8.pf.b.case1.3}%
\end{align}


On the other hand, define a sequence $\left(  i_{0}^{\prime\prime}%
,i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},r,i_{L+1},i_{L+2},...\right)  $.

By the definition of $i_{L}^{\prime\prime}$, we have $i_{L}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r$ (since $L=L$).

But we have%
\begin{equation}
i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}
\label{pf.glinf.glinfact.welldef.8.pf.b.case1.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5}):}
Let $k\in\mathbb{N}$.
\par
Notice that $\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r$ (since $L=L$). Thus, $i_{L}^{\prime\prime}=r=\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  $. In other words, (\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5})
holds in the case when $k=L$. Hence, for the rest of the proof of
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5}), we can WLOG assume that
$k=L$ does not hold. Let us assume this.
\par
We have assumed that $k=L$ does not hold. In other words, $k\neq L$. By the
definition of $i_{k}^{\prime\prime}$, we have $i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  =i_{k}$ (since $k\neq L$). But%
\begin{align*}
\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.   &  =i_{k}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq
L\right) \\
&  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\end{align*}
Compared with $i_{k}^{\prime\prime}=i_{k}$, this yields $i_{k}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  $. This proves (\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5}).}.
Hence, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{L-1}^{\prime
},r,i_{L+1}^{\prime},i_{L+2}^{\prime},...\right)  $.

Recall that $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} (applied to $r$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{r}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,r,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime\prime},i_{1}%
^{\prime\prime},i_{2}^{\prime\prime},...\right)  \right) \nonumber\\
&  =v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{L-1}%
^{\prime}}\wedge v_{r}\wedge v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}%
}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0}^{\prime\prime}%
,i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  =\left(  i_{0}^{\prime
},i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  \right) \nonumber\\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.3})}\right)  .
\label{pf.glinf.glinfact.welldef.8.pf.b.case1.9}%
\end{align}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  -\underbrace{\delta_{s,u}}%
_{\substack{=1\\\text{(since }s=u\text{)}}}\underbrace{F_{E_{r,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }%
_{\substack{=F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  \\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.9}))}}}\\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  -F_{E_{r,s}}\left(  F_{E_{u,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =0.
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is proven in
Case 1.

Let us now consider Case 2. In this case, we don't have $s=u$. Thus, $s\neq
u$. Hence, $s\notin\left\{  u\right\}  $. Combined with $s=v\notin
I\setminus\left\{  v\right\}  $, this yields $s\notin\left\{  u\right\}
\cup\left(  I\setminus\left\{  v\right\}  \right)  =\left\{  i_{0}^{\prime
},i_{1}^{\prime},i_{2}^{\prime},...\right\}  $ (by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.1})). Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)} (applied to $i_{k}^{\prime}$,
$r$ and $s$ instead of $i_{k}$, $i$ and $j$) yields
\begin{equation}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)  =0.
\label{pf.glinf.glinfact.welldef.8.pf.b.case2.1}%
\end{equation}
Thus,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case2.1}))}}}\right)
-\underbrace{\delta_{s,u}}_{\substack{=0\\\text{(since }s\neq u\text{)}%
}}F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case2.1}))}}}-\underbrace{0F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }%
_{=0}=0-0=0.
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is proven in
Case 2.

Hence, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is proven in
each of the Cases 1 and 2. Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(b)} is proven in all cases (since Cases 1 and 2 cover all possible situations).

\textbf{(c)} It is easy to see that $\left(  \mathbf{w}\left(  i_{0}\right)
,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)
$ is a straying $m$-degression\footnote{\textit{Proof.} Recall that $\left(
i_{0},i_{1},i_{2},...\right)  $ is a straying $m$-degression. Due to the
definition of a straying $m$-degression, this means that every sufficiently
high $k\in\mathbb{N}$ satisfies $i_{k}+k=m$. Thus, we know that every
sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}+k=m$. In other words,
there exists a $K\in\mathbb{N}$ such that every nonnegative integer $k\geq K$
satisfies $i_{k}+k=m$. Consider this $k$.
\par
Now, let $k$ be a nonnegative integer such that $k\geq\max\left\{
K,m-s+1,m-v+1\right\}  $. Then, $k\geq\max\left\{  K,m-s+1,m-v+1\right\}  \geq
K$. Hence, $i_{k}+k=m$, so that $i_{k}=m-k$. Moreover, $k\geq\max\left\{
K,m-s+1,m-v+1\right\}  \geq m-s+1>m-s$, so that $m-k<s$. Thus, $m-k\neq s$.
Also, $k\geq\max\left\{  K,m-s+1,m-v+1\right\}  \geq m-v+1>m-v$, so that
$m-k<v$. Hence, $m-k\neq v$. Since $m-k\neq s$ and $m-k\neq v$, we have
neither $m-k=s$ nor $m-k=v$. Now,%
\begin{align*}
\mathbf{w}\left(  \underbrace{i_{k}}_{=m-k}\right)   &  =\mathbf{w}\left(
m-k\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }m-k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }m-k=v;\\
m-k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\mathbf{w}\left(  m-k\right)  \right) \\
&  =m-k\ \ \ \ \ \ \ \ \ \ \left(  \text{since we have neither }m-k=s\text{
nor }m-k=v\right)
\end{align*}
and thus $\mathbf{w}\left(  i_{k}\right)  +k=m$.
\par
Now, forget that we fixed $k$. We thus have shown that every nonnegative
integer $k\geq\max\left\{  K,m-s+1,m-v+1\right\}  $ satisfies $\mathbf{w}%
\left(  i_{k}\right)  +k=m$. Hence, every sufficiently high $k\in\mathbb{N}$
satisfies $\mathbf{w}\left(  i_{k}\right)  +k=m$. According the definition of
a straying $m$-degression, this means precisely that $\left(  \mathbf{w}%
\left(  i_{0}\right)  ,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(
i_{2}\right)  ,...\right)  $ is a straying $m$-degression. We have thus proven
that $\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(  i_{1}\right)
,\mathbf{w}\left(  i_{2}\right)  ,...\right)  $ is a straying $m$-degression,
qed.}. Thus, in order to prove Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(c)}, it only remains to show that%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}


If $v\notin I$, then Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(c)} is obviously true\footnote{\textit{Proof.} Assume that $v\notin
I$. Then, due to Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(a)},
we have $\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0$.
\par
But since $v\notin I$, the assertion $v\in I$ does not hold. Hence, $\left[
v\in I\right]  =0$. Thus,
\[
\left[  s\in I\right]  \cdot\underbrace{\left[  v\in I\right]  }_{=0}\cdot
v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...=0.
\]
Thus,%
\[
\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0=\left[  s\in
I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}\left(  i_{0}\right)
}\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}\left(
i_{2}\right)  }\wedge....
\]
In other words, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is
true, qed.}. Hence, for the rest of the proof of Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}, we can WLOG assume that we
don't have $v\notin I$. Assume this.

So we have $v\in I$ (since we don't have $v\notin I$). Thus, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$%
.\ \ \ \ \footnote{\textit{Proof:} Since $v\in I=\left\{  i_{0},i_{1}%
,i_{2},...\right\}  $, there exists at least one $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$.
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $v=i_{\ell}$. Then, $v=i_{\ell_{1}}$ (since $\ell_{1}$ is an element
$\ell$ of $\mathbb{N}$ such that $v=i_{\ell}$) and $v=i_{\ell_{2}}$
(similarly). Hence, $i_{\ell_{1}}=v=i_{\ell_{2}}$. If we had $\ell_{1}\neq
\ell_{2}$, then we would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the
numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise distinct), which would
contradict $i_{\ell_{1}}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq
\ell_{2}$. Hence, $\ell_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$v=i_{\ell}$, then $\ell_{1}=\ell_{2}$. In other words, there exists at most
one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$. Hence, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$ (since we also know
that there exists at least one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$),
qed.} Denote this $\ell$ by $L$. Then, $v=i_{L}$ (since $L$ is an $\ell
\in\mathbb{N}$ such that $v=i_{\ell}$). Now, define a sequence $\left(
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
=\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $. Thus,
\begin{align}
\left\{  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}   &
=\left\{  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right\} \nonumber\\
&  =\left\{  u\right\}  \cup\underbrace{\left\{  i_{0},i_{1},...,i_{L-1}%
,i_{L+1},i_{L+2},...\right\}  }_{\substack{=I\setminus\left\{  i_{L}\right\}
\\\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}), applied to
}w=L\text{)}}}\nonumber\\
&  =\left\{  u\right\}  \cup\left(  I\setminus\left\{  \underbrace{i_{L}}%
_{=v}\right\}  \right)  =\left\{  u\right\}  \cup\left(  I\setminus\left\{
v\right\}  \right)  . \label{pf.glinf.glinfact.welldef.8.pf.c.1}%
\end{align}


By the definition of $i_{L}^{\prime}$, we have $i_{L}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =u$ (since $L=L$).

On the other hand, $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} (applied to $u$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{u}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}%
\wedge...\label{pf.glinf.glinfact.welldef.8.pf.c.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,u,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime}%
,i_{2}^{\prime},...\right)  \right)  .\nonumber
\end{align}


Moreover, $\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $ is
a straying $m$-degression (by Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(a)}, applied to $u$, $u$ and $L$ instead of $i$, $j$ and $\ell$). In
other words, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
$ is a straying $m$-degression (since $\left(  i_{0}^{\prime},i_{1}^{\prime
},i_{2}^{\prime},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1}%
,i_{L+2},...\right)  $).

We now distinguish between two cases:

\textit{Case 1:} We have $s=u$.

\textit{Case 2:} We don't have $s=u$.

Let us first consider Case 1. In this case, $s=u$. Hence, $s=u=i_{L}^{\prime}$.

Let us define a sequence $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime
},i_{2}^{\prime\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},r,i_{L+1},i_{L+2},...\right)  $.

By the definition of $i_{L}^{\prime\prime}$, we have $i_{L}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r$ (since $L=L$).

But $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$.
Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} (applied to
$r$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{r}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge
...\label{pf.glinf.glinfact.welldef.8.pf.c.case1.o.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,r,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime\prime},i_{1}%
^{\prime\prime},i_{2}^{\prime\prime},...\right)  \right)  .\nonumber
\end{align}


Now, we distinguish between the following two subcases:

\textit{Subcase 1.1:} We have $s\in I$.

\textit{Subcase 1.2:} We don't have $s\in I$.

Let us consider Subcase 1.1 first. In this subcase, we have $s\in I$. Thus,
there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that $s=i_{\ell}%
$.\ \ \ \ \footnote{\textit{Proof:} Since $s\in I=\left\{  i_{0},i_{1}%
,i_{2},...\right\}  $, there exists at least one $\ell\in\mathbb{N}$ such that
$s=i_{\ell}$.
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $s=i_{\ell}$. Then, $s=i_{\ell_{1}}$ (since $\ell_{1}$ is an element
$\ell$ of $\mathbb{N}$ such that $s=i_{\ell}$) and $s=i_{\ell_{2}}$
(similarly). Hence, $i_{\ell_{1}}=s=i_{\ell_{2}}$. If we had $\ell_{1}\neq
\ell_{2}$, then we would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the
numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise distinct), which would
contradict $i_{\ell_{1}}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq
\ell_{2}$. Hence, $\ell_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$s=i_{\ell}$, then $\ell_{1}=\ell_{2}$. In other words, there exists at most
one $\ell\in\mathbb{N}$ such that $s=i_{\ell}$. Hence, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $s=i_{\ell}$ (since we also know
that there exists at least one $\ell\in\mathbb{N}$ such that $s=i_{\ell}$),
qed.} Denote this $\ell$ by $M$. Thus, $s=i_{M}$ (since $M$ is an $\ell
\in\mathbb{N}$ such that $s=i_{\ell}$). Since $i_{M}=s\neq v=i_{L}$, we have
$M\neq L$. Now, by the definition of $i_{M}^{\prime}$, we have%
\begin{align*}
i_{M}^{\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }M=L
\end{array}
\right.  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right) \\
&  =s=u=i_{L}^{\prime}.
\end{align*}
Hence, $v_{i_{M}^{\prime}}=v_{i_{L}^{\prime}}$. Since $M\neq L$, this yields
that two of the vectors $v_{i_{0}^{\prime}},v_{i_{1}^{\prime}},v_{i_{2}%
^{\prime}},...$ are equal (namely, the vectors $v_{i_{M}^{\prime}}$ and
$v_{i_{L}^{\prime}}$). Hence, (\ref{glinf.semiinfwedge.antisym}) (applied to
$b_{k}=v_{i_{k}^{\prime}}$) yields that $v_{i_{0}^{\prime}}\wedge
v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...=0$. Thus,
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}) becomes%
\begin{equation}
F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}%
\wedge...=0. \label{pf.glinf.glinfact.welldef.8.pf.c.case1.1.2}%
\end{equation}


Let $S_{\infty}$ be the group of all finitary permutations of $\mathbb{N}$.
Let $\tau\in S_{\infty}$ be the transposition $\left(  L,M\right)  $ (this is
well-defined because $M\neq L$). Clearly, $\left(  -1\right)  ^{\tau}=-1$
(since $\tau$ is a transposition).

Since $\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(
i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)  $ is a straying
$m$-degression, we know that every sufficiently high $k\in\mathbb{N}$
satisfies $\mathbf{w}\left(  i_{k}\right)  +k=m$ (by the definition of a
``straying $m$-degression''). In other words, we have $\mathbf{w}\left(
i_{k}\right)  +k=m$ for sufficiently large $k$. Renaming $k$ as $i$ in this
assertion, we conclude: We have $\mathbf{w}\left(  i_{i}\right)  +i=m$ for
sufficiently large $i$. In other words, $\mathbf{w}\left(  i_{i}\right)  =m-i$
for sufficiently large $i$. Therefore, $v_{\mathbf{w}\left(  i_{i}\right)
}=v_{m-i}$ for sufficiently large $i$. Hence, applying Proposition
\ref{prop.semiinfwedge.welldef} \textbf{(f)} to $b_{k}=v_{\mathbf{w}\left(
i_{k}\right)  }$ and $\pi=\tau$, we obtain%
\begin{align}
v_{\mathbf{w}\left(  i_{\tau\left(  0\right)  }\right)  }\wedge v_{\mathbf{w}%
\left(  i_{\tau\left(  1\right)  }\right)  }\wedge v_{\mathbf{w}\left(
i_{\tau\left(  2\right)  }\right)  }\wedge...  &  =\underbrace{\left(
-1\right)  ^{\tau}}_{=-1}\cdot v_{\mathbf{w}\left(  i_{0}\right)  }\wedge
v_{\mathbf{w}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)
}\wedge...\nonumber\\
&  =-v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\label{pf.glinf.glinfact.welldef.8.pf.c.case1.1.4}%
\end{align}
But every $k\in\mathbb{N}$ satisfies $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{\tau\left(  k\right)  }\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\mathbb{N}$. We have to prove that $i_{k}^{\prime\prime}=\mathbf{w}%
\left(  i_{\tau\left(  k\right)  }\right)  $.
\par
Notice that $\tau\left(  M\right)  =L$ (since $\tau=\left(  L,M\right)  $).
Thus,%
\begin{align*}
\mathbf{w}\left(  i_{\tau\left(  M\right)  }\right)   &  =\mathbf{w}\left(
\underbrace{i_{L}}_{=v}\right)  =\mathbf{w}\left(  v\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }v=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }v=v;\\
v,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{w}\left(
v\right)  \right) \\
&  =u\ \ \ \ \ \ \ \ \ \ \left(  \text{since }v=v\right)  .
\end{align*}
Compared with%
\begin{align*}
i_{M}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{M}%
^{\prime\prime}\right) \\
&  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right) \\
&  =s=u\ \ \ \ \ \ \ \ \ \ \left(  \text{since we are in Case 1}\right)  ,
\end{align*}
this yields $i_{M}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  M\right)
}\right)  $. In other words, $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{\tau\left(  k\right)  }\right)  $ holds if $k=M$. Hence, we have proven
$i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)  $
in the case when $k=M$. Thus, for the rest of the proof of $i_{k}%
^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)  $, we can
WLOG assume that we don't have $k=M$. Assume this. So we know that we don't
have $k=M$. In other words, $k\neq M$.
\par
Since $\tau=\left(  L,M\right)  $, we have $\tau\left(  L\right)  =M$. Hence,%
\begin{align*}
\mathbf{w}\left(  i_{\tau\left(  L\right)  }\right)   &  =\mathbf{w}\left(
\underbrace{i_{M}}_{=s}\right)  =\mathbf{w}\left(  s\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }s=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }s=v;\\
s,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{w}\left(
s\right)  \right) \\
&  =r\ \ \ \ \ \ \ \ \ \ \left(  \text{since }s=s\right)  .
\end{align*}
Compared with $i_{L}^{\prime\prime}=r$, this yields $i_{L}^{\prime\prime
}=\mathbf{w}\left(  i_{\tau\left(  L\right)  }\right)  $. In other words,
$i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)  $
holds if $k=L$. Hence, we have proven $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{\tau\left(  k\right)  }\right)  $ in the case when $k=L$. Thus, for the
rest of the proof of $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(
k\right)  }\right)  $, we can WLOG assume that we don't have $k=L$. Assume
this. So we know that we don't have $k=L$. In other words, $k\neq L$.
\par
Since $k\neq L$ and $k\neq M$, we have $k\notin\left\{  L,M\right\}  $, thus
$k\in\mathbb{N}\setminus\left\{  L,M\right\}  $ (since $k\in\mathbb{N}$).
\par
Now, by the definition of $i_{k}^{\prime\prime}$, we have%
\begin{align*}
i_{k}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\end{align*}
\par
On the other hand, the numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise
distinct. Thus, $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$ and
$\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to $\alpha=k$
and $\beta=L$, we obtain $i_{k}\neq i_{L}$ (since $k\neq L$). Since $i_{L}=v$,
this rewrites as $i_{k}\neq v$.
\par
Again, recall that $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$
and $\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to
$\alpha=k$ and $\beta=M$, we obtain $i_{k}\neq i_{M}$ (since $k\neq M$). Since
$i_{M}=s$, this rewrites as $i_{k}\neq s$.
\par
Since $\tau$ is the transposition $\left(  L,M\right)  $, we know that $\tau$
leaves any element of $\mathbb{N}$ other than $L$ and $M$ fixed. In other
words, $\tau\left(  \alpha\right)  =\alpha$ for every $\alpha\in
\mathbb{N}\setminus\left\{  L,M\right\}  $. Applying this to $\alpha=k$, we
obtain $\tau\left(  k\right)  =k$ (since $k\in\mathbb{N}\setminus\left\{
L,M\right\}  $).
\par
Since $i_{k}\neq s$ and $i_{k}\neq v$, we have neither $i_{k}=s$ nor $i_{k}%
=v$. Now,%
\begin{align*}
\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)   &  =\mathbf{w}\left(
i_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau\left(  k\right)
=k\right) \\
&  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=v;\\
i_{k},\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\mathbf{w}\left(  i_{k}\right)  \right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }i_{k}=s\text{ nor
}i_{k}=v\right)  .
\end{align*}
Thus, $i_{k}^{\prime\prime}=i_{k}=\mathbf{w}\left(  i_{\tau\left(  k\right)
}\right)  $, qed.}. In other words, $\left(  i_{0}^{\prime\prime}%
,i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  =\left(  \mathbf{w}%
\left(  i_{\tau\left(  0\right)  }\right)  ,\mathbf{w}\left(  i_{\tau\left(
1\right)  }\right)  ,\mathbf{w}\left(  i_{\tau\left(  2\right)  }\right)
,...\right)  $. Hence,%
\begin{align}
&  v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\nonumber\\
&  =v_{\mathbf{w}\left(  i_{\tau\left(  0\right)  }\right)  }\wedge
v_{\mathbf{w}\left(  i_{\tau\left(  1\right)  }\right)  }\wedge v_{\mathbf{w}%
\left(  i_{\tau\left(  2\right)  }\right)  }\wedge...\nonumber\\
&  =-v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge
...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.1.4})}\right)  .
\label{pf.glinf.glinfact.welldef.8.pf.c.case1.1.6}%
\end{align}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.1.2}))}}}\right)
-\underbrace{\delta_{s,u}}_{\substack{=1\\\text{(since }s=u\text{)}%
}}\underbrace{F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}%
^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.o.1}))}}}\\
&  =\underbrace{F_{E_{r,s}}\left(  0\right)  }_{\substack{=0\\\text{(since
}F_{E_{r,s}}\text{ is a}\\\text{linear map)}}}-v_{i_{0}^{\prime\prime}}\wedge
v_{i_{1}^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}}\wedge
...=-\underbrace{v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...}_{\substack{=-v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.1.6}))}}}\\
&  =-\left(  -v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\right)
=v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\left[  s\in I\right]  }_{\substack{=1\\\text{(since }s\in
I\text{)}}}\cdot\underbrace{\left[  v\in I\right]  }%
_{\substack{=1\\\text{(since }v\in I\text{)}}}\cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...,
\end{align*}
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 1.1.

Let us now consider Subcase 1.2. In this subcase, we don't have $s\in I$.
Thus, $s\notin I$.

Let us define a sequence $\left(  j_{0},j_{1},j_{2},...\right)  $ by%
\[
\left(  j_{k}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  j_{0},j_{1},j_{2},...\right)  =\left(  i_{0}^{\prime}%
,i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  $.

But $j_{k}=i_{k}^{\prime\prime}$ for every $k\in\mathbb{N}$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\mathbb{N}$. We need to prove that
$j_{k}=i_{k}^{\prime\prime}$.
\par
First, notice that the definition of $j_{L}$ yields%
\begin{align*}
j_{L}  &  =\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r\ \ \ \ \ \ \ \ \ \ \left(  \text{since }L=L\right) \\
&  =i_{L}^{\prime\prime}.
\end{align*}
In other words, $j_{k}=i_{k}^{\prime\prime}$ holds for $k=L$. Thus,
$j_{k}=i_{k}^{\prime\prime}$ is proven in the case when $k=L$. Hence, for the
rest of the proof of $j_{k}=i_{k}^{\prime\prime}$, we can WLOG assume that we
don't have $k=L$. Assume this.
\par
We don't have $k=L$. In other words, $k\neq L$. Now, the definition of
$i_{k}^{\prime\prime}$ yields%
\[
i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\]
On the other hand, from the definition of $j_{k}$, we obtain%
\begin{align*}
j_{k}  &  =\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  =i_{k}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq
L\right) \\
&  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right) \\
&  =i_{k}^{\prime\prime},
\end{align*}
qed.}. In other words, $\left(  j_{0},j_{1},j_{2},...\right)  =\left(
i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  $.
Hence,%
\begin{equation}
\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  =\left(  i_{0}^{\prime
},i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  . \label{pf.glinf.glinfact.welldef.8.pf.c.case1.2.0}%
\end{equation}


Now, recall that $s=i_{L}^{\prime}$. Thus,%
\begin{equation}
\text{there exists at least one }\ell\in\mathbb{N}\text{ satisfying }%
s=i_{\ell}^{\prime} \label{pf.glinf.glinfact.welldef.8.pf.c.case1.2.1}%
\end{equation}
(namely, $\ell=L$). But there exists at most one $\ell\in\mathbb{N}$
satisfying $s=i_{\ell}^{\prime}$\ \ \ \ \footnote{\textit{Proof.} Let $\ell
\in\mathbb{N}$ satisfy $s=i_{\ell}^{\prime}$. We will prove that $\ell=L$.
\par
Assume (for the sake of contradiction) that $\ell\neq L$. Then, by the
definition of $i_{\ell}^{\prime}$, we have $i_{\ell}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\ell\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }\ell=L
\end{array}
\right.  =i_{\ell}$ (since $\ell\neq L$). Thus, $s=i_{\ell}^{\prime}=i_{\ell
}\in\left\{  i_{0},i_{1},i_{2},...\right\}  =I$, contradicting $s\notin I$.
This contradiction shows that our assumption (that $\ell\neq L$) was wrong.
Hence, $\ell=L$.
\par
Now forget that we fixed $\ell$. We thus have shown that every $\ell
\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must satisfy $\ell=L$. In
other words, every $\ell\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must
equal $L$. Hence, there exists at most one $\ell\in\mathbb{N}$ satisfying
$s=i_{\ell}^{\prime}$, qed.}. Combined with
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.2.1}), this yields that there
exists a \textbf{unique }$\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$.
This $\ell$ is $L$ (because $s=i_{L}^{\prime}$). Therefore, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} to $i_{k}^{\prime}%
$, $r$ and $s$ instead of $i_{k}$, $i$ and $j$. As a result, we obtain%
\begin{align}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)   &  =v_{i_{0}^{\prime}}\wedge
v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{L-1}^{\prime}}\wedge v_{r}\wedge
v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge
...\label{pf.glinf.glinfact.welldef.8.pf.c.case1.2.5}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{L-1}^{\prime
},r,i_{L+1}^{\prime},i_{L+2}^{\prime},...\right)  =\left(  i_{0}^{\prime
\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right) \\
\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.2.0}))}%
\end{array}
\right)  .\nonumber
\end{align}
Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}))}}}\right)  -\underbrace{\delta
_{s,u}}_{\substack{=1\\\text{(since }s=u\text{)}}}\underbrace{F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
}_{\substack{=v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.o.1}))}}}\\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=v_{i_{0}%
^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}%
}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.2.5}))}%
}}-v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...-v_{i_{0}^{\prime\prime}}\wedge
v_{i_{1}^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}}\wedge...=0.
\end{align*}
Compared with%
\[
\underbrace{\left[  s\in I\right]  }_{\substack{=0\\\text{(since we don't have
}s\in I\text{)}}}\cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...=0,
\]
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 1.2.

We have now proven Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}
in each of the Subcases 1.1 and 1.2. Since these Subcases 1.1 and 1.2 cover
the whole Case 1, this yields that Assertion \ref{prop.glinf.glinfact.welldef}%
.8 \textbf{(c)} is proven in the whole Case 1.

Let us now consider Case 2. In this case, we don't have $s=u$. Thus, $s\neq u$.

Now, we distinguish between the following two subcases:

\textit{Subcase 2.1:} We have $s\in I$.

\textit{Subcase 2.2:} We don't have $s\in I$.

Let us first consider Subcase 2.1. In this subcase, we have $s\in I$. Recall
also that $s\neq u$, and the definition of $i_{k}^{\prime}$. It is now easy to
see that there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that
$s=i_{\ell}^{\prime}$.\ \ \ \ \footnote{\textit{Proof:} Since $s\in I=\left\{
i_{0},i_{1},i_{2},...\right\}  $, there exists at least one $\ell\in
\mathbb{N}$ such that $s=i_{\ell}$. Denote this $\ell$ by $M$. Then,
$M\in\mathbb{N}$ satisfies $s=i_{M}$.
\par
Since $i_{M}=s\neq v=i_{L}$, we have $M\neq L$. Now, by the definition of
$i_{M}^{\prime}$, we have%
\begin{align*}
i_{M}^{\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }M=L
\end{array}
\right.  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right) \\
&  =s.
\end{align*}
Thus, $s=i_{M}^{\prime}$. Hence, there exists at least one $\ell\in\mathbb{N}$
such that $s=i_{\ell}^{\prime}$ (namely, $\ell=M$).
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $s=i_{\ell}^{\prime}$. Then, $s=i_{\ell_{1}}^{\prime}$ (since $\ell_{1}$
is an element $\ell$ of $\mathbb{N}$ such that $s=i_{\ell}^{\prime}$). Hence,
$i_{\ell_{1}}^{\prime}=s\neq u=i_{L}^{\prime}$, so that $\ell_{1}\neq L$. Now,
by the definition of $i_{\ell_{1}}^{\prime}$, we have%
\[
i_{\ell_{1}}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{\ell_{1}},\ \ \ \ \ \ \ \ \ \ \text{if }\ell_{1}\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }\ell_{1}=L
\end{array}
\right.  =i_{\ell_{1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell_{1}\neq
L\right)  .
\]
Thus, $i_{\ell_{1}}=i_{\ell_{1}}^{\prime}=s$. The same argument, applied to
$\ell_{2}$ instead of $\ell_{1}$, shows that $i_{\ell_{2}}=s$. Thus,
$i_{\ell_{1}}=s=i_{\ell_{2}}$. Now, if we had $\ell_{1}\neq\ell_{2}$, then we
would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the numbers $i_{0}$, $i_{1}%
$, $i_{2}$, $...$ are pairwise distinct), which would contradict $i_{\ell_{1}%
}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq\ell_{2}$. Hence, $\ell
_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$s=i_{\ell}^{\prime}$, then $\ell_{1}=\ell_{2}$. In other words, there exists
at most one $\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$. Hence, there
exists a \textbf{unique} $\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$
(since we also know that there exists at least one $\ell\in\mathbb{N}$ such
that $s=i_{\ell}^{\prime}$), qed.} Denote this $\ell$ by $M$. Thus,
$s=i_{M}^{\prime}$ (since $M$ is an $\ell\in\mathbb{N}$ such that $s=i_{\ell
}^{\prime}$). Since $i_{M}^{\prime}=s\neq u=i_{L}^{\prime}$, we have $M\neq
L$. Now, by the definition of $i_{M}^{\prime}$, we have%
\[
i_{M}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }M=L
\end{array}
\right.  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right)  .
\]
Hence, $i_{M}=i_{M}^{\prime}=s$.

Let us define a sequence $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime
},i_{2}^{\prime\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=M
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then,
\begin{equation}
\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{M-1}^{\prime
},r,i_{M+1}^{\prime},i_{M+2}^{\prime},...\right)  .
\label{pf.glinf.glinfact.welldef.8.pf.c.case2.1.1}%
\end{equation}


By the definition of $i_{M}^{\prime\prime}$, we have $i_{M}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{M}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }M=M
\end{array}
\right.  =r$ (since $M=M$).

But every $k\in\mathbb{N}$ satisfies $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{k}\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $k\in\mathbb{N}$. We have
to prove that $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{k}\right)  $.
\par
If $k=M$, then%
\begin{align*}
i_{k}^{\prime\prime}  &  =i_{M}^{\prime\prime}=r=\mathbf{w}\left(  s\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the definition of }\mathbf{w}%
\left(  s\right)  \text{ yields }\mathbf{w}\left(  s\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }s=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }s=v;\\
s,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =r\text{ (since }s=s\text{)}\right) \\
&  =\mathbf{w}\left(  i_{M}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}s=i_{M}\right) \\
&  =\mathbf{w}\left(  i_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}M=k\right)  .
\end{align*}
Hence, if $k=M$, then $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{k}\right)  $
is proven. Thus, for the rest of the proof of $i_{k}^{\prime\prime}%
=\mathbf{w}\left(  i_{k}\right)  $, we can WLOG assume that we don't have
$k=M$. Assume this. So we know that we don't have $k=M$. In other words,
$k\neq M$.
\par
If $k=L$, then%
\begin{align*}
i_{k}^{\prime\prime}  &  =i_{L}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=M
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{L}%
^{\prime\prime}\right) \\
&  =i_{L}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }L\neq M\text{
(since }M\neq L\text{)}\right) \\
&  =u=\mathbf{w}\left(  v\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the definition of }\mathbf{w}%
\left(  v\right)  \text{ yields }\mathbf{w}\left(  v\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }v=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }v=v;\\
v,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =u\text{ (since }v=v\text{)}\right) \\
&  =\mathbf{w}\left(  i_{L}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}v=i_{L}\right) \\
&  =\mathbf{w}\left(  i_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}L=k\right)  .
\end{align*}
Hence, if $k=L$, then $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{k}\right)  $
is proven. Thus, for the rest of the proof of $i_{k}^{\prime\prime}%
=\mathbf{w}\left(  i_{k}\right)  $, we can WLOG assume that we don't have
$k=L$. Assume this. So we know that we don't have $k=L$. In other words,
$k\neq L$.
\par
Now, by the definition of $i_{k}^{\prime\prime}$, we have%
\begin{align*}
i_{k}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=M
\end{array}
\right.  =i_{k}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq
M\right) \\
&  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\end{align*}
\par
On the other hand, the numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise
distinct. Thus, $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$ and
$\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to $\alpha=k$
and $\beta=L$, we obtain $i_{k}\neq i_{L}$ (since $k\neq L$). Since $i_{L}=v$,
this rewrites as $i_{k}\neq v$.
\par
Again, recall that $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$
and $\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to
$\alpha=k$ and $\beta=M$, we obtain $i_{k}\neq i_{M}$ (since $k\neq M$). Since
$i_{M}=s$, this rewrites as $i_{k}\neq s$.
\par
Since $i_{k}\neq s$ and $i_{k}\neq v$, we have neither $i_{k}=s$ nor $i_{k}%
=v$. Now, by the definition of $\mathbf{w}\left(  i_{k}\right)  $, we have%
\[
\mathbf{w}\left(  i_{k}\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=v;\\
i_{k},\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }i_{k}=s\text{
nor }i_{k}=v\right)  .
\]
Thus, $i_{k}^{\prime\prime}=i_{k}=\mathbf{w}\left(  i_{k}\right)  $, qed.}. In
other words, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}%
^{\prime\prime},...\right)  =\left(  \mathbf{w}\left(  i_{0}\right)
,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)
$. Hence, (\ref{pf.glinf.glinfact.welldef.8.pf.c.case2.1.1}) becomes%
\begin{align*}
&  \left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{M-1}^{\prime},r,i_{M+1}%
^{\prime},i_{M+2}^{\prime},...\right) \\
&  =\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(
i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)  .
\end{align*}


Now, recall that $M$ is the unique $\ell\in\mathbb{N}$ such that $s=i_{\ell
}^{\prime}$. Hence, we can apply Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} to $M$, $i_{k}^{\prime}$, $r$ and $s$ instead of $L$, $i_{k}$,
$i$ and $j$. As a result, we obtain%
\begin{align}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)   &  =v_{i_{0}^{\prime}}\wedge
v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{M-1}^{\prime}}\wedge v_{r}\wedge
v_{i_{M+1}^{\prime}}\wedge v_{i_{M+2}^{\prime}}\wedge...\nonumber\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge
...\label{pf.glinf.glinfact.welldef.8.pf.c.case2.1.3}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0}^{\prime}%
,i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  =\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(
i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)  \right)
.\nonumber
\end{align}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}))}}}\right)  -\underbrace{\delta
_{s,u}}_{\substack{=0\\\text{(since }s\neq u\text{)}}}F_{E_{r,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case2.1.3}))}}}-\underbrace{0F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=0}\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge
...-0=v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\left[  s\in I\right]  }_{\substack{=1\\\text{(since }s\in
I\text{)}}}\cdot\underbrace{\left[  v\in I\right]  }%
_{\substack{=1\\\text{(since }v\in I\text{)}}}\cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...,
\end{align*}
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 2.1.

Finally, let us consider Subcase 2.2. In this subcase, we don't have $s\in I$.
Recall also that $s\neq u$. Now, it is easy to see that $s\notin\left\{
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, $s\in\left\{
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}  $. Thus,
$s\in\left\{  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}
=\left\{  u\right\}  \cup\left(  I\setminus\left\{  v\right\}  \right)  $ (by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.1})). Combining this with $s\neq u$, we
obtain%
\[
s\in\left(  \left\{  u\right\}  \cup\left(  I\setminus\left\{  v\right\}
\right)  \right)  \setminus\left\{  u\right\}  =\underbrace{\left(  \left\{
u\right\}  \setminus\left\{  u\right\}  \right)  }_{=\varnothing}%
\cup\underbrace{\left(  \left(  I\setminus\left\{  v\right\}  \right)
\setminus\left\{  u\right\}  \right)  }_{\subseteq I}\subseteq\varnothing\cup
I\subseteq I.
\]
This contradicts the fact that we don't have $s\in I$. This contradiction
shows that our assumption was wrong, qed.}. Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)} (applied to $i_{k}^{\prime}$,
$r$ and $s$ instead of $i_{k}$, $i$ and $j$) yields
\begin{equation}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)  =0.
\label{pf.glinf.glinfact.welldef.8.pf.c.case2.2.1}%
\end{equation}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}))}}}\right)  -\underbrace{\delta
_{s,u}}_{\substack{=0\\\text{(since }s\neq u\text{)}}}F_{E_{r,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case2.2.1}))}}}-\underbrace{0F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=0}\\
&  =0-0=0.
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\left[  s\in I\right]  }_{\substack{=0\\\text{(since we don't
have }s\in I\text{)}}}\cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\
&  =0,
\end{align*}
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 2.2.

We have now proven Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}
in each of the Subcases 2.1 and 2.2. Since these Subcases 2.1 and 2.2 cover
the whole Case 2, this yields that Assertion \ref{prop.glinf.glinfact.welldef}%
.8 \textbf{(c)} is proven in the whole Case 2.

Altogether, we have now proven Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(c)} in each of the two Cases 1 and 2. Since these two Cases 1 and 2
cover all possibilities, this yields that Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} always holds. This completes
the proof of Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}.

Now that Assertion \ref{prop.glinf.glinfact.welldef}.8 is completely proven,
let us prove Assertion \ref{prop.glinf.glinfact.welldef}.7:

\textit{Second proof of Assertion \ref{prop.glinf.glinfact.welldef}.7: }Let us
recall that
\begin{equation}
E_{r,s}E_{u,v}=\delta_{s,u}E_{r,v}\ \ \ \ \ \ \ \ \ \ \text{for any integers
}r\text{, }s\text{, }u\text{ and }v.
\label{pf.glinf.glinfact.welldef.7.pf.Emult}%
\end{equation}
(In fact, this is a property of elementary matrices that can be directly
proven in the same way as the completely analogous property of finite-size
elementary matrices.)

Let $a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$.

Since $\left(  E_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is a
basis of the vector space $\mathfrak{gl}_{\infty}$, we can write $a$ in the
form $a=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}%
E_{i,j}$ for some family $\left(  \alpha_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$ such that $\left(  \text{all
but finitely many }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfy }%
\alpha_{i,j}=0\right)  $. Consider this family $\left(  \alpha_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$.

Since $\left(  E_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is a
basis of the vector space $\mathfrak{gl}_{\infty}$, we can write $b$ in the
form $b=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}E_{i,j}$
for some family $\left(  \beta_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$ such that $\left(  \text{all
but finitely many }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfy }%
\beta_{i,j}=0\right)  $. Consider this family $\left(  \beta_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$.

We are now going to prove that $\left[  F_{E_{r,s}},F_{E_{u,v}}\right]
=F_{\left[  E_{r,s},E_{u,v}\right]  }$ for all $\left(  r,s\right)
\in\mathbb{Z}^{2}$ and $\left(  u,v\right)  \in\mathbb{Z}^{2}$. This will
easily yield Assertion \ref{prop.glinf.glinfact.welldef}.7.

Let $\left(  r,s\right)  \in\mathbb{Z}^{2}$ and $\left(  u,v\right)
\in\mathbb{Z}^{2}$ be arbitrary. Then,%
\begin{align}
\left[  E_{r,s},E_{u,v}\right]   &  =\underbrace{E_{r,s}E_{u,v}}%
_{\substack{=\delta_{s,u}E_{r,v}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.Emult}))}}}-\underbrace{E_{u,v}E_{r,s}%
}_{\substack{=\delta_{v,r}E_{u,s}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.Emult}), applied}\\\text{to }u\text{,
}v\text{, }r\text{ and }s\text{ instead of }r\text{, }s\text{, }u\text{ and
}v\text{)}}}=\delta_{s,u}E_{r,v}-\delta_{v,r}E_{u,s}%
\label{pf.glinf.glinfact.welldef.7.pf.Elie}\\
&  =\delta_{s,u}E_{r,v}+\left(  -\delta_{v,r}\right)  E_{u,s}.\nonumber
\end{align}
Hence,%
\[
F_{\left[  E_{r,s},E_{u,v}\right]  }=F_{\delta_{s,u}E_{r,v}+\left(
-\delta_{v,r}\right)  E_{u,s}}=\delta_{s,u}F_{E_{r,v}}+\left(  -\delta
_{v,r}\right)  F_{E_{u,s}}%
\]
(since Assertion \ref{prop.glinf.glinfact.welldef}.5 (applied to $\delta
_{s,u}$, $E_{r,v}$, $-\delta_{v,r}$ and $E_{u,s}$ instead of $\lambda$, $a$,
$\mu$ and $b$) yields $\delta_{s,u}F_{E_{r,v}}+\left(  -\delta_{v,r}\right)
F_{E_{u,s}}=F_{\delta_{s,u}E_{r,v}+\left(  -\delta_{v,r}\right)  E_{u,s}}$),
so that%
\begin{align}
&  \underbrace{\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  }_{=F_{E_{r,s}}\circ
F_{E_{u,v}}-F_{E_{u,v}}\circ F_{E_{r,s}}}-\underbrace{F_{\left[
E_{r,s},E_{u,v}\right]  }}_{=\delta_{s,u}F_{E_{r,v}}+\left(  -\delta
_{v,r}\right)  F_{E_{u,s}}}\nonumber\\
&  =\left(  F_{E_{r,s}}\circ F_{E_{u,v}}-F_{E_{u,v}}\circ F_{E_{r,s}}\right)
-\left(  \delta_{s,u}F_{E_{r,v}}+\left(  -\delta_{v,r}\right)  F_{E_{u,s}%
}\right) \nonumber\\
&  =\left(  F_{E_{r,s}}\circ F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
-\underbrace{\left(  F_{E_{u,v}}\circ F_{E_{r,s}}+\left(  -\delta
_{v,r}\right)  F_{E_{u,s}}\right)  }_{=F_{E_{u,v}}\circ F_{E_{r,s}}%
-\delta_{v,r}F_{E_{u,s}}}\nonumber\\
&  =\left(  F_{E_{r,s}}\circ F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
-\left(  F_{E_{u,v}}\circ F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}}\right)  .
\label{pf.glinf.glinfact.welldef.7.pf.Elie.applied.1}%
\end{align}
Now,%
\begin{align}
&  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  -F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\underbrace{\left(  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  -F_{\left[
E_{r,s},E_{u,v}\right]  }\right)  }_{\substack{=\left(  F_{E_{r,s}}F_{E_{u,v}%
}-\delta_{s,u}F_{E_{r,v}}\right)  -\left(  F_{E_{u,v}}F_{E_{r,s}}-\delta
_{v,r}F_{E_{u,s}}\right)  \\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.Elie.applied.1}))}}}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
-\left(  F_{E_{u,v}}F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}}\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -\left(  F_{E_{u,v}}F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}%
}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.glinf.glinfact.welldef.7.pf.Elie.applied}%
\end{align}


Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an $m$-degression. We are now
going to prove the following claim:%
\begin{equation}
\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.glinf.glinfact.welldef.7.pf.claim}%
\end{equation}


\textit{Proof of (\ref{pf.glinf.glinfact.welldef.7.pf.claim}):} We distinguish
between two cases:

\textit{Case 1:} We have $s=v$.

\textit{Case 2:} We have $s\neq v$.

Let us first consider Case 1. In this case, $s=v$. Now,
(\ref{pf.glinf.glinfact.welldef.7.pf.Elie.applied}) becomes%
\begin{align*}
&  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  -F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
}_{\substack{=0\\\text{(by Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(b)}}\\\text{(since }s=v\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  F_{E_{u,v}}F_{E_{r,s}}%
-\delta_{v,r}F_{E_{u,s}}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  }_{\substack{=0\\\text{(by Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)}}\\\text{(applied to }u\text{,
}v\text{, }r\text{, }s\text{ instead of }r\text{, }s\text{, }u\text{,
}v\text{) (since }v=s\text{))}}}\\
&  =0-0=0.
\end{align*}
Hence, $\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Thus,
(\ref{pf.glinf.glinfact.welldef.7.pf.claim}) is proven in Case 1.

Now, let us consider Case 2. In this case, $s\neq v$. Thus, $v\neq s$. Let
$\mathbf{w}_{1}:\mathbb{Z}\rightarrow\mathbb{Z}$ be the function defined by
\[
\left(  \mathbf{w}_{1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\right)  .
\]
\footnote{Here, the term $\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ makes sense, since $s\neq v$.} Let $\mathbf{w}_{2}:\mathbb{Z}%
\rightarrow\mathbb{Z}$ be the function defined by
\[
\left(  \mathbf{w}_{2}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\right)  .
\]
\footnote{Here, the term $\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ makes sense, since $s\neq v$.} Clearly, $\mathbf{w}_{1}%
=\mathbf{w}_{2}$\ \ \ \ \footnote{This is because every $k\in\mathbb{Z}$
satisfies%
\begin{align*}
\mathbf{w}_{1}\left(  k\right)   &  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =\mathbf{w}_{2}\left(  k\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{w}_{2}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \right)  .
\end{align*}
}.

Now, recall that $s\neq v$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}%
.8 \textbf{(c) }(applied to $\mathbf{w}_{1}$ instead of $\mathbf{w}$) yields
that $\left(  \mathbf{w}_{1}\left(  i_{0}\right)  ,\mathbf{w}_{1}\left(
i_{1}\right)  ,\mathbf{w}_{1}\left(  i_{2}\right)  ,...\right)  $ is a
straying $m$-degression, and satisfies
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
_{1}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{1}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{1}\left(  i_{2}\right)  }\wedge....
\end{align*}


On the other hand, $v\neq s$. Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(c) }(applied to $\mathbf{w}_{2}$,
$u$, $v$, $r$, $s$ instead of $\mathbf{w}$, $r$, $s$, $u$, $v$) yields that
$\left(  \mathbf{w}_{2}\left(  i_{0}\right)  ,\mathbf{w}_{2}\left(
i_{1}\right)  ,\mathbf{w}_{2}\left(  i_{2}\right)  ,...\right)  $ is a
straying $m$-degression, and satisfies
\begin{align*}
&  \left(  F_{E_{u,v}}F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  v\in I\right]  \cdot\left[  s\in I\right]  \cdot v_{\mathbf{w}%
_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge....
\end{align*}


Now, (\ref{pf.glinf.glinfact.welldef.7.pf.Elie.applied}) becomes%
\begin{align*}
&  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  -F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=\left[
s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}_{1}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}_{1}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}_{1}\left(  i_{2}\right)  }\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  F_{E_{u,v}}F_{E_{r,s}}%
-\delta_{v,r}F_{E_{u,s}}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  }_{=\left[  v\in I\right]  \cdot\left[  s\in
I\right]  \cdot v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}%
_{2}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)
}\wedge...}\\
&  =\underbrace{\left[  s\in I\right]  \cdot\left[  v\in I\right]  }_{=\left[
v\in I\right]  \cdot\left[  s\in I\right]  }\cdot\underbrace{v_{\mathbf{w}%
_{1}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{1}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{1}\left(  i_{2}\right)  }\wedge...}%
_{\substack{=v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}%
_{2}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)
}\wedge...\\\text{(since }\mathbf{w}_{1}=\mathbf{w}_{2}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\left[  v\in I\right]  \cdot\left[  s\in I\right]
\cdot v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge...\\
&  =\left[  v\in I\right]  \cdot\left[  s\in I\right]  \cdot v_{\mathbf{w}%
_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ -\left[  v\in I\right]  \cdot\left[  s\in I\right]
\cdot v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge...\\
&  =0.
\end{align*}
Hence, $\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Thus,
(\ref{pf.glinf.glinfact.welldef.7.pf.claim}) is proven in Case 1.

Now, we have proven (\ref{pf.glinf.glinfact.welldef.7.pf.claim}) in each of
the two Cases 1 and 2. Since these two Cases cover all possibilities, this
yields that (\ref{pf.glinf.glinfact.welldef.7.pf.claim}) always holds. Thus,
(\ref{pf.glinf.glinfact.welldef.7.pf.claim}) is proven.

Now, forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We have
thus proven that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies (\ref{pf.glinf.glinfact.welldef.7.pf.claim}). Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.4 (applied to $\left[  F_{E_{r,s}%
},F_{E_{u,v}}\right]  $ and $F_{\left[  E_{r,s},E_{u,v}\right]  }$ instead of
$f$ and $g$) allows us to conclude that $\left[  F_{E_{r,s}},F_{E_{u,v}%
}\right]  =F_{\left[  E_{r,s},E_{u,v}\right]  }$.

Now, forget that we fixed $\left(  r,s\right)  $ and $\left(  u,v\right)  $.
We have thus proven that%
\begin{equation}
\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  =F_{\left[  E_{r,s},E_{u,v}\right]
}\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  r,s\right)  \in\mathbb{Z}%
^{2}\text{ and }\left(  u,v\right)  \in\mathbb{Z}^{2}.
\label{pf.glinf.glinfact.welldef.7.pf.onbasis}%
\end{equation}


Let us now notice that the map%
\begin{align*}
\mathfrak{gl}_{\infty}  &  \rightarrow\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  ,\\
c  &  \mapsto F_{c}%
\end{align*}
is $\mathbb{C}$-linear (indeed, this follows immediately from Assertion
\ref{prop.glinf.glinfact.welldef}.5). Hence, whenever $S$ is a set, $\left(
c_{s}\right)  _{s\in S}$ is a family of elements of $\mathfrak{gl}_{\infty}$,
and $\left(  \gamma_{s}\right)  _{s\in S}$ is a family of elements of
$\mathbb{C}$ such that $\left(  \text{all but finitely many }s\in S\text{
satisfy }\gamma_{s}=0\right)  $, then%
\begin{equation}
F_{\sum\limits_{s\in S}\gamma_{s}c_{s}}=\sum\limits_{s\in S}\gamma_{s}%
F_{c_{s}}. \label{pf.glinf.glinfact.welldef.7.pf.linearity}%
\end{equation}


But we have $b=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta
_{i,j}E_{i,j}$. Thus,%
\[
\left[  a,b\right]  =\left[  a,\ \sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\beta_{i,j}E_{i,j}\right]  =\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\beta_{i,j}\left[  a,E_{i,j}\right]
\]
(since the Lie bracket is $\mathbb{C}$-bilinear), so that%
\[
F_{\left[  a,b\right]  }=F_{\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}%
^{2}}\beta_{i,j}\left[  a,E_{i,j}\right]  }=\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\beta_{i,j}F_{\left[  a,E_{i,j}\right]  }%
\]
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $\left[  a,E_{i,j}\right]  $ and
$\beta_{i,j}$ in lieu of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all but
finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy $\beta_{i,j}%
=0$)). Hence,%
\begin{equation}
F_{\left[  a,b\right]  }=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}\beta_{i,j}F_{\left[  a,E_{i,j}\right]  }=\sum\limits_{\left(  u,v\right)
\in\mathbb{Z}^{2}}\beta_{u,v}F_{\left[  a,E_{u,v}\right]  }
\label{pf.glinf.glinfact.welldef.7.pf.1}%
\end{equation}
(here, we renamed the index $\left(  i,j\right)  $ as $\left(  u,v\right)  $
in the sum). On the other hand, $a=\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}E_{i,j}$. Hence, every $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfies%
\[
\left[  a,E_{u,v}\right]  =\left[  \sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}E_{i,j},E_{u,v}\right]  =\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}\left[  E_{i,j},E_{u,v}\right]
\]
(since the Lie bracket is $\mathbb{C}$-bilinear). Therefore, every $\left(
u,v\right)  \in\mathbb{Z}^{2}$ satisfies
\begin{equation}
F_{\left[  a,E_{u,v}\right]  }=F_{\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}\left[  E_{i,j},E_{u,v}\right]  }=\sum
\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[
E_{i,j},E_{u,v}\right]  } \label{pf.glinf.glinfact.welldef.7.pf.2}%
\end{equation}
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $\left[  E_{i,j},E_{u,v}\right]  $
and $\alpha_{i,j}$ in lieu of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all
but finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy
$\alpha_{i,j}=0$)). Thus, (\ref{pf.glinf.glinfact.welldef.7.pf.1}) becomes%
\begin{equation}
F_{\left[  a,b\right]  }=\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}%
}\beta_{u,v}\underbrace{F_{\left[  a,E_{u,v}\right]  }}_{=\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[  E_{i,j},E_{u,v}\right]
}}=\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[
E_{i,j},E_{u,v}\right]  }. \label{pf.glinf.glinfact.welldef.7.pf.left}%
\end{equation}


On the other hand, recall that $a=\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}E_{i,j}$, so that%
\begin{equation}
F_{a}=F_{\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha
_{i,j}E_{i,j}}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha
_{i,j}F_{E_{i,j}} \label{pf.glinf.glinfact.welldef.7.pf.3}%
\end{equation}
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $E_{i,j}$ and $\alpha_{i,j}$ in lieu
of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all but finitely many $\left(
i,j\right)  \in\mathbb{Z}^{2}$ satisfy $\alpha_{i,j}=0$)). Also,
$b=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}E_{i,j}$, so
that%
\[
F_{b}=F_{\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}%
E_{i,j}}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta
_{i,j}F_{E_{i,j}}%
\]
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $E_{i,j}$ and $\beta_{i,j}$ in lieu
of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all but finitely many $\left(
i,j\right)  \in\mathbb{Z}^{2}$ satisfy $\beta_{i,j}=0$)). Thus,%
\begin{equation}
F_{b}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}%
F_{E_{i,j}}=\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta
_{u,v}F_{E_{u,v}} \label{pf.glinf.glinfact.welldef.7.pf.4}%
\end{equation}
(here, we renamed the index $\left(  i,j\right)  $ as $\left(  u,v\right)  $
in the sum). From (\ref{pf.glinf.glinfact.welldef.7.pf.3}) and
(\ref{pf.glinf.glinfact.welldef.7.pf.4}), we obtain%
\begin{align*}
\left[  F_{a},F_{b}\right]   &  =\left[  \sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}F_{E_{i,j}},\ \sum\limits_{\left(  u,v\right)
\in\mathbb{Z}^{2}}\beta_{u,v}F_{E_{u,v}}\right] \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\underbrace{\left[  \sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
\alpha_{i,j}F_{E_{i,j}},\ F_{E_{u,v}}\right]  }_{\substack{=\sum
\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}\left[  F_{E_{i,j}%
},F_{E_{u,v}}\right]  \\\text{(since the Lie bracket is }\mathbb{C}%
\text{-bilinear)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is
}\mathbb{C}\text{-bilinear}\right) \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}%
\underbrace{\left[  F_{E_{i,j}},F_{E_{u,v}}\right]  }_{\substack{=F_{\left[
E_{i,j},E_{u,v}\right]  }\\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.onbasis}), applied to }r=i\text{ and
}s=j\text{)}}}\\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[
E_{i,j},E_{u,v}\right]  }=F_{\left[  a,b\right]  }\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.glinf.glinfact.welldef.7.pf.left})}\right)  .
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.7.
\end{verlong}

We are now ready for the endgame:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.9:} There exists
\textbf{at least} one action of the Lie algebra $\mathfrak{gl}_{\infty}$ on
the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in
\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ satisfy%
\begin{equation}
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glinf.glinfact.welldef.9}%
\end{equation}


\textit{Assertion \ref{prop.glinf.glinfact.welldef}.10:} There exists
\textbf{at most} one action of the Lie algebra $\mathfrak{gl}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in
\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ satisfy%
\begin{equation}
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glinf.glinfact.welldef.10}%
\end{equation}



\end{quote}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.9:} Let $\rho$ be
the map
\begin{align*}
\mathfrak{gl}_{\infty}  &  \rightarrow\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  ,\\
c  &  \mapsto F_{c}.
\end{align*}
This map $\rho$ is $\mathbb{C}$-linear (by Assertion
\ref{prop.glinf.glinfact.welldef}.5) and hence a Lie algebra homomorphism (by
Assertion \ref{prop.glinf.glinfact.welldef}.7). Hence, $\rho$ is an action of
the Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge
^{\dfrac{\infty}{2},m}V$. Let us write this action in infix notation (i. e.,
let us write $c\rightharpoonup w$ for $\left(  \rho\left(  c\right)  \right)
w$ whenever $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2}%
,m}V$). Then, all $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty
}{2},m}V$ satisfy%
\[
c\rightharpoonup w=\underbrace{\left(  \rho\left(  c\right)  \right)
}_{\substack{=F_{c}\\\text{(by the definition of }\rho\left(  c\right)
\text{)}}}w=F_{c}\left(  w\right)  .
\]
Hence, all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy%
\begin{align*}
&  a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...
\end{align*}
(by the definition of $F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  $). In other words, all $a\in\mathfrak{gl}_{\infty}$ and
all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.9}).

We have thus constructed an action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.9}).
Therefore, there exists \textbf{at least} one such action. This proves
Assertion \ref{prop.glinf.glinfact.welldef}.9.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.9:} Let $\rho$ be
the map%
\begin{align*}
\mathfrak{gl}_{\infty}  &  \rightarrow\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  ,\\
c  &  \mapsto F_{c}.
\end{align*}
Any $a\in\mathfrak{gl}_{\infty}$, $b\in\mathfrak{gl}_{\infty}$, $\lambda
\in\mathbb{C}$ and $\mu\in\mathbb{C}$ satisfy%
\begin{align*}
&  \lambda\underbrace{\rho\left(  a\right)  }_{\substack{=F_{a}\\\text{(by the
definition of }\rho\left(  a\right)  \text{)}}}+\mu\underbrace{\rho\left(
b\right)  }_{\substack{=F_{b}\\\text{(by the definition of }\rho\left(
b\right)  \text{)}}}\\
&  =\lambda F_{a}+\mu F_{b}=F_{\lambda a+\mu b}\ \ \ \ \ \ \ \ \ \ \left(
\text{by Assertion \ref{prop.glinf.glinfact.welldef}.5}\right) \\
&  =\rho\left(  \lambda a+\mu b\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the definition of }\rho\left(  \lambda a+\mu b\right)  \text{
yields }\rho\left(  \lambda a+\mu b\right)  =F_{\lambda a+\mu b}\right)  .
\end{align*}
In other words, the map $\rho$ is $\mathbb{C}$-linear. Moreover, any
$a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ satisfy%
\begin{align*}
&  \left[  \underbrace{\rho\left(  a\right)  }_{\substack{=F_{a}\\\text{(by
the definition of }\rho\left(  a\right)  \text{)}}},\underbrace{\rho\left(
b\right)  }_{\substack{=F_{b}\\\text{(by the definition of }\rho\left(
b\right)  \text{)}}}\right] \\
&  =\left[  F_{a},F_{b}\right]  =F_{\left[  a,b\right]  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Assertion
\ref{prop.glinf.glinfact.welldef}.7}\right) \\
&  =\rho\left(  \left[  a,b\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the definition of }\rho\left(  \left[  a,b\right]  \right)  \text{
yields }\rho\left(  \left[  a,b\right]  \right)  =F_{\left[  a,b\right]
}\right)  .
\end{align*}
Thus, $\rho$ is a Lie algebra homomorphism (since $\rho$ is $\mathbb{C}%
$-linear). As a consequence, $\rho$ is an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$.
Let us write this action in infix notation (i. e., let us write
$c\rightharpoonup w$ for $\left(  \rho\left(  c\right)  \right)  w$ whenever
$c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$). Then,
all $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$
satisfy%
\begin{equation}
c\rightharpoonup w=\underbrace{\left(  \rho\left(  c\right)  \right)
}_{\substack{=F_{c}\\\text{(by the definition of }\rho\left(  c\right)
\text{)}}}w=F_{c}\left(  w\right)  . \label{pf.glinf.glinfact.welldef.9.pf.1}%
\end{equation}
Hence, all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy%
\begin{align*}
&  a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.9.pf.1}),
applied to }c=a\text{ and }w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.defFa})}\right)  .
\end{align*}
In other words, all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.9}).

We have thus constructed an action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.9}).
Therefore, there exists \textbf{at least} one action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$
such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.9}).
This proves Assertion \ref{prop.glinf.glinfact.welldef}.9.
\end{verlong}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.10:} Given an
action of the Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space
$\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and
all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.10}), it is clear that the value of
$a\rightharpoonup w$ is uniquely determined for every $a\in\mathfrak{gl}%
_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$ (by the bilinearity of the
action, because $w$ can be written as a $\mathbb{C}$-linear combination of
elementary semiinfinite wedges $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...$). Hence, there exists \textbf{at most} one such action. This
proves Assertion \ref{prop.glinf.glinfact.welldef}.10.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.10:} Define a
linear map $\rho:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ as in the proof of Assertion
\ref{prop.glinf.glinfact.welldef}.9. (However, here we are \textbf{not} going
to write $c\rightharpoonup w$ for $\left(  \rho\left(  c\right)  \right)  w$
whenever $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$).

Let $\widetilde{\rho}:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ be an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$
such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}).
Let us write this action in infix notation (i. e., let us write
$c\rightharpoonup w$ for $\left(  \widetilde{\rho}\left(  c\right)  \right)
w$ whenever $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2}%
,m}V$). (Of course, the term $a\rightharpoonup\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ in
(\ref{pf.glinf.glinfact.welldef.10}) has to be interpreted according to this
infix notation, i. e., it has to be read as $\left(  \widetilde{\rho}\left(
a\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $, not as $\left(  \rho\left(  a\right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $.)

Let $a\in\mathfrak{gl}_{\infty}$. We know that all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}).
In other words, all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfy (\ref{pf.glinf.glinfact.welldef.10}). In other words, all
$m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\]
Thus, every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
&  \left(  \widetilde{\rho}\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.defFa}%
)}\right)  .
\end{align*}
Since $\widetilde{\rho}\left(  a\right)  $ and $F_{a}$ are two endomorphisms
of the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}V$, this shows
that $\widetilde{\rho}\left(  a\right)  =F_{a}$ (according to Assertion
\ref{prop.glinf.glinfact.welldef}.4). But $\rho\left(  a\right)  =F_{a}$ (by
the definition of $\rho\left(  a\right)  $). Hence, $\widetilde{\rho}\left(
a\right)  =\rho\left(  a\right)  $.

Now, forget that we fixed $a$. We thus have proven that $\widetilde{\rho
}\left(  a\right)  =\rho\left(  a\right)  $ for every $a\in\mathfrak{gl}%
_{\infty}$. In other words, $\widetilde{\rho}=\rho$.

Now, forget that we fixed $\widetilde{\rho}$. We have thus proven that
whenever $\widetilde{\rho}$ is an action of the Lie algebra $\mathfrak{gl}%
_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}), then
we must have $\widetilde{\rho}=\rho$. In other words, every action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.10}) must be equal to $\rho$. Hence, there
exists \textbf{at most} one action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}). This
proves Assertion \ref{prop.glinf.glinfact.welldef}.10.
\end{verlong}

\begin{vershort}
Combining Assertion \ref{prop.glinf.glinfact.welldef}.9 with Assertion
\ref{prop.glinf.glinfact.welldef}.10, we see that there exists \textbf{one and
only one} action of the Lie algebra $\mathfrak{gl}_{\infty}$ on the vector
space $\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in\mathfrak{gl}%
_{\infty}$ and all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
In other words, Proposition \ref{prop.glinf.glinfact.welldef} is proven.
\end{vershort}

\begin{verlong}
We now know that there exists \textbf{one and only one} action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
(because Assertion \ref{prop.glinf.glinfact.welldef}.9 shows that there exists
\textbf{at least} one such action, and because Assertion
\ref{prop.glinf.glinfact.welldef}.10 shows that there exists \textbf{at most}
one such action). Hence, Definition \ref{def.glinf.semiinfwedge} really
defines a representation of the Lie algebra $\mathfrak{gl}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$. Proposition
\ref{prop.glinf.glinfact.welldef} is thus proven.
\end{verlong}

\begin{vershort}
\textit{Proof of Proposition \ref{prop.glinf.glinfact} and Proposition
\ref{prop.glinf.explicit}.} Both Proposition \ref{prop.glinf.glinfact} and
Proposition \ref{prop.glinf.explicit} boil down to facts that have been proven
during our proof of Proposition \ref{prop.glinf.glinfact.welldef} (indeed,
Proposition \ref{prop.glinf.glinfact} boils down to Assertion
\ref{prop.glinf.glinfact.welldef}.2, and Proposition \ref{prop.glinf.explicit}
to parts \textbf{(b)} and \textbf{(c)} of Assertion
\ref{prop.glinf.glinfact.welldef}.6).
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.glinf.glinfact}.} Let $\rho
:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(  \wedge^{\dfrac{\infty
}{2},m}V\right)  $ be the action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ defined in Definition
\ref{def.glinf.semiinfwedge}. Then,%
\begin{equation}
c\rightharpoonup w=\left(  \rho\left(  c\right)  \right)
w\ \ \ \ \ \ \ \ \ \ \text{for every }c\in\mathfrak{gl}_{\infty}\text{ and
}w\in\wedge^{\dfrac{\infty}{2},m}V. \label{pf.glinf.glinfact.taut}%
\end{equation}


For every $a\in\mathfrak{gl}_{\infty}$, define the map $F_{a}:\wedge
^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ as in the
proof of Proposition \ref{prop.glinf.glinfact.welldef}.

Let $a\in\mathfrak{gl}_{\infty}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $
be an $m$-degression. Then, applying (\ref{pf.glinf.glinfact.taut}) to $c=a$
and $w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, we obtain
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\]
so that%
\begin{align*}
&  \left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.defFa}%
)}\right)  .
\end{align*}


Now, forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We thus
have shown that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies $\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{a}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Hence, applying Assertion
\ref{prop.glinf.glinfact.welldef}.4 to $\rho\left(  a\right)  $ and $F_{a}$
instead of $f$ and $g$, we obtain $\rho\left(  a\right)  =F_{a}$ (since
$\rho\left(  a\right)  $ and $F_{a}$ are two endomorphisms of the $\mathbb{C}%
$-vector space $\wedge^{\dfrac{\infty}{2},m}V$).

Now, applying (\ref{pf.glinf.glinfact.taut}) to $c=a$ and $w=b_{0}\wedge
b_{1}\wedge b_{2}\wedge...$, we obtain%
\begin{align*}
&  a\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =\underbrace{\left(  \rho\left(  a\right)  \right)  }_{=F_{a}}\left(
b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{due to the Assertion
\ref{prop.glinf.glinfact.welldef}.2 in the proof of Proposition
\ref{prop.glinf.glinfact.welldef}}\right)  .
\end{align*}
This proves Proposition \ref{prop.glinf.glinfact}.

\textit{Proof of Proposition \ref{prop.glinf.explicit}.} Let $\rho
:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(  \wedge^{\dfrac{\infty
}{2},m}V\right)  $ be the action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ defined in Definition
\ref{def.glinf.semiinfwedge}. Then,%
\begin{equation}
c\rightharpoonup w=\left(  \rho\left(  c\right)  \right)
w\ \ \ \ \ \ \ \ \ \ \text{for every }c\in\mathfrak{gl}_{\infty}\text{ and
}w\in\wedge^{\dfrac{\infty}{2},m}V. \label{pf.glinf.explicit.taut}%
\end{equation}


For every $a\in\mathfrak{gl}_{\infty}$, define the map $F_{a}:\wedge
^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ as in the
proof of Proposition \ref{prop.glinf.glinfact.welldef}.

Let $a\in\mathfrak{gl}_{\infty}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $
be an $m$-degression. Then, applying (\ref{pf.glinf.glinfact.taut}) to $c=a$
and $w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, we obtain
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\]
so that%
\begin{align*}
&  \left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.defFa}%
)}\right)  .
\end{align*}


Now, forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We thus
have shown that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies $\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{a}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Hence, applying Assertion
\ref{prop.glinf.glinfact.welldef}.4 to $\rho\left(  a\right)  $ and $F_{a}$
instead of $f$ and $g$, we obtain $\rho\left(  a\right)  =F_{a}$ (since
$\rho\left(  a\right)  $ and $F_{a}$ are two endomorphisms of the $\mathbb{C}%
$-vector space $\wedge^{\dfrac{\infty}{2},m}V$).

Now, forget that we fixed $a$. We thus have shown that
\[
\rho\left(  a\right)  =F_{a}\ \ \ \ \ \ \ \ \ \ \text{for every }%
a\in\mathfrak{gl}_{\infty}.
\]
Now, every $a\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2}%
,m}V$ satisfy%
\begin{align}
a\rightharpoonup w  &  =\underbrace{\left(  \rho\left(  a\right)  \right)
}_{=F_{a}}w\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.explicit.taut}%
), applied to }c=a\right) \nonumber\\
&  =F_{a}\left(  w\right)  . \label{pf.glinf.explicit.rewriter}%
\end{align}


\textbf{(a)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
\begin{align*}
&  E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.explicit.rewriter}), applied to }a=E_{i,j}\text{ and
}w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{due to the Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)} in the proof of Proposition
\ref{prop.glinf.glinfact.welldef}}\right)  .
\end{align*}
This proves Proposition \ref{prop.glinf.explicit} \textbf{(a)}.

\textbf{(b)} Assume that there exists a \textbf{unique} $\ell\in\mathbb{N}$
such that $j=i_{\ell}$. Then, for this $\ell$, we have%
\begin{align*}
&  E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.explicit.rewriter}), applied to }a=E_{i,j}\text{ and
}w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{i}\wedge
v_{i_{\ell+1}}\wedge v_{i_{\ell+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{due to the Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} in
the proof of Proposition \ref{prop.glinf.glinfact.welldef}}\\
\text{(applied to }L=\ell\text{)}%
\end{array}
\right)  .
\end{align*}
This proves Proposition \ref{prop.glinf.explicit} \textbf{(b)}.

Proposition \ref{prop.glinf.explicit} is thus completely proven.
\end{verlong}

\subsubsection{Properties of \texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$}
{the semi-infinite wedge space}}

There is an easy way to define a grading on $\wedge^{\dfrac{\infty}{2},m}V$.
To do it, we notice that:

\begin{proposition}
\label{prop.glinf.wedge.grading}For every $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $, the sequence $\left(  i_{k}+k-m\right)  _{k\geq0}$
is a partition (i. e., a nonincreasing sequence of nonnegative integers such
that all but finitely many of its elements are $0$). In particular, every
integer $k\geq0$ satisfies $i_{k}+k-m\geq0$, and only finitely many integers
$k\geq0$ satisfy $i_{k}+k-m\neq0$. Hence, the sum $\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  $ is well-defined and equals a nonnegative integer.
\end{proposition}

The proof of this is very easy and left to the reader. As a consequence of
this proposition, we have:

\begin{definition}
\label{def.glinf.wedge.grading}Let $m\in\mathbb{Z}$. We define a grading on
the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}V$ by setting%
\begin{align*}
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  d\right]   &
=\left\langle v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid
\ \left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression
}\phantom{\sum\limits_{k}}\right. \\
&
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.
\text{satisfying }\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)
=-d\right\rangle \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for every }d\in\mathbb{Z}\right.  .
\end{align*}
In other words, we define a grading on the $\mathbb{C}$-vector space
$\wedge^{\dfrac{\infty}{2},m}V$ by setting%
\[
\deg\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=-\sum\limits_{k}\left(  i_{k}+k-m\right)
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.

This grading satisfies $\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\leq
0}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  d\right]  $ (since
Proposition \ref{prop.glinf.wedge.grading} yields that $\sum\limits_{k\geq
0}\left(  i_{k}+k-m\right)  $ is nonnegative for every $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $). In other words, $\wedge^{\dfrac{\infty}%
{2},m}V$ is nonpositively graded.
\end{definition}

Note that, for every given $m\in\mathbb{Z}$, the $m$-degressions are in a
1-to-1 correspondence with the partitions. This correspondence maps any
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ to the sequence
$\left(  i_{k}+k-m\right)  _{k\geq0}$ (this sequence is a partition due to
Proposition \ref{prop.glinf.wedge.grading}). The degree $\deg\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ of the semiinfinite wedge
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ equals minus the sum of
the parts of this partition.

It is easy to check that:

\begin{proposition}
Let $m\in\mathbb{Z}$. With the grading defined in Definition
\ref{def.glinf.wedge.grading}, the $\mathfrak{gl}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2},m}V$ is graded (where the grading on $\mathfrak{gl}%
_{\infty}$ is the one from Definition \ref{def.glinf.grade}).
\end{proposition}

Let us say more about this module:

\begin{proposition}
\label{prop.Lomegam}Let $m\in\mathbb{Z}$. The graded $\mathfrak{gl}_{\infty}%
$-module $\wedge^{\dfrac{\infty}{2},m}V$ is the irreducible highest-weight
representation $L_{\omega_{m}}$ of $\mathfrak{gl}_{\infty}$ with highest
weight $\omega_{m}=\left(  ...,1,1,0,0,...\right)  $, where the last $1$ is on
place $m$ and the first $0$ is on place $m+1$. Moreover, $L_{\omega_{m}}$ is unitary.
\end{proposition}

Before we prove this, let us define the vectors that will turn out to be the
highest-weight vectors:

\begin{definition}
\label{def.psim}For every $m\in\mathbb{Z}$, we denote by $\psi_{m}$ the vector
$v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V$.
(This is well-defined since the infinite sequence $\left(
m,m-1,m-2,...\right)  $ is an $m$-degression.)

(Let us repeat that we are no longer using the notations of Definition
\ref{def.Cdelta}, so that this $\psi_{m}$ has nothing to do with the $\psi
_{j}$ from Definition \ref{def.Cdelta}.)

Note that $\psi_{m}\in\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
0\right]  $ by the definition of the grading on $\wedge^{\dfrac{\infty}{2}%
,m}V$.
\end{definition}

\textit{Proof of Proposition \ref{prop.Lomegam}.} It is easy to see that
$\mathfrak{n}_{+}\cdot\psi_{m}=0$. (In fact, if $E_{i,j}\in\mathfrak{n}_{+}$
then $i<j$ and thus indices are replaced by smaller indices when computing
$E_{i,j}\rightharpoonup\psi_{m}$... For an alternative proof, just use the
fact that $\psi_{m}\in\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
0\right]  $ and that $\wedge^{\dfrac{\infty}{2},m}V$ is concentrated in
nonpositive degrees.) Moreover, every $h\in\mathfrak{h}$ satisfies $h\psi
_{m}=\omega_{m}\left(  h\right)  \psi_{m}$ (in fact, test at $h=E_{i,i}$).
Also, $\psi_{m}$ generates the $\mathfrak{gl}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$. Thus, $\wedge^{\dfrac{\infty}{2},m}V$ is a
highest-weight representation with highest weight $\omega_{m}$ (and
highest-weight vector $\psi_{m}$).

Next let us prove that it is unitary. This will yield that it is
irreducible.\footnote{We could also show the irreducibility more directly, by
showing that every sum of wedges can be used to get back $\psi_{m}$.}

The unitarity is because the form in which the wedges are orthonormal is
$\dag$-invariant. Thus, irreducible. (We used Lemma \ref{lem.unitrick}.)
Proposition \ref{prop.Lomegam} is proven.

\begin{corollary}
\label{cor.lomegam.unit}For every finite sum $\sum\limits_{i\in\mathbb{Z}%
}k_{i}\omega_{i}$ with $k_{i}\in\mathbb{N}$, the representation $L_{\sum
\limits_{i\in\mathbb{Z}}k_{i}\omega_{i}}$ is unitary.
\end{corollary}

\textit{Proof.} Take the module $\bigotimes\limits_{i}L_{\omega_{i}}^{\otimes
k_{i}}$, and let $v$ be the tensor product of their respective highest-weight
vectors. Let $L$ be the submodule generated by $v$. Then, $L$ is a
highest-weight module, and is unitary since it is a submodule of a unitary
module. Hence it is irreducible, and thus $L\cong L_{\sum\limits_{i}%
k_{i}\omega_{i}}$, qed.

\subsection{\texorpdfstring{$\overline{\mathfrak{a}_{\infty}}$}
{a-infinity-bar}}

The Lie algebra $\mathfrak{gl}_{\infty}$ is fairly small (it doesn't even
contain the identity matrix) - too small for several applications. Here is a
larger Lie algebra with roughly similar properties:

\begin{definition}
We define $\overline{\mathfrak{a}_{\infty}}$ to be the vector space of
infinite matrices with rows and columns labeled by integers (not only positive
integers) such that only finitely many \textbf{diagonals} are nonzero. This is
an associative algebra with $1$ (due to Remark \ref{rmk.ainf.mult}
\textbf{(a)} below), and thus, by the commutator, a Lie algebra.
\end{definition}

We can think of the elements of $\overline{\mathfrak{a}_{\infty}}$ as
difference operators:

Consider $V$ as the space of sequences\footnote{In the following,
``sequences'' means ``sequences labeled by integers''.} with finitely many
nonzero entries. One very important endomorphism of $V$ is defined as follows:

\begin{definition}
\label{def.shiftoperator}Let $T:V\rightarrow V$ be the linear map given by
\[
\left(  Tx\right)  _{n}=x_{n+1}\ \ \ \ \ \ \ \ \ \ \text{for all }x\in V\text{
and }n\in\mathbb{Z}.
\]
This map $T$ is called the \textit{shift operator}. It satisfies
$Tv_{i+1}=v_{i}$ for every $i\in\mathbb{Z}$.

We can also write $T$ in the form $T=\sum\limits_{i\in\mathbb{Z}}E_{i,i+1}$,
where the sum is infinite but makes sense entrywise (i. e., for every $\left(
a,b\right)  \in\mathbb{Z}^{2}$, there are only finitely many $i\in\mathbb{Z}$
for which the matrix $E_{i,i+1}$ has nonzero $\left(  a,b\right)  $-th entry).
\end{definition}

Note that:

\begin{proposition}
\label{prop.shiftoperator.Tj}The shift operator $T$ is invertible. Every
$j\in\mathbb{Z}$ satisfies $T^{j}=\sum\limits_{i\in\mathbb{Z}}E_{i,i+j}$.
\end{proposition}

A \textit{difference operator} is an operator of the form $A=\sum
\limits_{i=p}^{q}\gamma_{i}\left(  n\right)  T^{i}$, where $p$ and $q$ are
some integers, and $\gamma_{i}:\mathbb{Z}\rightarrow\mathbb{C}$ are some
functions.\footnote{The sum $\sum\limits_{i=p}^{q}\gamma_{i}\left(  n\right)
T^{i}$ has to be understood as the linear map $X:V\rightarrow V$ given by%
\[
\left(  Xx\right)  _{n}=\sum\limits_{i=p}^{q}\gamma_{i}\left(  n\right)
x_{n+i}\ \ \ \ \ \ \ \ \ \ \text{for all }x\in V\text{ and }n\in\mathbb{Z}.
\]
} Then, $\overline{\mathfrak{a}_{\infty}}$ is the algebra of all such
operators. (These operators also act on the space of \textit{all} sequences,
not only on the space of sequences with finitely many nonzero entries.) In
particular, $T\in\overline{\mathfrak{a}_{\infty}}$, and $T^{i}\in
\overline{\mathfrak{a}_{\infty}}$ for every $i\in\mathbb{Z}$.

Note that $\overline{\mathfrak{a}_{\infty}}$ is no longer countably
dimensional. The family $\left(  E_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}$ is no longer a vector space basis, but it is a topological
basis in an appropriately defined topology.

Let us make a remark on multiplication of infinite matrices:

\begin{remark}
\label{rmk.ainf.mult}\textbf{(a)} For every $A\in\overline{\mathfrak{a}%
_{\infty}}$ and $B\in\overline{\mathfrak{a}_{\infty}}$, the matrix $AB$ is
well-defined and lies in $\overline{\mathfrak{a}_{\infty}}$.

\textbf{(b)} For every $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\mathfrak{gl}_{\infty}$, the matrix $AB$ is well-defined and lies in
$\mathfrak{gl}_{\infty}$.
\end{remark}

\textit{Proof of Remark \ref{rmk.ainf.mult}.} \textbf{(a)} Let $A\in
\overline{\mathfrak{a}_{\infty}}$ and $B\in\overline{\mathfrak{a}_{\infty}}$.
Write the matrix $A$ in the form $\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$, and write the matrix $B$ in the form $\left(
b_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$.

Since $A\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of
$A$ are nonzero. Hence, there exists a finite subset $\mathfrak{A}$ of
$\mathbb{Z}$ such that%
\begin{equation}
\text{for every }u\in\mathbb{Z}\diagdown\mathfrak{A}\text{, the }u\text{-th
diagonal of }A\text{ is zero.} \label{pf.ainf.mult.1}%
\end{equation}
Consider this $\mathfrak{A}$.

Since $B\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of
$B$ are nonzero. Hence, there exists a finite subset $\mathfrak{B}$ of
$\mathbb{Z}$ such that%
\begin{equation}
\text{for every }v\in\mathbb{Z}\diagdown\mathfrak{B}\text{, the }v\text{-th
diagonal of }B\text{ is zero.} \label{pf.ainf.mult.2}%
\end{equation}
Consider this $\mathfrak{B}$.

For every $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$, the infinite sum
$\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$ has a well-defined value, because
all but finitely many addends of this sum are zero\footnote{\textit{Proof.}
Every $k\in\mathbb{Z}$ such that $k-i\notin\mathfrak{A}$ satisfies $a_{i,k}=0$
(because $k-i\notin\mathfrak{A}$, so that $k-i\in\mathbb{Z}\diagdown
\mathfrak{A}$, and thus (\ref{pf.ainf.mult.1}) (applied to $u=k-i$) yields
that the $\left(  k-i\right)  $-th diagonal of $A$ is zero, and thus $a_{i,k}$
(being an entry in this diagonal) must be $=0$). Hence, every $k\in\mathbb{Z}$
such that $k-i\notin\mathfrak{A}$ satisfies $a_{i,k}b_{k,j}=0b_{k,j}=0$. Since
$\mathfrak{A}$ is a finite set, all but finitely many $k\in\mathbb{Z}$ satisfy
$k-i\notin\mathfrak{A}$, and thus all but finitely many $k\in\mathbb{Z}$
satisfy $a_{i,k}b_{k,j}=0$ (because every $k\in\mathbb{Z}$ such that
$k-i\notin\mathfrak{A}$ satisfies $a_{i,k}b_{k,j}=0$). In other words, all but
finitely many addends of the sum $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$
are zero, qed.}. Hence, the matrix $AB$ is well-defined (because the matrix
$AB$ is defined as the matrix whose $\left(  i,j\right)  $-th entry is
$\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$ for all $\left(  i,j\right)
\in\mathbb{Z}^{2}$), and satisfies%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}%
\]
for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$.

Now we must show that $AB\in\overline{\mathfrak{a}_{\infty}}$.

Let $\mathfrak{A}+\mathfrak{B}$ denote the set $\left\{  a+b\ \mid\ \left(
a,b\right)  \in\mathfrak{A}\times\mathfrak{B}\right\}  $. Clearly,
$\mathfrak{A}+\mathfrak{B}$ is a finite set (since $\mathfrak{A}$ and
$\mathfrak{B}$ are finite). Now, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $j-i\notin\mathfrak{A}+\mathfrak{B}$, every $k\in\mathbb{Z}$
satisfies $a_{i,k}b_{k,j}=0$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $j-i\notin\mathfrak{A}%
+\mathfrak{B}$, and let $k\in\mathbb{Z}$. Assume that $a_{i,k}b_{k,j}\neq0$.
Then, $a_{i,k}\neq0$ and $b_{k,j}\neq0$.
\par
Since $a_{i,k}$ is an entry of the $\left(  k-i\right)  $-th diagonal of $A$,
we see that some entry of the $\left(  k-i\right)  $-th diagonal of $A$ is
nonzero (since $a_{i,k}\neq0$). Hence, the $\left(  k-i\right)  $-th diagonal
of $A$ is nonzero. Thus, $k-i\notin\mathbb{Z}\diagdown\mathfrak{A}$ (because
otherwise, we would have $k-i\in\mathbb{Z}\diagdown\mathfrak{A}$, so that
(\ref{pf.ainf.mult.1}) (applied to $u=k-i$) would yield that the $\left(
k-i\right)  $-th diagonal of $A$ is zero, contradicting the fact that it is
nonzero), so that $k-i\in\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $\left(  j-k\right)  $-th diagonal of $B$,
we see that some entry of the $\left(  j-k\right)  $-th diagonal of $B$ is
nonzero (since $b_{k,j}\neq0$). Hence, the $\left(  j-k\right)  $-th diagonal
of $B$ is nonzero. Thus, $j-k\notin\mathbb{Z}\diagdown\mathfrak{B}$ (because
otherwise, we would have $j-k\in\mathbb{Z}\diagdown\mathfrak{B}$, so that
(\ref{pf.ainf.mult.2}) (applied to $v=j-k$) would yield that the $\left(
j-k\right)  $-th diagonal of $B$ is zero, contradicting the fact that it is
nonzero), so that $j-k\in\mathfrak{B}$.
\par
Now, $j-i=\underbrace{\left(  k-i\right)  }_{\in\mathfrak{A}}%
+\underbrace{\left(  j-k\right)  }_{\in\mathfrak{B}}\in\mathfrak{A}%
+\mathfrak{B}$. This contradicts $j-i\notin\mathfrak{A}+\mathfrak{B}$. Thus,
our assumption that $a_{i,k}b_{k,j}\neq0$ must have been wrong. Hence,
$a_{i,k}b_{k,j}=0$, qed.}. Thus, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $j-i\notin\mathfrak{A}+\mathfrak{B}$, we have%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}b_{k,j}}%
_{\substack{=0\\\text{(since }j-i\notin\mathfrak{A}+\mathfrak{B}\text{)}%
}}=\sum\limits_{k\in\mathbb{Z}}0=0.
\]
Thus, for every integer $w\notin\mathfrak{A}+\mathfrak{B}$, and any
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfying $j-i=w$, we have $\left(
\left(  i,j\right)  \text{-th entry of the matrix }AB\right)  =0$ (since
$j-i=w\notin\mathfrak{A}+\mathfrak{B}$). In other words, for every integer
$w\notin\mathfrak{A}+\mathfrak{B}$, the $w$-th diagonal of $AB$ is zero. Since
$\mathfrak{A}+\mathfrak{B}$ is a finite set, this yields that all but finitely
many diagonals of $AB$ are zero. In other words, only finitely many diagonals
of $AB$ are nonzero. In other words, $AB\in\overline{\mathfrak{a}_{\infty}}$.
This proves Remark \ref{rmk.ainf.mult} \textbf{(a)}.

\textbf{(b)} We know from Remark \ref{rmk.ainf.mult} \textbf{(a)} that the
matrix $AB$ is well-defined (since $B\in\mathfrak{gl}_{\infty}\subseteq
\overline{\mathfrak{a}_{\infty}}$).

The matrix $B$ lies in $\mathfrak{gl}_{\infty}$ and thus has only finitely
many nonzero entries. Hence, $B$ has only finitely many nonzero rows. In other
words, there exists a finite subset $\mathfrak{R}$ of $\mathbb{Z}$ such that%
\begin{equation}
\text{for every }x\in\mathbb{Z}\diagdown\mathfrak{R}\text{, the }x\text{-th
row of }B\text{ is zero.} \label{pf.ainf.mult.3}%
\end{equation}


Also, $B$ has only finitely many nonzero entries, and thus only finitely many
nonzero columns. In other words, there exists a finite subset $\mathfrak{C}$
of $\mathbb{Z}$ such that%
\begin{equation}
\text{for every }y\in\mathbb{Z}\diagdown\mathfrak{C}\text{, the }y\text{-th
column of }B\text{ is zero.} \label{pf.ainf.mult.4}%
\end{equation}


Define $\mathfrak{A}$ as in the proof of Remark \ref{rmk.ainf.mult}
\textbf{(a)}. Let $\mathfrak{R}-\mathfrak{A}$ denote the set $\left\{
r-a\ \mid\ \left(  r,a\right)  \in\mathfrak{R}\times\mathfrak{A}\right\}  $.
Clearly, $\mathfrak{R}-\mathfrak{A}$ is a finite set (since $\mathfrak{A}$ and
$\mathfrak{R}$ are finite), and thus $\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$ is a finite set (since $\mathfrak{C}$, too, is
finite). Now, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfying
$\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}\right)
\times\mathfrak{C}$, we have $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}%
=0$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
be such that $\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$. Assume that $\sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}\neq0$. Then, there exists some $k\in\mathbb{Z}$ such that
$a_{i,k}b_{k,j}\neq0$. Consider this $k$.
\par
Since $a_{i,k}b_{k,j}\neq0$, we have $a_{i,k}\neq0$ and $b_{k,j}\neq0$.
\par
Since $a_{i,k}$ is an entry of the $\left(  k-i\right)  $-th diagonal of $A$,
we see that some entry of the $\left(  k-i\right)  $-th diagonal of $A$ is
nonzero (since $a_{i,k}\neq0$). Hence, the $\left(  k-i\right)  $-th diagonal
of $A$ is nonzero. Thus, $k-i\notin\mathbb{Z}\diagdown\mathfrak{A}$ (because
otherwise, we would have $k-i\in\mathbb{Z}\diagdown\mathfrak{A}$, so that
(\ref{pf.ainf.mult.1}) (applied to $u=k-i$) would yield that the $\left(
k-i\right)  $-th diagonal of $A$ is zero, contradicting the fact that it is
nonzero), so that $k-i\in\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $k$-th row of $B$, we see that some entry
of the $k$-th row of $B$ is nonzero (since $b_{k,j}\neq0$). Hence, the $k$-th
row of $B$ is nonzero. Thus, $k\notin\mathbb{Z}\diagdown\mathfrak{R}$ (because
otherwise, we would have $k\in\mathbb{Z}\diagdown\mathfrak{R}$, so that
(\ref{pf.ainf.mult.3}) (applied to $x=k$) would yield that the $k$-th row of
$B$ is zero, contradicting the fact that it is nonzero), so that
$k\in\mathfrak{R}$.
\par
Thus, $i=\underbrace{k}_{\in\mathfrak{R}}-\underbrace{\left(  k-i\right)
}_{\in\mathfrak{A}}\in\mathfrak{R}-\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $j$-th column of $B$, we see that some
entry of the $j$-th column of $B$ is nonzero (since $b_{k,j}\neq0$). Hence,
the $j$-th column of $B$ is nonzero. Thus, $j\notin\mathbb{Z}\diagdown
\mathfrak{C}$ (because otherwise, we would have $j\in\mathbb{Z}\diagdown
\mathfrak{C}$, so that (\ref{pf.ainf.mult.4}) (applied to $y=j$) would yield
that the $j$-th column of $B$ is zero, contradicting the fact that it is
nonzero), so that $j\in\mathfrak{C}$. Combined with $i\in\mathfrak{R}%
-\mathfrak{A}$, this yields $\left(  i,j\right)  \in\left(  \mathfrak{R}%
-\mathfrak{A}\right)  \times\mathfrak{C}$, contradicting $\left(  i,j\right)
\notin\left(  \mathfrak{R}-\mathfrak{A}\right)  \times\mathfrak{C}$. Hence,
the assumption that $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}\neq0$ must
have been wrong. In other words, $\sum\limits_{k\in\mathbb{Z}}a_{i,k}%
b_{k,j}=0$, qed.}. Hence, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$, we have%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}=0.
\]
Since $\left(  \mathfrak{R}-\mathfrak{A}\right)  \times\mathfrak{C}$ is a
finite set, this yields that all but finitely many entries of the matrix $AB$
are zero. In other words, $AB$ has only finitely many nonzero entries. Thus,
$AB\in\mathfrak{gl}_{\infty}$. Remark \ref{rmk.ainf.mult} \textbf{(b)} is proven.

Let us make $\overline{\mathfrak{a}_{\infty}}$ into a graded Lie algebra:

\begin{definition}
\label{def.ainf.grade}For every $i\in\mathbb{Z}$, let $\overline
{\mathfrak{a}_{\infty}^{i}}$ be the subspace of $\overline{\mathfrak{a}%
_{\infty}}$ which consists of matrices which have nonzero entries only on the
$i$-th diagonal. (The $i$\textit{-th diagonal} consists of the entries in the
$\left(  \alpha,\beta\right)  $-th places with $\beta-\alpha=i$.)

Then, $\overline{\mathfrak{a}_{\infty}}=\bigoplus\limits_{i\in\mathbb{Z}%
}\overline{\mathfrak{a}_{\infty}^{i}}$, and this makes $\overline
{\mathfrak{a}_{\infty}}$ into a $\mathbb{Z}$-graded Lie algebra. Note that
$\overline{\mathfrak{a}_{\infty}^{0}}$ is abelian. Let $\overline
{\mathfrak{a}_{\infty}}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\overline{\mathfrak{a}%
_{\infty}}$, so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}%
\overline{\mathfrak{a}_{\infty}^{i}}$ is the space of all strictly
lower-triangular matrices in $\overline{\mathfrak{a}_{\infty}}$, the subspace
$\mathfrak{h}=\overline{\mathfrak{a}_{\infty}^{0}}$ is the space of all
diagonal matrices in $\overline{\mathfrak{a}_{\infty}}$, and the subspace
$\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\overline{\mathfrak{a}_{\infty}^{i}}$
is the space of all strictly upper-triangular matrices in $\overline
{\mathfrak{a}_{\infty}}$.
\end{definition}

Note that this was completely analogous to Definition \ref{def.glinf.grade}.

\subsection{\texorpdfstring{$\mathfrak{a}_{\infty}$}{a-infinity} and its
action on \texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$}{the
semi-infinite wedge space}}

\begin{definition}
\label{def.glinf.rho}Let $m\in\mathbb{Z}$. Let $\rho:\mathfrak{gl}_{\infty
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
be the representation of $\mathfrak{gl}_{\infty}$ on $\wedge^{\dfrac{\infty
}{2},m}V$ defined in Definition \ref{def.glinf.semiinfwedge}.
\end{definition}

The following question poses itself naturally now: Can we extend this
representation $\rho$ to a representation of $\overline{\mathfrak{a}_{\infty}%
}$ in a reasonable way?

This question depends on what we mean by \textquotedblleft
reasonable\textquotedblright. One way to concretize this is by noticing that
$\overline{\mathfrak{a}_{\infty}}=\bigoplus\limits_{i\in\mathbb{Z}}%
\overline{\mathfrak{a}_{\infty}^{i}}$, where $\overline{\mathfrak{a}_{\infty
}^{i}}$ is the space of all matrices with nonzero entries only on the $i$-th
diagonal. For each $i\in\mathbb{Z}$, the vector space $\overline
{\mathfrak{a}_{\infty}^{i}}$ can be given the product topology (i. e., the
topology in which a net $\left(  s_{z}\right)  _{z\in Z}$ of matrices
converges to a matrix $s$ if and only if for any $\left(  m,n\right)
\in\mathbb{Z}^{2}$ satisfying $n-m=i$, the net of the $\left(  m,n\right)
$-th entries of the matrices $s_{z}$ converge to the $\left(  m,n\right)  $-th
entry of $s$ in the discrete topology). Then, $\mathfrak{gl}_{\infty}^{i}$ in
dense in $\overline{\mathfrak{a}_{\infty}^{i}}$ for every $i\in\mathbb{Z}$. We
can also make $\wedge^{\dfrac{\infty}{2},m}V$ into a topological space by
using the discrete topology. Our question can now be stated as follows: Can we
extend $\rho$ by continuity to a representation of $\overline{\mathfrak{a}%
_{\infty}}$ (where \textquotedblleft continuous\textquotedblright\ means
\textquotedblleft continuous on each $\overline{\mathfrak{a}_{\infty}^{i}}%
$\textquotedblright, since we have not defined a topology on the whole space
$\overline{\mathfrak{a}_{\infty}}$) ?

Answer: Almost, but not precisely. We cannot make $\overline{\mathfrak{a}%
_{\infty}}$ act on $\wedge^{\dfrac{\infty}{2},m}V$ in such a way that its
action extends $\rho$ continuously, but we can make a central extension of
$\overline{\mathfrak{a}_{\infty}}$ act on $\wedge^{\dfrac{\infty}{2},m}V$ in a
way that only slightly differs from $\rho$.

Let us first see what goes wrong if we try to find an extension of $\rho$ to
$\overline{\mathfrak{a}_{\infty}}$ by continuity:

For $i\neq0$, a typical element $X\in\overline{\mathfrak{a}_{\infty}^{i}}$ is
of the form $X=\sum\limits_{j\in\mathbb{Z}}z_{j}E_{j,j+i}$ with $z_{j}%
\in\mathbb{C}$. Now we can define $\rho\left(  X\right)  v=\sum\limits_{j\in
\mathbb{Z}}z_{j}\rho\left(  E_{j,j+i}\right)  v$ for every $v\in\wedge
^{\dfrac{\infty}{2},m}V$; this sum has only finitely many nonzero
addends\footnote{\textit{Proof.} We must prove that, for every $v\in
\wedge^{\dfrac{\infty}{2},m}V$, the sum $\sum\limits_{j\in\mathbb{Z}}z_{j}%
\rho\left(  E_{j,j+i}\right)  v$ has only finitely many nonzero addends. It is
clearly enough to prove this in the case when $v$ is an elementary
semiinfinite wedge. So let us WLOG assume that $v$ is an elementary
semiinfinite wedge. In other words, WLOG assume that $v=v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ for some $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $. Consider this $m$-degression. By the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every integer $k\geq K$ satisfies $i_{k}+k=m$. Consider this $K$. Then,
every integer $j\leq i_{K}$ appears in the $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $.
\par
Now, we have the following two observations:
\par
\begin{itemize}
\item Every integer $j>i_{0}-i$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$
(because for every integer $j>i_{0}-i$, we have $j+i>i_{0}$, so that the
integer $j+i$ does not appear in the $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $).
\par
\item Every integer $j\leq i_{K}$ satisfies $\rho\left(  E_{j,j+i}\right)
v=0$ (because every integer $j\leq i_{K}$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, and because $i\neq0$).
\end{itemize}
\par
Combining these two observations, we conclude that every sufficiently large
integer $j$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$ and that every
sufficiently small integer $j$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$.
Hence, only finitely many integers $j$ satisfy $\rho\left(  E_{j,j+i}\right)
v\neq0$. Thus, the sum $\sum\limits_{j\in\mathbb{Z}}z_{j}\rho\left(
E_{j,j+i}\right)  v$ has only finitely many nonzero addends, qed.} and thus
makes sense.

But when $i=0$, we run into a problem with this approach: $\rho\left(
\sum\limits_{j\in\mathbb{Z}}z_{j}E_{j,j}\right)  v=\sum\limits_{j\in
\mathbb{Z}}z_{j}\rho\left(  E_{j,j}\right)  v$ is an infinite sum which may
very well have infinitely many nonzero addends, and thus makes no sense.

To fix this problem, we define a map $\widehat{\rho}$ which will be a
``small'' modification of $\rho$:

\begin{definition}
\label{def.glinf.rhohat.abar}Define a linear map $\widehat{\rho}%
:\overline{\mathfrak{a}_{\infty}}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ by%
\begin{align}
\widehat{\rho}\left(  \left(  a_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\right)   &  =\sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right. \label{def.glinf.rhohat.generalcase}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for every }\left(  a_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\overline{\mathfrak{a}_{\infty}%
}\right. \nonumber
\end{align}
(where $1$ means the endomorphism $\operatorname*{id}$ of $\wedge
^{\dfrac{\infty}{2},m}V$). Here, the infinite sum $\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  $ is well-defined as an endomorphism of $\wedge^{\dfrac{\infty}{2}%
,m}V$, because for every $v\in\wedge^{\dfrac{\infty}{2},m}V$, the sum
$\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v$ has only finitely many nonzero addends (as Proposition
\ref{prop.glinf.rhohat.welldef} shows).
\end{definition}

The map $\widehat{\rho}$ just defined does not extend the map $\rho$, but is
the unique continuous (in the sense explained above) extension of the map
$\widehat{\rho}\mid_{\mathfrak{gl}_{\infty}}$ to $\overline{\mathfrak{a}%
_{\infty}}$ as a linear map. The map $\widehat{\rho}\mid_{\mathfrak{gl}%
_{\infty}}$ is, in a certain sense, a ``very close approximation to $\rho$'',
as can be seen from the following remark:

\begin{remark}
\label{rmk.glinf.rhohat.abar}From Definition \ref{def.glinf.rhohat.abar}, it
follows that%
\begin{equation}
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)
\in\mathbb{Z}^{2}. \label{def.glinf.rhohat}%
\end{equation}

\end{remark}

We are not done yet: This map $\widehat{\rho}$ is not a representation of
$\overline{\mathfrak{a}_{\infty}}$. We will circumvent this by defining a
central extension $\mathfrak{a}_{\infty}$ of $\overline{\mathfrak{a}_{\infty}%
}$ for which the map $\widehat{\rho}$ (once suitably extended) will be a
representation. But first, let us show a lemma that we owe for the definition
of $\widehat{\rho}$:

\begin{proposition}
\label{prop.glinf.rhohat.welldef}Let $\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\in\overline{\mathfrak{a}_{\infty}}$ and
$v\in\wedge^{\dfrac{\infty}{2},m}V$. Then, the sum%
\[
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v
\]
has only finitely many nonzero addends.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.rhohat.welldef}.} We know that
$v$ is an element of $\wedge^{\dfrac{\infty}{2},m}V$. Hence, $v$ is a
$\mathbb{C}$-linear combination of elements of the form $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ being an $m$-degression (since $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$ is a basis of
$\wedge^{\dfrac{\infty}{2},m}V$). Hence, we can WLOG assume that $v$ is an
element of the form $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with
$\left(  i_{0},i_{1},i_{2},...\right)  $ being an $m$-degression (because the
claim of Proposition \ref{prop.glinf.rhohat.welldef} is clearly linear in
$v$). Assume this. Then, $v=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ for some $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
Consider this $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. By the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every integer $k\geq K$ satisfies $i_{k}+k=m$. Consider this $K$. Then,
every integer which is less or equal to $i_{K}$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $.

For every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, let $r_{i,j}$ be the map
$\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \in\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)
$. Then, the sum%
\[
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v
\]
clearly rewrites as $\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}a_{i,j}r_{i,j}v$. Hence, in order to prove Proposition
\ref{prop.glinf.rhohat.welldef}, we only need to prove that the sum
$\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}r_{i,j}v$ has only
finitely many nonzero addends.

Since $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of the
matrix $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ are
nonzero. In other words, there exists an $M\in\mathbb{N}$ such that%
\begin{equation}
\left(  \text{the }m\text{-th diagonal of the matrix }\left(  a_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\text{ is zero for every }%
m\in\mathbb{Z}\text{ such that }\left\vert m\right\vert \geq M\right)  .
\label{pf.glinf.rhohat.welldef.M}%
\end{equation}
Consider this $M$.

Now, we have the following three observations:

\begin{itemize}
\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $j>\max\left\{
i_{0},0\right\}  $ satisfies $r_{i,j}v=0\ \ \ \ $\footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ be such that $j>\max\left\{
i_{0},0\right\}  $. Then, $j>i_{0}$ and $j>0$.
\par
Since $j>i_{0}$, the integer $j$ does not appear in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, $\rho\left(  E_{i,j}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0$. Since
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=v$, this rewrites as
$\rho\left(  E_{i,j}\right)  v=0$.
\par
Since $j>0$, we cannot have $i=j$ and $i\leq0$. Now, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  $ (since we cannot have $i=j$ and
$i\leq0$), so that $r_{i,j}v=\rho\left(  E_{i,j}\right)  v=0$, qed.} and thus
$a_{i,j}\underbrace{r_{i,j}v}_{=0}=0$.

\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $i\leq
\min\left\{  i_{K},0\right\}  $ satisfies $r_{i,j}v=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\mathbb{Z}^{2}$
be such that $i\leq\min\left\{  i_{K},0\right\}  $. Then, $i\leq i_{K}$ and
$i\leq0$.
\par
Since $i\leq i_{K}$, the integer $i$ appears in the $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $ (because every integer which is less or equal
to $i_{K}$ appears in the $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $). We now must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq j$.
\par
\textit{Case 2:} We have $i=j$.
\par
Let us first consider Case 1. In this case, $i\neq j$. Thus, $\rho\left(
E_{i,j}\right)  v=0$ (because the integer $i$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, so that after applying $\rho\left(
E_{i,j}\right)  $ to $v=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$,
we obtain a wedge in which $v_{i}$ appears twice). On the other hand, $i\neq
j$, so that we cannot have $i=j$ and $i\leq0$. Now, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  $ (since we cannot have $i=j$ and
$i\leq0$), and thus $r_{i,j}v=\rho\left(  E_{i,j}\right)  v=0$.
\par
Now, let us consider Case 2. In this case, $i=j$. Thus, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  -1$ (since $i=j$ and $i\leq0$). Since
$E_{i,j}=E_{i,i}$ (because $j=i$), this rewrites as $r_{i,j}=\rho\left(
E_{i,i}\right)  -1$. On the other hand, the integer $i$ appears in the
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, so that $\rho\left(
E_{i,i}\right)  v=v$. Hence, from $r_{i,j}=\rho\left(  E_{i,i}\right)  -1$, we
get $r_{i,j}v=\left(  \rho\left(  E_{i,i}\right)  -1\right)
v=\underbrace{\rho\left(  E_{i,i}\right)  v}_{=v}-v=v-v=0$.
\par
Thus, in each of the cases 1 and 2, we have proven that $r_{i,j}v=0$. Hence,
$r_{i,j}v=0$ always holds, qed.} and thus $a_{i,j}\underbrace{r_{i,j}v}%
_{=0}=0$.

\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert
i-j\right\vert \geq M$ satisfies $a_{i,j}=0$\ \ \ \ \footnote{\textit{Proof.}
Let $\left(  u,v\right)  \in\mathbb{Z}^{2}$ be such that $\left\vert
u-v\right\vert \geq M$. Then, since $\left\vert v-u\right\vert =\left\vert
u-v\right\vert \geq M$, the $\left(  v-u\right)  $-th diagonal of the matrix
$\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is zero (by
(\ref{pf.glinf.rhohat.welldef.M}), applied to $m=v-u$), and thus $a_{u,v}=0$
(since $a_{u,v}$ is an entry on the $\left(  v-u\right)  $-th diagonal of the
matrix $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$). We
thus have shown that every $\left(  u,v\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert u-v\right\vert \geq M$ satisfies $a_{u,v}=0$. Renaming $\left(
u,v\right)  $ as $\left(  i,j\right)  $ in this fact, we obtain: Every
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert i-j\right\vert
\geq M$ satisfies $a_{i,j}=0$, qed.} and thus $\underbrace{a_{i,j}}%
_{=0}r_{i,j}v=0$.
\end{itemize}

Now, for any $\alpha\in\mathbb{Z}$ and $\beta\in\mathbb{Z}$, let $\left[
\alpha,\beta\right]  _{\mathbb{Z}}$ denote the set $\left\{  x\in
\mathbb{Z}\ \mid\ \alpha\leq x\leq\beta\right\}  $ (this set is finite). It is
easy to see that%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that }%
a_{i,j}r_{i,j}v\neq0\text{ satisfies}\\
\left(  i,j\right)  \in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{
i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}}\times\left[  \min\left\{
i_{K},0\right\}  -M+2,\max\left\{  i_{0},0\right\}  \right]  _{\mathbb{Z}}%
\end{array}
\right)  \label{pf.gli.rhohat.welldef.bnd}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gli.rhohat.welldef.bnd}):} Let $\left(
i,j\right)  \in\mathbb{Z}^{2}$ be such that $a_{i,j}r_{i,j}v\neq0$. Then, we
cannot have $j>\max\left\{  i_{0},0\right\}  $ (since every $\left(
i,j\right)  \in\mathbb{Z}^{2}$ such that $j>\max\left\{  i_{0},0\right\}  $
satisfies $a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq0$). In
other words, $j\leq\max\left\{  i_{0},0\right\}  $. Also, we cannot have
$i\leq\min\left\{  i_{K},0\right\}  $ (since every $\left(  i,j\right)
\in\mathbb{Z}^{2}$ such that $i\leq\min\left\{  i_{K},0\right\}  $ satisfies
$a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq0$). Thus, we have
$i>\min\left\{  i_{K},0\right\}  $, so that $i\geq\min\left\{  i_{K}%
,0\right\}  +1$ (since $i$ and $\min\left\{  i_{K},0\right\}  $ are integers).
Finally, we cannot have $\left\vert i-j\right\vert \geq M$ (since every
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert i-j\right\vert
\geq M$ satisfies $a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq
0$). Thus, we have $\left\vert i-j\right\vert <M$, so that $\left\vert
i-j\right\vert \leq M-1$ (since $\left\vert i-j\right\vert $ and $M$ are
integers). Thus, $i-j\leq\left\vert i-j\right\vert \leq M-1$. Hence,
$i\leq\underbrace{j}_{\leq\max\left\{  i_{0},0\right\}  }+M-1\leq\max\left\{
i_{0},0\right\}  +M-1$. Combined with $i\geq\min\left\{  i_{K},0\right\}  +1$,
this yields $i\in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{
i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}}$. From $i-j\leq M-1$, we also
obtain $j\geq\underbrace{i}_{\geq\min\left\{  i_{K},0\right\}  +1}-\left(
M-1\right)  \geq\min\left\{  i_{K},0\right\}  +1-\left(  M-1\right)
=\min\left\{  i_{K},0\right\}  -M+2$. Combined with $j\leq\max\left\{
i_{0},0\right\}  $, this yields $j\in\left[  \min\left\{  i_{K},0\right\}
-M+2,\max\left\{  i_{0},0\right\}  \right]  _{\mathbb{Z}}$. Combined with
$i\in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}
+M-1\right]  _{\mathbb{Z}}$, this yields $\left(  i,j\right)  \in\left[
\min\left\{  i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}  +M-1\right]
_{\mathbb{Z}}\times\left[  \min\left\{  i_{K},0\right\}  -M+2,\max\left\{
i_{0},0\right\}  \right]  _{\mathbb{Z}}$. This proves
(\ref{pf.gli.rhohat.welldef.bnd}).}. Since $\left[  \min\left\{
i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}%
}\times\left[  \min\left\{  i_{K},0\right\}  -M+2,\max\left\{  i_{0}%
,0\right\}  \right]  _{\mathbb{Z}}$ is a finite set, this shows that only
finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy $a_{i,j}%
r_{i,j}v\neq0$. In other words, the sum $\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}a_{i,j}r_{i,j}v$ has only finitely many nonzero addends.
This proves Proposition \ref{prop.glinf.rhohat.welldef}.

Our definition of $\widehat{\rho}$ is somewhat unwieldy, since computing
$\widehat{\rho}\left(  a\right)  v$ for a matrix $a\in\overline{\mathfrak{a}%
_{\infty}}$ and a $v\in\wedge^{\dfrac{\infty}{2},m}V$ using it requires
writing $v$ as a linear combination of elementary semiinfinite wedges.
However, since our $\widehat{\rho}$ only slightly differs from $\rho$, there
are many matrices $a$ for which $\widehat{\rho}\left(  a\right)  $ behaves
exactly as $\rho\left(  a\right)  $ would if we could extend $\rho$ to
$\overline{\mathfrak{a}_{\infty}}$:

\begin{proposition}
\label{prop.glinf.ainfact}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
Let $a\in\overline{\mathfrak{a}_{\infty}}$. Assume that, for every integer
$i\leq0$, the $\left(  i,i\right)  $-th entry of $a$ is $0$. Then,%
\[
\left(  \widehat{\rho}\left(  a\right)  \right)  \left(  b_{0}\wedge
b_{1}\wedge b_{2}\wedge...\right)  =\sum\limits_{k\geq0}b_{0}\wedge
b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)
\wedge b_{k+1}\wedge b_{k+2}\wedge....
\]
In particular, the infinite sum $\sum\limits_{k\geq0}b_{0}\wedge b_{1}%
\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge
b_{k+1}\wedge b_{k+2}\wedge...$ is well-defined (i. e., all but finitely many
integers $k\geq0$ satisfy $b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}%
\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...=0$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.ainfact}.} For every $\left(
i,j\right)  \in\mathbb{Z}^{2}$, let $a_{i,j}$ be the $\left(  i,j\right)  $-th
entry of the matrix $a$. Then, $a=\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}=\sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}a_{i,j}E_{i,j}$. But every $\left(  i,j\right)  \in
\mathbb{Z}^{2}$ such that $i=j$ and $i\leq0$ satisfies $a_{i,j}=a_{i,i}=0$
(because we assumed that, for every integer $i\leq0$, the $\left(  i,i\right)
$-th entry of $a$ is $0$). Thus, $\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\i=j\text{ and }i\leq0}}\underbrace{a_{i,j}}_{=0}%
E_{i,j}=\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0E_{i,j}=0$, so that%
\[
a=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}E_{i,j}%
=\underbrace{\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0E_{i,j}}_{=0}+\sum\limits_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }%
i\leq0\right)  }}a_{i,j}E_{i,j}=\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}E_{i,j}.
\]


But from $a=\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$,
we have%
\begin{align*}
\widehat{\rho}\left(  a\right)   &  =\widehat{\rho}\left(  \left(
a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}\right)
=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{def.glinf.rhohat.generalcase})}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\i=j\text{
and }i\leq0}}\underbrace{a_{i,j}}_{=0}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  }_{\substack{=\rho\left(  E_{i,j}\right)  \\\text{(since we do not
have }\left(  i=j\text{ and }i\leq0\right)  \text{)}}}\\
&  =\underbrace{\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  }_{=0}+\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\rho\left(
E_{i,j}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\rho\left(  E_{i,j}\right)  ,
\end{align*}
so that%
\begin{align*}
&  \left(  \widehat{\rho}\left(  a\right)  \right)  \left(  b_{0}\wedge
b_{1}\wedge b_{2}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\underbrace{\rho\left(
E_{i,j}\right)  \left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)
}_{\substack{=E_{i,j}\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge
b_{2}\wedge...\right)  \\=\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge
b_{k-1}\wedge\left(  E_{i,j}\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...\\\text{(by Proposition \ref{prop.glinf.glinfact}, applied to
}E_{i,j}\text{ instead of }a\text{)}}}\\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\sum\limits_{k\geq0}%
b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  E_{i,j}\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}%
\wedge\underbrace{\left(  \sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}\left(  E_{i,j}\rightharpoonup b_{k}\right)  \right)  }%
_{\substack{=\left(  \sum\limits_{\substack{\left(  i,j\right)  \in
\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}E_{i,j}\right)  \rightharpoonup b_{k}=a\rightharpoonup b_{k}%
\\\text{(since }\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}%
E_{i,j}=a\text{)}}}\wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we interchanged the summation signs; this is allowed because (as
the reader}\\
\text{can check) all but finitely many }\left(  \left(  i,j\right)  ,k\right)
\in\mathbb{Z}^{2}\times\mathbb{Z}\text{ satisfying }k\geq0\text{ and not}\\
\left(  i=j\text{ and }i\leq0\right)  \text{ satisfy }a_{i,j}\cdot b_{0}\wedge
b_{1}\wedge...\wedge b_{k-1}\wedge\left(  E_{i,j}\rightharpoonup b_{k}\right)
\wedge b_{k+1}\wedge b_{k+2}\wedge...=0
\end{array}
\right) \\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...
\end{align*}
(and en passant, this argument has shown that the infinite sum $\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...$ is
well-defined). This proves Proposition \ref{prop.glinf.ainfact}.

The issue that remains is that $\widehat{\rho}$ is not a representation of
$\overline{\mathfrak{a}_{\infty}}$. To mitigate this, we will define a central
extension of $\overline{\mathfrak{a}_{\infty}}$ by the so-called
\textit{Japanese cocycle}. Let us define this cocycle first:

\begin{theorem}
\label{thm.japan}For any $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\overline{\mathfrak{a}_{\infty}}$, we have $\widehat{\rho}\left(  \left[
A,B\right]  \right)  -\left[  \widehat{\rho}\left(  A\right)  ,\widehat{\rho
}\left(  B\right)  \right]  =\alpha\left(  A,B\right)  $ where $\alpha\left(
A,B\right)  $ is a scalar depending on $A$ and $B$ (and where we identify any
scalar $\lambda\in\mathbb{C}$ with the matrix $\lambda\cdot\operatorname*{id}%
\in\overline{\mathfrak{a}_{\infty}}$). This $\alpha\left(  A,B\right)  $ can
be computed as follows: Write $A$ and $B$ as block matrices $A=\left(
\begin{array}
[c]{cc}%
A_{11} & A_{12}\\
A_{21} & A_{22}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
B_{11} & B_{12}\\
B_{21} & B_{22}%
\end{array}
\right)  $, where the blocks are separated as follows:

- The left blocks contain the $j$-th columns for all $j\leq0$; the right
blocks contain the $j$-th columns for all $j>0$.

- The upper blocks contain the $i$-th rows for all $i\leq0$; the lower blocks
contain the $i$-th rows for all $i>0$.

Then, $\alpha\left(  A,B\right)  =\operatorname*{Tr}\left(  -B_{12}%
A_{21}+A_{12}B_{21}\right)  $. (This trace makes sense because the matrices
$A_{12}$, $B_{21}$, $A_{21}$, $B_{12}$ have only finitely many nonzero entries.)
\end{theorem}

\begin{corollary}
\label{cor.japan}The bilinear map $\alpha:\overline{\mathfrak{a}_{\infty}%
}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ defined in
Theorem \ref{thm.japan} is a $2$-cocycle on $\overline{\mathfrak{a}_{\infty}}$.

We define $\mathfrak{a}_{\infty}$ as the $1$-dimensional central extension
$\widehat{\overline{\mathfrak{a}_{\infty}}}_{\alpha}$ of $\overline
{\mathfrak{a}_{\infty}}$ by $\mathbb{C}$ using this cocycle $\alpha$ (see
Definition \ref{def.centex} for what this means).
\end{corollary}

\begin{definition}
\label{def.japan}The $2$-cocycle $\alpha:\overline{\mathfrak{a}_{\infty}%
}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ introduced in
Corollary \ref{cor.japan} is called the \textit{Japanese cocycle}.
\end{definition}

The proofs of Theorem \ref{thm.japan} and Corollary \ref{cor.japan} are a
homework problem. A few remarks on the Japanese cocycle are in order. It can
be explicitly computed by the formula%
\begin{align*}
&  \alpha\left(  \left(  a_{i,j}\right)  _{\left(  i,j\right)  \in
\mathbb{Z}^{2}},\left(  b_{i,j}\right)  _{\left(  i,j\right)  \in
\mathbb{Z}^{2}}\right)  \\
&  =-\sum_{\substack{i\leq0;\\j>0}}b_{i,j}a_{j,i}+\sum_{\substack{i\leq
0;\\j>0}}a_{i,j}b_{j,i}=-\sum_{\substack{i>0;\\j\leq0}}a_{i,j}b_{j,i}%
+\sum_{\substack{i\leq0;\\j>0}}a_{i,j}b_{j,i}\\
&  =\sum_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}b_{j,i}\left(  \left[
j>0\right]  -\left[  i>0\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}},\left(
b_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\overline
{\mathfrak{a}_{\infty}}%
\end{align*}
where we are using the \textit{Iverson bracket notation}\footnote{This is the
notation $\left[  \mathcal{S}\right]  $ for the truth value of any logical
statement $\mathcal{S}$ (that is, $\left[  \mathcal{S}\right]  $ denotes the
integer $%
\begin{cases}
1, & \text{if }\mathcal{S}\text{ is true;}\\
0, & \text{if }\mathcal{S}\text{ is false}%
\end{cases}
$).}. The cocycle $\alpha$\ owes its name \textquotedblleft Japanese
cocycle\textquotedblright\ to the fact that it (first?) appeared in the work
of the Tokyo mathematical physicists Date, Jimbo, Kashiwara and
Miwa\footnote{More precisely, it is the skew-symmetric bilinear form $c$ in
the following paper:
\par
\begin{itemize}
\item Etsuro Date, Michio Jimbo, Masaki Kashiwara, Tetuji Miwa,
\textit{Transformation Groups for Soliton Equations -- Euclidean Lie Algebras
and Reduction of the KP Hierarchy}, Publ. RIMS, Kyoto Univ. 18 (1982), pp.
1077--1110.
\end{itemize}
\par
In this paper, the Lie algebras that we are denoting by $\overline
{\mathfrak{a}_{\infty}}$ and $\mathfrak{a}_{\infty}$ are called
$\mathfrak{pgl}\left(  \infty\right)  $ and $\mathfrak{gl}\left(
\infty\right)  $, respectively.}.

We are going to prove soon (Proposition \ref{prop.japan.nontr} and Corollary
\ref{cor.japan.triv}) that $\alpha$ is a nontrivial $2$-cocycle, but its
restriction to $\mathfrak{gl}_{\infty}$ is trivial. This is a strange
situation (given that $\mathfrak{gl}_{\infty}$ is a dense Lie subalgebra of
$\overline{\mathfrak{a}_{\infty}}$ with respect to a reasonably defined
topology), but we will later see the reason for this behavior.

\begin{theorem}
Let us extend the linear map $\widehat{\rho}:\overline{\mathfrak{a}_{\infty}%
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
(introduced in Definition \ref{def.glinf.rhohat.abar}) to a linear map
$\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ by setting $\widehat{\rho}\left(
K\right)  =\operatorname*{id}$. (This makes sense since $\mathfrak{a}_{\infty
}=\overline{\mathfrak{a}_{\infty}}\oplus\mathbb{C}K$ as vector spaces.) Then,
this map $\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}%
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ is a representation of
$\mathfrak{a}_{\infty}$.

Thus, $\wedge^{\dfrac{\infty}{2},m}V$ becomes an $\mathfrak{a}_{\infty}$-module.
\end{theorem}

\begin{definition}
\label{def.ainf.grade2}Since $\mathfrak{a}_{\infty}=\overline{\mathfrak{a}%
_{\infty}}\oplus\mathbb{C}K$ as vector space, we can define a grading on
$\mathfrak{a}_{\infty}$ as the direct sum of the grading on $\overline
{\mathfrak{a}_{\infty}}$ (which was defined in Definition \ref{def.ainf.grade}%
) and the trivial grading on $\mathbb{C}K$ (that is the grading which puts $K$
in degree $0$). This is easily seen to make $\mathfrak{a}_{\infty}$ a
$\mathbb{Z}$-graded Lie algebra. We will consider $\mathfrak{a}_{\infty}$ to
be $\mathbb{Z}$-graded in this way.
\end{definition}

\begin{proposition}
Let $m\in\mathbb{Z}$. With the grading defined in Definition
\ref{def.ainf.grade2}, the $\mathfrak{a}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$ is graded.
\end{proposition}

\begin{corollary}
\label{cor.japan.triv}The restriction of $\alpha$ to $\mathfrak{gl}_{\infty
}\times\mathfrak{gl}_{\infty}$ is a $2$-coboundary.
\end{corollary}

\textit{Proof of Corollary \ref{cor.japan.triv}.} Let $J$ be the block matrix
$\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & -I_{\infty}%
\end{array}
\right)  \in\overline{\mathfrak{a}_{\infty}}$, where the blocks are separated
in the same way as in Theorem \ref{thm.japan}. Define a linear map
$f:\mathfrak{gl}_{\infty}\rightarrow\mathbb{C}$ by
\[
\left(  f\left(  A\right)  =\operatorname*{Tr}\left(  JA\right)
\ \ \ \ \ \ \ \ \ \ \text{for any }A\in\mathfrak{gl}_{\infty}\right)
\]
\footnote{Note that $\operatorname*{Tr}\left(  JA\right)  $ is well-defined
for every $A\in\mathfrak{gl}_{\infty}$, since Remark \ref{rmk.ainf.mult}
\textbf{(b)} (applied to $J$ and $A$ instead of $A$ and $B$) yields that
$JA\in\mathfrak{gl}_{\infty}$.}. Then, any $A\in\mathfrak{gl}_{\infty}$ and
$B\in\mathfrak{gl}_{\infty}$ satisfy $\alpha\left(  A,B\right)  =f\left(
\left[  A,B\right]  \right)  $. This is because (for any $A\in\mathfrak{gl}%
_{\infty}$ and $B\in\mathfrak{gl}_{\infty}$) we can write the matrix $\left[
A,B\right]  $ in the form $\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
\ast & \ast\\
\ast & \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}A_{12}%
\end{array}
\right)  $ (where asterisks mean blocks which we don't care about), so that
$J\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
0 & 0\\
\ast & -\left(  \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}%
A_{12}\right)
\end{array}
\right)  $ and thus
\begin{align*}
&  \operatorname*{Tr}\left(  J\left[  A,B\right]  \right) \\
&  =-\operatorname*{Tr}\left(  \left[  A_{22},B_{22}\right]  +A_{21}%
B_{12}-B_{21}A_{12}\right)  =-\underbrace{\operatorname*{Tr}\left[
A_{22},B_{22}\right]  }_{=0}-\underbrace{\operatorname*{Tr}\left(
A_{21}B_{12}\right)  }_{=\operatorname*{Tr}\left(  B_{12}A_{21}\right)
}+\underbrace{\operatorname*{Tr}\left(  B_{21}A_{12}\right)  }%
_{=\operatorname*{Tr}\left(  A_{12}B_{21}\right)  }\\
&  =-\operatorname*{Tr}\left(  B_{12}A_{21}\right)  +\operatorname*{Tr}\left(
A_{12}B_{21}\right)  =\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}%
B_{21}\right)  =\alpha\left(  A,B\right)  .
\end{align*}
The proof of Corollary \ref{cor.japan.triv} is thus finished.

But note that this proof does not extend to $\overline{\mathfrak{a}_{\infty}}%
$, because $f$ does not continuously extend to $\overline{\mathfrak{a}%
_{\infty}}$ (for any reasonable notion of continuity).

\begin{proposition}
\label{prop.japan.nontr}The $2$-cocycle $\alpha$ itself is not a $2$-coboundary.
\end{proposition}

\textit{Proof of Proposition \ref{prop.japan.nontr}.} Let $T$ be the shift
operator defined above. The span $\left\langle T^{j}\ \mid\ j\in
\mathbb{Z}\right\rangle $ is an abelian Lie subalgebra of $\overline
{\mathfrak{a}_{\infty}}$ (isomorphic to the abelian Lie algebra $\mathbb{C}%
\left[  t,t^{-1}\right]  $, and to the quotient $\overline{\mathcal{A}}$ of
the Heisenberg algebra $\mathcal{A}$ by its central subalgebra $\left\langle
K\right\rangle $). Any $2$-coboundary must become zero when restricted onto an
abelian Lie subalgebra. But the $2$-cocycle $\alpha$, restricted onto the span
$\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $, does not become $0$,
since%
\[
\alpha\left(  T^{i},T^{j}\right)  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq-j;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }i=-j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\mathbb{Z}.
\]
Proposition \ref{prop.japan.nontr} is thus proven.

In this proof, we have constructed an embedding $\overline{\mathcal{A}%
}\rightarrow\overline{\mathfrak{a}_{\infty}}$ which sends $\overline{a_{j}}$
to $T^{j}$ for every $j\in\mathbb{Z}$. This embedding is crucial to what we
are going to do, so let us give it a formal definition:

\begin{definition}
\label{def.ainf.A}The map%
\[
\overline{\mathcal{A}}\rightarrow\overline{\mathfrak{a}_{\infty}%
},\ \ \ \ \ \ \ \ \ \ a_{j}\mapsto T^{j}%
\]
(where $\overline{\mathcal{A}}$ is the quotient of the Heisenberg algebra
$\mathcal{A}$ by its central subalgebra $\left\langle K\right\rangle $) is an
embedding of Lie algebras. We will regard this embedding as an inclusion, and
thus we will regard $\overline{\mathcal{A}}$ as a Lie subalgebra of
$\overline{\mathfrak{a}_{\infty}}$.

This embedding is easily seen to give rise to an embedding $\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}$ of Lie algebras which sends $K$ to $K$ and
sends $a_{j}$ to $T^{j}$ for every $j\in\mathbb{Z}$. This embedding will also
be regarded as an inclusion, so that $\mathcal{A}$ will be considered as a Lie
subalgebra of $\mathfrak{a}_{\infty}$.
\end{definition}

It is now easy to see:

\begin{proposition}
Extend our map $\widehat{\rho}:\overline{\mathfrak{a}_{\infty}}\rightarrow
\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ to a map
$\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  $, also denoted by $\widehat{\rho}$, by
setting $\widehat{\rho}\left(  K\right)  =\operatorname*{id}$. Then, this map
$\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ is a Lie algebra homomorphism, i. e.,
it makes $\wedge^{\dfrac{\infty}{2},m}V$ into an $\mathfrak{a}_{\infty}%
$-module. The element $K$ of $\mathfrak{a}_{\infty}$ acts as
$\operatorname*{id}$ on this module.

By means of the embedding $\mathcal{A}\rightarrow\mathfrak{a}_{\infty}$, this
$\mathfrak{a}_{\infty}$-module gives rise to an $\mathcal{A}$-module
$\wedge^{\dfrac{\infty}{2},m}V$, on which $K$ acts as $\operatorname*{id}$.
\end{proposition}

In Proposition \ref{prop.Lomegam}, we identified $\wedge^{\dfrac{\infty}{2}%
,m}V$ as an irreducible highest-weight $\mathfrak{gl}_{\infty}$-module;
similarly, we can identify it as an irreducible highest-weight $\mathfrak{a}%
_{\infty}$-module:

\begin{proposition}
\label{prop.Lomegam.a}Let $m\in\mathbb{Z}$. Let $\overline{\omega}_{m}$ be the
$\mathbb{C}$-linear map $\mathfrak{a}_{\infty}\left[  0\right]  \rightarrow
\mathbb{C}$ which sends every infinite diagonal matrix $\operatorname*{diag}%
\left(  ...,d_{-2},d_{-1},d_{0},d_{1},d_{2},...\right)  \in\overline
{\mathfrak{a}_{\infty}}$ to $\left\{
\begin{array}
[c]{c}%
\sum\limits_{j=1}^{m}d_{j},\ \ \ \ \ \ \ \ \ \ \text{if }m\geq0;\\
-\sum\limits_{j=m+1}^{0}d_{j},\ \ \ \ \ \ \ \ \ \ \text{if }m<0
\end{array}
\right.  $, and sends $K$ to $1$. Then, the graded $\mathfrak{a}_{\infty}%
$-module $\wedge^{\dfrac{\infty}{2},m}V$ is the irreducible highest-weight
representation $L_{\overline{\omega}_{m}}$ of $\mathfrak{a}_{\infty}$ with
highest weight $L_{\overline{\omega}_{m}}$. Moreover, $L_{\overline{\omega
}_{m}}$ is unitary.
\end{proposition}

\begin{remark}
Note the analogy between the weight $\overline{\omega}_{m}$ in Proposition
\ref{prop.Lomegam.a} and the weight $\omega_{m}$ in Proposition
\ref{prop.Lomegam}: The weight $\omega_{m}$ in Proposition \ref{prop.Lomegam}
sends every diagonal matrix $\operatorname*{diag}\left(  ...,d_{-2}%
,d_{-1},d_{0},d_{1},d_{2},...\right)  \in\mathfrak{gl}_{\infty}$ to
$\sum\limits_{j=-\infty}^{m}d_{j}$. Note that this sum $\sum\limits_{j=-\infty
}^{m}d_{j}$ is well-defined (because for a diagonal matrix
$\operatorname*{diag}\left(  ...,d_{-2},d_{-1},d_{0},d_{1},d_{2},...\right)  $
to lie in $\mathfrak{gl}_{\infty}$, it has to satisfy $d_{j}=0$ for all but
finitely many $j\in\mathbb{Z}$).
\end{remark}

In analogy to Corollary \ref{cor.lomegam.unit}, we can also show:

\begin{corollary}
\label{cor.lomegam.unit.a}For every finite sum $\sum\limits_{i\in\mathbb{Z}%
}k_{i}\overline{\omega}_{i}$ with $k_{i}\in\mathbb{N}$, the representation
$L_{\sum\limits_{i\in\mathbb{Z}}k_{i}\overline{\omega}_{i}}$ of $\mathfrak{a}%
_{\infty}$ is unitary.
\end{corollary}

\subsection{Virasoro actions on
\texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$} {the semi-infinite wedge
space}}

We can also embed the Virasoro algebra $\operatorname*{Vir}$ into
$\mathfrak{a}_{\infty}$, and not just in one way, but in infinitely many ways
depending on two parameters:

\begin{proposition}
Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Let the
$\operatorname*{Vir}$-module $V_{\alpha,\beta}$ be defined as in Proposition
\ref{prop.Vab.1}.

For every $k\in\mathbb{Z}$, let $v_{k}=t^{-k+\alpha}\left(  dt\right)
^{\beta}\in V_{\alpha,\beta}$. Here, for any $\ell\in\mathbb{Z}$, the term
$t^{\ell+\alpha}\left(  dt\right)  ^{\beta}$ denotes $t^{\ell}t^{\alpha
}\left(  dt\right)  ^{\beta}$.

According to Proposition \ref{prop.Vab.1} \textbf{(b)}, every $m\in\mathbb{Z}$
satisfies%
\[
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\]
Thus, if we write $L_{m}$ as a matrix with respect to the basis $\left(
v_{k}\right)  _{k\in\mathbb{Z}}$ of $V_{\alpha,\beta}$, then this matrix lies
in $\overline{\mathfrak{a}_{\infty}}$ (in fact, its only nonzero diagonal is
the $m$-th one).

This defines an injective map $\overline{\varphi_{\alpha,\beta}}%
:W\rightarrow\overline{\mathfrak{a}_{\infty}}$, which sends every $L_{m}\in W$
to the matrix representing the action of $L_{m}$ on $V_{\alpha,\beta}$. This
map $\overline{\varphi_{\alpha,\beta}}$ is a Lie algebra homomorphism (since
the $\operatorname*{Vir}$-module $V_{\alpha,\beta}$ has central charge $0$, i.
e., is an $W$-module). Hence, this map $\overline{\varphi_{\alpha,\beta}}$
lifts to an injective map $\widehat{W}\rightarrow\mathfrak{a}_{\infty}$, where
$\widehat{W}$ is defined as follows: Let $\widetilde{\alpha}:\overline
{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow
\mathbb{C}$ be the Japanese cocycle (this cocycle has been called $\alpha$ in
Definition \ref{def.japan}, but here we use the letter $\alpha$ for something
different), and let $\widetilde{\alpha}^{\prime}:W\times W\rightarrow
\mathbb{C}$ be the restriction of this Japanese cocycle $\widetilde{\alpha
}:\overline{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}%
}\rightarrow\mathbb{C}$ to $W\times W$ via the map $\overline{\varphi
_{\alpha,\beta}}\times\overline{\varphi_{\alpha,\beta}}:W\times W\rightarrow
\overline{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}}$. Then,
$\widehat{W}$ denotes the central extension of $W$ defined by the $2$-cocycle
$\widetilde{\alpha}^{\prime}$.

But let us now compute $\widetilde{\alpha}^{\prime}$ and $\widehat{W}$. In
fact, from a straightforward calculation (Homework Set 4 exercise 3) it
follows that%
\[
\widetilde{\alpha}^{\prime}\left(  L_{m},L_{n}\right)  =\delta_{n,-m}\left(
\dfrac{n^{3}-n}{12}c_{\beta}+2nh_{\alpha,\beta}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z},
\]
where
\[
c_{\beta}=-12\beta^{2}+12\beta-2\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ h_{\alpha,\beta}=\dfrac{1}{2}\alpha\left(  \alpha
+2\beta-1\right)  .
\]
Thus, the $2$-cocycle $\widetilde{\alpha}^{\prime}$ differs from the
$2$-cocycle $\omega$ (defined in Theorem \ref{thm.H^2(W)}) merely by a
multiplicative factor ($\dfrac{c_{\beta}}{2}$) and a $2$-coboundary (which
sends every $\left(  L_{m},L_{n}\right)  $ to $\delta_{n,-m}\cdot
2nh_{\alpha,\beta}$). Thus, the central extension $\widehat{W}$ of $W$ defined
by the $2$-cocycle $\widetilde{\alpha}^{\prime}$ is isomorphic (as a Lie
algebra) to the central extension of $W$ defined by the $2$-cocycle $\omega$,
that is, to the Virasoro algebra $\operatorname*{Vir}$. This turns the Lie
algebra homomorphism $\widehat{W}\rightarrow\mathfrak{a}_{\infty}$ into a
homomorphism $\operatorname*{Vir}\rightarrow\mathfrak{a}_{\infty}$. Let us
describe this homomorphism explicitly:

Let $\widehat{L_{0}}$ be the element $\overline{\varphi_{\alpha,\beta}}\left(
L_{0}\right)  +h_{\alpha,\beta}K\in\mathfrak{a}_{\infty}$. Then, the linear
map%
\begin{align*}
\operatorname*{Vir}  &  \rightarrow\mathfrak{a}_{\infty},\\
L_{n}  &  \mapsto\overline{\varphi_{\alpha,\beta}}\left(  L_{n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for }n\neq0,\\
L_{0}  &  \mapsto\widehat{L_{0}},\\
C  &  \mapsto c_{\beta}K
\end{align*}
is a Lie algebra homomorphism. Denote this map by $\varphi_{\alpha,\beta}$. By
means of this homomorphism, we can restrict the $\mathfrak{a}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2},m}V$ to a $\operatorname*{Vir}$-module. Denote this
$\operatorname*{Vir}$-module by $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}%
$. Note that $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is a Virasoro
module with central charge $c=c_{\beta}$. This $\wedge^{\dfrac{\infty}{2}%
,m}V_{\alpha,\beta}$ is called the \textit{module of semiinfinite forms}. The
vector $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ (defined in
Definition \ref{def.psim}) has highest degree (namely, $0$).

We have $L_{i}\psi_{m}=0$ for $i>0$, and we have $L_{0}\psi_{m}=\dfrac{1}%
{2}\left(  \alpha-m\right)  \left(  \alpha+2\beta-1-m\right)  \psi_{m}$.
(Proof: Homework exercise.)
\end{proposition}

\begin{corollary}
Let $\alpha,\beta\in\mathbb{C}$. We have a homomorphism%
\begin{align*}
M_{\lambda}  &  \rightarrow\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta},\\
v_{\lambda}  &  \mapsto\psi_{m}%
\end{align*}
of Virasoro modules, where%
\[
\lambda=\left(  \dfrac{1}{2}\left(  \alpha-m\right)  \left(  \alpha
+2\beta-1-m\right)  ,-12\beta^{2}+12\beta-2\right)  .
\]

\end{corollary}

We will see that this is an isomorphism for generic $\lambda$. For concrete
$\lambda$ it is not always one, and can have a rather complicated kernel.

\subsection{The dimensions of the homogeneous components of
\texorpdfstring{$\wedge ^{\dfrac{\infty}{2},m}V$}{the semi-infinite wedge
space}}

Fix $m\in\mathbb{Z}$. We already know from Definition
\ref{def.glinf.wedge.grading} that $\wedge^{\dfrac{\infty}{2},m}V$ is a graded
$\mathbb{C}$-vector space. More concretely,%
\[
\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\geq0}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  ,
\]
where every $d\geq0$ satisfies%
\[
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  =\left\langle
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ \sum\limits_{k\geq
0}\left(  i_{k}+k-m\right)  =d\right\rangle .
\]


We also know that the $m$-degressions are in a 1-to-1 correspondence with the
partitions. This correspondence maps any $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ to the partition $\left(  i_{k}+k-m\right)
_{k\geq0}$; this is a partition of the integer $\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  $. As a consequence, for every integer $d\geq0$, the
$m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfying
$\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)  =d$ are in 1-to-1
correspondence with the partitions of $d$. Hence, for every integer $d\geq0$,
the number of all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfying $\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)  =d$ equals the
number of the partitions of $d$. Thus, for every integer $d\geq0$, we have%
\begin{align*}
&  \dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right) \\
&  =\left(  \text{the number of }m\text{-degressions }\left(  i_{0}%
,i_{1},i_{2},...\right)  \text{ satisfying}\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  =d\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression satisfying }\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)
=d}\\
\text{ is a basis of }\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]
\end{array}
\right) \\
&  =\left(  \text{the number of partitions of }d\right)  =p\left(  d\right)  ,
\end{align*}
where $p$ is the partition function. Hence:

\begin{proposition}
\label{prop.wedge.genfun}Let $m\in\mathbb{Z}$. Every integer $d\geq0$
satisfies $\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right)  =p\left(  d\right)  $, where $p$ is the partition
function. As a consequence, in the ring of formal power series $\mathbb{C}%
\left[  \left[  q\right]  \right]  $, we have%
\[
\sum\limits_{d\geq0}\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)
\left[  -d\right]  \right)  q^{d}=\sum\limits_{d\geq0}p\left(  d\right)
q^{d}=\dfrac{1}{\left(  1-q\right)  \left(  1-q^{2}\right)  \left(
1-q^{3}\right)  \cdots}.
\]

\end{proposition}

\subsection{The Boson-Fermion correspondence}

\begin{proposition}
\label{prop.wedge.fock}Let $m\in\mathbb{Z}$. Recall the vector $\psi_{m}$
defined in Definition \ref{def.psim}.

\textbf{(a)} As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}{2},m}V$ is
isomorphic to the Fock module $F_{m}$. More precisely, there exists a graded
$\mathcal{A}$-module isomorphism $\widetilde{\sigma}_{m}:F_{m}\rightarrow
\wedge^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such that
$\widetilde{\sigma}_{m}\left(  1\right)  =\psi_{m}$.

\textbf{(b)} As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}{2},m}V$ is
isomorphic to the Fock module $\widetilde{F}_{m}$. More precisely, there
exists a graded $\mathcal{A}$-module isomorphism $\sigma_{m}:\widetilde{F}%
_{m}\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such
that $\sigma_{m}\left(  1\right)  =\psi_{m}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.wedge.fock}.} \textbf{(a)} Let us first
notice that in the ring $\mathbb{C}\left[  \left[  q\right]  \right]  $, we
have
\begin{align*}
\sum\limits_{d\geq0}\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)
\left[  -d\right]  \right)  q^{d}  &  =\dfrac{1}{\left(  1-q\right)  \left(
1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}\ \ \ \ \ \ \ \ \ \ \left(
\text{by Proposition \ref{prop.wedge.genfun}}\right) \\
&  =\sum\limits_{n\geq0}\dim\left(  \underbrace{F}_{\substack{=F_{m}%
\\\text{(as vector spaces)}}}\left[  -n\right]  \right)  q^{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Definition \ref{def.fock.grad}}\right) \\
&  =\sum\limits_{n\geq0}\dim\left(  F_{m}\left[  -n\right]  \right)
q^{n}=\sum\limits_{d\geq0}\dim\left(  F_{m}\left[  -d\right]  \right)  q^{d}.
\end{align*}
By comparing coefficients, this yields that every integer $d\geq0$ satisfies
\begin{equation}
\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]
\right)  =\dim\left(  F_{m}\left[  -d\right]  \right)  .
\label{pf.wedge.fock.dim}%
\end{equation}


We have $a_{i}\psi_{m}=0$ for all $i>0$ (by degree considerations), and we
also have $K\psi_{m}=\psi_{m}$. Besides, it is easy to see that $a_{0}\psi
_{m}=m\psi_{m}$\ \ \ \ \footnote{\textit{Proof.} The embedding $\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}$ sends $a_{0}$ to $T^{0}=\mathbf{1}$, where
$\mathbf{1}$ denotes the identity matrix in $\mathfrak{a}_{\infty}$. Thus,
$a_{0}\psi_{m}=\mathbf{1}\psi_{m}$. (Note that $\mathbf{1}\psi_{m}$ needs not
equal $\psi_{m}$ in general, since the action of $\mathfrak{a}_{\infty}$ on
$\wedge^{\dfrac{\infty}{2},m}V$ is not an associative algebra action, but just
a Lie algebra action.) Recall that $\wedge^{\dfrac{\infty}{2},m}V$ became an
$\mathfrak{a}_{\infty}$-module via the map $\widehat{\rho}$, so that
$U\psi_{m}=\widehat{\rho}\left(  U\right)  \psi_{m}$ for every $U\in
\mathfrak{a}_{\infty}$. Now,%
\begin{align*}
a_{0}\psi_{m}  &  =\mathbf{1}\psi_{m}=\sum\limits_{i\in\mathbb{Z}}E_{i,i}%
\psi_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{1}=\sum
\limits_{i\in\mathbb{Z}}E_{i,i}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\underbrace{\widehat{\rho}\left(
E_{i,i}\right)  }_{\substack{=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  \\\text{(by the definition of }\widehat{\rho}\text{)}}%
}\underbrace{\psi_{m}}_{=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U\psi_{m}=\widehat{\rho}\left(
U\right)  \psi_{m}\text{ for every }U\in\mathfrak{a}_{\infty}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  }_{=\rho\left(  E_{i,i}\right)  }\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  }_{=\rho\left(  E_{i,i}\right)  -1}\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\rho\left(  E_{i,i}\right)
\cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...+\sum\limits_{\substack{i\in
\mathbb{Z};\\i\leq0}}\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot
v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge....
\end{align*}
\par
Now, we distinguish between two cases:
\par
\textit{Case 1:} We have $m\geq0$.
\par
\textit{Case 2:} We have $m<0$.
\par
In Case 1, we have%
\begin{align*}
a_{0}\psi_{m}  &  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\rho\left(
E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...+\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}\left(  \rho\left(  E_{i,i}%
\right)  -1\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}\underbrace{\rho\left(
E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...}%
_{\substack{=0\\\text{(since }i\text{ does not appear in the }%
m\text{-degression }\left(  m,m-1,m-2,...\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq
m}}\underbrace{\rho\left(  E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...}_{\substack{=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge
...\\\text{(since }i\text{ appears in the }m\text{-degression }\left(
m,m-1,m-2,...\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\underbrace{\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot
v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...}_{\substack{=0\\\text{(since
}i\text{ appears in the }m\text{-degression }\left(  m,m-1,m-2,...\right)
\\\text{and thus we have }\rho\left(  E_{i,i}\right)  \cdot v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...=v_{m}\wedge v_{m-1}\wedge v_{m-2}%
\wedge...\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since we are in Case 1, so that }%
m\geq0\right) \\
&  =\underbrace{\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}0}%
_{=0}+\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}\underbrace{v_{m}%
\wedge v_{m-1}\wedge v_{m-2}\wedge...}_{=\psi_{m}}+\underbrace{\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}0}_{=0}=\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}\psi_{m}=m\psi_{m}.
\end{align*}
Hence, $a_{0}\psi_{m}=m\psi_{m}$ is proven in Case 1. In Case 2, the proof of
$a_{0}\psi_{m}=m\psi_{m}$ is similar (but instead of splitting the
$\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}$ sum into a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}$ and a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}$ sum, we must now split
the $\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}$ sum into a
$\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0;\ i>m}}$ and a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0;\ i\leq m}}$ sum). Thus, $a_{0}%
\psi_{m}=m\psi_{m}$ holds in both cases 1 and 2. In other words, the proof of
$a_{0}\psi_{m}=m\psi_{m}$ is complete.}.

Hence, Lemma \ref{lem.V=F.A.gr} (applied to $m$ and $\wedge^{\dfrac{\infty}%
{2},m}V$ instead of $\mu$ and $V$) yields that there exists a $\mathbb{Z}%
$-graded homomorphism $\widetilde{\sigma}_{m}:F_{m}\rightarrow\wedge
^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such that $\widetilde{\sigma
}_{m}\left(  1\right)  =\psi_{m}$. (An alternative way to prove the existence
of this $\widetilde{\sigma}_{m}$ would be to apply Lemma \ref{lem.singvec},
making use of the fact (Proposition \ref{prop.fockverma.A}) that $F_{m}$ is a
Verma module for $\mathcal{A}$.)

This $\widetilde{\sigma}_{m}$ is injective (since $F_{m}$ is irreducible) and
$\mathbb{Z}$-graded. Hence, for every integer $d\geq0$, it induces a
homomorphism from $F_{m}\left[  -d\right]  $ to $\left(  \wedge^{\dfrac
{\infty}{2},m}V\right)  \left[  -d\right]  $. This induced homomorphism must
be injective (since $\widetilde{\sigma}_{m}$ was injective), and thus is an
isomorphism (since the vector spaces $F_{m}\left[  -d\right]  $ and $\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  $ have the same
dimension (by (\ref{pf.wedge.fock.dim})) and are both finite-dimensional).
Since this holds for every integer $d\geq0$, this yields that
$\widetilde{\sigma}_{m}$ itself must be an isomorphism. This proves
Proposition \ref{prop.wedge.fock} \textbf{(a)}.

Proposition \ref{prop.wedge.fock} \textbf{(b)} follows from Proposition
\ref{prop.wedge.fock} \textbf{(a)} due to Proposition \ref{prop.resc}
\textbf{(b)}.

Note that Proposition \ref{prop.wedge.fock} is surprising: It gives an
isomorphism between a space of polynomials (the Fock space $F_{m}$, also
called a \textit{bosonic space}) and a space of wedge products (the space
$\wedge^{\dfrac{\infty}{2},m}V$, also called a \textit{fermionic space});
isomorphisms like this are unheard of in finite-dimensional contexts.

\begin{definition}
We write $\mathcal{B}^{\left(  m\right)  }$ for the $\mathcal{A}$-module
$\widetilde{F}_{m}$. We write $\mathcal{B}$ for the $\mathcal{A}$-module
$\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }=\bigoplus\limits_{m}%
\widetilde{F}_{m}$. We write $\mathcal{F}^{\left(  m\right)  }$ for the
$\mathcal{A}$-module $\wedge^{\dfrac{\infty}{2},m}V$. We write $\mathcal{F}$
for the $\mathcal{A}$-module $\bigoplus\limits_{m}\mathcal{F}^{\left(
m\right)  }$.

The isomorphism $\sigma_{m}$ (constructed in Proposition \ref{prop.wedge.fock}
\textbf{(b)}) is thus an isomorphism $\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{F}^{\left(  m\right)  }$. We write $\sigma$ for the
$\mathcal{A}$-module isomorphism $\bigoplus\limits_{m}\sigma_{m}%
:\mathcal{B}\rightarrow\mathcal{F}$. This $\sigma$ is called the
\textit{Boson-Fermion Correspondence}.
\end{definition}

Note that we can do the same for the Virasoro algebra: If $M_{\lambda}$ is
irreducible, then the homomorphism $M_{\lambda}\rightarrow\wedge
^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is an isomorphism. And we know that
$\operatorname*{Vir}$ is nondegenerate, so $M_{\lambda}$ is irreducible for
Weil-generic $\lambda$.

\begin{corollary}
For generic $\alpha$ and $\beta$, the $\operatorname*{Vir}$-module
$\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is irreducible.
\end{corollary}

But now, back to the Boson-Fermion Correspondence:

Both $\mathcal{B}$ and $\mathcal{F}$ are $\mathcal{A}$-modules, and
Proposition \ref{prop.wedge.fock} \textbf{(b)} showed us that they are
isomorphic as such through the isomorphism $\sigma:\mathcal{B}\rightarrow
\mathcal{F}$. However, $\mathcal{F}$ is also an $\mathfrak{a}_{\infty}%
$-module, whereas $\mathcal{B}$ is not. But of course, with the isomorphism
$\sigma$ being given, we can transfer the $\mathfrak{a}_{\infty}$-module
structure from $\mathcal{F}$ to $\mathcal{B}$. The same can be done with the
$\mathfrak{gl}_{\infty}$-module structure. Let us explicitly define these:

\begin{definition}
\textbf{(a)} We make $\mathcal{B}$ into an $\mathfrak{a}_{\infty}$-module by
transferring the $\mathfrak{a}_{\infty}$-module structure on $\mathcal{F}$
(given by the map $\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow
\operatorname*{End}\mathcal{F}$) to $\mathcal{B}$ via the isomorphism
$\sigma:\mathcal{B}\rightarrow\mathcal{F}$. Note that the $\mathcal{A}$-module
$\mathcal{B}$ is a restriction of the $\mathfrak{a}_{\infty}$-module
$\mathcal{B}$ (since the $\mathcal{A}$-module $\mathcal{F}$ is the restriction
of the $\mathfrak{a}_{\infty}$-module $\mathcal{F}$). We denote the
$\mathfrak{a}_{\infty}$-module structure on $\mathcal{B}$ by $\widehat{\rho
}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\mathcal{B}$.

\textbf{(b)} We make $\mathcal{B}$ into a $\mathfrak{gl}_{\infty}$-module by
transferring the $\mathfrak{gl}_{\infty}$-module structure on $\mathcal{F}$
(given by the map $\rho:\mathfrak{gl}_{\infty}\rightarrow\operatorname*{End}%
\mathcal{F}$) to $\mathcal{B}$ via the isomorphism $\sigma:\mathcal{B}%
\rightarrow\mathcal{F}$. We denote the $\mathfrak{gl}_{\infty}$-module
structure on $\mathcal{B}$ by $\rho:\mathfrak{gl}_{\infty}\rightarrow
\operatorname*{End}\mathcal{B}$.
\end{definition}

How do we describe these module structures on $\mathcal{B}$ explicitly (i. e.,
in formulas?) This question is answered using the so-called \textit{vertex
operator construction}.

But first, some easier things:

\begin{definition}
\label{def.createdestroy}Let $m\in\mathbb{Z}$. Let $i\in\mathbb{Z}$.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}}:\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}%
^{\left(  m+1\right)  }$ by%
\[
\widehat{v_{i}}\cdot\psi=v_{i}\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all
}\psi\in\mathcal{F}^{\left(  m\right)  }.
\]
Here, $v_{i}\wedge\psi$ is formally defined as follows: Write $\psi$ as a
$\mathbb{C}$-linear combination of (well-defined) semiinfinite wedge products
$b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ (for instance, elementary
semiinfinite wedges); then, $v_{i}\wedge\psi$ is obtained by replacing each
such product $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ by $v_{i}\wedge
b_{0}\wedge b_{1}\wedge b_{2}\wedge...$.

\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}}:\mathcal{F}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m-1\right)  }$ as follows:

For every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, we let
$\overset{\vee}{v_{i}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\wedge v_{i_{j-1}}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},i_{2},...\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $i_{k}=i$. Thus, the map $\overset{\vee
}{v_{i}}$ is defined on all elementary semiinfinite wedges; we extend this to
a map $\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(
m-1\right)  }$ by linearity.
\end{definition}

Note that the somewhat unwieldy definition of $\overset{\vee}{v_{i}}$ can be
slightly improved: While it only gave a formula for $m$-degressions, it is
easy to see that the same formula holds for straying $m$-degressions:

\begin{proposition}
Let $m\in\mathbb{Z}$ and $i\in\mathbb{Z}$. Let $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ be a straying $m$-degression which has no two equal
elements. Then,%
\begin{align*}
&  \overset{\vee}{v_{i}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\wedge v_{i_{j-1}}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},i_{2},...\right\}
\end{array}
\right.  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $i_{k}=i$.
\end{proposition}

These operators satisfy the relations%
\begin{align*}
\widehat{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\widehat{v_{i}}  &
=0,\ \ \ \ \ \ \ \ \ \ \overset{\vee}{v_{i}}\overset{\vee}{v_{j}%
}+\overset{\vee}{v_{j}}\overset{\vee}{v_{i}}=0,\\
\overset{\vee}{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\overset{\vee}{v_{i}}  &
=\delta_{i,j}%
\end{align*}
for all $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$.

\begin{definition}
For every $i\in\mathbb{Z}$, define $\xi_{i}=\widehat{v_{i}}$ and $\xi
_{i}^{\ast}=\overset{\vee}{v_{i}}$.
\end{definition}

Then, all $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $\rho\left(
E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}$ and
\[
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  .
\]


The $\xi_{i}$ and $\xi_{i}^{\ast}$ are called \textit{fermionic operators}.

So what are the $\xi_{i}$ in terms of $a_{j}$ ?

\subsection{The vertex operator construction}

We identify the space $\mathbb{C}\left[  z,z^{-1},x_{1},x_{2},...\right]
=\bigoplus\limits_{m}z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ with
$\mathcal{B}=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }$ by means of
identifying $z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ with
$\mathcal{B}^{\left(  m\right)  }$ for every $m\in\mathbb{Z}$ (the
identification being made through the map%
\begin{align*}
\mathcal{B}^{\left(  m\right)  }  &  \rightarrow z^{m}\mathbb{C}\left[
x_{1},x_{2},...\right]  ,\\
p  &  \mapsto z^{m}\cdot p
\end{align*}
).

Note also that $z$ (that is, multiplication by $z$) is an isomorphism of
$\mathcal{A}_{0}$-modules, but not of $\mathcal{A}$-modules.

The Boson-Fermion correspondence goes like this:%
\[
\mathcal{F}=\bigoplus\limits_{m}\mathcal{F}^{\left(  m\right)  }%
\overset{\sigma=\bigoplus\limits_{m}\sigma_{m}}{\leftarrow}\mathcal{B}%
=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }.
\]
On $\mathcal{F}$ there are operators $\widehat{v_{i}}=\xi_{i}$, $\overset{\vee
}{v_{i}}=\xi_{i}^{\ast}$, $\rho\left(  E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}%
$, \newline$\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  $. By conjugating with the Boson-Fermion correspondence $\sigma$,
these operators give rise to operators on $\mathcal{B}$. How do the latter
operators look like?

\begin{definition}
\label{def.euler.XGamma}Introduce the quantum fields%
\begin{align*}
X\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}\in\left(
\operatorname*{End}\mathcal{F}\right)  \left[  \left[  u,u^{-1}\right]
\right]  ,\\
X^{\ast}\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}^{\ast
}u^{-n}\in\left(  \operatorname*{End}\mathcal{F}\right)  \left[  \left[
u,u^{-1}\right]  \right]  ,\\
\Gamma\left(  u\right)   &  =\sigma^{-1}\circ X\left(  u\right)  \circ
\sigma\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u,u^{-1}\right]  \right]  ,\\
\Gamma^{\ast}\left(  u\right)   &  =\sigma^{-1}\circ X^{\ast}\left(  u\right)
\circ\sigma\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u,u^{-1}\right]  \right]  .
\end{align*}
Note that $\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$ is to be read as
``conjugate every term of the power series $X\left(  u\right)  $ by $\sigma
$''; in other words, $\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$ means
$\sum\limits_{n\in\mathbb{Z}}\left(  \sigma^{-1}\circ\xi_{n}\circ
\sigma\right)  u^{n}$.
\end{definition}

Recall that $\xi_{n}=\widehat{v_{n}}$ sends $\mathcal{F}^{\left(  m\right)  }$
to $\mathcal{F}^{\left(  m+1\right)  }$ for any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$. Thus, every term of the power series $X\left(  u\right)
=\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}$ sends $\mathcal{F}^{\left(
m\right)  }$ to $\mathcal{F}^{\left(  m+1\right)  }$ for any $m\in\mathbb{Z}$.
Abusing notation, we will abbreviate this fact by saying that $X\left(
u\right)  :\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(
m+1\right)  }$ for any $m\in\mathbb{Z}$. Similarly, $X^{\ast}\left(  u\right)
:\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$ (since $\xi_{n}^{\ast}=\overset{\vee}{v_{n}}$
sends $\mathcal{F}^{\left(  m\right)  }$ to $\mathcal{F}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$ and $n\in\mathbb{Z}$). As a consequence,
$\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{B}^{\left(  m+1\right)  }$ and $\Gamma^{\ast}\left(  u\right)
:\mathcal{B}^{\left(  m\right)  }\rightarrow\mathcal{B}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$.

Now, here is how we can describe $\Gamma\left(  u\right)  $ and $\Gamma^{\ast
}\left(  u\right)  $ (and therefore the operators $\sigma^{-1}\circ\xi
_{n}\circ\sigma$ and $\sigma^{-1}\circ\xi_{n}^{\ast}\circ\sigma$) in terms of
$\mathcal{B}$:

\begin{theorem}
\label{thm.euler}Let $m\in\mathbb{Z}$. On $\mathcal{B}^{\left(  m\right)  }$,
we have%
\begin{align*}
\Gamma\left(  u\right)   &  =u^{m+1}z\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  ;\\
\Gamma^{\ast}\left(  u\right)   &  =u^{-m}z^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
Here, $\exp A$ means $1+A+\dfrac{A^{2}}{2!}+\dfrac{A^{3}}{3!}+...$ for any $A$
for which this series makes any sense.
\end{theorem}

Let us explain what we mean by the products $\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  $ and $\exp\left(  -\sum\limits_{j>0}\dfrac{a_{-j}%
}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  $ in Theorem \ref{thm.euler}. Why do these products (which are
products of exponentials of infinite sums) make any sense? This is easily answered:

\begin{itemize}
\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)  $ is
well-defined and is valued in $\mathcal{B}^{\left(  m\right)  }\left[
u^{-1}\right]  $. (In fact, if we blindly expand%
\begin{align*}
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)   &
=\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  ^{\ell}\\
&  =\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\left(  -1\right)  ^{\ell
}\sum\limits_{j_{1},j_{2},...,j_{\ell}\text{ positive integers}}%
\dfrac{a_{j_{1}}a_{j_{2}}...a_{j_{\ell}}}{j_{1}j_{2}...j_{\ell}}u^{-\left(
j_{1}+j_{2}+...+j_{\ell}\right)  },
\end{align*}
and apply every term of the resulting power series to $v$, then (for fixed
$v$) only finitely many of these terms yield a nonzero result, since $v$ is a
polynomial and thus has finite degree, whereas each $a_{j}$ lowers degree by
$j$.)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  v$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (In fact, we have just shown that $\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)  \in\mathcal{B}^{\left(
m\right)  }\left[  u^{-1}\right]  $; therefore, applying $\exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \in\left(  \operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u\right]  \right]  $ to this gives a well-defined power series in
$\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $ (because
if $\mathfrak{A}$ is an algebra and $\mathfrak{M}$ is an $\mathfrak{A}%
$-module, then the application of a power series in $\mathfrak{A}\left[
\left[  u\right]  \right]  $ to an element of $\mathfrak{M}\left[
u^{-1}\right]  $ gives a well-defined element of $\mathfrak{M}\left(  \left(
u\right)  \right)  $).)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
-\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  v$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (This is proven similarly.)
\end{itemize}

Thus, the formulas of Theorem \ref{thm.euler} make sense.

\begin{remark}
Here is some of physicists' intuition for the right hand sides of the
equations in Theorem \ref{thm.euler}. [Note: I (=Darij) don't fully understand
it, so don't expect me to explain it well.]

Consider the quantum field $a\left(  u\right)  =\sum\limits_{j\in\mathbb{Z}%
}a_{j}u^{-j-1}\in U\left(  \mathcal{A}\right)  \left[  \left[  u,u^{-1}%
\right]  \right]  $ defined in Section \ref{subsect.quantumfields}. Let us
work on an informal level, and pretend that integration of series in $U\left(
\mathcal{A}\right)  \left[  \left[  u,u^{-1}\right]  \right]  $ is
well-defined and behaves similar to that of functions on $\mathbb{R}$. Then,
$\int a\left(  u\right)  du=-\sum\limits_{j\neq0}\dfrac{a_{j}}{j}u^{-j}%
+a_{0}\log u$. Exponentiating this \textbf{``in the normal ordering''} (this
means we expand the series $\exp\left(  -\sum\limits_{j\neq0}\dfrac{a_{j}}%
{j}u^{-j}+a_{0}\log u\right)  $ and replace all products by their normal
ordered versions, i. e., shovel all $a_{m}$ with $m<0$ to the left and all
$a_{m}$ with $m>0$ to the right), we obtain%
\begin{align*}
&  \left.  :\exp\left(  \int a\left(  u\right)  du\right)  :\right.  =\left.
:\exp\left(  -\sum\limits_{j\neq0}\dfrac{a_{j}}{j}u^{-j}+a_{0}\log u\right)
:\right. \\
&  =\exp\left(  \underbrace{-\sum\limits_{j<0}\dfrac{a_{j}}{j}u^{-j}}%
_{=\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}}\right)  \cdot\exp\left(  a_{0}\log
u\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right) \\
&  =\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  a_{0}\log u\right)  \cdot\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
But for every $m\in\mathbb{Z}$, we have%
\begin{align*}
&  \Gamma\left(  u\right) \\
&  =u^{m+1}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.euler}}\right) \\
&  =uz\cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\underbrace{u^{m}}_{\substack{=\exp\left(  m\log u\right)  =\exp\left(
a_{0}\log u\right)  \\\text{(since }a_{0}\text{ acts by }m\text{ on
}\mathcal{B}^{\left(  m\right)  }\text{,}\\\text{and thus }\exp\left(
a_{0}\log u\right)  =\exp\left(  m\log u\right)  \text{ on }\mathcal{B}%
^{\left(  m\right)  }\text{)}}}\cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right) \\
&  =uz\cdot\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  a_{0}\log u\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  }_{=\left.  :\exp\left(  \int
a\left(  u\right)  du\right)  :\right.  }\\
&  =uz\cdot\left.  :\exp\left(  \int a\left(  u\right)  du\right)  :\right.  .
\end{align*}
Since the right hand side of this equality does not depend on $m$, we thus
have $\Gamma\left(  u\right)  =uz\left.  :\exp\left(  \int a\left(  u\right)
du\right)  :\right.  $.

Hence, we have rewritten half of the statement of Theorem \ref{thm.euler} as
the identity $\Gamma\left(  u\right)  =uz\left.  :\exp\left(  \int a\left(
u\right)  du\right)  :\right.  $ (which holds on all of $\mathcal{B}$).
Similarly, the other half of Theorem \ref{thm.euler} rewrites as the identity
$\Gamma^{\ast}\left(  u\right)  =z^{-1}\left.  :\exp\left(  -\int a\left(
u\right)  du\right)  :\right.  $.

This is reminiscent of Euler's formula $y=c\exp\left(  \int a\left(  u\right)
du\right)  $ for the solution $y$ of the differential equation $y^{\prime}=ay$.
\end{remark}

Before we can show Theorem \ref{thm.euler}, we state a lemma about the action
of $\mathcal{A}$ on $\mathcal{B}$:

\begin{lemma}
\label{lem.euler.aGamma}For every $j\in\mathbb{Z}$, we have $\left[
a_{j},\Gamma\left(  u\right)  \right]  =u^{j}\Gamma\left(  u\right)  $ and
$\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]  =-u^{j}\Gamma^{\ast
}\left(  u\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.euler.aGamma}.} Let us prove the first
formula. Let $j\in\mathbb{Z}$.

On the fermionic space $\mathcal{F}$, the element $a_{j}\in\mathcal{A}$ acts
as%
\begin{align*}
\widehat{\rho}\left(  T^{j}\right)   &  =\sum\limits_{i}\widehat{\rho}\left(
E_{i,i+j}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }T^{j}%
=\sum\limits_{i\in\mathbb{Z}}E_{i,i+j}\right) \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.
\end{align*}
(since $\widehat{\rho}\left(  E_{i,i+j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.  $ for every $i\in\mathbb{Z}$). Hence, on $\mathcal{F}$, we have%
\begin{align*}
\left[  a_{j},X\left(  u\right)  \right]   &  =\left[  \sum\limits_{i}\left\{
%
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.  ,X\left(  u\right)  \right] \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\left[  \xi_{i}\xi_{i+j}^{\ast}-1,X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }i\leq0,\\
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and }i\leq0
\end{array}
\right. \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }i\leq0,\\
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and }i\leq0
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  \xi_{i}\xi_{i+j}^{\ast
}-1,X\left(  u\right)  \right]  =\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(
u\right)  \right]  \right) \\
&  =\sum\limits_{i}\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
=\sum\limits_{i}\left[  \xi_{i}\xi_{i+j}^{\ast},\sum\limits_{m}\xi_{m}%
u^{m}\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }X\left(  u\right)
=\sum\limits_{m}\xi_{m}u^{m}\right) \\
&  =\sum\limits_{i}\sum\limits_{m}\underbrace{\left[  \xi_{i}\xi_{i+j}^{\ast
},\xi_{m}\right]  }_{\substack{=\delta_{m,i+j}\xi_{i}\\\text{(this is easy to
check)}}}u^{m}=\sum\limits_{i}\sum\limits_{m}\delta_{m,i+j}\xi_{i}u^{m}\\
&  =\sum\limits_{m}\xi_{m-j}u^{m}=u^{j}\underbrace{\sum\limits_{m}\xi
_{m-j}u^{m-j}}_{=X\left(  u\right)  }=u^{j}X\left(  u\right)  .
\end{align*}
Conjugating this equation by $\sigma$, we obtain $\left[  a_{j},\Gamma\left(
u\right)  \right]  =u^{j}\Gamma\left(  u\right)  $. Similarly, we can prove
$\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]  =-u^{j}\Gamma^{\ast
}\left(  u\right)  $. Lemma \ref{lem.euler.aGamma} is proven.

\textit{Proof of Theorem \ref{thm.euler}.} Define an element $\Gamma
_{+}\left(  u\right)  $ of the $\mathbb{C}$-algebra $\left(
\operatorname*{End}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]
$ by $\Gamma_{+}\left(  u\right)  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  $. Then,%
\begin{align}
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\label{pf.euler.1}\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &  =u^{i}\Gamma_{+}\left(
u\right)  \ \ \ \ \ \ \ \ \ \ \text{if }i<0. \label{pf.euler.2}%
\end{align}
In fact, (\ref{pf.euler.1}) is trivial (because when $i\geq0$, the element
$a_{i}$ commutes with $a_{j}$ for every $j>0$, and thus also commutes with
$\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $). To prove
(\ref{pf.euler.2}), it is enough to show that $\left[  a_{i},\exp\left(
-\dfrac{a_{-i}}{-i}u^{i}\right)  \right]  =u^{i}\exp\left(  -\dfrac{a_{-i}%
}{-i}u^{i}\right)  $ (since we can write $\Gamma_{+}\left(  u\right)  $ in the
form
\[
\Gamma_{+}\left(  u\right)  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right)  =\prod\limits_{j>0}\exp\left(  -\dfrac{a_{j}}{j}%
u^{-j}\right)  ,
\]
and it is clear that $a_{i}$ commutes with all terms $-\dfrac{a_{j}}{j}u^{-j}$
for $j\neq-i$). But this is easily checked using the fact that $\left[
a_{i},a_{-i}\right]  =i$ and Lemma \ref{lem.powerseries1} (applied to
$K=\mathbb{Q}$, $R=\left(  \operatorname*{End}\mathcal{B}\right)  \left[
\left[  u^{-1}\right]  \right]  $, $\alpha=a_{i}$, $\beta=a_{-i}$ and
$P=\exp\left(  -\dfrac{X}{-i}u^{i}\right)  $). This completes the proof of
(\ref{pf.euler.2}).

Since $\Gamma_{+}\left(  u\right)  $ is an invertible power series in $\left(
\operatorname*{End}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]
$ (because the constant term of $\Gamma_{+}\left(  u\right)  $ is $1$), it
makes sense to speak of the power series $\Gamma_{+}\left(  u\right)  ^{-1}%
\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u^{-1}\right]  \right]  $. From (\ref{pf.euler.1}) and (\ref{pf.euler.2}), we
can derive the formulas%
\begin{align}
\left[  a_{i},\Gamma_{+}\left(  u\right)  ^{-1}\right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\label{pf.euler.1inv}\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  ^{-1}\right]   &  =-u^{i}\Gamma
_{+}\left(  u\right)  ^{-1}\ \ \ \ \ \ \ \ \ \ \text{if }i<0
\label{pf.euler.2inv}%
\end{align}
(using the standard fact that $\left[  \alpha,\beta^{-1}\right]  =-\beta
^{-1}\left[  \alpha,\beta\right]  \beta^{-1}$ for any two elements $\alpha$
and $\beta$ of a ring such that $\beta$ is invertible).

Now define a map $\Delta\left(  u\right)  :\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$ by $\Delta\left(  u\right)  =\Gamma\left(  u\right)  \Gamma_{+}\left(
u\right)  ^{-1}z^{-1}$. Let us check why this definition makes sense:

\begin{itemize}
\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, we have $z^{-1}%
v\in\mathcal{B}^{\left(  m-1\right)  }$, and the term $\Gamma_{+}\left(
u\right)  ^{-1}z^{-1}v$ is well-defined and is valued in $\mathcal{B}^{\left(
m-1\right)  }\left[  u^{-1}\right]  $.\ \ \ \ \footnote{\textit{Proof.} Recall
that $\mathcal{A}$ is a $\mathbb{Z}$-graded Lie algebra, and that
$\mathcal{B}$ is a $\mathbb{Z}$-graded $\mathcal{A}$-module concentrated in
nonpositive degrees. Let us (for this single proof!) change the $\mathbb{Z}%
$-gradings on both $\mathcal{A}$ and $\mathcal{B}$ to their inverses (i. e.,
switch $\mathcal{A}\left[  N\right]  $ with $\mathcal{A}\left[  -N\right]  $
for every $N\in\mathbb{Z}$, and switch $\mathcal{B}\left[  N\right]  $ with
$\mathcal{B}\left[  -N\right]  $ for every $N\in\mathbb{Z}$); then,
$\mathcal{A}$ remains still a $\mathbb{Z}$-graded Lie algebra, but
$\mathcal{B}$ is now a $\mathbb{Z}$-graded $\mathcal{A}$-module concentrated
in nonnegative degrees. Moreover, $\mathcal{B}$ is actually a $\mathbb{Z}%
$-graded $\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}$-module
concentrated in nonnegative degrees.
\par
The power series $\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\in\left(
\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}\right)  \left[
\left[  u^{-1}\right]  \right]  $ is now equigraded (since our modified
grading on $\mathcal{A}$ has the property that $\deg\left(  a_{j}\right)
=-j$), so that the power series $\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right)  \in\left(  \operatorname*{End}\nolimits_{\operatorname{hg}%
}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $ is equigraded
as well (because a consequence of Proposition \ref{prop.equigraded.basics}
\textbf{(b)} is that whenever the exponential of an equigraded power series is
well-defined, this exponential is also equigraded). Since%
\[
\Gamma_{+}\left(  u\right)  ^{-1}=\left(  \exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  \right)  ^{-1}=\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)
\]
(since Corollary \ref{cor.exp(-w)} (applied to $R=\left(  \operatorname*{End}%
\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $, $I=\left(
\text{the ideal of }R\text{ consisting of all power series with constant term
}1\right)  $, and $\gamma=-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}$) yields
$\left(  \exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)
\cdot\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\right)  =1$), this rewrites as follows: The power series $\Gamma_{+}\left(
u\right)  ^{-1}\in\left(  \operatorname*{End}\nolimits_{\operatorname{hg}%
}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $ is equigraded.
\par
Therefore, Proposition \ref{prop.equigraded.fx} \textbf{(c)} (applied to
$\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}$, $\mathcal{B}$,
$\Gamma_{+}\left(  u\right)  ^{-1}$ and $z^{-1}v$ instead of $A$, $M$, $f$ and
$x$) yields that $\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}v$ is a well-defined
element of $\mathcal{B}^{\left(  m-1\right)  }\left[  u^{-1}\right]  $, qed.}

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\Gamma\left(
u\right)  \Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$.\ \ \ \ \footnote{\textit{Proof.} We have just shown that $\Gamma_{+}\left(
u\right)  ^{-1}z^{-1}v\in\mathcal{B}^{\left(  m-1\right)  }\left[
u^{-1}\right]  $. Thus, $\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}%
v\in\mathcal{B}^{\left(  m-1\right)  }\left[  u^{-1}\right]  \subseteq
\mathcal{B}\left[  u^{-1}\right]  \subseteq\mathcal{B}\left[  u,u^{-1}\right]
$.
\par
Recall that $\mathcal{A}$ is a $\mathbb{Z}$-graded Lie algebra, and that
$\mathcal{B}$ and $\mathcal{F}$ are $\mathbb{Z}$-graded $\mathcal{A}$-modules
concentrated in nonpositive degrees. Let us (for this single proof!) change
the $\mathbb{Z}$-gradings on all of $\mathcal{A}$, $\mathcal{B}$ and
$\mathcal{F}$ to their inverses (i. e., switch $\mathcal{A}\left[  N\right]  $
with $\mathcal{A}\left[  -N\right]  $ for every $N\in\mathbb{Z}$, and switch
$\mathcal{B}\left[  N\right]  $ with $\mathcal{B}\left[  -N\right]  $ for
every $N\in\mathbb{Z}$, and switch $\mathcal{F}\left[  N\right]  $ with
$\mathcal{F}\left[  -N\right]  $ for every $N\in\mathbb{Z}$); then,
$\mathcal{A}$ remains still a $\mathbb{Z}$-graded Lie algebra, but
$\mathcal{B}$ and $\mathcal{F}$ now are $\mathbb{Z}$-graded $\mathcal{A}%
$-modules concentrated in nonnegative degrees. Moreover, $\mathcal{B}$ is
actually a $\mathbb{Z}$-graded $\operatorname*{End}%
\nolimits_{\operatorname{hg}}\mathcal{B}$-module concentrated in nonnegative
degrees, and $\mathcal{F}$ is a $\mathbb{Z}$-graded $\operatorname*{End}%
\nolimits_{\operatorname{hg}}\mathcal{F}$-module concentrated in nonnegative
degrees.
\par
It is easy to see (from the definition of $X\left(  u\right)  $) that
$X\left(  u\right)  \in\left(  \operatorname*{End}\nolimits_{\operatorname{hg}%
}\mathcal{F}\right)  \left[  \left[  u,u^{-1}\right]  \right]  $ is
equigraded. As a consequence, $\Gamma\left(  u\right)  \in\left(
\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}\right)  \left[
\left[  u,u^{-1}\right]  \right]  $ is equigraded (since $\Gamma\left(
u\right)  =\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$). Therefore,
Proposition \ref{prop.equigraded.fx} \textbf{(b)} (applied to
$\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}$, $\mathcal{B}$,
$\Gamma\left(  u\right)  $ and $\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}v$
instead of $A$, $M$, $f$ and $x$) yields that $\Gamma\left(  u\right)
\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$ is a well-defined element of
$\mathcal{B}\left(  \left(  u\right)  \right)  $. This element actually lies
in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $
(since $\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m-1\right)  }%
\rightarrow\mathcal{B}^{\left(  m\right)  }$), qed.}
\end{itemize}

Since $\left[  a_{0},z\right]  =z$ and $\left[  a_{i},z\right]  =0$ for all
$i\neq0$, we have%
\begin{equation}
\left[  a_{i},\Delta\left(  u\right)  \right]  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\leq0;\\
u^{i}\Delta\left(  u\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }i>0
\end{array}
\right.  \label{pf.euler.Del}%
\end{equation}
(due to (\ref{pf.euler.1inv}), (\ref{pf.euler.2inv}) and Lemma
\ref{lem.euler.aGamma}). In particular, $\left[  a_{i},\Delta\left(  u\right)
\right]  =0$ if $i\leq0$. Thus, $\Delta\left(  u\right)  $ is a homomorphism
of $\mathcal{A}_{-}$-modules, where $\mathcal{A}_{-}$ is the Lie subalgebra
$\left\langle a_{-1},a_{-2},a_{-3},...\right\rangle $ of $\mathcal{A}$. (Of
course, this formulation means that every term of the formal power series
$\Delta\left(  u\right)  $ is a homomorphism of $\mathcal{A}_{-}$-modules.)

Consider now the element $z^{m}$ of $z^{m}\mathbb{C}\left[  x_{1}%
,x_{2},...\right]  =\mathcal{B}^{\left(  m\right)  }=\widetilde{F}_{m}$. Also,
consider the element $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ of
$\wedge^{\dfrac{\infty}{2},m}V=\mathcal{F}^{\left(  m\right)  }$ defined in
Definition \ref{def.psim}. By the definition of $\sigma_{m}$, we have
$\sigma_{m}\left(  z^{m}\right)  =\psi_{m}$. (In fact, $z^{m}$ is what was
denoted by $1$ in Proposition \ref{prop.wedge.fock}.)

From Lemma \ref{lem.F.P1=P}, it is clear that the Fock module $F$ is generated
by $1$ as an $\mathcal{A}_{-}$-module (since $\mathcal{A}_{-}=\left\langle
a_{-1},a_{-2},a_{-3},...\right\rangle $). Since there exists an $\mathcal{A}%
_{-}$-module isomorphism $F\rightarrow\widetilde{F}$ which sends $1$ to $1$
(in fact, the map $\operatorname*{resc}$ of Proposition \ref{prop.resc} is
such an isomorphism), this yields that $\widetilde{F}$ is generated by $1$ as
an $\mathcal{A}_{-}$-module. Since there exists an $\mathcal{A}_{-}$-module
isomorphism $\widetilde{F}\rightarrow\widetilde{F}_{m}$ which sends $1$ to
$z^{m}$ (in fact, multiplication by $z^{m}$ is such an isomorphism), this
yields that $\widetilde{F}_{m}$ is generated by $z^{m}$ as an $\mathcal{A}%
_{-}$-module. Consequently, the $m$-th term of the power series $\Delta\left(
u\right)  $ is completely determined by $\left(  \Delta\left(  u\right)
\right)  \left(  z^{m}\right)  $ (because we know that $\Delta\left(
u\right)  $ is a homomorphism of $\mathcal{A}_{-}$-modules). So let us compute
$\left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right)  $. Since
$\Delta\left(  u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $, we know
that $\left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right)  $ is an
element of $\underbrace{\mathcal{B}^{\left(  m\right)  }}_{=z^{m}%
\widetilde{F}}\left(  \left(  u\right)  \right)  =z^{m}\widetilde{F}\left(
\left(  u\right)  \right)  $. In other words, $\left(  \Delta\left(  u\right)
\right)  \left(  z^{m}\right)  $ is $z^{m}$ times a Laurent series in $u$
whose coefficients are polynomials in $x_{1},x_{2},x_{3},...$. Denote this
Laurent series by $Q$. Thus, $\left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  =z^{m}Q$.

For every $i>0$, we have
\[
a_{i}\Delta\left(  u\right)  =\Delta\left(  u\right)  a_{i}%
+\underbrace{\left[  a_{i},\Delta\left(  u\right)  \right]  }%
_{\substack{=u^{i}\Delta\left(  u\right)  \\\text{(by (\ref{pf.euler.Del}))}%
}}=\Delta\left(  u\right)  a_{i}+u^{i}\Delta\left(  u\right)  ,
\]
so that%
\begin{align*}
\left(  a_{i}\Delta\left(  u\right)  \right)  \left(  z^{m}\right)   &
=\left(  \Delta\left(  u\right)  a_{i}+u^{i}\Delta\left(  u\right)  \right)
\left(  z^{m}\right)  =\Delta\left(  u\right)  \underbrace{a_{i}z^{m}%
}_{\substack{=0\\\text{(since }a_{i}=\dfrac{\partial}{\partial x_{i}}\text{)}%
}}+u^{i}\underbrace{\left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  }_{=z^{m}Q}\\
&  =u^{i}z^{m}Q=z^{m}u^{i}Q.
\end{align*}
Since $\left(  a_{i}\Delta\left(  u\right)  \right)  \left(  z^{m}\right)
=a_{i}\underbrace{\left(  \left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  \right)  }_{=z^{m}Q}=z^{m}\underbrace{a_{i}}_{=\dfrac{\partial
}{\partial x_{i}}}Q=z^{m}\dfrac{\partial Q}{\partial x_{i}}$, this rewrites as
$z^{m}\dfrac{\partial Q}{\partial x_{i}}=z^{m}u^{i}Q$. Hence, for every $i>0$,
we have $\dfrac{\partial Q}{\partial x_{i}}=u^{i}Q$. Thus, we can write the
formal Laurent series $Q$ in the form $Q=f\left(  u\right)  \exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  $ for some Laurent series $f\left(
u\right)  \in\mathbb{C}\left(  \left(  u\right)  \right)  \ \ \ \ $%
.\footnote{This follows from Proposition \ref{prop.euler.recognizing-exp},
applied to $R=\mathbb{C}\left[  u\right]  $, $U=\mathbb{C}\left(  \left(
u\right)  \right)  $, $\left(  \alpha_{1},\alpha_{2},\alpha_{3},...\right)
=\left(  u^{1},u^{2},u^{3},...\right)  $ and $P=Q$.} Thus,
\begin{align*}
&  \left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right) \\
&  =z^{m}Q=z^{m}f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}%
u^{j}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }Q=f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \left(  z^{m}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
each }\dfrac{a_{-j}}{j}\text{ acts as multiplication by }x_{j}\text{ on
}\widetilde{F}\right)  .
\end{align*}
In other words, the two maps $\Delta\left(  u\right)  $ and $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ are
equal on $z^{m}$. Since each of these two maps is an $\mathcal{A}_{-}$-module
homomorphism\footnote{In fact, we know that $\Delta\left(  u\right)  $ is an
$\mathcal{A}_{-}$-module homomorphism, and it is clear that $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ is an
$\mathcal{A}_{-}$-module homomorphism because $\mathcal{A}_{-}$ is an abelian
Lie algebra.}, this yields that these two maps must be identical (because
$\widetilde{F}_{m}$ is generated by $z^{m}$ as an $\mathcal{A}_{-}$-module).
In other words, $\Delta\left(  u\right)  =f\left(  u\right)  \exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $. Since $\Delta\left(
u\right)  =\Gamma\left(  u\right)  \Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$,
this becomes $\Gamma\left(  u\right)  \Gamma_{+}\left(  u\right)  ^{-1}%
z^{-1}=f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  $, so that%
\begin{align}
\Gamma\left(  u\right)   &  =f\left(  u\right)  \exp\left(  \sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot z\cdot\Gamma_{+}\left(
u\right)  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot z\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Gamma_{+}\left(  u\right)
=\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)
\nonumber\\
&  =f\left(  u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \label{pf.euler.Gamma-through-f}%
\end{align}
on $\mathcal{B}^{\left(  m\right)  }$. It remains to show that $f\left(
u\right)  =u^{m+1}$.

In order to do this, we recall that
\begin{align*}
\left(  \Gamma\left(  u\right)  \right)  \left(  z^{m}\right)   &  =f\left(
u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\underbrace{\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\left(  z^{m}\right)  }_{\substack{=z^{m}\\\text{(because }a_{j}\left(
z^{m}\right)  =0\text{ for every }j>0\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.euler.Gamma-through-f})}\right) \\
&  =f\left(  u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \left(  z^{m}\right)  =f\left(  u\right)  z\exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  z^{m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since each }\dfrac{a_{-j}}{j}\text{ acts
as multiplication by }x_{j}\text{ on }\widetilde{F}\right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)
z^{m+1}.
\end{align*}
On the other hand, back on the fermionic side, for the vector $\psi_{m}%
=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$, we have%
\begin{align*}
\left(  X\left(  u\right)  \right)  \psi_{m}  &  =\sum\limits_{n\in\mathbb{Z}%
}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }X\left(  u\right)  =\sum\limits_{n\in\mathbb{Z}}\underbrace{\xi
_{n}}_{=\widehat{v_{n}}}u^{n}=\sum\limits_{n\in\mathbb{Z}}\widehat{v_{n}}%
u^{n}\right) \\
&  =\sum\limits_{\substack{n\in\mathbb{Z};\\n\leq m}%
}\underbrace{\widehat{v_{n}}\left(  \psi_{m}\right)  }%
_{\substack{=0\\\text{(since }n\leq m\text{, so that }v_{n}\\\text{appears in
}v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...=\psi_{m}\text{)}}}u^{n}%
+\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(
\psi_{m}\right)  u^{n}=\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}.
\end{align*}
Thus, $\sigma^{-1}\left(  \left(  X\left(  u\right)  \right)  \psi_{m}\right)
=\sigma^{-1}\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)  $. Compared with%
\begin{align*}
\sigma^{-1}\left(  \left(  X\left(  u\right)  \right)  \underbrace{\psi_{m}%
}_{=\sigma\left(  z^{m}\right)  }\right)   &  =\sigma^{-1}\left(  \left(
X\left(  u\right)  \right)  \left(  \sigma\left(  z^{m}\right)  \right)
\right)  =\underbrace{\left(  \sigma^{-1}\circ X\left(  u\right)  \circ
\sigma\right)  }_{=\Gamma\left(  u\right)  }\left(  z^{m}\right)  =\left(
\Gamma\left(  u\right)  \right)  \left(  z^{m}\right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)
z^{m+1},
\end{align*}
this yields $\sigma^{-1}\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)  =f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}$, so that%
\begin{equation}
\sigma\left(  f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}%
u^{j}\right)  z^{m+1}\right)  =\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}. \label{pf.euler.compare}%
\end{equation}
We want to find $f\left(  u\right)  $ by comparing the sides of this equation.
In order to do this, we recall that each space $\mathcal{B}^{\left(  i\right)
}$ is graded; hence, $\mathcal{B}$ (being the direct sum of the $\mathcal{B}%
^{\left(  i\right)  }$) is also graded (by taking the direct sum of all the
gradings). Also, each space $\mathcal{F}^{\left(  i\right)  }$ is graded;
hence, $\mathcal{F}$ (being the direct sum of the $\mathcal{F}^{\left(
i\right)  }$) is also graded (by taking the direct sum of all the gradings).
Since each $\sigma_{m}$ is a graded map, the direct sum $\sigma=\bigoplus
\limits_{m\in\mathbb{Z}}\sigma_{m}$ is also graded. Therefore,%
\begin{align}
&  \sigma\left(  0\text{-th homogeneous component of }f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right) \nonumber\\
&  =\left(  0\text{-th homogeneous component of }\sigma\left(  f\left(
u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right)
\right) \nonumber\\
&  =\left(  0\text{-th homogeneous component of }\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)
\label{pf.euler.compare2}%
\end{align}
(by (\ref{pf.euler.compare})). Now, for every $n\in\mathbb{Z}$ satisfying
$n\geq m+1$, the element $\widehat{v_{n}}\left(  \psi_{m}\right)  $ equals
$v_{n}\wedge v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$, and thus has degree
$-\left(  n-m-1\right)  $. Hence, for every nonpositive $i\in\mathbb{Z}$, the
$i$-th homogeneous component of the sum $\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}%
\in\mathcal{F}$ is $\widehat{v_{m+1-i}}\left(  \psi_{m}\right)  u^{m+1-i}$. In
particular, the $0$-th homogeneous component of $\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}$ is
$\widehat{v_{m+1}}\left(  \psi_{m}\right)  u^{m+1}=\psi_{m+1}u^{m+1}$ (since
$\widehat{v_{m+1}}\left(  \psi_{m}\right)  =v_{m+1}\wedge v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...=\psi_{m+1}$). Therefore,
(\ref{pf.euler.compare2}) becomes%
\begin{equation}
\sigma\left(  0\text{-th homogeneous component of }f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right)  =\psi
_{m+1}u^{m+1}. \label{pf.euler.compare3}%
\end{equation}
On the other hand, the $0$-th homogeneous component of the element $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}%
\in\mathcal{B}$ is clearly $f\left(  u\right)  z^{m+1}$ (because $\exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  =1+\left(  \text{terms involving at least
one }x_{j}\right)  $, and every $x_{j}$ lowers the degree). Thus,
(\ref{pf.euler.compare3}) becomes $\sigma\left(  f\left(  u\right)
z^{m+1}\right)  =\psi_{m+1}u^{m+1}$. Since $\sigma\left(  f\left(  u\right)
z^{m+1}\right)  =f\left(  u\right)  \underbrace{\sigma\left(  z^{m+1}\right)
}_{=\psi_{m+1}}=f\left(  u\right)  \psi_{m+1}$, this rewrites as $f\left(
u\right)  \psi_{m+1}=\psi_{m+1}u^{m+1}$, so that $f\left(  u\right)  =u^{m+1}%
$. Hence, (\ref{pf.euler.Gamma-through-f}) becomes
\begin{align*}
\Gamma\left(  u\right)   &  =\underbrace{f\left(  u\right)  }_{=u^{m+1}}%
z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right) \\
&  =u^{m+1}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m\right)  }.
\end{align*}
This proves one of the equalities of Theorem \ref{thm.euler}. The other is
proven similarly.

Theorem \ref{thm.euler} is proven.

\begin{corollary}
\label{cor.euler}Let $m\in\mathbb{Z}$. On $\mathcal{B}^{\left(  m\right)  }$,
we have%
\[
\rho\left(  \sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}u^{i}%
v^{-j}E_{i,j}\right)  =\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}u^{i}v^{-j}\xi_{i}\xi_{j}^{\ast}=X\left(  u\right)  X^{\ast}\left(  v\right)
,
\]
thus%
\begin{align*}
&  \sigma^{-1}\circ\rho\left(  \sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\\
&  =\sigma^{-1}\circ X\left(  u\right)  X^{\ast}\left(  v\right)  \circ
\sigma=\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)
\end{align*}
as linear maps from $\mathcal{B}^{\left(  m\right)  }$ to $\mathcal{B}%
^{\left(  m\right)  }\left(  \left(  u,v\right)  \right)  $.
\end{corollary}

\begin{remark}
It must be pointed out that the term%
\[
\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)
\]
only makes sense as a map from $\mathcal{B}^{\left(  m\right)  }$ to
$\mathcal{B}^{\left(  m\right)  }\left(  \left(  u,v\right)  \right)  $, but
not (for example) as a map from $\mathcal{B}^{\left(  m\right)  }$ to
$\mathcal{B}^{\left(  m\right)  }\left[  \left[  u,u^{-1},v,v^{-1}\right]
\right]  $ or as an element of $\left(  \operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u,u^{-1},v,v^{-1}\right]  \right]  $. Indeed, $1-\dfrac{v}{u}$ is a
zero-divisor in $\mathbb{C}\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]
$ (since $\left(  1-\dfrac{v}{u}\right)  \sum\limits_{k\in\mathbb{Z}}\left(
\dfrac{v}{u}\right)  ^{k}=0$), so it does not make sense, for example, to
multiply a generic element of $\mathcal{B}^{\left(  m\right)  }\left[  \left[
u,u^{-1},v,v^{-1}\right]  \right]  $ by $\dfrac{1}{1-\dfrac{v}{u}}$. An
element of $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u,v\right)
\right)  $ needs not always be a multiple of $1-\dfrac{v}{u}$, but at least
when it is, the quotient is unique.
\end{remark}

The importance of Corollary \ref{cor.euler} lies in the fact that it gives an
easy way to compute the $\rho$-action of $\mathfrak{gl}_{\infty}$ on
$\mathcal{B}^{\left(  m\right)  }$: In fact, for any $p\in\mathbb{Z}$ and
$q\in\mathbb{Z}$, the coefficient of $\sigma^{-1}\circ\rho\left(
\sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\in\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)
\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]  $ before $u^{p}v^{-q}$ is
$\sigma^{-1}\circ\rho\left(  E_{p,q}\right)  \circ\sigma$, and this is exactly
the action of $E_{p,q}$ on $\mathcal{B}^{\left(  m\right)  }$ obtained by
transferring the action $\rho$ of $\mathfrak{gl}_{\infty}$ on $\mathcal{F}%
^{\left(  m\right)  }$ to $\mathcal{B}^{\left(  m\right)  }$.

\textit{Proof of Corollary \ref{cor.euler}.} First of all, we clearly have%
\begin{align*}
\rho\left(  \sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}u^{i}%
v^{-j}E_{i,j}\right)   &  =\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}u^{i}v^{-j}\underbrace{\rho\left(  E_{i,j}\right)  }_{=\xi_{i}\xi_{j}^{\ast}%
}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}u^{i}v^{-j}\xi_{i}\xi
_{j}^{\ast}\\
&  =\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}\right)
}_{=\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}=X\left(  u\right)  }%
\underbrace{\sum\limits_{j\in\mathbb{Z}}\xi_{j}^{\ast}v^{-j}}_{=\sum
\limits_{n\in\mathbb{Z}}\xi_{n}^{\ast}v^{-n}=X^{\ast}\left(  v\right)
}=X\left(  u\right)  X^{\ast}\left(  v\right)  ,
\end{align*}
so that%
\begin{align*}
&  \sigma^{-1}\circ\rho\left(  \sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\\
&  =\sigma^{-1}\circ X\left(  u\right)  X^{\ast}\left(  v\right)  \circ
\sigma=\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  .
\end{align*}
It thus only remains to prove that%
\[
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\dfrac{1}{1-\dfrac
{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(  \sum\limits_{j>0}%
\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\]


By Theorem \ref{thm.euler} (applied to $m-1$ instead of $m$), we have%
\[
\Gamma\left(  u\right)  =u^{m}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m-1\right)
}.
\]
By Theorem \ref{thm.euler}, we have%
\[
\Gamma^{\ast}\left(  v\right)  =v^{-m}z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}v^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}v^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
m\right)  }.
\]
Multiplying these two equalities, we obtain%
\begin{align*}
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)   &  =u^{m}v^{-m}%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}%
{j}a_{-j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
\ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m\right)  }%
\end{align*}
(since multiplication by $z$ commutes with each of $\exp\left(  \sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ and $\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $). We wish to ``switch'' the
second and the third exponential on the right hand side of this equation
(although they don't commute). To do so, we notice that each of $-\sum
\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}$ and $-\sum\limits_{j>0}\dfrac{v^{j}}%
{j}a_{-j}$ lies in the ring $\left(  \operatorname*{End}\left(  \mathcal{B}%
^{\left(  m\right)  }\right)  \right)  \left[  \left[  u^{-1},v\right]
\right]  $\ \ \ \ \footnote{This is the ring of formal power series in the
indeterminates $u^{-1}$ and $v$ over the ring $\operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  $. Note that $\operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  $ is non-commutative, but the
ring of formal power series is still defined in the same way as over
commutative rings. The indeterminates $u^{-1}$ and $v$ themselves commute with
each other and with each element of $\operatorname*{End}\left(  \mathcal{B}%
^{\left(  m\right)  }\right)  $.}. Let $I$ be the ideal of the ring $\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)
\left[  \left[  u^{-1},v\right]  \right]  $ consisting of all power series
with constant term $0$. This ring $\left(  \operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u^{-1},v\right]  \right]  $ is a $\mathbb{Q}$-algebra and is complete and
Hausdorff with respect to the $I$-adic topology. Let $\alpha=-\sum
\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}$ and $\beta=-\sum\limits_{j>0}\dfrac
{v^{j}}{j}a_{-j}$. Clearly, both $\alpha$ and $\beta$ lie in $I$. Also,%
\begin{align*}
\left[  \alpha,\beta\right]   &  =\left[  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j},-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right]  =\sum\limits_{j>0}%
\sum\limits_{k>0}\dfrac{u^{-j}v^{k}}{jk}\underbrace{\left[  a_{j}%
,a_{-k}\right]  }_{=\delta_{j,k}j}\\
&  =\sum\limits_{j>0}\sum\limits_{k>0}\dfrac{u^{-j}v^{k}}{jk}\delta
_{j,k}j=\sum\limits_{j>0}\dfrac{u^{-j}v^{j}}{jj}j=\sum\limits_{j>0}\dfrac
{1}{j}\left(  \dfrac{v}{u}\right)  ^{j}=-\log\left(  1-\dfrac{v}{u}\right)
\end{align*}
is a power series with coefficients in $\mathbb{Q}$, and thus lies in the
center of $\left(  \operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \right)  \left[  \left[  u^{-1},v\right]  \right]  $, and hence
commutes with each of $\alpha$ and $\beta$. Thus, we can apply Lemma
\ref{lem.powerseries3} to $K=\mathbb{Q}$ and $R=\left(  \operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u^{-1},v\right]  \right]  $, and obtain $\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  =\left(  \exp\beta\right)  \cdot\left(
\exp\alpha\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $.
Hence,%
\begin{align*}
&  \Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =u^{m}v^{-m}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}%
a_{-j}\right)  \exp\underbrace{\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  }_{=\alpha}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\underbrace{\left(  -\sum\limits_{j>0}%
\dfrac{v^{j}}{j}a_{-j}\right)  }_{=\beta}\exp\left(  \sum\limits_{j>0}%
\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =u^{m}v^{-m}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}%
a_{-j}\right)  \cdot\underbrace{\left(  \exp\alpha\right)  \cdot\left(
\exp\beta\right)  }_{=\left(  \exp\beta\right)  \cdot\left(  \exp
\alpha\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =\underbrace{u^{m}v^{-m}}_{=\left(  \dfrac{u}{v}\right)  ^{m}}\cdot
\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \cdot
\exp\underbrace{\beta}_{=-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\underbrace{\alpha}_{=-\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}}\cdot\underbrace{\exp\left[  \alpha,\beta\right]
}_{\substack{=\dfrac{1}{1-\dfrac{v}{u}}\\\text{(since }\left[  \alpha
,\beta\right]  =-\log\left(  1-\dfrac{v}{u}\right)  \text{)}}}\cdot\exp\left(
\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =\left(  \dfrac{u}{v}\right)  ^{m}\cdot\exp\left(  \sum\limits_{j>0}%
\dfrac{u^{j}}{j}a_{-j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{v^{j}}{j}a_{-j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  \cdot\dfrac{1}{1-\dfrac{v}{u}}\cdot\exp\left(  \sum
\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
\end{align*}%
\begin{align*}
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}%
\cdot\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}%
a_{-j}\right)  \\\text{(by Theorem \ref{thm.exp(u+v)}, applied to
}R=\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \left[
\left[  u,v\right]  \right]  \text{,}\\I=\left(  \text{the ideal of }R\text{
consisting of all power series with constant term }0\right)  \text{,}%
\\\alpha=\sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\text{ and }\beta
=-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac
{v^{-j}}{j}a_{j}\right)  }_{\substack{=\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  \\\text{(by Theorem \ref{thm.exp(u+v)},
applied to }R=\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \left[  \left[  u^{-1},v^{-1}\right]  \right]  \text{,}\\I=\left(
\text{the ideal of }R\text{ consisting of all power series with constant term
}0\right)  \text{,}\\\alpha=-\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\text{ and
}\beta=\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\text{)}}}\\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}
This proves Corollary \ref{cor.euler}.

\subsection{Expliciting \texorpdfstring{$\sigma^{-1}$}{the inverse of the
Boson-Fermion correspondence} using Schur polynomials}

Next we are going to give an explicit (in as far as one can do) formula for
$\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
$ for an elementary semiinfinite wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$. Before we do so, we need to introduce the notion of
\textit{Schur polynomials}. We first define \textit{elementary Schur
polynomials}:

\subsubsection{Schur polynomials}

\begin{Convention}
\label{conv.schur.x}In the following, we let $x$ denote the countable family
of indeterminates $\left(  x_{1},x_{2},x_{3},...\right)  $. Thus, for any
polynomial $P$ in countably many indeterminates, we write $P\left(  x\right)
$ for $P\left(  x_{1},x_{2},x_{3},...\right)  $.
\end{Convention}

\begin{definition}
\label{def.schur.Sk}For every $k\in\mathbb{N}$, let $S_{k}\in\mathbb{Q}\left[
x_{1},x_{2},x_{3},...\right]  $ be the coefficient of the power series
$\exp\left(  \sum\limits_{i\geq1}x_{i}z^{i}\right)  \in\mathbb{Q}\left[
x_{1},x_{2},x_{3},...\right]  \left[  \left[  z\right]  \right]  $ before
$z^{k}$. Then, obviously,%
\begin{equation}
\sum\limits_{k\geq0}S_{k}\left(  x\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}x_{i}z^{i}\right)  . \label{def.schur.sk.genfun}%
\end{equation}

\end{definition}

For example, $S_{0}\left(  x\right)  =1$, $S_{1}\left(  x\right)  =x_{1}$,
$S_{2}\left(  x\right)  =\dfrac{x_{1}^{2}}{2}+x_{2}$, $S_{3}\left(  x\right)
=\dfrac{x_{1}^{3}}{6}+x_{1}x_{2}+x_{3}$.

Note that the polynomials $S_{k}$ that we just defined are \textbf{not}
symmetric polynomials. Instead, they ``represent'' the complete symmetric
functions in terms of the $\dfrac{p_{i}}{i}$ (where $p_{i}$ are the power
sums). Here is what exactly we mean by this:

\begin{definition}
\label{def.schur.y}Let $N\in\mathbb{N}$, and let $y$ denote a family of $N$
indeterminates $\left(  y_{1},y_{2},...,y_{N}\right)  $. Thus, for any
polynomial $P$ in $N$ indeterminates, we write $P\left(  y\right)  $ for
$P\left(  y_{1},y_{2},...,y_{N}\right)  $.
\end{definition}

\begin{definition}
\label{def.schur.hk}For every $k\in\mathbb{N}$, define the $k$\textit{-th
complete symmetric function} $h_{k}$ in the variables $y_{1},y_{2},...,y_{N}$
by $h_{k}\left(  y_{1},y_{2},...,y_{N}\right)  =\sum\limits_{\substack{p_{1}%
,p_{2},...,p_{N}\in\mathbb{N};\\p_{1}+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}%
y_{2}^{p_{2}}...y_{N}^{p_{N}}$.
\end{definition}

\begin{proposition}
\label{prop.schur.hk}In the ring $\mathbb{Q}\left[  y_{1},y_{2},...,y_{N}%
\right]  \left[  \left[  z\right]  \right]  $, we have%
\[
\sum\limits_{k\geq0}z^{k}h_{k}\left(  y\right)  =\prod\limits_{j=1}^{N}%
\dfrac{1}{1-zy_{j}}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.hk}.} For every $j\in\left\{
1,2,...,N\right\}  $, the sum formula for the geometric series yields
$\dfrac{1}{1-zy_{j}}=\sum\limits_{p\in\mathbb{N}}\left(  zy_{j}\right)
^{p}=\sum\limits_{p\in\mathbb{N}}y_{j}^{p}z^{p}$. Hence,%
\begin{align*}
\prod\limits_{j=1}^{N}\dfrac{1}{1-zy_{j}}  &  =\prod\limits_{j=1}^{N}\left(
\sum\limits_{p\in\mathbb{N}}y_{j}^{p}z^{p}\right)  =\sum\limits_{p_{1}%
,p_{2},...,p_{N}\in\mathbb{N}}\underbrace{\left(  y_{1}^{p_{1}}z^{p_{1}%
}\right)  \left(  y_{2}^{p_{2}}z^{p_{2}}\right)  ...\left(  y_{N}^{p_{N}%
}z^{p_{N}}\right)  }_{=y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}%
z^{p_{1}+p_{2}+...+p_{N}}}\\
&  =\sum\limits_{p_{1},p_{2},...,p_{N}\in\mathbb{N}}y_{1}^{p_{1}}y_{2}^{p_{2}%
}...y_{N}^{p_{N}}z^{p_{1}+p_{2}+...+p_{N}}=\sum\limits_{k\geq0}%
\underbrace{\sum\limits_{\substack{p_{1},p_{2},...,p_{N}\in\mathbb{N}%
;\\p_{1}+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}%
}_{=h_{k}\left(  y_{1},y_{2},...,y_{N}\right)  =h_{k}\left(  y\right)  }%
z^{k}\\
&  =\sum\limits_{k\geq0}h_{k}\left(  y\right)  z^{k}=\sum\limits_{k\geq0}%
z^{k}h_{k}\left(  y\right)  .
\end{align*}
This proves Proposition \ref{prop.schur.hk}.

\begin{definition}
Let $N\in\mathbb{N}$. We define a map $\operatorname*{PSE}\nolimits_{N}%
:\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \rightarrow\mathbb{C}\left[
y_{1},y_{2},...,y_{N}\right]  $ as follows: For every polynomial $P\in\left[
x_{1},x_{2},x_{3},...\right]  $, let $\operatorname*{PSE}\nolimits_{N}\left(
P\right)  $ be the result of substituting $x_{j}=\dfrac{y_{1}^{j}+y_{2}%
^{j}+...+y_{N}^{j}}{j}$ for all positive integers $j$ into the polynomial $P$.

Clearly, this map $\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra homomorphism.
\end{definition}

(The notation $\operatorname*{PSE}\nolimits_{N}$ is mine and has been chosen
as an abbreviation for ``Power Sum Evaluation in $N$ variables''.)

\begin{proposition}
\label{prop.schur.h_k.as.schur}For every $N\in\mathbb{N}$, we have
$h_{k}\left(  y\right)  =\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  $ for each $k\in\mathbb{N}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.h_k.as.schur}.} Fix
$N\in\mathbb{N}$. We know that $\sum\limits_{k\geq0}S_{k}\left(  x\right)
z^{k}=\exp\left(  \sum\limits_{i\geq1}x_{i}z^{i}\right)  $. Since
$\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra homomorphism,
this yields%
\begin{align*}
\sum\limits_{k\geq0}\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  z^{k}  &  =\exp\left(  \sum\limits_{i\geq1}%
\operatorname*{PSE}\nolimits_{N}\left(  x_{i}\right)  z^{i}\right)
=\exp\left(  \sum\limits_{i\geq1}\sum\limits_{j=1}^{N}\dfrac{y_{j}^{i}}%
{i}z^{i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\left(  x_{i}\right)  =\dfrac{y_{1}^{i}+y_{2}^{i}+...+y_{N}^{i}}{i}%
=\sum\limits_{j=1}^{N}\dfrac{y_{j}^{i}}{i}\right) \\
&  =\exp\left(  \sum\limits_{j=1}^{N}\sum\limits_{i\geq1}\dfrac{y_{j}^{i}}%
{i}z^{i}\right)  =\prod\limits_{j=1}^{N}\exp\left(  \sum\limits_{i\geq1}%
\dfrac{y_{j}^{i}}{i}z^{i}\right) \\
&  =\prod\limits_{j=1}^{N}\exp\underbrace{\left(  \sum\limits_{i\geq1}%
\dfrac{y_{j}^{i}z^{i}}{i}\right)  }_{=-\log\left(  1-y_{j}z\right)  }%
=\prod\limits_{j=1}^{N}\underbrace{\exp\left(  -\log\left(  1-y_{j}z\right)
\right)  }_{=\dfrac{1}{1-y_{j}z}=\dfrac{1}{1-zy_{j}}}\\
&  =\prod\limits_{j=1}^{N}\dfrac{1}{1-zy_{j}}=\sum\limits_{k\geq0}z^{k}%
h_{k}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.schur.hk}}\right)  .
\end{align*}
By comparing coefficients in this equality, we conclude that
$\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)
=h_{k}\left(  y\right)  $ for each $k\in\mathbb{N}$. Proposition
\ref{prop.schur.h_k.as.schur} is proven.

\begin{definition}
\label{def.schur.Slambda}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition, so that $\lambda_{1}\geq
\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers.

We define $S_{\lambda}\left(  x\right)  \in\mathbb{Q}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to be the polynomial%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
S_{\lambda_{1}}\left(  x\right)  & S_{\lambda_{1}+1}\left(  x\right)  &
S_{\lambda_{1}+2}\left(  x\right)  & ... & S_{\lambda_{1}+m-1}\left(  x\right)
\\
S_{\lambda_{2}-1}\left(  x\right)  & S_{\lambda_{2}}\left(  x\right)  &
S_{\lambda_{2}+1}\left(  x\right)  & ... & S_{\lambda_{2}+m-2}\left(  x\right)
\\
S_{\lambda_{3}-2}\left(  x\right)  & S_{\lambda_{3}-1}\left(  x\right)  &
S_{\lambda_{3}}\left(  x\right)  & ... & S_{\lambda_{3}+m-3}\left(  x\right)
\\
... & ... & ... & ... & ...\\
S_{\lambda_{m}-m+1}\left(  x\right)  & S_{\lambda_{m}-m+2}\left(  x\right)  &
S_{\lambda_{m}-m+3}\left(  x\right)  & ... & S_{\lambda_{m}}\left(  x\right)
\end{array}
\right) \\
&  =\det\left(  \left(  S_{\lambda_{i}+j-i}\left(  x\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  ,
\end{align*}
where $S_{j}$ denotes $0$ if $j<0$. (Note that this does not depend on
trailing zeroes in the partition; in other words, $S_{\left(  \lambda
_{1},\lambda_{2},...,\lambda_{m}\right)  }\left(  x\right)  =S_{\left(
\lambda_{1},\lambda_{2},...,\lambda_{m},0,0,...,0\right)  }\left(  x\right)  $
for any number of zeroes. This is because any nonnegative integers $m$ and
$\ell$, any $m\times m$-matrix $A$, any $m\times\ell$-matrix $B$ and any upper
unitriangular $\ell\times\ell$-matrix $C$ satisfy $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0 & C
\end{array}
\right)  =\det A$.)

We refer to $S_{\lambda}\left(  x\right)  $ as the \textit{bosonic Schur
polynomial corresponding to the partition }$\lambda$.
\end{definition}

To a reader acquainted with the Schur polynomials of combinatorics (and
representation theory of symmetric groups), this definition may look familiar,
but it should be reminded that our polynomial $S_{\lambda}\left(  x\right)  $
is \textbf{not a symmetric function per se} (this is why we call it ``bosonic
Schur polynomial'' and not just simply ``Schur polynomial''); instead, it can
be made into a symmetric function -- and this will, indeed, be the $\lambda
$-Schur polynomial known from combinatorics -- by substituting for each
$x_{j}$ the term $\dfrac{\left(  j\text{-th power sum symmetric function}%
\right)  }{j}$. We will prove this in Proposition \ref{prop.schur.Schur=schur}
(albeit only for finitely many variables). Let us first formulate one of the
many definitions of Schur polynomials from combinatorics:

\begin{definition}
\label{def.schur.schurpoly}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition, so that $\lambda_{1}\geq
\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers. We define $\lambda
_{\ell}$ to mean $0$ for all integers $\ell>m$; thus, we obtain a
nonincreasing sequence $\left(  \lambda_{1},\lambda_{2},\lambda_{3}%
,...\right)  $ of nonnegative integers.

Let $N\in\mathbb{N}$.

The so-called $\lambda$\textit{-Schur module} $V_{\lambda}$ \textit{over
}$\operatorname*{GL}\left(  N\right)  $ is defined to be the
$\operatorname*{GL}\left(  N\right)  $-module $\operatorname*{Hom}%
\nolimits_{S_{n}}\left(  S^{\lambda},\left(  \mathbb{C}^{N}\right)  ^{\otimes
n}\right)  $, where $n$ denotes the number $\lambda_{1}+\lambda_{2}%
+...+\lambda_{m}$ and $S^{\lambda}$ denotes the Specht module over the
symmetric group $S_{n}$ corresponding to the partition $\lambda$. (The
$\operatorname*{GL}\left(  N\right)  $-module structure on
$\operatorname*{Hom}\nolimits_{S_{n}}\left(  S^{\lambda},\left(
\mathbb{C}^{N}\right)  ^{\otimes n}\right)  $ is obtained from the
$\operatorname*{GL}\left(  N\right)  $-module structure on $\mathbb{C}^{N}$.)
This $\lambda$-Schur module $V_{\lambda}$ is not only a $\operatorname*{GL}%
\left(  N\right)  $-module, but also a $\mathfrak{gl}\left(  N\right)
$-module. If $\lambda_{N+1}=0$, then $V_{\lambda}$ is irreducible both as a
representation of $\operatorname*{GL}\left(  N\right)  $ and as a
representation of $\mathfrak{gl}\left(  N\right)  $. If $\lambda_{N+1}\neq0$,
then $V_{\lambda}=0$.

It is known that there exists a unique polynomial $\chi_{\lambda}\in
\mathbb{Q}\left[  y_{1},y_{2},...,y_{N}\right]  $ (depending both on $\lambda$
and on $N$) such that every diagonal matrix $A=\operatorname*{diag}\left(
a_{1},a_{2},...,a_{N}\right)  \in\operatorname*{GL}\left(  N\right)  $
satisfies $\chi_{\lambda}\left(  a_{1},a_{2},...,a_{N}\right)  =\left(
\operatorname*{Tr}\mid_{V_{\lambda}}\right)  \left(  A\right)  $ (where
$\left(  \operatorname*{Tr}\mid_{V_{\lambda}}\right)  \left(  A\right)  $
means the trace of the action of $A\in\operatorname*{GL}\left(  N\right)  $ on
$V_{\lambda}$ by means of the $\operatorname*{GL}\left(  N\right)  $-module
structure on $V_{\lambda}$). In the language of representation theory,
$\chi_{\lambda}$ is thus the character of the $\operatorname*{GL}\left(
N\right)  $-module $V_{\lambda}$. This polynomial $\chi_{\lambda}$ is called
the $\lambda$\textit{-th Schur polynomial in }$N$ \textit{variables}.
\end{definition}

Now, the relation between the $S_{\lambda}$ and the Schur polynomials looks
like this:

\begin{proposition}
\label{prop.schur.Schur=schur}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition. Then, $\chi_{\lambda}\left(
y_{1},y_{2},...,y_{N}\right)  =\operatorname*{PSE}\nolimits_{N}\left(
S_{\lambda}\left(  x\right)  \right)  $.
\end{proposition}

This generalizes Proposition \ref{prop.schur.h_k.as.schur} (in fact, set
$\lambda=\left(  k\right)  $ and notice that $V_{\lambda}=S^{k}\mathbb{C}^{N}$).

\textit{Proof of Proposition \ref{prop.schur.Schur=schur}.} Define $h_{k}$ to
mean $0$ for every $k<0$.

Proposition \ref{prop.schur.h_k.as.schur} yields $h_{k}\left(  y\right)
=\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)  $ for
each $k\in\mathbb{N}$. Since $h_{k}\left(  y\right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)  $ also holds for every
negative integer $k$ (since every negative integer $k$ satisfies $h_{k}=0$ and
$S_{k}=0$), we thus conclude that%
\begin{equation}
h_{k}\left(  y\right)  =\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\label{pf.schur.Schur=schur.1}%
\end{equation}


We know that $\chi_{\lambda}$ is the $\lambda$-th Schur polynomial in $N$
variables. By the first Giambelli formula, this yields that%
\begin{align*}
&  \chi_{\lambda}\left(  y_{1},y_{2},...,y_{N}\right) \\
&  =\det\underbrace{\left(
\begin{array}
[c]{ccccc}%
h_{\lambda_{1}}\left(  y\right)  & h_{\lambda_{1}+1}\left(  y\right)  &
h_{\lambda_{1}+2}\left(  y\right)  & ... & h_{\lambda_{1}+m-1}\left(  y\right)
\\
h_{\lambda_{2}-1}\left(  y\right)  & h_{\lambda_{2}}\left(  y\right)  &
h_{\lambda_{2}+1}\left(  y\right)  & ... & h_{\lambda_{2}+m-2}\left(  y\right)
\\
h_{\lambda_{3}-2}\left(  y\right)  & h_{\lambda_{3}-1}\left(  y\right)  &
h_{\lambda_{3}}\left(  y\right)  & ... & h_{\lambda_{3}+m-3}\left(  y\right)
\\
... & ... & ... & ... & ...\\
h_{\lambda_{m}-m+1}\left(  y\right)  & h_{\lambda_{m}-m+2}\left(  y\right)  &
h_{\lambda_{m}-m+3}\left(  y\right)  & ... & h_{\lambda_{m}}\left(  y\right)
\end{array}
\right)  }_{=\left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}}\\
&  =\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  =\det\left(  \left(  \operatorname*{PSE}%
\nolimits_{N}\left(  S_{\lambda_{i}+j-i}\left(  x\right)  \right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.Schur=schur.1})}\right)
\\
&  =\operatorname*{PSE}\nolimits_{N}\underbrace{\left(  \det\left(  \left(
S_{\lambda_{i}+j-i}\left(  x\right)  \right)  _{1\leq i\leq m,\ 1\leq j\leq
m}\right)  \right)  }_{=S_{\lambda}\left(  x\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{PSE}\nolimits_{N}\text{ is a }\mathbb{C}%
\text{-algebra homomorphism, whereas }\det\text{ is a polynomial}\\
\text{(and any }\mathbb{C}\text{-algebra homomorphism commutes with any
polynomial)}%
\end{array}
\right) \\
&  =\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)
\right)  .
\end{align*}
Proposition \ref{prop.schur.Schur=schur} is proven.

\subsubsection{The statement of the fact}

\begin{theorem}
\label{thm.schur}Whenever $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
$0$-degression (see Definition \ref{def.glinf.m-deg} for what this means), we
have $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =S_{\lambda}\left(  x\right)  $ where $\lambda=\left(
i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $. (Note that this $\lambda$ is indeed a
partition since $\left(  i_{0},i_{1},i_{2},...\right)  $ is a $0$-degression.)
\end{theorem}

We are going to give two proofs of this theorem. The first proof will be
covered in Section \ref{subsect.schur.pf1}, whereas the second proof will
encompass Section \ref{subsect.schur.pf2}.

\subsection{\label{subsect.schur.pf1}Expliciting
\texorpdfstring{$\sigma^{-1}$}{the inverse of the Boson-Fermion
correspondence} using Schur polynomials: first proof}

\subsubsection{\label{subsubsect.powersums}The power sums are algebraically
independent}

Our first proof of Theorem \ref{thm.schur} will require some lemmata from
algebraic combinatorics. First of all:

\begin{lemma}
\label{lem.schur.algind}Let $N\in\mathbb{N}$. For every positive integer $j$,
let $p_{j}$ denote the polynomial $y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}%
\in\mathbb{C}\left[  y_{1},y_{2},...,y_{N}\right]  $. Then, the polynomials
$p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically independent.
\end{lemma}

In order to prove this fact, we need the following known facts (which we won't prove):

\begin{lemma}
\label{lem.schur.elsym}Let $N\in\mathbb{N}$. For every $j\in\mathbb{N}$, let
$e_{j}$ denote the $j$-th elementary symmetric polynomial $\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{j}\leq N}y_{i_{1}}y_{i_{2}}...y_{i_{j}}$ in $\mathbb{C}%
\left[  y_{1},y_{2},...,y_{N}\right]  $. Then, the elements $e_{1}$, $e_{2}$,
$...$, $e_{N}$ are algebraically independent.
\end{lemma}

Lemma \ref{lem.schur.elsym} is one half of a known theorem. The other half
says that the elements $e_{1}$, $e_{2}$, $...$, $e_{N}$ generate the
$\mathbb{C}$-algebra of symmetric polynomials in $\mathbb{C}\left[
y_{1},y_{2},...,y_{N}\right]  $. We will prove neither of these halves; they
are both classical and well-known (under the name ``fundamental theorem of
symmetric polynomials'', which is usually formulated in a more general setting
when $\mathbb{C}$ is replaced by any commutative ring).

\begin{lemma}
\label{lem.schur.newtonid}Let $N\in\mathbb{N}$. For every positive integer
$j$, define $p_{j}$ as in Lemma \ref{lem.schur.algind}. For every
$j\in\mathbb{N}$, define $e_{j}$ as in Lemma \ref{lem.schur.elsym}. Then,
every $k\in\mathbb{N}$ satisfies $ke_{k}=\sum\limits_{i=1}^{k}\left(
-1\right)  ^{i-1}e_{k-i}p_{i}$.
\end{lemma}

This lemma is known as the \textit{Newton identity} (or identities), and won't
be proven due to being well-known. But we will use it to derive the following corollary:

\begin{corollary}
\label{cor.schur.newton}Let $N\in\mathbb{N}$. For every positive integer $j$,
define $p_{j}$ as in Lemma \ref{lem.schur.algind}. For every $j\in\mathbb{N}$,
define $e_{j}$ as in Lemma \ref{lem.schur.elsym}. Then, for every positive
$k\in\mathbb{N}$, there exists a polynomial $P_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k}\right]  $ such that $p_{k}=P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  $ and $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $. (Here, of course,
$\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $ is identified with a
subalgebra of $\mathbb{Q}\left[  T_{1},T_{2},...,T_{k}\right]  $.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.newton}.} We will prove Corollary
\ref{cor.schur.newton} by strong induction over $k$:

\textit{Induction step:} Let $\ell$ be a positive integer. Assume that
Corollary \ref{cor.schur.newton} holds for every positive integer $k<\ell$. We
must then prove that Corollary \ref{cor.schur.newton} holds for $k=\ell$.

Corollary \ref{cor.schur.newton} holds for every positive integer $k<\ell$ (by
the induction hypothesis). In other words, for every $k<\ell$, there exists a
polynomial $P_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k}\right]  $ such
that $p_{k}=P_{k}\left(  e_{1},e_{2},...,e_{k}\right)  $ and $P_{k}-\left(
-1\right)  ^{k-1}kT_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $.
Consider these polynomials $P_{1}$, $P_{2}$, $...$, $P_{\ell-1}$.

Applying Lemma \ref{lem.schur.newtonid} to $k=\ell$, we obtain%
\begin{align*}
\ell e_{\ell}  &  =\sum\limits_{i=1}^{\ell}\left(  -1\right)  ^{i-1}e_{\ell
-i}p_{i}=\sum\limits_{k=1}^{\ell}\left(  -1\right)  ^{k-1}e_{\ell-k}%
p_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed }i\text{ as }k\text{
in the sum}\right) \\
&  =\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}+\left(
-1\right)  ^{\ell-1}\underbrace{e_{\ell-\ell}}_{=e_{0}=1}p_{\ell}%
=\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}+\left(
-1\right)  ^{\ell-1}p_{\ell},
\end{align*}
so that $\left(  -1\right)  ^{\ell-1}p_{\ell}=\ell e_{\ell}-\sum
\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}$ and thus%
\[
p_{\ell}=\left(  -1\right)  ^{\ell-1}\left(  \ell e_{\ell}-\sum\limits_{k=1}%
^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}\right)  =\left(  -1\right)
^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell
-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}.
\]


Now, define a polynomial $P_{\ell}\in\mathbb{Q}\left[  T_{1},T_{2}%
,...,T_{\ell}\right]  $ by%
\[
P_{\ell}=\left(  -1\right)  ^{\ell-1}\ell T_{\ell}-\left(  -1\right)
^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}T_{\ell-k}%
P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  .
\]
Then,%
\[
P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}=-\left(  -1\right)
^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}%
\underbrace{T_{\ell-k}}_{\substack{\in\mathbb{Q}\left[  T_{1},T_{2}%
,...,T_{\ell-1}\right]  \\\text{(since }\ell-k\leq\ell-1\text{)}%
}}\underbrace{P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  }_{\substack{\in
\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  \\\text{(since }k\leq
\ell-1\text{)}}}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  .
\]
Moreover, $P_{\ell}=\left(  -1\right)  ^{\ell-1}\ell T_{\ell}-\left(
-1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}%
T_{\ell-k}P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  $ yields%
\begin{align*}
P_{\ell}\left(  e_{1},e_{2},...,e_{\ell}\right)   &  =\left(  -1\right)
^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell
-1}\left(  -1\right)  ^{k-1}e_{\ell-k}\underbrace{P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  }_{\substack{=p_{k}\\\text{(by the definition of
}P_{k}\text{)}}}\\
&  =\left(  -1\right)  ^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}%
\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}=p_{\ell}.
\end{align*}


We thus have shown that $p_{\ell}=P_{\ell}\left(  e_{1},e_{2},...,e_{\ell
}\right)  $ and $P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  $. Thus, there exists
a polynomial $P_{\ell}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell}\right]  $
such that $p_{\ell}=P_{\ell}\left(  e_{1},e_{2},...,e_{\ell}\right)  $ and
$P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{\ell-1}\right]  $. In other words, Corollary
\ref{cor.schur.newton} holds for $k=\ell$. This completes the induction step.
The induction proof of Corollary \ref{cor.schur.newton} is thus complete.

\textit{Proof of Lemma \ref{lem.schur.algind}.} Assume the contrary. Thus, the
polynomials $p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically dependent.
Hence, there exists a nonzero polynomial $Q\in\mathbb{C}\left[  U_{1}%
,U_{2},...,U_{N}\right]  $ such that $Q\left(  p_{1},p_{2},...,p_{N}\right)
=0$. Consider this $Q$.

Consider the lexicographic order on the monomials in $\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $ given by $T_{1}<T_{2}<...<T_{N}$.

For every $j\in\mathbb{N}$, define $e_{j}$ as in Lemma \ref{lem.schur.elsym}.
For every positive $k\in\mathbb{N}$, Corollary \ref{cor.schur.newton}
guarantees the existence of a polynomial $P_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k}\right]  $ such that $p_{k}=P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  $ and $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $. Consider such a
polynomial $P_{k}$.

For every $k\in\left\{  1,2,...,N\right\}  $, there exists a polynomial
$Q_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $ such that
$P_{k}-\left(  -1\right)  ^{k-1}kT_{k}=Q_{k}\left(  T_{1},T_{2},...,T_{k-1}%
\right)  $ (since $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k-1}\right]  $). Consider such a polynomial $Q_{k}$.

For every $k\in\left\{  1,2,...,N\right\}  $, let $\widetilde{P}_{k}$ be the
polynomial $P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  \in\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $. (This is the same polynomial as $P_{k}$, but
now considered as a polynomial in $N$ variables over $\mathbb{C}$ rather than
in $k$ variables over $\mathbb{Q}$.)

Then, for every $k\in\left\{  1,2,...,N\right\}  $, we have%
\begin{align*}
\widetilde{P}_{k}\left(  e_{1},e_{2},...,e_{N}\right)   &  =P_{k}\left(
e_{1},e_{2},...,e_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\widetilde{P}_{k}=P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  \right) \\
&  =p_{k}.
\end{align*}


Also, for every $k\in\left\{  1,2,...,N\right\}  $, the leading
monomial\footnote{Here, ``monomial'' means ``monomial without coefficient'',
and the ``leading monomial'' of a polynomial means the highest monomial (with
nonzero coefficient) of the polynomial.} of $\widetilde{P}_{k}$ (with respect
to the lexicographic order defined above) is $T_{k}$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,...,N\right\}  $.
Then,
\begin{align*}
\underbrace{\widetilde{P}_{k}}_{=P_{k}\left(  T_{1},T_{2},...,T_{k}\right)
}-\left(  -1\right)  ^{k-1}kT_{k}  &  =P_{k}\left(  T_{1},T_{2},...,T_{k}%
\right)  -\left(  -1\right)  ^{k-1}kT_{k}\\
&  =\underbrace{\left(  P_{k}-\left(  -1\right)  ^{k-1}kT_{k}\right)
}_{=Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  }\left(  T_{1},T_{2}%
,...,T_{k}\right) \\
&  =\left(  Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  \right)  \left(
T_{1},T_{2},...,T_{k}\right)  =Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  ,
\end{align*}
so that $\widetilde{P}_{k}=\left(  -1\right)  ^{k-1}kT_{k}+Q_{k}\left(
T_{1},T_{2},...,T_{k-1}\right)  $. Hence, the only monomials which occur with
nonzero coefficient in the polynomial $\widetilde{P}_{k}$ are the monomial
$T_{k}$ (occurring with coefficient $\left(  -1\right)  ^{k-1}k$) and the
monomials of the polynomial $Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  $.
But the latter monomials don't contain any variable other than $T_{1}$,
$T_{2}$, $...$, $T_{k-1}$ (because they are monomials of the polynomial
$Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  $), and thus are smaller than
the monomial $T_{k}$ (because any monomial which doesn't contain any variable
other than $T_{1}$, $T_{2}$, $...$, $T_{k-1}$ is smaller than any monomial
which contains $T_{k}$ (since we have a lexicographic order given by
$T_{1}<T_{2}<...<T_{N}$)). Hence, the leading monomial of $\widetilde{P}_{k}$
must be $T_{k}$, qed.}. Since the leading monomial of a product of polynomials
equals the product of their leading monomials, this yields that for every
$\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$,%
\begin{equation}
\text{the leading monomial of }\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\text{ is }T_{1}^{\alpha
_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}. \label{pf.schur.algind.3}%
\end{equation}


Since every $k\in\left\{  1,2,...,N\right\}  $ satisfies $p_{k}=\widetilde{P}%
_{k}\left(  e_{1},e_{2},...,e_{N}\right)  $, we have%
\begin{align*}
Q\left(  p_{1},p_{2},...,p_{N}\right)   &  =Q\left(  \widetilde{P}_{1}\left(
e_{1},e_{2},...,e_{N}\right)  ,\widetilde{P}_{2}\left(  e_{1},e_{2}%
,...,e_{N}\right)  ,...,\widetilde{P}_{N}\left(  e_{1},e_{2},...,e_{N}\right)
\right) \\
&  =\left(  Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}%
_{N}\right)  \right)  \left(  e_{1},e_{2},...,e_{N}\right)  .
\end{align*}
Hence, $Q\left(  p_{1},p_{2},...,p_{N}\right)  =0$ rewrites as $\left(
Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}_{N}\right)
\right)  \left(  e_{1},e_{2},...,e_{N}\right)  =0$. Since $e_{1}$, $e_{2}$,
$...$, $e_{N}$ are algebraically independent (by Lemma \ref{lem.schur.elsym}),
this yields $Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}%
_{N}\right)  =0$. Since $Q\neq0$, this shows that the elements $\widetilde{P}%
_{1}$, $\widetilde{P}_{2}$, $...$, $\widetilde{P}_{N}$ are algebraically
dependent. In other words, the family $\left(  \widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\right)
_{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}}$ is
linearly dependent. Thus, there exists a family $\left(  \lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\right)  _{\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}}$ of elements of $\mathbb{C}$
such that:

\begin{itemize}
\item all but finitely many $\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$;

\item not all $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}=0$;

\item we have $\sum\limits_{\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}=0$.
\end{itemize}

Consider this family. By identifying every $N$-tuple $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ with the monomial
$T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}\in\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $, we obtain a lexicographic order on the
$N$-tuples $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  $ (from the
lexicographic order on the monomials in $\mathbb{C}\left[  T_{1}%
,T_{2},...,T_{N}\right]  $).

Since all but finitely many $\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$, but not all $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1}%
,\alpha_{2},...,\alpha_{N}}=0$, there exists a highest (with respect to the
above-defined order) $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}$ satisfying $\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}\neq0$. Let this $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  $ be
called $\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $. Then, $\left(
\beta_{1},\beta_{2},...,\beta_{N}\right)  $ is the highest $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfying
$\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\neq0$. Thus, $\lambda
_{\beta_{1},\beta_{2},...,\beta_{N}}\neq0$, but
\begin{equation}
\text{every }\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}\text{ higher than }\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  \text{ satisfies }\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}=0. \label{pf.schur.algind.4}%
\end{equation}


Now it is easy to see that for every $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfying $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $, the term%
\begin{equation}
\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\text{ is a
}\mathbb{C}\text{-linear combination of monomials smaller than }T_{1}%
^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}. \label{pf.schur.algind.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.algind.5}).} Let $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\left(
\alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $. Since the lexicographic order is a total order,
we must be in one of the following two cases:
\par
\textit{Case 1:} We have $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\geq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $.
\par
\textit{Case 2:} We have $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
<\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $.
\par
First, consider Case 1. In this case, $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \geq\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  $, so that $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
>\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $ (since $\left(
\alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $). Thus, $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ is higher than $\left(  \beta
_{1},\beta_{2},...,\beta_{N}\right)  $. Hence, $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$ (by (\ref{pf.schur.algind.4})), so that $\lambda
_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}=0$ is clearly
a $\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\alpha
_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}$. Thus, (\ref{pf.schur.algind.5})
holds in Case 1.
\par
Now, let us consider Case 2. In this case, $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  <\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)
$, so that $T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}%
<T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$ (because the order on
$N$-tuples is obtained from the order on monomials by identifying every
$N$-tuple $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in
\mathbb{N}^{N}$ with the monomial $T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}%
}...T_{N}^{\alpha_{N}}\in\mathbb{C}\left[  T_{1},T_{2},...,T_{N}\right]  $).
\par
Due to (\ref{pf.schur.algind.3}), every monomial which occurs with nonzero
coefficient in $\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}%
}...\widetilde{P}_{N}^{\alpha_{N}}$ is smaller or equal to $T_{1}^{\alpha_{1}%
}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}$. Combined with $T_{1}^{\alpha_{1}%
}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}<T_{1}^{\beta_{1}}T_{2}^{\beta_{2}%
}...T_{N}^{\beta_{N}}$, this yields that every monomial which occurs with
nonzero coefficient in $\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}$ is smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Hence,
$\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}$ is a $\mathbb{C}$-linear combination of monomials smaller
than $T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Thus,
$\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. We have thus proven that
(\ref{pf.schur.algind.5}) holds in Case 2.
\par
Hence, (\ref{pf.schur.algind.5}) holds in each of cases 1 and 2. Since no
other cases are possible, this yields that (\ref{pf.schur.algind.5}) always
holds.} As a consequence,%
\[
\sum\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}%
\]
is a sum of $\mathbb{C}$-linear combinations of monomials smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$, and thus itself a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$.

Now,%
\begin{align*}
0  &  =\sum\limits_{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}%
\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}\\
&  =\lambda_{\beta_{1},\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta_{1}%
}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}+\sum
\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}},
\end{align*}
so that%
\[
\sum\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}=-\lambda_{\beta_{1}%
,\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}_{2}%
^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}.
\]
Since we know that $\sum\limits_{\substack{\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  }}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}%
_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}%
^{\alpha_{N}}$ is a $\mathbb{C}$-linear combination of monomials smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$, we thus conclude
that $-\lambda_{\beta_{1},\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta
_{1}}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Since $-\lambda_{\beta_{1},\beta
_{2},...,\beta_{N}}$ is invertible (because $\lambda_{\beta_{1},\beta
_{2},...,\beta_{N}}\neq0$), this yields that $\widetilde{P}_{1}^{\beta_{1}%
}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. In other words, every monomial which
occurs with nonzero coefficient in $\widetilde{P}_{1}^{\beta_{1}}%
\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is less than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. In particular, the
leading monomial of $\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}_{2}^{\beta
_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is less than $T_{1}^{\beta_{1}}%
T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. But this contradicts the fact that
(due to (\ref{pf.schur.algind.3}), applied to $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  =\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)
$) the leading monomial of $\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}%
_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is $T_{1}^{\beta_{1}}%
T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$.

This contradiction shows that our assumption was wrong. Hence, Lemma
\ref{lem.schur.algind} is proven.

(I have learned the above proof from:

Julia Pevtsova and Nate Bottman, \textit{504A Fall 2009 Homework Set
3},\newline%
\texttt{\url{http://www.math.washington.edu/~julia/teaching/504_Fall2009/HW7_sol.pdf}}
.)

We will apply Lemma \ref{lem.schur.algind} not directly, but through the
following corollary:

\begin{corollary}
\label{cor.schur.PSEinj}Let $P$ and $Q$ be polynomials in $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $. Assume that $\operatorname*{PSE}%
\nolimits_{N}\left(  P\right)  =\operatorname*{PSE}\nolimits_{N}\left(
Q\right)  $ for every sufficiently high $N\in\mathbb{N}$. Then, $P=Q$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.PSEinj}.} Any polynomial (even if it
is a polynomial in infinitely many indeterminates) has only finitely many
indeterminates actually appear in it. Hence, only finitely many indeterminates
appear in $P-Q$. Thus, there exists an $M\in\mathbb{N}$ such that no
indeterminates other than $x_{1}$, $x_{2}$, $...$, $x_{M}$ appear in $P-Q$.
Consider this $M$.

Recall that $\operatorname*{PSE}\nolimits_{N}\left(  P\right)
=\operatorname*{PSE}\nolimits_{N}\left(  Q\right)  $ for every sufficiently
high $N\in\mathbb{N}$. Thus, there exists an $N\in\mathbb{N}$ such that $N\geq
M$ and $\operatorname*{PSE}\nolimits_{N}\left(  P\right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  Q\right)  $. Pick such an $N$.

No indeterminates other than $x_{1}$, $x_{2}$, $...$, $x_{M}$ appear in $P-Q$.
Since $N\geq M$, this clearly yields that no indeterminates other than $x_{1}%
$, $x_{2}$, $...$, $x_{N}$ appear in $P-Q$. Hence, there exists a polynomial
$R\in\mathbb{C}\left[  x_{1},x_{2},...,x_{N}\right]  $ such that $P-Q=R\left(
x_{1},x_{2},...,x_{N}\right)  $. Consider this $R$.

Now, let us use the notations of Lemma \ref{lem.schur.algind}.

We defined $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  $ as the
result of substituting $x_{j}=\dfrac{y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}}{j}$
for all positive integers $j$ into the polynomial $P-Q$. Since $y_{1}%
^{j}+y_{2}^{j}+...+y_{N}^{j}=p_{j}$ for all positive integers $j$, this
rewrites as follows: $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  $ is
the result of substituting $x_{j}=\dfrac{p_{j}}{j}$ for all positive integers
$j$ into the polynomial $P-Q$. In other words,
\begin{align*}
\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)   &  =\underbrace{\left(
P-Q\right)  }_{=R\left(  x_{1},x_{2},...,x_{N}\right)  }\left(  \dfrac{p_{1}%
}{1},\dfrac{p_{2}}{2},\dfrac{p_{3}}{3},...\right)  =\left(  R\left(
x_{1},x_{2},...,x_{N}\right)  \right)  \left(  \dfrac{p_{1}}{1},\dfrac{p_{2}%
}{2},\dfrac{p_{3}}{3},...\right) \\
&  =R\left(  \dfrac{p_{1}}{1},\dfrac{p_{2}}{2},...,\dfrac{p_{N}}{N}\right)  .
\end{align*}
But since $\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra
homomorphism, we have $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)
=\operatorname*{PSE}\nolimits_{N}\left(  P\right)  -\operatorname*{PSE}%
\nolimits_{N}\left(  Q\right)  =0$ (since $\operatorname*{PSE}\nolimits_{N}%
\left(  P\right)  =\operatorname*{PSE}\nolimits_{N}\left(  Q\right)  $). Thus,%
\[
R\left(  \dfrac{p_{1}}{1},\dfrac{p_{2}}{2},...,\dfrac{p_{N}}{N}\right)
=\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  =0.
\]
Since $\dfrac{p_{1}}{1}$, $\dfrac{p_{2}}{2}$, $...$, $\dfrac{p_{N}}{N}$ are
algebraically independent (because Lemma \ref{lem.schur.algind} yields that
$p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically independent), this yields
$R=0$, so that $P-Q=\underbrace{R}_{=0}\left(  x_{1},x_{2},...,x_{N}\right)
=0$, thus $P=Q$. Corollary \ref{cor.schur.PSEinj} is proven.

Corollary \ref{cor.schur.PSEinj} allows us to prove equality of polynomials in
$\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ by means of evaluating them
at power sums. Now, let us show what such evaluations look like for the Schur functions:

\subsubsection{\label{subsubsect.schur1}First proof of Theorem \ref{thm.schur}%
}

\begin{theorem}
\label{thm.schur.altern}Let $\lambda=\left(  \lambda_{1},\lambda_{2}%
,\lambda_{3},...\right)  $ be a partition, so that $\lambda_{1}\geq\lambda
_{2}\geq...$ are nonnegative integers.

Let $N$ be a nonnegative integer such that $\lambda_{N+1}=0$. Then,%
\[
\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)  \right)
=\dfrac{\det\left(  \left(  y_{i}^{\lambda_{j}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }.
\]

\end{theorem}

\textit{Proof of Theorem \ref{thm.schur.altern}.} We will not really prove
this theorem; we will just reduce it to a known fact about Schur functions.

In fact, let $m$ be an integer such that $\lambda_{m+1}=0$ (such an integer
clearly exists). Then, the partition $\lambda$ can also be written in the form
$\left(  \lambda_{1},\lambda_{2},...,\lambda_{m}\right)  $. Hence, by the
first Giambelli formula, the $\lambda$-th Schur polynomial evaluated at
$\left(  y_{1},y_{2},...,y_{N}\right)  $ equals%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
h_{\lambda_{1}}\left(  y\right)  & h_{\lambda_{1}+1}\left(  y\right)  &
h_{\lambda_{1}+2}\left(  y\right)  & ... & h_{\lambda_{1}+m-1}\left(  y\right)
\\
h_{\lambda_{2}-1}\left(  y\right)  & h_{\lambda_{2}}\left(  y\right)  &
h_{\lambda_{2}+1}\left(  y\right)  & ... & h_{\lambda_{2}+m-2}\left(  y\right)
\\
h_{\lambda_{3}-2}\left(  y\right)  & h_{\lambda_{3}-1}\left(  y\right)  &
h_{\lambda_{3}}\left(  y\right)  & ... & h_{\lambda_{3}+m-3}\left(  y\right)
\\
... & ... & ... & ... & ...\\
h_{\lambda_{m}-m+1}\left(  y\right)  & h_{\lambda_{m}-m+2}\left(  y\right)  &
h_{\lambda_{m}-m+3}\left(  y\right)  & ... & h_{\lambda_{m}}\left(  y\right)
\end{array}
\right) \\
&  =\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  .
\end{align*}
But since the $\lambda$-th Schur polynomial evaluated at $\left(  y_{1}%
,y_{2},...,y_{N}\right)  $ also equals \newline$\dfrac{\det\left(  \left(
y_{i}^{\lambda_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }$ (by the ``Vandermonde-determinant'' definition of Schur
polynomials), this yields that
\[
\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  =\dfrac{\det\left(  \left(  y_{i}%
^{\lambda_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }.
\]
Comparing this with the equality $\det\left(  \left(  h_{\lambda_{i}%
+j-i}\left(  y\right)  \right)  _{1\leq i\leq m,\ 1\leq j\leq m}\right)
=\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)  \right)
$ (which was verified during the proof of Proposition
\ref{prop.schur.Schur=schur}), we obtain%
\[
\dfrac{\det\left(  \left(  y_{i}^{\lambda_{j-1}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }=\operatorname*{PSE}\nolimits_{N}\left(
S_{\lambda}\left(  x\right)  \right)  .
\]
Theorem \ref{thm.schur.altern} is thus proven.

We will now use a harmless-looking result about determinants:

\begin{proposition}
\label{prop.schur.det}Let $N\in\mathbb{N}$. Let $\left(  a_{i,j}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}$ be an $N\times N$-matrix of elements of a
commutative ring $R$. Let $b_{1}$, $b_{2}$, $...$, $b_{N}$ be $N$ elements of
$R$. Then,%
\begin{equation}
\sum\limits_{k=1}^{N}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  =\left(  b_{1}+b_{2}+...+b_{N}%
\right)  \det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  . \label{prop.schur.det.1}%
\end{equation}
Equivalently (in more reader-friendly terms):%
\begin{align}
&  \det\left(
\begin{array}
[c]{cccc}%
b_{1}a_{1,1} & a_{1,2} & ... & a_{1,N}\\
b_{2}a_{2,1} & a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
b_{N}a_{N,1} & a_{N,2} & ... & a_{N,N}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & b_{1}a_{1,2} & ... & a_{1,N}\\
a_{2,1} & b_{2}a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & b_{N}a_{N,2} & ... & a_{N,N}%
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +...+\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & ... & b_{1}a_{1,N}\\
a_{2,1} & a_{2,2} & ... & b_{2}a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & a_{N,2} & ... & b_{N}a_{N,N}%
\end{array}
\right) \nonumber\\
&  =\left(  b_{1}+b_{2}+...+b_{N}\right)  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & ... & a_{1,N}\\
a_{2,1} & a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & a_{N,2} & ... & a_{N,N}%
\end{array}
\right)  . \label{prop.schur.det.2}%
\end{align}

\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.det}.} Recall the explicit
formula for a determinant of a matrix as a sum over permutations: For every
$N\times N$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$,
we have%
\begin{equation}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}c_{\sigma\left(  j\right)  ,j}. \label{pf.schur.det.1}%
\end{equation}
Applied to $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}=\left(
a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$, this yields%
\begin{equation}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}a_{\sigma\left(  j\right)  ,j}. \label{pf.schur.det.2}%
\end{equation}


For every $k\in\left\{  1,2,...,N\right\}  $, we can apply
(\ref{pf.schur.det.1}) to $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}=\left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}$, and obtain%
\begin{align*}
\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)   &  =\sum\limits_{\sigma\in S_{N}}\left(
-1\right)  ^{\sigma}\underbrace{\prod\limits_{j=1}^{N}\left(  a_{\sigma\left(
j\right)  ,j}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}\right)  }%
_{=\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\prod\limits_{j=1}%
^{N}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\underbrace{\prod
\limits_{j=1}^{N}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}_{=b_{\sigma
\left(  k\right)  }^{\delta_{k,k}}\prod\limits_{\substack{j\in\left\{
1,2,...,N\right\}  ;\\j\neq k}}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\underbrace{b_{\sigma\left(
k\right)  }^{\delta_{k,k}}}_{\substack{=b_{\sigma\left(  k\right)
}\\\text{(since }\delta_{k,k}=1\text{)}}}\prod\limits_{\substack{j\in\left\{
1,2,...,N\right\}  ;\\j\neq k}}\underbrace{b_{\sigma\left(  j\right)
}^{\delta_{j,k}}}_{\substack{=1\\\text{(since }j\neq k\text{ and thus }%
\delta_{j,k}=0\text{)}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}b_{\sigma\left(  k\right)
}\underbrace{\prod\limits_{\substack{j\in\left\{  1,2,...,N\right\}  ;\\j\neq
k}}1}_{=1}=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}%
\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}b_{\sigma\left(  k\right)
}.
\end{align*}
Hence,%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\sum\limits_{k=1}^{N}\sum\limits_{\sigma\in S_{N}}\left(  -1\right)
^{\sigma}\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}b_{\sigma\left(
k\right)  }=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}%
\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\underbrace{\sum
\limits_{k=1}^{N}b_{\sigma\left(  k\right)  }}_{\substack{=\sum\limits_{k=1}%
^{N}b_{k}\\\text{(since }\sigma\text{ is a permutation)}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\sum\limits_{k=1}^{N}%
b_{k}=\underbrace{\left(  \sum\limits_{k=1}^{N}b_{k}\right)  }_{=b_{1}%
+b_{2}+...+b_{N}}\underbrace{\sum\limits_{\sigma\in S_{N}}\left(  -1\right)
^{\sigma}\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}}%
_{\substack{=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  \\\text{(by (\ref{pf.schur.det.2}))}}}\\
&  =\left(  b_{1}+b_{2}+...+b_{N}\right)  \det\left(  \left(  a_{i,j}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
This proves Proposition \ref{prop.schur.det}.

\begin{corollary}
\label{cor.schur.det}Let $N\in\mathbb{N}$. Let $\left(  i_{0},i_{1}%
,...,i_{N-1}\right)  \in\mathbb{Z}^{N}$ be such that $i_{j-1}+N>0$ for every
$j\in\left\{  1,2,...,N\right\}  $. Let $m\in\mathbb{N}$. Then,%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j,k}%
m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}

\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.det}.} Applying Proposition
\ref{prop.schur.det} to $R=\mathbb{C}\left[  y_{1},y_{2},...,y_{N}\right]  $,
$\left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}=\left(
y_{i}^{i_{j-1}+N}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ and $b_{i}%
=y_{i}^{m}$, we obtain%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\left(
y_{i}^{m}\right)  ^{\delta_{j,k}}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
Since any $i\in\left\{  1,2,...,N\right\}  $, $j\in\left\{  1,2,...,N\right\}
$ and $k\in\left\{  1,2,...,N\right\}  $ satisfy $y_{i}^{i_{j-1}+N-1}\left(
y_{i}^{m}\right)  ^{\delta_{j,k}}=y_{i}^{i_{j-1}+N+\delta_{j,k}m}%
=y_{i}^{i_{j-1}+\delta_{j,k}m+N-1}$, this rewrites as%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j,k}%
m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
Corollary \ref{cor.schur.det} is proven.

Now, to the main proof.

\textit{Proof of Theorem \ref{thm.schur}.} Define a $\mathbb{C}$-linear map
$\tau:\mathcal{F}^{\left(  0\right)  }\rightarrow\mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  $ by%
\[
\tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }0\text{-degression }\left(  i_{0}%
,i_{1},i_{2},...\right)  .
\]
(This definition makes sense, because we know that $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is a }0\text{-degression}}$ is a basis of
$\wedge^{\dfrac{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$.)

Our aim is to prove that $\tau=\sigma^{-1}$.

\textit{1st step:} First of all, the definition of $\tau$ (applied to the
$0$-degression $\left(  0,-1,-2,...\right)  $) yields%
\[
\tau\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  =S_{\left(
0+0,-1+1,-2+2,...\right)  }\left(  x\right)  =S_{\left(  0,0,0,...\right)
}\left(  x\right)  =1.
\]


\textit{2nd step:} If $N\in\mathbb{N}$, and $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $0$-degression, then we say that $\left(
i_{0},i_{1},i_{2},...\right)  $ is $N$\textit{-finished} if the following two
conditions (\ref{pf.schur.step2.fin1}) and (\ref{pf.schur.step2.fin2}) hold:%
\begin{align}
&  \left(  \text{every integer }k\geq N\text{ satisfies }i_{k}+k=0\right)
;\label{pf.schur.step2.fin1}\\
&  \left(  \text{each of the integers }i_{0}\text{, }i_{1}\text{, }...\text{,
}i_{N-1}\text{ is }>-N\right)  . \label{pf.schur.step2.fin2}%
\end{align}


Now, we claim the following:

For any $N\in\mathbb{N}$, and any $N$-finished straying $0$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, we have%
\begin{equation}
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\dfrac{\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step2}%
\end{equation}


\textit{Proof of (\ref{pf.schur.step2}):} Let $N\in\mathbb{N}$, and let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be an $N$-finished straying $0$-degression.

Since $\left(  i_{0},i_{1},i_{2},...\right)  $ is $N$-finished, we conclude
(by the definition of ``$N$-finished'') that it satisfies the conditions
(\ref{pf.schur.step2.fin1}) and (\ref{pf.schur.step2.fin2}).

If some two of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are equal, then
(\ref{pf.schur.step2}) is true.\footnote{\textit{Proof.} Assume that some two
of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are equal. Then, some two
elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ are equal,
so that $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=0$ (by the
definition of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$) and thus
$\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  0\right)  =0$. Thus, the left hand side of
(\ref{pf.schur.step2}) is $0$. On the other hand, the matrix $\left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ has two equal
columns (since two of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are
equal) and thus its determinant vanishes, i. e., we have $\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  =0$, so
that the right hand side of (\ref{pf.schur.step2}) is $0$.
\par
Thus, both the left hand side and the right hand side of (\ref{pf.schur.step2}%
) are $0$. Hence, (\ref{pf.schur.step2}) is true, qed.} Hence, for the rest of
this proof, we assume that no two of the integers $i_{0}$, $i_{1}$, $...$,
$i_{N-1}$ are equal. Then, there exists a permutation $\phi$ of the set
$\left\{  0,1,...,N-1\right\}  $ such that $i_{\phi^{-1}\left(  0\right)
}>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }$.
Consider this $\phi$.

It is easy to see that $i_{\phi^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(
1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }>-N$%
.\ \ \ \ \footnote{\textit{Proof.} Every $j\in\left\{  0,1,...,N-1\right\}  $
satisfies $\phi^{-1}\left(  j\right)  \in\phi^{-1}\left(  \left\{
0,1,...,N-1\right\}  \right)  =\left\{  0,1,...,N-1\right\}  $. Hence, for
every $j\in\left\{  0,1,...,N-1\right\}  $, the integer $i_{\phi^{-1}\left(
j\right)  }$ is one of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$, and
therefore $>-N$ (due to (\ref{pf.schur.step2.fin2})). That is, $i_{\phi
^{-1}\left(  j\right)  }>-N$ for every $j\in\left\{  0,1,...,N-1\right\}  $.
Combining this with $i_{\phi^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(
1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }$, we get $i_{\phi
^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi
^{-1}\left(  N-1\right)  }>-N$.}

Let $\pi$ be the finitary permutation of $\mathbb{N}$ which sends every
$k\in\mathbb{N}$ to \newline$\left\{
\begin{array}
[c]{l}%
\phi\left(  k\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k\in\left\{
0,1,...,N-1\right\}  ;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k\notin\left\{  0,1,...,N-1\right\}
\end{array}
\right.  $. Then, $\left(  -1\right)  ^{\pi}=\left(  -1\right)  ^{\phi}$;
moreover, every $k\in\mathbb{N}$ satisfies%
\begin{equation}
\pi^{-1}\left(  k\right)  =\left\{
\begin{array}
[c]{l}%
\phi^{-1}\left(  k\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k\in\left\{
0,1,...,N-1\right\}  ;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k\notin\left\{  0,1,...,N-1\right\}
\end{array}
\right.  . \label{pf.schur.step2.pf.2}%
\end{equation}
In particular, every integer $k\geq N$ satisfies $\pi^{-1}\left(  k\right)
=k$.

From (\ref{pf.schur.step2.pf.2}), it is clear that%
\begin{equation}
\text{every }k\in\left\{  0,1,...,N-1\right\}  \text{ satisfies }\pi
^{-1}\left(  k\right)  =\phi^{-1}\left(  k\right)  .
\label{pf.schur.step2.pf.4}%
\end{equation}
Hence, $i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)
}>...>i_{\pi^{-1}\left(  N-1\right)  }>-N$ (since $i_{\phi^{-1}\left(
0\right)  }>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(
N-1\right)  }>-N$).

Now, every integer $k\geq N$ satisfies $\pi^{-1}\left(  k\right)  =k$, thus
$i_{\pi^{-1}\left(  k\right)  }=i_{k}=-k$ (since (\ref{pf.schur.step2.fin1})
yields $i_{k}+k=0$). Hence, $-N=i_{\pi^{-1}\left(  N\right)  }>i_{\pi
^{-1}\left(  N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>...$ (because
$-N=-N>-\left(  N+1\right)  >-\left(  N+2\right)  >...$). Combined with
$i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>-N$, this becomes%
\[
i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>-N=i_{\pi^{-1}\left(  N\right)  }>i_{\pi
^{-1}\left(  N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>....
\]
Thus,%
\[
i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>i_{\pi^{-1}\left(  N\right)  }>i_{\pi^{-1}\left(
N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>....
\]
In other words, the sequence $\left(  i_{\pi^{-1}\left(  0\right)  }%
,i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)  },...\right)  $
is strictly decreasing. Since every sufficiently high $k\in\mathbb{N}$
satisfies $i_{\pi^{-1}\left(  k\right)  }+k=0$ (in fact, every $k\geq N$
satisfies $i_{\pi^{-1}\left(  k\right)  }=-k$ and thus $i_{\pi^{-1}\left(
k\right)  }+k=0$), this sequence $\left(  i_{\pi^{-1}\left(  0\right)
},i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)  },...\right)  $
must thus be a $0$-degression. Hence, by the definition of $\tau$, we have%
\[
\tau\left(  v_{i_{\pi^{-1}\left(  0\right)  }}\wedge v_{i_{\pi^{-1}\left(
1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }}\wedge...\right)
=S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  .
\]
Since $\pi$ is a finitary permutation of $\mathbb{N}$ such that $\left(
i_{\pi^{-1}\left(  0\right)  },i_{\pi^{-1}\left(  1\right)  },i_{\pi
^{-1}\left(  2\right)  },...\right)  $ is a $0$-degression, it is clear that
$\pi$ is the straightening permutation of $\left(  i_{0},i_{1},i_{2}%
,...\right)  $. Thus, by the definition of $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$, we have%
\begin{align*}
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...  &  =\underbrace{\left(
-1\right)  ^{\pi}}_{=\left(  -1\right)  ^{\phi}}v_{i_{\pi^{-1}\left(
0\right)  }}\wedge v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi
^{-1}\left(  2\right)  }}\wedge...\\
&  =\left(  -1\right)  ^{\phi}v_{i_{\pi^{-1}\left(  0\right)  }}\wedge
v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }%
}\wedge...,
\end{align*}
so that%
\begin{align}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \nonumber\\
&  =\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  \left(  -1\right)
^{\phi}v_{i_{\pi^{-1}\left(  0\right)  }}\wedge v_{i_{\pi^{-1}\left(
1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }}\wedge...\right)  \right)
\nonumber\\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}%
\underbrace{\left(  \tau\left(  v_{i_{\pi^{-1}\left(  0\right)  }}\wedge
v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }%
}\wedge...\right)  \right)  }_{\substack{=S_{\left(  i_{\pi^{-1}\left(
0\right)  }+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi^{-1}\left(  2\right)
}+2,...\right)  }\left(  x\right)  \\\text{(by the definition of }\tau\text{,
since }\left(  i_{\pi^{-1}\left(  0\right)  },i_{\pi^{-1}\left(  1\right)
},i_{\pi^{-1}\left(  2\right)  },...\right)  \text{ is a }0\text{-degression)}%
}}\nonumber\\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(
S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  \right)  .
\label{pf.schur.step2.pf.4a}%
\end{align}


Let $\mu$ be the partition $\left(  i_{\pi^{-1}\left(  0\right)  }%
+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi^{-1}\left(  2\right)
}+2,...\right)  $. For every positive integer $\alpha$, let $\mu_{\alpha}$
denote the $\alpha$-th part of the partition $\mu$, so that $\mu=\left(
\mu_{1},\mu_{2},\mu_{3},...\right)  $. Then, every $j\in\left\{
1,2,...,N\right\}  $ satisfies%
\begin{align*}
\mu_{j}  &  =i_{\pi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mu\right) \\
&  =i_{\phi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since (\ref{pf.schur.step2.pf.4}) (applied
to }k=j-1\text{) yields }\pi^{-1}\left(  j-1\right)  =\phi^{-1}\left(
j-1\right)  \right)  ,
\end{align*}
so that $\mu_{j}+N-j=i_{\phi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
+N-j=i_{\phi^{-1}\left(  j-1\right)  }+N-1$. Hence,%
\begin{equation}
\det\left(  \left(  y_{i}^{\mu_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  =\det\left(  \left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\label{pf.schur.step2.pf.5}%
\end{equation}
But the matrix $\left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }+N-1}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}$ is obtained from the matrix $\left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ by permuting the
columns using the permutation $\phi$. Hence,%
\[
\det\left(  \left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }+N-1}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  =\left(  -1\right)  ^{\phi}%
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)
\]
(since permuting the columns of a matrix changes the determinant by the sign
of the permutation). Combining this with (\ref{pf.schur.step2.pf.5}), we
obtain%
\begin{equation}
\det\left(  \left(  y_{i}^{\mu_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  =\left(  -1\right)  ^{\phi}\det\left(  \left(  y_{i}^{i_{j-1}%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\label{pf.schur.step2.pf.7}%
\end{equation}


Also, by the definition of $\mu$, we have $\mu_{N+1}=i_{\pi^{-1}\left(
N\right)  }+N=0$ (because $-N=i_{\pi^{-1}\left(  N\right)  }$), and thus we
can apply Theorem \ref{thm.schur.altern} to $\mu$ instead of $\lambda$. This
results in
\begin{align}
\operatorname*{PSE}\nolimits_{N}\left(  S_{\mu}\left(  x\right)  \right)   &
=\dfrac{\det\left(  \left(  y_{i}^{\mu_{j}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }\nonumber\\
&  =\dfrac{\left(  -1\right)  ^{\phi}\det\left(  \left(  y_{i}^{i_{j-1}%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(
y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }
\label{pf.schur.step2.pf.8}%
\end{align}
(by (\ref{pf.schur.step2.pf.7})). But (\ref{pf.schur.step2.pf.4a}) becomes%
\begin{align*}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(
S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  \right) \\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(  S_{\mu
}\left(  x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi
^{-1}\left(  2\right)  }+2,...\right)  =\mu\right) \\
&  =\left(  -1\right)  ^{\phi}\dfrac{\left(  -1\right)  ^{\phi}\det\left(
\left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.step2.pf.8}%
)}\right) \\
&  =\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }.
\end{align*}
This proves (\ref{pf.schur.step2}). The proof of the 2nd step is thus complete.

\textit{3rd step:} Consider the action of the Heisenberg algebra $\mathcal{A}$
on $\widetilde{F}=\mathcal{B}^{\left(  0\right)  }$ and $\wedge^{\dfrac
{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$. We will now prove that the
map $\tau:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ satisfies%
\begin{equation}
\tau\circ a_{-m}=a_{-m}\circ\tau\ \ \ \ \ \ \ \ \ \ \text{for every positive
integer }m. \label{pf.schur.step3}%
\end{equation}


\textit{Proof of (\ref{pf.schur.step3}):} Let $m$ be a positive integer.

Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. By the
definition of a $0$-degression, $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
strictly decreasing sequence of integers such that every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=0$. In other words, there exists an
$\ell\in\mathbb{N}$ such that every integer $k\geq\ell$ satisfies $i_{k}+k=0$.
Consider this $\ell$.

Let $N$ be any integer satisfying $N\geq\ell+m$. Then, it is easy to see that,
for every integer $k\geq N$, we have $i_{k}+m=i_{k-m}$.

By the definition of the $\mathcal{A}$-module structure on $\wedge
^{\dfrac{\infty}{2},0}V$, the action of $a_{-m}$ on $\wedge^{\dfrac{\infty}%
{2},0}V$ is $\widehat{\rho}\left(  T^{-m}\right)  $, where $T$ is the shift
operator. Thus,%
\begin{equation}
a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\left(  \widehat{\rho}\left(  T^{-m}\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  . \label{pf.schur.step3.2}%
\end{equation}


Since $m\neq0$, the matrix $T^{-m}$ has the property that, for every integer
$i$, the $\left(  i,i\right)  $-th entry of $T^{-m}$ is $0$. Hence,
Proposition \ref{prop.glinf.ainfact} (applied to $0$, $T^{-m}$ and $v_{i_{k}}$
instead of $m$, $a$ and $b_{k}$) yields%
\begin{align*}
&  \left(  \widehat{\rho}\left(  T^{-m}\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{-m}\rightharpoonup v_{i_{k}}\right)
}_{=v_{i_{k}+m}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\underbrace{\sum\limits_{\substack{k\geq0;\\k<N}}}_{=\sum\limits_{k=0}%
^{N-1}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{\substack{=v_{i_{0}%
+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge...\wedge v_{i_{k-1}%
+\delta_{k-1,k}m}\wedge v_{i_{k}+\delta_{k,k}m}\wedge v_{i_{k+1}%
+\delta_{k+1,k}m}\wedge v_{i_{k+2}+\delta_{k+2,k}m}\wedge...\\\text{(here we
are simply making use of the fact that every }j\in\mathbb{N}\text{ such that
}j\neq k\text{ satisfies}\\i_{j}=i_{j}+\delta_{j,k}m\text{ (since }%
\delta_{j,k}=0\text{), whereas }i_{k}+m=i_{k}+\delta_{k,k}m\text{ (since
}\delta_{k,k}=1\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq N}\underbrace{v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...}_{\substack{=0\text{ (because the sequence }\left(
i_{0},i_{1},...,i_{k-1},i_{k}+m,i_{k+1},i_{k+2},...\right)  \\\text{has two
equal elements (since }i_{k}+m=i_{k-m}\text{))}}}\\
&  =\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta
_{1,k}m}\wedge...\wedge v_{i_{k-1}+\delta_{k-1,k}m}\wedge v_{i_{k}%
+\delta_{k,k}m}\wedge v_{i_{k+1}+\delta_{k+1,k}m}\wedge v_{i_{k+2}%
+\delta_{k+2,k}m}\wedge...\\
&  =\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta
_{1,k}m}\wedge v_{i_{2}+\delta_{2,k}m}\wedge....
\end{align*}
Combined with (\ref{pf.schur.step3.2}), this yields%
\[
a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}%
m}\wedge v_{i_{2}+\delta_{2,k}m}\wedge...,
\]
so that%
\begin{align}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
\nonumber\\
&  =\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  \sum\limits_{k=0}%
^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge
v_{i_{2}+\delta_{2,k}m}\wedge...\right)  \right) \nonumber\\
&  =\sum\limits_{k=0}^{N-1}\underbrace{\operatorname*{PSE}\nolimits_{N}\left(
\tau\left(  v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge
v_{i_{2}+\delta_{2,k}m}\wedge...\right)  \right)  }_{\substack{=\dfrac
{\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j-1,k}m+N-1}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }\\\text{(by (\ref{pf.schur.step2}), applied
to }\left(  i_{0}+\delta_{0,k}m,i_{1}+\delta_{1,k}m,i_{2}+\delta
_{2,k}m,...\right)  \\\text{instead of }\left(  i_{0},i_{1},i_{2},...\right)
\text{ (since }\left(  i_{0}+\delta_{0,k}m,i_{1}+\delta_{1,k}m,i_{2}%
+\delta_{2,k}m,...\right)  \\\text{is easily seen to be an }N\text{-finished
straying }0\text{-degression))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\text{ and }\tau\text{ are both linear}\right) \nonumber\\
&  =\sum\limits_{k=0}^{N-1}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j-1,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\nonumber\\
&  =\sum\limits_{k=1}^{N}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j-1,k-1}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k-1\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{k=1}^{N}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{j-1,k-1}%
=\delta_{j,k}\text{ for all }j\text{ and }k\right) \nonumber\\
&  =\dfrac{1}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }\underbrace{\sum\limits_{k=1}^{N}\det\left(  \left(
y_{i}^{i_{j-1}+\delta_{j,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }_{\substack{=\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  \\\text{(by Corollary \ref{cor.schur.det})}}}\nonumber\\
&  =\dfrac{1}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right) \nonumber\\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \cdot\dfrac{\det\left(
\left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step3.8}%
\end{align}
On the other hand, since $\left(  i_{0},i_{1},i_{2},...\right)  $ is strictly
decreasing, $\left(  i_{0},i_{1},i_{2},...\right)  $ is $N$-finished. Thus,
(\ref{pf.schur.step2}) yields%
\begin{equation}
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\dfrac{\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step3.9}%
\end{equation}
Now, (\ref{pf.schur.step3.8}) becomes%
\begin{align*}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right) \\
&  =\underbrace{\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
}_{\substack{=m\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)
\\\text{(since the definition of }\operatorname*{PSE}\nolimits_{N}\text{
yields}\\\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)  =\dfrac
{y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}}{m}\text{)}}}\cdot\underbrace{\dfrac
{\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }}_{\substack{=\operatorname*{PSE}\nolimits_{N}\left(
\tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\right)  \\\text{(by (\ref{pf.schur.step3.9}))}}}\\
&  =m\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)  \cdot
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\operatorname*{PSE}\nolimits_{N}\underbrace{\left(  mx_{m}\cdot\tau\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)
}_{\substack{=a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  \\\text{(since }a_{-m}\text{ acts on
}\widetilde{F}\text{ as multiplication by }mx_{m}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\text{ is a }\mathbb{C}\text{-algebra homomorphism}\right) \\
&  =\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)  .
\end{align*}
Now forget that we fixed $N$. We thus have shown that every integer $N\geq
\ell+m$ satisfies%
\[
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
=\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)  .
\]
Hence,%
\[
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
=\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
\]
for every sufficiently high $N\in\mathbb{N}$. Thus, Corollary
\ref{cor.schur.PSEinj} (applied to $P=\tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  $ and
$Q=a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  $) yields that
\[
\tau\left(  a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  =a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  .
\]
In other words,%
\[
\left(  \tau\circ a_{-m}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  =\left(  a_{-m}\circ\tau\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\]
Now forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We have
thus shown that $\left(  \tau\circ a_{-m}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(  a_{-m}\circ\tau\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ for every
$0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, the maps
$\tau\circ a_{-m}$ and $a_{-m}\circ\tau$ are equal to each other on a basis of
$\wedge^{\dfrac{\infty}{2},0}V$ (namely, on the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is a }0\text{-degression}}$). Since these two maps
are linear, this yields that these two maps must be identical, i. e., we have
$\tau\circ a_{-m}=a_{-m}\circ\tau$. This proves (\ref{pf.schur.step3}). The
proof of the 3rd step is thus complete.

\textit{4th step:} We can now easily conclude Theorem \ref{thm.schur}.

Let $\mathcal{A}_{-}$ be the Lie subalgebra $\left\langle a_{-1},a_{-2}%
,a_{-3},...\right\rangle $ of $\mathcal{A}$. Then, $\tau$ is an $\mathcal{A}%
_{-}$-module homomorphism $\wedge^{\dfrac{\infty}{2},0}V\rightarrow
\widetilde{F}$ (according to (\ref{pf.schur.step3})).

Consider the element $\psi_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$ of
$\wedge^{\dfrac{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$. By the
definition of $\sigma_{0}$, we have $\sigma_{0}\left(  1\right)  =\psi_{0}$,
so that $\sigma_{0}^{-1}\left(  \psi_{0}\right)  =1$. Compared with%
\begin{align*}
\tau\left(  \psi_{0}\right)   &  =\tau\left(  v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi
_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right) \\
&  =1,
\end{align*}
this yields $\tau\left(  \psi_{0}\right)  =\sigma_{0}^{-1}\left(  \psi
_{0}\right)  $.

From Lemma \ref{lem.F.P1=P}, it is clear that the Fock module $F$ is generated
by $1$ as an $\mathcal{A}_{-}$-module (since $\mathcal{A}_{-}=\left\langle
a_{-1},a_{-2},a_{-3},...\right\rangle $). Since there exists an $\mathcal{A}%
_{-}$-module isomorphism $F\rightarrow\widetilde{F}$ which sends $1$ to $1$
(in fact, the map $\operatorname*{resc}$ of Proposition \ref{prop.resc} is
such an isomorphism), this yields that $\widetilde{F}$ is generated by $1$ as
an $\mathcal{A}_{-}$-module. Since there exists an $\mathcal{A}_{-}$-module
isomorphism $\widetilde{F}\rightarrow\wedge^{\dfrac{\infty}{2},0}V$ which
sends $1$ to $\psi_{0}$ (in fact, the map $\sigma_{0}$ is such an isomorphism,
since $\sigma_{0}\left(  1\right)  =\psi_{0}$), this yields that
$\wedge^{\dfrac{\infty}{2},0}V$ is generated by $\psi_{0}$ as an
$\mathcal{A}_{-}$-module. Hence, if two $\mathcal{A}_{-}$-module homomorphisms
from $\wedge^{\dfrac{\infty}{2},0}V$ to another $\mathcal{A}_{-}$-module are
equal to each other on $\psi_{0}$, then they must be identical. We can apply
this observation to the two $\mathcal{A}_{-}$-module homomorphisms
$\tau:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ and $\sigma
_{0}^{-1}:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ (which are
equal to each other on $\psi_{0}$, since $\tau\left(  \psi_{0}\right)
=\sigma_{0}^{-1}\left(  \psi_{0}\right)  $), and conclude that these
homomorphisms are identical, i. e., we have $\tau=\sigma_{0}^{-1}$. Now, every
$0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =\underbrace{\sigma_{0}^{-1}}_{=\tau}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\right) \\
&  =S_{\lambda}\left(  x\right)  ,
\end{align*}
where $\lambda=\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $. This proves
Theorem \ref{thm.schur}.

\subsection{\label{subsect.schur.pf2}Expliciting
\texorpdfstring{$\sigma^{-1}$}{the inverse of the Boson-Fermion
correspondence} using Schur polynomials: second proof}

We are next going to give a second proof of Theorem \ref{thm.schur}. We will
give this proof in two versions: The first version (Subsection
\ref{subsubsect.schur2}) will proceed by manipulations with infinite matrices,
using various properties of infinite matrices acting on $\wedge^{\dfrac
{\infty}{2},m}V$. Since we are not going to prove all these properties, this
first version is not completely self-contained (although the missing proofs
are easy to fill in). The second version (Subsection
\ref{subsubsect.schur2.finitary}) will be a rewriting of the first version
without the use of all these properties of infinite matrices; it is
self-contained. Both versions of the proof require lengthy preparations, some
of which (like the definition of $\operatorname*{GL}\left(  \infty\right)  $)
will also turn out useful to us later.

\subsubsection{\label{subsubsect.newton}The multivariate Taylor formula}

Before we step to the second proof of Theorem \ref{thm.schur}, we show a lemma
about polynomials over $\mathbb{Q}$-algebras:

\begin{lemma}
\label{lem.hirota.newton}Let $K$ be a commutative $\mathbb{Q}$-algebra, let
$\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of elements of $K$, and
let $\left(  z_{1},z_{2},z_{3},...\right)  $ be a sequence of new symbols.
Denote the sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ by $y$. Denote
the sequence $\left(  z_{1},z_{2},z_{3},...\right)  $ by $z$. Then, every
$P\in K\left[  z_{1},z_{2},z_{3},...\right]  $ satisfies%
\[
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
P\left(  z\right)  =P\left(  y+z\right)  .
\]
Here, $y+z$ means the componentwise sum of the sequences $y$ and $z$ (so that
$y+z=\left(  y_{1}+z_{1},y_{2}+z_{2},y_{3}+z_{3},...\right)  $).
\end{lemma}

Lemma \ref{lem.hirota.newton} is actually a multivariate generalization of the
famous Taylor formula%
\[
\exp\left(  \alpha\dfrac{\partial}{\partial\xi}\right)  P\left(  \xi\right)
=P\left(  \alpha+\xi\right)
\]
which holds for any polynomial $P\in K\left[  \xi\right]  $ and any $\alpha\in
K$.

\textit{Proof of Lemma \ref{lem.hirota.newton}.} Let $A$ be the map%
\[
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
:K\left[  z_{1},z_{2},z_{3},...\right]  \rightarrow K\left[  z_{1},z_{2}%
,z_{3},...\right]
\]
(this is easily seen to be well-defined). Let $B$ be the map%
\[
K\left[  z_{1},z_{2},z_{3},...\right]  \rightarrow K\left[  z_{1},z_{2}%
,z_{3},...\right]  ,\ \ \ \ \ \ \ \ \ \ P\mapsto P\left(  y+z\right)  .
\]


We have $A=\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  $, so that $A$ is the exponential of a derivation (since
$\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}$ is a derivation).
Thus, $A$ is a $K$-algebra homomorphism (since there is a known fact that the
exponential of a derivation is a $K$-algebra homomorphism). Combined with the
fact that $B$ is a $K$-algebra homomorphism (in fact, $B$ is an evaluation
homomorphism), this yields that both $A$ and $B$ are $K$-algebra homomorphisms.

Now, let $k$ be a positive integer. We will prove that $Az_{k}=Bz_{k}$.

We have
\[
\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
z_{k}=\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}z_{k}%
=y_{k}\underbrace{\dfrac{\partial}{\partial z_{k}}z_{k}}_{=1}+\sum
\limits_{\substack{s>0;\\s\neq k}}y_{s}\underbrace{\dfrac{\partial}{\partial
z_{s}}z_{k}}_{\substack{=0\\\text{(since }s\neq k\text{)}}}=y_{k}%
+\underbrace{\sum\limits_{\substack{s>0;\\s\neq k}}y_{s}0}_{=0}=y_{k},
\]
so that%
\[
\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
^{2}z_{k}=\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  \underbrace{\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
z_{s}}\right)  z_{k}}_{=y_{k}}=\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial
}{\partial z_{s}}\right)  y_{k}=\sum\limits_{s>0}y_{s}\underbrace{\dfrac
{\partial}{\partial z_{s}}y_{k}}_{=0}=0.
\]
As a consequence,
\begin{equation}
\text{every integer }i\geq2\text{ satisfies }\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}z_{k}=0.
\label{pf.hirota.newton.1}%
\end{equation}
Now, since $A=\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
z_{s}}\right)  =\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}\left(
\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}$, we have%
\begin{align*}
Az_{k}  &  =\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}\left(  \sum
\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}z_{k}\\
&  =\underbrace{\dfrac{1}{0!}}_{=1}\underbrace{\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{0}}_{=\operatorname*{id}}%
z_{k}+\underbrace{\dfrac{1}{1!}}_{=1}\underbrace{\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{1}}_{=\sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}}z_{k}+\sum\limits_{i\geq2}\dfrac{1}%
{i!}\underbrace{\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  ^{i}z_{k}}_{\substack{=0\\\text{(by (\ref{pf.hirota.newton.1}))}}}\\
&  =\underbrace{\operatorname*{id}z_{k}}_{=z_{k}}+\underbrace{\left(
\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  z_{k}}_{=y_{k}%
}+\underbrace{\sum\limits_{i\geq2}\dfrac{1}{i!}0}_{=0}=z_{k}+y_{k}=y_{k}%
+z_{k}.
\end{align*}
Compared to
\begin{align*}
Bz_{k}  &  =z_{k}\left(  y+z\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }B\right) \\
&  =y_{k}+z_{k},
\end{align*}
this yields $Az_{k}=Bz_{k}$.

Now, forget that we fixed $k$. We thus have shown that $Az_{k}=Bz_{k}$ for
every positive integer $k$. In other words, the maps $A$ and $B$ coincide on
the set $\left\{  z_{1},z_{2},z_{3},...\right\}  $. Since the set $\left\{
z_{1},z_{2},z_{3},...\right\}  $ generates $K\left[  z_{1},z_{2}%
,z_{3},...\right]  $ as a $K$-algebra, this yields that the maps $A$ and $B$
coincide on a generating set of the $K$-algebra $K\left[  z_{1},z_{2}%
,z_{3},...\right]  $. Since $A$ and $B$ are $K$-algebra homomorphisms, this
yields that $A=B$ (because if two $K$-algebra homomorphisms coincide on a
$K$-algebra generating set of their domain, then they must be equal). Hence,
every $P\in K\left[  z_{1},z_{2},z_{3},...\right]  $ satisfies%
\[
\underbrace{\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  }_{=A=B}\underbrace{P\left(  z\right)  }_{=P}=BP=P\left(
y+z\right)
\]
(by the definition of $B$). This proves Lemma \ref{lem.hirota.newton}.

\subsubsection{\label{subsubsect.GLinfty}
\texorpdfstring{$\operatorname*{GL}\left( \infty\right)  $}{GL-infinity} and
\texorpdfstring{$\operatorname*{M}\left(  \infty\right)  $}{M-infinity}}

We now introduce the groups $\operatorname*{GL}\left(  \infty\right)  $ and
$\operatorname*{M}\left(  \infty\right)  $ and their actions on $\wedge
^{\dfrac{\infty}{2},m}V$. On the one hand, this will prepare us to the second
proof of Theorem \ref{thm.schur}; on the other hand, these group actions are
of autonomous interest, and we will meet them again in Subsection
\ref{subsubsect.infgrass}.

\begin{definition}
\label{def.Minf}We let $\operatorname*{M}\left(  \infty\right)  $ denote the
set $\operatorname*{id}+\mathfrak{gl}_{\infty}$. In other words, we let
$\operatorname*{M}\left(  \infty\right)  $ denote the set of all infinite
matrices (infinite in both directions) which are equal to the infinite
identity matrix $\operatorname*{id}$ in all but finitely many entries.
\end{definition}

Clearly, $\operatorname*{M}\left(  \infty\right)  \subseteq\overline
{\mathfrak{a}_{\infty}}$ as sets. We notice that:

\begin{proposition}
\label{prop.Minf.monoid}\textbf{(a)} For every $A\in\operatorname*{M}\left(
\infty\right)  $ and $B\in\operatorname*{M}\left(  \infty\right)  $, the
matrix $AB$ is well-defined and lies in $\operatorname*{M}\left(
\infty\right)  $.

\textbf{(b)} We have $\operatorname*{id}\in\operatorname*{M}\left(
\infty\right)  $ (where $\operatorname*{id}$ denotes the infinite identity matrix).

\textbf{(c)} The set $\operatorname*{M}\left(  \infty\right)  $ becomes a
monoid under multiplication of matrices.

\textbf{(d)} If a matrix $A\in\operatorname*{M}\left(  \infty\right)  $ is
invertible, then its inverse also lies in $\operatorname*{M}\left(
\infty\right)  $.

\textbf{(e)} Denote by $\operatorname*{GL}\left(  \infty\right)  $ the subset
$\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ A\text{ is
invertible}\right\}  $ of $\operatorname*{M}\left(  \infty\right)  $. Then,
$\operatorname*{GL}\left(  \infty\right)  $ becomes a group under
multiplication of matrices.
\end{proposition}

\begin{remark}
\label{rmk.GLinf}In Proposition \ref{prop.Minf.monoid}, a matrix
$A\in\operatorname*{M}\left(  \infty\right)  $ is said to be
\textit{invertible} if there exists an infinite matrix $B$ (with rows and
columns indexed by integers) satisfying $AB=BA=\operatorname*{id}$. The matrix
$B$ is then called the \textit{inverse} of $A$. Note that we don't a-priori
require that $B$ lie in $\operatorname*{M}\left(  \infty\right)  $, or any
other ``finiteness conditions'' for $B$; Proposition \ref{prop.Minf.monoid}
\textbf{(d)} shows that these conditions are automatically satisfied.
\end{remark}

\begin{definition}
\label{def.GLinf}Let $\operatorname*{GL}\left(  \infty\right)  $ denote the
group $\operatorname*{GL}\left(  \infty\right)  $ defined in Proposition
\ref{prop.Minf.monoid} \textbf{(e)}.
\end{definition}

\textit{Proof of Proposition \ref{prop.Minf.monoid}.} \textbf{(a)} Let
$A\in\operatorname*{M}\left(  \infty\right)  $ and $B\in\operatorname*{M}%
\left(  \infty\right)  $. Since $A\in\operatorname*{M}\left(  \infty\right)
=\operatorname*{id}+\mathfrak{gl}_{\infty}$, there exists an $a\in
\mathfrak{gl}_{\infty}$ such that $A=\operatorname*{id}+a$. Consider this $a$.

Since $B\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists a $b\in\mathfrak{gl}_{\infty}$ such
that $B=\operatorname*{id}+b$. Consider this $b$.

Since $A=\operatorname*{id}+a$ and $B=\operatorname*{id}+b$, we have
$AB=\left(  \operatorname*{id}+a\right)  \left(  \operatorname*{id}+b\right)
=\operatorname*{id}+a+b+ab$, which is clearly well-defined (because
$a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ lead to $ab$
being well-defined) and lies in $\operatorname*{M}\left(  \infty\right)  $
(since $\underbrace{a}_{\in\mathfrak{gl}_{\infty}}+\underbrace{b}%
_{\in\mathfrak{gl}_{\infty}}+\underbrace{ab}_{\substack{\in\mathfrak{gl}%
_{\infty}\\\text{(since }a\in\mathfrak{gl}_{\infty}\text{ and }b\in
\mathfrak{gl}_{\infty}\text{)}}}\in\mathfrak{gl}_{\infty}+\mathfrak{gl}%
_{\infty}+\mathfrak{gl}_{\infty}\subseteq\mathfrak{gl}_{\infty}$ and thus
$\operatorname*{id}+a+b+ab\in\operatorname*{id}+\mathfrak{gl}_{\infty
}=\operatorname*{M}\left(  \infty\right)  $). This proves Proposition
\ref{prop.Minf.monoid} \textbf{(a)}.

\textbf{(b)} Trivial.

\textbf{(c)} Follows from \textbf{(a)} and \textbf{(b)}.

\textbf{(d)} Let $A\in\operatorname*{M}\left(  \infty\right)  $ be invertible.

Since $A\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists an $a\in\mathfrak{gl}_{\infty}$ such
that $A=\operatorname*{id}+a$. Consider this $a$.

Since $A$ is invertible, there exists an infinite matrix $B$ (with rows and
columns indexed by integers) satisfying $AB=BA=\operatorname*{id}$ (according
to how we defined ``invertible'' in Remark \ref{rmk.GLinf}). Consider this
$B$. This $B$ is the inverse of $A$. Let $b=B-\operatorname*{id}$. Then,
$B=\operatorname*{id}+b$. Since $A=\operatorname*{id}+a$ and
$B=\operatorname*{id}+b$, we have $AB=\left(  \operatorname*{id}+a\right)
\left(  \operatorname*{id}+b\right)  =\operatorname*{id}+a+b+ab$, which is
clearly well-defined (because $a\in\mathfrak{gl}_{\infty}$ leads to $ab$ being
well-defined). Since $\operatorname*{id}=AB=\operatorname*{id}+ab+a+b$, we
have $0=ab+a+b$.

Let us introduce two notations that we will use during this proof:

\begin{itemize}
\item For any infinite matrix $M$ and any pair $\left(  i,j\right)  $ of
integers, let us denote by $M_{i,j}$ the $\left(  i,j\right)  $-th entry of
the matrix $M$. (In particular, for any pair $\left(  i,j\right)  $ of
integers, we denote by $a_{i,j}$ the $\left(  i,j\right)  $-th entry of the
matrix $a$ (not of the matrix $A$ !), and we denote by $b_{i,j}$ the $\left(
i,j\right)  $-th entry of the matrix $b$ (not of the matrix $B$ !).)

\item For any assertion $\mathcal{A}$, let $\left[  \mathcal{A}\right]  $
denote the integer $\left\{
\begin{array}
[c]{l}%
1,\text{ if }\mathcal{A}\text{ is true;}\\
0,\text{ if }\mathcal{A}\text{ is wrong}%
\end{array}
\right.  $.
\end{itemize}

Since $a\in\mathfrak{gl}_{\infty}$, only finitely many entries of the matrix
$a$ are nonzero. In particular, this yields that only finitely many columns of
the matrix $a$ are nonzero. Hence, there exists a nonnegative integer $N$ such
that
\begin{equation}
\left(  \text{for every integer }j\text{ with }\left\vert j\right\vert
>N\text{, the }j\text{-th column of }a\text{ is zero}\right)  .
\label{pf.Minf.monoid.1}%
\end{equation}
Consider this $N$. Clearly,%
\begin{equation}
\left(  \text{for every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that
}\left\vert j\right\vert >N\text{, we have }a_{i,j}=0\right)
\label{pf.Minf.monoid.1a}%
\end{equation}
(because for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert j\right\vert >N$, the $j$-th column of $a$ is zero (by
(\ref{pf.Minf.monoid.1})), so that every entry on the $j$-th column of $a$ is
zero, so that $a_{i,j}$ is zero (because the element $a_{i,j}$ is the $\left(
i,j\right)  $-th entry of $a$, hence an entry on the $j$-th column of $a$)).

Recall that only finitely many entries of the matrix $a$ are nonzero. In
particular, this yields that only finitely many rows of the matrix $a$ are
nonzero. Hence, there exists a nonnegative integer $M$ such that
\begin{equation}
\left(  \text{for every integer }i\text{ with }\left\vert i\right\vert
>M\text{, the }i\text{-th row of }a\text{ is zero}\right)  .
\label{pf.Minf.monoid.2}%
\end{equation}
Consider this $M$. Clearly,%
\begin{equation}
\left(  \text{for every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that
}\left\vert i\right\vert >M\text{, we have }a_{i,j}=0\right)
\label{pf.Minf.monoid.2a}%
\end{equation}
(because for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert i\right\vert >M$, the $i$-th row of $a$ is zero (by
(\ref{pf.Minf.monoid.2})), so that every entry on the $i$-th row of $a$ is
zero, so that $a_{i,j}$ is zero (because the element $a_{i,j}$ is the $\left(
i,j\right)  $-th entry of $a$, hence an entry on the $i$-th row of $a$)).

Let $P=\max\left\{  M,N\right\}  $. Clearly, $P\geq M$ and $P\geq N$. It is
now easy to see that
\begin{equation}
\text{any }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfies }%
a_{i,j}=\left[  \left\vert i\right\vert \leq P\right]  \cdot a_{i,j}.
\label{pf.Minf.monoid.a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.Minf.monoid.a}):} Let $\left(  i,j\right)
\in\mathbb{Z}^{2}$. Then, we must be in one of the following three cases:
\par
\textit{Case 1:} We don't have $\left\vert i\right\vert \leq P$.
\par
\textit{Case 2:} We have $\left\vert i\right\vert \leq P$.
\par
Let us consider Case 1 first. In this case, we don't have $\left\vert
i\right\vert \leq P$. Thus, $\left[  \left\vert i\right\vert \leq P\right]
=0$ and $\left\vert i\right\vert >P$. From $\left\vert i\right\vert >P\geq M$,
we conclude that $a_{i,j}=0$ (by (\ref{pf.Minf.monoid.2a})). Compared with
$\underbrace{\left[  \left\vert i\right\vert \leq P\right]  }_{=0}\cdot
a_{i,j}=0$, this yields $a_{i,j}=\left[  \left\vert i\right\vert \leq
P\right]  \cdot a_{i,j}$. Hence, (\ref{pf.Minf.monoid.a}) is proven in Case 1.
\par
Finally, let us consider Case 2. In this case, we have $\left\vert
i\right\vert \leq P$. Hence, $\left[  \left\vert i\right\vert \leq P\right]
=1$. Thus, $\underbrace{\left[  \left\vert i\right\vert \leq P\right]  }%
_{=1}\cdot a_{i,j}=a_{i,j}$. Hence, (\ref{pf.Minf.monoid.a}) is proven in Case
2.
\par
Altogether, we have thus proven (\ref{pf.Minf.monoid.a}) in each of the two
cases 1 and 2. Since these two cases cover all possibilities, this shows that
(\ref{pf.Minf.monoid.a}) always holds. Thus, (\ref{pf.Minf.monoid.a}) is
proven.} Similarly,%
\begin{equation}
\text{any }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfies }%
a_{i,j}=\left[  \left\vert j\right\vert \leq P\right]  \cdot a_{i,j}.
\label{pf.Minf.monoid.a'}%
\end{equation}


Let $b^{\prime}$ be the infinite matrix (with rows and columns indexed by
integers) defined by%
\begin{equation}
\left(  b_{i,j}^{\prime}=\left[  \left\vert i\right\vert \leq P\right]
\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{i,j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  i,j\right)  \in\mathbb{Z}%
^{2}\right)  . \label{pf.Minf.monoid.bprime}%
\end{equation}
It is clear that only finitely many entries of $b^{\prime}$ are
nonzero\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\mathbb{Z}^{2}$
such that $b_{i,j}^{\prime}\neq0$. Then, $\left\vert i\right\vert \leq P$
(because otherwise, we would have $\left[  \left\vert i\right\vert \leq
P\right]  =0$, so that $b_{i,j}^{\prime}=\underbrace{\left[  \left\vert
i\right\vert \leq P\right]  }_{=0}\cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}=0$, contradicting to $b_{i,j}^{\prime}\neq0$), so that
$i\in\left\{  -P,-P+1,...,P\right\}  $, and similarly $j\in\left\{
-P,-P+1,...,P\right\}  $. Hence, $\left(  i,j\right)  \in\left\{
-P,-P+1,...,P\right\}  ^{2}$ (since $i\in\left\{  -P,-P+1,...,P\right\}  $ and
$j\in\left\{  -P,-P+1,...,P\right\}  $).
\par
Now forget that we fixed $\left(  i,j\right)  $. We thus have showed that
every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $b_{i,j}^{\prime}%
\neq0$ satisfies $\left(  i,j\right)  \in\left\{  -P,-P+1,...,P\right\}  ^{2}%
$. Since there are only finitely many $\left(  i,j\right)  \in\left\{
-P,-P+1,...,P\right\}  ^{2}$, this yields that there are only finitely many
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $b_{i,j}^{\prime}\neq0$. In
other words, there are only finitely many $\left(  i,j\right)  \in
\mathbb{Z}^{2}$ such that the $\left(  i,j\right)  $-th entry of $b^{\prime}$
is nonzero. In other words, only finitely many entries of $b^{\prime}$ are
nonzero, qed.}. In other words, $b^{\prime}\in\mathfrak{gl}_{\infty}$, so that
$\operatorname*{id}+b^{\prime}\in\operatorname*{id}+\mathfrak{gl}_{\infty
}=\operatorname*{M}\left(  \infty\right)  $.

We will now prove that $A\left(  \operatorname*{id}+b^{\prime}\right)
=\operatorname*{id}$.

For every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, we have%
\begin{align*}
&  \left(  ab^{\prime}+a+b^{\prime}\right)  _{i,j}\\
&  =\underbrace{\left(  ab^{\prime}\right)  _{i,j}}_{\substack{=\sum
\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}^{\prime}\\\text{(by the definition of
the}\\\text{product of two matrices)}}}+a_{i,j}+\underbrace{b_{i,j}^{\prime}%
}_{\substack{=\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\\text{(by
(\ref{pf.Minf.monoid.bprime}))}}}\\
&  =\sum\limits_{k\in\mathbb{Z}}a_{i,k}\underbrace{b_{k,j}^{\prime}%
}_{\substack{=\left[  \left\vert k\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{k,j}\\\text{(by
(\ref{pf.Minf.monoid.bprime}), applied to}\\k\text{ instead of }i\text{)}%
}}+a_{i,j}+\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}\left[  \left\vert
k\right\vert \leq P\right]  }_{\substack{=\left[  \left\vert k\right\vert \leq
P\right]  \cdot a_{i,k}=a_{i,k}\\\text{(since (\ref{pf.Minf.monoid.a'})
(applied to}\\k\text{ instead of }j\text{) yields }a_{i,k}=\left[  \left\vert
k\right\vert \leq P\right]  \cdot a_{i,k}\text{)}}}\cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot b_{k,j}+\underbrace{a_{i,j}}%
_{\substack{=\left[  \left\vert i\right\vert \leq P\right]  \cdot
a_{i,j}\\\text{(by (\ref{pf.Minf.monoid.a}))}}}+\left[  \left\vert
i\right\vert \leq P\right]  \cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}}_{\substack{=\left[
\left\vert i\right\vert \leq P\right]  \cdot a_{i,k}\\\text{(by
(\ref{pf.Minf.monoid.a}), applied to}\\k\text{ instead of }j\text{)}}%
}\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{k,j}+\left[
\left\vert i\right\vert \leq P\right]  \cdot\underbrace{a_{i,j}}%
_{\substack{=\left[  \left\vert j\right\vert \leq P\right]  \cdot
a_{i,j}\\\text{(by (\ref{pf.Minf.monoid.a'}))}}}+\left[  \left\vert
i\right\vert \leq P\right]  \cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\left[  \left\vert i\right\vert \leq P\right]
\cdot a_{i,k}\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot
b_{k,j}+\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot a_{i,j}+\left[  \left\vert i\right\vert \leq
P\right]  \cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\
&  =\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}+a_{i,j}+b_{i,j}\right)  =\left[  \left\vert i\right\vert \leq
P\right]  \cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot
\underbrace{\left(  \left(  ab\right)  _{i,j}+a_{i,j}+b_{i,j}\right)
}_{\substack{=\left(  ab+a+b\right)  _{i,j}=0\\\text{(since }ab+a+b=0\text{)}%
}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  ab\right)  _{i,j}=\sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}\text{ (by the definition of the product of two matrices),}\\
\text{so that }\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}=\left(  ab\right)
_{i,j}%
\end{array}
\right) \\
&  =0.
\end{align*}
Thus, $ab^{\prime}+a+b^{\prime}=0$. Since $A=\operatorname*{id}+a$, we have
$A\left(  \operatorname*{id}+b^{\prime}\right)  =\left(  \operatorname*{id}%
+a\right)  \left(  \operatorname*{id}+b^{\prime}\right)  =\operatorname*{id}%
+\underbrace{ab^{\prime}+a+b^{\prime}}_{=0}=\operatorname*{id}$.

We thus have shown that $A\left(  \operatorname*{id}+b^{\prime}\right)
=\operatorname*{id}$.

Now, it is easy to see that the products $B\left(  A\left(  \operatorname*{id}%
+b^{\prime}\right)  \right)  $ and $\left(  BA\right)  \left(
\operatorname*{id}+b^{\prime}\right)  $ are well-defined and satisfy
associativity, i. e., we have $B\left(  A\left(  \operatorname*{id}+b^{\prime
}\right)  \right)  =\left(  BA\right)  \left(  \operatorname*{id}+b^{\prime
}\right)  $. Now,%
\[
B=B\cdot\underbrace{\operatorname*{id}}_{=A\left(  \operatorname*{id}%
+b^{\prime}\right)  }=B\left(  A\left(  \operatorname*{id}+b^{\prime}\right)
\right)  =\underbrace{\left(  BA\right)  }_{=\operatorname*{id}}\left(
\operatorname*{id}+b^{\prime}\right)  =\operatorname*{id}+b^{\prime}%
\in\operatorname*{M}\left(  \infty\right)  .
\]
Since $B$ is the inverse of $A$, this yields that the inverse of $A$ lies in
$\operatorname*{M}\left(  \infty\right)  $. This proves Proposition
\ref{prop.Minf.monoid} \textbf{(d)}.

\textbf{(e)} Follows from \textbf{(c)} and \textbf{(d)}.

The proof of Proposition \ref{prop.Minf.monoid} is complete.

We now construct a group action of $\operatorname*{GL}\left(  \infty\right)  $
on $\mathcal{F}^{\left(  m\right)  }$ that is related to the Lie algebra
action $\rho$ of $\mathfrak{gl}_{\infty}$ on $\mathcal{F}^{\left(  m\right)
}$ in the same way as the action of a Lie group on a representation is usually
related to its ``derivative'' action of the corresponding Lie algebra:

\begin{definition}
\label{def.GLinf.act}Let $m\in\mathbb{Z}$. We define an action $\varrho
:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ of the monoid $\operatorname*{M}%
\left(  \infty\right)  $ on the vector space $\mathcal{F}^{\left(  m\right)
}=\wedge^{\dfrac{\infty}{2},m}V$ as follows: For every $A\in\operatorname*{M}%
\left(  \infty\right)  $ and every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we set%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge
Av_{i_{2}}\wedge....
\]
(This is then extended to the whole $\mathcal{F}^{\left(  m\right)  }$ by
linearity.) It is very easy to see that this is well-defined (because
$Av_{k}=v_{k}$ for all sufficiently small $k$) and indeed gives a monoid action.

The restriction $\varrho\mid_{\operatorname*{GL}\left(  \infty\right)
}:\operatorname*{GL}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}^{\left(  m\right)  }\right)  $ to $\operatorname*{GL}%
\left(  \infty\right)  $ is thus a group action of $\operatorname*{GL}\left(
\infty\right)  $ on $\mathcal{F}^{\left(  m\right)  }$.

Since we have defined an action of $\operatorname*{M}\left(  \infty\right)  $
on $\mathcal{F}^{\left(  m\right)  }$ for every $m\in\mathbb{Z}$, we thus
obtain an action of $\operatorname*{M}\left(  \infty\right)  $ on
$\mathcal{F}=\bigoplus\limits_{m\in\mathbb{Z}}\mathcal{F}^{\left(  m\right)
}$ (namely, the direct sum of the previous actions). This latter action will
also be denoted by $\varrho$.
\end{definition}

Note that the letter $\varrho$ is a capital rho, as opposed to $\rho$ which is
the lowercase rho.

When $A$ is a matrix in $\operatorname*{M}\left(  \infty\right)  $, the
endomorphism $\varrho\left(  A\right)  $ of $\mathcal{F}^{\left(  m\right)  }$
can be seen as an infinite analogue of the endomorphisms $\wedge^{\ell}A$ of
$\wedge^{\ell}V$ defined for all $\ell\in\mathbb{N}$.

We are next going to give an explicit formula for the action of $\varrho
\left(  A\right)  $ on $\mathcal{F}^{\left(  m\right)  }$ in terms of
(infinite) minors of $A$. The formula will be an infinite analogue of the
following well-known formula:

\begin{proposition}
\label{prop.GLinf.det.fin}Let $P$ be a finite-dimensional $\mathbb{C}$-vector
space with basis $\left(  e_{1},e_{2},...,e_{n}\right)  $, and let $Q$ be a
finite-dimensional $\mathbb{C}$-vector space with basis $\left(  f_{1}%
,f_{2},...,f_{m}\right)  $. Let $\ell\in\mathbb{N}$.

Let $f:P\rightarrow Q$ be a linear map, and let $A$ be the $m\times n$-matrix
which represents this map $f$ with respect to the bases $\left(  e_{1}%
,e_{2},...,e_{n}\right)  $ and $\left(  f_{1},f_{2},...,f_{m}\right)  $ of $P$
and $Q$.

Let $i_{1}$, $i_{2}$, $...$, $i_{\ell}$ be integers such that $1\leq
i_{1}<i_{2}<...<i_{\ell}\leq n$. For any $\ell$ integers $j_{1}$, $j_{2}$,
$...$, $j_{\ell}$ satisfying $1\leq j_{1}<j_{2}<...<j_{\ell}\leq m$, let
$A_{j_{1},j_{2},...,j_{\ell}}^{i_{1},i_{2},...,i_{\ell}}$ denote the matrix
which is obtained from $A$ by removing all columns except for the $i_{1}$-th,
the $i_{2}$-th, $...$, the $i_{\ell}$-th ones and removing all rows except for
the $j_{1}$-th, the $j_{2}$-th, $...$, the $j_{\ell}$-th ones. Then,%
\[
\left(  \wedge^{\ell}f\right)  \left(  e_{i_{1}}\wedge e_{i_{2}}%
\wedge...\wedge e_{i_{\ell}}\right)  =\sum\limits_{\substack{j_{1}\text{,
}j_{2}\text{, }...\text{, }j_{\ell}\text{ are }\ell\text{ integers;}\\1\leq
j_{1}<j_{2}<...<j_{\ell}\leq m}}\det\left(  A_{j_{1},j_{2},...,j_{\ell}%
}^{i_{1},i_{2},...,i_{\ell}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{\ell}}.
\]

\end{proposition}

Note that Proposition \ref{prop.GLinf.det.fin} is the main link between
exterior powers and minors of matrices. It is commonly used both to prove
results involving exterior powers and to give slick proofs of identities
involving minors.

In order to obtain an infinite analogue of this result, we need to first
define determinants of infinite matrices. This cannot be done for arbitrary
infinite matrices, but there exist classes of infinite matrices for which a
notion of determinant can be made sense of. Let us define it for so-called
``upper almost-unitriangular'' matrices:

\begin{definition}
\label{def.infdet}\textbf{(a)} In the following, when $S$ and $T$ are two sets
of integers (not necessarily finite), an $S\times T$\textit{-matrix} will mean
a matrix whose rows are indexed by the elements of $S$ and whose columns are
indexed by the elements of $T$. (Hence, the elements of $\mathfrak{gl}%
_{\infty}$, as well as those of $\overline{\mathfrak{a}_{\infty}}$ and those
of $\operatorname*{M}\left(  \infty\right)  $, are $\mathbb{Z}\times
\mathbb{Z}$-matrices.)

\textbf{(b)} If $S$ is a set of integer, then an $S\times S$-matrix $B$ over
$\mathbb{C}$ is said to be\textit{ upper unitriangular} if it satisfies the
following two assertions:

-- All entries on the main diagonal of $B$ are $=1$.

-- All entries of $B$ below the main diagonal are $=0$.

\textbf{(c)} An $\mathbb{N}\times\mathbb{N}$-matrix $B$ over $\mathbb{C}$ is
said to be\textit{ upper almost-unitriangular} if it satisfies the following
two assertions:

-- All but finitely many of the entries on the main diagonal of $B$ are $=1$.

-- All but finitely many of the entries of $B$ below the main diagonal are
$=0$.

\textbf{(d)} Let $B$ be an upper almost-unitriangular $\mathbb{N}%
\times\mathbb{N}$-matrix over $\mathbb{C}$. Then, we can write the matrix $B$
in the form $\left(
\begin{array}
[c]{cc}%
C & D\\
0 & E
\end{array}
\right)  $ for some $n\in\mathbb{N}$, some $\left\{  0,1,...,n-1\right\}
\times\left\{  0,1,...,n-1\right\}  $-matrix $C$, some $\left\{
0,1,...,n-1\right\}  \times\left\{  n,n+1,n+2,...\right\}  $-matrix $D$, and
some upper unitriangular $\left\{  n,n+1,n+2,...\right\}  \times\left\{
n,n+1,n+2,...\right\}  $-matrix $E$. The matrix $C$ in such a representation
of $B$ will be called a \textit{faithful block-triangular truncation} of $B$.

\textbf{(e)} Let $B$ be an upper almost-unitriangular $\mathbb{N}%
\times\mathbb{N}$-matrix over $\mathbb{C}$. We define the \textit{determinant}
$\det B$ of the matrix $B$ to be $\det C$, where $C$ is a faithful
block-triangular truncation of $B$. This is well-defined, because a faithful
block-triangular truncation of $B$ exists and because the determinant $\det C$
does not depend on the choice of the faithful block-triangular truncation $C$.
(The latter assertion follows from the fact that $\det\left(
\begin{array}
[c]{cc}%
F & G\\
0 & H
\end{array}
\right)  =\det F$ for any $n\in\mathbb{N}$, any $k\in\mathbb{N}$, any $n\times
n$-matrix $F$, any $n\times k$-matrix $G$, and any upper unitriangular
$k\times k$-matrix $H$.)
\end{definition}

Now, the following fact (an analogue of Proposition \ref{prop.GLinf.det.fin})
gives an explicit formula for the action of $\varrho\left(  A\right)  $:

\begin{remark}
\label{rmk.GLinf.det}Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an
$m$-degression. Let $A\in\operatorname*{M}\left(  \infty\right)  $. For any
$m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, let $A_{j_{0}%
,j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$ denote the $\mathbb{N}%
\times\mathbb{N}$-matrix defined by%
\[
\left(  \left(  \text{the }\left(  u,v\right)  \text{-th entry of }%
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  =\left(  \text{the
}\left(  j_{u},i_{v}\right)  \text{-th entry of }A\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  u,v\right)  \in\mathbb{N}%
^{2}\right)  .
\]
(In other words, let $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$
denote the matrix which is obtained from $A$ by removing all columns except
for the $i_{0}$-th, the $i_{1}$-th, the $i_{2}$-th, etc. ones and removing all
rows except for the $j_{0}$-th, the $j_{1}$-th, the $j_{2}$-th, etc. ones, and
then inverting the order of the rows, and inverting the order of the columns.)
Then, for any $m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, the
matrix $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$ is upper
almost-unitriangular (in fact, one can easily check that more is true: all but
finitely many entries of $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$
are equal to the corresponding entries of the identity $\mathbb{N}%
\times\mathbb{N}$ matrix), and thus the determinant $\det\left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  $ makes sense
(according to Definition \ref{def.infdet} \textbf{(e)}). We have%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is an }m\text{-degression}}\det\left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge....
\]

\end{remark}

The analogy between Remark \ref{rmk.GLinf.det} and Proposition
\ref{prop.GLinf.det.fin} is slightly obscured by technicalities (such as the
fact that Remark \ref{rmk.GLinf.det} only concerns itself with certain
endomorphisms of $V$ and not with homomorphisms between different vector
spaces, and the fact that the $m$-degressions in Remark \ref{rmk.GLinf.det}
are decreasing, while the $\ell$-tuples $\left(  i_{1},i_{2},...,i_{\ell
}\right)  $ and $\left(  j_{1},j_{2},...,j_{\ell}\right)  $ in Proposition
\ref{prop.GLinf.det.fin} are increasing). Still, it should be rather evident
why Remark \ref{rmk.GLinf.det} is (informally speaking) a consequence of ``the
$\ell=\infty$ case'' of Proposition \ref{prop.GLinf.det.fin}.

\subsubsection{\label{subsubsect.Uinf}Semiinfinite vectors and actions of
\texorpdfstring{$\mathfrak{u}_{\infty}$}{u-infinity} and
\texorpdfstring{$\operatorname*{U}\left(  \infty\right)  $}{U-infinity} on
\texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$}{the semi-infinite wedge
space}}

The actions of $\mathfrak{gl}_{\infty}$, $\mathfrak{a}_{\infty}$,
$\operatorname*{M}\left(  \infty\right)  $ and $\operatorname*{GL}\left(
\infty\right)  $ on $\wedge^{\dfrac{\infty}{2},m}V$ have many good properties,
but for what we want to do with them, they are in some sense ``too small''
(even $\mathfrak{a}_{\infty}$). Of course, we cannot let the space of
\textbf{all} infinite matrices act on $\wedge^{\dfrac{\infty}{2},m}V$ (this
space is not even a Lie algebra), but it turns out that we can get away with
restricting ourselves to strictly upper-triangular infinite matrices. First,
let us define a kind of completion of $V$:

\begin{definition}
\label{def.uinf.Vhat}\textbf{(a)} A family $\left(  x_{i}\right)
_{i\in\mathbb{Z}}$ of elements of some additive group indexed by integers is
said to be \textit{semiinfinite} if every sufficiently high $i\in\mathbb{Z}$
satisfies $x_{i}=0$.

\textbf{(b)} Let $\widehat{V}$ be the vector subspace $\left\{  v\in
\mathbb{C}^{\mathbb{Z}}\text{\ }\mid\ v\text{ is semiinfinite}\right\}  $ of
$\mathbb{C}^{\mathbb{Z}}$. Let $\mathfrak{u}_{\infty}$ denote the Lie algebra
of all \textbf{strictly} upper-triangular infinite matrices (with rows and
columns indexed by integers). It is easy to see that the Lie algebra
$\mathfrak{u}_{\infty}$ acts on the vector space $\widehat{V}$ in the obvious
way: namely, for any $a\in\mathfrak{u}_{\infty}$ and $v\in\widehat{V}$, we let
$a\rightharpoonup v$ be the product of the matrix $a$ with the column vector
$v$. Here, every element $\left(  x_{i}\right)  _{i\in\mathbb{Z}}$ of
$\widehat{V}$ is identified with the column vector $\left(
\begin{array}
[c]{c}%
...\\
x_{-2}\\
x_{-1}\\
x_{0}\\
x_{1}\\
x_{2}\\
...
\end{array}
\right)  $.

The vector space $V$ defined in Definition \ref{def.glinf.V} clearly is a
subspace of $\widehat{V}$. Restricting the $\mathfrak{u}_{\infty}$-action on
$\widehat{V}$ to an $\left(  \mathfrak{u}_{\infty}\cap\mathfrak{gl}_{\infty
}\right)  $-action on $V$ yields the same $\left(  \mathfrak{u}_{\infty}%
\cap\mathfrak{gl}_{\infty}\right)  $-module as restricting the $\mathfrak{gl}%
_{\infty}$-action on $V$ to an $\left(  \mathfrak{u}_{\infty}\cap
\mathfrak{gl}_{\infty}\right)  $-action on $V$.
\end{definition}

We thus have obtained an $\mathfrak{u}_{\infty}$-module $\widehat{V}$, which
is a kind of completion of $V$. One could now hope that this allows us to
construct an $\mathfrak{u}_{\infty}$-module structure on some kind of
completion of $\wedge^{\dfrac{\infty}{2},m}V$. A quick observation shows that
this works better than one would expect, because we don't have to take any
completion of $\wedge^{\dfrac{\infty}{2},m}V$ (although we can if we want to).
We can make $\wedge^{\dfrac{\infty}{2},m}V$ itself an $\mathfrak{u}_{\infty}$-module:

\begin{definition}
\label{def.uinf.Vhatproj}Let $\ell\in\mathbb{Z}$. Let $\pi_{\ell}%
:\widehat{V}\rightarrow V$ be the linear map which sends every $\left(
x_{i}\right)  _{i\in\mathbb{Z}}\in\widehat{V}$ to $\left(  \left\{
\begin{array}
[c]{c}%
x_{i}\text{, if }i\geq\ell;\\
0\text{, if }i<\ell
\end{array}
\right.  \right)  _{i\in\mathbb{Z}}\in V$. (It is very easy to see that this
map $\pi_{\ell}$ is well-defined.)
\end{definition}

\begin{definition}
\label{def.uinf.Vhatwedge}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $\widehat{V}$ which satisfy%
\[
\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all
sufficiently large }i.
\]
Define an element $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ of $\wedge
^{\dfrac{\infty}{2},m}V$ as follows: Pick some $N\in\mathbb{N}$ such that
every $i>N$ satisfies $\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}$. (Such an $N$
exists, since we know that $\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}$ for all
sufficiently large $i$.) Then, we define $b_{0}\wedge b_{1}\wedge b_{2}%
\wedge...$ to be the element
\[
\pi_{m-N}\left(  b_{0}\right)  \wedge\pi_{m-N}\left(  b_{1}\right)
\wedge...\wedge\pi_{m-N}\left(  b_{N}\right)  \wedge v_{m-N-1}\wedge
v_{m-N-2}\wedge v_{m-N-3}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V.
\]
This element does not depend on the choice of $N$ (according to Proposition
\ref{prop.uinf.Vhatwedge.welldef} below). Hence, $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ is well-defined.
\end{definition}

The next few propositions state some properties of wedge products of elements
of $\widehat{V}$ similar to some properties of wedge products of elements of
$V$ stated above. We will not prove them; neither of them is actually
difficult to verify.

\begin{proposition}
\label{prop.uinf.Vhatwedge.welldef}Let $m\in\mathbb{Z}$. Let $b_{0}%
,b_{1},b_{2},...$ be vectors in $\widehat{V}$ which satisfy%
\[
\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all
sufficiently large }i.
\]


If we pick some $N\in\mathbb{N}$ such that every $i>N$ satisfies $\pi
_{m-i}\left(  b_{i}\right)  =v_{m-i}$, then the element%
\[
\pi_{m-N}\left(  b_{0}\right)  \wedge\pi_{m-N}\left(  b_{1}\right)
\wedge...\wedge\pi_{m-N}\left(  b_{N}\right)  \wedge v_{m-N-1}\wedge
v_{m-N-2}\wedge v_{m-N-3}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V
\]
does not depend on the choice of $N$.
\end{proposition}

\begin{proposition}
The wedge product defined in Definition \ref{def.uinf.Vhatwedge} is
antisymmetric and multilinear (in the appropriate sense).
\end{proposition}

\begin{definition}
\label{def.uinf.semiinfwedge}Let $m\in\mathbb{Z}$. Define an action of the Lie
algebra $\mathfrak{u}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}%
{2},m}V$ by the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{u}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ (and by linear extension).
\end{definition}

\begin{proposition}
Let $m\in\mathbb{Z}$. Then, Definition \ref{def.uinf.semiinfwedge} really
defines a representation of the Lie algebra $\mathfrak{u}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$.
\end{proposition}

\begin{proposition}
Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be vectors in $\widehat{V}$
which satisfy%
\[
\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all
sufficiently large }i.
\]
Let $a\in\mathfrak{u}_{\infty}$. Then,%
\[
a\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)
=\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]

\end{proposition}

\begin{definition}
\label{def.Uinf.rho}Let $m\in\mathbb{Z}$. Let $\rho:\mathfrak{u}_{\infty
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
be the representation of $\mathfrak{u}_{\infty}$ on $\wedge^{\dfrac{\infty}%
{2},m}V$ defined in Definition \ref{def.uinf.semiinfwedge}. (We denote this
representation by the same letter $\rho$ as the representation $\mathfrak{gl}%
_{\infty}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2}%
,m}V\right)  $ from Definition \ref{def.glinf.rho}. This is intentional and
unproblematic, because both of these representations have the same restriction
onto $\mathfrak{u}_{\infty}\cap\mathfrak{gl}_{\infty}$.)
\end{definition}

\begin{remark}
\label{rmk.Uinf.rhorhohat}Let $m\in\mathbb{Z}$. Let $a\in\mathfrak{u}_{\infty
}\cap\overline{\mathfrak{a}_{\infty}}$. Then, $\rho\left(  a\right)
=\widehat{\rho}\left(  a\right)  $ (where $\rho\left(  a\right)  $ is defined
according to Definition \ref{def.Uinf.rho}, and $\widehat{\rho}\left(
a\right)  $ is defined according to Definition \ref{def.glinf.rhohat.abar}).
\end{remark}

\begin{definition}
\label{def.Uinf}We let $\operatorname*{U}\left(  \infty\right)  $ denote the
set $\operatorname*{id}+\mathfrak{u}_{\infty}$. In other words,
$\operatorname*{U}\left(  \infty\right)  $ is the set of all upper-triangular
infinite matrices (with rows and columns indexed by integers) whose all
diagonal entries are $=1$. This set $\operatorname*{U}\left(  \infty\right)  $
is easily seen to be a group (with respect to matrix multiplication). Inverses
in this group can be computed by means of the formula $\left(  I_{\infty
}+a\right)  ^{-1}=\sum\limits_{k=0}^{\infty}a^{k}$ for all $a\in
\mathfrak{u}_{\infty}$\ \ \ \ \footnotemark
\end{definition}

\footnotetext{Here, we are using the fact that, for every $a\in\mathfrak{u}%
_{\infty}$, the sum $\sum\limits_{k=0}^{\infty}a^{k}$ converges entrywise (i.
e., for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, the sum $\sum
\limits_{k=0}^{\infty}\left(  \text{the }\left(  i,j\right)  \text{-th entry
of }a^{k}\right)  $ converges in the discrete topology). Here is why this
holds:
\par
Since $a\in\mathfrak{u}_{\infty}$, we know that the $\left(  i,j\right)  $-th
entry of $a$ is $0$ for all $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfying
$i>j-1$. From this, it is easy to conclude (by induction over $k$) that for
every $k\in\mathbb{N}$, the $\left(  i,j\right)  $-th entry of $a^{k}$ is $0$
for all $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfying $i>j-k$. Hence, for
every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, the $\left(  i,j\right)  $-th
entry of $a^{k}$ is $0$ for all nonnegative integers $k$ satisfying $k>j-i$.
As a consequence, for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, all but
finitely many addends of the sum%
\[
\sum\limits_{k=0}^{\infty}\left(  \text{the }\left(  i,j\right)  \text{-th
entry of }a^{k}\right)
\]
are $0$. In other words, for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$,
the sum $\sum\limits_{k=0}^{\infty}\left(  \text{the }\left(  i,j\right)
\text{-th entry of }a^{k}\right)  $ converges in the discrete topology. Hence,
the sum $\sum\limits_{k=0}^{\infty}a^{k}$ converges entrywise, qed.}

\begin{definition}
\label{def.Uinf.act}Let $m\in\mathbb{Z}$. We define an action $\varrho
:\operatorname*{U}\left(  \infty\right)  \rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ of the group $\operatorname*{U}%
\left(  \infty\right)  $ on the vector space $\mathcal{F}^{\left(  m\right)
}=\wedge^{\dfrac{\infty}{2},m}V$ as follows: For every $A\in\operatorname*{U}%
\left(  \infty\right)  $ and every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we set%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge
Av_{i_{2}}\wedge....
\]
(This is then extended to the whole $\mathcal{F}^{\left(  m\right)  }$ by
linearity.) It is very easy to see that this is well-defined (because
$\pi_{v_{k}}\left(  Av_{k}\right)  =v_{k}$ for all sufficiently small $k$) and
indeed gives a group action. (We denote this action by the same letter
$\varrho$ as the action $\operatorname*{M}\left(  \infty\right)
\rightarrow\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)
$ from Definition \ref{def.GLinf.act}. This is intentional and unproblematic,
because both of these actions have the same restriction onto
$\operatorname*{U}\left(  \infty\right)  \cap\operatorname*{M}\left(
\infty\right)  $.)
\end{definition}

In analogy to Remark \ref{rmk.GLinf.det}, we have:

\begin{remark}
\label{rmk.Uinf.det}Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an
$m$-degression. Let $A\in\operatorname*{U}\left(  \infty\right)  $. For any
$m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, let $A_{j_{0}%
,j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$ denote the $\mathbb{N}%
\times\mathbb{N}$-matrix defined by%
\[
\left(  \left(  \text{the }\left(  u,v\right)  \text{-th entry of }%
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  =\left(  \text{the
}\left(  j_{u},i_{v}\right)  \text{-th entry of }A\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  u,v\right)  \in\mathbb{N}%
^{2}\right)  .
\]
(In other words, let $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$
denote the matrix which is obtained from $A$ by removing all columns except
for the $i_{0}$-th, the $i_{1}$-th, the $i_{2}$-th, etc. ones and removing all
rows except for the $j_{0}$-th, the $j_{1}$-th, the $j_{2}$-th, etc. ones, and
then inverting the order of the rows, and inverting the order of the columns.)
Then, for any $m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, the
matrix $\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)
^{T}$ is upper almost-unitriangular, and thus the determinant $\det\left(
\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)
$ makes sense (according to Definition \ref{def.infdet} \textbf{(e)}). We have%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is an }m\text{-degression}}\det\left(  \left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\]

\end{remark}

The analogy between Remark \ref{rmk.GLinf.det} and Remark \ref{rmk.Uinf.det}
is somewhat marred by the fact that the transposed matrix $\left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}$ is used in
Remark \ref{rmk.Uinf.det} instead of the matrix $A_{j_{0},j_{1},j_{2}%
,...}^{i_{0},i_{1},i_{2},...}$. This is merely a technical difference, and if
we would have defined the determinant of a \textbf{lower} almost-unitriangular
matrix, we could have avoided using the transpose in Remark \ref{rmk.Uinf.det}.

\begin{remark}
There is a way to ``merge'' $\operatorname*{GL}\left(  \infty\right)  $ and
$\operatorname*{U}\left(  \infty\right)  $ into a bigger group of infinite
matrices. Indeed, let $\operatorname*{M}\nolimits^{\operatorname*{U}}\left(
\infty\right)  $ the set of all matrices $A\in\operatorname*{U}\left(
\infty\right)  $ such that all but finitely many among the $\left(
i,j\right)  \in\mathbb{Z}^{2}$ satisfying $i\geq j$ satisfy $\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }A\right)  =\delta_{i,j}$. (Note that
this condition does not restrict the $\left(  i,j\right)  $-th entry of $A$
for any $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfying $i<j$. That is, the
entries of $A$ above the main diagonal can be arbitrary, but the entries of
$A$ below and on the main diagonal have to coincide with the respective
entries of the identity matrix save for finitely many exceptions, if $A$ is to
lie in $\operatorname*{M}\nolimits^{\operatorname*{U}}\left(  \infty\right)
$.) Then, it is easy to see that $\operatorname*{M}%
\nolimits^{\operatorname*{U}}\left(  \infty\right)  $ is a monoid. The group
of all invertible elements of this monoid (where ``invertible'' means ``having
an inverse in the monoid $\operatorname*{M}\nolimits^{\operatorname*{U}%
}\left(  \infty\right)  $'') is a group which has both $\operatorname*{GL}%
\left(  \infty\right)  $ and $\operatorname*{U}\left(  \infty\right)  $ as
subgroups. Actually, this group is $\operatorname*{GL}\left(  \infty\right)
\cdot\operatorname*{U}\left(  \infty\right)  $, as the reader can easily check.

We will need neither the monoid $\operatorname*{M}\nolimits^{\operatorname*{U}%
}\left(  \infty\right)  $ nor this group in the following.
\end{remark}

\subsubsection{The exponential relation between \texorpdfstring{$\rho$}{rho}
and \texorpdfstring{$\varrho$}{Rho}}

We now come to a relation which connects the actions $\rho$ and $\varrho$. It
comes in a $\operatorname*{GL}\left(  \infty\right)  $ version, a
$\operatorname*{U}\left(  \infty\right)  $ version, and a finitary version; we
will formulate all three, but only prove the latter. First, the
$\operatorname*{GL}\left(  \infty\right)  $ version:

\begin{theorem}
\label{thm.GLinf.rhoRho}Let $a\in\mathfrak{gl}_{\infty}$. Let $m\in\mathbb{Z}%
$. Then, the exponential $\exp a$ is a well-defined element of
$\operatorname*{GL}\left(  \infty\right)  $ and satisfies $\varrho\left(  \exp
a\right)  =\exp\left(  \rho\left(  a\right)  \right)  $ in
$\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $.
\end{theorem}

It should be noticed that Theorem \ref{thm.GLinf.rhoRho}, unlike most of the
other results we have been stating, does rely on the ground field being
$\mathbb{C}$; otherwise, there would be no guarantee that $\exp a$ is
well-defined. However, if we assume, for example, that $a$ is strictly
upper-triangular, or that the entries of $a$ belong to some ideal $I$ of the
ground ring such that the ground ring is complete and Hausdorff in the
$I$-adic topology, then the statement of Theorem \ref{thm.GLinf.rhoRho} would
be guaranteed over any ground ring which is a commutative $\mathbb{Q}$-algebra.

The $\operatorname*{U}\left(  \infty\right)  $ version does not depend on the
ground ring at all (as long as the ground ring is a $\mathbb{Q}$-algebra):

\begin{theorem}
\label{thm.Uinf.rhoRho}Let $a\in\mathfrak{u}_{\infty}$. Let $m\in\mathbb{Z}$.
Then, the exponential $\exp a$ is a well-defined element of $\operatorname*{U}%
\left(  \infty\right)  $ and satisfies $\varrho\left(  \exp a\right)
=\exp\left(  \rho\left(  a\right)  \right)  $ in $\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $.
\end{theorem}

We have now stated the $\operatorname*{GL}\left(  \infty\right)  $ and the
$\operatorname*{U}\left(  \infty\right)  $ versions of the relation between
$\rho$ and $\varrho$. Before we state the finitary version, we define a finite
analogue of the map $\rho$:

\begin{definition}
Let $P$ be a vector space, and let $\ell\in\mathbb{N}$. Let $\rho_{P,\ell
}:\mathfrak{gl}\left(  P\right)  \rightarrow\operatorname*{End}\left(
\wedge^{\ell}P\right)  $ denote the representation of the Lie algebra
$\mathfrak{gl}\left(  P\right)  $ on the $\ell$-th exterior power of the
defining representation $P$ of $\mathfrak{gl}\left(  P\right)  $. By the
definition of the $\ell$-th exterior power of a representation of a Lie
algebra, this representation $\rho_{P,\ell}$ satisfies%
\begin{equation}
\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  =\sum\limits_{k=1}^{\ell}p_{1}\wedge
p_{2}\wedge...\wedge p_{k-1}\wedge\left(  a\rightharpoonup p_{k}\right)
\wedge p_{k+1}\wedge p_{k+2}\wedge...\wedge p_{\ell} \label{def.finitary.rho}%
\end{equation}
for every $a\in\mathfrak{gl}\left(  P\right)  $ and any $p_{1},p_{2}%
,...,p_{\ell}\in P$. (Recall that $a\rightharpoonup p=ap$ for every
$a\in\mathfrak{gl}\left(  P\right)  $ and $p\in P$.)
\end{definition}

Finally, let us state the finitary version of Theorem \ref{thm.GLinf.rhoRho}
and Theorem \ref{thm.Uinf.rhoRho}. To see why it is analogous to the two
aforementioned theorems, one should keep in mind that $\rho_{P,\ell}$ is an
analogue of $\rho$ in the finite case, while $\wedge^{\ell}A$ is an analogue
of $\varrho\left(  A\right)  $.

\begin{theorem}
\label{thm.finitary.rhoRho}Let $P$ be a vector space. Let $a\in\mathfrak{gl}%
\left(  P\right)  $ be a nilpotent linear map. Then, the exponential $\exp a$
is a well-defined element of $\operatorname*{GL}\left(  P\right)  $ and
satisfies $\wedge^{\ell}\left(  \exp a\right)  =\exp\left(  \rho_{P,\ell
}\left(  a\right)  \right)  $ in $\operatorname*{End}\left(  \wedge^{\ell
}P\right)  $ for every $\ell\in\mathbb{N}$.
\end{theorem}

Note that we have formulated Theorem \ref{thm.finitary.rhoRho} only for
nilpotent $a\in\mathfrak{gl}\left(  P\right)  $. We could have also formulated
it for arbitrary $a\in\mathfrak{gl}\left(  P\right)  $ under some mild
conditions on $P$ (such as $P$ being finite-dimensional), but then it would
depend on the ground field being $\mathbb{C}$, which is something we would
like to avoid (as we are going to apply this theorem to a different ground field).

\begin{vershort}
\textit{First proof of Theorem \ref{thm.finitary.rhoRho} (sketched).} Since
$a$ is nilpotent, it is known that the exponential $\exp a$ is a well-defined
element of $\operatorname*{GL}\left(  P\right)  $.
\end{vershort}

\begin{verlong}
\textit{First proof of Theorem \ref{thm.finitary.rhoRho}.} Since $a$ is
nilpotent, it is known that the exponential $\exp a$ is a well-defined element
of $\operatorname*{GL}\left(  P\right)  $.
\end{verlong}

Let $\ell\in\mathbb{N}$. Now define an endomorphism $\rho_{P,\ell}^{\prime
}\left(  a\right)  :P^{\otimes\ell}\rightarrow P^{\otimes\ell}$ by%
\[
\rho_{P,\ell}^{\prime}\left(  a\right)  =\sum\limits_{k=1}^{\ell
}\operatorname*{id}\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes
a\otimes\operatorname*{id}\nolimits_{P}^{\otimes\left(  \ell-k\right)  }.
\]


Let also $\pi:P^{\otimes\ell}\rightarrow\wedge^{\ell}P$ be the canonical
projection (since $\wedge^{\ell}P$ is defined as a quotient vector space of
$P^{\otimes\ell}$). Clearly, $\pi$ is surjective.

\begin{vershort}
It is easy to see that $\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  =\left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi$.
From this, one can conclude that%
\begin{equation}
\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{m}\circ\pi
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}.
\label{pf.finitary.rhoRho.projm.short}%
\end{equation}

\end{vershort}

\begin{verlong}
It is easy to see that%
\begin{equation}
\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  =\left(
\rho_{P,\ell}\left(  a\right)  \right)  \circ\pi.
\label{pf.finitary.rhoRho.proj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.rhoRho.proj}):} Let $p_{1}%
,p_{2},...,p_{\ell}\in P$ be arbitrary. Then, $p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}=\pi\left(  p_{1}\otimes p_{2}\otimes...\otimes
p_{\ell}\right)  $ (by the definition of $p_{1}\wedge p_{2}\wedge...\wedge
p_{\ell}$), and%
\begin{align*}
&  \left(  \left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi\right)
\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right) \\
&  =\left(  \rho_{P,\ell}\left(  a\right)  \right)  \underbrace{\left(
\pi\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  \right)
}_{=p_{1}\wedge p_{2}\wedge...\wedge p_{\ell}}=\left(  \rho_{P,\ell}\left(
a\right)  \right)  \left(  p_{1}\wedge p_{2}\wedge...\wedge p_{\ell}\right) \\
&  =\sum\limits_{k=1}^{\ell}\underbrace{p_{1}\wedge p_{2}\wedge...\wedge
p_{k-1}\wedge\left(  a\rightharpoonup p_{k}\right)  \wedge p_{k+1}\wedge
p_{k+2}\wedge...\wedge p_{\ell}}_{\substack{=\pi\left(  p_{1}\otimes
p_{2}\otimes...\otimes p_{k-1}\otimes\left(  a\rightharpoonup p_{k}\right)
\otimes p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)  \\\text{(by
the definition of }p_{1}\wedge p_{2}\wedge...\wedge p_{k-1}\wedge\left(
a\rightharpoonup p_{k}\right)  \wedge p_{k+1}\wedge p_{k+2}\wedge...\wedge
p_{\ell\ }\text{)}}}\\
&  =\sum\limits_{k=1}^{\ell}\pi\left(  p_{1}\otimes p_{2}\otimes...\otimes
p_{k-1}\otimes\left(  a\rightharpoonup p_{k}\right)  \otimes p_{k+1}\otimes
p_{k+2}\otimes...\otimes p_{\ell}\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\underbrace{p_{1}\otimes p_{2}%
\otimes...\otimes p_{k-1}}_{=\operatorname*{id}\nolimits_{P}^{\otimes\left(
k-1\right)  }\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{k-1}\right)
}\otimes\underbrace{\left(  a\rightharpoonup p_{k}\right)  }%
_{\substack{=ap_{k}\\\text{(since }a\rightharpoonup p=ap\\\text{for every
}p\in P\text{)}}}\otimes\underbrace{p_{k+1}\otimes p_{k+2}\otimes...\otimes
p_{\ell}}_{=\operatorname*{id}\nolimits_{P}^{\otimes\left(  \ell-k\right)
}\left(  p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)  }\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\underbrace{\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\left(  p_{1}\otimes p_{2}%
\otimes...\otimes p_{k-1}\right)  \otimes ap_{k}\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\left(  p_{k+1}\otimes
p_{k+2}\otimes...\otimes p_{\ell}\right)  }_{=\left(  \operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \left(  \left(
p_{1}\otimes p_{2}\otimes...\otimes p_{k-1}\right)  \otimes p_{k}%
\otimes\left(  p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)
\right)  }\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\left(  \operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \underbrace{\left(
\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{k-1}\right)  \otimes
p_{k}\otimes\left(  p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)
\right)  }_{=p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}}\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\left(  \operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \left(  p_{1}\otimes
p_{2}\otimes...\otimes p_{\ell}\right)  \right)  =\left(  \pi\circ
\underbrace{\left(  \sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  }_{=\rho_{P,\ell
}^{\prime}\left(  a\right)  }\right)  \left(  p_{1}\otimes p_{2}%
\otimes...\otimes p_{\ell}\right) \\
&  =\left(  \pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)  \left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  .
\end{align*}
Now, forget that we fixed $p_{1},p_{2},...,p_{\ell}\in P$. We thus have proven
that any $p_{1},p_{2},...,p_{\ell}\in P$ satisfy%
\[
\left(  \left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi\right)
\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  =\left(  \pi
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  \right)  \left(
p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  .
\]
In other words, the two linear maps $\left(  \rho_{P,\ell}\left(  a\right)
\right)  \circ\pi$ and $\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  $ are equal to each other on every pure tensor. Since any
two linear maps from a tensor product which are equal to each other on every
pure tensor must be identical, this yields that the maps $\left(  \rho
_{P,\ell}\left(  a\right)  \right)  \circ\pi$ and $\pi\circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  $ are identical. In other
words, $\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
=\left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi$, and
(\ref{pf.finitary.rhoRho.proj}) is proven.} Using this, we obtain%
\begin{equation}
\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{m}\circ\pi
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}.
\label{pf.finitary.rhoRho.projm}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.rhoRho.projm}):} We will prove
(\ref{pf.finitary.rhoRho.projm}) by induction over $m$:
\par
\textit{Induction base:} Comparing $\pi\circ\underbrace{\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{0}}_{=\operatorname*{id}}=\pi$ with
$\underbrace{\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{0}%
}_{=\operatorname*{id}}\circ\pi=\pi$, we obtain $\pi\circ\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{0}=\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{0}\circ\pi$. In other words,
(\ref{pf.finitary.rhoRho.projm}) holds for $m=0$. This completes the induction
base.
\par
\textit{Induction step:} Let $\mu\in\mathbb{N}$. Assume that
(\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu$. We must now prove that
(\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu+1$ as well.
\par
Since (\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu$, we have $\pi
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu}=\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}\circ\pi$. Now,%
\begin{align*}
\pi\circ\underbrace{\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{\mu+1}}_{=\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu
}\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  }  &
=\underbrace{\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{\mu}}_{=\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}\circ\pi}%
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  =\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}\circ\underbrace{\pi\circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  }_{\substack{=\left(
\rho_{P,\ell}\left(  a\right)  \right)  \circ\pi\\\text{(according to
(\ref{pf.finitary.rhoRho.proj}))}}}\\
&  =\underbrace{\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}%
\circ\left(  \rho_{P,\ell}\left(  a\right)  \right)  }_{=\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{\mu+1}}\circ\pi=\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{\mu+1}\circ\pi.
\end{align*}
In other words, (\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu+1$. Thus,
the induction step is finished. The induction proof of
(\ref{pf.finitary.rhoRho.projm}) is therefore complete.}
\end{verlong}

\begin{vershort}
On the other hand, a routine induction proves that every $m\in\mathbb{N}$
satisfies%
\begin{equation}
\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}.
\label{pf.finitary.rhoRho.mpower.short}%
\end{equation}

\end{vershort}

\begin{verlong}
On the other hand, let us show that every $m\in\mathbb{N}$ satisfies%
\begin{equation}
\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}.
\label{pf.finitary.rhoRho.mpower}%
\end{equation}


\textit{Proof of (\ref{pf.finitary.rhoRho.mpower}):} We will prove
(\ref{pf.finitary.rhoRho.mpower}) by induction over $m$:

\textit{Induction base:} There exists only one $\ell$-tuple $\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}$ satisfying $i_{1}%
+i_{2}+...+i_{\ell}=0$, namely $\left(  i_{1},i_{2},...,i_{\ell}\right)
=\left(  \underbrace{0,0,...,0}_{\ell\text{ zeroes}}\right)  $. Thus,%
\begin{align*}
\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in
\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=0}}\dfrac{0!}{i_{1}%
!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}
&  =\underbrace{\dfrac{0!}{0!0!...0!}}_{=1}\underbrace{a^{0}\otimes
a^{0}\otimes...\otimes a^{0}}_{\ell\text{ tensorands}}=\underbrace{a^{0}%
\otimes a^{0}\otimes...\otimes a^{0}}_{\ell\text{ tensorands}}\\
&  =\underbrace{\operatorname*{id}\otimes\operatorname*{id}\otimes
...\otimes\operatorname*{id}}_{\ell\text{ tensorands}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }a^{0}=\operatorname*{id}\right) \\
&  =\operatorname*{id}\nolimits_{P^{\otimes\ell}}.
\end{align*}
Thus, $\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{0}%
=\operatorname*{id}\nolimits_{P^{\otimes\ell}}=\sum\limits_{\substack{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}%
+...+i_{\ell}=0}}\dfrac{0!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes
a^{i_{2}}\otimes...\otimes a^{i_{\ell}}$. Hence,
(\ref{pf.finitary.rhoRho.mpower}) holds for $m=0$. This completes the
induction base.

\textit{Induction step:} Let $\mu\in\mathbb{N}$. Assume that
(\ref{pf.finitary.rhoRho.mpower}) holds for $m=\mu$. We must prove that
(\ref{pf.finitary.rhoRho.mpower}) also holds for $m=\mu+1$.

Since (\ref{pf.finitary.rhoRho.mpower}) holds for $m=\mu$, we have%
\begin{equation}
\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu}=\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}.
\label{pf.finitary.rhoRho.mpower.pf.indhyp}%
\end{equation}


Now, for every $\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}%
$, let $C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }$ denote the endomorphism%
\[
\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\in\operatorname*{End}\left(  P^{\otimes\ell
}\right)  .
\]
Then, every $\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}$
satisfies%
\begin{align}
&  \left(  \underbrace{\operatorname*{id}\nolimits_{P}^{\otimes\left(
k-1\right)  }\otimes a\otimes\operatorname*{id}\nolimits_{P}^{\otimes\left(
\ell-k\right)  }}_{=\underbrace{\operatorname*{id}\nolimits_{P}\otimes
\operatorname*{id}\nolimits_{P}\otimes...\otimes\operatorname*{id}%
\nolimits_{P}}_{k-1\text{ tensorands}}\otimes a\otimes
\underbrace{\operatorname*{id}\nolimits_{P}\otimes\operatorname*{id}%
\nolimits_{P}\otimes...\otimes\operatorname*{id}\nolimits_{P}}_{\ell-k\text{
tensorands}}}\right)  \circ\underbrace{C_{\left(  i_{1},i_{2},...,i_{\ell
}\right)  }}_{=\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes
a^{i_{2}}\otimes...\otimes a^{i_{\ell}}}\nonumber\\
&  =\left(  \underbrace{\operatorname*{id}\nolimits_{P}\otimes
\operatorname*{id}\nolimits_{P}\otimes...\otimes\operatorname*{id}%
\nolimits_{P}}_{k-1\text{ tensorands}}\otimes a\otimes
\underbrace{\operatorname*{id}\nolimits_{P}\otimes\operatorname*{id}%
\nolimits_{P}\otimes...\otimes\operatorname*{id}\nolimits_{P}}_{\ell-k\text{
tensorands}}\right)  \circ\left(  \dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}\right) \nonumber\\
&  =\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}\underbrace{\left(
\underbrace{\operatorname*{id}\nolimits_{P}\otimes\operatorname*{id}%
\nolimits_{P}\otimes...\otimes\operatorname*{id}\nolimits_{P}}_{k-1\text{
tensorands}}\otimes a\otimes\underbrace{\operatorname*{id}\nolimits_{P}%
\otimes\operatorname*{id}\nolimits_{P}\otimes...\otimes\operatorname*{id}%
\nolimits_{P}}_{\ell-k\text{ tensorands}}\right)  \circ\left(  a^{i_{1}%
}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}\right)  }_{=\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{1}}\right)  \otimes\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{2}}\right)  \otimes...\otimes
\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k-1}}\right)
\otimes\left(  a\circ a^{i_{k}}\right)  \otimes\left(  \operatorname*{id}%
\nolimits_{P}\circ a^{i_{k+1}}\right)  \otimes\left(  \operatorname*{id}%
\nolimits_{P}\circ a^{i_{k+2}}\right)  \otimes...\otimes\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{\ell}}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right) \nonumber\\
&  =\underbrace{\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}}_{\substack{=\dfrac
{\left(  i_{k}+1\right)  !}{i_{k}!}\cdot\dfrac{\mu!}{i_{1}!i_{2}%
!...i_{k-1}!\left(  i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\\=\left(
i_{k}+1\right)  \cdot\dfrac{\mu!}{i_{1}!i_{2}!...i_{k-1}!\left(
i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\\\text{(since }\dfrac{\left(
i_{k}+1\right)  !}{i_{k}!}=i_{k}+1\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{1}}\right)  \otimes\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{2}}\right)  \otimes...\otimes
\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k-1}}\right)  }_{=a^{i_{1}%
}\otimes a^{i_{2}}\otimes...\otimes a^{i_{k-1}}}\otimes\underbrace{\left(
a\circ a^{i_{k}}\right)  }_{=a^{i_{k}+1}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \otimes
\underbrace{\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k+1}}\right)
\otimes\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k+2}}\right)
\otimes...\otimes\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{\ell}%
}\right)  }_{=a^{i_{k+1}}\otimes a^{i_{k+2}}\otimes...\otimes a^{i_{\ell}}%
}\nonumber\\
&  =\left(  i_{k}+1\right)  \cdot\underbrace{\dfrac{\mu!}{i_{1}!i_{2}%
!...i_{k-1}!\left(  i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\cdot
a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{k-1}}\otimes a^{i_{k}%
+1}\otimes a^{i_{k+1}}\otimes a^{i_{k+2}}\otimes...\otimes a^{i_{\ell}}%
}_{\substack{=C_{\left(  i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1}%
,i_{k+2},...,i_{\ell}\right)  }\\\text{(since the definition of }C_{\left(
i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  }\text{
yields}\\C_{\left(  i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2}%
,...,i_{\ell}\right)  }\\=\dfrac{\mu!}{i_{1}!i_{2}!...i_{k-1}!\left(
i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\cdot a^{i_{1}}\otimes a^{i_{2}%
}\otimes...\otimes a^{i_{k-1}}\otimes a^{i_{k}+1}\otimes a^{i_{k+1}}\otimes
a^{i_{k+2}}\otimes...\otimes a^{i_{\ell}}\text{)}}}\nonumber\\
&  =\left(  i_{k}+1\right)  \cdot C_{\left(  i_{1},i_{2},...,i_{k-1}%
,i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  }
\label{pf.finitary.rhoRho.bigcalc}%
\end{align}
and%
\begin{align}
i_{1}+i_{2}+...+i_{\ell}  &  =\sum\limits_{k\in\left\{  1,2,...,\ell\right\}
}i_{k}=\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}  ;\\i_{k}%
\geq1}}i_{k}+\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}<1}}\underbrace{i_{k}}_{\substack{=0\\\text{(since }i_{k}<1\text{ and
}i_{k}\in\mathbb{N}\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every }k\in\left\{  1,2,...,\ell
\right\}  \text{ satisfies either }i_{k}\geq1\text{ or }i_{k}<1\right)
\nonumber\\
&  =\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}  ;\\i_{k}\geq
1}}i_{k}+\underbrace{\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}<1}}0}_{=0}=\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}\geq1}}i_{k}. \label{pf.finitary.rhoRho.smallcalc}%
\end{align}


But now,%
\begin{align*}
&  \left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu+1}\\
&  =\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  \circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu}\\
&  =\left(  \sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \circ\left(
\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in
\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}\underbrace{\dfrac{\mu
!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes
a^{i_{\ell}}}_{=C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{due to }\rho_{P,\ell}^{\prime}\left(
a\right)  =\sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\text{ and due to
(\ref{pf.finitary.rhoRho.mpower.pf.indhyp})}\right) \\
&  =\left(  \sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \circ\left(
\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in
\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}C_{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  }\right) \\
&  =\underbrace{\sum\limits_{k=1}^{\ell}}_{=\sum\limits_{k\in\left\{
1,2,...,\ell\right\}  }}\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu
}}\underbrace{\left(  \operatorname*{id}\nolimits_{P}^{\otimes\left(
k-1\right)  }\otimes a\otimes\operatorname*{id}\nolimits_{P}^{\otimes\left(
\ell-k\right)  }\right)  \circ C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }%
}_{\substack{=\left(  i_{k}+1\right)  \cdot C_{\left(  i_{1},i_{2}%
,...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  }\\\text{(by
(\ref{pf.finitary.rhoRho.bigcalc}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right)
\end{align*}%
\begin{align*}
&  =\sum\limits_{k\in\left\{  1,2,...,\ell\right\}  }\underbrace{\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}\left(  i_{k}+1\right)  \cdot
C_{\left(  i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell
}\right)  }}_{\substack{=\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}%
=\mu+1;\\i_{k}\geq1}}i_{k}\cdot C_{\left(  i_{1},i_{2},...,i_{k-1}%
,i_{k},i_{k+1},i_{k+2},...,i_{\ell}\right)  }\\\text{(here we substituted
}\left(  i_{1},i_{2},...,i_{\ell}\right)  \text{ for }\left(  i_{1}%
,i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  \text{ in the
sum)}}}\\
&  =\underbrace{\sum\limits_{k=1}^{\ell}\sum\limits_{\substack{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}%
+...+i_{\ell}=\mu+1;\\i_{k}\geq1}}}_{=\sum\limits_{\substack{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}%
+...+i_{\ell}=\mu+1}}\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}\geq1}}}i_{k}\cdot\underbrace{C_{\left(  i_{1},i_{2},...,i_{k-1}%
,i_{k},i_{k+1},i_{k+2},...,i_{\ell}\right)  }}_{=C_{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  }}=\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell
}=\mu+1}}\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}\geq1}}i_{k}\cdot C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\underbrace{\left(
\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}  ;\\i_{k}\geq1}%
}i_{k}\right)  }_{\substack{=i_{1}+i_{2}+...+i_{\ell}\\\text{(by
(\ref{pf.finitary.rhoRho.smallcalc}))}}}\cdot C_{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  }\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\underbrace{\left(
i_{1}+i_{2}+...+i_{\ell}\right)  }_{=\mu+1}\cdot C_{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  }\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\left(  \mu+1\right)
\cdot\underbrace{C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }}_{=\dfrac{\mu
!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes
a^{i_{\ell}}}\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\dfrac{\left(
\mu+1\right)  \cdot\mu!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}%
}\otimes...\otimes a^{i_{\ell}}\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\dfrac{\left(
\mu+1\right)  !}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
\mu+1\right)  \cdot\mu!=\left(  \mu+1\right)  !\right)  .
\end{align*}
In other words, (\ref{pf.finitary.rhoRho.mpower}) also holds for $m=\mu+1$.
This completes the induction step. Thus, the induction proof of
(\ref{pf.finitary.rhoRho.mpower}) is complete.
\end{verlong}

\begin{vershort}
Now, $\exp a=\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}$, whence%
\begin{align*}
\left(  \exp a\right)  ^{\otimes\ell}  &  =\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  ^{\otimes\ell}=\sum\limits_{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}%
{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)
\otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the product rule}\right) \\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes
...\otimes a^{i_{\ell}}\\
&  =\sum\limits_{m\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}%
=m}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}}%
_{\substack{=\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{m}\\\text{(by (\ref{pf.finitary.rhoRho.mpower.short}))}}}=\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  ^{m}\\
&  =\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  .
\end{align*}
Note that this shows that $\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  $ is well-defined. But since $\exp\left(  \rho_{P,\ell}^{\prime
}\left(  a\right)  \right)  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}$, we have%
\begin{align*}
\pi\circ\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)   &  =\pi\circ\left(  \sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  =\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\pi\circ\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{m}}_{\substack{=\left(  \rho_{P,\ell
}\left(  a\right)  \right)  ^{m}\circ\pi\\\text{(by
(\ref{pf.finitary.rhoRho.projm.short}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right) \\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\circ\pi=\underbrace{\left(  \sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(  a\right)  \right)
^{m}\right)  }_{=\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  }\circ
\pi=\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)
\circ\pi,
\end{align*}
and this also shows that $\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)
$ is well-defined (since $\pi$ is surjective).

Since we have proven earlier that $\left(  \exp a\right)  ^{\otimes\ell}%
=\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  $, the equality
$\pi\circ\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)  =\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)
\circ\pi$ rewrites as $\pi\circ\left(  \exp a\right)  ^{\otimes\ell}=\left(
\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)  \circ\pi$.
\end{vershort}

\begin{verlong}
Now, $\exp a=\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}$, so that $\left(
\exp a\right)  ^{\otimes\ell}=\left(  \sum\limits_{i\in\mathbb{N}}\dfrac
{1}{i!}a^{i}\right)  ^{\otimes\ell}$. But by the product rule, we have%
\begin{equation}
\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  ^{\otimes\ell
}=\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}%
}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}%
!}a^{i_{2}}\right)  \otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}%
}\right)  \label{pf.finitary.rhoRho.prodrule}%
\end{equation}
(since the tensor product of linear maps is bilinear)\footnote{Here is a more
formal proof of (\ref{pf.finitary.rhoRho.prodrule}):
\par
Let $\operatorname*{Prod}$ be the canonical linear map $\left(
\operatorname*{End}P\right)  ^{\otimes\ell}\rightarrow\operatorname*{End}%
\left(  P^{\otimes\ell}\right)  $ which sends the (abstract) tensor product
$f_{1}\otimes f_{2}\otimes...\otimes f_{\ell}$ of any $\ell$ endomorphisms
$f_{1},f_{2},...,f_{\ell}\in\operatorname*{End}P$ to the endomorphism
$f_{1}\otimes f_{2}\otimes...\otimes f_{\ell}$ of $P^{\otimes\ell}$. (We need
to carefully distinguish the former $f_{1}\otimes f_{2}\otimes...\otimes
f_{\ell}$ from the latter $f_{1}\otimes f_{2}\otimes...\otimes f_{\ell}$, even
if the same notation is used for both terms.) Note that $\operatorname*{Prod}:
\left(  \operatorname*{End}P\right)  ^{\otimes\ell}\rightarrow
\operatorname*{End}\left(  P^{\otimes\ell}\right)  $ is a linear map (and even
an algebra homomorphism if both $\operatorname*{End}P$ and
$\operatorname*{End}\left(  P^{\otimes\ell}\right)  $ are regarded as algebras
with respect to composition of endomorphisms).
\par
We denote by $u^{\otimes\ell}$ the $\ell$-th power of an element $u\in
\otimes\left(  \operatorname*{End}P\right)  $ in the tensor algebra
$\otimes\left(  \operatorname*{End}P\right)  $. Since the multiplication in
the tensor algebra $\otimes\left(  \operatorname*{End}P\right)  $ is the
tensor product, we have $u^{\otimes\ell}=\underbrace{u\otimes u\otimes
...\otimes u}_{\ell\text{ times}}$.
\par
In the tensor algebra $\otimes\left(  \operatorname*{End}P\right)  $, we have%
\begin{equation}
\underbrace{\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes...\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}%
a^{i}\right)  }_{\ell\text{ times}}=\sum\limits_{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}%
}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes
...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)
\label{pf.finitary.rhoRho.prodrule.pf.1}%
\end{equation}
(by the product rule), since the multiplication in the tensor algebra
$\otimes\left(  \operatorname*{End}P\right)  $ is the tensor product. Applying
the map $\operatorname*{Prod}:\left(  \operatorname*{End}P\right)
^{\otimes\ell}\rightarrow\operatorname*{End}\left(  P^{\otimes\ell}\right)  $
to the equality (\ref{pf.finitary.rhoRho.prodrule.pf.1}), we obtain%
\begin{align*}
&  \operatorname*{Prod}\left(  \underbrace{\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes...\otimes\left(  \sum
\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  }_{\ell\text{ times}%
}\right) \\
&  =\operatorname*{Prod}\left(  \sum\limits_{\left(  i_{1},i_{2},...,i_{\ell
}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)
\otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes...\otimes\left(
\dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)  \right) \\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\underbrace{\operatorname*{Prod}\left(  \left(  \dfrac{1}{i_{1}!}a^{i_{1}%
}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes
...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)  \right)
}_{\substack{=\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)  \otimes\left(
\dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes...\otimes\left(  \dfrac{1}{i_{\ell
}!}a^{i_{\ell}}\right)  \\\text{(by the definition of }\operatorname*{Prod}%
\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Prod}\text{ is a
linear map}\right) \\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}%
!}a^{i_{2}}\right)  \otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}%
}\right)
\end{align*}
in $\operatorname*{End}\left(  P^{\otimes\ell}\right)  $. Since
\begin{align*}
&  \operatorname*{Prod}\left(  \underbrace{\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes...\otimes\left(  \sum
\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  }_{\ell\text{ times}%
}\right) \\
&  =\underbrace{\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes...\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}%
a^{i}\right)  }_{\ell\text{ times}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\operatorname*{Prod}\right) \\
&  =\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
^{\otimes\ell},
\end{align*}
this rewrites as $\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}%
a^{i}\right)  ^{\otimes\ell}=\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell
}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)
\otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes...\otimes\left(
\dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)  $ in $\operatorname*{End}\left(
P^{\otimes\ell}\right)  $. This proves (\ref{pf.finitary.rhoRho.prodrule}).}.
Now,%
\begin{align}
\left(  \exp a\right)  ^{\otimes\ell}  &  =\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  ^{\otimes\ell}=\sum\limits_{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}%
{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)
\otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.finitary.rhoRho.prodrule}%
)}\right) \nonumber\\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes
...\otimes a^{i_{\ell}}\nonumber\\
&  =\sum\limits_{m\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}%
=m}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\nonumber\\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}}%
_{\substack{=\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{m}\\\text{(by (\ref{pf.finitary.rhoRho.mpower}))}}}=\sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  ^{m}\nonumber\\
&  =\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  .
\label{pf.finitary.rhoRho.expexp}%
\end{align}


Note that this shows that $\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  $ is well-defined. Thus, for every $w\in P^{\otimes\ell}$, the term
$\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  \right)
w$ is well-defined. Hence, for every $x\in\wedge^{\ell}P$, the term $\left(
\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)  x$ is
well-defined as well\footnote{\textit{Proof.} Let $x\in\wedge^{\ell}P$. Since
the map $\pi:P^{\otimes\ell}\rightarrow\wedge^{\ell}P$ is surjective, there
exists a $w\in P^{\otimes\ell}$ such that $x=\pi\left(  w\right)  $. Consider
this $w$. We know that $\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  \right)  w$ is well-defined. Since $\exp\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  =\sum\limits_{m\in\mathbb{N}%
}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}$,
we have $\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)  w=\left(  \sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  w=\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  ^{m}\left(  w\right)  $. Thus,%
\begin{align*}
\pi\left(  \left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  \right)  w\right)   &  =\pi\left(  \sum\limits_{m\in\mathbb{N}}%
\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{m}\left(  w\right)  \right)  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}%
{m!}\underbrace{\pi\left(  \left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  ^{m}\left(  w\right)  \right)  }_{=\left(  \pi\circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  w}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\pi\text{ is linear}\right) \\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\left(  \pi
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)
}_{\substack{=\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{m}\circ
\pi\\\text{(by (\ref{pf.finitary.rhoRho.projm}))}}}w=\sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\underbrace{\left(  \left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\circ\pi\right)  w}_{=\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\left(  \pi\left(  w\right)  \right)  }\\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\underbrace{\left(  \pi\left(  w\right)  \right)
}_{=x}=\underbrace{\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{m}}_{=\exp\left(  \rho_{P,\ell
}\left(  a\right)  \right)  }\left(  x\right)  =\left(  \exp\left(
\rho_{P,\ell}\left(  a\right)  \right)  \right)  x.
\end{align*}
Thus, the term $\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)
\right)  x$ is well-defined, qed.}. In other words, the endomorphism
$\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  $ of $\wedge^{\ell}P$ is
well-defined. Since $\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{m}$, we have%
\begin{align*}
\pi\circ\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)   &  =\pi\circ\left(  \sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  =\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\pi\circ\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{m}}_{\substack{=\left(  \rho_{P,\ell
}\left(  a\right)  \right)  ^{m}\circ\pi\\\text{(by
(\ref{pf.finitary.rhoRho.projm}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right) \\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\circ\pi=\underbrace{\left(  \sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(  a\right)  \right)
^{m}\right)  }_{=\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  }\circ
\pi=\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)
\circ\pi.
\end{align*}
Since $\left(  \exp a\right)  ^{\otimes\ell}=\exp\left(  \rho_{P,\ell}%
^{\prime}\left(  a\right)  \right)  $ (by (\ref{pf.finitary.rhoRho.expexp})),
this rewrites as $\pi\circ\left(  \exp a\right)  ^{\otimes\ell}=\left(
\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)  \circ\pi$.
\end{verlong}

\begin{vershort}
On the other hand, since the projection $\pi:P^{\otimes\ell}\rightarrow
\wedge^{\ell}P$ is functorial in $P$, we have $\pi\circ\left(  \exp a\right)
^{\otimes\ell}=\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi$.
Thus,
\[
\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi=\pi\circ\left(
\exp a\right)  ^{\otimes\ell}=\left(  \exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  \right)  \circ\pi.
\]
Since the morphism $\pi$ is right-cancellable (since it is surjective), this
yields $\wedge^{\ell}\left(  \exp a\right)  =\exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  $. This proves Theorem \ref{thm.finitary.rhoRho}.
\end{vershort}

\begin{verlong}
On the other hand, since the projection $\pi:P^{\otimes\ell}\rightarrow
\wedge^{\ell}P$ is functorial in $P$, the diagram%
\[%
%TCIMACRO{\TeXButton{tensor vs wedge functoriality}{\xymatrixcolsep{4pc}
%\xymatrix{
%P^{\otimes\ell} \ar[r]^{\left(\exp a\right)^{\otimes\ell}} \ar@{->>}[d]_{\pi}
%& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
%\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \left(\exp a\right)} & \wedge^{\ell} P
%}}}%
%BeginExpansion
\xymatrixcolsep{4pc}
\xymatrix{
P^{\otimes\ell} \ar[r]^{\left(\exp a\right)^{\otimes\ell}} \ar@{->>}[d]_{\pi}
& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \left(\exp a\right)} & \wedge^{\ell} P
}%
%EndExpansion
\]
commutes. In other words, $\pi\circ\left(  \exp a\right)  ^{\otimes\ell
}=\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi$. Thus,
\[
\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi=\pi\circ\left(
\exp a\right)  ^{\otimes\ell}=\left(  \exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  \right)  \circ\pi.
\]
Since the morphism $\pi$ is right-cancellable (since it is surjective), this
yields $\wedge^{\ell}\left(  \exp a\right)  =\exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  $. This proves Theorem \ref{thm.finitary.rhoRho}.
\end{verlong}

\begin{vershort}
\textit{Second proof of Theorem \ref{thm.finitary.rhoRho} (sketched).} Since
$a$ is nilpotent, it is known that the exponential $\exp a$ is a well-defined
unipotent element of $\operatorname*{GL}\left(  P\right)  $. But for every
$\ell\in\mathbb{N}$, the $\ell$-th exterior power of any unipotent element of
$\operatorname*{GL}\left(  P\right)  $ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $. Since $\exp a$ is a
unipotent element of $\operatorname*{GL}\left(  P\right)  $, this yields that
$\wedge^{\ell}\left(  \exp a\right)  $ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $ for every $\ell
\in\mathbb{N}$. Hence, the logarithm $\log\left(  \wedge^{\ell}\left(  \exp
a\right)  \right)  $ is well-defined for every $\ell\in\mathbb{N}$.
\end{vershort}

\begin{verlong}
\textit{Second proof of Theorem \ref{thm.finitary.rhoRho}.} Since $a$ is
nilpotent, it is known that the exponential $\exp a$ is a well-defined
unipotent element of $\operatorname*{GL}\left(  P\right)  $. But for every
$\ell\in\mathbb{N}$, the $\ell$-th exterior power of any unipotent element of
$\operatorname*{GL}\left(  P\right)  $ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\ell\in\mathbb{N}$. Let $\alpha$ be a
unipotent element of $\operatorname*{GL}\left(  P\right)  $. We must prove
that $\wedge^{\ell}\alpha$ is a unipotent element of $\operatorname*{GL}%
\left(  \wedge^{\ell}P\right)  $.
\par
Since $\alpha$ is unipotent, $\alpha-1\in\operatorname*{End}P$ is nilpotent.
That is, there exists an $n\in\mathbb{N}$ such that $\left(  \alpha-1\right)
^{n}=0$. Consider this $n$.
\par
We will denote the identity map $\operatorname*{id}:P\rightarrow P$ by $1$
(since it is the unity of the algebra $\operatorname*{End}P$).
\par
For every $i\in\left\{  1,2,...,\ell\right\}  $, let $\alpha_{i}$ denote the
endomorphism $\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha
}_{i-1\text{ times }\alpha}\otimes\left(  \alpha-1\right)  \otimes
\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1}$ of
$P^{\otimes\ell}$. Then, the endomorphisms $\alpha_{1}$, $\alpha_{2}$, $...$,
$\alpha_{n}$ commute with each other (because $\alpha$, $\alpha-1$ and $1$
commute with each other). But for every $i\in\left\{  1,2,...,\ell\right\}  $,
we have%
\[
\alpha_{i}=\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{
times }\alpha}\otimes\left(  \alpha-1\right)  \otimes\underbrace{1\otimes
1\otimes...\otimes1}_{\ell-i\text{ times }1},
\]
so that%
\begin{align*}
\alpha_{i}^{n}  &  =\left(  \underbrace{\alpha\otimes\alpha\otimes
...\otimes\alpha}_{i-1\text{ times }\alpha}\otimes\left(  \alpha-1\right)
\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1
}\right)  ^{n}\\
&  =\underbrace{\alpha^{n}\otimes\alpha^{n}\otimes...\otimes\alpha^{n}%
}_{i-1\text{ times }\alpha^{n}}\otimes\underbrace{\left(  \alpha-1\right)
^{n}}_{=0}\otimes\underbrace{1^{n}\otimes1^{n}\otimes...\otimes1^{n}}%
_{\ell-i\text{ times }1^{n}}=0,
\end{align*}
so that $\alpha_{i}$ is nilpotent.
\par
Meanwhile, it is known that if finitely many nilpotent elements of an algebra
commute with each other, then the sum of these elements must also be
nilpotent. Applying this result to the nilpotent elements $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{\ell}$ of the algebra $\operatorname*{End}V$
(these elements commute with each other, as we know), we conclude that the sum
$\sum\limits_{i=1}^{\ell}\alpha_{i}$ is nilpotent. But every $i\in\left\{
1,2,...,\ell\right\}  $ satisfies%
\begin{align*}
\alpha_{i}  &  =\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha
}_{i-1\text{ times }\alpha}\otimes\left(  \alpha-1\right)  \otimes
\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1}\\
&  =\underbrace{\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha
}_{i-1\text{ times }\alpha}\otimes\alpha}_{=\underbrace{\alpha\otimes
\alpha\otimes...\otimes\alpha}_{i\text{ times }\alpha}}\otimes
\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1 }%
-\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{ times
}\alpha}\otimes\underbrace{1\otimes\underbrace{1\otimes1\otimes...\otimes
1}_{\ell-i\text{ times }1}}_{=\underbrace{1\otimes1\otimes...\otimes1}%
_{\ell-i+1\text{ times }1}=\underbrace{1\otimes1\otimes...\otimes1}%
_{\ell-\left(  i-1\right)  \text{ times }1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the multilinearity of the tensor
product}\right) \\
&  =\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i\text{ times
}\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times
}1}-\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{ times
}\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-\left(
i-1\right)  \text{ times }1}.
\end{align*}
Hence,%
\begin{align*}
\sum\limits_{i=1}^{\ell}\alpha_{i}  &  =\sum\limits_{i=1}^{\ell}\left(
\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i\text{ times }%
\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }%
1}-\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{ times
}\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-\left(
i-1\right)  \text{ times }1}\right) \\
&  =\underbrace{\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}%
_{\ell\text{ times }\alpha}}_{=\alpha^{\otimes\ell}}\otimes
\underbrace{\underbrace{1\otimes1\otimes...\otimes1}_{\ell-\ell\text{ times
}1}}_{=\left(  \text{empty tensor product}\right)  }%
-\underbrace{\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{0\text{
times }\alpha}}_{=\left(  \text{empty tensor product}\right)  }\otimes
\underbrace{\underbrace{1\otimes1\otimes...\otimes1}_{\ell-0\text{ times }1}%
}_{=1^{\otimes\left(  \ell-0\right)  }=1^{\otimes\ell}=\operatorname*{id}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the telescope principle}\right) \\
&  =\alpha^{\otimes\ell}-\operatorname*{id}.
\end{align*}
Since we know that $\sum\limits_{i=1}^{\ell}\alpha_{i}$ is nilpotent, this
yields that $\alpha^{\otimes\ell}-\operatorname*{id}$ is nilpotent. In other
words, $\alpha^{\otimes\ell}$ is unipotent.
\par
But let $\pi$ be the canonical projection $P^{\otimes\ell}\rightarrow
\wedge^{\ell}P$. Then, we have a commutative diagram%
\[%
%TCIMACRO{\TeXButton{tensor vs wedge functoriality}{\xymatrixcolsep{4pc}
%\xymatrix{
%P^{\otimes\ell} \ar[r]^{\alpha^{\otimes\ell}} \ar@{->>}[d]_{\pi}
%& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
%\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \alpha} & \wedge^{\ell} P
%}}}%
%BeginExpansion
\xymatrixcolsep{4pc}
\xymatrix{
P^{\otimes\ell} \ar[r]^{\alpha^{\otimes\ell}} \ar@{->>}[d]_{\pi}
& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \alpha} & \wedge^{\ell} P
}%
%EndExpansion
\]
(since the canonical projection $P^{\otimes\ell}\rightarrow\wedge^{\ell}P$ is
functorial). Thus, $\left(  \wedge^{\ell}\alpha\right)  \circ\pi=\pi
\circ\alpha^{\otimes\ell}$. Hence,
\[
\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  \circ\pi
=\underbrace{\left(  \wedge^{\ell}\alpha\right)  \circ\pi}_{=\pi\circ
\alpha^{\otimes\ell}} - \underbrace{\operatorname*{id}\circ\pi}_{=\pi=\pi
\circ\operatorname*{id}} = \pi\circ\alpha^{\otimes\ell} - \pi\circ
\operatorname*{id} = \pi\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}%
\right)
\]
(since composition of linear maps is bilinear).
\par
Now, for every $m\in\mathbb{N}$, we have
\begin{equation}
\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{m}\circ\pi=\pi
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{m}.
\label{pf.finitary.rhoRho.m}%
\end{equation}
(\textit{Proof of (\ref{pf.finitary.rhoRho.m}):} We will prove
(\ref{pf.finitary.rhoRho.m}) by induction over $m$:
\par
\textit{Induction base:} Comparing $\underbrace{\left(  \wedge^{\ell}%
\alpha-\operatorname*{id}\right)  ^{0}}_{=\operatorname*{id}}\circ\pi=\pi$ and
$\pi\circ\underbrace{\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)
^{0}}_{=\operatorname*{id}}=\pi$, we obtain $\left(  \wedge^{\ell}%
\alpha-\operatorname*{id}\right)  ^{0}\circ\pi=\pi\circ\left(  \alpha
^{\otimes\ell}-\operatorname*{id}\right)  ^{0}$. Thus,
(\ref{pf.finitary.rhoRho.m}) holds for $m=0$. This completes the induction
base.
\par
\textit{Induction step:} Let $\mu\in\mathbb{N}$. Assume that
(\ref{pf.finitary.rhoRho.m}) holds for $m=\mu$. We must prove that
(\ref{pf.finitary.rhoRho.m}) holds for $m=\mu+1$ as well.
\par
Since (\ref{pf.finitary.rhoRho.m}) holds for $m=\mu$, we have $\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\pi=\pi\circ\left(
\alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu}$. Now,%
\begin{align*}
\underbrace{\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu+1}%
}_{=\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  }\circ\pi &  =\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\underbrace{\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  \circ\pi}_{=\pi\circ\left(
\alpha^{\otimes\ell}-\operatorname*{id}\right)  }=\underbrace{\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\pi}_{=\pi
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu}}%
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right) \\
&  =\pi\circ\underbrace{\left(  \alpha^{\otimes\ell}-\operatorname*{id}%
\right)  ^{\mu}\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)
}_{=\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu+1}}=\pi
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu+1}.
\end{align*}
Thus, (\ref{pf.finitary.rhoRho.m}) holds for $m=\mu+1$. This completes the
induction step. The induction proof of (\ref{pf.finitary.rhoRho.m}) is thus
complete.)
\par
But since $\alpha^{\otimes\ell}-\operatorname*{id}$ is nilpotent, there exists
some $m\in\mathbb{N}$ such that $\left(  \alpha^{\otimes\ell}%
-\operatorname*{id}\right)  ^{m}=0$. Consider this $m$. By
(\ref{pf.finitary.rhoRho.m}), we have $\left(  \wedge^{\ell}\alpha
-\operatorname*{id}\right)  ^{m}\circ\pi=\pi\circ\underbrace{\left(
\alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{m}}_{=0}=\pi\circ0=0$. Since
$\pi$ is right-cancellable (because $\pi$ is a projection and thus
surjective), this yields $\left(  \wedge^{\ell}\alpha-\operatorname*{id}%
\right)  ^{m}=0$. Hence, $\wedge^{\ell}\alpha-\operatorname*{id}$ is
nilpotent, so that $\wedge^{\ell}\alpha$ is unipotent.
\par
We have thus shown that $\wedge^{\ell}\alpha$ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $ whenever $\alpha$ is a
unipotent element of $\operatorname*{GL}\left(  P\right)  $. In other words,
the $\ell$-th exterior power of any unipotent element of $\operatorname*{GL}%
\left(  P\right)  $ is a unipotent element of $\operatorname*{GL}\left(
\wedge^{\ell}P\right)  $, qed.}. Since $\exp a$ is a unipotent element of
$\operatorname*{GL}\left(  P\right)  $, this yields that $\wedge^{\ell}\left(
\exp a\right)  $ is a unipotent element of $\operatorname*{GL}\left(
\wedge^{\ell}P\right)  $ for every $\ell\in\mathbb{N}$. Hence, the logarithm
$\log\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  $ is well-defined
for every $\ell\in\mathbb{N}$.
\end{verlong}

On the other hand, consider the map $\wedge\left(  \exp a\right)  :\wedge
P\rightarrow\wedge P$. This map is an algebra homomorphism (because generally,
if $Q$ and $R$ are two vector spaces, and $f:Q\rightarrow R$ is a linear map,
then $\wedge f:\wedge Q\rightarrow\wedge R$ is an algebra homomorphism) and
identical with the direct sum $\bigoplus\limits_{\ell\in\mathbb{N}}%
\wedge^{\ell}\left(  \exp a\right)  :\bigoplus\limits_{\ell\in\mathbb{N}%
}\wedge^{\ell}P\rightarrow\bigoplus\limits_{\ell\in\mathbb{N}}\wedge^{\ell}P$
of the linear maps $\wedge^{\ell}\left(  \exp a\right)  :\wedge^{\ell
}P\rightarrow\wedge^{\ell}P$.

Since $\wedge\left(  \exp a\right)  =\bigoplus\limits_{\ell\in\mathbb{N}%
}\wedge^{\ell}\left(  \exp a\right)  $, we have $\log\left(  \wedge\left(
\exp a\right)  \right)  =\log\left(  \bigoplus\limits_{\ell\in\mathbb{N}%
}\wedge^{\ell}\left(  \exp a\right)  \right)  =\bigoplus\limits_{\ell
\in\mathbb{N}}\log\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  $
(because logarithms on direct sums are componentwise).\footnote{Note that the
map $\wedge\left(  \exp a\right)  $ needs not be unipotent, but the logarithm
$\log\left(  \wedge\left(  \exp a\right)  \right)  $ nevertheless makes sense
because the map $\wedge\left(  \exp a\right)  $ is a direct sum of unipotent
maps (and thus is locally unipotent).} As a consequence, every $\ell
\in\mathbb{N}$ and every $p_{1},p_{2},...,p_{\ell}\in P$ satisfy $p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\in\wedge^{\ell}P$ and thus $\left(  \log\left(
\wedge\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(  \wedge^{\ell}\left(
\exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}\wedge...\wedge
p_{\ell}\right)  $.

But it is well-known that if $A$ is an algebra and $f:A\rightarrow A$ is an
algebra endomorphism such that $\log f$ is well-defined, then $\log
f:A\rightarrow A$ is a derivation. Applied to $A=\wedge P$ and $f=\wedge
\left(  \exp a\right)  $, this yields that $\log\left(  \wedge\left(  \exp
a\right)  \right)  :\wedge P\rightarrow\wedge P$ is a derivation.

But every $p\in P$ satisfies
\begin{equation}
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p\right)  =a\rightharpoonup p, \label{pf.finitary.rhoRho.deg1}%
\end{equation}
where $p$ is viewed as an element of $\wedge^{1}P\subseteq\wedge
P$.\ \ \ \ \footnote{\textit{Proof of (\ref{pf.finitary.rhoRho.deg1}):} Let
$p\in P$. Since $\log\left(  \wedge\left(  \exp a\right)  \right)
=\bigoplus\limits_{\ell\in\mathbb{N}}\log\left(  \wedge^{\ell}\left(  \exp
a\right)  \right)  $ and $p\in P=\wedge^{1}P$, we have
\[
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p\right)  =\left(  \log\underbrace{\left(  \wedge^{1}\left(  \exp a\right)
\right)  }_{=\exp a}\right)  \left(  p\right)  =\underbrace{\left(
\log\left(  \exp a\right)  \right)  }_{=a}\left(  p\right)
=ap=a\rightharpoonup p.
\]
This proves (\ref{pf.finitary.rhoRho.deg1}).}

Now recall the Leibniz identity for derivations. In its general form, it says
that if $A$ is an algebra, $M$ is an $A$-bimodule, and $d:A\rightarrow M$ is a
derivation, then every $\ell\in\mathbb{N}$ and every $p_{1},p_{2},...,p_{\ell
}\in A$ satisfy
\[
d\left(  p_{1}p_{2}...p_{\ell}\right)  =\sum\limits_{k=1}^{\ell}p_{1}%
p_{2}...p_{k-1}d\left(  p_{k}\right)  p_{k+1}p_{k+2}...p_{\ell}.
\]
Applying this to $A=\wedge P$, $M=\wedge P$ and $d=\log\left(  \wedge\left(
\exp a\right)  \right)  $, we conclude that every $\ell\in\mathbb{N}$ and
every $p_{1},p_{2},...,p_{\ell}\in\wedge P$ satisfy%
\[
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p_{1}p_{2}...p_{\ell}\right)  =\sum\limits_{k=1}^{\ell}p_{1}p_{2}%
...p_{k-1}\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)
\left(  p_{k}\right)  p_{k+1}p_{k+2}...p_{\ell}%
\]
(since $\log\left(  \wedge\left(  \exp a\right)  \right)  :\wedge
P\rightarrow\wedge P$ is a derivation). Thus, every $\ell\in\mathbb{N}$ and
every $p_{1},p_{2},...,p_{\ell}\in P$ satisfy
\begin{align}
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p_{1}p_{2}...p_{\ell}\right)   &  =\sum\limits_{k=1}^{\ell}p_{1}%
p_{2}...p_{k-1}\underbrace{\left(  \log\left(  \wedge\left(  \exp a\right)
\right)  \right)  \left(  p_{k}\right)  }_{\substack{=a\rightharpoonup
p_{k}\\\text{(by (\ref{pf.finitary.rhoRho.deg1}), applied to }p=p_{k}\text{)}%
}}p_{k+1}p_{k+2}...p_{\ell}\nonumber\\
&  =\sum\limits_{k=1}^{\ell}\underbrace{p_{1}p_{2}...p_{k-1}\left(
a\rightharpoonup p_{k}\right)  p_{k+1}p_{k+2}...p_{\ell}}_{\substack{=p_{1}%
\wedge p_{2}\wedge...\wedge p_{k-1}\wedge\left(  a\rightharpoonup
p_{k}\right)  \wedge p_{k+1}\wedge p_{k+2}\wedge...\wedge p_{\ell
}\\\text{(since the multiplication in }\wedge P\text{ is given by the wedge
product)}}}\nonumber\\
&  =\sum\limits_{k=1}^{\ell}p_{1}\wedge p_{2}\wedge...\wedge p_{k-1}%
\wedge\left(  a\rightharpoonup p_{k}\right)  \wedge p_{k+1}\wedge
p_{k+2}\wedge...\wedge p_{\ell}\nonumber\\
&  =\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{def.finitary.rho})}\right)  . \label{pf.finitary.rhoRho.calc}%
\end{align}
On the other hand, every $\ell\in\mathbb{N}$ and every $p_{1},p_{2}%
,...,p_{\ell}\in P$ satisfy
\begin{align*}
&  \left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)
\underbrace{\left(  p_{1}p_{2}...p_{\ell}\right)  }_{\substack{=p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\\\text{(since the multiplication in }\wedge
P\text{ is given by the wedge product)}}}\\
&  =\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p_{1}\wedge p_{2}\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(
\wedge^{\ell}\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  .
\end{align*}
Compared with (\ref{pf.finitary.rhoRho.calc}), this yields%
\[
\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(  \wedge^{\ell
}\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}\right)
\]
for every $\ell\in\mathbb{N}$ and every $p_{1},p_{2},...,p_{\ell}\in P$.

Now fix $\ell\in\mathbb{N}$. We know that
\[
\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(  \wedge^{\ell
}\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}\right)
\]
for every $p_{1},p_{2},...,p_{\ell}\in P$. Since the vector space
$\wedge^{\ell}P$ is spanned by elements of the form $p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}$ with $p_{1},p_{2},...,p_{\ell}\in P$, this yields
that the two linear maps $\rho_{P,\ell}\left(  a\right)  $ and $\log\left(
\wedge^{\ell}\left(  \exp a\right)  \right)  $ are equal to each other on a
spanning set of the vector space $\wedge^{\ell}P$. Therefore, these two maps
must be identical (because if two linear maps are equal to each other on a
spanning set of their domain, then they must always be identical). In other
words, $\rho_{P,\ell}\left(  a\right)  =\log\left(  \wedge^{\ell}\left(  \exp
a\right)  \right)  $. Exponentiating this equality, we obtain $\exp\left(
\rho_{P,\ell}\left(  a\right)  \right)  =\wedge^{\ell}\left(  \exp a\right)
$. This proves Theorem \ref{thm.finitary.rhoRho}.

\subsubsection{\label{subsubsect.schur2.reduct}Reduction to fermions}

We are now going to reduce Theorem \ref{thm.schur} to a ``purely fermionic''
statement -- a statement (Theorem \ref{thm.schur.fermi}) not involving the
bosonic space $\mathcal{B}$ or the Boson-Fermion correspondence $\sigma$ in
any way. We will later (Subsection \ref{subsubsect.skewschur}) generalize this
statement, and yet later prove the generalization.

First, a definition:

\begin{definition}
Let $\mathbf{R}$ (not to be confused with the field $\mathbb{R}$) be a
commutative $\mathbb{Q}$-algebra. We denote by $\mathcal{A}_{\mathbf{R}}$ the
Heisenberg algebra defined over the ground ring $\mathbf{R}$ in lieu of
$\mathbb{C}$. We denote by $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ the
$\mathcal{A}_{\mathbf{R}}$-module $\mathcal{B}^{\left(  0\right)  }$ defined
over the ground ring $\mathbf{R}$ in lieu of $\mathbb{C}$. We denote by
$\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ the $\mathcal{A}_{\mathbf{R}}%
$-module $\mathcal{F}^{\left(  0\right)  }$ defined over the ground ring
$\mathbf{R}$ in lieu of $\mathbb{C}$. We denote by $\sigma_{\mathbf{R}}$ the
map $\sigma$ defined over the ground ring $\mathbf{R}$ in lieu of $\mathbb{C}%
$. (This $\sigma_{\mathbf{R}}$ is thus a graded $\mathcal{A}_{\mathbf{R}}%
$-module homomorphism $\mathcal{B}_{\mathbf{R}}\rightarrow\mathcal{F}%
_{\mathbf{R}}$, where $\mathcal{B}_{\mathbf{R}}$ and $\mathcal{F}_{\mathbf{R}%
}$ are the $\mathcal{A}_{\mathbf{R}}$-modules $\mathcal{B}$ and $\mathcal{F}$
defined over the ground ring $\mathbf{R}$ in lieu of $\mathbb{C}$.)
\end{definition}

Next, some preparations:

\begin{proposition}
\label{prop.schur.fermi.welldef}Let $\mathbf{R}$ be a commutative $\mathbb{Q}%
$-algebra. Let $y_{1},y_{2},y_{3},...$ be some elements of $\mathbf{R}$.

\textbf{(a)} Let $M$ be a $\mathbb{Z}$-graded $\mathcal{A}_{\mathbf{R}}%
$-module concentrated in nonpositive degrees (i. e., satisfying $M\left[
n\right]  =0$ for all positive integers $n$). The map $\exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  :M\rightarrow M$ is well-defined, in
the following sense: For every $m\in M$, expanding the expression $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with
only finitely many nonzero addends.

\textbf{(b)} Let $M$ and $N$ be two $\mathbb{Z}$-graded $\mathcal{A}%
_{\mathbf{R}}$-modules concentrated in nonpositive degrees. Let $\eta
:M\rightarrow N$ be an $\mathcal{A}_{\mathbf{R}}$-module homomorphism. Then,%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ\eta=\eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)
\]
as maps from $M$ to $N$.

\textbf{(c)} Consider the $\mathbb{Z}$-graded $\mathcal{A}_{\mathbf{R}}%
$-module $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$. This $\mathbb{Z}%
$-graded $\mathcal{A}_{\mathbf{R}}$-module $\mathcal{F}_{\mathbf{R}}^{\left(
0\right)  }$ is concentrated in nonpositive degrees. Hence, by Theorem
\ref{thm.schur.fermi}, the map $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  :\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\rightarrow\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ is well-defined.
Thus, $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ is well-defined
for every $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.fermi.welldef}.} \textbf{(a)} Let
$m\in M$. We will prove that expanding the expression $\exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with only
finitely many nonzero terms.

Since $M$ is $\mathbb{Z}$-graded, we can write $m$ in the form $m=\sum
\limits_{n\in\mathbb{Z}}m_{n}$ for a family $\left(  m_{n}\right)
_{n\in\mathbb{Z}}$ of elements of $M$ which satisfy $\left(  m_{n}\in M\left[
n\right]  \text{ for every }n\in\mathbb{Z}\right)  $ and $\left(
m_{n}=0\text{ for all but finitely many }n\in\mathbb{Z}\right)  $. Consider
this family $\left(  m_{n}\right)  _{n\in\mathbb{Z}}$. We know that $m_{n}=0$
for all but finitely many $n\in\mathbb{Z}$. In other words, there exists a
finite subset $I$ of $\mathbb{Z}$ such that every $n\in\mathbb{Z}\setminus I$
satisfies $m_{n}=0$. Consider this $I$. Let $s$ be an integer which is smaller
than every element of $I$. (Such an $s$ exists since $I$ is finite.) Then,%
\begin{equation}
fm=0\ \ \ \ \ \ \ \ \ \ \text{for every integer }q\geq-s\text{ and every }f\in
U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  q\right]
\label{pf.schur.fermi.welldef.fm=0}%
\end{equation}
(where $U_{\mathbf{R}}$ means ``enveloping algebra over the ground ring
$\mathbf{R}$ '').\ \ \ \ \footnote{\textit{Proof of
(\ref{pf.schur.fermi.welldef.fm=0}):} Let $q\geq-s$ be an integer, and let
$f\in U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  q\right]
$. Since $s$ is smaller than every element of $I$, we have $s<n$ for every
$n\in I$. Thus, $q\geq-\underbrace{s}_{<n}>-n$ for every $n\in I$, so that
$q+n>0$ for every $n\in I$ and thus $M\left[  q+n\right]  =0$ for every $n\in
I$ (since $M$ is concentrated in nonpositive degrees).
\par
Notice that $M$ is a graded $\mathcal{A}_{\mathbf{R}}$-module, thus a graded
$U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $-module. But%
\[
m=\sum\limits_{n\in\mathbb{Z}}m_{n}=\sum\limits_{n\in I}m_{n}+\sum
\limits_{n\in\mathbb{Z}\setminus I}\underbrace{m_{n}}%
_{\substack{=0\\\text{(since }n\in\mathbb{Z}\setminus I\text{)}}%
}=\sum\limits_{n\in I}m_{n}+\underbrace{\sum\limits_{n\in\mathbb{Z}\setminus
I}0}_{=0}=\sum\limits_{n\in I}m_{n},
\]
so that%
\[
fm=f\sum\limits_{n\in I}m_{n}=\sum\limits_{n\in I}\underbrace{fm_{n}%
}_{\substack{\in M\left[  q+n\right]  \\\text{(since }f\in U_{\mathbf{R}%
}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  q\right]  \text{ and }%
m_{n}\in M\left[  n\right]  \text{,}\\\text{and since }M\text{ is a graded
}U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \text{-module)}}%
}\in\sum\limits_{n\in I}\underbrace{M\left[  q+n\right]  }%
_{\substack{=0\\\text{(since }n\in I\text{)}}}=\sum\limits_{n\in I}0=0,
\]
so that $fm=0$, qed.}

Expanding the expression $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  m$, we obtain%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m\\
&  =\sum\limits_{i=0}^{\infty}\dfrac{1}{i!}\left(  \underbrace{y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...}_{=\sum\limits_{j\in\left\{
1,2,3,...\right\}  }y_{j}a_{j}}\right)  ^{i}m=\sum\limits_{i=0}^{\infty}%
\dfrac{1}{i!}\underbrace{\left(  \sum\limits_{j\in\left\{  1,2,3,...\right\}
}y_{j}a_{j}\right)  ^{i}}_{=\sum\limits_{\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}%
}a_{j_{2}}...a_{j_{i}}}m\\
&  =\sum\limits_{i=0}^{\infty}\dfrac{1}{i!}\sum\limits_{\left(  j_{1}%
,j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}y_{j_{1}}%
y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\\
&  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}%
\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m.
\end{align*}
But this infinite sum has only finitely many nonzero
addends\footnote{\textit{Proof.} Let $i\in\mathbb{N}$ and $\left(  j_{1}%
,j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}$ be such that
$\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}%
}m\neq0$. Since $a_{j_{k}}\in U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}%
}\right)  \left[  j_{k}\right]  $ for every $k\in\left\{  1,2,...,i\right\}
$, we have
\begin{align*}
a_{j_{1}}a_{j_{2}}...a_{j_{i}}  &  \in\left(  U_{\mathbf{R}}\left(
\mathcal{A}_{\mathbf{R}}\right)  \left[  j_{1}\right]  \right)  \left(
U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  j_{2}\right]
\right)  ...\left(  U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)
\left[  j_{i}\right]  \right) \\
&  \subseteq U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[
j_{1}+j_{2}+...+j_{i}\right]  ,
\end{align*}
so that%
\[
\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}%
\in\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}U_{\mathbf{R}}\left(
\mathcal{A}_{\mathbf{R}}\right)  \left[  j_{1}+j_{2}+...+j_{i}\right]
\subseteq U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[
j_{1}+j_{2}+...+j_{i}\right]  .
\]
Hence, if $j_{1}+j_{2}+...+j_{i}\geq-s$, then $\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m=0$ (by
(\ref{pf.schur.fermi.welldef.fm=0}), applied to $f=\dfrac{1}{i!}y_{j_{1}%
}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}$ and $q=j_{1}%
+j_{2}+...+j_{i}$), contradicting $\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}%
}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\neq0$. As a consequence, we cannot have
$j_{1}+j_{2}+...+j_{i}\geq-s$. We must thus have $j_{1}+j_{2}+...+j_{i}<-s$.
\par
Now forget that we fixed $i$ and $\left(  j_{1},j_{2},...,j_{i}\right)  $. We
thus have shown that every $i\in\mathbb{N}$ and $\left(  j_{1},j_{2}%
,...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}$ such that $\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\neq0$
must satisfy $j_{1}+j_{2}+...+j_{i}<-s$. Since there are only finitely many
pairs $\left(  i,\left(  j_{1},j_{2},...,j_{i}\right)  \right)  $ of
$i\in\mathbb{N}$ and $\left(  j_{1},j_{2},...,j_{i}\right)  \in\left\{
1,2,3,...\right\}  ^{i}$ satisfying $j_{1}+j_{2}+...+j_{i}<-s$, this yields
that there are only finitely many pairs $\left(  i,\left(  j_{1}%
,j_{2},...,j_{i}\right)  \right)  $ of $i\in\mathbb{N}$ and $\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}$ satisfying
$\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}%
}m\neq0$. In other words, the infinite sum $\sum\limits_{\substack{i\in
\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)  \in\left\{
1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}%
}a_{j_{2}}...a_{j_{i}}m$ has only finitely many nonzero addends, qed.}. Thus,
we have shown that for every $m\in M$, expanding the expression $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with
only finitely many nonzero addends. This proves Proposition
\ref{prop.schur.fermi.welldef} \textbf{(a)}.

\begin{verlong}
\textbf{(b)} Just as in the proof of Proposition
\ref{prop.schur.fermi.welldef} \textbf{(a)} above, we can show that%
\begin{equation}
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m=\sum
\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\ \ \ \ \ \ \ \ \ \ \text{for
every }m\in M, \label{pf.schur.fermi.welldef.b.1}%
\end{equation}
with the infinite sum $\sum\limits_{\substack{i\in\mathbb{N};\\\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m$ having
only finitely many nonzero addends (for every fixed $m$). Similarly,%
\begin{equation}
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  n=\sum
\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}n\ \ \ \ \ \ \ \ \ \ \text{for
every }n\in N. \label{pf.schur.fermi.welldef.b.2}%
\end{equation}


Now, let $m\in M$. Since $\eta$ is an $\mathcal{A}_{\mathbf{R}}$-module
homomorphism, $\eta$ must also be an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism (since every $\mathcal{A}%
_{\mathbf{R}}$-module homomorphism is an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism). Thus,
\begin{equation}
\eta\left(  gm\right)  =g\cdot\eta\left(  m\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }g\in U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  . \label{pf.schur.fermi.welldef.b.3}%
\end{equation}
Since $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $ is not (in
general) an element of $U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)
$, we cannot directly apply this to $g=\exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  $. However, we have%
\begin{align*}
&  \left(  \eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  \right)  \left(  m\right) \\
&  =\eta\left(  \underbrace{\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  m}_{=\sum\limits_{\substack{i\in\mathbb{N};\\\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m}\right)
\\
&  =\eta\left(  \sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1}%
,j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}%
{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}%
\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}\underbrace{\eta\left(  a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\right)
}_{\substack{=a_{j_{1}}a_{j_{2}}...a_{j_{i}}\eta\left(  m\right)  \\\text{(by
(\ref{pf.schur.fermi.welldef.b.3}), applied to }g=a_{j_{1}}a_{j_{2}%
}...a_{j_{i}}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\eta\text{ is }\mathbf{R}\text{-linear, while the infinite sum
}\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\\
\text{has only finitely many nonzero addends}%
\end{array}
\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}%
\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}\eta\left(  m\right) \\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \left(
\eta\left(  m\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{pf.schur.fermi.welldef.b.2}) (applied to }n=\eta\left(
m\right)  \text{) yields}\\
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \left(  \eta\left(
m\right)  \right)  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}\eta\left(
m\right)
\end{array}
\right) \\
&  =\left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ\eta\right)  \left(  m\right)  .
\end{align*}


Now forget that we fixed $m$. We thus have proven that every $m\in M$
satisfies%
\[
\left(  \eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  \right)  \left(  m\right)  =\left(  \left(
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \circ
\eta\right)  \left(  m\right)  .
\]
In other words, $\eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  \right)  \circ\eta$. This proves Proposition
\ref{prop.schur.fermi.welldef} \textbf{(b)}.
\end{verlong}

\begin{vershort}
\textbf{(b)} In order to prove Proposition \ref{prop.schur.fermi.welldef}
\textbf{(b)}, we must clearly show that%
\begin{equation}
\eta\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m\right)
=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\eta\left(
m\right)  \label{pf.schur.fermi.welldef.b.short.1}%
\end{equation}
for every $m\in M$.

Fix $m\in M$. Since $\eta$ is an $\mathcal{A}_{\mathbf{R}}$-module
homomorphism, $\eta$ must also be an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism (since every $\mathcal{A}%
_{\mathbf{R}}$-module homomorphism is an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism). Thus,
\[
\eta\left(  gm\right)  =g\cdot\eta\left(  m\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }g\in U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  .
\]
If $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $ was an element
of $U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $, then we could
apply this to $g=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $
and conclude (\ref{pf.schur.fermi.welldef.b.short.1}) immediately.
Unfortunately, $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $ is
not an element of $U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $,
but this problem is easy to amend: By Proposition
\ref{prop.schur.fermi.welldef} \textbf{(a)}, we can find a finite partial sum
$g$ of the expanded power series $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ satisfying%
\begin{align*}
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m  &
=gm\ \ \ \ \ \ \ \ \ \ \text{and}\\
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\eta\left(
m\right)   &  =g\cdot\eta\left(  m\right)  .
\end{align*}
Consider such a $g$. Since $g$ is only a finite partial sum, we have $g\in
U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $, and thus
$\eta\left(  gm\right)  =g\cdot\eta\left(  m\right)  $. Hence,%
\begin{align*}
\eta\left(  \underbrace{\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  m}_{=gm}\right)   &  =\eta\left(  gm\right)  =g\cdot\eta\left(
m\right) \\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\eta\left(
m\right)  ,
\end{align*}
so that (\ref{pf.schur.fermi.welldef.b.short.1}) is proven. Thus, Proposition
\ref{prop.schur.fermi.welldef} \textbf{(b)} is proven.
\end{vershort}

\textbf{(c)} This is obvious.

Let us make a remark which we will only use in the ``finitary'' version of our
proof of Theorem \ref{thm.schur.fermi}. First, a definition:

\begin{definition}
For every commutative ring $\mathbf{R}$, let $\mathcal{A}_{+\mathbf{R}}$ be
the Lie algebra $\mathcal{A}_{+}$ defined for the ground ring $\mathbf{R}$
instead of $\mathbb{C}$.
\end{definition}

Now, it is easy to see that Proposition \ref{prop.schur.fermi.welldef} holds
with $\mathcal{A}_{\mathbf{R}}$ replaced by $\mathcal{A}_{+\mathbf{R}}$. We
will only use the analogues of parts \textbf{(a)} and \textbf{(b)}:

\begin{proposition}
\label{prop.schur.fermi.welldef.A+}Let $\mathbf{R}$ be a commutative
$\mathbb{Q}$-algebra. Let $y_{1},y_{2},y_{3},...$ be some elements of
$\mathbf{R}$.

\textbf{(a)} Let $M$ be a $\mathbb{Z}$-graded $\mathcal{A}_{+\mathbf{R}}%
$-module concentrated in nonpositive degrees (i. e., satisfying $M\left[
n\right]  =0$ for all positive integers $n$). The map $\exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  :M\rightarrow M$ is well-defined, in
the following sense: For every $m\in M$, expanding the expression $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with
only finitely many nonzero addends.

\textbf{(b)} Let $M$ and $N$ be two $\mathbb{Z}$-graded $\mathcal{A}%
_{+\mathbf{R}}$-modules concentrated in nonpositive degrees. Let
$\eta:M\rightarrow N$ be an $\mathcal{A}_{+\mathbf{R}}$-module homomorphism.
Then,%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ\eta=\eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)
\]
as maps from $M$ to $N$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.fermi.welldef.A+}.} In order to
obtain proofs of Proposition \ref{prop.schur.fermi.welldef.A+}, it is enough
to simply replace $\mathcal{A}_{\mathbf{R}}$ by $\mathcal{A}_{+\mathbf{R}}$
throughout the proof of parts \textbf{(a)} and \textbf{(b)} of Proposition
\ref{prop.schur.fermi.welldef}.

Now, let us state the ``fermionic'' version of Theorem \ref{thm.schur}:

\begin{theorem}
\label{thm.schur.fermi}Let $\mathbf{R}$ be a commutative $\mathbb{Q}$-algebra.
Let $y_{1},y_{2},y_{3},...$ be some elements of $\mathbf{R}$. Denote by $y$
the family $\left(  y_{1},y_{2},y_{3},...\right)  $. Let $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ be a $0$-degression.

The $\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  $-coordinate of
$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ (this is a
well-defined element of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ due to
Proposition \ref{prop.schur.fermi.welldef} \textbf{(c)}) with respect to the
basis\footnotemark\ $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$
equals $S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)  $. (Here, we
are using the fact that $\left(  i_{k}+k\right)  _{k\geq0}$ is a partition for
every $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. This follows
from Proposition \ref{prop.glinf.wedge.grading}, applied to $m=0$.)
\end{theorem}

\footnotetext{Here, ``basis'' means ``$\mathbf{R}$-module basis'', not
``$\mathbb{C}$-vector space basis''.}Let us see how this yields Theorem
\ref{thm.schur}:

\textit{Proof of Theorem \ref{thm.schur} using Theorem \ref{thm.schur.fermi}.}
Fix a $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $; then,
$i_{0}>i_{1}>i_{2}>...$ and $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\mathcal{F}^{\left(  0\right)  }$. Let $\lambda$ be the partition
$\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $.

Denote the element $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \in\mathcal{B}^{\left(  0\right)  }$ by $P\left(
x\right)  $. We need to show that $P\left(  x\right)  =S_{\lambda}\left(
x\right)  $.

From now on, we let $y$ denote another countable family of indeterminates
$\left(  y_{1},y_{2},y_{3},...\right)  $ (rather than a finite family like the
$\left(  y_{1},y_{2},...,y_{N}\right)  $ of Definition \ref{def.schur.y}).
Thus, whenever $Q$ is a polynomial in countably many indeterminates, $Q\left(
y\right)  $ will mean $Q\left(  y_{1},y_{2},y_{3},...\right)  $.

Let $\mathbf{R}$ be the polynomial ring $\mathbb{C}\left[  y_{1},y_{2}%
,y_{3},...\right]  $. Then, $y$ is a family of elements of $\mathbf{R}$.

By the definition of $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$, we have
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }=\mathbf{R}\left[  x_{1}%
,x_{2},x_{3},...\right]  $ as a vector space, so that $\mathcal{B}%
_{\mathbf{R}}^{\left(  0\right)  }=\left(  \mathbb{C}\left[  y_{1},y_{2}%
,y_{3},...\right]  \right)  \left[  x_{1},x_{2},x_{3},...\right]  $ as a
vector space. Let us denote by $1\in\mathcal{B}^{\left(  0\right)  }$ the
unity of the algebra $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $.
Clearly, $\mathcal{B}^{\left(  0\right)  }\subseteq\mathcal{B}_{\mathbf{R}%
}^{\left(  0\right)  }$, and thus $1\in\mathcal{B}^{\left(  0\right)
}\subseteq\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$.

We still let $x$ denote the whole collection of variables $\left(  x_{1}%
,x_{2},x_{3},...\right)  $. Also, let $x+y$ denote the family $\left(
x_{1}+y_{1},x_{2}+y_{2},x_{3}+y_{3},...\right)  $ of elements of
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$.

Recall the $\mathbb{C}$-bilinear form $\left(  \cdot,\cdot\right)  :F\times
F\rightarrow\mathbb{C}$ defined in Proposition \ref{prop.A.contravariantform}.
Since $F=\widetilde{F}=\mathcal{B}^{\left(  0\right)  }$ (as vector spaces),
this form $\left(  \cdot,\cdot\right)  $ is a $\mathbb{C}$-bilinear form
$\mathcal{B}^{\left(  0\right)  }\times\mathcal{B}^{\left(  0\right)
}\rightarrow\mathbb{C}$. Since the definition of the form did not depend of
the ground ring, we can analogously define an $\mathbf{R}$-bilinear form
$\left(  \cdot,\cdot\right)  :\mathcal{B}_{\mathbf{R}}^{\left(  0\right)
}\times\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }\rightarrow\mathbf{R}$.
The restriction of this latter $\mathbf{R}$-bilinear form $\left(  \cdot
,\cdot\right)  :\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }\times
\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }\rightarrow\mathbf{R}$ to
$\mathcal{B}^{\left(  0\right)  }\times\mathcal{B}^{\left(  0\right)  }$ is
clearly the former $\mathbb{C}$-bilinear form $\left(  \cdot,\cdot\right)
:\mathcal{B}^{\left(  0\right)  }\times\mathcal{B}^{\left(  0\right)
}\rightarrow\mathbb{C}$; therefore we will use the same notation for these two forms.

In the following, elements of $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)
}=\mathbf{R}\left[  x_{1},x_{2},x_{3},...\right]  $ will be considered as
polynomials in the variables $x_{1},x_{2},x_{3},...$ over the ring
$\mathbf{R}$, and not as polynomials in the variables $x_{1},x_{2}%
,x_{3},...,y_{1},y_{2},y_{3},...$ over the field $\mathbb{C}$. Hence, for an
$R\in\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$, the notation $R\left(
0,0,0,...\right)  $ will mean the result of substituting $0$ for the variables
$x_{1},x_{2},x_{3},...$ in $R$ (but the variables $y_{1},y_{2},y_{3},...$ will
stay unchanged!). We will abbreviate $R\left(  0,0,0,...\right)  $ by
$R\left(  0\right)  $.

Every polynomial $R\in\mathcal{B}^{\left(  0\right)  }$ satisfies:%
\begin{equation}
R\left(  0\right)  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  R\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right)  \label{pf.schur.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.1}).} Let $R\in\mathcal{B}^{\left(
0\right)  }$. Thus, $R\in\mathcal{B}^{\left(  0\right)  }=\widetilde{F}$.
\par
Let $p_{0,\mathcal{B}}$ be the canonical projection of the graded space
$\mathcal{B}^{\left(  0\right)  }$ onto its $0$-th homogeneous component
$\mathcal{B}^{\left(  0\right)  }\left[  0\right]  =\mathbb{C}\cdot1$, and let
$p_{0,\mathcal{F}}$ be the canonical projection of the graded space
$\mathcal{F}^{\left(  0\right)  }$ onto its $0$-th homogeneous component
$\mathcal{F}^{\left(  0\right)  }\left[  0\right]  =\mathbb{C}\psi_{0}$. Since
$\sigma_{0}:\mathcal{B}^{\left(  0\right)  }\rightarrow\mathcal{F}^{\left(
0\right)  }$ is a graded homomorphism, $\sigma_{0}$ commutes with the
projections on the $0$-th graded components; in other words, $\sigma_{0}\circ
p_{0,\mathcal{B}}=p_{0,\mathcal{F}}\circ\sigma_{0}$. Now, we know that
$p_{0,\mathcal{B}}\left(  R\right)  =R\left(  0\right)  \cdot1$ (since
$\mathcal{B}=\widetilde{F}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $),
and thus $\left(  \sigma_{0}\circ p_{0,\mathcal{B}}\right)  \left(  R\right)
=\sigma_{0}\left(  \underbrace{p_{0,\mathcal{B}}\left(  R\right)  }_{=R\left(
1\right)  \cdot1}\right)  =\sigma_{0}\left(  R\left(  0\right)  \cdot1\right)
=R\left(  0\right)  \cdot\underbrace{\sigma_{0}\left(  1\right)  }_{=\psi_{0}%
}=R\left(  0\right)  \psi_{0}$.
\par
On the other hand, let $\kappa$ denote the $\left(  v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right)  $-coordinate of $\sigma\left(  R\right)  $ with
respect to the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}^{\left(  0\right)  }$. Then, the
projection of $\sigma\left(  R\right)  $ onto the $0$-th graded component
$\mathcal{F}^{\left(  0\right)  }\left[  0\right]  $ of $\mathcal{F}^{\left(
0\right)  }$ is $\kappa\cdot v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$
(because the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}^{\left(  0\right)  }$ is a graded
basis, and the $0$-th graded component $\mathcal{F}^{\left(  0\right)
}\left[  0\right]  $ of $\mathcal{F}^{\left(  0\right)  }$ is spanned by
$\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  $). In other words,
$p_{0,\mathcal{F}}\left(  \sigma\left(  R\right)  \right)  =\kappa
\cdot\underbrace{v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...}_{=\psi_{0}}%
=\kappa\psi_{0}$. Hence,%
\[
R\left(  0\right)  \psi_{0}=\underbrace{\left(  \sigma_{0}\circ
p_{0,\mathcal{B}}\right)  }_{=p_{0,\mathcal{F}}\circ\sigma_{0}}\left(
R\right)  =\left(  p_{0,\mathcal{F}}\circ\sigma_{0}\right)  \left(  R\right)
=p_{0,\mathcal{F}}\left(  \sigma\left(  R\right)  \right)  =\kappa\psi_{0}.
\]
Thus, $\left(  R\left(  0\right)  -\kappa\right)  \psi_{0}%
=\underbrace{R\left(  0\right)  \psi_{0}}_{=\kappa\psi_{0}}-\kappa\psi
_{0}=\kappa\psi_{0}-\kappa\psi_{0}=0$.
\par
But $\psi_{0}$ is an element of a basis of $\mathcal{F}^{\left(  0\right)  }$
(namely, of the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$). Thus, every scalar $\mu\in\mathbb{C}$ satisfying
$\mu\psi_{0}=0$ must satisfy $\mu=0$. Applying this to $\mu=R\left(  0\right)
-\kappa$, we obtain $R\left(  0\right)  -\kappa=0$ (since $\left(  R\left(
0\right)  -\kappa\right)  \psi_{0}=0$). Thus,%
\[
R\left(  0\right)  =\kappa=\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  R\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right)  .
\]
This proves (\ref{pf.schur.1}).}. Since the proof of (\ref{pf.schur.1})
clearly does not depend on the ground ring, an analogous result holds over the
ring $\mathbf{R}$: Every polynomial $R\in\mathcal{B}_{\mathbf{R}}^{\left(
0\right)  }$ satisfies%
\begin{equation}
R\left(  0\right)  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma_{\mathbf{R}}\left(  R\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right)  \label{pf.schur.1R}%
\end{equation}
\footnote{Of course, ``basis'' means ``$\mathbf{R}$-module basis'' and no
longer ``$\mathbb{C}$-vector space basis'' in this statement.}.

On the other hand, for every polynomial $R\in\mathcal{B}^{\left(  0\right)  }%
$, we can view $R=R\left(  x\right)  $ as an element of $\mathcal{B}%
_{\mathbf{R}}^{\left(  0\right)  }$ (since $\mathcal{B}^{\left(  0\right)
}\subseteq\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$), and this way we
obtain%
\begin{align}
&  \left(  1,\exp\left(  \underbrace{y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...}_{=\sum\limits_{s>0}y_{s}a_{s}}\right)  R\left(  x\right)  \right)
=\left(  1,\exp\left(  \sum\limits_{s>0}y_{s}a_{s}\right)  R\left(  x\right)
\right) \nonumber\\
&  =\left(  1,\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
x_{s}}\right)  R\left(  x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }a_{s}\text{ acts as }\dfrac{\partial}{\partial x_{s}}\text{ on
}\mathcal{B}^{\left(  0\right)  }\text{ for every }s\geq1\right) \nonumber\\
&  =\left(  1,R\left(  x+y\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
x_{s}}\right)  R\left(  x\right)  =R\left(  x+y\right) \\
\text{by Lemma \ref{lem.hirota.newton} (applied to }R\text{, }\left(
x_{1},x_{2},x_{3},...\right)  \text{ and }\mathbf{R}\\
\text{ instead of }P\text{, }\left(  z_{1},z_{2},z_{3},...\right)  \text{ and
}K\text{)}%
\end{array}
\right) \nonumber\\
&  =\left(  R\left(  x+y\right)  \right)  \left(  0\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the analogue of Proposition \ref{prop.A.contravariantform}
\textbf{(b)} for}\\
\text{the ground ring }\mathbf{R}\text{ yields }\left(  1,Q\right)  =Q\left(
0\right)  \text{ for every }Q\in\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \nonumber\\
&  =R\left(  y\right)  \label{pf.schur.usingfermi.1}%
\end{align}
in $\mathbf{R}$.

Recall that the map $\sigma_{\mathbf{R}}$ is defined analogously to $\sigma$
but for the ground ring $\mathbf{R}$ instead of $\mathbb{C}$. Thus,
$\sigma_{\mathbf{R}}\left(  Q\right)  =\sigma\left(  Q\right)  $ for every
$Q\in\mathcal{B}^{\left(  0\right)  }$. Applied to $Q=P\left(  x\right)  $,
this yields $\sigma_{\mathbf{R}}\left(  P\left(  x\right)  \right)
=\sigma\left(  P\left(  x\right)  \right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$ (since $P\left(  x\right)  =\sigma^{-1}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $).

On the other hand, since $\sigma_{\mathbf{R}}:\mathcal{B}_{\mathbf{R}%
}^{\left(  0\right)  }\rightarrow\mathcal{F}_{\mathbf{R}}^{\left(  0\right)
}$ is an $\mathcal{A}_{\mathbf{R}}$-module homomorphism, and since
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ and $\mathcal{F}_{\mathbf{R}%
}^{\left(  0\right)  }$ are two $\mathcal{A}_{\mathbf{R}}$-modules
concentrated in nonpositive degrees, we can apply Proposition
\ref{prop.schur.fermi.welldef} \textbf{(b)} to $\sigma_{\mathbf{R}}$,
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ and $\mathcal{F}_{\mathbf{R}%
}^{\left(  0\right)  }$ instead of $\eta$, $M$ and $N$. As a result, we obtain%
\[
\sigma_{\mathbf{R}}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  \right)  \circ\sigma_{\mathbf{R}}%
\]
as maps from $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ to $\mathcal{F}%
_{\mathbf{R}}^{\left(  0\right)  }$. This easily yields%
\begin{align}
\sigma_{\mathbf{R}}\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  P\left(  x\right)  \right)   &  =\underbrace{\left(
\sigma_{\mathbf{R}}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  \right)  }_{=\left(  \exp\left(  y_{1}a_{1}%
+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \circ\sigma_{\mathbf{R}}}\left(
P\left(  x\right)  \right) \nonumber\\
&  =\left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ\sigma_{\mathbf{R}}\right)  \left(  P\left(  x\right)  \right)
\nonumber\\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot
\underbrace{\sigma_{\mathbf{R}}\left(  P\left(  x\right)  \right)
}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...}\nonumber\\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.schur.usingfermi.4}%
\end{align}


But (\ref{pf.schur.usingfermi.1}) (applied to $R=P$) yields%
\[
\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(
x\right)  \right)  =P\left(  y\right)  ,
\]
so that%
\begin{align*}
&  P\left(  y\right) \\
&  =\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
P\left(  x\right)  \right) \\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(
x\right)  \right)  \left(  0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the analogue of Proposition \ref{prop.A.contravariantform}
\textbf{(b)} for}\\
\text{the ground ring }\mathbf{R}\text{ yields }\left(  1,Q\right)  =Q\left(
0\right)  \text{ for every }Q\in\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma_{\mathbf{R}}\left(  \exp\left(  y_{1}a_{1}%
+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(  x\right)  \right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.1R}), applied to
}R=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(  x\right)
\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of}\\
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.usingfermi.4})}\right)
\\
&  =S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.schur.fermi}}\right) \\
&  =S_{\lambda}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  i_{k}+k\right)  _{k\geq0}=\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)
=\lambda\right)  .
\end{align*}
Substituting $x_{i}$ for $y_{i}$ in this equation, we obtain $P\left(
x\right)  =S_{\lambda}\left(  x\right)  $ (since both $P$ and $S_{\lambda}$
are polynomials in $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $). Thus,%
\[
S_{\lambda}\left(  x\right)  =P\left(  x\right)  =\sigma^{-1}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\]
This proves Theorem \ref{thm.schur}.

\subsubsection{\label{subsubsect.skewschur}Skew Schur polynomials}

Rather than prove Theorem \ref{thm.schur.fermi} directly, let us formulate and
verify a stronger statement which will be in no way harder to prove. First, we
need a definition:

\begin{definition}
\label{def.skewschur}Let $\lambda$ and $\mu$ be two partitions.

\textbf{(a)} We write $\mu\subseteq\lambda$ if every $i\in\left\{
1,2,3,...\right\}  $ satisfies $\lambda_{i}\geq\mu_{i}$, where the partitions
$\lambda$ and $\mu$ have been written in the forms $\lambda=\left(
\lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ and $\mu=\left(  \mu_{1}%
,\mu_{2},\mu_{3},...\right)  $.

\textbf{(b)} We define a polynomial $S_{\lambda\diagup\mu}\left(  x\right)
\in\mathbb{Q}\left[  x_{1},x_{2},x_{3},...\right]  $ as follows: Write
$\lambda$ and $\mu$ in the forms $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ and $\mu=\left(  \mu_{1},\mu_{2},...,\mu
_{m}\right)  $ for some $m\in\mathbb{N}$. Then, let $S_{\lambda\diagup\mu
}\left(  x\right)  $ be the polynomial%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
S_{\lambda_{1}-\mu_{1}}\left(  x\right)  & S_{\lambda_{1}-\mu_{2}+1}\left(
x\right)  & S_{\lambda_{1}-\mu_{3}+2}\left(  x\right)  & ... & S_{\lambda
_{1}-\mu_{m}+m-1}\left(  x\right) \\
S_{\lambda_{2}-\mu_{1}-1}\left(  x\right)  & S_{\lambda_{2}-\mu_{2}}\left(
x\right)  & S_{\lambda_{2}-\mu_{3}+1}\left(  x\right)  & ... & S_{\lambda
_{2}-\mu_{m}+m-2}\left(  x\right) \\
S_{\lambda_{3}-\mu_{1}-2}\left(  x\right)  & S_{\lambda_{3}-\mu_{2}-1}\left(
x\right)  & S_{\lambda_{3}-\mu_{3}}\left(  x\right)  & ... & S_{\lambda
_{3}-\mu_{m}+m-3}\left(  x\right) \\
... & ... & ... & ... & ...\\
S_{\lambda_{m}-\mu_{1}-m+1}\left(  x\right)  & S_{\lambda_{m}-\mu_{2}%
-m+2}\left(  x\right)  & S_{\lambda_{m}-\mu_{3}-m+3}\left(  x\right)  & ... &
S_{\lambda_{m}-\mu_{m}}\left(  x\right)
\end{array}
\right) \\
&  =\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right)  ,
\end{align*}
where $S_{j}$ denotes $0$ if $j<0$. (Note that this does not depend on the
choice of $m$ (that is, increasing $m$ at the cost of padding the partitions
$\lambda$ and $\mu$ with trailing zeroes does not change the value of
$\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right)  $). This is because any nonnegative
integers $m$ and $\ell$, any $m\times m$-matrix $A$, any $m\times\ell$-matrix
$B$ and any upper unitriangular $\ell\times\ell$-matrix $C$ satisfy
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0 & C
\end{array}
\right)  =\det A$.)

We refer to $S_{\lambda\diagup\mu}\left(  x\right)  $ as the \textit{bosonic
Schur polynomial corresponding to the skew partition }$\lambda\diagup\mu$.
\end{definition}

Before we formulate the strengthening of Theorem \ref{thm.schur.fermi}, three remarks:

\begin{remark}
\label{rmk.skewschur.empty}Let $\varnothing$ denote the partition $\left(
0,0,0,...\right)  $. For every partition $\lambda$, we have $\varnothing
\subseteq\lambda$ and $S_{\lambda\diagup\varnothing}\left(  x\right)
=S_{\lambda}\left(  x\right)  $.
\end{remark}

\begin{remark}
\label{rmk.skewschur.0}Let $\lambda$ and $\mu$ be two partitions. Then,
$S_{\lambda\diagup\mu}\left(  x\right)  =0$ unless $\mu\subseteq\lambda$.
\end{remark}

\begin{remark}
\label{rmk.skewschur.infdet}Recall that in Definition \ref{def.infdet}
\textbf{(c)}, we defined the notion of an ``upper almost-unitriangular''
$\mathbb{N}\times\mathbb{N}$-matrix. In the same way, we can define the notion
of an ``upper almost-unitriangular'' $\left\{  1,2,3,...\right\}
\times\left\{  1,2,3,...\right\}  $-matrix.

In Definition \ref{def.infdet} \textbf{(e)}, we defined the determinant of an
upper almost-unitriangular $\mathbb{N}\times\mathbb{N}$-matrix. Analogously,
we can define the determinant of an upper almost-unitriangular $\left\{
1,2,3,...\right\}  \times\left\{  1,2,3,...\right\}  $-matrix.

Let $\lambda=\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ and
$\mu=\left(  \mu_{1},\mu_{2},\mu_{3},...\right)  $ be two partitions. Then,
the $\left\{  1,2,3,...\right\}  \times\left\{  1,2,3,...\right\}  $-matrix
$\left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)  _{\left(
i,j\right)  \in\left\{  1,2,3,...\right\}  ^{2}}$ is upper
almost-unitriangular, and we have%
\begin{align}
S_{\lambda\diagup\mu}\left(  x\right)   &  =\det\left(  \left(  S_{\lambda
_{i}-\mu_{j}+j-i}\left(  x\right)  \right)  _{\left(  i,j\right)  \in\left\{
1,2,3,...\right\}  ^{2}}\right) \label{rmk.skewschur.infdet.1}\\
&  =\det\left(
\begin{array}
[c]{cccc}%
S_{\lambda_{1}-\mu_{1}}\left(  x\right)  & S_{\lambda_{1}-\mu_{2}+1}\left(
x\right)  & S_{\lambda_{1}-\mu_{3}+2}\left(  x\right)  & ...\\
S_{\lambda_{2}-\mu_{1}-1}\left(  x\right)  & S_{\lambda_{2}-\mu_{2}}\left(
x\right)  & S_{\lambda_{2}-\mu_{3}+1}\left(  x\right)  & ...\\
S_{\lambda_{3}-\mu_{1}-2}\left(  x\right)  & S_{\lambda_{3}-\mu_{2}-1}\left(
x\right)  & S_{\lambda_{3}-\mu_{3}}\left(  x\right)  & ...\\
... & ... & ... & ...
\end{array}
\right)  .\nonumber
\end{align}

\end{remark}

All of the above three remarks follow easily from Definition
\ref{def.skewschur}.

Now, let us finally give the promised strengthening of Theorem
\ref{thm.schur.fermi}:

\begin{theorem}
\label{thm.schur.fermi.skew}Let $\mathbf{R}$ be a commutative $\mathbb{Q}%
$-algebra. Let $y_{1},y_{2},y_{3},...$ be some elements of $\mathbf{R}$.
Denote by $y$ the family $\left(  y_{1},y_{2},y_{3},...\right)  $. Let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. Recall that
$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ is a well-defined
element of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ due to Proposition
\ref{prop.schur.fermi.welldef} \textbf{(c)}. Recall also that $\left(
j_{k}+k\right)  _{k\geq0}$ is a partition for every $0$-degression $\left(
j_{0},j_{1},j_{2},...\right)  $ (this follows from Proposition
\ref{prop.glinf.wedge.grading}, applied to $0$ and $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ instead of $m$ and $\left(  i_{0},i_{1},i_{2},...\right)
$). In particular, $\left(  i_{k}+k\right)  _{k\geq0}$ is a partition.

We have%
\begin{align}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge.... \label{thm.schur.fermi.skew.eq}%
\end{align}
(Note that the sum on the right hand side of (\ref{thm.schur.fermi.skew.eq})
is a finite sum, since only finitely many $0$-degressions $\left(  j_{0}%
,j_{1},j_{2},...\right)  $ satisfy $\left(  j_{k}+k\right)  _{k\geq0}%
\subseteq\left(  i_{k}+k\right)  _{k\geq0}$.)
\end{theorem}

Before we prove this, let us see how this yields Theorem \ref{thm.schur.fermi}:

\textit{Proof of Theorem \ref{thm.schur.fermi} using Theorem
\ref{thm.schur.fermi.skew}.} Remark \ref{rmk.skewschur.empty} (applied to
$\lambda=\left(  i_{k}+k\right)  _{k\geq0}$) yields $\varnothing
\subseteq\left(  i_{k}+k\right)  _{k\geq0}$ and $S_{\left(  i_{k}+k\right)
_{k\geq0}\diagup\varnothing}\left(  x\right)  =S_{\left(  i_{k}+k\right)
_{k\geq0}}\left(  x\right)  $. By substituting $y$ for $x$ in the equality
$S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\varnothing}\left(  x\right)
=S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  x\right)  $, we conclude
$S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\varnothing}\left(  y\right)
=S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)  $.

Theorem \ref{thm.schur.fermi.skew} yields that (\ref{thm.schur.fermi.skew.eq}) holds.

On the other hand, every $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)
$ satisfying $\left(  j_{k}+k\right)  _{k\geq0}\not \subseteq \left(
i_{k}+k\right)  _{k\geq0}$ must satisfy%
\begin{equation}
S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}%
}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...=0
\label{pf.schur.fermi.notcontained}%
\end{equation}
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  j_{0},j_{1},j_{2},...\right)  $
be a $0$-degression satisfying $\left(  j_{k}+k\right)  _{k\geq0}%
\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}$. We know that $\left(
i_{k}+k\right)  _{k\geq0}$ and $\left(  j_{k}+k\right)  _{k\geq0}$ are
partitions. Thus, Remark \ref{rmk.skewschur.0} (applied to $\lambda=\left(
i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(  j_{k}+k\right)  _{k\geq0}$) yields
that $S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  x\right)  =0$ unless $\left(  j_{k}+k\right)  _{k\geq
0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$. Since we don't have $\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$ (because
by assumption, we have $\left(  j_{k}+k\right)  _{k\geq0}\not \subseteq
\left(  i_{k}+k\right)  _{k\geq0}$), we thus know that $S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
x\right)  =0$. Substituting $y$ for $x$ in this equation, we obtain
$S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}%
}\left(  y\right)  =0$, so that $S_{\left(  i_{k}+k\right)  _{k\geq0}%
\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...=0$, qed.}. Hence, each of the
addends of the infinite sum $\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}}}S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$ equals
$0$. Thus, the infinite sum $\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}}}S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$ is
well-defined and equals $0$. We thus have%
\begin{equation}
0=\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\not \subseteq \left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge.... \label{pf.schur.fermi.onlysubdiagrams}%
\end{equation}
Adding this equality to (\ref{thm.schur.fermi.skew.eq}), we obtain%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}}}S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\\
&  =\sum\limits_{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a }%
0\text{-degression}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
Hence, the $\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
$-coordinate of $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\cdot\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ with
respect to the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$
equals
\begin{align*}
S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  -k+k\right)  _{k\geq0}%
}\left(  y\right)   &  =S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup
\varnothing}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
-k+k\right)  _{k\geq0}=\left(  0\right)  _{k\geq0}=\left(  0,0,0,...\right)
=\varnothing\right) \\
&  =S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)  .
\end{align*}
This proves Theorem \ref{thm.schur.fermi} using Theorem
\ref{thm.schur.fermi.skew}.

\subsubsection{\label{subsubsect.schur2}Proof of Theorem
\ref{thm.schur.fermi.skew} using
\texorpdfstring{$\operatorname*{U}\left(  \infty\right)  $}{U-infinity}}

One final easy lemma:

\begin{lemma}
\label{lem.schur.fermi.skew.toeplitzmatrix}For every $n\in\mathbb{Z}$, let
$c_{n}$ be an element of $\mathbb{C}$. Assume that $c_{n}=0$ for every
negative $n\in\mathbb{Z}$. Consider the shift operator $T:V\rightarrow V$ of
Definition \ref{def.shiftoperator}. Then, $\sum\limits_{k\geq0}c_{k}%
T^{k}=\left(  c_{j-i}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$.
\end{lemma}

\begin{vershort}
The proof of this lemma is immediate from the definition of $T$.
\end{vershort}

\begin{verlong}
\textit{Proof of Lemma \ref{lem.schur.fermi.skew.toeplitzmatrix}.} By
Definition \ref{def.shiftoperator}, we have $Tv_{i+1}=v_{i}$ for every
$i\in\mathbb{Z}$. Substituting $i-1$ for $i$ in this equality, we obtain:
$Tv_{i}=v_{i-1}$ for every $i\in\mathbb{Z}$. Using this, we can readily find
that%
\begin{equation}
T^{k}v_{i}=v_{i-k}\ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\text{ and
}i\in\mathbb{Z}. \label{pf.schur.fermi.skew.toeplitzmatrix.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}):} We
will prove (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) by induction over $k$.
\par
\textit{Induction base:} For $k=0$, we have $\underbrace{T^{k}}_{=T^{0}%
=\operatorname*{id}}v_{i}=\operatorname*{id}v_{i}=v_{i}=v_{i-0}=v_{i-k}$
(since $0=k$) for every $i\in\mathbb{Z}$. In other words,
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) is true for $k=0$. This completes
the induction base.
\par
\textit{Induction step:} Let $\ell\in\mathbb{N}$. Assume that
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) holds for $k=\ell$. We must prove
that (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) also holds for $k=\ell+1$.
\par
Let $i\in\mathbb{Z}$. Since (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) holds
for $k=\ell$, we can apply (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) to
$\ell$ and $i-1$ instead of $k$ and $i$, and obtain $T^{\ell}v_{i-1}%
=v_{i-1-\ell}$. Now, $T^{\ell+1}=T^{\ell}T$, so that $T^{\ell+1}v_{i}=T^{\ell
}\underbrace{Tv_{i}}_{=v_{i-1}}=T^{\ell}v_{i-1}=v_{i-1-\ell}=v_{i-\left(
\ell+1\right)  }$. Now, forget that we fixed $i$. We thus have proven that
$T^{\ell+1}v_{i}=v_{i-\left(  \ell+1\right)  }$ for every $i\in\mathbb{Z}$.
Thus, (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) holds for $k=\ell+1$. This
completes the induction step. The induction proof of
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) is thus finished.} Hence, every
$k\in\mathbb{N}$ and $i\in\mathbb{Z}$ satisfy%
\begin{align*}
&  \left(  \text{the }i\text{-th column of the matrix }\left(  \delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}\right) \\
&  =\sum\limits_{u\in\mathbb{Z}}\delta_{i-u,k}v_{u}=\sum
\limits_{\substack{u\in\mathbb{Z};\\u\neq i-k}}\underbrace{\delta_{i-u,k}%
}_{\substack{=0\\\text{(since }i-u\neq k\\\text{(since }u\neq i-k\text{))}%
}}v_{u}+\underbrace{\delta_{i-\left(  i-k\right)  ,k}}%
_{\substack{=1\\\text{(since }i-\left(  i-k\right)  =k\text{)}}}v_{i-k}\\
&  =\underbrace{\sum\limits_{\substack{u\in\mathbb{Z};\\u\neq i-k}}0v_{u}%
}_{=0}+v_{i-k}=v_{i-k}=T^{k}v_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1})}\right) \\
&  =\left(  \text{the }i\text{-th column of the matrix }T^{k}\right)  .
\end{align*}
Thus, every $k\in\mathbb{N}$ satisfies $\left(  \delta_{v-u,k}\right)
_{\left(  u,v\right)  \in\mathbb{Z}^{2}}=T^{k}$. Hence, $\sum\limits_{k\geq
0}c_{k}\underbrace{\left(  \delta_{v-u,k}\right)  _{\left(  u,v\right)
\in\mathbb{Z}^{2}}}_{=T^{k}}=\sum\limits_{k\geq0}c_{k}T^{k}$, so that%
\[
\sum\limits_{k\geq0}c_{k}T^{k}=\sum\limits_{k\geq0}c_{k}\left(  \delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
\sum\limits_{k\geq0}c_{k}\delta_{v-u,k}\right)  _{\left(  u,v\right)
\in\mathbb{Z}^{2}}.
\]
But recall that $c_{n}=0$ for every negative $n\in\mathbb{Z}$. In other words,
$c_{k}=0$ for every negative $k\in\mathbb{Z}$. Hence, $c_{k}\delta_{v-u,k}=0$
for every negative $k\in\mathbb{Z}$ and every $\left(  u,v\right)
\in\mathbb{Z}^{2}$. Thus, the sum $\sum\limits_{k<0}c_{k}\delta_{v-u,k}$ is
well-defined and equals $0$ for every $\left(  u,v\right)  \in\mathbb{Z}^{2}$.
Hence, in $\overline{\mathfrak{a}_{\infty}}$, we have $\left(  \sum
\limits_{k<0}c_{k}\delta_{v-u,k}\right)  _{\left(  u,v\right)  \in
\mathbb{Z}^{2}}=\left(  0\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=0$.
Now,%
\begin{align*}
\sum\limits_{k\geq0}c_{k}T^{k}  &  =\underbrace{\sum\limits_{k\geq0}c_{k}%
T^{k}}_{=\left(  \sum\limits_{k\geq0}c_{k}\delta_{v-u,k}\right)  _{\left(
u,v\right)  \in\mathbb{Z}^{2}}}+\underbrace{0}_{=\left(  \sum\limits_{k<0}%
c_{k}\delta_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}}\\
&  =\left(  \sum\limits_{k\geq0}c_{k}\delta_{v-u,k}\right)  _{\left(
u,v\right)  \in\mathbb{Z}^{2}}+\left(  \sum\limits_{k<0}c_{k}\delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
\sum\limits_{k\geq0}c_{k}\delta_{v-u,k}+\sum\limits_{k<0}c_{k}\delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}.
\end{align*}
But since every $\left(  u,v\right)  \in\mathbb{Z}^{2}$ satisfies%
\begin{align*}
\sum\limits_{k\geq0}c_{k}\delta_{v-u,k}+\sum\limits_{k<0}c_{k}\delta_{v-u,k}
&  =\sum\limits_{k\in\mathbb{Z}}c_{k}\delta_{v-u,k}=\sum
\limits_{\substack{k\in\mathbb{Z};\\k\neq v-u}}c_{k}\underbrace{\delta
_{v-u,k}}_{\substack{=0\\\text{(since }v-u\neq k\text{)}}}+c_{v-u}%
\underbrace{\delta_{v-u,v-u}}_{=1}\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k\neq v-u}}c_{k}%
0}_{=0}+c_{v-u}=c_{v-u},
\end{align*}
this rewrites as $\sum\limits_{k\geq0}c_{k}T^{k}=\left(  \underbrace{\sum
\limits_{k\geq0}c_{k}\delta_{v-u,k}+\sum\limits_{k<0}c_{k}\delta_{v-u,k}%
}_{=c_{v-u}}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
c_{v-u}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
c_{j-i}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ (here, we renamed
$\left(  u,v\right)  $ as $\left(  i,j\right)  $). This proves Lemma
\ref{lem.schur.fermi.skew.toeplitzmatrix}.
\end{verlong}

We now give a proof of Theorem \ref{thm.schur.fermi.skew} using the actions
$\rho:\mathfrak{u}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ and $\varrho:\operatorname*{U}\left(
\infty\right)  \rightarrow\operatorname*{GL}\left(  \wedge^{\dfrac{\infty}%
{2},m}V\right)  $ introduced in Subsection \ref{subsubsect.Uinf} and their properties.

\textit{First proof of Theorem \ref{thm.schur.fermi.skew}.} In order to
simplify notation, we assume that $\mathbf{R}=\mathbb{C}$. (All the arguments
that we will make in the following are independent of the ground ring, as long
as the ground ring is a commutative $\mathbb{Q}$-algebra. Therefore, we are
actually allowed to assume that $\mathbf{R}=\mathbb{C}$.) Since we assumed
that $\mathbf{R}=\mathbb{C}$, we have $\mathcal{A}_{\mathbf{R}}=\mathcal{A}$
and $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }=\mathcal{F}^{\left(
0\right)  }$.

Now consider the shift operator $T:V\rightarrow V$ of Definition
\ref{def.shiftoperator}. As a matrix in $\overline{\mathfrak{a}_{\infty}}$,
this $T$ is the matrix which has $1$'s on the diagonal right above the main
one, and $0$'s everywhere else. The embedding $\mathcal{A}\rightarrow
\mathfrak{a}_{\infty}$ that we are using to define the action of $\mathcal{A}$
on $\mathcal{F}^{\left(  0\right)  }$ sends $a_{j}$ to $T^{j}$ for every
$j\in\mathbb{Z}$. Thus, every positive integer $j$ satisfies%
\begin{align*}
a_{j}\mid_{\mathcal{F}^{\left(  0\right)  }}  &  =T^{j}\mid_{\mathcal{F}%
^{\left(  0\right)  }}=\widehat{\rho}\left(  T^{j}\right)  =\rho\left(
T^{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.Uinf.rhorhohat},
applied to }m=0\text{ and }a=T^{j}\text{ (since }T^{j}\in\mathfrak{u}_{\infty
}\cap\overline{\mathfrak{a}_{\infty}}\text{)}\right)  .
\end{align*}
Since $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...=\sum\limits_{j\geq1}y_{j}a_{j}$,
we have%
\begin{align*}
\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \mid_{\mathcal{F}%
^{\left(  0\right)  }}  &  =\left(  \sum\limits_{j\geq1}y_{j}a_{j}\right)
\mid_{\mathcal{F}^{\left(  0\right)  }}=\sum\limits_{j\geq1}y_{j}%
\underbrace{\left(  a_{j}\mid_{\mathcal{F}^{\left(  0\right)  }}\right)
}_{=\rho\left(  T^{j}\right)  }=\sum\limits_{j\geq1}y_{j}\rho\left(
T^{j}\right) \\
&  =\rho\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  .
\end{align*}
Here, we have used the fact that $\sum\limits_{j\geq1}y_{j}T^{j}%
\in\mathfrak{u}_{\infty}$ (this ensures that $\rho\left(  \sum\limits_{j\geq
1}y_{j}T^{j}\right)  $ is well-defined).

On the other hand, substituting $y$ for $x$ in (\ref{def.schur.sk.genfun}), we
obtain%
\[
\sum\limits_{k\geq0}S_{k}\left(  y\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}y_{i}z^{i}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }%
\mathbb{C}\left[  \left[  z\right]  \right]  .
\]
Substituting $T$ for $z$ in this equality, we obtain $\sum\limits_{k\geq
0}S_{k}\left(  y\right)  T^{k}=\exp\left(  \sum\limits_{i\geq1}y_{i}%
T^{i}\right)  $. Thus,%
\begin{equation}
\exp\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  =\exp\left(  \sum
\limits_{i\geq1}y_{i}T^{i}\right)  =\sum\limits_{k\geq0}S_{k}\left(  y\right)
T^{k}=\left(  S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}} \label{pf.schur.fermi.skew.1}%
\end{equation}
(by Lemma \ref{lem.schur.fermi.skew.toeplitzmatrix}, applied to $c_{n}%
=S_{n}\left(  y\right)  $ (since $S_{n}\left(  y\right)  =0$ for every
negative $n\in\mathbb{Z}$)).

Now,%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\mid_{\mathcal{F}^{\left(  0\right)  }}\\
&  =\exp\underbrace{\left(  \left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  \mid_{\mathcal{F}^{\left(  0\right)  }}\right)  }_{=\rho\left(
\sum\limits_{j\geq1}y_{j}T^{j}\right)  }\\
&  =\exp\left(  \rho\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right)
=\varrho\left(  \exp\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Theorem \ref{thm.Uinf.rhoRho} (applied to }a=\sum\limits_{j\geq
1}y_{j}T^{j}\text{) yields}\\
\varrho\left(  \exp\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right)
=\exp\left(  \rho\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right)
\end{array}
\right) \\
&  =\varrho\left(  \left(  S_{j-i}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.1})}\right)  .
\end{align*}
Denote the matrix $\left(  S_{j-i}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\in\operatorname*{U}\left(  \infty\right)  $ by
$A$. Thus, we have%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\mid_{\mathcal{F}^{\left(  0\right)  }}=\varrho\left(  \underbrace{\left(
S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
}_{=A}\right)  =\varrho\left(  A\right)  .
\]
Hence,%
\begin{align}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{ is a }0\text{-degression}}\det\left(  \left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge
...\label{pf.schur.fermi.skew.3}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.Uinf.det}, applied to
}m=0\right)  .\nonumber
\end{align}


But a close look at the matrix $\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0}%
,i_{1},i_{2},...}\right)  ^{T}$ proves that%
\begin{equation}
\det\left(  \left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)
^{T}\right)  =S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }0\text{-degression }\left(  j_{0},j_{1},j_{2},...\right)
\label{pf.schur.fermi.skew.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.fermi.skew.2}):} Let $\left(
j_{0},j_{1},j_{2},...\right)  $ be a $0$-degression. Since $A=\left(
S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$,
we have $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}=\left(
S_{i_{v}-j_{u}}\left(  y\right)  \right)  _{\left(  u,v\right)  \in
\mathbb{N}^{2}}$, so that $\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0}%
,i_{1},i_{2},...}\right)  ^{T}=\left(  S_{i_{v}-j_{u}}\left(  y\right)
\right)  _{\left(  v,u\right)  \in\mathbb{N}^{2}}$. But define two partitions
$\lambda$ and $\mu$ by $\lambda=\left(  i_{k}+k\right)  _{k\geq0}$ and
$\mu=\left(  j_{k}+k\right)  _{k\geq0}$. Write the partitions $\lambda$ and
$\mu$ in the forms $\lambda=\left(  \lambda_{1},\lambda_{2},\lambda
_{3},...\right)  $ and $\mu=\left(  \mu_{1},\mu_{2},\mu_{3},...\right)  $.
Then, $\lambda_{v}=i_{v-1}+\left(  v-1\right)  $ for every $v\in\left\{
1,2,3,...\right\}  $, and $\mu_{u}=j_{u-1}+\left(  u-1\right)  $ for every
$u\in\left\{  1,2,3,...\right\}  $. Thus, for every $\left(  u,v\right)
\in\left\{  1,2,3,...\right\}  ^{2}$, we have%
\begin{equation}
\underbrace{\lambda_{v}}_{=i_{v-1}+\left(  v-1\right)  }-\underbrace{\mu_{u}%
}_{=j_{u-1}+\left(  u-1\right)  }+u-v=\left(  i_{v-1}+\left(  v-1\right)
\right)  -\left(  j_{u-1}+\left(  u-1\right)  \right)  +u-v=i_{v-1}-j_{u-1}.
\label{pf.schur.fermi.skew.2.pf.1}%
\end{equation}
But (\ref{rmk.skewschur.infdet.1}) yields $S_{\lambda\diagup\mu}\left(
x\right)  =\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,3,...\right\}  ^{2}}\right)  $.
Substituting $y$ for $x$ in this equality, we obtain%
\begin{align*}
S_{\lambda\diagup\mu}\left(  y\right)   &  =\det\left(  \left(  S_{\lambda
_{i}-\mu_{j}+j-i}\left(  y\right)  \right)  _{\left(  i,j\right)  \in\left\{
1,2,3,...\right\}  ^{2}}\right)  =\det\left(  \left(  S_{\lambda_{v}-\mu
_{u}+u-v}\left(  y\right)  \right)  _{\left(  v,u\right)  \in\left\{
1,2,3,...\right\}  ^{2}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(  v,u\right)
\text{ for }\left(  i,j\right)  \right) \\
&  =\det\left(  \left(  S_{i_{v-1}-j_{u-1}}\left(  y\right)  \right)
_{\left(  v,u\right)  \in\left\{  1,2,3,...\right\}  ^{2}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.fermi.skew.2.pf.1}%
)}\right) \\
&  =\det\left(  \underbrace{\left(  S_{i_{v}-j_{u}}\left(  y\right)  \right)
_{\left(  v,u\right)  \in\mathbb{N}^{2}}}_{=\left(  A_{j_{0},j_{1},j_{2}%
,...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(  v,u\right)
\text{ for }\left(  v-1,u-1\right)  \right) \\
&  =\det\left(  \left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2}%
,...}\right)  ^{T}\right)  .
\end{align*}
Since $\lambda=\left(  i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(
j_{k}+k\right)  _{k\geq0}$, this rewrites as $S_{\left(  i_{k}+k\right)
_{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)
=\det\left(  \left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)
^{T}\right)  $. This proves (\ref{pf.schur.fermi.skew.2}).}.

Now, (\ref{pf.schur.fermi.skew.3}) becomes%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression}}\underbrace{\det\left(  \left(  A_{j_{0},j_{1},j_{2}%
,...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)  }_{\substack{=S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \\\text{(by (\ref{pf.schur.fermi.skew.2}))}}}v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...\\
&  =\sum\limits_{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}%
}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq
0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge
...}_{\substack{=0\\\text{(by (\ref{pf.schur.fermi.onlysubdiagrams}))}}}\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
This proves Theorem \ref{thm.schur.fermi.skew}.

We can now combine the above to obtain a proof of Theorem \ref{thm.schur}:

\textit{Second proof of Theorem \ref{thm.schur}.} We have proven Theorem
\ref{thm.schur.fermi} using Theorem \ref{thm.schur.fermi.skew}. Since we know
that Theorem \ref{thm.schur.fermi.skew} holds, this yields that Theorem
\ref{thm.schur.fermi} holds. This, in turn, entails that Theorem
\ref{thm.schur} holds (since we have proven Theorem \ref{thm.schur} using
Theorem \ref{thm.schur.fermi}).

\subsubsection{\label{subsubsect.schur2.finitary}``Finitary'' proof of Theorem
\ref{thm.schur.fermi.skew}}

The above second proof of Theorem \ref{thm.schur} had the drawback of
requiring a slew of new notions (those of $\mathfrak{u}_{\infty}$, of
$\operatorname*{U}\left(  \infty\right)  $, of the determinant of an almost
upper-triangular matrix etc.) and of their properties (Proposition
\ref{prop.uinf.Vhatwedge.welldef}, Remark \ref{rmk.Uinf.det}, Theorem
\ref{thm.Uinf.rhoRho} and others). We will now give a proof of Theorem
\ref{thm.schur} which is more or less equivalent to the second proof of
Theorem \ref{thm.schur} shown above, but avoiding these new notions. It will
eschew using infinite matrices other than those in $\overline{\mathfrak{a}%
_{\infty}}$, and instead work with finite objects most of the time.

Since we already know how to derive Theorem \ref{thm.schur} from Theorem
\ref{thm.schur.fermi.skew}, we only need to verify Theorem
\ref{thm.schur.fermi.skew}.

Let us first introduce some finite-dimensional subspaces of the vector space
$V$:

\begin{definition}
\label{def.finitary.Valphabeta}Let $\alpha$ and $\beta$ be integers such that
$\alpha-1\leq\beta$.

\textbf{(a)} Then, $V_{\left]  \alpha,\beta\right]  }$ will denote the vector
subspace of $V$ spanned by the vectors $v_{\alpha+1}$, $v_{\alpha+2}$, $...$,
$v_{\beta}$. It is clear that $\left(  v_{\alpha+1},v_{\alpha+2},...,v_{\beta
}\right)  $ is a basis of this vector space $V_{\left]  \alpha,\beta\right]
}$, so that $\dim\left(  V_{\left]  \alpha,\beta\right]  }\right)
=\beta-\alpha$.

\textbf{(b)} Let $T_{\left]  \alpha,\beta\right]  }$ be the endomorphism of
the vector space $V_{\left]  \alpha,\beta\right]  }$ defined by%
\[
\left(  T_{\left]  \alpha,\beta\right]  }\left(  v_{i}\right)  =\left\{
\begin{array}
[c]{l}%
v_{i-1},\ \ \ \ \ \ \ \ \ \ \text{if }i>\alpha+1;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i=\alpha+1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  \right)  .
\]


\textbf{(c)} We let $\mathcal{A}_{+}$ be the Lie subalgebra $\left\langle
a_{1},a_{2},a_{3},...\right\rangle $ of $\mathcal{A}$. This Lie subalgebra
$\mathcal{A}_{+}$ is abelian. We define an $\mathcal{A}_{+}$-module structure
on the vector space $V_{\left]  \alpha,\beta\right]  }$ by letting $a_{i}$ act
as $T_{\left]  \alpha,\beta\right]  }^{i}$ for every positive integer $i$.
(This is well-defined, since the powers of $T_{\left]  \alpha,\beta\right]  }$
commute, just as the elements of $\mathcal{A}_{+}$.) Thus, for every $\ell
\in\mathbb{N}$, the $\ell$-th exterior power $\wedge^{\ell}\left(  V_{\left]
\alpha,\beta\right]  }\right)  $ is canonically equipped with an
$\mathcal{A}_{+}$-module structure.

\textbf{(d)} For every $\ell\in\mathbb{N}$, let $R_{\ell,\left]  \alpha
,\beta\right]  }:\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\wedge^{\dfrac{\infty}{2},\alpha+\ell}V$ be the linear
map defined by%
\[
\left(
\begin{array}
[c]{r}%
R_{\ell,\left]  \alpha,\beta\right]  }\left(  b_{1}\wedge b_{2}\wedge...\wedge
b_{\ell}\right)  =b_{1}\wedge b_{2}\wedge...\wedge b_{\ell}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\
\ \ \ \ \ \ \ \ \ \ \text{for any }b_{1},b_{2},...,b_{\ell}\in V_{\left]
\alpha,\beta\right]  }%
\end{array}
\right)  .
\]

\end{definition}

\begin{remark}
\label{rmk.finitary.Valphabeta}Let $\alpha$ and $\beta$ be integers such that
$\alpha-1\leq\beta$.

\textbf{(a)} The $\left(  \beta-\alpha\right)  $-tuple $\left(  v_{\beta
},v_{\beta-1},...,v_{\alpha+1}\right)  $ is a basis of this vector space
$V_{\left]  \alpha,\beta\right]  }$. With respect to this basis, the
endomorphism $T_{\left]  \alpha,\beta\right]  }$ of $V_{\left]  \alpha
,\beta\right]  }$ is represented by the $\left(  \beta-\alpha\right)
\times\left(  \beta-\alpha\right)  $ matrix $\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 0 & ... & 0 & 0\\
1 & 0 & 0 & ... & 0 & 0\\
0 & 1 & 0 & ... & 0 & 0\\
0 & 0 & 1 & ... & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & 0
\end{array}
\right)  $.

\textbf{(b)} We have $T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}=0$.

\textbf{(c)} For every sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ of
elements of $\mathbb{C}$, the endomorphism $\sum\limits_{i=1}^{\infty}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}$ of $V_{\left]  \alpha
,\beta\right]  }$ is well-defined and nilpotent.

\textbf{(d)} For every sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ of
elements of $\mathbb{C}$, the endomorphism $\exp\left(  \sum\limits_{i=1}%
^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $V_{\left]
\alpha,\beta\right]  }$ is well-defined.

\textbf{(e)} For every sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ of
elements of $\mathbb{C}$, the endomorphism $\exp\left(  y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ is well-defined.

\textbf{(f)} Every $j\in\mathbb{N}$ satisfies%
\begin{equation}
T_{\left]  \alpha,\beta\right]  }^{j}v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-j},\ \ \ \ \ \ \ \ \ \ \text{if }u-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-j\leq\alpha
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  . \label{pf.finitary.Valphabeta.R.Tabjvu}%
\end{equation}


\textbf{(g)} For every $n\in\mathbb{Z}$, let $c_{n}$ be an element of
$\mathbb{C}$. Assume that $c_{n}=0$ for every negative $n\in\mathbb{Z}$. Then,
the sum $\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]  }^{k}$ is a
well-defined endomorphism of $V_{\left]  \alpha,\beta\right]  }$, and the
matrix representing this endomorphism with respect to the basis $\left(
v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ of $V_{\left]  \alpha
,\beta\right]  }$ is $\left(  c_{i-j}\right)  _{\left(  i,j\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$.
\end{remark}

\begin{vershort}
\textit{Proof of Remark \ref{rmk.finitary.Valphabeta}.} Parts \textbf{(a)}
through \textbf{(f)} of Remark \ref{rmk.finitary.Valphabeta} are trivial, and
part \textbf{(g)} is just the finitary analogue of Lemma
\ref{lem.schur.fermi.skew.toeplitzmatrix} and proven in the same way.
\end{vershort}

\begin{verlong}
\textit{Proof of Remark \ref{rmk.finitary.Valphabeta}.} \textbf{(a)} We know
that $\left(  v_{\alpha+1},v_{\alpha+2},...,v_{\beta}\right)  $ is a basis of
this vector space $V_{\left]  \alpha,\beta\right]  }$. Thus, $\left(
v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ also is a basis of this
vector space $V_{\left]  \alpha,\beta\right]  }$. With respect to this basis,
the endomorphism $T_{\left]  \alpha,\beta\right]  }$ of $V_{\left]
\alpha,\beta\right]  }$ is represented by the $\left(  \beta-\alpha\right)
\times\left(  \beta-\alpha\right)  $ matrix $\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 0 & ... & 0 & 0\\
1 & 0 & 0 & ... & 0 & 0\\
0 & 1 & 0 & ... & 0 & 0\\
0 & 0 & 1 & ... & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & 0
\end{array}
\right)  $ (this follows readily from the definition of $T_{\left]
\alpha,\beta\right]  }$). This proves Remark \ref{rmk.finitary.Valphabeta}
\textbf{(a)}.

\textbf{(b)} We know (from Remark \ref{rmk.finitary.Valphabeta} \textbf{(a)})
that the endomorphism $T_{\left]  \alpha,\beta\right]  }$ of $V_{\left]
\alpha,\beta\right]  }$ is represented by the $\left(  \beta-\alpha\right)
\times\left(  \beta-\alpha\right)  $ matrix $\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 0 & ... & 0 & 0\\
1 & 0 & 0 & ... & 0 & 0\\
0 & 1 & 0 & ... & 0 & 0\\
0 & 0 & 1 & ... & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & 0
\end{array}
\right)  $. This matrix is a strictly lower-triangular $\left(  \beta
-\alpha\right)  \times\left(  \beta-\alpha\right)  $ matrix, and therefore its
$\left(  \beta-\alpha\right)  $-th power is $0$\ \ \ \ \footnote{Here, we are
using the following simple fact from linear algebra: If $n$ is a nonnegative
integer, and $M$ is a strictly lower-triangular $n\times n$-matrix, then
$M^{n}=0$.}. That is, $T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}=0$.
This proves Remark \ref{rmk.finitary.Valphabeta} \textbf{(b)}.

\textbf{(c)} Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of
elements of $\mathbb{C}$. We have $T_{\left]  \alpha,\beta\right]  }%
^{\beta-\alpha}=0$. Thus, $T_{\left]  \alpha,\beta\right]  }$ is nilpotent, so
that the endomorphism $\sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}$ is well-defined.

We have $\sum\limits_{i=1}^{\infty}y_{i}\underbrace{T_{\left]  \alpha
,\beta\right]  }^{i}}_{=T_{\left]  \alpha,\beta\right]  }^{i-1}\circ
T_{\left]  \alpha,\beta\right]  }}=\left(  \sum\limits_{i=1}^{\infty}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\right)  \circ T_{\left]
\alpha,\beta\right]  }$ (here, the endomorphism $\sum\limits_{i=1}^{\infty
}y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}$ is well-defined, since
$T_{\left]  \alpha,\beta\right]  }$ is nilpotent), so that%
\begin{align*}
\left(  \sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  ^{\beta-\alpha}  &  =\left(  \left(  \sum\limits_{i=1}^{\infty
}y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\right)  \circ T_{\left]
\alpha,\beta\right]  }\right)  ^{\beta-\alpha}=\left(  \sum\limits_{i=1}%
^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\right)  ^{\beta-\alpha
}\circ\underbrace{T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}}_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\sum\limits_{i=1}^{\infty}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\text{ and }T_{\left]
\alpha,\beta\right]  }\text{ commute}\right) \\
&  =0,
\end{align*}
so that the endomorphism $\sum\limits_{i=1}^{\infty}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}$ is nilpotent. This proves Remark
\ref{rmk.finitary.Valphabeta} \textbf{(c)}.

\textbf{(d)} Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of
elements of $\mathbb{C}$. By Remark \ref{rmk.finitary.Valphabeta}
\textbf{(c)}, the endomorphism $\sum\limits_{i=1}^{\infty}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}$ is nilpotent. Thus, the endomorphism $\exp\left(
\sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$ is well-defined. This proves Remark
\ref{rmk.finitary.Valphabeta} \textbf{(d)}.

\textbf{(e)} Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of
elements of $\mathbb{C}$. We know that the endomorphism $\exp\left(
\sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $
is well-defined. Since $T_{\left]  \alpha,\beta\right]  }^{i}$ is the action
of $a_{i}$ on $V_{\left]  \alpha,\beta\right]  }$ for every positive integer
$i$, this endomorphism rewrites as
\[
\exp\left(  \sum\limits_{i=1}^{\infty}y_{i}\underbrace{T_{\left]  \alpha
,\beta\right]  }^{i}}_{=a_{i}}\right)  =\exp\left(  \underbrace{\sum
\limits_{i=1}^{\infty}y_{i}a_{i}}_{=y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...}\right)  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  .
\]
Hence, the endomorphism $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ is well-defined.
This proves Remark \ref{rmk.finitary.Valphabeta} \textbf{(e)}.

\textbf{(f)} We will prove (\ref{pf.finitary.Valphabeta.R.Tabjvu}) by
induction over $j$:

\textit{Induction base:} For every $u\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  $, we have $\underbrace{T_{\left]  \alpha,\beta\right]
}^{0}}_{=\operatorname*{id}}v_{u}=\operatorname*{id}\left(  v_{u}\right)
=v_{u}$ and $\left\{
\begin{array}
[c]{l}%
v_{u-0},\ \ \ \ \ \ \ \ \ \ \text{if }u-0>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-0\leq\alpha
\end{array}
\right.  =\left\{
\begin{array}
[c]{l}%
v_{u},\ \ \ \ \ \ \ \ \ \ \text{if }u>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u\leq\alpha
\end{array}
\right.  =v_{u}$ (since $u>\alpha$). Thus, for every $u\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $, we have $T_{\left]  \alpha
,\beta\right]  }^{0}v_{u}=v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-0},\ \ \ \ \ \ \ \ \ \ \text{if }u-0>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-0\leq\alpha
\end{array}
\right.  $. This proves (\ref{pf.finitary.Valphabeta.R.Tabjvu}) for $j=0$.
This completes the induction base.

\textit{Induction step:} Let $J\in\mathbb{N}$. Assume that
(\ref{pf.finitary.Valphabeta.R.Tabjvu}) holds for $j=J$. We now must prove
that (\ref{pf.finitary.Valphabeta.R.Tabjvu}) also holds for $j=J+1$.

Since (\ref{pf.finitary.Valphabeta.R.Tabjvu}) holds for $j=J$, we have%
\begin{equation}
T_{\left]  \alpha,\beta\right]  }^{J}v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-J},\ \ \ \ \ \ \ \ \ \ \text{if }u-J>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-J\leq\alpha
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  . \label{pf.finitary.Valphabeta.g.1}%
\end{equation}


Now let $u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $ be arbitrary. We
will prove that%
\begin{equation}
T_{\left]  \alpha,\beta\right]  }^{J+1}v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right.  . \label{pf.finitary.Valphabeta.g.2}%
\end{equation}


In order to do so, we distinguish between two cases:

\textit{Case 1:} We have $u>\alpha+1$.

\textit{Case 2:} We have $u\leq\alpha+1$.

First, consider Case 1. In this case, $u>\alpha+1$. But the definition of
$T_{\left]  \alpha,\beta\right]  }$ yields $T_{\left]  \alpha,\beta\right]
}\left(  v_{u}\right)  =\left\{
\begin{array}
[c]{l}%
v_{u-1},\ \ \ \ \ \ \ \ \ \ \text{if }u>\alpha+1;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u=\alpha+1
\end{array}
\right.  =v_{u-1}$ (since $u>\alpha+1$). Since $T_{\left]  \alpha
,\beta\right]  }^{J+1}=T_{\left]  \alpha,\beta\right]  }^{J}\circ T_{\left]
\alpha,\beta\right]  }$, we have%
\begin{align*}
T_{\left]  \alpha,\beta\right]  }^{J+1}v_{u}  &  =\left(  T_{\left]
\alpha,\beta\right]  }^{J}\circ T_{\left]  \alpha,\beta\right]  }\right)
\left(  v_{u}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}\left(
\underbrace{T_{\left]  \alpha,\beta\right]  }\left(  v_{u}\right)  }%
_{=v_{u-1}}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}v_{u-1}\\
&  =\left\{
\begin{array}
[c]{l}%
v_{u-1-J},\ \ \ \ \ \ \ \ \ \ \text{if }u-1-J>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-1-J\leq\alpha
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{according to
(\ref{pf.finitary.Valphabeta.g.1}), applied to }u-1\text{ instead of }u\right)
\\
&  =\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }u-1-J=u-\left(  J+1\right)
\right)  .
\end{align*}
This proves (\ref{pf.finitary.Valphabeta.g.2}) in the Case 1.

Now, let us consider Case 2. In this case, $u\leq\alpha+1$. Thus, $u=\alpha+1$
(since $u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $). Hence,
$u-\left(  J+1\right)  =\left(  \alpha+1\right)  -\left(  J+1\right)
=\alpha-\underbrace{J}_{\geq0}\leq\alpha$, so that $\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right.  =0$. But the definition of $T_{\left]  \alpha,\beta\right]  }$ yields
$T_{\left]  \alpha,\beta\right]  }\left(  v_{u}\right)  =\left\{
\begin{array}
[c]{l}%
v_{u-1},\ \ \ \ \ \ \ \ \ \ \text{if }u>\alpha+1;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u=\alpha+1
\end{array}
\right.  =0$ (since $u=\alpha+1$). Since $T_{\left]  \alpha,\beta\right]
}^{J+1}=T_{\left]  \alpha,\beta\right]  }^{J}\circ T_{\left]  \alpha
,\beta\right]  }$, we have%
\begin{align*}
T_{\left]  \alpha,\beta\right]  }^{J+1}v_{u}  &  =\left(  T_{\left]
\alpha,\beta\right]  }^{J}\circ T_{\left]  \alpha,\beta\right]  }\right)
\left(  v_{u}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}\left(
\underbrace{T_{\left]  \alpha,\beta\right]  }\left(  v_{u}\right)  }%
_{=0}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}\left(  0\right) \\
&  =0=\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right.  .
\end{align*}
This proves (\ref{pf.finitary.Valphabeta.g.2}) in the Case 2.

We have thus proven (\ref{pf.finitary.Valphabeta.g.2}) in each of the Cases 1
and 2. Thus, (\ref{pf.finitary.Valphabeta.g.2}) always holds.

We have thus proven (\ref{pf.finitary.Valphabeta.g.2}) for every $u\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $. In other words,
(\ref{pf.finitary.Valphabeta.R.Tabjvu}) holds for $j=J+1$. This completes the
induction step. Thus, (\ref{pf.finitary.Valphabeta.R.Tabjvu}) is proven by
induction over $j$. In other words, Remark \ref{rmk.finitary.Valphabeta}
\textbf{(f)} is proven.

\textbf{(g)} Since $T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}=0$, the
endomorphism $T_{\left]  \alpha,\beta\right]  }$ is nilpotent. Thus, the
infinite sum $\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]  }^{k}$
converges (with respect to the discrete topology), and hence is a well-defined
endomorphism of $V_{\left]  \alpha,\beta\right]  }$.

Let us identify every endomorphism of the vector space $V_{\left]
\alpha,\beta\right]  }$ with the matrix representing this endomorphism with
respect to the basis $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$. Then, every $k\in\mathbb{N}$ and
$i\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $ satisfy%
\begin{align*}
&  \left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}\cdot v_{i}\\
&  =\left(  \text{the }\left(  \beta+1-i\right)  \text{-th column of the
matrix }\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v_{i}\text{ is the }\left(
\beta+1-i\right)  \text{-th element of the basis }\left(  v_{\beta}%
,v_{\beta-1},...,v_{\alpha+1}\right)  \right) \\
&  =\sum\limits_{u\in\left\{  1,2,...,\beta-\alpha\right\}  }%
\underbrace{\delta_{u-\left(  \beta+1-i\right)  ,k}}_{\substack{=\delta
_{i-\left(  \beta+1-u\right)  ,k}\\\text{(since }u-\left(  \beta+1-i\right)
=i-\left(  \beta+1-u\right)  \text{)}}}v_{\beta+1-u}=\sum\limits_{u\in\left\{
1,2,...,\beta-\alpha\right\}  }\delta_{i-\left(  \beta+1-u\right)  ,k}%
v_{\beta+1-u}\\
&  =\sum\limits_{u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  }%
\delta_{i-u,k}v_{u}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted
}u\text{ for }\beta+1-u\right) \\
&  =\sum\limits_{\substack{u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}
;\\u\neq i-k}}\underbrace{\delta_{i-u,k}}_{\substack{=0\\\text{(since }i-u\neq
k\\\text{(since }u\neq i-k\text{))}}}v_{u}+\sum\limits_{\substack{u\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  ;\\u=i-k}}\underbrace{\delta_{i-u,k}%
}_{\substack{=1\\\text{(since }i-u=k\\\text{(since }u=i-k\text{))}}}v_{u}\\
&  =\underbrace{\sum\limits_{\substack{u\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  ;\\u\neq i-k}}0v_{u}}_{=0}+\sum\limits_{\substack{u\in
\left\{  \alpha+1,\alpha+2,...,\beta\right\}  ;\\u=i-k}}v_{u}=\sum
\limits_{\substack{u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}
;\\u=i-k}}v_{u}\\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  ;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i-k>\alpha\text{ and }%
i-k\leq\beta\right)  ;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }i-k\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  \text{ is equivalent to }\left(  i-k>\alpha
\text{ and }i-k\leq\beta\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  i-k>\alpha\text{ and }i-k\leq\beta\right)  \text{ is
equivalent to }i-k>\alpha\\
\text{(because }i-k\leq\beta\text{ is automatically satisfied (since }%
i\in\left\{  \alpha+1,\alpha+2,...,\beta\right\} \\
\text{yields }i\leq\beta\text{, while }k\in\mathbb{N}\text{ yields }%
k\geq0\text{, so that }\underbrace{i}_{\leq\beta}-\underbrace{k}_{\geq0}%
\leq\beta-0=\beta\text{))}%
\end{array}
\right) \\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i-k\leq\alpha
\end{array}
\right.  .
\end{align*}
and%
\[
T_{\left]  \alpha,\beta\right]  }^{k}v_{i}=\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i-k\leq\alpha
\end{array}
\right.  .
\]
(by (\ref{pf.finitary.Valphabeta.R.Tabjvu}), applied to $k$ and $i$ instead of
$j$ and $u$). Hence, every $k\in\mathbb{N}$ and $i\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  $ satisfy%
\[
\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}\cdot v_{i}=\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i-k\leq\alpha
\end{array}
\right.  =T_{\left]  \alpha,\beta\right]  }^{k}v_{i}.
\]
Thus, $\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}=T_{\left]  \alpha,\beta\right]  }^{k}$ for
every $k\in\mathbb{N}$. Hence,%
\begin{equation}
\sum\limits_{k\geq0}c_{k}\underbrace{T_{\left]  \alpha,\beta\right]  }^{k}%
}_{=\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}}=\sum\limits_{k\geq0}c_{k}\left(
\delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{  1,2,...,\beta
-\alpha\right\}  ^{2}}=\left(  \sum\limits_{k\geq0}c_{k}\delta_{u-v,k}\right)
_{\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}.
\label{pf.finitary.Valphabeta.g.7}%
\end{equation}
On the other hand, recall that $c_{n}=0$ for every negative $n\in\mathbb{Z}$.
In other words, $c_{k}=0$ for every negative $k\in\mathbb{Z}$. Hence,
$c_{k}\delta_{u-v,k}=0$ for every negative $k\in\mathbb{Z}$ and every $\left(
u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}$. Thus, the sum
$\sum\limits_{k<0}c_{k}\delta_{u-v,k}$ is well-defined and equals $0$ for
every $\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}$.
Hence, in $\left(  \sum\limits_{k<0}c_{k}\delta_{u-v,k}\right)  _{\left(
u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=\left(  0\right)
_{\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=0$. In
other words, $0=\left(  \sum\limits_{k<0}c_{k}\delta_{u-v,k}\right)  _{\left(
u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$. Adding this
equality to (\ref{pf.finitary.Valphabeta.g.7}), we obtain%
\begin{align*}
\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]  }^{k}  &  =\left(
\sum\limits_{k\geq0}c_{k}\delta_{u-v,k}\right)  _{\left(  u,v\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}+\left(  \sum\limits_{k<0}%
c_{k}\delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{  1,2,...,\beta
-\alpha\right\}  ^{2}}\\
&  =\left(  \sum\limits_{k<0}c_{k}\delta_{u-v,k}+\sum\limits_{k<0}c_{k}%
\delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{  1,2,...,\beta
-\alpha\right\}  ^{2}}.
\end{align*}
But since every $\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}
^{2}$ satisfies%
\begin{align*}
\sum\limits_{k\geq0}c_{k}\delta_{u-v,k}+\sum\limits_{k<0}c_{k}\delta_{u-v,k}
&  =\sum\limits_{k\in\mathbb{Z}}c_{k}\delta_{u-v,k}=\sum
\limits_{\substack{k\in\mathbb{Z};\\k\neq u-v}}c_{k}\underbrace{\delta
_{u-v,k}}_{\substack{=0\\\text{(since }u-v\neq k\text{)}}}+c_{u-v}%
\underbrace{\delta_{u-v,u-v}}_{=1}\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k\neq u-v}}c_{k}%
0}_{=0}+c_{u-v}=c_{u-v},
\end{align*}
this rewrites as $\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]
}^{k}=\left(  \underbrace{\sum\limits_{k<0}c_{k}\delta_{u-v,k}+\sum
\limits_{k<0}c_{k}\delta_{u-v,k}}_{=c_{u-v}}\right)  _{\left(  u,v\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=\left(  c_{u-v}\right)
_{\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=\left(
c_{i-j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ (here, we renamed
$\left(  u,v\right)  $ as $\left(  i,j\right)  $). In other words, the matrix
representing the endomorphism $\sum\limits_{k\geq0}c_{k}T_{\left]
\alpha,\beta\right]  }^{k}$ of $V_{\left]  \alpha,\beta\right]  }$ with
respect to the basis $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$ is $\left(  c_{i-j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$. This proves Remark
\ref{rmk.finitary.Valphabeta} \textbf{(g)}.
\end{verlong}

This completes the proof of Remark \ref{rmk.finitary.Valphabeta}.

A less trivial observation is the following:

\begin{proposition}
\label{prop.finitary.Valphabeta.R}Let $\alpha$ and $\beta$ be integers such
that $\alpha-1\leq\beta$. Let $\ell\in\mathbb{N}$. Then, $R_{\ell,\left]
\alpha,\beta\right]  }:\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\mathcal{F}^{\left(  \alpha+\ell\right)  }$ is an
$\mathcal{A}_{+}$-module homomorphism (where the $\mathcal{A}_{+}$-module
structure on $\mathcal{F}^{\left(  \alpha+\ell\right)  }$ is obtained by
restricting the $\mathcal{A}$-module structure on $\mathcal{F}^{\left(
\alpha+\ell\right)  }$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.finitary.Valphabeta.R}.} Let $T$ be the
shift operator defined in Definition \ref{def.shiftoperator}.

Let $j$ be a positive integer. Then, for every integer $i\leq0$, the $\left(
i,i\right)  $-th entry of the matrix $T^{j}$ is $0$ (in fact, the matrix
$T^{j}$ has only zeroes on its main diagonal). Moreover, from the definition
of $T$, it follows quickly that%
\begin{equation}
T^{j}v_{u}=v_{u-j}\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\mathbb{Z}.
\label{pf.finitary.Valphabeta.R.Tjvu}%
\end{equation}


\begin{verlong}
(Actually, (\ref{pf.finitary.Valphabeta.R.Tjvu}) follows from
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}), applied to $j$ and $u$ instead
of $k$ and $i$.)
\end{verlong}

Let $\left(  i_{0},i_{1},...,i_{\ell-1}\right)  $ be an $\ell$-tuple of
elements of $\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $ such that
$i_{0}>i_{1}>...>i_{\ell-1}$. We will prove that%
\begin{equation}
a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
=R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  .
\label{pf.finitary.Valphabeta.R.1}%
\end{equation}


Indeed, let us extend the $\ell$-tuple $\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  $ to a sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ of
integers by setting $\left(  i_{k}=\ell+\alpha-k\text{ for every }k\in\left\{
\ell,\ell+1,\ell+2,...\right\}  \right)  $. Then, $\left(  i_{0},i_{1}%
,i_{2},...\right)  =\left(  i_{0},i_{1},...,i_{\ell-1},\alpha,\alpha
-1,\alpha-2,...\right)  $. As a consequence, the sequence $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ is strictly decreasing (since $i_{0}>i_{1}%
>...>i_{\ell-1}>\alpha>\alpha-1>\alpha-2>...$) and hence an $\left(
\alpha+\ell\right)  $-degression. Note that
\begin{equation}
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0\ \ \ \ \ \ \ \ \ \ \text{for every
}k\in\mathbb{N}\text{ satisfying }k\geq\ell\label{pf.finitary.Valphabeta.R.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.Valphabeta.R.2}):} Let
$k\in\mathbb{N}$ satisfy $k\geq\ell$. Then, $k+j\geq\ell$ as well (since $j$
is positive), so that $i_{k+j}=\ell+\alpha-\left(  k+j\right)
=\underbrace{\left(  \ell+\alpha-k\right)  }_{\substack{=i_{k}\\\text{(since
}k\geq\ell\text{)}}}-j=i_{k}-j$. Thus, the sequence $\left(  i_{0}%
,i_{1},...,i_{k-1},i_{k}-j,i_{k+1},i_{k+2},...\right)  $ has two equal terms
(since $k+j\neq k$ (due to $j$ being positive)). Thus, $v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...=0$. This proves (\ref{pf.finitary.Valphabeta.R.2}).}.
Also,%
\begin{equation}
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0\ \ \ \ \ \ \ \ \ \ \text{for every
}k\in\mathbb{N}\text{ satisfying }k<\ell\text{ and }i_{k}-j\leq\alpha.
\label{pf.finitary.Valphabeta.R.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.Valphabeta.R.3}):} Let
$k\in\mathbb{N}$ satisfy $k<\ell$ and $i_{k}-j\leq\alpha$. Every integer
$\leq\alpha$ is contained in the sequence $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ (since $\left(  i_{0},i_{1},i_{2},...\right)  =\left(
i_{0},i_{1},...,i_{\ell-1},\alpha,\alpha-1,\alpha-2,...\right)  $). Since
$i_{k}-j\leq\alpha$, this yields that the integer $i_{k}-j$ is contained in
the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, there exists a
$p\in\mathbb{N}$ such that $i_{p}=i_{k}-j$. Consider this $p$.
\par
Since $k<\ell$, we have $i_{k}\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}
$, so that $i_{k}>\alpha\geq i_{k}-j=i_{p}$, and hence $i_{k}\neq i_{p}$.
Thus, $k\neq p$. Since $i_{k}-j=i_{p}$ and $k\neq p$, the sequence $\left(
i_{0},i_{1},...,i_{k-1},i_{k}-j,i_{k+1},i_{k+2},...\right)  $ has two equal
terms. Thus, $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0$, and this proves
(\ref{pf.finitary.Valphabeta.R.3}).}

\begin{vershort}
The definition of $R_{\ell,\left]  \alpha,\beta\right]  }$ yields%
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{\ell
-1},\alpha,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2}%
,...\right)  \right)  ,
\end{align*}
so that%
\begin{align}
&  a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
\nonumber\\
&  =a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =\left(  \widehat{\rho}\left(  T^{j}\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\mid_{\mathcal{F}^{\left(
\alpha+\ell\right)  }}=T^{j}\mid_{\mathcal{F}^{\left(  \alpha+\ell\right)  }%
}=\widehat{\rho}\left(  T^{j}\right)  \right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T^{j}v_{i_{k}}=v_{i_{k}-j}\\\text{(by
(\ref{pf.finitary.Valphabeta.R.Tjvu}), applied to }u=i_{k}\text{)}}}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.glinf.ainfact}, applied to }\left(  b_{0}%
,b_{1},b_{2},...\right)  =\left(  v_{i_{0}},v_{i_{1}},v_{i_{2}},...\right)
\text{ and }a=T^{j}\\
\text{(since for every integer }i\leq0\text{, the }\left(  i,i\right)
\text{-th entry of }T^{j}\text{ is }0\text{).}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....\nonumber
\end{align}
The sum on the right hand side of this equation is infinite, but lots of its
terms vanish: Namely, all its terms with $k\geq\ell$ vanish (because of
(\ref{pf.finitary.Valphabeta.R.2})), and all its terms with $k<\ell$ and
$i_{k}-j\leq\alpha$ vanish (because of (\ref{pf.finitary.Valphabeta.R.3})). We
can thus replace the $\sum\limits_{k\geq0}$ sign by a $\sum
\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}$ sign, and obtain%
\begin{align}
&  a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.finitary.Valphabeta.R.oneside.short}%
\end{align}


On the other hand, by the definition of the $\mathcal{A}_{+}$-module
$\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, we have%
\begin{align*}
&  a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}\right) \\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\underbrace{\left(  a_{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\\\text{(since
}a_{j}\text{ acts as }T_{\left]  \alpha,\beta\right]  }^{j}\\\text{on
}V_{\left]  \alpha,\beta\right]  }\text{)}}}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}.
\end{align*}
Applying the linear map $R_{\ell,\left]  \alpha,\beta\right]  }$ to this
equation, we obtain
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right) \\
&  =\sum\limits_{k=0}^{\ell-1}\underbrace{R_{\ell,\left]  \alpha,\beta\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\right)  }%
_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha}\wedge
v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\\text{(by the definition of }%
R_{\ell,\left]  \alpha,\beta\right]  }\text{)}}}\\
&  =\underbrace{\sum\limits_{k=0}^{\ell-1}}_{=\sum\limits_{\substack{k\geq
0;\\k<\ell}}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...}_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\\text{(since }\left(  i_{0},i_{1},...,i_{\ell-1},\alpha
,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2},...\right)
\text{)}}}\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  }_{\substack{=\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \\\text{(by (\ref{pf.finitary.Valphabeta.R.Tabjvu}), applied to
}u=i_{k}\text{)}}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge....
\end{align*}
Compared with (\ref{pf.finitary.Valphabeta.R.oneside.short}), this yields%
\[
a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
=R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  .
\]
We have thus proven (\ref{pf.finitary.Valphabeta.R.1}).
\end{vershort}

\begin{verlong}
The definition of $R_{\ell,\left]  \alpha,\beta\right]  }$ yields%
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{\ell
-1},\alpha,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2}%
,...\right)  \right)  ,
\end{align*}
so that%
\begin{align}
&  a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
\nonumber\\
&  =a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =\left(  \widehat{\rho}\left(  T^{j}\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\mid_{\mathcal{F}^{\left(
\alpha+\ell\right)  }}=T^{j}\mid_{\mathcal{F}^{\left(  \alpha+\ell\right)  }%
}=\widehat{\rho}\left(  T^{j}\right)  \right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T^{j}v_{i_{k}}=v_{i_{k}-j}\\\text{(by
(\ref{pf.finitary.Valphabeta.R.Tjvu}), applied to }u=i_{k}\text{)}}}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.glinf.ainfact}, applied to }\left(  b_{0}%
,b_{1},b_{2},...\right)  =\left(  v_{i_{0}},v_{i_{1}},v_{i_{2}},...\right)
\text{ and }a=T^{j}\\
\text{(since for every integer }i\leq0\text{, the }\left(  i,i\right)
\text{-th entry of }T^{j}\text{ is }0\text{).}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k\geq\ell
}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}%
_{\substack{=0\\\text{(due to (\ref{pf.finitary.Valphabeta.R.2}))}%
}}\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}%
-j\leq\alpha}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}%
_{\substack{=0\\\text{(due to (\ref{pf.finitary.Valphabeta.R.3}))}%
}}\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.finitary.Valphabeta.R.oneside}%
\end{align}
On the other hand, by the definition of the $\mathcal{A}_{+}$-module
$\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, we have%
\begin{align*}
&  a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}\right) \\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\underbrace{\left(  a_{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\\\text{(since
}a_{j}\text{ acts as }T_{\left]  \alpha,\beta\right]  }^{j}\\\text{on
}V_{\left]  \alpha,\beta\right]  }\text{)}}}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}},
\end{align*}
so that%
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right) \\
&  =R_{\ell,\left]  \alpha,\beta\right]  }\left(  \sum\limits_{k=0}^{\ell
-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\right) \\
&  =\sum\limits_{k=0}^{\ell-1}\underbrace{R_{\ell,\left]  \alpha,\beta\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\right)  }%
_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha}\wedge
v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\\text{(by the definition of }%
R_{\ell,\left]  \alpha,\beta\right]  }\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }R_{\ell,\left]  \alpha
,\beta\right]  }\text{ is linear}\right) \\
&  =\underbrace{\sum\limits_{k=0}^{\ell-1}}_{=\sum\limits_{\substack{k\geq
0;\\k<\ell}}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...}_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\\text{(since }\left(  i_{0},i_{1},...,i_{\ell-1},\alpha
,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2},...\right)
\text{)}}}\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  }_{\substack{=\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \\\text{(by (\ref{pf.finitary.Valphabeta.R.Tabjvu}), applied to
}u=i_{k}\text{)}}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\underbrace{\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  }_{\substack{=v_{i_{k}-j}\\\text{(since }i_{k}-j>\alpha\text{)}%
}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}%
-j\leq\alpha}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}%
\wedge\underbrace{\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  }_{\substack{=0\\\text{(since }i_{k}-j\leq\alpha\text{)}}}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{k\geq
0;\\k<\ell;\\i_{k}-j\leq\alpha}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge0\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{=0}\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge....
\end{align*}
Compared with (\ref{pf.finitary.Valphabeta.R.oneside}), this yields%
\[
a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
=R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  .
\]
We have thus proven (\ref{pf.finitary.Valphabeta.R.1}).
\end{verlong}

Now, forget that we fixed $j$ and $\left(  i_{0},i_{1},...,i_{\ell-1}\right)
$. We have thus proven the equality (\ref{pf.finitary.Valphabeta.R.1}) for
every positive integer $j$ and every $\ell$-tuple $\left(  i_{0}%
,i_{1},...,i_{\ell-1}\right)  $ of elements of $\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  $ such that $i_{0}>i_{1}>...>i_{\ell-1}$.

\begin{vershort}
Since $\mathcal{A}_{+}=\left\langle a_{1},a_{2},a_{3},...\right\rangle $ and
since $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)
_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$ is a basis of the vector
space $\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, this
yields (by linearity) that $x\rightharpoonup\left(  R_{\ell,\left]
\alpha,\beta\right]  }\left(  w\right)  \right)  =R_{\ell,\left]  \alpha
,\beta\right]  }\left(  x\rightharpoonup w\right)  $ holds for every
$x\in\mathcal{A}_{+}$ and $w\in\wedge^{\ell}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Thus, $R_{\ell,\left]  \alpha,\beta\right]  }$ is
an $\mathcal{A}_{+}$-module homomorphism. This proves Proposition
\ref{prop.finitary.Valphabeta.R}.
\end{vershort}

\begin{verlong}
From the above, we can easily obtain that%
\begin{equation}
x\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
w\right)  \right)  =R_{\ell,\left]  \alpha,\beta\right]  }\left(
x\rightharpoonup w\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
x\in\mathcal{A}_{+}\text{ and }w\in\wedge^{\ell}\left(  V_{\left]
\alpha,\beta\right]  }\right)  . \label{pf.finitary.Valphabeta.R.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.Valphabeta.R.7}):} Let
$x\in\mathcal{A}_{+}$ and $w\in\wedge^{\ell}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Since $x\in\mathcal{A}_{+}=\left\langle
a_{1},a_{2},a_{3},...\right\rangle $, there exists a sequence $\left(
\lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ of elements of $\mathbb{C}$
such that $x=\sum\limits_{j=1}^{\infty}\lambda_{j}a_{j}$ and such that all but
finitely many positive integers $j$ satisfy $\lambda_{j}=0$. Consider this
sequence $\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)  $.
\par
We know that $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}%
}\right)  _{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$ is a basis of
the vector space $\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $ (because $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
is a basis of the vector space $V_{\left]  \alpha,\beta\right]  }$). Since
$w\in\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, there
must thus exist a family $\left(  \alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }\right)  _{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$ of
elements of $\mathbb{C}$ such that $w=\sum\limits_{\beta\geq i_{0}%
>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}$.
Consider this family $\left(  \alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }\right)  _{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$.
\par
Since $x=\sum\limits_{j=1}^{\infty}\lambda_{j}a_{j}$ and $w=\sum
\limits_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(
i_{0},i_{1},...,i_{\ell-1}\right)  }v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}$, we have%
\begin{align*}
x\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
w\right)  \right)   &  =\left(  \sum\limits_{j=1}^{\infty}\lambda_{j}%
a_{j}\right)  \rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]
}\left(  \sum\limits_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}%
\alpha_{\left(  i_{0},i_{1},...,i_{\ell-1}\right)  }v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right)  \right) \\
&  =\sum\limits_{j=1}^{\infty}\lambda_{j}\sum\limits_{\beta\geq i_{0}%
>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }\underbrace{a_{j}\rightharpoonup\left(  R_{\ell,\left]
\alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}\right)  \right)  }_{\substack{=R_{\ell,\left]  \alpha
,\beta\right]  }\left(  a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  \\\text{(by
(\ref{pf.finitary.Valphabeta.R.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the action of }\mathcal{A}%
_{+}\text{ is bilinear, and }R_{\ell,\left]  \alpha,\beta\right]  }\text{ is
linear}\right) \\
&  =\sum\limits_{j=1}^{\infty}\lambda_{j}\sum\limits_{\beta\geq i_{0}%
>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}%
\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}%
}\right)  \right) \\
&  =R_{\ell,\left]  \alpha,\beta\right]  }\left(  \underbrace{\left(
\sum\limits_{j=1}^{\infty}\lambda_{j}a_{j}\right)  }_{=x}\rightharpoonup
\underbrace{\left(  \sum\limits_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}%
\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell-1}\right)  }v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  }_{=w}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the action of }\mathcal{A}%
_{+}\text{ is bilinear, and }R_{\ell,\left]  \alpha,\beta\right]  }\text{ is
linear}\right) \\
&  =R_{\ell,\left]  \alpha,\beta\right]  }\left(  x\rightharpoonup w\right)  .
\end{align*}
This proves (\ref{pf.finitary.Valphabeta.R.7}).} As a consequence,
$R_{\ell,\left]  \alpha,\beta\right]  }:\wedge^{\ell}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \rightarrow\mathcal{F}^{\left(  \alpha
+\ell\right)  }$ is an $\mathcal{A}_{+}$-module homomorphism. This proves
Proposition \ref{prop.finitary.Valphabeta.R}.
\end{verlong}

Now, we can turn to the promised proof:

\textit{Second proof of Theorem \ref{thm.schur.fermi.skew}.} In order to
simplify notation, we assume that $\mathbf{R}=\mathbb{C}$. (All the arguments
that we will make in the following are independent of the ground ring, as long
as the ground ring is a commutative $\mathbb{Q}$-algebra. Therefore, we are
actually allowed to assume that $\mathbf{R}=\mathbb{C}$.) Since we assumed
that $\mathbf{R}=\mathbb{C}$, we have $\mathcal{A}_{\mathbf{R}}=\mathcal{A}$
and $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }=\mathcal{F}^{\left(
0\right)  }$.

Since $\left(  i_{0},i_{1},i_{2},...\right)  $ is a $0$-degression, every
sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}+k=0$. In other words,
there exists some $K\in\mathbb{N}$ such that every $k\in\mathbb{N}$ satisfying
$k\geq K$ satisfies $i_{k}+k=0$. Consider this $K$. WLOG assume that $K>0$
(else, replace $K$ by $K+1$). Since every $k\in\mathbb{N}$ satisfying $k\geq
K$ satisfies $i_{k}+k=0$ and thus $i_{k}=-k$, we have $\left(  i_{0}%
,i_{1},i_{2},...\right)  =\left(  i_{0},i_{1},i_{2},...,i_{K-1},-K,-\left(
K+1\right)  ,-\left(  K+2\right)  ,...\right)  =\left(  i_{0},i_{1}%
,i_{2},...,i_{K-1},-K,-K-1,-K-2,...\right)  $. In particular, $i_{K}=-K$.

Let $\alpha=i_{K}$ and $\beta=i_{0}$. Since $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a $0$-degression, we have $i_{0}>i_{1}>i_{2}>...$.
Thus, $i_{0}>i_{1}>i_{2}>...>i_{K-1}>i_{K}$. In other words, $i_{0}\geq
i_{0}>i_{1}>i_{2}>...>i_{K-1}>i_{K}$. Since $i_{0}=\beta$ and $i_{K}=\alpha$,
this rewrites as $\beta\geq i_{0}>i_{1}>i_{2}>...>i_{K-1}>\alpha$. Thus, the
integers $i_{0}$, $i_{1}$, $i_{2}$, $...$, $i_{K-1}$ lie in the set $\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $. Hence, the vectors $v_{i_{0}}$,
$v_{i_{1}}$, $...$, $v_{i_{K-1}}$ lie in the vector space $V_{\left]
\alpha,\beta\right]  }$. Thus, the definition of the map $R_{K,\left]
\alpha,\beta\right]  }$ (defined according to Definition
\ref{def.finitary.Valphabeta} \textbf{(d)}) yields%
\begin{align}
R_{K,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{K-1}}\right)   &  =v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge
v_{\alpha-2}\wedge...\nonumber\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\wedge v_{-K}\wedge
v_{-K-1}\wedge v_{-K-2}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha=i_{K}=-K\right) \nonumber\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...
\label{pf.finitary.Valphabeta.R.8}%
\end{align}
(since $\left(  i_{0},i_{1},i_{2},...,i_{K-1},-K,-K-1,-K-2,...\right)
=\left(  i_{0},i_{1},i_{2},...\right)  $).

For every $p\in\left\{  1,2,...,K\right\}  $, define an integer $\widetilde{i}%
_{p}$ by $\widetilde{i}_{p}=\beta+1-i_{p-1}$.

Subtracting the chain of inequalities $\beta\geq i_{0}>i_{1}>i_{2}%
>...>i_{K-1}>\alpha$ from $\beta+1$, we obtain $\beta+1-\beta\leq\beta
+1-i_{0}<\beta+1-i_{1}<\beta+1-i_{2}<...<\beta+1-i_{K-1}<\beta+1-\alpha$.
Since $\beta+1-i_{p-1}=\widetilde{i}_{p}$ for every $p\in\left\{
1,2,...,K\right\}  $, this rewrites as $\beta+1-\beta\leq\widetilde{i}%
_{1}<\widetilde{i}_{2}<\widetilde{i}_{3}<...<\widetilde{i}_{K}<\beta+1-\alpha$.

This simplifies to $1\leq\widetilde{i}_{1}<\widetilde{i}_{2}<\widetilde{i}%
_{3}<...<\widetilde{i}_{K}<\beta+1-\alpha$. Since $\widetilde{i}_{K}$ and
$\beta+1-\alpha$ are integers, we obtain $\widetilde{i}_{K}\leq\beta-\alpha$
from $\widetilde{i}_{K}<\beta+1-\alpha$. Thus, $1\leq\widetilde{i}%
_{1}<\widetilde{i}_{2}<\widetilde{i}_{3}<...<\widetilde{i}_{K}\leq\beta
-\alpha$.

On the other hand, substituting $y$ for $x$ in (\ref{def.schur.sk.genfun}), we
obtain%
\[
\sum\limits_{k\geq0}S_{k}\left(  y\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}y_{i}z^{i}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }%
\mathbb{C}\left[  \left[  z\right]  \right]  .
\]
Substituting $T_{\left]  \alpha,\beta\right]  }$ for $z$ in this equality, we
obtain
\begin{equation}
\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]
}^{k}=\exp\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]
}^{i}\right)  . \label{pf.schur.fermi.skew.finitary.1}%
\end{equation}


From Remark \ref{rmk.finitary.Valphabeta}, we know that the endomorphisms
$\exp\left(  \sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]
}^{i}\right)  $ and \newline$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ are well-defined.
Denote the endomorphism \newline$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ by $f$. Then,
\begin{align*}
f  &  =\exp\left(  \underbrace{y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...}%
_{=\sum\limits_{i=1}^{\infty}y_{i}a_{i}}\right)  =\exp\left(  \sum
\limits_{i=1}^{\infty}y_{i}\underbrace{a_{i}}_{\substack{=T_{\left]
\alpha,\beta\right]  }^{i}\\\text{(since the action of }a_{i}\text{ on
}V_{\left]  \alpha,\beta\right]  }\\\text{was defined to be }T_{\left]
\alpha,\beta\right]  }^{i}\text{)}}}\right)  =\exp\left(  \sum\limits_{i=1}%
^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right) \\
&  =\exp\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  =\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]
\alpha,\beta\right]  }^{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.1})}\right)  .
\end{align*}
Note that $S_{n}\left(  y\right)  \in\mathbb{C}$ for every $n\in\mathbb{Z}$
(since we assumed that $\mathbf{R}=\mathbb{C}$). Also note that $S_{n}\left(
y\right)  =0$ for every negative $n\in\mathbb{Z}$ (since $S_{n}=0$ for every
negative $n$). Hence, according to Remark \ref{rmk.finitary.Valphabeta}
\textbf{(g)} (applied to $c_{n}=S_{n}\left(  y\right)  $), the sum
$\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]
}^{k}$ is a well-defined endomorphism of $V_{\left]  \alpha,\beta\right]  }$,
and the matrix representing this endomorphism with respect to the basis
$\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ of $V_{\left]
\alpha,\beta\right]  }$ is $\left(  S_{i-j}\left(  y\right)  \right)
_{\left(  i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$. Denote
this matrix $\left(  S_{i-j}\left(  y\right)  \right)  _{\left(  i,j\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$ by $A$. Let $n=\beta-\alpha$,
and denote the basis $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$ by $\left(  e_{1},e_{2},...,e_{n}%
\right)  $. Then,%
\begin{equation}
e_{k}=v_{\beta+1-k}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,...,n\right\}  . \label{pf.schur.fermi.skew.finitary.2}%
\end{equation}
As a consequence,%
\begin{equation}
e_{\beta+1-k}=v_{k}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  \label{pf.schur.fermi.skew.finitary.2a}%
\end{equation}
(because for every $k\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $, we
can apply (\ref{pf.schur.fermi.skew.finitary.2}) to $\beta+1-k$ instead of
$k$, and thus obtain $e_{\beta+1-k}=v_{\beta+1-\left(  \beta+1-k\right)
}=v_{k}$).

We have shown that the matrix representing the endomorphism $\sum
\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]  }^{k}$
of $V_{\left]  \alpha,\beta\right]  }$ with respect to the basis $\left(
v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ of $V_{\left]  \alpha
,\beta\right]  }$ is $\left(  S_{i-j}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$. Since
$\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]
}^{k}=f$, $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  =\left(
e_{1},e_{2},...,e_{n}\right)  $, and $\left(  S_{i-j}\left(  y\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}%
}=A$, this rewrites as follows: The matrix representing the endomorphism $f$
of $V_{\left]  \alpha,\beta\right]  }$ with respect to the basis $\left(
e_{1},e_{2},...,e_{n}\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ is $A$.
In other words, $A$ is the $n\times n$-matrix which represents the map $f$
with respect to the bases $\left(  e_{1},e_{2},...,e_{n}\right)  $ and
$\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V_{\left]  \alpha,\beta\right]
}$ and $V_{\left]  \alpha,\beta\right]  }$. Therefore, we can apply
Proposition \ref{prop.GLinf.det.fin} to $n$, $V_{\left]  \alpha,\beta\right]
}$, $V_{\left]  \alpha,\beta\right]  }$, $\left(  e_{1},e_{2},...,e_{n}%
\right)  $, $\left(  e_{1},e_{2},...,e_{n}\right)  $, $K$, $f$, $A$ and
$\left(  \widetilde{i}_{1},\widetilde{i}_{2},\widetilde{i}_{3}%
,...,\widetilde{i}_{K}\right)  $ instead of $m$, $P$, $Q$, $\left(
e_{1},e_{2},...,e_{n}\right)  $, $\left(  f_{1},f_{2},...,f_{m}\right)  $,
$\ell$, $f$, $A$ and $\left(  i_{1},i_{2},...,i_{\ell}\right)  $. As a result,
we obtain%
\begin{equation}
\left(  \wedge^{K}\left(  f\right)  \right)  \left(  e_{\widetilde{i}_{1}%
}\wedge e_{\widetilde{i}_{2}}\wedge...\wedge e_{\widetilde{i}_{K}}\right)
=\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}%
}\det\left(  A_{j_{1},j_{2},...,j_{K}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{K}}. \label{pf.schur.fermi.skew.finitary.3}%
\end{equation}


But every $p\in\left\{  1,2,...,K\right\}  $ satisfies $e_{\widetilde{i}_{p}%
}=v_{i_{p-1}}$\ \ \ \ \footnote{This is because $\widetilde{i}_{p}%
=\beta+1-i_{p-1}$, so that $\beta+1-\widetilde{i}_{p}=i_{p-1}$ and now%
\begin{align*}
e_{\widetilde{i}_{p}}  &  =v_{\beta+1-\widetilde{i}_{p}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.fermi.skew.finitary.2}),
applied to }\widetilde{i}_{p}\text{ instead of }k\right) \\
&  =v_{i_{p-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\beta+1-\widetilde{i}%
_{p}=i_{p-1}\right)  .
\end{align*}
}. Hence, $\left(  e_{\widetilde{i}_{1}},e_{\widetilde{i}_{2}}%
,...,e_{\widetilde{i}_{K}}\right)  =\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{K-1}%
}\right)  $. Consequently, $e_{\widetilde{i}_{1}}\wedge e_{\widetilde{i}_{2}%
}\wedge...\wedge e_{\widetilde{i}_{K}}=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{K-1}}$. Thus, (\ref{pf.schur.fermi.skew.finitary.3})
rewrites as%
\begin{equation}
\left(  \wedge^{K}\left(  f\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{K-1}}\right)  =\sum\limits_{\substack{j_{1}\text{,
}j_{2}\text{, }...\text{, }j_{K}\text{ are }K\text{ integers;}\\1\leq
j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}}\det\left(  A_{j_{1},j_{2},...,j_{K}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge e_{j_{K}}.
\label{pf.schur.fermi.skew.finitary.4}%
\end{equation}


But $\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}%
\in\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $ is a
nilpotent linear map (by Remark \ref{rmk.finitary.Valphabeta} \textbf{(c)}).
Hence, Theorem \ref{thm.finitary.rhoRho} (applied to $P=V_{\left]
\alpha,\beta\right]  }$, $a=\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}$ and $\ell=K$) yields that the exponential $\exp\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ is a
well-defined element of $\operatorname*{U}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $ and satisfies $\wedge^{K}\left(  \exp\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\right)  =\exp\left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\right)  $. Since $\exp\left(  \sum\limits_{i\geq1}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}\right)  =f$, this rewrites as%
\begin{equation}
\wedge^{K}\left(  f\right)  =\exp\left(  \rho_{V_{\left]  \alpha,\beta\right]
},K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  \right)  . \label{pf.schur.fermi.skew.finitary.11}%
\end{equation}


\begin{vershort}
But it is easy to see that%
\begin{equation}
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+... \label{pf.schur.fermi.skew.finitary.12.short}%
\end{equation}
as endomorphisms of $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $\ \ \ \ \footnote{\textit{Proof of
(\ref{pf.schur.fermi.skew.finitary.12.short}).} By the definition of
$\rho_{V_{\left]  \alpha,\beta\right]  },K}$, we know that $\rho_{V_{\left]
\alpha,\beta\right]  },K}:\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\operatorname*{End}\left(  \wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \right)  $ denotes the representation of the
Lie algebra $\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $
on the $K$-th exterior power of the defining representation $V_{\left]
\alpha,\beta\right]  }$ of $\mathfrak{gl}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Hence,
\[
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\mid_{\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  }.
\]
Hence, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]
}$ satisfy%
\begin{align*}
&  \left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right) \\
&  =\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  \rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi
_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{T_{\left]  \alpha,\beta\right]  }%
^{i}\rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)
}_{\substack{=\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}\wedge...\wedge
\xi_{k-1}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{i}\rightharpoonup
\xi_{k}\right)  \wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi
_{K}\\\text{(by the definition of the }\mathfrak{gl}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{-module }\wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{i}\rightharpoonup\xi_{k}\right)  }_{=T_{\left]  \alpha
,\beta\right]  }^{i}\xi_{k}}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge
\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}.
\end{align*}
On the other hand, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]
\alpha,\beta\right]  }$ satisfy%
\begin{align*}
&  \underbrace{\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  }%
_{=\sum\limits_{i\geq1}y_{i}a_{i}}\left(  \xi_{1}\wedge\xi_{2}\wedge
...\wedge\xi_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{a_{i}\left(  \xi_{1}\wedge\xi
_{2}\wedge...\wedge\xi_{K}\right)  }_{\substack{=\sum\limits_{k=1}^{K}\xi
_{1}\wedge\xi_{2}\wedge...\wedge\xi_{k-1}\wedge a_{i}\xi_{k}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\\text{(by the definition of the
}\mathcal{A}_{+}\text{-module }\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{a_{i}\xi_{k}}_{\substack{=T_{\left]
\alpha,\beta\right]  }^{i}\xi_{k}\\\text{(since the element }a_{i}\text{ of
}\mathcal{A}_{+}\text{ acts}\\\text{on }V_{\left]  \alpha,\beta\right]
}\text{ by }T_{\left]  \alpha,\beta\right]  }^{i}\text{)}}}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)  .
\end{align*}
In other words, the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$
and $\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq
1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ are equal to each other on the set
$\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1},\xi
_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $. Since the
set $\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1}%
,\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $ is a
spanning set of the vector space $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $, this entails that the two endomorphisms
$y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ and $\rho_{V_{\left]  \alpha
,\beta\right]  },K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $ are identical. This proves
(\ref{pf.schur.fermi.skew.finitary.12.short}).}. Hence,
(\ref{pf.schur.fermi.skew.finitary.11}) rewrites as%
\[
\wedge^{K}\left(  f\right)  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  .
\]
Hence,%
\begin{align}
&  \underbrace{\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  \right)  }_{=\wedge^{K}\left(  f\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\left(  \wedge^{K}\left(  f\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}%
}\det\left(  A_{j_{1},j_{2},...,j_{K}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{K}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.4})}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{e_{\beta+1-j_{0}}\wedge
e_{\beta+1-j_{1}}\wedge...\wedge e_{\beta+1-j_{K-1}}}_{\substack{=v_{j_{0}%
}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\\\text{(due to
(\ref{pf.schur.fermi.skew.finitary.2a}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(
\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}\right)  \text{ for }\left(
j_{1},j_{2},...,j_{K}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}. \label{pf.schur.fermi.skew.finitary.22.short}%
\end{align}
But every $K$-tuple $\left(  j_{0},j_{1},...,j_{K-1}\right)  $ of integers
such that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha$ satisfies%
\[
\sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }j_{K+2}\text{, }...\text{
are integers;}\\j_{k}=-k\text{ for every }k\geq K}}\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  =\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)
\]
(since the sum $\sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }%
j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for every }k\geq
K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  $ has
only one addend). Thus, (\ref{pf.schur.fermi.skew.finitary.22.short}) becomes%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\underbrace{\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  }_{=\sum\limits_{\substack{j_{K}\text{,
}j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for
every }k\geq K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta
+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}%
}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\\
&  =\underbrace{\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{,
}j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\sum\limits_{\substack{j_{K}%
\text{, }j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}%
\\j_{k}=-k\text{ for every }k\geq K}}}_{=\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}%
\\j_{k}=-k\text{ for every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}}.
\end{align*}
Applying the linear map $R_{K,\left]  \alpha,\beta\right]  }$ to this
equality, we obtain%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{R_{K,\left]
\alpha,\beta\right]  }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\right)  }_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge v_{\alpha-2}%
\wedge...\\\text{(by the definition of }R_{K,\left]  \alpha,\beta\right]
}\text{)}}}\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge
v_{\alpha-2}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\wedge v_{-K}\wedge v_{-K-1}\wedge
v_{-K-2}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\\\text{(since every }k\geq K\text{ satisfies }-k=j_{k}\text{, and
therefore}\\\left(  j_{0},j_{1},...,j_{K-1},-K,-K-1,-K-2,...\right)  =\left(
j_{0},j_{1},...,j_{K-1},j_{K},j_{K+1},j_{K+2},...\right)  =\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha=i_{K}=-K\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.32.short}%
\end{align}
Let us now notice that when $\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}$ is a sequence of integers satisfying $\left(
j_{k}=-k\text{ for every }k\geq K\right)  $, then a very straightforward
argument shows that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}%
\leq\beta-\alpha$ holds if and only if $\left(  j_{0},j_{1},j_{2},...\right)
$ is a $0$-degression satisfying $j_{0}\leq\beta$. Hence, we can replace the
sum sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}$ by $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2}%
,...\right)  \text{ is a }0\text{-degression;}\\j_{k}=-k\text{ for every
}k\geq K\text{;}\\j_{0}\leq\beta}}$ in
(\ref{pf.schur.fermi.skew.finitary.32.short}). Hence,
(\ref{pf.schur.fermi.skew.finitary.32.short}) becomes%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.35.short}%
\end{align}
But it is easily revealed that%
\begin{equation}
\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  =S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \label{pf.schur.fermi.skew.finitary.36.short}%
\end{equation}
for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and $j_{0}\leq\beta
$\ \ \ \ \footnote{The proof of (\ref{pf.schur.fermi.skew.finitary.36.short})
is completely straightforward and left to the reader. The ingredients of the
proof are the equality $A=\left(  S_{i-j}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$ (which we used to
define $A$), the definition of $\widetilde{i}_{v}$ (namely, $\widetilde{i}%
_{v}=\beta+1-i_{v-1}$ for every $v\in\left\{  1,2,...,K\right\}  $), and the
definition of the skew Schur function $S_{\lambda\diagup\mu}\left(  x\right)
$ as a determinant of a (finite!) matrix.}. Therefore,
(\ref{pf.schur.fermi.skew.finitary.35.short}) simplifies to%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge.... \label{pf.schur.fermi.skew.finitary.42.short}%
\end{align}

\end{vershort}

\begin{verlong}
But it is easy to see that%
\begin{equation}
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+... \label{pf.schur.fermi.skew.finitary.12}%
\end{equation}
as endomorphisms of $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $\ \ \ \ \footnote{\textit{Proof of
(\ref{pf.schur.fermi.skew.finitary.12}).} By the definition of $\rho
_{V_{\left]  \alpha,\beta\right]  },K}$, we know that $\rho_{V_{\left]
\alpha,\beta\right]  },K}:\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\operatorname*{End}\left(  \wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \right)  $ denotes the representation of the
Lie algebra $\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $
on the $K$-th exterior power of the defining representation $V_{\left]
\alpha,\beta\right]  }$ of $\mathfrak{gl}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Hence,
\[
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\mid_{\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  }.
\]
Hence, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]
}$ satisfy%
\begin{align*}
&  \left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right) \\
&  =\left(  \left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]
}^{i}\right)  \mid_{\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  }\right)  \left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)
\\
&  =\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  \rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi
_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{T_{\left]  \alpha,\beta\right]  }%
^{i}\rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)
}_{\substack{=\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}\wedge...\wedge
\xi_{k-1}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{i}\rightharpoonup
\xi_{k}\right)  \wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi
_{K}\\\text{(by the definition of the }\mathfrak{gl}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{-module }\wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{i}\rightharpoonup\xi_{k}\right)  }_{=T_{\left]  \alpha
,\beta\right]  }^{i}\xi_{k}}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge
\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}.
\end{align*}
On the other hand, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]
\alpha,\beta\right]  }$ satisfy%
\begin{align*}
&  \underbrace{\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  }%
_{=\sum\limits_{i\geq1}y_{i}a_{i}}\left(  \xi_{1}\wedge\xi_{2}\wedge
...\wedge\xi_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{a_{i}\left(  \xi_{1}\wedge\xi
_{2}\wedge...\wedge\xi_{K}\right)  }_{\substack{=\sum\limits_{k=1}^{K}\xi
_{1}\wedge\xi_{2}\wedge...\wedge\xi_{k-1}\wedge a_{i}\xi_{k}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\\text{(by the definition of the
}\mathcal{A}_{+}\text{-module }\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{a_{i}\xi_{k}}_{\substack{=T_{\left]
\alpha,\beta\right]  }^{i}\xi_{k}\\\text{(since the element }a_{i}\text{ of
}\mathcal{A}_{+}\text{ acts}\\\text{on }V_{\left]  \alpha,\beta\right]
}\text{ by }T_{\left]  \alpha,\beta\right]  }^{i}\text{)}}}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)  .
\end{align*}
In other words, the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$
and $\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq
1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ are equal to each other on the set
$\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1},\xi
_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $. Since the
set $\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1}%
,\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $ is a
spanning set of the vector space $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $, this yields the following: The two endomorphisms
$y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ and $\rho_{V_{\left]  \alpha
,\beta\right]  },K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $ are equal to each other on a spanning set of the
vector space $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $.
But when two linear maps from the same domain are equal to each other on a
spanning set of their domain, then these two maps must be identical. In
particular, if two endomorphisms of a vector space are equal to each other on
a spanning set of this vector space, then these two endomorphisms must be
identical. Applying this to the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...$ and $\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of
$\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, we conclude
that the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ and
$\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ are identical. This proves
(\ref{pf.schur.fermi.skew.finitary.12}).}. Hence,
(\ref{pf.schur.fermi.skew.finitary.11}) rewrites as%
\[
\wedge^{K}\left(  f\right)  =\exp\left(  \underbrace{\rho_{V_{\left]
\alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}\right)  }_{\substack{=y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\\\text{(by (\ref{pf.schur.fermi.skew.finitary.12}))}}}\right)
=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  .
\]
Hence,%
\begin{align}
&  \underbrace{\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  \right)  }_{=\wedge^{K}\left(  f\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\left(  \wedge^{K}\left(  f\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}%
}\det\left(  A_{j_{1},j_{2},...,j_{K}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{K}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.4})}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq\beta+1-j_{1}<\beta+1-j_{2}<...<\beta
+1-j_{K}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{1},\beta+1-j_{2}%
,...,\beta+1-j_{K}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}%
_{K}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{e_{\beta+1-j_{1}}\wedge
e_{\beta+1-j_{2}}\wedge...\wedge e_{\beta+1-j_{K}}}_{\substack{=v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\wedge v_{j_{K}}\\\text{(since every }p\in\left\{
1,2,...,K\right\}  \text{ satisfies }e_{\beta+1-j_{p}}=v_{j_{p}}\text{ (by
(\ref{pf.schur.fermi.skew.finitary.2a}), applied to }k=j_{p}\text{),}%
\\\text{so that }\left(  e_{\beta+1-j_{1}},e_{\beta+1-j_{2}},...,e_{\beta
+1-j_{K}}\right)  =\left(  v_{j_{1}},v_{j_{2}},...,v_{j_{K}}\right)  \text{
and thus}\\e_{\beta+1-j_{1}}\wedge e_{\beta+1-j_{2}}\wedge...\wedge
e_{\beta+1-j_{K}}=v_{j_{1}}\wedge v_{j_{2}}\wedge...\wedge v_{j_{K}}\text{)}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(
\beta+1-j_{1},\beta+1-j_{2},...,\beta+1-j_{K}\right)  \text{ for }\left(
j_{1},j_{2},...,j_{K}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq\beta+1-j_{1}<\beta+1-j_{2}<...<\beta
+1-j_{K}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{1},\beta+1-j_{2}%
,...,\beta+1-j_{K}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}%
_{K}}\right)  v_{j_{1}}\wedge v_{j_{2}}\wedge...\wedge v_{j_{K}}\nonumber\\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\label{pf.schur.fermi.skew.finitary.22}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed }\left(  j_{1}%
,j_{2},...,j_{K}\right)  \text{ as }\left(  j_{0},j_{1},...,j_{K-1}\right)
\right)  .\nonumber
\end{align}
But every $K$-tuple $\left(  j_{0},j_{1},...,j_{K-1}\right)  $ of integers
such that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha$ satisfies%
\begin{align}
&  \sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }j_{K+2}\text{,
}...\text{ are integers;}\\j_{k}=-k\text{ for every }k\geq K}}\det\left(
A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right) \nonumber\\
&  =\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\label{pf.schur.fermi.skew.finitary.23}%
\end{align}
(since the sum $\sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }%
j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for every }k\geq
K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  $ has
only one addend). Thus, (\ref{pf.schur.fermi.skew.finitary.22}) becomes%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\underbrace{\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  }_{=\sum\limits_{\substack{j_{K}\text{,
}j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for
every }k\geq K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta
+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}%
}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\\
&  =\underbrace{\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{,
}j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\sum\limits_{\substack{j_{K}%
\text{, }j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}%
\\j_{k}=-k\text{ for every }k\geq K}}}_{=\sum\limits_{\substack{j_{0}\text{,
}j_{1}\text{, }j_{2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for
every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta
+1-j_{K-1}\leq\beta-\alpha}}=\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for
every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta
+1-j_{K-1}\leq\beta-\alpha}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}},
\end{align*}
so that
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =R_{K,\left]  \alpha,\beta\right]  }\left(  \sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}%
\\j_{k}=-k\text{ for every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{R_{K,\left]
\alpha,\beta\right]  }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\right)  }_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge v_{\alpha-2}%
\wedge...\\\text{(by the definition of }R_{K,\left]  \alpha,\beta\right]
}\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }R_{K,\left]  \alpha,\beta\right]
}\text{ is linear}\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge
v_{\alpha-2}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\wedge v_{-K}\wedge v_{-K-1}\wedge
v_{-K-2}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\\\text{(since every }k\geq K\text{ satisfies }-k=j_{k}\text{,
hence}\\\left(  -K,-K-1,-K-2,...\right)  =\left(  j_{K},j_{K+1},j_{K+2}%
,...\right)  \text{ and thus}\\\left(  j_{0},j_{1},...,j_{K-1}%
,-K,-K-1,-K-2,...\right)  =\left(  j_{0},j_{1},...,j_{K-1},j_{K}%
,j_{K+1},j_{K+2},...\right)  =\left(  j_{0},j_{1},j_{2},...\right)
\\\text{and thus }v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\wedge
v_{-K}\wedge v_{-K-1}\wedge v_{-K-2}\wedge...=v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha=i_{K}=-K\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.32}%
\end{align}
Let us now notice that when $\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}$ is a sequence of integers satisfying $\left(
j_{k}=-k\text{ for every }k\geq K\right)  $, then $1\leq\beta+1-j_{0}%
<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha$ holds if and only if
$\left(  j_{0},j_{1},j_{2},...\right)  $ is a $0$-degression satisfying
$j_{0}\leq\beta$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  j_{0}%
,j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}$ be a sequence of integers
satisfying $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $. We need to
prove the following two assertions:
\par
\textit{Assertion 1:} If $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta
+1-j_{K-1}\leq\beta-\alpha$, then $\left(  j_{0},j_{1},j_{2},...\right)  $ is
a $0$-degression satisfying $j_{0}\leq\beta$.
\par
\textit{Assertion 2:} If $\left(  j_{0},j_{1},j_{2},...\right)  $ is a
$0$-degression satisfying $j_{0}\leq\beta$, then $1\leq\beta+1-j_{0}%
<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha$.
\par
\textit{Proof of Assertion 1:} Assume that $1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha$. Subtracting this chain of
inequalities from $\beta+1$, we obtain%
\[
\beta+1-1\geq\beta+1-\left(  \beta+1-j_{0}\right)  >\beta+1-\left(
\beta+1-j_{1}\right)  >...>\beta+1-\left(  \beta+1-j_{K-1}\right)  \geq
\beta+1-\left(  \beta+\alpha\right)  .
\]
This simplifies to $\beta\geq j_{0}>j_{1}>...>j_{K-1}\geq\alpha+1$ (since
$\beta+1-1=\beta$, since $\beta+1-\left(  \beta+1-j_{p}\right)  =j_{p}$ for
every $p\in\left\{  0,1,...,K-1\right\}  $, and since $\beta+1-\left(
\beta+\alpha\right)  =\alpha+1$). Thus, $\beta\geq j_{0}$, so that $j_{0}%
\leq\beta$. Also $-K>-\left(  K+1\right)  >-\left(  K+2\right)  >...$. Since
$j_{k}=-k$ for every $k\geq K$, this rewrites as $j_{K}>j_{K+1}>j_{K+2}>...$.
Also, since $j_{k}=-k$ for every $k\geq K$, we have $j_{K}=-K$. Compared with
$i_{K}=-K$, this yields $j_{K}=i_{K}=\alpha$. Thus, $j_{K-1}\geq
\alpha+1>\alpha=j_{K}$. Combined with $j_{0}>j_{1}>...>j_{K-1}$, this yields
$j_{0}>j_{1}>...>j_{K}$. Combined with $j_{K}>j_{K+1}>j_{K+2}>...$, this
yields $j_{0}>j_{1}>j_{2}>...$. Thus, $\left(  j_{0},j_{1},j_{2},...\right)  $
is a strictly decreasing sequence of integers. Since we know that $\left(
j_{k}=-k\text{ for every }k\geq K\right)  $, this yields that $\left(
j_{0},j_{1},j_{2},...\right)  $ is a $0$-degression. Recall also that
$j_{0}\leq\beta$. This proves Assertion 1.
\par
\textit{Proof of Assertion 2:} Assume that $\left(  j_{0},j_{1},j_{2}%
,...\right)  $ is a $0$-degression satisfying $j_{0}\leq\beta$. Since $\left(
j_{0},j_{1},j_{2},...\right)  $ is a $0$-degression, we have $j_{0}%
>j_{1}>...>j_{K-1}>j_{K}$.
\par
Recall that $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $. Applied to
$k=K$, this yields $j_{K}=-K$. Compared with $i_{K}=-K$, this yields
$j_{K}=i_{K}=\alpha$. Thus, $j_{K-1}>j_{K}=\alpha$. Since $j_{K-1}$ and
$\alpha$ are integers, this yields $j_{K-1}\geq\alpha+1$. Combined with
$j_{0}>j_{1}>...>j_{K-1}$, this becomes $j_{0}>j_{1}>...>j_{K-1}\geq\alpha+1$.
Combined with $\beta\geq j_{0}$ (since $j_{0}\leq\beta$), this becomes
$\beta\geq j_{0}>j_{1}>...>j_{K-1}\geq\alpha+1$. Subtracting this chain of
inequalities from $\beta+1$, we obtain $\beta+1-1\leq\beta+1-j_{0}%
<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta+1-\left(  \alpha+1\right)  $.
Since $\beta+1-1=\beta$ and $\beta+1-\left(  \alpha+1\right)  =\beta-\alpha$,
this simplifies to $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}%
\leq\beta-\alpha$. This proves Assertion 2.
\par
Now, both Assertions 1 and 2 are proven. Combining these two assertions, we
conclude that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha$ holds if and only if $\left(  j_{0},j_{1},j_{2},...\right)  $ is
a $0$-degression satisfying $j_{0}\leq\beta$, qed.}. Hence, we can replace the
sum sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}$ by $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2}%
,...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every
}k\geq K\text{;}\\\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression such that }j_{0}\leq\beta}}$ in
(\ref{pf.schur.fermi.skew.finitary.32}). Hence,
(\ref{pf.schur.fermi.skew.finitary.32}) becomes%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\underbrace{\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}}_{=\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a }%
0\text{-degression such that }j_{0}\leq\beta}}=\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\j_{k}%
=-k\text{ for every }k\geq K\text{;}\\j_{0}\leq\beta}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.35}%
\end{align}
But it is easily revealed that%
\begin{equation}
\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  =S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \label{pf.schur.fermi.skew.finitary.36}%
\end{equation}
for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and $j_{0}\leq\beta
$\ \ \ \ \footnote{\textit{Proof of (\ref{pf.schur.fermi.skew.finitary.36}):}
Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be a $0$-degression satisfying
$\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and $j_{0}\leq\beta$. By
the definition of the matrix $A$, we have $A=\left(  S_{i-j}\left(  y\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}%
}$. Hence,%
\begin{align*}
&  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\\
&  =\left(  S_{\left(  \beta+1-j_{u-1}\right)  -\widetilde{i}_{v}}\left(
y\right)  \right)  _{\left(  u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}%
}=\left(  S_{\left(  \beta+1-j_{u-1}\right)  -\left(  \beta+1-i_{v-1}\right)
}\left(  y\right)  \right)  _{\left(  u,v\right)  \in\left\{
1,2,...,K\right\}  ^{2}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{i}_{v}=\beta
+1-i_{v-1}\text{ (by the definition of }\widetilde{i}_{v}\text{) for every
}v\in\left\{  1,2,...,K\right\}  \right) \\
&  =\left(  S_{i_{v-1}-j_{u-1}}\left(  y\right)  \right)  _{\left(
u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  \beta+1-j_{u-1}\right)
-\left(  \beta+1-i_{v-1}\right)  =i_{v-1}-j_{u-1}\text{ for every }\left(
u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}\right)  .
\end{align*}
\par
Now, define two partitions $\lambda$ and $\mu$ by $\lambda=\left(
i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(  j_{k}+k\right)  _{k\geq0}$. Write
the partitions $\lambda$ and $\mu$ in the forms $\lambda=\left(  \lambda
_{1},\lambda_{2},\lambda_{3},...\right)  $ and $\mu=\left(  \mu_{1},\mu
_{2},\mu_{3},...\right)  $. Then, $\lambda_{v}=i_{v-1}+\left(  v-1\right)  $
for every $v\in\left\{  1,2,3,...\right\}  $, and $\mu_{u}=j_{u-1}+\left(
u-1\right)  $ for every $u\in\left\{  1,2,3,...\right\}  $. Thus, for every
$\left(  u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}$, we have%
\begin{equation}
\underbrace{\lambda_{v}}_{=i_{v-1}+\left(  v-1\right)  }-\underbrace{\mu_{u}%
}_{=j_{u-1}+\left(  u-1\right)  }+u-v=\left(  i_{v-1}+\left(  v-1\right)
\right)  -\left(  j_{u-1}+\left(  u-1\right)  \right)  +u-v=i_{v-1}-j_{u-1}.
\label{pf.schur.fermi.skew.finitary.37}%
\end{equation}
\par
But every integer $v\geq K+1$ satisfies $\lambda_{v}=i_{v-1}+\left(
v-1\right)  =0$ (because every integer $v\geq K+1$ satisfies $v-1\geq K$, and
therefore $i_{v-1}+\left(  v-1\right)  =0$ (due to the fact that $\left(
i_{k}+k=0\text{ for every }k\geq K\right)  $, applied to $k=v-1$). Hence, the
partition $\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ can be
written in the form $\left(  \lambda_{1},\lambda_{2},...,\lambda_{K}\right)
$.
\par
Also, every integer $u\geq K+1$ satisfies $\mu_{u}=j_{u-1}+\left(  u-1\right)
=0$ (because every integer $u\geq K+1$ satisfies $u-1\geq K$, and therefore
$j_{u-1}+\left(  u-1\right)  =0$ (due to the fact that $\left(  j_{k}%
+k=0\text{ for every }k\geq K\right)  $, applied to $k=u-1$). Hence, the
partition $\left(  \mu_{1},\mu_{2},\mu_{3},...\right)  $ can be written in the
form $\left(  \mu_{1},\mu_{2},...,\mu_{K}\right)  $.
\par
Since $\lambda=\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)
=\left(  \lambda_{1},\lambda_{2},...,\lambda_{K}\right)  $ and $\mu=\left(
\mu_{1},\mu_{2},\mu_{3},...\right)  =\left(  \mu_{1},\mu_{2},...,\mu
_{K}\right)  $, the definition of $S_{\lambda\diagup\mu}\left(  x\right)  $
yields $S_{\lambda\diagup\mu}\left(  x\right)  =\det\left(
\underbrace{\left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)
_{1\leq i\leq K,\ 1\leq j\leq K}}_{=\left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(
x\right)  \right)  _{\left(  i,j\right)  \in\left\{  1,2,...,K\right\}  ^{2}}%
}\right)  =\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,...,K\right\}  ^{2}}\right)
=\left(  \left(  S_{\lambda_{v}-\mu_{u}+u-v}\left(  y\right)  \right)
_{\left(  v,u\right)  \in\left\{  1,2,...,K\right\}  ^{2}}\right)  $ (here, we
substituted $\left(  v,u\right)  $ for $\left(  i,j\right)  $). Substituting
$y$ for $x$ in this equality, we obtain%
\begin{align*}
S_{\lambda\diagup\mu}\left(  y\right)   &  =\det\left(  \left(  S_{\lambda
_{v}-\mu_{u}+u-v}\left(  y\right)  \right)  _{\left(  v,u\right)  \in\left\{
1,2,...,K\right\}  ^{2}}\right) \\
&  =\det\left(  \underbrace{\left(  S_{i_{v-1}-j_{u-1}}\left(  y\right)
\right)  _{\left(  v,u\right)  \in\left\{  1,2,...,K\right\}  ^{2}}%
}_{=A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.fermi.skew.finitary.37}%
)}\right) \\
&  =\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  .
\end{align*}
Thus, $\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
=S_{\lambda\diagup\mu}\left(  y\right)  =S_{\left(  i_{k}+k\right)  _{k\geq
0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)  $ (since
$\lambda=\left(  i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(  j_{k}+k\right)
_{k\geq0}$). This proves (\ref{pf.schur.fermi.skew.finitary.36}).}. Therefore,
(\ref{pf.schur.fermi.skew.finitary.35}) becomes%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}\underbrace{\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1}%
,...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}%
_{K}}\right)  }_{\substack{=S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \\\text{(by
(\ref{pf.schur.fermi.skew.finitary.36}))}}}v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge.... \label{pf.schur.fermi.skew.finitary.42}%
\end{align}

\end{verlong}

But Proposition \ref{prop.finitary.Valphabeta.R} (applied to $K$ instead of
$\ell$) yields that $R_{K,\left]  \alpha,\beta\right]  }:\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  \rightarrow\mathcal{F}^{\left(
\alpha+K\right)  }$ is an $\mathcal{A}_{+}$-module homomorphism. Since
$\underbrace{\alpha}_{=i_{K}=-K}+K=-K+K=0$, this rewrites as follows:
$R_{K,\left]  \alpha,\beta\right]  }:\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  \rightarrow\mathcal{F}^{\left(  0\right)  }$ is an
$\mathcal{A}_{+}$-module homomorphism.

\begin{vershort}
Now, $\mathcal{A}_{+}$ is a graded Lie subalgebra of $\mathcal{A}$, and it is
easy to define a grading on the $\mathcal{A}_{+}$-module $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ such that $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ is concentrated in nonpositive
degrees.\footnote{Indeed, let us define a grading on the vector space
$V_{\left]  \alpha,\beta\right]  }$ by setting the degree of $v_{i}$ to be
$\alpha+1-i$ for every $i\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $.
Then, the vector space $V_{\left]  \alpha,\beta\right]  }$ is concentrated in
nonpositive degrees, so that its $K$-th exterior power $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ is also concentrated in
nonpositive degrees. On the other hand, $V_{\left]  \alpha,\beta\right]  }$ is
a graded $\mathcal{A}_{+}$-module (this is very easy to check), so that its
$K$-th exterior power $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $ is also a graded $\mathcal{A}_{+}$-module.} Thus, applying
Proposition \ref{prop.schur.fermi.welldef.A+} \textbf{(b)} to $\mathbb{C}$,
$\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, $\mathcal{F}%
^{\left(  0\right)  }$ and $R_{K,\left]  \alpha,\beta\right]  }$ instead of
$\mathbf{R}$, $M$, $N$ and $\eta$, we obtain%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ R_{K,\left]  \alpha,\beta\right]  }=R_{K,\left]  \alpha,\beta\right]
}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)
\]
as maps from $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $ to
$\mathcal{F}^{\left(  0\right)  }$. Hence,%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  R_{K,\left]  \alpha,\beta\right]  }\circ\left(  \exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.42.short})}\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\underbrace{\left(  R_{K,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right)  }_{\substack{=v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\\\text{(by
(\ref{pf.finitary.Valphabeta.R.8}))}}}\\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\end{align*}
this becomes%
\begin{align}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge... \label{pf.schur.fermi.skew.finitary.48.short}%
\end{align}
(here, we deprived the sum of all addends for which $\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}$, because
(\ref{pf.schur.fermi.notcontained}) shows that all such addends are $0$).
\end{vershort}

\begin{verlong}
Let us define a grading on the vector space $V_{\left]  \alpha,\beta\right]
}$ by setting the degree of $v_{i}$ to be $\alpha+1-i$ for every $i\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $. Then, the vector space $V_{\left]
\alpha,\beta\right]  }$ is concentrated in nonpositive degrees, so that its
$K$-th exterior power $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $ is also concentrated in nonpositive degrees. On the other hand,
$\mathcal{A}_{+}$ is a graded Lie subalgebra of $\mathcal{A}$, and $V_{\left]
\alpha,\beta\right]  }$ is a graded $\mathcal{A}_{+}$-module (this is very
easy to check), so that its $K$-th exterior power $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ is also a graded $\mathcal{A}_{+}$-module.

Now, applying Proposition \ref{prop.schur.fermi.welldef.A+} \textbf{(b)} to
$\mathbb{C}$, $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $,
$\mathcal{F}^{\left(  0\right)  }$ and $R_{K,\left]  \alpha,\beta\right]  }$
instead of $\mathbf{R}$, $M$, $N$ and $\eta$, we obtain%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ R_{K,\left]  \alpha,\beta\right]  }=R_{K,\left]  \alpha,\beta\right]
}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)
\]
as maps from $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $ to
$\mathcal{F}^{\left(  0\right)  }$. Hence,%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  R_{K,\left]  \alpha,\beta\right]  }\circ\left(  \exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.42})}\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\underbrace{\left(  R_{K,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right)  }_{\substack{=v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\\\text{(by
(\ref{pf.finitary.Valphabeta.R.8}))}}}\\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\end{align*}
this becomes%
\begin{align}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for
every }k\geq K\text{;}\\j_{0}\leq\beta}}\underbrace{S_{\left(  i_{k}+k\right)
_{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...}_{\substack{=0\\\text{(by
(\ref{pf.schur.fermi.notcontained}), since }\left(  j_{0},j_{1},j_{2}%
,...\right)  \text{ is a }0\text{-degression}\\\text{satisfying }\left(
j_{k}+k\right)  _{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq
0}\text{)}}}\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...+\underbrace{\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq
0}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}\leq\beta}}0}%
_{=0}\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge.... \label{pf.schur.fermi.skew.finitary.48}%
\end{align}

\end{verlong}

\begin{vershort}
But for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$,
we automatically have $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and
$j_{0}\leq\beta$ (this is very easy to see). Hence, we can replace the
summation sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\text{ is a }0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}%
\subseteq\left(  i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\j_{0}\leq\beta}}$ on the right hand side of
(\ref{pf.schur.fermi.skew.finitary.48.short}) by a $\sum
\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}$ sign. Thus,
(\ref{pf.schur.fermi.skew.finitary.48.short}) simplifies to%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
This proves Theorem \ref{thm.schur.fermi.skew}.
\end{vershort}

\begin{verlong}
But for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$,
we automatically have $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and
$j_{0}\leq\beta$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  j_{0}%
,j_{1},j_{2},...\right)  $ be a $0$-degression satisfying $\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$. Since
$\left(  j_{k}+k\right)  _{k\geq0}$ is a partition, every $k\geq0$ satisfies
$j_{k}+k\geq0$.
\par
Now recall that $\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}$. In other words, $j_{k}+k\leq i_{k}+k$ for every
$k\geq0$. In other words, $j_{k}\leq i_{k}$ for every $k\geq0$. Applied to
$k=0$, this yields $j_{0}\leq i_{0}=\beta$.
\par
Now, let $k\in\mathbb{N}$ satisfy $k\geq K$. Then, $j_{k}\leq i_{k}$ (since
$k\geq0$) and $i_{k}=-k$ (since $k\geq K$). Hence, $j_{k}\leq i_{k}=-k$.
Combined with $j_{k}\geq-k$ (since $j_{k}+k=0$), this yields $j_{k}=-k$. Now
forget that we fixed $k$. We thus have proven that $\left(  j_{k}=-k\text{ for
every }k\geq K\right)  $.
\par
Altogether, we now know that $\left(  j_{k}=-k\text{ for every }k\geq
K\right)  $ and $j_{0}\leq\beta$, qed.}. Hence, we can replace the summation
sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}$ on the right hand side of
(\ref{pf.schur.fermi.skew.finitary.48}) by a $\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}}}$ sign.
Thus, (\ref{pf.schur.fermi.skew.finitary.48}) becomes%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\text{ is a }0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}%
\subseteq\left(  i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\j_{0}\leq\beta}}}_{=\sum\limits_{\substack{\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}}%
}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq
0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
This proves Theorem \ref{thm.schur.fermi.skew}.
\end{verlong}

\subsection{Applications to integrable systems}

Let us show how these things can be applied to partial differential equations.

\begin{Convention}
If $v$ is a function in several variables $x_{1}$, $x_{2}$, $...$, $x_{k}$,
then, for every $i\in\left\{  1,2,...,k\right\}  $, the derivative of $v$ by
the variable $x_{i}$ will be denoted by $\partial_{x_{i}}v$ and by $v_{x_{i}}%
$. In other words, $\partial_{x_{i}}v=v_{x_{i}}=\dfrac{\partial}{\partial
x_{i}}v$. (For example, if $v$ is a function in two variables $x$ and $t$,
then $v_{t}$ will mean the derivative of $v$ by $t$.)
\end{Convention}

The PDE (partial differential equation) we will be concerned with is the
\textbf{Korteweg-de Vries equation} (abbreviated as \textbf{KdV equation}%
)\textbf{:} This is the equation $u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}%
u_{xxx}$ for a function $u\left(  t,x\right)  $.\ \ \ \ \footnote{There seems
to be no consistent definition of the KdV equation across literature. We
defined the KdV equation as $u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}$
because this is the form most suited to our approach. Some other authors,
instead, define the KdV equation as $v_{t}=v_{xxx}+6vv_{x}$ for a function
$v\left(  t,x\right)  $. Others define it as $w_{t}+ww_{x}+w_{xxx}=0$ for a
function $w\left(  t,x\right)  $. Yet others define it as $q_{t}%
+q_{xxx}+6qq_{x}=0$ for a function $q\left(  t,x\right)  $. These equations
are not literally equivalent, but can be transformed into each other by very
simple substitutions. In fact, for a function $u\left(  t,x\right)  $, we have
the following equivalence of assertions:%
\begin{align*}
&  \ \left(  \text{the function }u\left(  t,x\right)  \text{ satisfies the
equation }u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }v\left(  t,x\right)
:=u\left(  4t,x\right)  \text{ satisfies the equation }v_{t}=v_{xxx}%
+6vv_{x}\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }w\left(  t,x\right)
:=6u\left(  -4t,x\right)  \text{ satisfies the equation }w_{t}+ww_{x}%
+w_{xxx}=0\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }q\left(  t,x\right)
:=u\left(  -4t,x\right)  \text{ satisfies the equation }q_{t}+q_{xxx}%
+6qq_{x}=0\right)  .
\end{align*}
}

We will discuss several interesting solutions of this equation. Here is the
most basic family of solutions:%
\[
u\left(  t\right)  =\dfrac{2a^{2}}{\cosh^{2}\left(  a\left(  x+a^{2}t\right)
\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{for }a\text{ being arbitrary but
fixed}\right)  .
\]
These are so-called ``traveling wave solutions''. It is a peculiar kind of
wave: it has only one bump; it is therefore called a \textit{soliton} (or
\textit{solitary wave}). Such waves never occur in linear systems. Note that
when we speak of ``wave'', we are imagining a time-dependent 2-dimensional
graph with the x-axis showing $t$, the y-axis showing $u\left(  t\right)  $,
and the time parameter being $x$. So when we speak of ``traveling wave'', we
mean that it is a wave for any fixed time $x$ and ``travels'' when $x$ moves.

The first to study this kind of waves was J. S. Russell in 1834, describing
the motion of water in a shallow canal (tsunami waves are similar). The first
models for these waves were found by Korteweg-de Vries in 1895.

The term $\dfrac{1}{4}u_{xxx}$ in the Korteweg-de Vries equation $u_{t}%
=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}$ is called the \textit{dispersion
term}.

\textbf{Exercise:} Solve the equation $u_{t}=\dfrac{3}{2}uu_{x}$. (Note that
the waves solving this equation develop shocks, in contrast to those solving
the Korteweg-de Vries equation.)

The Korteweg-de Vries equation is famous for having lots of explicit solutions
(unexpectedly for a nonlinear partial differential equation). We will
construct some of them using infinite-dimensional Lie algebras. (There are
many other ways to construct solutions. In some sense, every field of
mathematics is related to some of its solutions.)

We will also study the \textbf{Kadomtsev-Petviashvili equation} (abbreviated
as \textbf{KP equation})
\[
u_{yy}=\left(  u_{t}-\dfrac{3}{2}uu_{x}-\dfrac{1}{4}u_{xxx}\right)  _{x}%
\]
(or, after some rescaling, $\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(
\partial_{t}u-\dfrac{3}{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)
$) on a function $u\left(  t,x,y\right)  $. We will obtain functions which
solve this equation (among others).

We are going to use the \textit{infinite Grassmannian} for this. First, recall
what the \textit{finite Grassmannian} is:

\subsubsection{The finite Grassmannian}

\begin{definition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. Let $V$ be the
$\mathbb{C}$-vector space $\mathbb{C}^{n}$. Let $\left(  v_{1},v_{2}%
,...,v_{n}\right)  $ be the standard basis of $\mathbb{C}^{n}$. Recall that
$\wedge^{k}V$ is a representation of $\operatorname*{GL}\left(  V\right)  $
with a highest-weight vector $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Denote
by $\Omega$ the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $.
\end{definition}

\begin{proposition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. We have $\Omega
=\left\{  x\in\wedge^{k}V\text{ nonzero}\ \mid\ x=x_{1}\wedge x_{2}%
\wedge...\wedge x_{k}\text{ for some }x_{i}\in V\right\}  $. Also,
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\neq0$ if and only if $x_{1}$, $x_{2}$,
$...$, $x_{k}$ are linearly independent.
\end{proposition}

\textit{Proof.} Very easy.

\begin{definition}
Let $V$ be a $\mathbb{C}$-vector space. Let $k$ be a nonnegative integer. The
$k$\textit{-Grassmannian of }$V$ is defined to be the set of all
$k$-dimensional vector subspaces of $V$. This set is denoted by
$\operatorname*{Gr}\left(  k,V\right)  $.
\end{definition}

When $V$ is a finite-dimensional $\mathbb{C}$-vector space, there is a way to
define the structure of a projective variety on the Grassmannian
$\operatorname*{Gr}\left(  k,V\right)  $. While we won't ever need the
existence of this structure, we will need the so-called Pl\"{u}cker embedding
which is the main ingredient in defining this structure:\footnote{In the
following definition (and further below), we use the notation $\mathbb{P}%
\left(  W\right)  $ for the projective space of a $\mathbb{C}$-vector space
$W$. This projective space is defined to be the quotient set $\left(  W
\setminus\left\{  0\right\}  \right)  / \sim$, where $\sim$ is the
proportionality relation (i.e., two vectors $w_{1}$ and $w_{2}$ in $W
\setminus\left\{  0\right\}  $ satisfy $w_{1} \sim w_{2}$ if and only if they
are linearly dependent).}

\begin{definition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. Let $V$ be the
$\mathbb{C}$-vector space $\mathbb{C}^{n}$. The \textit{Pl\"{u}cker embedding}
(corresponding to $n$ and $k$) is defined as the map%
\begin{align*}
\operatorname*{Pl}:\operatorname*{Gr}\left(  k,V\right)   &  \rightarrow
\mathbb{P}\left(  \wedge^{k}V\right)  ,\\
\left(
\begin{array}
[c]{c}%
k\text{-dimensional subspace of }V\\
\text{with basis }x_{1},x_{2},...,x_{k}%
\end{array}
\right)   &  \mapsto\left(
\begin{array}
[c]{c}%
\text{projection of}\\
x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge^{k}V\diagdown\left\{
0\right\} \\
\text{on }\mathbb{P}\left(  \wedge^{k}V\right)
\end{array}
\right)  .
\end{align*}
It is easy to see that this is well-defined (i. e., that the projection of
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge^{k}V\diagdown\left\{
0\right\}  $ on $\mathbb{P}\left(  \wedge^{k}V\right)  $ does not depend on
the choice of basis $x_{1},x_{2},...,x_{k}$). The image of this map is
$\operatorname*{Im}\operatorname*{Pl}=\Omega\diagup\left(  \text{scalars}%
\right)  $.
\end{definition}

\begin{proposition}
\label{prop.plu.injective}This map $\operatorname*{Pl}$ is injective.
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.injective}.} Proving Proposition
\ref{prop.plu.injective} boils down to showing that if $\lambda$ is a complex
number and $v_{1}$, $v_{2}$, $...$, $v_{k}$, $w_{1}$, $w_{2}$, $...$, $w_{k}$
are any vectors in a vector space $U$ satisfying $v_{1}\wedge v_{2}%
\wedge...\wedge v_{k}=\lambda\cdot w_{1}\wedge w_{2}\wedge...\wedge w_{k}%
\neq0$, then the vector subspace of $U$ spanned by the vectors $v_{1}$,
$v_{2}$, $...$, $v_{k}$ is identical with the vector subspace of $U$ spanned
by the vectors $w_{1}$, $w_{2}$, $...$, $w_{k}$. This is a well-known fact.
The details are left to the reader.

Thus, $\operatorname*{Gr}\left(  k,V\right)  \cong\Omega\diagup\left(
\text{scalars}\right)  $. (For algebraic geometers: $\Omega$ is the total
space of the determinant bundle on $\operatorname*{Gr}\left(  k,V\right)  $
(but only the nonzero elements).)

We are now going to describe the image $\operatorname*{Im}\operatorname*{Pl}$
by algebraic equations. These equations go under the name \textit{Pl\"{u}cker
relations}.

First, we define (in analogy to Definition \ref{def.createdestroy})
``wedging'' and ``contraction'' operators on the exterior algebra of $V$:

\begin{definition}
\label{def.createdestroy.fin}Let $n\in\mathbb{N}$. Let $k\in\mathbb{Z}$. Let
$V$ be the vector space $\mathbb{C}^{n}$. Let $\left(  v_{1},v_{2}%
,...,v_{n}\right)  $ be the standard basis of $V$. Let $i\in\left\{
1,2,...,n\right\}  $.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ by%
\[
\widehat{v_{i}}\cdot\psi=v_{i}\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all
}\psi\in\wedge^{k}V.
\]


\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k-1}V$ as follows:

For every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$, we let $\overset{\vee}{v_{i}%
}\left(  v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{k}}\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}
;\\
\left(  -1\right)  ^{j-1}v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{j-1}%
}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{1},i_{2},...,i_{k}\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell}=i$. Thus, the map
$\overset{\vee}{v_{i}}$ is defined on a basis of the vector space $\wedge
^{k}V$; we extend this to a map $\wedge^{k}V\rightarrow\wedge^{k-1}V$ by linearity.

Note that, for every negative $\ell\in\mathbb{Z}$, we understand $\wedge
^{\ell}V$ to mean the zero space.
\end{definition}

Now we can formulate the Pl\"{u}cker relations as follows:

\begin{theorem}
\label{thm.plu}Let $n\in\mathbb{N}$. Let $k\in\mathbb{Z}$. We consider the
vector space $V=\mathbb{C}^{n}$ with its standard basis $\left(  v_{1}%
,v_{2},...,v_{n}\right)  $. Let $S=\sum\limits_{i=1}^{n}\widehat{v_{i}}%
\otimes\overset{\vee}{v_{i}}:\wedge^{k}V\otimes\wedge^{k}V\rightarrow
\wedge^{k+1}V\otimes\wedge^{k-1}V$.

\textbf{(a)} This map $S$ does not depend on the choice of the basis and is
$\operatorname*{GL}\left(  V\right)  $-invariant\footnotemark. In other words,
for \textbf{any} basis $\left(  w_{1},w_{2},...,w_{n}\right)  $ of $V$, we
have $S=\sum\limits_{i=1}^{n}\widehat{w_{i}}\otimes\overset{\vee}{w_{i}}$
(where the maps $\widehat{w_{i}}$ and $\overset{\vee}{w_{i}}$ are defined just
as $\widehat{v_{i}}$ and $\overset{\vee}{v_{i}}$, but with respect to the
basis $\left(  w_{1},w_{2},...,w_{n}\right)  $).

\textbf{(b)} Let $k\in\left\{  1,2,...,n\right\}  $. A nonzero element
$\tau\in\wedge^{k}V$ belongs to $\Omega$ if and only if $S\left(  \tau
\otimes\tau\right)  =0$.

\textbf{(c)} The map $S$ is $\operatorname*{M}\left(  V\right)  $-invariant.
(Here, $\operatorname*{M}\left(  V\right)  $ denotes the multiplicative monoid
of all endomorphisms of $V$.)
\end{theorem}

\footnotetext{The word ``$\operatorname*{GL}\left(  V\right)  $-invariant''
here means ``invariant under the action of $\operatorname*{GL}\left(
V\right)  $ on the space of all linear operators $\wedge^{k}V\otimes\wedge
^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$''. So, for an operator from
$\wedge^{k}V\otimes\wedge^{k}V$ to $\wedge^{k+1}V\otimes\wedge^{k-1}V$ to be
$\operatorname*{GL}\left(  V\right)  $-invariant means the same as for it to
be $\operatorname*{GL}\left(  V\right)  $-equivariant.}Part \textbf{(b)} of
this theorem is what is actually called the Pl\"{u}cker relations, although it
is not how these relations are usually formulated in literature. For a more
classical formulation, see Theorem \ref{thm.plu.coo}. Of course, Theorem
\ref{thm.plu} \textbf{(b)} not only shows when an element of $\wedge^{k}V$
belongs to $\Omega$, but also shows when an element of $\mathbb{P}\left(
\wedge^{k}V\right)  $ lies in $\operatorname*{Im}\operatorname*{Pl}$ (because
an element of $\mathbb{P}\left(  \wedge^{k}V\right)  $ is an equivalence class
of elements of $\wedge^{k}V\diagdown\left\{  0\right\}  $, and lies in
$\operatorname*{Im}\operatorname*{Pl}$ if and only if its representatives lie
in $\Omega$).

\textit{Proof of Theorem \ref{thm.plu}.} Before we start proving the theorem,
let us introduce some notations.

First of all, for every basis $\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V$,
let $\left(  e_{1}^{\ast},e_{2}^{\ast},...,e_{n}^{\ast}\right)  $ denote its
dual basis (this is a basis of $V^{\ast}$).

Next, for any element $v\in V$ we define the so called $v$\textit{-wedging
operator} $\widehat{v}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ by%
\[
\widehat{v}\cdot\psi=v\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all }\psi
\in\wedge^{k}V.
\]
Of course, this definition does not conflict with Definition
\ref{def.createdestroy.fin} \textbf{(a)}. (In fact, for every $i\in\left\{
1,2,...,n\right\}  $, the $v_{i}$-wedging operator that we just defined is
exactly identical with the $i$-th wedging operator defined in Definition
\ref{def.createdestroy.fin} \textbf{(a)}, and hence there is no harm from
denoting both of them by $\widehat{v_{i}}$.)

Further, for any $f\in V^{\ast}$, we define the so called $f$%
\textit{-contraction operator} $\overset{\vee}{f}:\wedge^{k}V\rightarrow
\wedge^{k-1}V$ by%
\begin{align*}
\overset{\vee}{f}\cdot\left(  u_{1}\wedge u_{2}\wedge...\wedge u_{k}\right)
&  =\sum\limits_{i=1}^{k}\left(  -1\right)  ^{i-1}f\left(  u_{i}\right)  \cdot
u_{1}\wedge u_{2}\wedge...\wedge u_{i-1}\wedge u_{i+1}\wedge u_{i+2}%
\wedge...\wedge u_{k}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{1},u_{2},...,u_{k}\in V.
\end{align*}
\footnote{In order to prove that this is well-defined, we need to check that
the term $\sum\limits_{i=1}^{k}\left(  -1\right)  ^{i-1}f\left(  u_{i}\right)
\cdot u_{1}\wedge u_{2}\wedge...\wedge u_{i-1}\wedge u_{i+1}\wedge
u_{i+2}\wedge...\wedge u_{k}$ depends multilinearly and antisymmetrically on
$u_{1},u_{2},...,u_{k}$. This is easy and left to the reader.} These
contraction operators are connected to the contraction operators defined in
Definition \ref{def.createdestroy.fin} \textbf{(b)}: Namely, $\overset{\vee
}{v_{i}}=\overset{\vee}{v_{i}^{\ast}}$ for every $i\in\left\{
1,2,...,n\right\}  $. More generally, $\overset{\vee}{e_{i}}=\overset{\vee
}{e_{i}^{\ast}}$ for every basis $\left(  e_{1},e_{2},...,e_{n}\right)  $ of
$V$ (where the maps $\widehat{e_{i}}$ and $\overset{\vee}{e_{i}}$ are defined
just as $\widehat{v_{i}}$ and $\overset{\vee}{v_{i}}$, but with respect to the
basis $\left(  e_{1},e_{2},...,e_{n}\right)  $).

The $f$-contraction operators, however, have a major advantage against the
contraction operators defined in Definition \ref{def.createdestroy.fin}
\textbf{(b)}: In fact, the former are canonical (i. e., they can be defined in
the same way for every vector space instead of $V$, and then they are
canonical maps that don't depend on any choice of basis), while the latter
have the basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ ``hard-coded'' into them.

Note that many sources denote the $f$-contraction operator by $i_{f}$ and call
it the \textit{interior product operator} with $f$.

It is easy to see that%
\begin{equation}
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}=f\left(  v\right)
\cdot\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{
and }v\in V \label{pf.plu.fv+vf}%
\end{equation}
(where, in the case $k=0$, we interpret $\widehat{v}\overset{\vee}{f}$ as $0$).

\textbf{(a)} We will give a basis-free definition of $S$. This will prove the
basis independence.

There is a unique vector space isomorphism $\Phi:V^{\ast}\otimes
V\rightarrow\operatorname*{End}V$ which satisfies%
\[
\Phi\left(  f\otimes v\right)  =\left(  \text{the map }V\rightarrow V\text{
sending each }w\text{ to }f\left(  w\right)  v\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{ and }v\in V.
\]
This $\Phi$ and its inverse isomorphism $\Phi^{-1}$ are actually basis-independent.

Now, define a map
\[
T:V^{\ast}\otimes V\otimes\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge
^{k+1}V\otimes\wedge^{k-1}V
\]
by%
\[
T\left(  f\otimes v\otimes\psi\otimes\phi\right)  =\left(  \widehat{v}%
\cdot\psi\right)  \otimes\left(  \overset{\vee}{f}\cdot\phi\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{, }v\in V\text{, }%
\psi\in\wedge^{k}V\text{ and }\phi\in\wedge^{k}V.
\]
This map $T$ is clearly well-defined (because $\widehat{v}\cdot\psi$ depends
bilinearly on $v$ and $\psi$, and because $\overset{\vee}{f}\cdot\phi$ depends
bilinearly on $f$ and $\phi$).

It is now easy to show that $S$ is the map $\wedge^{k}V\otimes\wedge
^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$ which sends $\psi
\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}%
\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in\wedge^{k}V$ and
$\phi\in\wedge^{k}V$.\ \ \ \ \footnote{\textit{Proof.} Consider the map
$\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$
which sends $\psi\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}%
\nolimits_{V}\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in
\wedge^{k}V$ and $\phi\in\wedge^{k}V$. This map is clearly well-defined. Now,
since $\Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}\right)  =\sum
\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}$ (because every $w\in V$ satisfies
\begin{align*}
\left(  \Phi\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}\right)
\right)  \left(  w\right)   &  =\sum\limits_{i=1}^{n}\underbrace{\left(
\Phi\left(  v_{i}^{\ast}\otimes v_{i}\right)  \right)  \left(  w\right)
}_{\substack{=v_{i}^{\ast}\left(  w\right)  v_{i}\\\text{(by the definition of
}\Phi\text{)}}}=\sum\limits_{i=1}^{n}v_{i}^{\ast}\left(  w\right)  v_{i}=w\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  v_{1}^{\ast},v_{2}^{\ast
},...,v_{n}^{\ast}\right)  \text{ is the dual basis of }\left(  v_{1}%
,v_{2},...,v_{n}\right)  \right) \\
&  =\operatorname*{id}\nolimits_{V}\left(  w\right)  ,
\end{align*}
so that $\Phi\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}\right)
=\operatorname*{id}\nolimits_{V}$), this map sends $\psi\otimes\phi$ to%
\begin{align*}
T\left(  \underbrace{\Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}\right)
}_{=\sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}}\otimes\psi\otimes
\phi\right)   &  =T\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes
v_{i}\otimes\psi\otimes\phi\right)  =\sum\limits_{i=1}^{n}\underbrace{T\left(
v_{i}^{\ast}\otimes v_{i}\otimes\psi\otimes\phi\right)  }_{\substack{=\left(
\widehat{v_{i}}\cdot\psi\right)  \otimes\left(  \overset{\vee}{v_{i}^{\ast}%
}\cdot\phi\right)  \\\text{(by the definition of }T\text{)}}}\\
&  =\sum\limits_{i=1}^{n}\left(  \widehat{v_{i}}\cdot\psi\right)
\otimes\left(  \underbrace{\overset{\vee}{v_{i}^{\ast}}}_{=\overset{\vee
}{v_{i}}}\cdot\phi\right)  =\sum\limits_{i=1}^{n}\left(  \widehat{v_{i}}%
\cdot\psi\right)  \otimes\left(  \overset{\vee}{v_{i}}\cdot\phi\right)
\end{align*}
for all $\psi\in\wedge^{k}V$ and $\phi\in\wedge^{k}V$. In other words, this
map is the map $\sum\limits_{i=1}^{n}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}=S$. So we have shown that $S$ is the map $\wedge^{k}V\otimes
\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$ which sends
$\psi\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}%
\nolimits_{V}\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in
\wedge^{k}V$ and $\phi\in\wedge^{k}V$, qed.} This shows immediately that $S$
is basis-independent (since $T$ and $\Phi^{-1}$ are basis-independent).

Since $S$ is basis-independent, it is clear that $S$ is $\operatorname*{GL}%
\left(  V\right)  $-invariant (because the action of $\operatorname*{GL}%
\left(  V\right)  $ transforms $S$ into the same operator $S$ but constructed
for a different basis; but since $S$ is basis-independent, this other $S$ must
be the $S$ that we started with). This proves Theorem \ref{thm.plu}
\textbf{(a)}.

\textbf{(b)} Let $\tau\in\Omega$ be nonzero.

\textbf{1)} First let us show that if $\tau\in\Omega$, then $S\left(
\tau\otimes\tau\right)  =0$.

In order to show this, it is enough to prove that $S\left(  \tau\otimes
\tau\right)  =0$ holds in the case $\tau=v_{1}\wedge v_{2}\wedge...\wedge
v_{k}$ (since $S$ is $\operatorname*{GL}\left(  V\right)  $-invariant, and
$\Omega$ is the $\operatorname*{GL}\left(  V\right)  $-orbit of $v_{1}\wedge
v_{2}\wedge...\wedge v_{k}$).

But this is obvious, because for every $i\in\left\{  1,2,...,n\right\}  $,
either $\widehat{v_{i}}$ or $\overset{\vee}{v_{i}}$ annihilates $v_{1}\wedge
v_{2}\wedge...\wedge v_{k}$.

\textbf{2)} Let us now (conversely) prove that if $S\left(  \tau\otimes
\tau\right)  =0$, then $\tau\in\Omega$.

(There is a combinatorial proof of this in the infinite setting in the
Kac-Raina book, but we will make a different proof here.)

Define $E\subseteq V$ to be the set $\left\{  v\in V\ \mid\ \widehat{v}%
\tau=0\right\}  $. Define $E^{\prime}\subseteq V^{\ast}$ to be the set
$\left\{  f\in V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  $. Clearly,
$E$ is a subspace of $V$, and $E^{\prime}$ is a subspace of $V^{\ast}$.

We know that all $v\in E$ and $f\in E^{\prime}$ satisfy $\left(
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}\right)  \tau=0$
(since the definition of $E$ yields $\widehat{v}\tau=0$, and the definition of
$E^{\prime}$ yields $\overset{\vee}{f}\tau=0$). But $\underbrace{\left(
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}\right)
}_{\substack{=f\left(  v\right)  \operatorname*{id}\\\text{(by
(\ref{pf.plu.fv+vf}))}}}\tau=f\left(  v\right)  \tau$, so this yields
$f\left(  v\right)  \tau=0$, and thus $f\left(  v\right)  =0$ (since $\tau
\neq0$). Thus, $E\subseteq E^{\prime\perp}$.

Let $m=\dim E$ and $r=\dim\left(  E^{\prime\perp}\right)  $. Pick a basis
$\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V$ such that $\left(  e_{1}%
,e_{2},...,e_{m}\right)  $ is a basis of $E$ and such that $\left(
e_{1},e_{2},...,e_{r}\right)  $ is a basis of $E^{\prime\perp}$. (Such a basis
clearly exists.)

Clearly, for every $i\in\left\{  1,2,...,m\right\}  $, we have $e_{i}\in E$
and thus $\widehat{e_{i}}\tau=0$ (by the definition of $E$).

Also, for every $i\in\left\{  r+1,r+2,...,n\right\}  $, we have $\overset{\vee
}{e_{i}^{\ast}}\tau=0$ (because $i>r$, so that $e_{i}^{\ast}\left(
e_{j}\right)  =0$ for all $j\in\left\{  1,2,...,r\right\}  $, so that
$e_{i}^{\ast}\left(  E^{\prime}\right)  =0$ (since $\left(  e_{1}%
,e_{2},...,e_{r}\right)  $ is a basis of $E^{\prime\perp}$), so that
$e_{i}^{\ast}\in\left(  E^{\prime\perp}\right)  ^{\perp}=E^{\prime}$).

The vectors $\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,n\right\}  $
are linearly independent (because if some linear combination of them was zero,
then some linear combination of the $e_{i}$ with $i\in\left\{
m+1,m+2,...,n\right\}  $ would lie in $\left\{  v\in V\ \mid\ \widehat{v}%
\tau=0\right\}  =E$, but this contradicts the fact that $\left(  e_{1}%
,e_{2},...,e_{m}\right)  $ is a basis of $E$). Hence, the vectors
$\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,r\right\}  $ are linearly independent.

We defined $S$ using the basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ of $V$
by the formula $S=\sum\limits_{i=1}^{n}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$. Since $S$ did not depend on the basis, we get the same $S$ if we
define it using the basis $\left(  e_{1},e_{2},...,e_{n}\right)  $. Thus, we
have $S=\sum\limits_{i=1}^{n}\widehat{e_{i}}\otimes\overset{\vee}{e_{i}}$.
Hence,%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =\sum\limits_{i=1}^{m}%
\underbrace{\widehat{e_{i}}\tau}_{\substack{=0\\\text{(since }i\in\left\{
1,2,...,m\right\}  \text{)}}}\otimes\overset{\vee}{e_{i}^{\ast}}\tau
+\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}%
}\tau+\sum\limits_{i=r+1}^{n}\widehat{e_{i}}\tau\otimes
\underbrace{\overset{\vee}{e_{i}^{\ast}}\tau}_{\substack{=0\\\text{(since
}i\in\left\{  r+1,r+2,...,n\right\}  \text{)}}}\\
&  =\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee
}{e_{i}^{\ast}}\tau.
\end{align*}
Thus, $S\left(  \tau\otimes\tau\right)  =0$ rewrites as $\sum\limits_{i=m+1}%
^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}}\tau=0$. But since
the vectors $\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,r\right\}  $
are linearly independent, this yields that $\overset{\vee}{e_{i}^{\ast}}%
\tau=0$ for any $i\in\left\{  m+1,m+2,...,r\right\}  $. Thus, for every
$i\in\left\{  m+1,m+2,...,r\right\}  $, we have $e_{i}^{\ast}\in\left\{  f\in
V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  =E^{\prime}$, so that
$e_{i}^{\ast}\left(  E^{\prime\perp}\right)  =0$. But on the other hand, for
every $i\in\left\{  m+1,m+2,...,r\right\}  $, we have $e_{i}\in E^{\prime
\perp}$ (since $\left(  e_{1},e_{2},...,e_{r}\right)  $ is a basis of
$E^{\prime\perp}$, and since $i\leq r$). Thus, for every $i\in\left\{
m+1,m+2,...,r\right\}  $, we have $1=e_{i}^{\ast}\left(  \underbrace{e_{i}%
}_{\in E^{\prime\perp}}\right)  \in e_{i}^{\ast}\left(  E^{\prime\perp
}\right)  =0$. This is a contradiction unless there are no $i\in\left\{
m+1,m+2,...,r\right\}  $ at all.

So we conclude that there are no $i\in\left\{  m+1,m+2,...,r\right\}  $ at
all. In other words, $m=r$. Thus, $\dim E=m=r=\dim\left(  E^{\prime\perp
}\right)  $. Combined with $E\subseteq E^{\prime\perp}$, this yields
$E=E^{\prime\perp}$.

Now, recall that $\left(  e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}%
}\right)  _{1\leq i_{1}<i_{2}<...<i_{k}\leq n}$ is a basis of $\wedge^{k}V$.
Hence, we can write $\tau$ in the form $\tau=\sum\limits_{1\leq i_{1}%
<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}$ for some scalars $\lambda_{i_{1}%
,i_{2},...,i_{k}}\in\mathbb{C}$.

Now, we will prove:

\textit{Observation 1:} For every $k$-tuple $\left(  j_{1},j_{2}%
,...,j_{k}\right)  $ of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq
n$ and $\left\{  1,2,...,m\right\}  \not \subseteq \left\{  j_{1}%
,j_{2},...,j_{k}\right\}  $, we have $\lambda_{j_{1},j_{2},...,j_{k}}=0$.

\textit{Proof of Observation 1:} Let $\left(  j_{1},j_{2},...,j_{k}\right)  $
be a $k$-tuple of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ and
$\left\{  1,2,...,m\right\}  \not \subseteq \left\{  j_{1},j_{2}%
,...,j_{k}\right\}  $. Then, there exists an $i\in\left\{  1,2,...,m\right\}
$ such that $i\notin\left\{  j_{1},j_{2},...,j_{k}\right\}  $. Consider this
$i$. As we saw above, this yields $\widehat{e_{i}}\tau=0$. Thus,%
\begin{align*}
0  &  =\widehat{e_{i}}\tau=e_{i}\wedge\tau=\sum\limits_{1\leq i_{1}%
<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i}\wedge e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau=\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  =\sum\limits_{\substack{1\leq i_{1}<i_{2}<...<i_{k}\leq n;\\i\notin\left\{
i_{1},i_{2},...,i_{k}\right\}  }}\lambda_{i_{1},i_{2},...,i_{k}}e_{i}\wedge
e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since all terms of the sum with }%
i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  \text{ are }0\right)  .
\end{align*}
Thus, for every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$ and $i\notin\left\{
i_{1},i_{2},...,i_{k}\right\}  $, we must have $\lambda_{i_{1},i_{2}%
,...,i_{k}}=0$ (because the wedge products $e_{i}\wedge e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}$ for all such $k$-tuples are linearly
independent elements of $\wedge^{k+1}V$). Applied to $\left(  i_{1}%
,i_{2},...,i_{k}\right)  =\left(  j_{1},j_{2},...,j_{k}\right)  $, this yields
that $\lambda_{j_{1},j_{2},...,j_{k}}=0$. Observation 1 is proven.

\textit{Observation 2:} For every $k$-tuple $\left(  j_{1},j_{2}%
,...,j_{k}\right)  $ of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq
n$ and $\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $, we have $\lambda_{j_{1},j_{2},...,j_{k}}=0$.

\textit{Proof of Observation 2:} Let $\left(  j_{1},j_{2},...,j_{k}\right)  $
be a $k$-tuple of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ and
$\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $. Then, there exists an $i\in\left\{  j_{1},j_{2}%
,...,j_{k}\right\}  $ such that $i\notin\left\{  1,2,...,m\right\}  $.
Consider this $i$. Then, $i\notin\left\{  1,2,...,m\right\}  $, so that
$i>m=r$, so that $i\in\left\{  r+1,r+2,...,n\right\}  $. As we saw above, this
yields $\overset{\vee}{e_{i}^{\ast}}\tau=0$. Thus,
\begin{align*}
0  &  =\underbrace{\overset{\vee}{e_{i}^{\ast}}}_{=\overset{\vee}{e_{i}}}%
\tau=\overset{\vee}{e_{i}}\tau=\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq
n}\lambda_{i_{1},i_{2},...,i_{k}}\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau=\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  =\sum\limits_{\substack{1\leq i_{1}<i_{2}<...<i_{k}\leq n;\\i\in\left\{
i_{1},i_{2},...,i_{k}\right\}  }}\lambda_{i_{1},i_{2},...,i_{k}}%
\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge
e_{i_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since all terms of the sum with }%
i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}  \text{ are }0\right)  .
\end{align*}
Thus, for every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$ and $i\in\left\{  i_{1}%
,i_{2},...,i_{k}\right\}  $, we must have $\lambda_{i_{1},i_{2},...,i_{k}}=0$
(because the wedge products $\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right)  $ for all such $k$-tuples are
linearly independent elements of $\wedge^{k-1}V$\ \ \ \ \footnote{To check
this, it is enough to recall how $\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\right)  $ was defined: It was
defined to be $\left(  -1\right)  ^{j-1}e_{i_{1}}\wedge e_{i_{2}}%
\wedge...\wedge e_{i_{j-1}}\wedge e_{i_{j+1}}\wedge e_{i_{j+2}}\wedge...\wedge
e_{i_{k}}$, where $j$ is the integer $\ell$ satisfying $i_{\ell}=i$.}).
Applied to $\left(  i_{1},i_{2},...,i_{k}\right)  =\left(  j_{1}%
,j_{2},...,j_{k}\right)  $, this yields that $\lambda_{j_{1},j_{2},...,j_{k}%
}=0$. Observation 2 is proven.

Now, every $k$-tuple $\left(  j_{1},j_{2},...,j_{k}\right)  $ of integers
satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ must satisfy either $\left\{
1,2,...,m\right\}  \not \subseteq \left\{  j_{1},j_{2},...,j_{k}\right\}  $,
or $\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $, or $\left(  1,2,...,m\right)  =\left(  j_{1}%
,j_{2},...,j_{k}\right)  $. In the first of these three cases, we have
$\lambda_{j_{1},j_{2},...,j_{k}}=0$ by Observation 1; in the second case, we
have $\lambda_{j_{1},j_{2},...,j_{k}}=0$ by Observation 2. Hence, the only
case where $\lambda_{j_{1},j_{2},...,j_{k}}$ can be nonzero is the third case,
i. e., the case when $\left(  1,2,...,m\right)  =\left(  j_{1},j_{2}%
,...,j_{k}\right)  $. Hence, the only nonzero addend that the sum
$\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2}%
,...,i_{k}}e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}$ can have is the
addend for $\left(  i_{1},i_{2},...,i_{k}\right)  =\left(  1,2,...,m\right)
$. Thus, all other addends of this sum can be removed, and therefore
$\tau=\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1}%
,i_{2},...,i_{k}}e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}$ rewrites
as $\tau=\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}$. Since
$\tau\neq0$, we thus have $\lambda_{1,2,...,m}\neq0$. Hence, $m=k$ (because
$\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}=\tau\in\wedge^{k}%
V$). Hence,%
\[
\tau=\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}=\lambda
_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{k}=\left(  \lambda
_{1,2,...,m}e_{1}\right)  \wedge e_{2}\wedge e_{3}\wedge...\wedge e_{k}.
\]


Now, since $\lambda_{1,2,...,m}\neq0$, the $n$-tuple $\left(  \lambda
_{1,2,...,m}e_{1},e_{2},e_{3},...,e_{n}\right)  $ is a basis of $V$. Thus,
there exists an element of $\operatorname*{GL}\left(  V\right)  $ which sends
$\left(  v_{1},v_{2},...,v_{n}\right)  $ to $\left(  \lambda_{1,2,...,m}%
e_{1},e_{2},e_{3},...,e_{n}\right)  $. This element therefore sends
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ to $\left(  \lambda_{1,2,...,m}%
e_{1}\right)  \wedge e_{2}\wedge e_{3}\wedge...\wedge e_{k}=\tau$. Hence,
$\tau$ lies in the $\operatorname*{GL}\left(  V\right)  $-orbit of
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Since this orbit was called $\Omega
$, this becomes $\tau\in\Omega$.

We thus have shown that if $S\left(  \tau\otimes\tau\right)  =0$, then
$\tau\in\Omega$. This completes the proof of Theorem \ref{thm.plu}
\textbf{(b)}.

\textbf{(c)} We know from Theorem \ref{thm.plu} \textbf{(a)} that $S$ is
$\operatorname*{GL}\left(  V\right)  $-invariant. Since $\operatorname*{GL}%
\left(  V\right)  $ is Zariski-dense in $\operatorname*{M}\left(  V\right)  $,
this yields that $S$ is $\operatorname*{M}\left(  V\right)  $-invariant
(because the $\operatorname*{M}\left(  V\right)  $-invariance of $S$ can be
written as a collection of polynomial identities). This proves Theorem
\ref{thm.plu} \textbf{(c)}.

We can rewrite Theorem \ref{thm.plu} \textbf{(b)} in coordinates:

\begin{theorem}
\label{thm.plu.coo}Let $n\in\mathbb{N}$. Let $k\in\left\{  1,2,...,n\right\}
$. We consider the vector space $V=\mathbb{C}^{n}$ with its standard basis
$\left(  v_{1},v_{2},...,v_{n}\right)  $.

Let $\tau\in\wedge^{k}V$ be nonzero.

For every subset $K$ of $\left\{  1,2,...,n\right\}  $, let $v_{K}$ denote the
element of $\wedge^{\left\vert K\right\vert }V$ defined by $v_{K}=v_{k_{1}%
}\wedge v_{k_{2}}\wedge...\wedge v_{k_{\ell}}$ where $k_{1}$, $k_{2}$, $...$,
$k_{\ell}$ are the elements of $K$ in increasing order. We know that $\left(
v_{K}\right)  _{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert
K\right\vert =k}$ is a basis of the vector space $\wedge^{k}V$. For every
subset $K$ of $\left\{  1,2,...,n\right\}  $ satisfying $\left\vert
K\right\vert =k$, let $P_{K}$ be the $K$-coordinate of $\tau$ with respect to
this basis.

Then, $\tau\in\Omega$ if and only if
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{for all }I\subseteq\left\{  1,2,...,n\right\}  \text{ with }\left\vert
I\right\vert =k-1\text{ and all }J\subseteq\left\{  1,2,...,n\right\} \\
\text{with }\left\vert J\right\vert =k+1\text{, we have }\sum\limits_{j\in
J;\ j\notin I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)
^{\nu\left(  j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{
j\right\}  }=0
\end{array}
\right)  , \label{thm.plu.coo.plu}%
\end{equation}
where $\nu\left(  j\right)  $ is the integer $\ell$ for which $j$ is the
$\ell$-th smallest element of the set $J$, and where $\mu\left(  j\right)  $
is the number of elements of the set $I$ which are smaller than $j$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.plu.coo} (sketched).} We know that $\left(
v_{K}\right)  _{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert
K\right\vert =k+1}$ is a basis of $\wedge^{k+1}V$, and $\left(  v_{K}\right)
_{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert K\right\vert =k-1}$ is a
basis of $\wedge^{k-1}V$. Hence, $\left(  v_{K}\otimes v_{L}\right)
_{\substack{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert K\right\vert
=k+1,\\L\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert L\right\vert
=k-1}}$ is a basis of $\wedge^{k+1}V\otimes\wedge^{k-1}V$. It is not hard to
check that the $v_{J}\otimes v_{I}$-coordinate (with respect to this basis) of
$S\left(  \tau\otimes\tau\right)  $ is precisely $\sum\limits_{j\in
J;\ j\notin I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)
^{\nu\left(  j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{
j\right\}  }$ for all $I\subseteq\left\{  1,2,...,n\right\}  $ with
$\left\vert I\right\vert =k-1$ and all $J\subseteq\left\{  1,2,...,n\right\}
$ with $\left\vert J\right\vert =k+1$. Hence, (\ref{thm.plu.coo.plu}) holds if
and only if every coordinate of $S\left(  \tau\otimes\tau\right)  $ is zero,
i. e., if $S\left(  \tau\otimes\tau\right)  =0$, but the latter condition is
equivalent to $\tau\in\Omega$ (because of Theorem \ref{thm.plu} \textbf{(b)}).
This proves Theorem \ref{thm.plu.coo}.

Note that the $\Longrightarrow$ direction of Theorem \ref{thm.plu.coo} can be
formulated as a determinantal identity:

\begin{corollary}
\label{cor.plu.matrix}Let $n\in\mathbb{N}$. Let $k\in\left\{
1,2,...,n\right\}  $. Let $\left(
\begin{array}
[c]{cccc}%
x_{11} & x_{12} & ... & x_{1k}\\
x_{21} & x_{22} & ... & x_{2k}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & ... & x_{nk}%
\end{array}
\right)  $ be any matrix with $n$ rows and $k$ columns.

For every $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k$, let $P_{I}$ be the minor of this matrix obtained by only
keeping the rows whose indices lie in $I$ (and throwing all other rows away).

Then, for all $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k-1$ and all $J\subseteq\left\{  1,2,...,n\right\}  $ with
$\left\vert J\right\vert =k+1$, we have $\sum\limits_{j\in J;\ j\notin
I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)  ^{\nu\left(
j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{  j\right\}
}=0$ (where $\mu\left(  j\right)  $ and $\nu\left(  j\right)  $ are defined as
in Theorem \ref{thm.plu.coo}).
\end{corollary}

\textit{Example:} If $n=4$ and $k=2$, then the claim of Corollary
\ref{cor.plu.matrix} is easily simplified to the single equation $P_{12}%
P_{34}+P_{14}P_{23}-P_{13}P_{24}=0$ (where we abbreviate two-element sets
$\left\{  i,j\right\}  $ by $ij$).

\textit{Proof of Corollary \ref{cor.plu.matrix} (sketched).} WLOG assume
$k\leq n$ (else, everything is vacuously true).

For every $i\in\left\{  1,2,...,k\right\}  $, let $x_{i}\in V$ be the vector
$\left(
\begin{array}
[c]{c}%
x_{1i}\\
x_{2i}\\
\vdots\\
x_{ni}%
\end{array}
\right)  $, where $V$ is as in Theorem \ref{thm.plu.coo}. Since Corollary
\ref{cor.plu.matrix} is a collection of polynomial identities, we can WLOG
assume that the vectors $x_{1}$, $x_{2}$, $...$, $x_{k}$ are linearly
independent (since the set of linearly independent $k$-tuples $\left(
x_{1},x_{2},...,x_{k}\right)  $ of vectors in $V$ is Zariski-dense in $V^{k}%
$). Then, there exists an element of $\operatorname*{GL}\left(  V\right)  $
which maps $v_{1}$, $v_{2}$, $...$, $v_{k}$ to $x_{1}$, $x_{2}$, $...$,
$x_{k}$. Thus, $x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\Omega$ (since
$\Omega$ is the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $). Now, apply Theorem \ref{thm.plu.coo}
to $\tau=x_{1}\wedge x_{2}\wedge...\wedge x_{k}$, and Corollary
\ref{cor.plu.matrix} follows.

Of course, this was not the easiest way to prove Corollary
\ref{cor.plu.matrix}. We could just as well have derived Corollary
\ref{cor.plu.matrix} from the Cauchy-Binet identity, and thus given a new
proof for the $\Longrightarrow$ direction of Theorem \ref{thm.plu.coo}; but
the $\Longleftarrow$ direction is not that easy.

\subsubsection{\label{subsubsect.infgrass}The semiinfinite Grassmannian:
preliminary work}

Now we prepare for the semiinfinite Grassmannian:

Let $\psi_{0}$ denote the elementary semiinfinite wedge $v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\in\mathcal{F}^{\left(  0\right)  }$. We recall
the action $\varrho:\operatorname*{M}\left(  \infty\right)  \rightarrow
\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $ of the
monoid $\operatorname*{M}\left(  \infty\right)  $ on $\mathcal{F}^{\left(
m\right)  }$ for every $m\in\mathbb{Z}$. This action was defined in Definition
\ref{def.GLinf.act}.

\begin{definition}
From now on, $\Omega$ denotes the subset $\operatorname*{GL}\left(
\infty\right)  \cdot\psi_{0}$ of $\mathcal{F}^{\left(  0\right)  }$. (Here and
in the following, we abbreviate $\left(  \varrho\left(  A\right)  \right)  v$
by $Av$ for every $A\in\operatorname*{M}\left(  \infty\right)  $ and
$v\in\mathcal{F}^{\left(  m\right)  }$ and every $m\in\mathbb{Z}$. In
particular, $\operatorname*{GL}\left(  \infty\right)  \psi_{0}$ means $\left(
\varrho\left(  \operatorname*{GL}\left(  \infty\right)  \right)  \right)
\psi_{0}$.)
\end{definition}

\begin{proposition}
\label{prop.plu.inf.pure}For all $0$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we have $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\Omega$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.inf.pure}.} Let $\left(
i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. Then, there exists a
permutation $\sigma:\mathbb{Z}\rightarrow\mathbb{Z}$ which fixes all but
finitely many integers (i. e., is a finitary permutation of $\mathbb{Z}$), and
satisfies $i_{k}=\sigma\left(  -k\right)  $ for every $k\in\mathbb{N}$. Since
$\sigma$ fixes all but finitely many integers, we can represent $\sigma$ by a
matrix in $\operatorname*{GL}\left(  \infty\right)  $. Let us (by abuse of
notation) denote this matrix by $\sigma$ again. Then, every $k\in\mathbb{N}$
satisfies $v_{i_{k}}=v_{\sigma\left(  -k\right)  }=\sigma v_{-k}$. Thus,
\[
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=\sigma v_{0}\wedge\sigma
v_{-1}\wedge\sigma v_{-2}\wedge...=\sigma\underbrace{\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\right)  }_{=\psi_{0}}=\sigma\psi_{0}%
\in\operatorname*{GL}\left(  \infty\right)  \psi_{0}=\Omega.
\]
This proves Proposition \ref{prop.plu.inf.pure}.

Next, an ``infinite'' analogue of Theorem \ref{thm.plu}:

\begin{theorem}
\label{thm.plu.inf}For every $m\in\mathbb{Z}$, define a map $S:\mathcal{F}%
^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m+1\right)  }\otimes\mathcal{F}^{\left(  m-1\right)  }$
by $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}}%
$. (Note that the map $S$ is well-defined because, for every $T\in
\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }$, only
finitely many terms of the infinite sum $\sum\limits_{i\in\mathbb{Z}}\left(
\widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)  \left(  T\right)  $ are nonzero.)

\textbf{(a)} For every $m\in\mathbb{Z}$, this map $S$ is $\operatorname*{GL}%
\left(  \infty\right)  $-invariant.

\textbf{(b)} Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be nonzero. Then,
$\tau\in\Omega$ if and only if $S\left(  \tau\otimes\tau\right)  =0$.

\textbf{(c)} For every $m\in\mathbb{Z}$, the map $S$ is $\operatorname*{M}%
\left(  \infty\right)  $-invariant.
\end{theorem}

We are going to prove this theorem by reducing it to its ``finite-dimensional
version'' (i. e., Theorem \ref{thm.plu}). This reduction requires us to link
the set $\Omega$ with its finite-dimensional analoga. To do this, we set up
some definitions:

\subsubsection{Proof of Theorem \ref{thm.plu.inf}}

While the following definitions and results are, superficially seen, auxiliary
to the proof of Theorem \ref{thm.plu.inf}, their use is not confined to this
proof. They can be used to derive various results about semiinfinite wedges
(elements of $\mathcal{F}^{\left(  m\right)  }$ for integer $m$) from similar
statements about finite wedges (elements of $\wedge^{k}W$ for integer $k$ and
finite-dimensional $W$). Our proof of Theorem \ref{thm.plu.inf} below will be
just one example of such a derivation.

Note that most of the proofs in this subsection are straightforward and boring
and are easier to do by the reader than to understand from these notes.

\begin{definition}
\label{def.plu.inf.VN}Let $V$ be the vector space $\mathbb{C}^{\left(
\mathbb{Z}\right)  }=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}%
\text{\ }\mid\ x_{i}\in\mathbb{C}\text{; only finitely many }x_{i}\text{ are
nonzero}\right\}  $ as defined in Definition \ref{def.glinf.V}. Let $\left(
v_{j}\right)  _{j\in\mathbb{Z}}$ be the basis of $V$ introduced in Definition
\ref{def.glinf.V}.

For every $N\in\mathbb{N}$, let $V_{N}$ denote the $\left(  2N+1\right)
$-dimensional vector subspace $\left\langle v_{-N},v_{-N+1},...,v_{N}%
\right\rangle $ of $V$. It is clear that $V_{0}\subseteq V_{1}\subseteq
V_{2}\subseteq...$ and $V=\bigcup\limits_{N\in\mathbb{N}}V_{N}$.
\end{definition}

It should be noticed that this vector subspace $V_{N}$ is what has been called
$V_{\left]  -N-1,N\right]  }$ in Definition \ref{def.finitary.Valphabeta}.

\begin{definition}
\label{def.plu.inf.iN}Let $N\in\mathbb{N}$. Let $\operatorname*{M}\left(
V_{N}\right)  $ denote the set of all $\left(  2N+1\right)  \times\left(
2N+1\right)  $-matrices over $\mathbb{C}$ whose rows are indexed by elements
of $\left\{  -N,-N+1,...,N\right\}  $ and whose columns are also indexed by
elements of $\left\{  -N,-N+1,...,N\right\}  $. Define a map $i_{N}%
:\operatorname*{M}\left(  V_{N}\right)  \rightarrow\operatorname*{M}\left(
\infty\right)  $ as follows: For every matrix $A\in\operatorname*{M}\left(
V_{N}\right)  $, let $i_{N}\left(  A\right)  $ be the infinite matrix (with
rows and columns indexed by integers) such that%
\[
\left(
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }i_{N}\left(
A\right)  \right) \\
=\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{for
every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\end{array}
\right)  .
\]
It is easy to see that this map $i_{N}$ is well-defined (i. e., for every
$A\in\operatorname*{M}\left(  V_{N}\right)  $, the matrix $i_{N}\left(
A\right)  $ that we just defined really lies in $\operatorname*{M}\left(
\infty\right)  $), injective and a monoid homomorphism.

The vector space $V_{N}$ has a basis $\left(  v_{-N},v_{-N+1},...,v_{N}%
\right)  $ which is indexed by the set $\left\{  -N,-N+1,...,N\right\}  $.
Thus, we can identify matrices in $\operatorname*{M}\left(  V_{N}\right)  $
with endomorphisms of the vector space $V_{N}$ in the obvious way. Hence, the
invertible elements of $\operatorname*{M}\left(  V_{N}\right)  $ are
identified with the invertible endomorphisms of the vector space $V_{N}$, i.
e., with the elements of $\operatorname*{GL}\left(  V_{N}\right)  $. The
injective map $i_{N}:\operatorname*{M}\left(  V_{N}\right)  \rightarrow
\operatorname*{M}\left(  \infty\right)  $ restricts to an injective map
$i_{N}\mid_{\operatorname*{GL}\left(  V_{N}\right)  }:\operatorname*{GL}%
\left(  V_{N}\right)  \rightarrow\operatorname*{GL}\left(  \infty\right)  $.
\end{definition}

\begin{remark}
Here is a more lucid way to describe the map $i_{N}$ we just defined:

Let $I_{-\infty}$ be the infinite identity matrix whose rows are indexed by
all negative integers, and whose columns are indexed by all negative integers.

Let $I_{\infty}$ be the infinite identity matrix whose rows are indexed by all
positive integers, and whose columns are indexed by all positive integers.

For any matrix $A\in\operatorname*{M}\left(  V_{N}\right)  $, we define
$i_{N}\left(  A\right)  $ to be the block-diagonal matrix $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & A & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ whose diagonal blocks are $I_{-\infty}$, $A$ and $I_{\infty}$,
where the first block covers the rows with indices smaller than $-N$ (and
therefore also the columns with indices smaller than $-N$), the second block
covers the rows with indices in $\left\{  -N,-N+1,...,N\right\}  $ (and
therefore also the columns with indices in $\left\{  -N,-N+1,...,N\right\}
$), and the third block covers the rows with indices larger than $N$ (and
therefore also the columns with indices larger than $N$). From this
definition, it becomes clear why $i_{N}$ is a monoid homomorphism. (In fact,
it is clear that the block-diagonal matrix $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & I_{2N+1} & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ is the identity matrix, and using the rules for computing with
block matrices it is also easy to see that $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & A & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & B & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & AB & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ for all $A\in\operatorname*{M}\left(  V_{N}\right)  $ and
$B\in\operatorname*{M}\left(  V_{N}\right)  $.)
\end{remark}

\begin{remark}
\label{rmk.plu.inf.iN}\textbf{(a)} Every $N\in\mathbb{N}$ satisfies%
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  =\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  .
\]


\textbf{(b)} We have $i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)
\right)  \subseteq i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)
\right)  \subseteq i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)
\right)  \subseteq...$.

\textbf{(c)} We have $\operatorname*{M}\left(  \infty\right)  =\bigcup
\limits_{N\in\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right)  $.
\end{remark}

\textit{Proof of Remark \ref{rmk.plu.inf.iN}.} \textbf{(a)} Let $N\in
\mathbb{N}$. Then,%
\[
\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  \subseteq i_{N}\left(  \operatorname*{M}\left(
V_{N}\right)  \right)
\]
\footnote{\textit{Proof.} To prove this, it is clearly enough to show that
every matrix $A\in\operatorname*{M}\left(  \infty\right)  $ which satisfies
\begin{equation}
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}\right)  \label{pf.plu.inf.iN.1}%
\end{equation}
lies in $i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $. So
let $A\in\operatorname*{M}\left(  \infty\right)  $ be a matrix which satisfies
(\ref{pf.plu.inf.iN.1}). We must prove that $A\in i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $.
\par
Indeed, let $B\in\operatorname*{M}\left(  V_{N}\right)  $ be the matrix
defined by%
\begin{equation}
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)  \text{
for every }\left(  i,j\right)  \in\left\{  -N,-N+1,...,N\right\}  ^{2}\right)
. \label{pf.plu.inf.iN.2}%
\end{equation}
Then, $i_{N}\left(  B\right)  =A$ (because for every $\left(  i,j\right)
\in\mathbb{Z}^{2}$, we have%
\begin{align*}
&  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }i_{N}\left(
B\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{N}\left(
B\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.plu.inf.iN.2})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\mathbb{Z}%
^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{i,j}=\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }A\right)  \text{ for every }\left(
i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}\text{ (by (\ref{pf.plu.inf.iN.1}))}\right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\end{align*}
). Thus, $A=i_{N}\left(  B\right)  \in i_{N}\left(  \operatorname*{M}\left(
V_{N}\right)  \right)  $ (since $B\in\operatorname*{M}\left(  V_{N}\right)
$), qed.} and%
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  \subseteq\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}
\]
(by the definition of $i_{N}$). Combining these two relations, we obtain
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  =\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  .
\]
This proves Remark \ref{rmk.plu.inf.iN} \textbf{(a)}.

\textbf{(b)} By Remark \ref{rmk.plu.inf.iN} \textbf{(a)}, for any
$N\in\mathbb{N}$, the set $i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right)  $ is the set of all matrices $A\in\operatorname*{M}\left(
\infty\right)  $ satisfying the condition%
\[
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}\right)  .
\]
If this condition is satisfied for some $N$, then it is (all the more)
satisfied for $N+1$ instead of $N$. Hence, $i_{N}\left(  \operatorname*{M}%
\left(  V_{N}\right)  \right)  \subseteq i_{N+1}\left(  \operatorname*{M}%
\left(  V_{N+1}\right)  \right)  $ for any $N\in\mathbb{N}$. Thus,
$i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)  \right)  \subseteq
i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)  \right)  \subseteq
i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)  \right)  \subseteq...$.
This proves Remark \ref{rmk.plu.inf.iN} \textbf{(b)}.

\textbf{(c)} Let $B\in\operatorname*{M}\left(  \infty\right)  $ be arbitrary.
We will now construct an $N\in\mathbb{N}$ such that $B\in i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $.

Since $B\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists a $b\in\mathfrak{gl}_{\infty}$ such
that $B=\operatorname*{id}+b$. Consider this $b$.

For any $\left(  i,j\right)  \in\mathbb{Z}^{2}$, let $b_{i,j}$ denote the
$\left(  i,j\right)  $-th entry of the matrix $b$.

Since $b\in\mathfrak{gl}_{\infty}$, only finitely many entries of the matrix
$b$ are nonzero. In other words, only finitely many $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfy $\left(  \left(  u,v\right)  \text{-th entry of
}b\right)  \neq0$. In other words, only finitely many $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfy $b_{u,v}\neq0$ (since $\left(  \left(  u,v\right)
\text{-th entry of }b\right)  =b_{u,v}$). In other words, the set $\left\{
\max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert \right\}
\ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}\neq0\right\}  $ is finite.

Let
\[
N=\max\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  .
\]
\footnote{Here, we set $\max\left\{  \max\left\{  \left\vert u\right\vert
,\left\vert v\right\vert \right\}  \ \mid\ \left(  u,v\right)  \in
\mathbb{Z}^{2};\ b_{u,v}\neq0\right\}  $ to be $0$ if the set $\left\{
\max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert \right\}
\ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}\neq0\right\}  $ is
empty.} This $N$ is a well-defined nonnegative integer (since the set
$\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  $ is finite).

Let $\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{
-N,-N+1,...,N\right\}  ^{2}$. Then, $\left(  i,j\right)  \notin\left\{
-N,-N+1,...,N\right\}  ^{2}$. We are now going to show that $b_{i,j}=0$.

In fact, assume (for the sake of contradiction) that $b_{i,j}\neq0$. Thus,
$\left(  i,j\right)  \in\left\{  \left(  u,v\right)  \in\mathbb{Z}^{2}%
\ \mid\ b_{u,v}\neq0\right\}  $. Hence,%
\[
\max\left\{  \left\vert i\right\vert ,\left\vert j\right\vert \right\}
\in\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  .
\]
Since any element of a finite set is less or equal to the maximum of the set,
this yields%
\[
\max\left\{  \left\vert i\right\vert ,\left\vert j\right\vert \right\}
\leq\max\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  =N.
\]
Thus, $\left\vert i\right\vert \leq\max\left\{  \left\vert i\right\vert
,\left\vert j\right\vert \right\}  \leq N$, so that $i\in\left\{
-N,-N+1,...,N\right\}  $ and similarly $j\in\left\{  -N,-N+1,...,N\right\}  $.
Hence, $\left(  i,j\right)  \in\left\{  -N,-N+1,...,N\right\}  ^{2}$ (because
$i\in\left\{  -N,-N+1,...,N\right\}  $ and $j\in\left\{
-N,-N+1,...,N\right\}  $), which contradicts $\left(  i,j\right)
\notin\left\{  -N,-N+1,...,N\right\}  ^{2}$. This contradiction shows that our
assumption (that $b_{i,j}\neq0$) was wrong. We thus have $b_{i,j}=0$.

Since $B=\operatorname*{id}+b$, we have:%
\[
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\underbrace{\left(  \text{the }\left(  i,j\right)  \text{-th entry of
}\operatorname*{id}\right)  }_{=\delta_{i,j}}+\underbrace{\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }b\right)  }_{=b_{i,j}=0}=\delta
_{i,j}.
\]


Now, forget that we fixed $\left(  i,j\right)  $. We thus have shown that
$\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\delta_{i,j}$ for every $\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown
\left\{  -N,-N+1,...,N\right\}  ^{2}$. In other words,%
\begin{align*}
B  &  \in\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  =i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.plu.inf.iN}
\textbf{(a)}}\right) \\
&  \subseteq\bigcup\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}%
\left(  V_{P}\right)  \right)  .
\end{align*}
Now forget that we fixed $B$. We thus have proven that every $B\in
\operatorname*{M}\left(  \infty\right)  $ satisfies $B\in\bigcup
\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}\left(  V_{P}\right)
\right)  $. In other words, $\operatorname*{M}\left(  \infty\right)
\subseteq\bigcup\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}\left(
V_{P}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $ (here, we renamed the index
$P$ as $N$). Combined with the obvious inclusion $\bigcup\limits_{N\in
\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)
\subseteq\operatorname*{M}\left(  \infty\right)  $, this yields
$\operatorname*{M}\left(  \infty\right)  =\bigcup\limits_{N\in\mathbb{N}}%
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $. Remark
\ref{rmk.plu.inf.iN} \textbf{(c)} is therefore proven.

\begin{definition}
\label{def.plu.inf.jmN}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. We define a
linear map $j_{N}^{\left(  m\right)  }:\wedge^{N+m+1}\left(  V_{N}\right)
\rightarrow\mathcal{F}^{\left(  m\right)  }$ by setting%
\[
\left(
\begin{array}
[c]{r}%
j_{N}^{\left(  m\right)  }\left(  b_{0}\wedge b_{1}\wedge...\wedge
b_{N+m}\right)  =b_{0}\wedge b_{1}\wedge...\wedge b_{N+m}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\\
\text{for any }b_{0},b_{1},...,b_{N+m}\in V_{N}%
\end{array}
\right)  .
\]
This map $j_{N}^{\left(  m\right)  }$ is well-defined (because $b_{0}\wedge
b_{1}\wedge...\wedge b_{N+m}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...$ is easily seen to lie in $\mathcal{F}^{\left(  m\right)  }$
and depend multilinearly and antisymmetrically on $b_{0},b_{1},...,b_{N+m}$)
and injective (because the elements of the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}%
>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $ are sent by
$j_{N}^{\left(  m\right)  }$ to pairwise distinct elements of the basis
$\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(
i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ of
$\mathcal{F}^{\left(  m\right)  }$).
\end{definition}

In the terminology of Definition \ref{def.finitary.Valphabeta}, the map
$j_{N}^{\left(  m\right)  }$ that we have just defined is the map
$R_{N+m+1,\left]  -N-1,N\right]  }$.

Our definitions of $j_{N}^{\left(  m\right)  }$ and of $i_{N}$ satisfy
reasonable compatibilities:

\begin{proposition}
\label{prop.plu.inf.iNjmN}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. For any
$u\in\wedge^{N+m+1}\left(  V_{N}\right)  $ and $A\in\operatorname*{M}\left(
V_{N}\right)  $, we have%
\[
i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)  }\left(  u\right)
=j_{N}^{\left(  m\right)  }\left(  Au\right)  .
\]
(Here, of course, $i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)
}\left(  u\right)  $ stands for $\left(  \varrho\left(  i_{N}\left(  A\right)
\right)  \right)  \left(  j_{N}^{\left(  m\right)  }\left(  u\right)  \right)
$.)
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.inf.iNjmN}.} Let $A\in
\operatorname*{M}\left(  V_{N}\right)  $ and $u\in\wedge^{N+m+1}\left(
V_{N}\right)  $. We must prove the equality $i_{N}\left(  A\right)  \cdot
j_{N}^{\left(  m\right)  }\left(  u\right)  =j_{N}^{\left(  m\right)  }\left(
Au\right)  $. Since this equality is linear in $u$, we can WLOG assume that
$u$ is an element of the basis $\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of
$\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume this. Then, there exists an
$N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $ of integers such that
$N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

By the definition of $i_{N}\left(  A\right)  $, we have
\begin{equation}
\left(  i_{N}\left(  A\right)  \cdot v_{k}=Av_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }k\in\left\{  -N,-N+1,...,N\right\}  \right)
\label{pf.plu.inf.iNjmN.in}%
\end{equation}
and
\begin{equation}
\left(  i_{N}\left(  A\right)  \cdot v_{k}=v_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }k\in\mathbb{Z}\diagdown\left\{  -N,-N+1,...,N\right\}  \right)  .
\label{pf.plu.inf.iNjmN.out}%
\end{equation}


Note that every $\ell\in\left\{  0,1,...,N+m\right\}  $ satisfies $i_{\ell}%
\in\left\{  -N,-N+1,...,N\right\}  $ (since $N\geq i_{0}>i_{1}>...>i_{N+m}%
\geq-N$ and thus $N\geq i_{\ell}\geq-N$) and thus
\begin{equation}
i_{N}\left(  A\right)  \cdot v_{i_{\ell}}=Av_{i_{\ell}}
\label{pf.plu.inf.iNjmN.in2}%
\end{equation}
(by (\ref{pf.plu.inf.iNjmN.in}), applied to $k=i_{\ell}$). Also, every
positive integer $r$ satisfies $-N-r\in\mathbb{Z}\diagdown\left\{
-N,-N+1,...,N\right\}  $ and thus%
\begin{equation}
i_{N}\left(  A\right)  \cdot v_{-N-r}=v_{-N-r} \label{pf.plu.inf.iNjmN.out2}%
\end{equation}
(by (\ref{pf.plu.inf.iNjmN.out}), applied to $k=-N-r$).

Now, since $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we have%
\begin{align*}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...
\end{align*}
(by the definition of $j_{N}^{\left(  m\right)  }$), so that%
\begin{align}
&  i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)  }\left(  u\right)
\nonumber\\
&  =i_{N}\left(  A\right)  \cdot\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}%
\wedge...\right) \nonumber\\
&  =\underbrace{i_{N}\left(  A\right)  \cdot v_{i_{0}}\wedge i_{N}\left(
A\right)  \cdot v_{i_{1}}\wedge...\wedge i_{N}\left(  A\right)  \cdot
v_{i_{N+m}}}_{\substack{=Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge
Av_{i_{N+m}}\\\text{(because every }\ell\in\left\{  0,1,...,N+m\right\}
\text{ satisfies }i_{N}\left(  A\right)  \cdot v_{i_{\ell}}=Av_{i_{\ell}%
}\text{ (by (\ref{pf.plu.inf.iNjmN.in2})))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \wedge\underbrace{i_{N}\left(  A\right)  \cdot
v_{-N-1}\wedge i_{N}\left(  A\right)  \cdot v_{-N-2}\wedge i_{N}\left(
A\right)  \cdot v_{-N-3}\wedge...}_{\substack{=v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(because every positive integer }r\text{ satisfies
}i_{N}\left(  A\right)  \cdot v_{-N-r}=v_{-N-r}\text{ (by
(\ref{pf.plu.inf.iNjmN.out2})))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the action }%
\varrho:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}^{\left(  m\right)  }\right)  \right) \nonumber\\
&  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge.... \label{pf.plu.inf.iNjmN.left}%
\end{align}
On the other hand, since $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}$, we have $Au=Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge
Av_{i_{N+m}}$, so that%
\begin{align*}
j_{N}^{\left(  m\right)  }\left(  Au\right)   &  =j_{N}^{\left(  m\right)
}\left(  Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\right) \\
&  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...
\end{align*}
(by the definition of $j_{N}^{\left(  m\right)  }$). Compared with
(\ref{pf.plu.inf.iNjmN.left}), this yields $i_{N}\left(  A\right)  \cdot
j_{N}^{\left(  m\right)  }\left(  u\right)  =j_{N}^{\left(  m\right)  }\left(
Au\right)  $. This proves Proposition \ref{prop.plu.inf.iNjmN}.

An important property of the maps $j_{N}^{\left(  m\right)  }$ is that their
images (for fixed $m$ and varying $N$) cover (not just span, but actually
cover) all of $\mathcal{F}^{\left(  m\right)  }$:

\begin{proposition}
\label{prop.plu.inf.cover}Let $m\in\mathbb{Z}$.

\textbf{(a)} We have%
\[
j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)
\subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq....
\]


\textbf{(b)} For every $Q\in\mathbb{N}$, we have $\mathcal{F}^{\left(
m\right)  }=\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  $.
\end{proposition}

Actually, the ``$N\geq Q$'' in Proposition \ref{prop.plu.inf.cover}
\textbf{(b)} doesn't have much effect since Proposition
\ref{prop.plu.inf.cover} \textbf{(a)} yields $\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  $; but we
prefer to put it in because it is needed in our application.

\textit{Proof of Proposition \ref{prop.plu.inf.cover}.} \textbf{(a)} Let
$N\in\mathbb{N}$. From the definitions of $j_{N}$ and $j_{N+1}$, it is easy to
see that%
\[
j_{N}^{\left(  m\right)  }\left(  b_{0}\wedge b_{1}\wedge...\wedge
b_{N+m}\right)  =j_{N+1}^{\left(  m\right)  }\left(  b_{0}\wedge b_{1}%
\wedge...\wedge b_{N+m}\wedge v_{-N-1}\right)
\]
for any $b_{0},b_{1},...,b_{N+m}\in V_{N}$. Due to linearity, this yields that
$j_{N}^{\left(  m\right)  }\left(  a\right)  =j_{N+1}^{\left(  m\right)
}\left(  a\wedge v_{-N-1}\right)  $ for any $a\in\wedge^{N+m+1}\left(
V_{N}\right)  $. Hence, $j_{N}^{\left(  m\right)  }\left(  a\right)
=j_{N+1}^{\left(  m\right)  }\left(  a\wedge v_{-N-1}\right)  \in
j_{N+1}^{\left(  m\right)  }\left(  \wedge^{\left(  N+1\right)  +m+1}\left(
V_{N+1}\right)  \right)  $ for any $a\in\wedge^{N+m+1}\left(  V_{N}\right)  $.
In other words, $j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  \subseteq j_{N+1}^{\left(  m\right)  }\left(
\wedge^{\left(  N+1\right)  +m+1}\left(  V_{N+1}\right)  \right)  $.

We thus have proven that every $N\in\mathbb{N}$ satisfies%
\[
j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\subseteq j_{N+1}^{\left(  m\right)  }\left(  \wedge^{\left(  N+1\right)
+m+1}\left(  V_{N+1}\right)  \right)  .
\]
In other words,%
\[
j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)
\subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq....
\]
Proposition \ref{prop.plu.inf.cover} \textbf{(a)} is proven.

\textbf{(b)} We need three notations:

\begin{itemize}
\item For any $m$-degression $\mathbf{i}$, define a nonnegative integer
$\operatorname*{exting}\left(  \mathbf{i}\right)  $ as the largest
$k\in\mathbb{N}$ satisfying $i_{k}+k\neq m$\ \ \ \ \footnote{If no such $k$
exists, then we set $\operatorname*{exting}\left(  \mathbf{i}\right)  $ to be
$0$.}, where $\mathbf{i}$ is written in the form $\left(  i_{0},i_{1}%
,i_{2},...\right)  $. (Such a largest $k$ indeed exists, because (by the
definition of an $m$-degression) every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$.)

\item For any $m$-degression $\mathbf{i}$, define an integer
$\operatorname*{head}\left(  \mathbf{i}\right)  $ by $\operatorname*{head}%
\left(  \mathbf{i}\right)  =i_{0}$, where $\mathbf{i}$ is written in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $.

\item For any $m$-degression $\mathbf{i}$, define an element $v_{\mathbf{i}}$
of $\mathcal{F}^{\left(  m\right)  }$ by $v_{\mathbf{i}}=v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$, where $\mathbf{i}$ is written in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{itemize}

Thus, $\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }%
m\text{-degression}}=\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression}}$. Since \newline$\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is
an }m\text{-degression}}$ is a basis of the vector space $\mathcal{F}^{\left(
m\right)  }$, we thus conclude that $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\text{ is an }m\text{-degression}}$ is a basis of the vector space
$\mathcal{F}^{\left(  m\right)  }$.

Now we prove a simple fact:%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{If }\mathbf{i}\text{ is an }m\text{-degression, and }P\text{ is an
integer such that }\\
P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \text{, then
}v_{\mathbf{i}}\in j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)
\end{array}
\right)  . \label{pf.plu.inf.cover.1}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.cover.1}):} Let $\mathbf{i}$ be an
$m$-degression, and $P$ be an integer such that $P\geq\max\left\{
0,\operatorname*{exting}\left(  \mathbf{i}\right)  -m,\operatorname*{head}%
\left(  \mathbf{i}\right)  \right\}  $. Write $\mathbf{i}$ in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $. Then, $\operatorname*{exting}\left(
\mathbf{i}\right)  $ is the largest $k\in\mathbb{N}$ satisfying $i_{k}+k\neq
m$ (by the definition of $\operatorname*{exting}\left(  \mathbf{i}\right)  $).
Hence,%
\begin{equation}
\text{every }k\in\mathbb{N}\text{ such that }k>\operatorname*{exting}\left(
\mathbf{i}\right)  \text{ satisfies }i_{k}+k=m. \label{pf.plu.inf.cover.2}%
\end{equation}


Since $P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \geq0$, the map
$j_{P}^{\left(  m\right)  }$ and the space $V_{P}$ are well-defined.

Since $P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \geq
\operatorname*{exting}\left(  \mathbf{i}\right)  -m$, we have $P+m\geq
\operatorname*{exting}\left(  \mathbf{i}\right)  \geq0$. Now,%
\begin{equation}
\text{every positive integer }\ell\text{ satisfies }i_{P+m+\ell}%
=-P-\ell\label{pf.plu.inf.cover.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.cover.3}):} Let $\ell\in
\mathbb{N}$ be a positive integer. Then, $P+m+\underbrace{\ell}_{>0}%
>P+m\geq\operatorname*{exting}\left(  \mathbf{i}\right)  $. Hence,
(\ref{pf.plu.inf.cover.2}) (applied to $k=P+m+\ell$) yields $i_{P+m+\ell
}+P+m+\ell=m$. In other words, $i_{P+m+\ell}=-P-\ell$. This proves
(\ref{pf.plu.inf.cover.3}).}. Applied to $\ell=1$, this yields $i_{P+m+1}%
=-P-1$.

Notice also that $P\geq\max\left\{  0,\operatorname*{exting}\left(
\mathbf{i}\right)  -m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}
\geq\operatorname*{head}\left(  \mathbf{i}\right)  =i_{0}$ (by the definition
of $\operatorname*{head}\left(  \mathbf{i}\right)  $). Now it is easy to see
that%
\begin{equation}
\text{every }k\in\mathbb{N}\text{ such that }k\leq P+m\text{ satisfies
}v_{i_{k}}\in V_{P}. \label{pf.plu.inf.cover.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.cover.4}):} Let $k\in\mathbb{N}$
be such that $k\leq P+m$. Thus, $k<P+m+1$.
\par
Since $\left(  i_{0},i_{1},i_{2},...\right)  =\mathbf{i}$ is an $m$%
-degression, the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ is strictly
decreasing, i. e., we have $i_{0}>i_{1}>i_{2}>...$. As a consequence,
$i_{0}\geq i_{k}$ (since $0\leq k$) and $i_{k}>i_{P+m+1}$ (since $k<P+m+1$).
Since $i_{k}>i_{P+m+1}=-P-1$, we have $i_{k}\geq-P$ (since both $i_{k}$ and
$-P$ are integers). Combining $P\geq i_{0}\geq i_{k}$ with $i_{k}\geq-P$, we
obtain $P\geq i_{k}\geq-P$. Hence, $v_{i_{k}}\in\left\langle v_{-P}%
,v_{-P+1},...,v_{P}\right\rangle =V_{P}$ (because $V_{P}$ is defined as
$\left\langle v_{-P},v_{-P+1},...,v_{P}\right\rangle $). This proves
(\ref{pf.plu.inf.cover.4}).} Hence, $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{P+m}}\in\wedge^{P+m+1}\left(  V_{P}\right)  $. Now, by the definition of
$j_{P}^{\left(  m\right)  }$, we have%
\begin{align*}
&  j_{P}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{P+m}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\wedge
\underbrace{v_{-P-1}\wedge v_{-P-2}\wedge v_{-P-3}\wedge...}%
_{\substack{=v_{i_{P+m+1}}\wedge v_{i_{P+m+2}}\wedge v_{i_{P+m+3}}%
\wedge...\\\text{(because every positive integer }\ell\\\text{satisfies
}-P-\ell=i_{P+m+\ell}\text{ (by (\ref{pf.plu.inf.cover.3})))}}}\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\wedge v_{i_{P+m+1}%
}\wedge v_{i_{P+m+2}}\wedge v_{i_{P+m+3}}\wedge...=v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...=v_{\mathbf{i}}%
\end{align*}
(since $v_{\mathbf{i}}$ was defined as $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$). Thus, $v_{\mathbf{i}}=j_{P}^{\left(  m\right)  }\left(
\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}}_{\in
\wedge^{P+m+1}\left(  V_{P}\right)  }\right)  \in j_{P}^{\left(  m\right)
}\left(  \wedge^{P+m+1}\left(  V_{P}\right)  \right)  $. This proves
(\ref{pf.plu.inf.cover.1}).

Now, fix an arbitrary $Q\in\mathbb{N}$.

Let $w$ be any element of $\mathcal{F}^{\left(  m\right)  }$. Since $\left(
v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }m\text{-degression}}$ is a
basis of $\mathcal{F}^{\left(  m\right)  }$, we can write $w$ as a linear
combination of elements of the family $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\text{ is an }m\text{-degression}}$. Since every linear
combination contains only finitely many vectors, this yields that we can write
$w$ as a linear combination of \textbf{finitely many} elements of the family
$\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }m\text{-degression}%
}$. In other words, there exists a finite set $S$ of $m$-degressions such that
$w$ is a linear combination of the family $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\in S}$. Consider this $S$. Since $w$ is a linear combination of
the family $\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\in S}$, we can find a
scalar $\lambda_{\mathbf{i}}\in\mathbb{C}$ for every $\mathbf{i}\in S$ such
that $w=\sum\limits_{\mathbf{i}\in S}\lambda_{\mathbf{i}}v_{\mathbf{i}}$.
Consider these scalars $\lambda_{\mathbf{i}}$. Let
\[
P=\max\left\{  Q,\max\left\{  \max\left\{  0,\operatorname*{exting}\left(
\mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}
\ \mid\ \mathbf{j}\in S\right\}  \right\}
\]
(where the maximum of the empty set is to be understood as $0$). Then, first
of all, $P\geq Q$. Second, every $\mathbf{i}\in S$ satisfies%
\begin{align*}
P  &  =\max\left\{  Q,\max\left\{  \max\left\{  0,\operatorname*{exting}%
\left(  \mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)
\right\}  \ \mid\ \mathbf{j}\in S\right\}  \right\} \\
&  \geq\max\left\{  \max\left\{  0,\operatorname*{exting}\left(
\mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}
\ \mid\ \mathbf{j}\in S\right\} \\
&  \geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \text{ is an
element of the set}\\
\left\{  \max\left\{  0,\operatorname*{exting}\left(  \mathbf{j}\right)
-m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}  \ \mid
\ \mathbf{j}\in S\right\}  \text{ (because }\mathbf{i}\in S\text{),}\\
\text{and the maximum of a set is }\geq\text{ to any element of this set}%
\end{array}
\right)
\end{align*}
and thus $v_{\mathbf{i}}\in j_{P}^{\left(  m\right)  }\left(  \wedge
^{P+m+1}\left(  V_{P}\right)  \right)  $ (by (\ref{pf.plu.inf.cover.1})).
Hence,%
\begin{align*}
w  &  =\sum\limits_{\mathbf{i}\in S}\lambda_{\mathbf{i}}%
\underbrace{v_{\mathbf{i}}}_{\in j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  }\in\sum\limits_{\mathbf{i}\in
S}\lambda_{\mathbf{i}}j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  \subseteq j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  \text{ is a vector space}\right)
\\
&  \subseteq\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\geq Q\right)  .
\end{align*}


Now, forget that we fixed $w$. We thus have proven that every $w\in
\mathcal{F}^{\left(  m\right)  }$ satisfies $w\in\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $. Thus, $\mathcal{F}^{\left(
m\right)  }\subseteq\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}%
}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)
\right)  $. Combined with the obvious inclusion $\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  \subseteq\mathcal{F}^{\left(
m\right)  }$, this yields $\mathcal{F}^{\left(  m\right)  }=\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $. Proposition
\ref{prop.plu.inf.cover} \textbf{(b)} is thus proven.

\begin{verlong}
And a corollary of Proposition \ref{prop.plu.inf.cover} (that we won't need):

\begin{corollary}
\label{cor.plu.inf.cover.tensor}Let $m\in\mathbb{Z}$. We have $\mathcal{F}%
^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }=\bigcup
\limits_{N\in\mathbb{N}}\left(  j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  \otimes j_{N}^{\left(  m\right)
}\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  \right)  $.
\end{corollary}

To prove this, we need the following lemma:

\begin{lemma}
\label{lem.plu.inf.cover.tensor}Let $W$ be a vector space, and $\left(
W_{n}\right)  _{n\in\mathbb{N}}$ a family of vector subspaces of $W$ such that
$W_{0}\subseteq W_{1}\subseteq W_{2}\subseteq...$ and $W=\bigcup
\limits_{n\in\mathbb{N}}W_{n}$. Let $U$ be a vector space, and $\left(
U_{n}\right)  _{n\in\mathbb{N}}$ a family of vector subspaces of $U$ such that
$U_{0}\subseteq U_{1}\subseteq U_{2}\subseteq...$ and $U=\bigcup
\limits_{n\in\mathbb{N}}U_{n}$. Then, $U\otimes W=\bigcup\limits_{n\in
\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.cover.tensor}.} Let $t\in U\otimes W$
be arbitrary. Since $t$ is a tensor, we can write $t$ in the form
$t=\sum\limits_{i=1}^{m}u_{i}\otimes w_{i}$ for some $m\in\mathbb{N}$, some
elements $u_{1}$, $u_{2}$, $...$, $u_{m}$ of $U$, and some elements $w_{1}$,
$w_{2}$, $...$, $w_{m}$ of $W$. Consider this $u$, these $u_{1}$, $u_{2}$,
$...$, $u_{m}$ and these $w_{1}$, $w_{2}$, $...$, $w_{m}$.

For every $i\in\left\{  1,2,...,m\right\}  $, there exists some $\alpha_{i}%
\in\mathbb{N}$ such that $u_{i}\in U_{\alpha_{i}}$ (since $u_{i}\in
U=\bigcup\limits_{n\in\mathbb{N}}U_{n}$). Consider this $\alpha_{i}$.

For every $i\in\left\{  1,2,...,m\right\}  $, there exists some $\beta_{i}%
\in\mathbb{N}$ such that $w_{i}\in W_{\beta_{i}}$ (since $w_{i}\in
W=\bigcup\limits_{n\in\mathbb{N}}W_{n}$). Consider this $\beta_{i}$.

Let $N=\max\left(  \left\{  \alpha_{1},\alpha_{2},...,\alpha_{m}\right\}
\cup\left\{  \beta_{1},\beta_{2},...,\beta_{m}\right\}  \right)  $. Then,
every $i\in\left\{  1,2,...,m\right\}  $ satisfies $\alpha_{i}\in\left\{
\alpha_{1},\alpha_{2},...,\alpha_{m}\right\}  \subseteq\left\{  \alpha
_{1},\alpha_{2},...,\alpha_{m}\right\}  \cup\left\{  \beta_{1},\beta
_{2},...,\beta_{m}\right\}  $, so that%
\begin{align*}
\alpha_{i}  &  \leq\max\left(  \left\{  \alpha_{1},\alpha_{2},...,\alpha
_{m}\right\}  \cup\left\{  \beta_{1},\beta_{2},...,\beta_{m}\right\}  \right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since any element of a finite set is
}\leq\text{ to the maximum of this set}\right) \\
&  =N
\end{align*}
and thus $U_{\alpha_{i}}\subseteq U_{N}$ (since $U_{0}\subseteq U_{1}\subseteq
U_{2}\subseteq...$), so that $u_{i}\in U_{\alpha_{i}}\subseteq U_{N}$.
Similarly, every $i\in\left\{  1,2,...,m\right\}  $ satisfies $w_{i}\in W_{N}%
$. Thus,%
\begin{align*}
t  &  =\sum\limits_{i=1}^{m}\underbrace{u_{i}}_{\in U_{N}}\otimes
\underbrace{w_{i}}_{\in W_{N}}\in\sum\limits_{i=1}^{m}U_{N}\otimes
W_{N}\subseteq U_{N}\otimes W_{N}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U_{N}\otimes W_{N}\text{ is a
}k\text{-vector space}\right) \\
&  \subseteq\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)
.
\end{align*}


Now, forget that we fixed $t$. We thus have proven that every $t\in U\otimes
W$ satisfies $t\in\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes
W_{n}\right)  $. In other words, $U\otimes W\subseteq\bigcup\limits_{n\in
\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)  $. Combined with the obvious
inclusion $\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)
\subseteq U\otimes W$, this yields $U\otimes W=\bigcup\limits_{n\in\mathbb{N}%
}\left(  U_{n}\otimes W_{n}\right)  $, so that Lemma
\ref{lem.plu.inf.cover.tensor} is proven.

\textit{Proof of Corollary \ref{cor.plu.inf.cover.tensor}.} Proposition
\ref{prop.plu.inf.cover} \textbf{(b)} (applied to $Q=0$) yields
\[
\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in\mathbb{N}%
;\\N\geq0}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  .
\]
Proposition \ref{prop.plu.inf.cover} \textbf{(a)} yields $j_{0}^{\left(
m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)  \subseteq
j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(  V_{1}\right)  \right)
\subseteq j_{2}^{\left(  m\right)  }\left(  \wedge^{2+m+1}\left(
V_{2}\right)  \right)  \subseteq...$. Thus, Lemma
\ref{lem.plu.inf.cover.tensor} (applied to $W=\mathcal{F}^{\left(  m\right)
}$, $W_{i}=j_{i}^{\left(  m\right)  }\left(  \wedge^{i+m+1}\left(
V_{1}\right)  \right)  $, $U=\mathcal{F}^{\left(  m\right)  }$ and
$U_{i}=j_{i}^{\left(  m\right)  }\left(  \wedge^{i+m+1}\left(  V_{1}\right)
\right)  $) yields $\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}%
^{\left(  m\right)  }=\bigcup\limits_{N\in\mathbb{N}}\left(  j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  \otimes
j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\right)  $. This proves Corollary \ref{cor.plu.inf.cover.tensor}.
\end{verlong}

What comes next is almost a carbon copy of Definition
\ref{def.createdestroy.fin}:

\begin{definition}
\label{def.plu.inf.createdestroy}Let $N\in\mathbb{N}$. Let $k\in\mathbb{Z}$.
Let $i\in\left\{  -N,-N+1,...,N\right\}  $.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}^{\left(  N\right)  }}:\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k+1}\left(  V_{N}\right)  $ by%
\[
\widehat{v_{i}^{\left(  N\right)  }}\cdot\psi=v_{i}\wedge\psi
\ \ \ \ \ \ \ \ \ \ \text{for all }\psi\in\wedge^{k}\left(  V_{N}\right)  .
\]


\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}^{\left(  N\right)  }}:\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k-1}\left(  V_{N}\right)  $ as follows:

For every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $N\geq i_{1}>i_{2}>...>i_{k}\geq-N$, we let $\overset{\vee
}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge
v_{i_{k}}\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}
;\\
\left(  -1\right)  ^{j-1}v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{j-1}%
}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{1},i_{2},...,i_{k}\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell}=i$. Thus, the map
$\overset{\vee}{v_{i}^{\left(  N\right)  }}$ is defined on a basis of the
vector space $\wedge^{k}\left(  V_{N}\right)  $; we extend this to a map
$\wedge^{k}\left(  V_{N}\right)  \rightarrow\wedge^{k-1}\left(  V_{N}\right)
$ by linearity.

Note that, for every negative $\ell\in\mathbb{Z}$, we understand $\wedge
^{\ell}\left(  V_{N}\right)  $ to mean the zero space.
\end{definition}

Also:

\begin{definition}
For every $N\in\mathbb{N}$ and $k\in\left\{  1,2,...,2N+1\right\}  $, let
$\Omega_{N}^{\left(  k\right)  }$ denote the orbit of $v_{N}\wedge
v_{N-1}\wedge...\wedge v_{N-k+1}$ under the action of $\operatorname*{GL}%
\left(  V_{N}\right)  $.
\end{definition}

The following lemma, then, is an easy corollary of Theorem \ref{thm.plu}:

\begin{lemma}
\label{lem.plu.inf.plu}Let $N\in\mathbb{N}$ and $k\in\mathbb{Z}$. Let
$S_{N}^{\left(  k\right)  }=\sum\limits_{i=-N}^{N}\widehat{v_{i}^{\left(
N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}:\wedge
^{k}\left(  V_{N}\right)  \otimes\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k+1}\left(  V_{N}\right)  \otimes\wedge^{k-1}\left(
V_{N}\right)  $.

\textbf{(a)} This map $S_{N}^{\left(  k\right)  }$ does not depend on the
choice of the basis of $V_{N}$, and is $\operatorname*{GL}\left(
V_{N}\right)  $-invariant. In other words, for \textbf{any} basis $\left(
w_{N},w_{N-1},...,w_{-N}\right)  $ of $V_{N}$, we have $S_{N}^{\left(
k\right)  }=\sum\limits_{i=-N}^{N}\widehat{w_{i}^{\left(  N\right)  }}%
\otimes\overset{\vee}{w_{i}^{\left(  N\right)  }}$ (where the maps
$\widehat{w_{i}^{\left(  N\right)  }}$ and $\overset{\vee}{w_{i}^{\left(
N\right)  }}$ are defined just as $\widehat{v_{i}^{\left(  N\right)  }}$ and
$\overset{\vee}{v_{i}^{\left(  N\right)  }}$, but with respect to the basis
$\left(  w_{N},w_{N-1},...,w_{-N}\right)  $).

\textbf{(b)} Let $k\in\left\{  1,2,...,2N+1\right\}  $. A nonzero element
$\tau\in\wedge^{k}\left(  V_{N}\right)  $ belongs to $\Omega_{N}^{\left(
k\right)  }$ if and only if $S_{N}^{\left(  k\right)  }\left(  \tau\otimes
\tau\right)  =0$.

\textbf{(c)} The map $S_{N}^{\left(  k\right)  }$ is $\operatorname*{M}\left(
V_{N}\right)  $-invariant.
\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.plu}.} If we set $n=2N+1$ in Theorem
\ref{thm.plu}, and do the following renaming operations:

\begin{itemize}
\item rename the standard basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ as
$\left(  v_{N},v_{N-1},...,v_{-N}\right)  $;

\item rename the vector space $V$ as $V_{N}$;

\item rename the map $S$ as $S_{N}^{\left(  k\right)  }$;

\item rename the basis $\left(  w_{1},w_{2},...,w_{n}\right)  $ as $\left(
w_{N},w_{N-1},...,w_{-N}\right)  $;

\item rename the maps $\widehat{v_{i}}$ as $\widehat{v_{i}^{\left(  N\right)
}}$;

\item rename the maps $\overset{\vee}{v_{i}}$ as $\overset{\vee}{v_{i}%
^{\left(  N\right)  }}$;

\item rename the maps $\widehat{w_{i}}$ as $\widehat{w_{i}^{\left(  N\right)
}}$;

\item rename the maps $\overset{\vee}{w_{i}}$ as $\overset{\vee}{w_{i}%
^{\left(  N\right)  }}$;

\item rename the set $\Omega$ as $\Omega_{N}^{\left(  k\right)  }$;
\end{itemize}

then what we obtain is exactly the statement of Lemma \ref{lem.plu.inf.plu}.
Thus, Lemma \ref{lem.plu.inf.plu} is proven.

The maps $S_{N}^{\left(  k\right)  }$ have their own compatibility relation
with the $j_{N}^{\left(  m\right)  }$:

\begin{lemma}
\label{lem.plu.inf.S.comp}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. Define
the notation $S_{N}^{\left(  N+m+1\right)  }$ as in Lemma
\ref{lem.plu.inf.plu}. Then,%
\[
\left(  j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)
}\right)  \circ S_{N}^{\left(  N+m+1\right)  }=S\circ\left(  j_{N}^{\left(
m\right)  }\otimes j_{N}^{\left(  m\right)  }\right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.S.comp}.} Define the maps
$\widehat{v_{i}^{\left(  N\right)  }}$ and $\overset{\vee}{v_{i}^{\left(
N\right)  }}$ (for all $i\in\left\{  -N,-N+1,...,N\right\}  $) as in
Definition \ref{def.plu.inf.createdestroy}. Define the maps $\widehat{v_{i}}$
and $\overset{\vee}{v_{i}}$ (for all $i\in\mathbb{Z}$) as in Definition
\ref{def.createdestroy}.

\textbf{a)} Let us first show that%
\begin{equation}
j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }%
}=\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left\{  N,N-1,...,-N\right\}  . \label{pf.plu.inf.S.comp.a}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.a}):} Let $i\in\left\{
N,N-1,...,-N\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.a}), it is
clearly enough to show that $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\widehat{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $
for every $u\in\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}%
^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $. Since this equality is
linear in $u$, we can WLOG assume that $u$ is an element of the basis $\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq
i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $.
Assume this. Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1}%
,...,i_{N+m}\right)  $ of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}%
\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider
this $N+m+1$-tuple.

Comparing%
\begin{align*}
\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }%
}\right)  \left(  u\right)   &  =j_{N}^{\left(  m+1\right)  }%
\underbrace{\left(  \widehat{v_{i}^{\left(  N\right)  }}\left(  u\right)
\right)  }_{\substack{=v_{i}\wedge u\\\text{(by the definition of
}\widehat{v_{i}^{\left(  N\right)  }}\text{)}}}=j_{N}^{\left(  m+1\right)
}\left(  v_{i}\wedge\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}}\right) \\
&  =j_{N}^{\left(  m+1\right)  }\left(  v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m+1\right)  }\right)
\end{align*}
with%
\begin{align*}
\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\widehat{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =v_{i}\wedge j_{N}^{\left(  m\right)  }\left(
\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{v_{i}}\right)
\\
&  =v_{i}\wedge\underbrace{j_{N}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  }_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(by the definition of }j_{N}^{\left(  m\right)
}\text{)}}}\\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...,
\end{align*}
we obtain $\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right)  =\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $. This is exactly what
we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.a}). The proof of (\ref{pf.plu.inf.S.comp.a}) is thus finished.

\textbf{b)} Let us next show that%
\begin{equation}
j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(  N\right)  }%
}=\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  N,N-1,...,-N\right\}  .
\label{pf.plu.inf.S.comp.b}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.b}):} Let $i\in\left\{
N,N-1,...,-N\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.b}), it is
clearly enough to show that $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)
=\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $ for every $u\in\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $. Since this equality is linear in $u$, we can WLOG assume that $u$
is an element of the basis $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge
^{N+m+1}\left(  V_{N}\right)  $. Assume this. Then, there exists an
$N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $ of integers such that
$N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be the sequence $\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  $. From $u=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m\right)  }\right) \nonumber\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\label{pf.plu.inf.S.comp.b.0}%
\end{align}


We distinguish between two cases:

\textit{Case 1:} We have $i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $.

\textit{Case 2:} We have $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $.

Let us first consider Case 1. In this case, from $u=v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align*}
&  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right) \\
&  =\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},...,i_{N+m}%
\right\}  ;\\
\left(  -1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\left(  j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge
v_{i_{\left(  j-1\right)  +2}}\wedge...\wedge v_{i_{N+m}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$.\ \ \ \ \footnote{If you
are wondering where the $-1$ (for example, in $i_{\ell-1}$ and in $i_{\left(
j-1\right)  -1}$) comes from: It comes from the fact that the indexing of our
$N+m+1$-tuple $\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{N+m}}\right)  $ begins
with $0$, and not with $1$ as in Definition \ref{def.createdestroy.fin}.}
Since $i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $ (because we are in
Case 1), this simplifies to%
\[
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)  =0.
\]


On the other hand, combining $i\notin\left\{  -N-1,-N-2,-N-3,...\right\}  $
(which is because $i\in\left\{  N,N-1,...,-N\right\}  $) with $i\notin\left\{
i_{0},i_{1},...,i_{N+m}\right\}  $ (which is because we are in Case 1), we
obtain%
\begin{align*}
i  &  \notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  \cup\left\{
-N-1,-N-2,-N-3,...\right\} \\
&  =\left\{  i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right\}  =\left\{
j_{0},j_{1},j_{2},...\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\end{align*}
Now,%
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.b.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\notin\left\{
j_{0},j_{1},j_{2},...\right\}  $, this simplifies to%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =0.
\]
Compared with%
\[
\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right)  =j_{N}^{\left(  m+1\right)
}\underbrace{\left(  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(
u\right)  \right)  }_{=0}=0,
\]
this yields $\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $. We have thus proven $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)
=\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $ in Case 1.

Next, let us consider Case 2. In this case, $i\in\left\{  i_{0},i_{1}%
,...,i_{N+m}\right\}  $, so there exists an $\ell\in\left\{
0,1,...,N+m\right\}  $ such that $i_{\ell}=i$. Denote this $\ell$ by $\kappa$.
Then, $i_{\kappa}=i$. Clearly,
\begin{align}
&  \left(  i_{0},i_{1},...,i_{\kappa-1},i_{\kappa+1},i_{\kappa+2}%
,...,i_{N+m},-N-1,-N-2,-N-3,...\right) \nonumber\\
&  =\left(  \text{result of removing the }\kappa+1\text{-th term from the
sequence} \phantom{\dfrac{\dfrac{I}{I}}{I}} \right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  \underbrace{\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  }_{=\left(  j_{0}%
,j_{1},j_{2},...\right)  }\right) \nonumber\\
&  =\left(  \text{result of removing the }\kappa+1\text{-th term from the
sequence }\left(  j_{0},j_{1},j_{2},...\right)  \right) \nonumber\\
&  =\left(  j_{0},j_{1},...,j_{\kappa-1},j_{\kappa+1},j_{\kappa+2},...\right)
. \label{pf.plu.inf.S.comp.b.2.triv}%
\end{align}


From $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align*}
&  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right) \\
&  =\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},...,i_{N+m}%
\right\}  ;\\
\left(  -1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\left(  j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge
v_{i_{\left(  j-1\right)  +2}}\wedge...\wedge v_{i_{N+m}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$.\ \ \ \ \footnote{If you
are wondering where the $-1$ (for example, in $i_{\ell-1}$ and in $i_{\left(
j-1\right)  -1}$) comes from: It comes from the fact that the indexing of our
$N+m+1$-tuple $\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{N+m}}\right)  $ begins
with $0$, and not with $1$ as in Definition \ref{def.createdestroy.fin}.}
Since $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, this simplifies to%
\[
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)  =\left(
-1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\left(
j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge v_{i_{\left(
j-1\right)  +2}}\wedge...\wedge v_{i_{N+m}},
\]
where we denote by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$. Since the
integer $\ell$ satisfying $i_{\ell-1}=i$ is $\kappa+1$ (because $i_{\left(
\kappa+1\right)  -1}=i_{\kappa}=i$), this rewrites as%
\begin{align*}
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)   &  =\left(
-1\right)  ^{\left(  \kappa+1\right)  -1}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{\left(  \left(  \kappa+1\right)  -1\right)  -1}}\wedge
v_{i_{\left(  \left(  \kappa+1\right)  -1\right)  +1}}\wedge v_{i_{\left(
\left(  \kappa+1\right)  -1\right)  +2}}\wedge...\wedge v_{i_{N+m}}\\
&  =\left(  -1\right)  ^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge
v_{i_{N+m}}%
\end{align*}
(since $\left(  \kappa+1\right)  -1=\kappa$). Thus,%
\begin{align}
&  \left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right) \nonumber\\
&  =j_{N}^{\left(  m+1\right)  }\left(  \underbrace{\overset{\vee
}{v_{i}^{\left(  N\right)  }}\left(  u\right)  }_{=\left(  -1\right)
^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\kappa-1}}\wedge
v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge v_{i_{N+m}}}\right)
\nonumber\\
&  =j_{N}^{\left(  m+1\right)  }\left(  \left(  -1\right)  ^{\kappa}v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge
v_{i_{\kappa+2}}\wedge...\wedge v_{i_{N+m}}\right) \nonumber\\
&  =\left(  -1\right)  ^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge
v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m+1\right)  }\right) \nonumber\\
&  =\left(  -1\right)  ^{\kappa}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\wedge v_{j_{\kappa-1}}\wedge v_{j_{\kappa+1}}\wedge v_{j_{\kappa
+2}}\wedge...\label{pf.plu.inf.S.comp.b.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{pf.plu.inf.S.comp.b.2.triv}) yields}\\
\left(  i_{0},i_{1},...,i_{\kappa-1},i_{\kappa+1},i_{\kappa+2},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right) \\
=\left(  j_{0},j_{1},...,j_{\kappa-1},j_{\kappa+1},j_{\kappa+2},...\right)
\end{array}
\right)  .\nonumber
\end{align}


On the other hand,
\begin{align*}
i  &  \in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  \subseteq\left\{
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right\}  =\left\{  j_{0}%
,j_{1},j_{2},...\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\end{align*}
Moreover, the integer $k$ satisfying $j_{k}=i$ is $\kappa$%
\ \ \ \ \footnote{because%
\begin{align*}
j_{\kappa}  &  =i_{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ and }\kappa\in\left\{  0,1,...,N+m\right\}  \right)
\\
&  =i
\end{align*}
}. Now,
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.b.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\in\left\{  j_{0}%
,j_{1},j_{2},...\right\}  $, this simplifies to%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge...,
\]
where we denote by $j$ the integer $k$ satisfying $j_{k}=i$. Since the integer
$k$ satisfying $j_{k}=i$ is $\kappa$, this rewrites as%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =\left(  -1\right)  ^{\kappa}v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\wedge v_{j_{\kappa-1}}\wedge v_{j_{\kappa+1}}\wedge
v_{j_{\kappa+2}}\wedge....
\]
Compared with (\ref{pf.plu.inf.S.comp.b.2}), this yields $\left(
j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }}\right)
\left(  u\right)  =\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  \left(  u\right)  $. This is exactly what we needed to prove in
order to complete the proof of (\ref{pf.plu.inf.S.comp.b}). The proof of
(\ref{pf.plu.inf.S.comp.b}) is thus finished.

\textbf{c)} Let us next show that%
\begin{equation}
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }=0\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left\{  -N-1,-N-2,-N-3,...\right\}  . \label{pf.plu.inf.S.comp.c}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.c}):} Let $i\in\left\{
-N-1,-N-2,-N-3,...\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.c}),
it is clearly enough to show that $\left(  \widehat{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \left(  u\right)  =0$ for every $u\in\wedge^{N+m+1}\left(
V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  \left(  u\right)  =0$. Since this equality is linear in $u$, we can
WLOG assume that $u$ is an element of the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}%
>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume this.
Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $
of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

The vector $v_{i}$ occurs twice in the semiinfinite wedge $v_{i}\wedge
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ (namely, it occurs once in the very
beginning of this wedge, and then it occurs again in the $v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ part (because $i\in\left\{
-N-1,-N-2,-N-3,...\right\}  $)). Hence, the semiinfinite wedge $v_{i}\wedge
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ equals $0$ (since a semiinfinite wedge in
which a vector occurs more than once must always be equal to $0$).

Now,%
\begin{align*}
\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\widehat{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =v_{i}\wedge j_{N}^{\left(  m\right)  }\left(
\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{v_{i}}\right)
\\
&  =v_{i}\wedge\underbrace{j_{N}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  }_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(by the definition of }j_{N}^{\left(  m\right)
}\text{)}}}\\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{as we proved above}\right)  .
\end{align*}
This is exactly what we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.c}). The proof of (\ref{pf.plu.inf.S.comp.c}) is thus finished.

\textbf{d)} Let us now show that
\begin{equation}
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }%
=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  N+1,N+2,N+3,...\right\}  .
\label{pf.plu.inf.S.comp.d}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.d}):} Let $i\in\left\{
N+1,N+2,N+3,...\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.b}), it
is clearly enough to show that $\left(  \overset{\vee}{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  =0$ for every $u\in
\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \left(  u\right)  =0$. Since this equality is linear in
$u$, we can WLOG assume that $u$ is an element of the basis $\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}%
>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume
this. Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}%
\right)  $ of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and
$u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider this
$N+m+1$-tuple.

Notice that $i\in\left\{  N+1,N+2,N+3,...\right\}  $, so that $i\notin\left\{
N,N-1,...,-N\right\}  $ and $i\notin\left\{  N,N-1,N-2,...\right\}  $.

Since $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$, we have $\left\{  i_{0}%
,i_{1},...,i_{N+m}\right\}  \subseteq\left\{  N,N-1,...,-N\right\}  $ and thus
$i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $ (because $i\notin\left\{
N,N-1,...,-N\right\}  $).

Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be the sequence $\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  $. Then,%
\begin{align*}
\left\{  j_{0},j_{1},j_{2},...\right\}   &  =\left\{  i_{0},i_{1}%
,...,i_{N+m},-N-1,-N-2,-N-3,...\right\} \\
&  =\underbrace{\left\{  i_{0},i_{1},...,i_{N+m}\right\}  }_{\subseteq\left\{
N,N-1,...,-N\right\}  }\cup\left\{  -N-1,-N-2,-N-3,...\right\} \\
&  \subseteq\left\{  N,N-1,...,-N\right\}  \cup\left\{
-N-1,-N-2,-N-3,...\right\}  =\left\{  N,N-1,N-2,...\right\}  .
\end{align*}
Thus, $i\notin\left\{  j_{0},j_{1},j_{2},...\right\}  $ (since $i\notin%
\left\{  N,N-1,N-2,...\right\}  $).

From $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m\right)  }\right) \nonumber\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  ,
\label{pf.plu.inf.S.comp.d.0}%
\end{align}
so that%
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.d.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\notin\left\{
j_{0},j_{1},j_{2},...\right\}  $, this simplifies to $\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  =0$.

This is exactly what we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.d}). The proof of (\ref{pf.plu.inf.S.comp.d}) is thus finished.

\textbf{e)} Now it is the time to draw conclusions.

We have $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$ (by the definition of $S$). Thus,%
\begin{align*}
&  S\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  =\left(  \sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes
\overset{\vee}{v_{i}}\right)  \circ\left(  j_{N}^{\left(  m\right)  }\otimes
j_{N}^{\left(  m\right)  }\right)  =\sum\limits_{i\in\mathbb{Z}}%
\underbrace{\left(  \widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)
\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  }_{=\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
\otimes\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
}\\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \widehat{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \otimes\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(
m\right)  }\right) \\
&  =\sum\limits_{i=-\infty}^{-N-1}\underbrace{\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  }_{\substack{=0\\\text{(by
(\ref{pf.plu.inf.S.comp.c}))}}}\otimes\left(  \overset{\vee}{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  +\sum\limits_{i=-N}^{N}\underbrace{\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{\substack{=j_{N}%
^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }}\\\text{(by
(\ref{pf.plu.inf.S.comp.a}))}}}\otimes\underbrace{\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{\substack{=j_{N}^{\left(
m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\\\text{(by
(\ref{pf.plu.inf.S.comp.b}))}}}+\sum\limits_{i=N+1}^{\infty}\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \otimes
\underbrace{\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  }_{\substack{=0\\\text{(by (\ref{pf.plu.inf.S.comp.d}))}}}\\
&  =\underbrace{\sum\limits_{i=-\infty}^{-N-1}0\otimes\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{=0}+\sum\limits_{i=-N}%
^{N}\underbrace{\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}%
^{\left(  N\right)  }}\right)  \otimes\left(  j_{N}^{\left(  m+1\right)
}\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  }_{=\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)
\circ\left(  \widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  }+\underbrace{\sum\limits_{i=N+1}%
^{\infty}\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
\otimes0}_{=0}\\
&  =\sum\limits_{i=-N}^{N}\left(  j_{N}^{\left(  m+1\right)  }\otimes
j_{N}^{\left(  m-1\right)  }\right)  \circ\left(  \widehat{v_{i}^{\left(
N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  =\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)
\circ\left(  \sum\limits_{i=-N}^{N}\widehat{v_{i}^{\left(  N\right)  }}%
\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  .
\end{align*}
But since $S_{N}^{\left(  N+m+1\right)  }=\sum\limits_{i=-N}^{N}%
\widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(
N\right)  }}$ (by the definition of $S_{N}^{\left(  N+m+1\right)  }$), this
rewrites as%
\[
S\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  =\left(  j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(
m-1\right)  }\right)  \circ\underbrace{\left(  \sum\limits_{i=-N}%
^{N}\widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  }_{=S_{N}^{\left(  N+m+1\right)  }}=\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)  \circ
S_{N}^{\left(  N+m+1\right)  }.
\]
This proves Lemma \ref{lem.plu.inf.S.comp}.

Now we can finally come to proving Theorem \ref{thm.plu.inf}:

\textit{Proof of Theorem \ref{thm.plu.inf}.} Let $\varrho^{\prime
}:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}\otimes\mathcal{F}\right)  $ be the action of the monoid
$\operatorname*{M}\left(  \infty\right)  $ on the tensor product of the
$\operatorname*{M}\left(  \infty\right)  $-module $\mathcal{F}$ with itself.
Clearly,%
\[
\varrho^{\prime}\left(  M\right)  =\varrho\left(  M\right)  \otimes
\varrho\left(  M\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }M\in
\operatorname*{M}\left(  \infty\right)
\]
(because this is how one defines the tensor product of two modules over a monoid).

\textbf{(c)} Let $m\in\mathbb{Z}$. Let $M\in\operatorname*{M}\left(
\infty\right)  $. Let $v\in\mathcal{F}^{\left(  m\right)  }$ and
$w\in\mathcal{F}^{\left(  m\right)  }$. We are going to prove that $\left(
S\circ\varrho^{\prime}\left(  M\right)  \right)  \left(  v\otimes w\right)
=\left(  \varrho^{\prime}\left(  M\right)  \circ S\right)  \left(  v\otimes
w\right)  $.

Since $M\in\operatorname*{M}\left(  \infty\right)  =\bigcup\limits_{N\in
\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $ (by
Remark \ref{rmk.plu.inf.iN} \textbf{(c)}), there exists an $R\in\mathbb{N}$
such that $M\in i_{R}\left(  \operatorname*{M}\left(  V_{R}\right)  \right)
$. Consider this $R$.

Since $v\in\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq R}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $Q=R$), there exists some $T\in\mathbb{N}$ such that
$T\geq R$ and $v\in j_{T}^{\left(  m\right)  }\left(  \wedge^{T+m+1}\left(
V_{T}\right)  \right)  $. Consider this $T$.

Since $w\in\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq T}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $Q=T$), there exists some $P\in\mathbb{N}$ such that
$P\geq T$ and $w\in j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  $. Consider this $P$. There exists a $w^{\prime}%
\in\wedge^{P+m+1}\left(  V_{P}\right)  $ such that $w=j_{P}^{\left(  m\right)
}\left(  w^{\prime}\right)  $ (because $w\in j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  $). Consider this $w^{\prime}$.

Applying Proposition \ref{prop.plu.inf.cover} \textbf{(a)}, we get
$j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)
\right)  \subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq...$. Thus,
$j_{T}^{\left(  m\right)  }\left(  \wedge^{T+m+1}\left(  V_{T}\right)
\right)  \subseteq j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  $ (since $T\leq P$), so that $v\in j_{T}^{\left(
m\right)  }\left(  \wedge^{T+m+1}\left(  V_{T}\right)  \right)  \subseteq
j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(  V_{P}\right)  \right)
$. Hence, there exists a $v^{\prime}\in\wedge^{P+m+1}\left(  V_{P}\right)  $
such that $v=j_{P}^{\left(  m\right)  }\left(  v^{\prime}\right)  $. Consider
this $v^{\prime}$. Since $v=j_{P}^{\left(  m\right)  }\left(  v^{\prime
}\right)  $ and $w=j_{P}^{\left(  m\right)  }\left(  w^{\prime}\right)  $, we
have%
\begin{equation}
v\otimes w=j_{P}^{\left(  m\right)  }\left(  v^{\prime}\right)  \otimes
j_{P}^{\left(  m\right)  }\left(  w^{\prime}\right)  =\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  \left(  v^{\prime
}\otimes w^{\prime}\right)  . \label{pf.plu.inf.-20}%
\end{equation}


Since $R\leq T\leq P$, we have $i_{R}\left(  \operatorname*{M}\left(
V_{R}\right)  \right)  \subseteq i_{P}\left(  \operatorname*{M}\left(
V_{P}\right)  \right)  $ (since Remark \ref{rmk.plu.inf.iN} \textbf{(b)}
yields $i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)  \right)  \subseteq
i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)  \right)  \subseteq
i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)  \right)  \subseteq...$).
Thus, $M\in i_{R}\left(  \operatorname*{M}\left(  V_{R}\right)  \right)
\subseteq i_{P}\left(  \operatorname*{M}\left(  V_{P}\right)  \right)  $. In
other words, there exists an $A\in\operatorname*{M}\left(  V_{P}\right)  $
such that $M=i_{P}\left(  A\right)  $. Consider this $A$.

In the following, we will write the action of $\operatorname*{M}\left(
\infty\right)  $ on $\mathcal{F}$ as a left action. In other words, we will
abbreviate $\left(  \varrho\left(  N\right)  \right)  u$ by $Nu$, wherever
$N\in\operatorname*{M}\left(  \infty\right)  $ and $u\in\mathcal{F}$.
Similarly, we will write the action of $\operatorname*{M}\left(
\infty\right)  $ on $\mathcal{F}\otimes\mathcal{F}$ (this action is obtained
by tensoring the $\operatorname*{M}\left(  \infty\right)  $-module
$\mathcal{F}$ with itself); this action satisfies $\varrho^{\prime}\left(
A\right)  =\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  $.

Let us also denote by $\varrho$ the action of the monoid $\operatorname*{M}%
\left(  V_{N}\right)  $ on $\wedge\left(  V_{N}\right)  $. Moreover, let us
denote by $\varrho^{\prime}$ the action of the monoid $\operatorname*{M}%
\left(  V_{N}\right)  $ on $\wedge\left(  V_{N}\right)  \otimes\wedge\left(
V_{N}\right)  $ (this action is obtained by tensoring the $\operatorname*{M}%
\left(  V_{N}\right)  $-module $\wedge\left(  V_{N}\right)  $ with itself).

We notice that every $\ell\in\mathbb{Z}$ satisfies%
\begin{equation}
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  \ell\right)
}=j_{P}^{\left(  \ell\right)  }\circ\left(  \varrho\left(  A\right)  \right)
. \label{pf.plu.inf.-10}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.-10}):} Let $\ell\in\mathbb{Z}$.
Every $u\in\mathcal{F}^{\left(  \ell\right)  }$ satisfies%
\begin{align*}
\left(  \left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(
\ell\right)  }\right)  \left(  u\right)   &  =\left(  \varrho\left(  M\right)
\right)  \left(  j_{P}^{\left(  \ell\right)  }\left(  u\right)  \right)
=\underbrace{M}_{=i_{P}\left(  A\right)  }\cdot j_{P}^{\left(  \ell\right)
}\left(  u\right)  =i_{P}\left(  A\right)  \cdot j_{P}^{\left(  \ell\right)
}u=j_{P}^{\left(  \ell\right)  }\underbrace{\left(  Au\right)  }_{=\left(
\varrho\left(  A\right)  \right)  u}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.plu.inf.iNjmN},
applied to }P\text{ and }\ell\text{ instead of }N\text{ and }m\right) \\
&  =j_{P}^{\left(  \ell\right)  }\left(  \left(  \varrho\left(  A\right)
\right)  u\right)  =\left(  j_{P}^{\left(  \ell\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \left(  u\right)  .
\end{align*}
Thus, $\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(
\ell\right)  }=j_{P}^{\left(  \ell\right)  }\circ\left(  \varrho\left(
A\right)  \right)  $, so that (\ref{pf.plu.inf.-10}) is proven.}

Applying Lemma \ref{lem.plu.inf.S.comp} to $N=P$, we obtain
\begin{equation}
\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ S_{P}^{\left(  P+m+1\right)  }=S\circ\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  . \label{pf.plu.inf.1}%
\end{equation}


On the other hand, the map $S_{P}^{\left(  P+m+1\right)  }$ is
$\operatorname*{M}\left(  \infty\right)  $-invariant (by Lemma
\ref{lem.plu.inf.plu} \textbf{(c)}, applied to $N=P$ and $k=P+m+1$), so that%
\[
S_{P}^{\left(  P+m+1\right)  }\circ\left(  \varrho^{\prime}\left(  A\right)
\right)  =\left(  \varrho^{\prime}\left(  A\right)  \right)  \circ
S_{P}^{\left(  P+m+1\right)  }.
\]
Since $\varrho^{\prime}\left(  A\right)  =\varrho\left(  A\right)
\otimes\varrho\left(  A\right)  $, this rewrites as
\begin{equation}
S_{P}^{\left(  P+m+1\right)  }\circ\left(  \varrho\left(  A\right)
\otimes\varrho\left(  A\right)  \right)  =\left(  \varrho\left(  A\right)
\otimes\varrho\left(  A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  }.
\label{pf.plu.inf.2}%
\end{equation}


Comparing%
\begin{align*}
&  S\circ\underbrace{\left(  \varrho^{\prime}\left(  M\right)  \right)
}_{=\varrho\left(  M\right)  \otimes\varrho\left(  M\right)  }\circ\left(
j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)  }\right) \\
&  =S\circ\underbrace{\left(  \varrho\left(  M\right)  \otimes\varrho\left(
M\right)  \right)  \circ\left(  j_{P}^{\left(  m\right)  }\otimes
j_{P}^{\left(  m\right)  }\right)  }_{=\left(  \left(  \varrho\left(
M\right)  \right)  \circ j_{P}^{\left(  m\right)  }\right)  \otimes\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m\right)
}\right)  }\\
&  =S\circ\left(  \underbrace{\left(  \left(  \varrho\left(  M\right)
\right)  \circ j_{P}^{\left(  m\right)  }\right)  }_{\substack{=j_{P}^{\left(
m\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m\text{)}}}\otimes\underbrace{\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m\right)
}\right)  }_{\substack{=j_{P}^{\left(  m\right)  }\circ\left(  \varrho\left(
A\right)  \right)  \\\text{(by (\ref{pf.plu.inf.-10}), applied to }%
\ell=m\text{)}}}\right) \\
&  =S\circ\underbrace{\left(  \left(  j_{P}^{\left(  m\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \otimes\left(  j_{P}^{\left(
m\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \right)  \right)
}_{=\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  }=\underbrace{S\circ\left(  j_{P}^{\left(  m\right)
}\otimes j_{P}^{\left(  m\right)  }\right)  }_{\substack{=\left(
j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)  }\right)  \circ
S_{P}^{\left(  P+m+1\right)  }\\\text{(by (\ref{pf.plu.inf.1}))}}}\circ\left(
\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  \right) \\
&  =\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\underbrace{S_{P}^{\left(  P+m+1\right)  }\circ\left(
\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  \right)
}_{\substack{=\left(  \varrho\left(  A\right)  \otimes\varrho\left(  A\right)
\right)  \circ S_{P}^{\left(  P+m+1\right)  }\\\text{(by (\ref{pf.plu.inf.2}%
))}}}=\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  }%
\end{align*}
with%
\begin{align*}
&  \underbrace{\left(  \varrho^{\prime}\left(  M\right)  \right)  }%
_{=\varrho\left(  M\right)  \otimes\varrho\left(  M\right)  }\circ
\underbrace{S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  }_{\substack{=\left(  j_{P}^{\left(  m+1\right)  }\otimes
j_{P}^{\left(  m-1\right)  }\right)  \circ S_{P}^{\left(  P+m+1\right)
}\\\text{(by (\ref{pf.plu.inf.1}))}}}\\
&  =\underbrace{\left(  \varrho\left(  M\right)  \otimes\varrho\left(
M\right)  \right)  \circ\left(  j_{P}^{\left(  m+1\right)  }\otimes
j_{P}^{\left(  m-1\right)  }\right)  }_{=\left(  \left(  \varrho\left(
M\right)  \right)  \circ j_{P}^{\left(  m+1\right)  }\right)  \otimes\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m-1\right)
}\right)  }\circ S_{P}^{\left(  P+m+1\right)  }\\
&  =\left(  \underbrace{\left(  \left(  \varrho\left(  M\right)  \right)
\circ j_{P}^{\left(  m+1\right)  }\right)  }_{\substack{=j_{P}^{\left(
m+1\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m+1\text{)}}}\otimes
\underbrace{\left(  \left(  \varrho\left(  M\right)  \right)  \circ
j_{P}^{\left(  m-1\right)  }\right)  }_{\substack{=j_{P}^{\left(  m-1\right)
}\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m-1\text{)}}}\right)  \circ
S_{P}^{\left(  P+m+1\right)  }\\
&  =\underbrace{\left(  \left(  j_{P}^{\left(  m+1\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \otimes\left(  j_{P}^{\left(
m-1\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \right)  \right)
}_{=\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  }\circ S_{P}^{\left(  P+m+1\right)  }\\
&  =\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  },
\end{align*}
we obtain%
\begin{equation}
S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \circ\left(
j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  =\left(
\varrho^{\prime}\left(  M\right)  \right)  \circ S\circ\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  . \label{pf.plu.inf.14}%
\end{equation}


Now,
\begin{align*}
&  \left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \right)
\underbrace{\left(  v\otimes w\right)  }_{\substack{=\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  \left(  v^{\prime
}\otimes w^{\prime}\right)  \\\text{(by (\ref{pf.plu.inf.-20}))}}}\\
&  =\left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \right)
\left(  \left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \left(  v^{\prime}\otimes w^{\prime}\right)  \right)
=\underbrace{\left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)
\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \right)  }_{\substack{=\left(  \varrho^{\prime}\left(  M\right)
\right)  \circ S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  \\\text{(by (\ref{pf.plu.inf.14}))}}}\left(  v^{\prime
}\otimes w^{\prime}\right) \\
&  =\left(  \left(  \varrho^{\prime}\left(  M\right)  \right)  \circ
S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \right)  \left(  v^{\prime}\otimes w^{\prime}\right)  =\left(
\left(  \varrho^{\prime}\left(  M\right)  \right)  \circ S\right)
\underbrace{\left(  \left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  \left(  v^{\prime}\otimes w^{\prime}\right)  \right)
}_{\substack{=v\otimes w\\\text{(by (\ref{pf.plu.inf.-20}))}}}\\
&  =\left(  \left(  \varrho^{\prime}\left(  M\right)  \right)  \circ S\right)
\left(  v\otimes w\right)  .
\end{align*}


Now forget that we fixed $v$ and $w$. We thus have proven that $\left(
S\circ\varrho^{\prime}\left(  M\right)  \right)  \left(  v\otimes w\right)
=\left(  \varrho^{\prime}\left(  M\right)  \circ S\right)  \left(  v\otimes
w\right)  $ for every $v\in\mathcal{F}^{\left(  m\right)  }$ and
$w\in\mathcal{F}^{\left(  m\right)  }$. In other words, the two maps
$S\circ\varrho^{\prime}\left(  M\right)  $ and $\varrho^{\prime}\left(
M\right)  \circ S$ are equal to each other on every pure tensor in
$\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }$.
Thus, these two maps must be identical (on $\mathcal{F}^{\left(  m\right)
}\otimes\mathcal{F}^{\left(  m\right)  }$). In other words, $S\circ
\varrho^{\prime}\left(  M\right)  =\varrho^{\prime}\left(  M\right)  \circ S$.

Now forget that we fixed $M$. We have proven that $S\circ\varrho^{\prime
}\left(  M\right)  =\varrho^{\prime}\left(  M\right)  \circ S$ for every
$M\in\operatorname*{M}\left(  \infty\right)  $. In other words, $S$ is
$\operatorname*{M}\left(  \infty\right)  $-invariant. This proves Theorem
\ref{thm.plu.inf} \textbf{(c)}.

\textbf{(a)} Theorem \ref{thm.plu.inf} \textbf{(a)} follows from Theorem
\ref{thm.plu.inf} \textbf{(c)} since $\operatorname*{GL}\left(  \infty\right)
\subseteq\operatorname*{M}\left(  \infty\right)  $.

\textbf{(b)} $\Longrightarrow:$ Assume that $\tau\in\Omega$. We want to prove
that $S\left(  \tau\otimes\tau\right)  =0$.

Since $\Omega=\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}$, we have
$\tau\in\Omega=\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}$. In
other words, there exists $A\in\operatorname*{GL}\left(  \infty\right)  $ such
that $\tau=A\psi_{0}$. Consider this $A$.

It is easy to see that%
\begin{equation}
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for
every integer }i>0. \label{pf.plu.inf.b1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.b1}):} Let $i>0$ be an integer.
Then,%
\begin{align*}
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)   &  =\overset{\vee}{v_{i}%
}\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi_{0}=v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  0,-1,-2,...\right\}  ;\\
\left(  -1\right)  ^{j}v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\wedge
v_{-\left(  j-1\right)  }\wedge v_{-\left(  j+1\right)  }\wedge v_{-\left(
j+2\right)  }\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{
0,-1,-2,...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  0,-1,-2,...\right\}  $, we denote by $j$ the
integer $k$ satisfying $-k=i$. Since $i\notin\left\{  0,-1,-2,...\right\}  $
(because $i>0$), this simplifies to $\overset{\vee}{v_{i}}\left(  \psi
_{0}\right)  =0$. This proves (\ref{pf.plu.inf.b1}).} Also,%
\begin{equation}
\widehat{v_{i}}\left(  \psi_{0}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for every
integer }i\leq0. \label{pf.plu.inf.b2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.b2}):} Let $i\leq0$ be an integer.
Since $\psi_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$, we have%
\[
\widehat{v_{i}}\left(  \psi_{0}\right)  =\widehat{v_{i}}\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\right)  =v_{i}\wedge v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...
\]
(by the definition of $\widehat{v_{i}}$). But the semiinfinite wedge
$v_{i}\wedge v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$ contains the vector
$v_{i}$ twice (in fact, it contains the vector $v_{i}$ once in its very
beginning, and once again in its $v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$
part (since $i\leq0$)), and thus must equal $0$ (since any semiinfinite wedge
which contains a vector more than once must equal $0$). We thus have
\[
\widehat{v_{i}}\left(  \psi_{0}\right)  =v_{i}\wedge v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...=0.
\]
This proves (\ref{pf.plu.inf.b2}).}

Since $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$, we have%
\begin{align*}
S\left(  \psi_{0}\otimes\psi_{0}\right)   &  =\sum\limits_{i\in\mathbb{Z}%
}\underbrace{\left(  \widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)
\left(  \psi_{0}\otimes\psi_{0}\right)  }_{=\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  }%
=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\left(  \psi_{0}\right)
\otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}\underbrace{\widehat{v_{i}%
}\left(  \psi_{0}\right)  }_{\substack{=0\\\text{(by (\ref{pf.plu.inf.b2}))}%
}}\otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  +\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0}}\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes\underbrace{\overset{\vee}{v_{i}}\left(  \psi_{0}\right)
}_{\substack{=0\\\text{(by (\ref{pf.plu.inf.b1}))}}}\\
&  =\underbrace{\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}0\otimes
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  }_{=0}+\underbrace{\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0}}\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes0}_{=0}=0.
\end{align*}


Now, since $\tau=A\psi_{0}$, we have $\tau\otimes\tau=A\psi_{0}\otimes
A\psi_{0}=A\left(  \psi_{0}\otimes\psi_{0}\right)  $, so that%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =S\left(  A\left(  \psi_{0}\otimes
\psi_{0}\right)  \right) \\
&  =A\cdot\underbrace{S\left(  \psi_{0}\otimes\psi_{0}\right)  }%
_{=0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }S\text{ is }\operatorname*{M}%
\left(  \infty\right)  \text{-linear (by Theorem \ref{thm.plu.inf}
\textbf{(c)})}\right) \\
&  =A\cdot0=0.
\end{align*}
This proves the $\Longrightarrow$ direction of Theorem \ref{thm.plu.inf}
\textbf{(b)}.

$\Longleftarrow:$ Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be such that
$S\left(  \tau\otimes\tau\right)  =0$. We want to prove that $\tau\in\Omega$.

Since $\tau\in\mathcal{F}^{\left(  0\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq0}}j_{N}^{\left(  0\right)  }\left(  \wedge^{N+0+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $m=0$ and $Q=0$), there exists some $N\in\mathbb{N}$
such that $N\geq0$ and $\tau\in j_{N}^{\left(  0\right)  }\left(
\wedge^{N+0+1}\left(  V_{N}\right)  \right)  $. Consider this $N$.

Lemma \ref{lem.plu.inf.S.comp} (applied to $m=0$) yields
\begin{equation}
\left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(  -1\right)  }\right)
\circ S_{N}^{\left(  N+1\right)  }=S\circ\left(  j_{N}^{\left(  0\right)
}\otimes j_{N}^{\left(  0\right)  }\right)  . \label{pf.plu.inf.b5}%
\end{equation}


Recall that the map $j_{N}^{\left(  m\right)  }$ is injective for every
$m\in\mathbb{Z}$. In particular, the maps $j_{N}^{\left(  1\right)  }$ and
$j_{N}^{\left(  -1\right)  }$ are injective, so that the map $j_{N}^{\left(
1\right)  }\otimes j_{N}^{\left(  -1\right)  }$ is also injective.

But $\tau\in j_{N}^{\left(  0\right)  }\left(  \wedge^{N+0+1}\left(
V_{N}\right)  \right)  =j_{N}^{\left(  0\right)  }\left(  \wedge^{N+1}\left(
V_{N}\right)  \right)  $. In other words, there exists some $\tau^{\prime}%
\in\wedge^{N+1}\left(  V_{N}\right)  $ such that $\tau=j_{N}^{\left(
0\right)  }\left(  \tau^{\prime}\right)  $. Consider this $\tau^{\prime}$.

Since $\tau=j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  $, we have
$\tau\otimes\tau=j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)
\otimes j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  =\left(
j_{N}^{\left(  0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \left(
\tau^{\prime}\otimes\tau^{\prime}\right)  $, so that%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =S\left(  \left(  j_{N}^{\left(
0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \left(  \tau^{\prime
}\otimes\tau^{\prime}\right)  \right)  =\underbrace{\left(  S\circ\left(
j_{N}^{\left(  0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \right)
}_{\substack{=\left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }\right)  \circ S_{N}^{\left(  N+1\right)  }\\\text{(by
(\ref{pf.plu.inf.b5}))}}}\left(  \tau^{\prime}\otimes\tau^{\prime}\right) \\
&  =\left(  \left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }\right)  \circ S_{N}^{\left(  N+1\right)  }\right)  \left(
\tau^{\prime}\otimes\tau^{\prime}\right)  =\left(  j_{N}^{\left(  1\right)
}\otimes j_{N}^{\left(  -1\right)  }\right)  \left(  S_{N}^{\left(
N+1\right)  }\left(  \tau^{\prime}\otimes\tau^{\prime}\right)  \right)  .
\end{align*}
Compared with $S\left(  \tau\otimes\tau\right)  =0$, this yields $\left(
j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(  -1\right)  }\right)  \left(
S_{N}^{\left(  N+1\right)  }\left(  \tau^{\prime}\otimes\tau^{\prime}\right)
\right)  =0$. Since $j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }$ is injective, this yields $S_{N}^{\left(  N+1\right)  }\left(
\tau^{\prime}\otimes\tau^{\prime}\right)  =0$. But Lemma \ref{lem.plu.inf.plu}
\textbf{(b)} (applied to $N+1$ and $\tau^{\prime}$ instead of $k$ and $\tau$)
yields that $\tau^{\prime}$ belongs to $\Omega_{N}^{\left(  N+1\right)  }$ if
and only if $S_{N}^{\left(  N+1\right)  }\left(  \tau^{\prime}\otimes
\tau^{\prime}\right)  =0$. Since we know that $S_{N}^{\left(  N+1\right)
}\left(  \tau^{\prime}\otimes\tau^{\prime}\right)  =0$, we can thus conclude
that $\tau^{\prime}$ belongs to $\Omega_{N}^{\left(  N+1\right)  }$. Since
$\Omega_{N}^{\left(  N+1\right)  }$ is the orbit of $v_{N}\wedge v_{N-1}%
\wedge...\wedge v_{N-\left(  N+1\right)  +1}$ under the action of
$\operatorname*{GL}\left(  V_{N}\right)  $ (this is how $\Omega_{N}^{\left(
N+1\right)  }$ was defined), this yields that $\tau^{\prime}$ belongs to the
orbit of $v_{N}\wedge v_{N-1}\wedge...\wedge v_{N-\left(  N+1\right)  +1}$
under the action of $\operatorname*{GL}\left(  V_{N}\right)  $. In other
words, there exists some $A\in\operatorname*{GL}\left(  V_{N}\right)  $ such
that $\tau^{\prime}=A\cdot\left(  v_{N}\wedge v_{N-1}\wedge...\wedge
v_{N-\left(  N+1\right)  +1}\right)  $. Consider this $A$.

We have $\tau^{\prime}=A\cdot\left(  v_{N}\wedge v_{N-1}\wedge...\wedge
\underbrace{v_{N-\left(  N+1\right)  +1}}_{=v_{0}}\right)  =A\cdot\left(
v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}\right)  $.

There clearly exists an invertible linear map $B\in\operatorname*{GL}\left(
V_{N}\right)  $ which sends $v_{N}$, $v_{N-1}$, $...$, $v_{0}$ to $v_{0}$,
$v_{-1}$, $...$, $v_{-N}$, respectively\footnote{\textit{Proof.} Since
$\left(  v_{N},v_{N-1},...,v_{-N}\right)  $ is a basis of $V_{N}$, there
exists a linear map $B\in\operatorname*{End}\left(  V_{N}\right)  $ which
sends $v_{i}$ to $\left\{
\begin{array}
[c]{c}%
v_{i-N},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\\
v_{-i},\ \ \ \ \ \ \ \ \ \ \text{if }i<0
\end{array}
\right.  $ for every $i\in\left\{  N,N-1,...,-N\right\}  $. This linear map
$B$ is invertible (since it permutes the elements of the basis $\left(
v_{N},v_{N-1},...,v_{-N}\right)  $ of $V_{N}$), and thus lies in
$\operatorname*{GL}\left(  V_{N}\right)  $, and it clearly sends $v_{N}$,
$v_{N-1}$, $...$, $v_{0}$ to $v_{0}$, $v_{-1}$, $...$, $v_{-N}$, respectively.
Qed.}. Pick such a $B$. Then, $B\cdot\left(  v_{N}\wedge v_{N-1}%
\wedge...\wedge v_{0}\right)  =v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}$
(since $B$ sends $v_{N}$, $v_{N-1}$, $...$, $v_{0}$ to $v_{0}$, $v_{-1}$,
$...$, $v_{-N}$, respectively), so that $B^{-1}\cdot\left(  v_{0}\wedge
v_{-1}\wedge...\wedge v_{-N}\right)  =v_{N}\wedge v_{N-1}\wedge...\wedge
v_{0}$ and thus%
\[
A\underbrace{B^{-1}\cdot\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }_{=v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}}=A\cdot\left(
v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}\right)  =\tau^{\prime}.
\]


Let $M=i_{N}\left(  AB^{-1}\right)  $. Then, $M=i_{N}\underbrace{\left(
AB^{-1}\right)  }_{\in\operatorname*{GL}\left(  V_{N}\right)  }\in
i_{N}\left(  \operatorname*{GL}\left(  V_{N}\right)  \right)  \subseteq
\operatorname*{GL}\left(  \infty\right)  $. Also,%
\begin{align*}
j_{N}^{\left(  0\right)  }\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)   &  =v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
0\right)  }\right) \\
&  =v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...=\psi_{0}.
\end{align*}
Now,%
\begin{align*}
&  \underbrace{M}_{=i_{N}\left(  AB^{-1}\right)  }\cdot\underbrace{\psi_{0}%
}_{=j_{N}^{\left(  0\right)  }\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }\\
&  =i_{N}\left(  AB^{-1}\right)  \cdot j_{N}^{\left(  0\right)  }\left(
v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}\right)  =j_{N}^{\left(  0\right)
}\left(  \underbrace{AB^{-1}\cdot\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }_{=\tau^{\prime}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.plu.inf.iNjmN},
applied to }0\text{, }AB^{-1}\text{ and }v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\text{ instead of }m\text{, }A\text{ and }u\right) \\
&  =j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  =\tau.
\end{align*}
Thus, $\tau=\underbrace{M}_{\in\operatorname*{GL}\left(  \infty\right)  }%
\cdot\psi_{0}\in\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}=\Omega
$. This proves the $\Longleftarrow$ direction of Theorem \ref{thm.plu.inf}
\textbf{(b)}.

\subsubsection{The semiinfinite Grassmannian}

Denote $\Omega\diagup\mathbb{C}^{\times}$ by $\operatorname*{Gr}$; this is
called the \textit{semiinfinite Grassmannian}.

Think of the space $V$ as $\mathbb{C}\left[  t,t^{-1}\right]  $ (by
identifying $v_{i}$ with $t^{-i}$). Then, $\left\langle v_{0},v_{-1}%
,v_{-2},...\right\rangle =\mathbb{C}\left[  t\right]  $.

\textbf{Exercise:} Then, $\operatorname*{Gr}$ is the set%
\[
\left\{  E\subseteq V\ \text{subspace\ }\mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  t\right]  \text{ for sufficiently large
}N\text{, and}\\
\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N\text{ for
sufficiently large }N
\end{array}
\right)  \right\}  .
\]
\footnote{Here, ``subspace'' means ``$\mathbb{C}$-vector subspace''.} (Note
that when the relations $E\supseteq t^{N}\mathbb{C}\left[  t\right]  $ and
$\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N$ hold for
\textit{some} $N$, it is easy to see that they also hold for all greater $N$.)

We can also replace $\mathbb{C}\left[  t,t^{-1}\right]  $ with $\mathbb{C}%
\left(  \left(  t\right)  \right)  $ (the formal Laurent series), and then
\[
\operatorname*{Gr}=\left\{  E\subseteq V\text{ subspace}\ \mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ for
sufficiently large }N\text{, and}\\
\dim\left(  E\diagup t^{N}\mathbb{C}\left[  \left[  t\right]  \right]
\right)  =N\text{ for sufficiently large }N
\end{array}
\right)  \right\}  .
\]


For any $E\in\operatorname*{Gr}$, there exists some $N\in\mathbb{N}$ such that
$t^{N}\mathbb{C}\left[  t\right]  \subseteq E\subseteq t^{-N}\mathbb{C}\left[
t\right]  $, so that the quotient $E\diagup t^{N}\mathbb{C}\left[  t\right]
\subseteq t^{-N}\mathbb{C}\left[  t\right]  \diagup t^{N}\mathbb{C}\left[
t\right]  \cong\mathbb{C}^{2N}$.

Thus, $\operatorname*{Gr}=\bigcup\limits_{N\geq1}\operatorname*{Gr}\left(
N,2N\right)  $ (a nested union). (By a variation of this construction,
$\operatorname*{Gr}=\bigcup\limits_{N\geq1}\bigcup\limits_{M\geq
1}\operatorname*{Gr}\left(  N,N+M\right)  $.)

\subsubsection{The preimage of the Grassmannian under the Boson-Fermion
correspondence: the Hirota bilinear relations}

Now, how do we actually use these things to find solutions to the
Kadomtsev-Petviashvili equations and other integrable systems?

By Theorem \ref{thm.plu.inf} \textbf{(b)}, the elements of $\Omega$ are
exactly the nonzero elements $\tau$ of $\mathcal{F}^{\left(  0\right)  }$
satisfying $S\left(  \tau\otimes\tau\right)  =0$. We might wonder what happens
to these elements under the Boson-Fermion correspondence $\sigma$: how can
their preimages under $\sigma$ be described? In other words, can we find a
necessary and sufficient condition for a polynomial $\tau\in\mathcal{B}%
^{\left(  0\right)  }$ to satisfy $\sigma\left(  \tau\right)  \in\Omega$
(without using $\sigma$ in this very condition)?

Recall the power series $X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}%
\xi_{i}u^{i}$ and $X^{\ast}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}%
\xi_{i}^{\ast}u^{-i}$ defined in Definition \ref{def.euler.XGamma}. These
power series ``act'' on the fermionic space $\mathcal{F}$. The word ``act''
has been put in inverted commas here because it is not the power series but
their coefficients which really act on $\mathcal{F}$, whereas the power series
themselves only map elements of $\mathcal{F}$ to elements of $\mathcal{F}%
\left(  \left(  u\right)  \right)  $. This, actually, is an important
observation:%
\begin{equation}
\text{every }\omega\in\mathcal{F}\text{ satisfies }X\left(  u\right)
\omega\in\mathcal{F}\left(  \left(  u\right)  \right)  \text{ and }X^{\ast
}\left(  u\right)  \omega\in\mathcal{F}\left(  \left(  u\right)  \right)  .
\label{KdV.F((u))}%
\end{equation}
\footnote{\textit{Proof of (\ref{KdV.F((u))}):} Let $\omega\in\mathcal{F}$.
Since $X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}$, we have
$X\left(  u\right)  \omega=\sum\limits_{i\in\mathbb{Z}}\xi_{i}\left(
\omega\right)  u^{i}\in\mathcal{F}\left(  \left(  u\right)  \right)  $,
because every sufficiently small $i\in\mathbb{Z}$ satisfies $\xi_{i}\left(
\omega\right)  =0$ (this is easy to see). On the other hand, since $X^{\ast
}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}u^{-i}$, we have
$X^{\ast}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}\left(
\omega\right)  u^{-i}\in\mathcal{F}\left(  \left(  u\right)  \right)  $, since
every sufficiently high $i\in\mathbb{Z}$ satisfies $\xi_{i}^{\ast}\left(
\omega\right)  =0$ (this, again, is easy to see). This proves
(\ref{KdV.F((u))}).}

Let $\tau\in\mathcal{B}^{\left(  0\right)  }$ be arbitrary. We want to find an
equivalent form for the equation $S\left(  \sigma\left(  \tau\right)
\otimes\sigma\left(  \tau\right)  \right)  =0$ which does not refer to
$\sigma$.

Let us give two definitions first:

\begin{definition}
\label{def.OMEGA}Let $A$ and $B$ be two $\mathbb{C}$-vector spaces, and let
$u$ be a symbol. Then, the map%
\begin{align*}
A\left(  \left(  u\right)  \right)  \times B\left(  \left(  u\right)  \right)
&  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)  \right)
,\\
\left(  \sum\limits_{i\in\mathbb{Z}}a_{i}u^{i},\sum\limits_{i\in\mathbb{Z}%
}b_{i}u^{i}\right)   &  \mapsto\sum\limits_{i\in\mathbb{Z}}\left(
\sum\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}\right)  u^{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{i}\text{ lie in }A\text{
and all }b_{i}\text{ lie in }B\right)
\end{align*}
is well-defined (in fact, it is easy to see that for any Laurent series
$\sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\in A\left(  \left(  u\right)  \right)
$ with all $a_{i}$ lying in $A$, any Laurent series $\sum\limits_{i\in
\mathbb{Z}}b_{i}u^{i}\in B\left(  \left(  u\right)  \right)  $ with all
$b_{i}$ lying in $B$, and any integer $i\in\mathbb{Z}$, the sum $\sum
\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}$ has only finitely many addends
and vanishes if $i$ is small enough) and $\mathbb{C}$-bilinear. Hence, it
induces a $\mathbb{C}$-linear map%
\begin{align*}
A\left(  \left(  u\right)  \right)  \otimes B\left(  \left(  u\right)
\right)   &  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)
\right)  ,\\
\left(  \sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\right)  \otimes\left(
\sum\limits_{i\in\mathbb{Z}}b_{i}u^{i}\right)   &  \mapsto\sum\limits_{i\in
\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}\right)
u^{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{i}\text{ lie in }A\text{
and all }b_{i}\text{ lie in }B\right)  .
\end{align*}
This map will be denoted by $\Omega_{A,B,u}$.
\end{definition}

More can be said about the map $\Omega_{A,B,u}$: It factors as a composition
of the canonical projection $A\left(  \left(  u\right)  \right)  \otimes
B\left(  \left(  u\right)  \right)  \rightarrow A\left(  \left(  u\right)
\right)  \otimes_{\mathbb{C}\left(  \left(  u\right)  \right)  }B\left(
\left(  u\right)  \right)  $ with a $\mathbb{C}\left(  \left(  u\right)
\right)  $-linear map $A\left(  \left(  u\right)  \right)  \otimes
_{\mathbb{C}\left(  \left(  u\right)  \right)  }B\left(  \left(  u\right)
\right)  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)
\right)  $. We won't need this in the following. What we will need is the
following observation:

\begin{remark}
\label{rmk.OMEGA.linear}Let $A$ and $B$ be two $\mathbb{C}$-algebras, and let
$u$ be a symbol. Then, the map $\Omega_{A,B,u}$ is $A\otimes B$-linear.
\end{remark}

\begin{definition}
Let $A$ be a $\mathbb{C}$-vector space, and let $u$ be a symbol. Then,
$\operatorname*{CT}\nolimits_{u}:A\left(  \left(  u\right)  \right)
\rightarrow A$ will denote the map which sends every Laurent series
$\sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\in A\left(  \left(  u\right)  \right)
$ (where all $a_{i}$ lie in $A$) to $a_{0}\in A$. The image of a Laurent
series $\alpha$ under $\operatorname*{CT}\nolimits_{u}$ will be called the
\textbf{constant term} of $\alpha$. The map $\operatorname*{CT}\nolimits_{u}$
is clearly $A$-linear.
\end{definition}

This notion of ``constant term'' we have thus defined for Laurent series is,
of course, completely analogous to the one used for polynomials and formal
power series. The label $\operatorname*{CT}\nolimits_{u}$ is an abbreviation
for ``constant term with respect to the variable $u$''.

Now, for every $\omega\in\mathcal{F}^{\left(  0\right)  }$ and $\rho
\in\mathcal{F}^{\left(  0\right)  }$, we have%
\begin{equation}
S\left(  \omega\otimes\rho\right)  =\operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{F},\mathcal{F},u}\left(  X\left(  u\right)  \omega\otimes
X^{\ast}\left(  u\right)  \rho\right)  \right)  . \label{KdV.S=CT}%
\end{equation}
\footnote{\textit{Proof of (\ref{KdV.S=CT}):} Let $\omega\in\mathcal{F}%
^{\left(  0\right)  }$ and $\rho\in\mathcal{F}^{\left(  0\right)  }$. Since
$X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}$ and $X^{\ast
}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}u^{-i}%
=\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}u^{i}$ (here, we substituted $-i$
for $i$ in the sum), we have%
\[
X\left(  u\right)  \omega\otimes X^{\ast}\left(  u\right)  \rho=\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}\right)  \omega\otimes\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}u^{i}\right)  \rho=\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{i}\left(  \omega\right)  u^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}\left(  \rho\right)
u^{i}\right)  ,
\]
so that%
\begin{align*}
&  \Omega_{\mathcal{F},\mathcal{F},u}\left(  X\left(  u\right)  \omega\otimes
X^{\ast}\left(  u\right)  \rho\right) \\
&  =\Omega_{\mathcal{F},\mathcal{F},u}\left(  \left(  \sum\limits_{i\in
\mathbb{Z}}\xi_{i}\left(  \omega\right)  u^{i}\right)  \otimes\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}\left(  \rho\right)  u^{i}\right)
\right)  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
\xi_{j}\left(  \omega\right)  \otimes\xi_{-\left(  i-j\right)  }^{\ast}\left(
\rho\right)  \right)  u^{i}%
\end{align*}
(by the definition of $\Omega_{\mathcal{F},\mathcal{F},u}$). Thus (by the
definition of $\operatorname*{CT}\nolimits_{u}$) we have%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\left(  X\left(  u\right)  \omega\otimes X^{\ast}\left(  u\right)
\rho\right)  \right) \\
&  =\sum\limits_{j\in\mathbb{Z}}\xi_{j}\left(  \omega\right)  \otimes
\xi_{-\left(  0-j\right)  }^{\ast}\left(  \rho\right)  =\sum\limits_{j\in
\mathbb{Z}}\xi_{j}\left(  \omega\right)  \otimes\xi_{j}^{\ast}\left(
\rho\right)  =\sum\limits_{i\in\mathbb{Z}}\underbrace{\xi_{i}}%
_{=\widehat{v_{i}}}\left(  \omega\right)  \otimes\underbrace{\xi_{i}^{\ast}%
}_{=\overset{\vee}{v_{i}}}\left(  \rho\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}j\text{ in the sum}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\left(  \omega\right)
\otimes\overset{\vee}{v_{i}}\left(  \rho\right)  =\underbrace{\left(
\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}%
}\right)  }_{\substack{=S\\\text{(because this is how }S\text{ was defined)}%
}}\left(  \omega\otimes\rho\right)  =S\left(  \omega\otimes\rho\right)  ,
\end{align*}
so that (\ref{KdV.S=CT}) is proven.}

Now, let $\tau\in\mathcal{B}^{\left(  0\right)  }$. Due to (\ref{KdV.F((u))})
(applied to $\omega=\sigma\left(  \tau\right)  $), we have $X\left(  u\right)
\sigma\left(  \tau\right)  \in\mathcal{F}\left(  \left(  u\right)  \right)  $
and $X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  \in\mathcal{F}%
\left(  \left(  u\right)  \right)  $.

Now, let us abuse notation and denote by $\sigma$ the map from $\mathcal{B}%
\left(  \left(  u\right)  \right)  $ to $\mathcal{F}\left(  \left(  u\right)
\right)  $ which is canonically induced by the Boson-Fermion correspondence
$\sigma:\mathcal{B}\rightarrow\mathcal{F}$. Then, of course, this new map
$\sigma:\mathcal{B}\left(  \left(  u\right)  \right)  \rightarrow
\mathcal{F}\left(  \left(  u\right)  \right)  $ is also an isomorphism. Then,
the equalities $\Gamma\left(  u\right)  =\sigma^{-1}\circ X\left(  u\right)
\circ\sigma$ and $\Gamma^{\ast}\left(  u\right)  =\sigma^{-1}\circ X^{\ast
}\left(  u\right)  \circ\sigma$ (from Definition \ref{def.euler.XGamma}) are
not just abbreviations for termwise equalities (as we explained them back in
Definition \ref{def.euler.XGamma}), but also hold literally (if we interpret
$\sigma$ to mean our isomorphism $\sigma:\mathcal{B}\left(  \left(  u\right)
\right)  \rightarrow\mathcal{F}\left(  \left(  u\right)  \right)  $ rather
than the original Boson-Fermion correspondence $\sigma:\mathcal{B}%
\rightarrow\mathcal{F}$). As a consequence, $\sigma\circ\Gamma\left(
u\right)  =X\left(  u\right)  \circ\sigma$ and $\sigma\circ\Gamma^{\ast
}\left(  u\right)  =X^{\ast}\left(  u\right)  \circ\sigma$. Thus,%
\[
\sigma\left(  \Gamma\left(  u\right)  \tau\right)  =\underbrace{\left(
\sigma\circ\Gamma\left(  u\right)  \right)  }_{=X\left(  u\right)  \circ
\sigma}\tau=\left(  X\left(  u\right)  \circ\sigma\right)  \tau=X\left(
u\right)  \sigma\left(  \tau\right)
\]
and%
\[
\sigma\left(  \Gamma^{\ast}\left(  u\right)  \tau\right)  =\underbrace{\left(
\sigma\circ\Gamma^{\ast}\left(  u\right)  \right)  }_{=X^{\ast}\left(
u\right)  \circ\sigma}\tau=\left(  X^{\ast}\left(  u\right)  \circ
\sigma\right)  \tau=X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  ,
\]
so that%
\[
\underbrace{X\left(  u\right)  \sigma\left(  \tau\right)  }_{=\sigma\left(
\Gamma\left(  u\right)  \tau\right)  }\otimes\underbrace{X^{\ast}\left(
u\right)  \sigma\left(  \tau\right)  }_{=\sigma\left(  \Gamma^{\ast}\left(
u\right)  \tau\right)  }=\sigma\left(  \Gamma\left(  u\right)  \tau\right)
\otimes\sigma\left(  \Gamma^{\ast}\left(  u\right)  \tau\right)  =\left(
\sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)  \tau\otimes
\Gamma^{\ast}\left(  u\right)  \tau\right)  .
\]
Now,
\begin{align*}
S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)  \right)
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\underbrace{\left(  X\left(  u\right)  \sigma\left(  \tau\right)  \otimes
X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  \right)  }_{=\left(
\sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)  \tau\otimes
\Gamma^{\ast}\left(  u\right)  \tau\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{KdV.S=CT}), applied to }%
\omega=\sigma\left(  \tau\right)  \text{ and }\rho=\sigma\left(  \tau\right)
\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\left(  \left(  \sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  \right) \\
&  =\underbrace{\left(  \operatorname*{CT}\nolimits_{u}\circ\Omega
_{\mathcal{F},\mathcal{F},u}\circ\left(  \sigma\otimes\sigma\right)  \right)
}_{\substack{=\left(  \sigma\otimes\sigma\right)  \circ\operatorname*{CT}%
\nolimits_{u}\circ\Omega_{\mathcal{B},\mathcal{B},u}\\\text{(since
}\operatorname*{CT}\nolimits_{u}\text{ and }\Omega_{A,B,u}\text{ are
functorial)}}}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right) \\
&  =\left(  \left(  \sigma\otimes\sigma\right)  \circ\operatorname*{CT}%
\nolimits_{u}\circ\Omega_{\mathcal{B},\mathcal{B},u}\right)  \left(
\Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)
\\
&  =\left(  \sigma\otimes\sigma\right)  \left(  \operatorname*{CT}%
\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)
\right)  .
\end{align*}
Therefore, the equation $S\left(  \sigma\left(  \tau\right)  \otimes
\sigma\left(  \tau\right)  \right)  =0$ is equivalent to \newline$\left(
\sigma\otimes\sigma\right)  \left(  \operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau
\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  \right)  =0$. This
latter equation, in turn, is equivalent to $\operatorname*{CT}\nolimits_{u}%
\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  =0$ (since
$\sigma\otimes\sigma$ is an isomorphism\footnote{because $\sigma$ is an
isomorphism}). This, in turn, is equivalent to $\left(  z^{-1}\otimes
z\right)  \cdot\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B}%
,\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  =0$ (because $z^{-1}\otimes z$ is an
invertible element of $\mathcal{B}\otimes\mathcal{B}$). Since%
\begin{align*}
&  \left(  z^{-1}\otimes z\right)  \cdot\operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau
\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \underbrace{\left(  z^{-1}\otimes
z\right)  \cdot\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)
}_{\substack{=\Omega_{\mathcal{B},\mathcal{B},u}\left(  \left(  z^{-1}\otimes
z\right)  \left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  \\\text{(since }\Omega_{\mathcal{B}%
,\mathcal{B},u}\text{ is }\mathcal{B}\otimes\mathcal{B}\text{-linear)}%
}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{CT}\nolimits_{u}%
\text{ is }\mathcal{B}\otimes\mathcal{B}\text{-linear (by Remark
\ref{rmk.OMEGA.linear})}\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\underbrace{\left(  \left(  z^{-1}\otimes z\right)  \left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)
}_{=z^{-1}\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(  u\right)
\tau}\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  ,
\end{align*}
this is equivalent to $\operatorname*{CT}\nolimits_{u}\left(  \Omega
_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes
z\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  =0$. Let us combine what
we have proven: We have proven the equivalence of assertions
\begin{equation}
\left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0\right)  \ \Longleftrightarrow\ \left(  \operatorname*{CT}%
\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}%
\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(  u\right)
\tau\right)  \right)  =0\right)  . \label{pf.hirota.firstrewriting}%
\end{equation}


Now, let us simplify $\operatorname*{CT}\nolimits_{u}\left(  \Omega
_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes
z\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  $.

For this, we recall that $\mathcal{B}^{\left(  0\right)  }=\widetilde{F}%
=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $. Thus, the elements of
$\mathcal{B}^{\left(  0\right)  }$ are polynomials in the countably many
indeterminates $x_{1}$, $x_{2}$, $x_{3}$, $...$. We are going to interpret the
elements of $\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }$ as polynomials in ``twice as many'' indeterminates; by this we
mean the following:

\begin{Convention}
\label{conv.hirota.x'x''}Let $\left(  x_{1}^{\prime},x_{2}^{\prime}%
,x_{3}^{\prime},...\right)  $ and $\left(  x_{1}^{\prime\prime},x_{2}%
^{\prime\prime},x_{3}^{\prime\prime},...\right)  $ be two countable families
of new symbols. We denote the family $\left(  x_{1}^{\prime},x_{2}^{\prime
},x_{3}^{\prime},...\right)  $ by $x^{\prime}$, and we denote the family
$\left(  x_{1}^{\prime\prime},x_{2}^{\prime\prime},x_{3}^{\prime\prime
},...\right)  $ by $x^{\prime\prime}$. Thus, if $P\in\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $, we will denote by $P\left(  x^{\prime
}\right)  $ the polynomial $P\left(  x_{1}^{\prime},x_{2}^{\prime}%
,x_{3}^{\prime},...\right)  $, and we will denote by $P\left(  x^{\prime
\prime}\right)  $ the polynomial $P\left(  x_{1}^{\prime\prime},x_{2}%
^{\prime\prime},x_{3}^{\prime\prime},...\right)  $.

The $\mathbb{C}$-linear map%
\begin{align*}
\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }  &
\rightarrow\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}%
^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]
,\\
P\otimes Q  &  \mapsto P\left(  x^{\prime}\right)  Q\left(  x^{\prime\prime
}\right)
\end{align*}
is a $\mathbb{C}$-algebra isomorphism. By means of this isomorphism, we are
going to identify $\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }$ with $\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime}%
,x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime
},...\right]  $.
\end{Convention}

Another convention:

\begin{Convention}
\label{conv.hirota.P(y)}For any $P\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $ and any family $\left(  y_{1},y_{2}%
,y_{3},...\right)  $ of pairwise commuting elements of a $\mathbb{C}$-algebra
$A$, we define an element $P\left(  y_{1},y_{2},y_{3},...\right)  $ of
$A\left(  \left(  u\right)  \right)  $ as follows: Write $P$ in the form
$P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$ for some $P_{i}\in
\mathcal{B}^{\left(  0\right)  }$, and set $P\left(  y_{1},y_{2}%
,y_{3},...\right)  =\sum\limits_{i\in\mathbb{Z}}P_{i}\left(  y_{1},y_{2}%
,y_{3},...\right)  \cdot u^{i}$. (In words, $P\left(  y_{1},y_{2}%
,y_{3},...\right)  $ is defined by substituting $y_{1},y_{2},y_{3},...$ for
the variables $x_{1},x_{2},x_{3},...$ in $P$ while keeping the variable $u$ unchanged).
\end{Convention}

Now, let us notice that:

\begin{lemma}
\label{lem.hirota.PQ}For any $P\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $ and $Q\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $, we have%
\[
\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)  =P\left(
x^{\prime}\right)  \cdot Q\left(  x^{\prime\prime}\right)
\]
(where $P\left(  x^{\prime}\right)  $ and $Q\left(  x^{\prime\prime}\right)  $
are to be understood according to Convention \ref{conv.hirota.P(y)} and
Convention \ref{conv.hirota.x'x''}, and where $\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }$ is identified with $\mathbb{C}%
\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime
\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  $ according to
Convention \ref{conv.hirota.x'x''}).
\end{lemma}

\textit{Proof of Lemma \ref{lem.hirota.PQ}.} Let $P\in\mathcal{B}^{\left(
0\right)  }\left(  \left(  u\right)  \right)  $ and $Q\in\mathcal{B}^{\left(
0\right)  }\left(  \left(  u\right)  \right)  $. Write $P$ in the form
$P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$ for some $P_{i}\in
\mathcal{B}^{\left(  0\right)  }$. Write $Q$ in the form $Q=\sum
\limits_{i\in\mathbb{Z}}Q_{i}\cdot u^{i}$ for some $Q_{i}\in\mathcal{B}%
^{\left(  0\right)  }$. Since $P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$
and $Q=\sum\limits_{i\in\mathbb{Z}}Q_{i}\cdot u^{i}$, we have
\begin{align*}
\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)   &
=\Omega_{\mathcal{B},\mathcal{B},u}\left(  \left(  \sum\limits_{i\in
\mathbb{Z}}P_{i}\cdot u^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{Z}}Q_{i}\cdot u^{i}\right)  \right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}%
}\underbrace{P_{j}\otimes Q_{i-j}}_{\substack{=P_{j}\left(  x^{\prime}\right)
\cdot Q_{i-j}\left(  x^{\prime\prime}\right)  \\\text{(due to our
identification of}\\\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}%
^{\left(  0\right)  }\text{ with}\\\mathbb{C}\left[  x_{1}^{\prime}%
,x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime}%
,x_{3}^{\prime\prime},...\right]  \text{)}}}\right)  u^{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Omega_{\mathcal{B}%
,\mathcal{B},u}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)
\right)  u^{i}%
\end{align*}
and%
\begin{align*}
P\left(  x^{\prime}\right)  \cdot Q\left(  x^{\prime\prime}\right)   &
=\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}\right)
\left(  x^{\prime}\right)  }_{\substack{=\sum\limits_{i\in\mathbb{Z}}%
P_{i}\left(  x^{\prime}\right)  \cdot u^{i}=\sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot u^{j}\\\text{(here, we renamed }i\text{
as }j\text{)}}}\cdot\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}Q_{i}\cdot
u^{i}\right)  \left(  x^{\prime\prime}\right)  }_{=\sum\limits_{i\in
\mathbb{Z}}Q_{i}\left(  x^{\prime\prime}\right)  \cdot u^{i}}\\
&  =\left(  \sum\limits_{j\in\mathbb{Z}}P_{j}\left(  x^{\prime}\right)  \cdot
u^{j}\right)  \cdot\left(  \sum\limits_{i\in\mathbb{Z}}Q_{i}\left(
x^{\prime\prime}\right)  \cdot u^{i}\right)  =\sum\limits_{j\in\mathbb{Z}}%
\sum\limits_{i\in\mathbb{Z}}P_{j}\left(  x^{\prime}\right)  \cdot u^{j}\cdot
Q_{i}\left(  x^{\prime\prime}\right)  \cdot u^{i}\\
&  =\sum\limits_{j\in\mathbb{Z}}\sum\limits_{i\in\mathbb{Z}}P_{j}\left(
x^{\prime}\right)  \cdot\underbrace{u^{j}\cdot Q_{i-j}\left(  x^{\prime\prime
}\right)  \cdot u^{i-j}}_{=Q_{i-j}\left(  x^{\prime\prime}\right)  \cdot
u^{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i-j\text{ for
}i\text{ in the second sum}\right) \\
&  =\sum\limits_{j\in\mathbb{Z}}\sum\limits_{i\in\mathbb{Z}}P_{j}\left(
x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)  \cdot
u^{i}=\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)
\right)  u^{i}=\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)  .
\end{align*}
This proves Lemma \ref{lem.hirota.PQ}.

Now, Theorem \ref{thm.euler} (applied to $m=0$) yields%
\begin{align}
\Gamma\left(  u\right)   &  =uz\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\label{pf.hirota.2}\\
\Gamma^{\ast}\left(  u\right)   &  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  \label{pf.hirota.3}%
\end{align}
on $\mathcal{B}^{\left(  0\right)  }$. Thus,%
\begin{align*}
z^{-1}\Gamma\left(  u\right)  \tau &  =z^{-1}uz\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  \tau\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.hirota.2})}\right) \\
&  =u\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\\
&  =u\exp\left(  \sum\limits_{j>0}\dfrac{jx_{j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{\left(  \dfrac{\partial}{\partial x_{j}%
}\right)  }{j}u^{-j}\right)  \tau\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{j}\text{ acts as }\dfrac{\partial}{\partial x_{j}}\text{ on
}\widetilde{F}\text{ for every }j>0\text{,}\\
\text{ and since }a_{-j}\text{ acts as }jx_{j}\text{ on }\widetilde{F}\text{
for every }j>0
\end{array}
\right) \\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau,
\end{align*}
so that%
\begin{align}
\left(  z^{-1}\Gamma\left(  u\right)  \tau\right)  \left(  x^{\prime}\right)
&  =\left(  u\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau\right)  \left(  x^{\prime}\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)  .
\label{pf.hirota.5}%
\end{align}
Also,%
\begin{align*}
z\Gamma^{\ast}\left(  u\right)  \tau &  =zz^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.hirota.3})}\right) \\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{jx_{j}}{j}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{\left(  \dfrac{\partial}{\partial x_{j}%
}\right)  }{j}u^{-j}\right)  \tau\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{j}\text{ acts as }\dfrac{\partial}{\partial x_{j}}\text{ on
}\widetilde{F}\text{ for every }j>0\text{, and }\\
\text{since }a_{-j}\text{ acts as }jx_{j}\text{ on }\widetilde{F}\text{ for
every }j>0
\end{array}
\right) \\
&  =\exp\left(  -\sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau,
\end{align*}
so that%
\begin{align}
\left(  z\Gamma^{\ast}\left(  u\right)  \tau\right)  \left(  x^{\prime\prime
}\right)   &  =\left(  \exp\left(  -\sum\limits_{j>0}x_{j}u^{j}\right)
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}%
}u^{-j}\right)  \tau\right)  \left(  x^{\prime\prime}\right) \nonumber\\
&  =\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  . \label{pf.hirota.6}%
\end{align}


Now,%
\begin{align}
&  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)
\tau\otimes z\Gamma^{\ast}\left(  u\right)  \tau\right) \nonumber\\
&  =\left(  z^{-1}\Gamma\left(  u\right)  \tau\right)  \left(  x^{\prime
}\right)  \cdot\left(  z\Gamma^{\ast}\left(  u\right)  \tau\right)  \left(
x^{\prime\prime}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.hirota.PQ}, applied to
}P=z^{-1}\Gamma\left(  u\right)  \tau\text{ and }Q=z\Gamma^{\ast}\left(
u\right)  \tau\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime
}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(
x^{\prime\prime}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.5}) and
(\ref{pf.hirota.6})}\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}%
\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(
x^{\prime}\right)  \right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime\prime}\right)  \right)  . \label{pf.hirota.7}%
\end{align}


We are going to rewrite the right hand side of this equality. First of all,
notice that Theorem \ref{thm.exp(u+v)} (applied to $R=\left(  \mathcal{B}%
^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(
\left(  u\right)  \right)  $, \newline$I=\left(  \text{closure of the ideal of
}R\text{ generated by }x_{j}^{\prime}\text{ and }x_{j}^{\prime\prime}\text{
with }j\text{ ranging over all positive integers}\right)  $, $\alpha
=\sum\limits_{j>0}x_{j}^{\prime}u^{j}$ and $\beta=-\sum\limits_{j>0}%
x_{j}^{\prime\prime}u^{j}$) yields%
\[
\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}+\left(  -\sum\limits_{j>0}%
x_{j}^{\prime\prime}u^{j}\right)  \right)  =\exp\left(  \sum\limits_{j>0}%
x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}x_{j}%
^{\prime\prime}u^{j}\right)  .
\]
Thus,%
\begin{align}
\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)   &  =\exp
\underbrace{\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}+\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)  \right)  }_{=\sum
\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}^{\prime\prime}\right)
}\nonumber\\
&  =\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  . \label{pf.hirota.8}%
\end{align}


Now, let us recall a very easy fact: If $\phi$ is an endomorphism of a vector
space $V$, and $v$ is a vector in $V$ such that $\phi v=0$, then $\left(
\exp\phi\right)  v$ is well-defined (in the sense that the power series
$\sum\limits_{n\geq0}\dfrac{1}{n!}\phi^{n}v$ converges) and satisfies $\left(
\exp\phi\right)  v=v$. Applying this fact to $V=\left(  \mathcal{B}^{\left(
0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left[
u,u^{-1}\right]  $, $\phi=\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}$ and $v=\tau\left(  x^{\prime}\right)
$, we see that $\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime
}\right)  \right)  $ is well-defined and satisfies
\begin{equation}
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)
\right)  =\tau\left(  x^{\prime}\right)  \label{pf.hirota.9}%
\end{equation}
(since $\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)
\right)  =\sum\limits_{j>0}\dfrac{1}{j}\underbrace{\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\left(  \tau\left(  x^{\prime}\right)  \right)  }%
_{=0}u^{-j}=0$). The same argument (with $x_{j}^{\prime}$ and $x_{j}%
^{\prime\prime}$ switching places) shows that $\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime\prime}\right)  \right)  $ is well-defined and satisfies%
\begin{equation}
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime}\right)
\right)  =\tau\left(  x^{\prime\prime}\right)  . \label{pf.hirota.10}%
\end{equation}


Now,%
\begin{align}
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)   &  =\exp\left(  \left(  -\sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  +\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}%
u^{-j}\right) \nonumber\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\label{pf.hirota.11}%
\end{align}
\footnote{Here, the last equality sign follows from Theorem \ref{thm.exp(u+v)}%
, applied to
\begin{align*}
R  &  =\left(
\begin{array}
[c]{c}%
\text{closure of the }\mathbb{C}\left[  u,u^{-1}\right]  \text{-subalgebra of
}\operatorname*{End}\nolimits_{\mathbb{C}\left[  u,u^{-1}\right]  }\left(
\left(  \mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)
}\right)  \left[  u,u^{-1}\right]  \right) \\
\text{generated by }\dfrac{\partial}{\partial x_{j}^{\prime}}\text{ and
}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\text{ with }j\text{ ranging
over all positive integers}%
\end{array}
\right)  ,\\
I  &  =\left(
\begin{array}
[c]{c}%
\text{closure of the ideal of }R\text{ generated by }\dfrac{\partial}{\partial
x_{j}^{\prime}}\text{ and }\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\text{ with}\\
j\text{ ranging over all positive integers}%
\end{array}
\right)  ,\\
\alpha &  =-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j},\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \beta
=\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}u^{-j}.
\end{align*}
} and similarly%
\begin{equation}
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)  =\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ\exp\left(  -\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  . \label{pf.hirota.12}%
\end{equation}


But since $-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  $ is a derivation (from $\left(  \mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left[  u,u^{-1}\right]  $ to
\newline$\left(  \mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }\right)  \left[  u,u^{-1}\right]  $), its exponential $\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial
x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)
\right)  $ is a $\mathbb{C}$-algebra homomorphism (since exponentials of
derivations are $\mathbb{C}$-algebra homomorphisms), so that%
\begin{align*}
&  \exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)  \left(  \tau\left(  x^{\prime}\right)  \tau\left(
x^{\prime\prime}\right)  \right) \\
&  =\underbrace{\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  }_{\substack{=\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \\\text{(by (\ref{pf.hirota.11}%
))}}}\left(  \tau\left(  x^{\prime}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}%
-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)  \right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ\exp\left(  -\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \\\text{(by (\ref{pf.hirota.12}))}}}\left(  \tau\left(
x^{\prime\prime}\right)  \right) \\
&  =\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime}}u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\right)  \left(  \tau\left(  x^{\prime}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\left(  \exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ
\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right) \\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \underbrace{\left(  \exp\left(  \sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)  \right)
}_{\substack{=\tau\left(  x^{\prime}\right)  \\\text{(by (\ref{pf.hirota.9}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}%
\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\underbrace{\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  \right)  }_{\substack{=\tau\left(  x^{\prime\prime}\right)
\\\text{(by (\ref{pf.hirota.10}))}}}\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  .
\end{align*}
Hence, (\ref{pf.hirota.7}) becomes%
\begin{align}
&  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)
\tau\otimes z\Gamma^{\ast}\left(  u\right)  \tau\right) \nonumber\\
&  =u\underbrace{\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}%
-x_{j}^{\prime\prime}\right)  \right)  \\\text{(by (\ref{pf.hirota.8}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime}\right)  \right)  \cdot\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\left(  \tau\left(  x^{\prime\prime}\right)  \right)  }_{=\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial
x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)
\right)  \left(  \tau\left(  x^{\prime}\right)  \tau\left(  x^{\prime\prime
}\right)  \right)  }\nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  \cdot\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}%
-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)  \right)  \left(
\tau\left(  x^{\prime}\right)  \tau\left(  x^{\prime\prime}\right)  \right)  .
\label{pf.hirota.15}%
\end{align}
Thus, (\ref{pf.hirota.firstrewriting}) rewrites as%
\begin{align}
&  \left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(
\tau\right)  \right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \operatorname*{CT}\nolimits_{u}\left(
u\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}^{\prime\prime
}\right)  \right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  \left(  \tau\left(  x^{\prime}\right)
\tau\left(  x^{\prime\prime}\right)  \right)  \right)  =0\right)  .
\label{pf.hirota.secondrewriting}%
\end{align}
This already gives a criterion for a $\tau\in\mathcal{B}^{\left(  0\right)  }$
to satisfy $\sigma\left(  \tau\right)  \in\Omega$, but it is yet a rather
messy one. We are going to simplify it in the following. First, we do a
substitution of variables:

\begin{Convention}
Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of new symbols. We
identify the $\mathbb{C}$-algebra $\mathbb{C}\left[  x_{1},y_{1},x_{2}%
,y_{2},x_{3},y_{3},...\right]  $ with the $\mathbb{C}$-algebra $\mathbb{C}%
\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime
\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  =\mathcal{B}^{\left(
0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }$ by the following
substitution:%
\begin{align*}
x_{j}^{\prime}  &  =x_{j}-y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j>0;\\
x_{j}^{\prime\prime}  &  =x_{j}+y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j>0.
\end{align*}


If we define the sum and the difference of two sequences by componentwise
addition resp. subtraction, then this rewrites as follows:%
\begin{align*}
x^{\prime}  &  =x-y;\\
x^{\prime\prime}  &  =x+y.
\end{align*}

\end{Convention}

It is now easy to see that%
\[
x_{j}^{\prime}-x_{j}^{\prime\prime}=-2y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every
}j>0,
\]
and%
\[
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}=-\dfrac{\partial}{\partial y_{j}}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j>0
\]
(where $\dfrac{\partial}{\partial x_{j}^{\prime}}$ and $\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}$ mean differentiation over the variables
$x_{j}^{\prime}$ and $x_{j}^{\prime\prime}$ in the polynomial ring
$\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime}%
,x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  $,
whereas $\dfrac{\partial}{\partial y_{j}}$ means differentiation over the
variable $y_{j}$ in the polynomial ring $\mathbb{C}\left[  x_{1},y_{1}%
,x_{2},y_{2},x_{3},y_{3},...\right]  $). As a consequence,%
\begin{align*}
&  u\exp\left(  \sum\limits_{j>0}u^{j}\underbrace{\left(  x_{j}^{\prime}%
-x_{j}^{\prime\prime}\right)  }_{=-2y_{j}}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\underbrace{\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  }_{=-\dfrac{\partial}{\partial y_{j}}}\right)  \left(  \tau\left(
\underbrace{x^{\prime}}_{=x-y}\right)  \tau\left(  \underbrace{x^{\prime
\prime}}_{=x+y}\right)  \right) \\
&  =u\exp\left(  -2\sum\limits_{j>0}u^{j}y_{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial}{\partial y_{j}}\right)
\left(  \tau\left(  x-y\right)  \tau\left(  x+y\right)  \right)  .
\end{align*}
Hence, (\ref{pf.hirota.secondrewriting}) rewrites as%
\begin{align}
&  \left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(
\tau\right)  \right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \operatorname*{CT}\nolimits_{u}\left(
u\exp\left(  -2\sum\limits_{j>0}u^{j}y_{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial}{\partial y_{j}}\right)
\left(  \tau\left(  x-y\right)  \tau\left(  x+y\right)  \right)  \right)
=0\right)  . \label{pf.hirota.thirdrewriting}%
\end{align}


To simplify this even further, a new notation is needed:

\begin{definition}
\label{def.hirota.A(P,f,g)}Let $K$ be a commutative ring. Let $\left(
x_{1},x_{2},x_{3},...\right)  $, $\left(  z_{1},z_{2},z_{3},...\right)  $, and
$\left(  w_{1},w_{2},w_{3},...\right)  $ be three disjoint families of
indeterminates. Denote by $x$ the family $\left(  x_{1},x_{2},x_{3}%
,...\right)  $, and denote by $z$ the family $\left(  z_{1},z_{2}%
,z_{3},...\right)  $.

\textbf{(a)} For any polynomial $r\in K\left[  x_{1},x_{2},x_{3}%
,...,z_{1},z_{2},z_{3},...\right]  $, let $r\mid_{z=0}$ denote the polynomial
in $K\left[  x_{1},x_{2},x_{3},...\right]  $ obtained by substituting $\left(
0,0,0,...\right)  $ for $\left(  z_{1},z_{2},z_{3},...\right)  $ in $P$.

\textbf{(b)} Consider the differential operators $\dfrac{\partial}{\partial
z_{1}},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}},...$
on $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $. For any
power series $P\in K\left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]  $,
let $P\left(  \partial_{z}\right)  $ mean the value of $P$ when applied to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $ (that is, the result of
substituting $\dfrac{\partial}{\partial z_{j}}$ for each $w_{j}$ in $P$). This
value is a well-defined differential operator on $K\left[  x_{1},x_{2}%
,x_{3},...,z_{1},z_{2},z_{3},...\right]  $ (due to Remark
\ref{rmk.hirota.welldef} below).

\textbf{(c)} For any power series $P\in K\left[  \left[  w_{1},w_{2}%
,w_{3},...\right]  \right]  $ and any two polynomials $f\in K\left[
x_{1},x_{2},x_{3},...\right]  $ and $g\in K\left[  x_{1},x_{2},x_{3}%
,...\right]  $, define a polynomial $A\left(  P,f,g\right)  \in K\left[
x_{1},x_{2},x_{3},...\right]  $ by
\[
A\left(  P,f,g\right)  =\left(  P\left(  \partial_{z}\right)  \left(  f\left(
x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}.
\]

\end{definition}

\begin{remark}
\label{rmk.hirota.welldef}Let $K$ be a commutative ring. Let $\left(
x_{1},x_{2},x_{3},...\right)  $, $\left(  z_{1},z_{2},z_{3},...\right)  $, and
$\left(  w_{1},w_{2},w_{3},...\right)  $ be three disjoint families of
indeterminates. Let $P\in K\left[  \left[  w_{1},w_{2},w_{3},...\right]
\right]  $ be a power series. Then, if we apply the power series $P$ to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $, we obtain a
well-defined endomorphism of $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2}%
,z_{3},...\right]  $.
\end{remark}

\textit{Proof of Remark \ref{rmk.hirota.welldef}.} Let $\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ be defined as in
Convention \ref{conv.fin}. Write the power series $P$ in the form
\[
P=\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }w_{1}^{i_{1}}w_{2}^{i_{2}}w_{3}^{i_{3}}...
\]
for $\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }\in K$. Then, if we apply
the power series $P$ to the family $\left(  \dfrac{\partial}{\partial z_{1}%
},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}%
},...\right)  $, we obtain%
\[
\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\left(  \dfrac{\partial}{\partial z_{1}%
}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}}\right)  ^{i_{2}%
}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}}....
\]
In order to prove that this is a well-defined endomorphism of $K\left[
x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $, we must prove that for
every $r\in K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $,
the sum%
\[
\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\left(  \left(  \dfrac{\partial}{\partial
z_{1}}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}}\right)
^{i_{2}}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}}...\right)
r
\]
is well-defined, i. e., has only finitely many nonzero addends. But this is
clear, because only finitely many $\left(  i_{1},i_{2},i_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfy
$\left(  \left(  \dfrac{\partial}{\partial z_{1}}\right)  ^{i_{1}}\left(
\dfrac{\partial}{\partial z_{2}}\right)  ^{i_{2}}\left(  \dfrac{\partial
}{\partial z_{3}}\right)  ^{i_{3}}...\right)  r\neq0$ \ \ \ \ \footnote{This
is because $r$ is a polynomial, so that only finitely many variables occur in
$r$, and the degrees of the monomials of $r$ are bounded from above.}. Hence,
we have proven that the sum $\sum\limits_{\left(  i_{1},i_{2},i_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}}\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }\left(  \dfrac{\partial
}{\partial z_{1}}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}%
}\right)  ^{i_{2}}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}%
}...$ is a well-defined endomorphism of $K\left[  x_{1},x_{2},x_{3}%
,...,z_{1},z_{2},z_{3},...\right]  $. Since this sum is the result of applying
the power series $P$ to the family $\left(  \dfrac{\partial}{\partial z_{1}%
},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}%
},...\right)  $, we thus conclude that applying the power series $P$ to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $ yields a well-defined
endomorphism of $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]
$. Remark \ref{rmk.hirota.welldef} is proven.

\textbf{Example:} If $P\left(  w\right)  =w_{1}$ (the first variable), then%
\[
A\left(  P,f,g\right)  =\left(  \dfrac{\partial}{\partial z_{1}}\left(
f\left(  x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}%
=-\dfrac{\partial f}{\partial x_{1}}g+\dfrac{\partial g}{\partial x_{1}}f.
\]


\begin{lemma}
For any three polynomials $P,f,g$, we have $A\left(  P,f,g\right)  =A\left(
P_{-},g,f\right)  $, where $P_{-}\left(  w\right)  =P\left(  -w\right)  $.
\end{lemma}

\begin{corollary}
\label{cor.hirota.odd}For any two polynomials $P$ and $f$, we have $A\left(
P,f,f\right)  =0$ if $P$ is odd.
\end{corollary}

This is clear from the definition.

We now state the so-called \textit{Hirota bilinear relations}, which are a
simplified version of (\ref{pf.hirota.thirdrewriting}):

\begin{theorem}
[Hirota bilinear relations]\label{thm.hirota}Let $\tau\in\mathcal{B}^{\left(
0\right)  }$ be a nonzero vector. Let $\left(  y_{1},y_{2},y_{3},...\right)  $
and $\left(  w_{1},w_{2},w_{3},...\right)  $ be two families of new symbols.
Let $\widetilde{w}$ denote the sequence $\left(  \dfrac{w_{1}}{1},\dfrac
{w_{2}}{2},\dfrac{w_{3}}{3},...\right)  $. Define the elementary Schur
polynomials $S_{k}$ as in Definition \ref{def.schur.Sk}.

Then, $\sigma\left(  \tau\right)  \in\Omega$ if and only if%
\begin{equation}
A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right)  =0, \label{thm.hirota.eqn}%
\end{equation}
where the term $A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  $ is to be interpreted by applying Definition
\ref{def.hirota.A(P,f,g)} \textbf{(c)} to $K=\mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  $ (since $\sum\limits_{j=0}^{\infty
}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(
\sum\limits_{s>0}y_{s}w_{s}\right)  \in\left(  \mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  \right)  \left[  \left[  w_{1}%
,w_{2},w_{3},...\right]  \right]  $ and $\tau\in\mathcal{B}^{\left(  0\right)
}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \subseteq\left(
\mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  \right)
\left[  x_{1},x_{2},x_{3},...\right]  $).
\end{theorem}

Before we prove this, we need a simple lemma about polynomials:

\begin{lemma}
\label{lem.hirota.y+z}Let $K$ be a commutative $\mathbb{Q}$-algebra. Let
$\left(  y_{1},y_{2},y_{3},...\right)  $ and $\left(  z_{1},z_{2}%
,z_{3},...\right)  $ be two sequences of new symbols. Denote the sequence
$\left(  y_{1},y_{2},y_{3},...\right)  $ by $y$. Denote the sequence $\left(
z_{1},z_{2},z_{3},...\right)  $ by $z$. Denote by $\widetilde{\partial_{y}}$
the sequence $\left(  \dfrac{1}{1}\dfrac{\partial}{\partial y_{1}},\dfrac
{1}{2}\dfrac{\partial}{\partial y_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial
y_{3}},...\right)  $ of endomorphisms of $\left(  K\left[  \left[  y_{1}%
,y_{2},y_{3},...\right]  \right]  \right)  \left[  z_{1},z_{2},z_{3}%
,...\right]  $. Denote by $\widetilde{\partial_{z}}$ the sequence $\left(
\dfrac{1}{1}\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial
}{\partial z_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $
of endomorphisms of $\left(  K\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  z_{1},z_{2},z_{3},...\right]  $. Let $P$ and $Q$ be
two elements of $K\left[  w_{1},w_{2},w_{3},...\right]  $ (where $\left(
w_{1},w_{2},w_{3},...\right)  $ is a further sequence of new symbols). Then,%
\[
Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(  y+z\right)
\right)  =Q\left(  \widetilde{\partial_{z}}\right)  \left(  P\left(
y+z\right)  \right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.hirota.y+z}.} Let $D$ be the $K$-subalgebra of
$\operatorname*{End}\left(  \left(  K\left[  \left[  y_{1},y_{2}%
,y_{3},...\right]  \right]  \right)  \left[  z_{1},z_{2},z_{3},...\right]
\right)  $ generated by $\dfrac{\partial}{\partial y_{1}},\dfrac{\partial
}{\partial y_{2}},\dfrac{\partial}{\partial y_{3}},...,\dfrac{\partial
}{\partial z_{1}},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial
z_{3}},...$. Then, clearly, $D$ is a commutative $K$-algebra (since its
generators commute), and all elements of the sequences $\widetilde{\partial
_{y}}$ and $\widetilde{\partial_{z}}$ lie in $D$ (since $\widetilde{\partial
_{y}}=\left(  \dfrac{1}{1}\dfrac{\partial}{\partial y_{1}},\dfrac{1}{2}%
\dfrac{\partial}{\partial y_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial y_{3}%
},...\right)  $ and $\widetilde{\partial_{z}}=\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial z_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $).

Let $I$ be the ideal of $D$ generated by $\dfrac{\partial}{\partial y_{i}%
}-\dfrac{\partial}{\partial z_{i}}$ with $i$ ranging over the positive
integers. Then, $\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial
z_{i}}\in I$ for every positive integer $i$. Hence, every positive integer $i$
satisfies $\dfrac{1}{i}\dfrac{\partial}{\partial y_{i}}\equiv\dfrac{1}%
{i}\dfrac{\partial}{\partial z_{i}}\operatorname{mod}I$ (since $\dfrac{1}%
{i}\dfrac{\partial}{\partial y_{i}}-\dfrac{1}{i}\dfrac{\partial}{\partial
z_{i}}=\dfrac{1}{i}\underbrace{\left(  \dfrac{\partial}{\partial y_{i}}%
-\dfrac{\partial}{\partial z_{i}}\right)  }_{\in I}\in I$). In other words,
for every positive integer $i$, the $i$-th element of the sequence
$\widetilde{\partial_{y}}$ is congruent to the $i$-th element of the sequence
$\widetilde{\partial_{z}}$ modulo $I$ (since the $i$-th element of the
sequence $\widetilde{\partial_{y}}$ is $\dfrac{1}{i}\dfrac{\partial}{\partial
y_{i}}$, while the $i$-th element of the sequence $\widetilde{\partial_{z}}$
is $\dfrac{1}{i}\dfrac{\partial}{\partial z_{i}}$). Thus, each element of the
sequence $\widetilde{\partial_{y}}$ is congruent to the corresponding element
of the sequence $\widetilde{\partial_{z}}$ modulo $I$. Hence, $Q\left(
\widetilde{\partial_{y}}\right)  \equiv Q\left(  \widetilde{\partial_{z}%
}\right)  \operatorname{mod}I$ (since $Q$ is a polynomial, and $I$ is an
ideal). Hence,
\begin{align*}
&  Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(  \widetilde{\partial
_{z}}\right)  \in I\\
&  =\left(  \text{ideal of }D\text{ generated by }\dfrac{\partial}{\partial
y_{i}}-\dfrac{\partial}{\partial z_{i}}\text{ with }i\text{ ranging over the
positive integers}\right)  .
\end{align*}
In other words, $Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  $ is a $D$-linear combinations of terms of
the form $\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}$
with $i$ ranging over the positive integers. Thus, we can write $Q\left(
\widetilde{\partial_{y}}\right)  -Q\left(  \widetilde{\partial_{z}}\right)  $
in the form $Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  =\sum\limits_{i>0}d_{i}\circ\left(
\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  $,
where each $d_{i}$ is an element of $D$, and all but finitely many $i>0$
satisfy $d_{i}=0$. Consider these $d_{i}$.

But it is easy to see that%
\begin{equation}
\text{every positive integer }i\text{ satisfies }\left(  \dfrac{\partial
}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  \left(  P\left(
y+z\right)  \right)  =0. \label{pf.hirota.y+z.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.hirota.y+z.1}):} Let $i$ be a positive
integer. Let us identify $\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $
with $\left(  \mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2}%
,...\right]  \right)  \left[  w_{i}\right]  $. Then, $P\in\mathbb{C}\left[
w_{1},w_{2},w_{3},...\right]  =\left(  \mathbb{C}\left[  w_{1},w_{2}%
,...,w_{i-1},w_{i+1},w_{i+2},...\right]  \right)  \left[  w_{i}\right]  $, so
that we can write $P$ as a polynomial in the variable $w_{i}$ over the ring
$\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $. In
other words, we can write $P$ in the form $P=\sum\limits_{n\in\mathbb{N}}%
p_{n}w_{i}^{n}$, where every $n\in\mathbb{N}$ satisfies $p_{n}\in
\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $ and
all but finitely many $n\in\mathbb{N}$ satisfy $p_{n}=0$. Consider these
$p_{n}$.
\par
Let $n\in\mathbb{N}$ be arbitrary. Consider $p_{n}\in\mathbb{C}\left[
w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $ as an element of
$\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $ (by means of the canonical
embedding $\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2}%
,...\right]  \subseteq\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $).
Then, $p_{n}$ is a polynomial in which the variable $w_{i}$ does not occur.
Hence, $p_{n}\left(  y+z\right)  $ is a polynomial in which neither of the
variables $y_{i}$ and $z_{i}$ occur. Thus, $\dfrac{\partial}{\partial y_{i}%
}\left(  p_{n}\left(  y+z\right)  \right)  =0$ and $\dfrac{\partial}{\partial
z_{i}}\left(  p_{n}\left(  y+z\right)  \right)  =0$.
\par
On the other hand, it is very easy to check that $\dfrac{\partial}{\partial
y_{i}}\left(  y_{i}+z_{i}\right)  ^{n}=\dfrac{\partial}{\partial z_{i}}\left(
y_{i}+z_{i}\right)  ^{n}$ (in fact, this is obvious in the case when $n=0$,
and in every other case follows from $\dfrac{\partial}{\partial y_{i}}\left(
y_{i}+z_{i}\right)  ^{n}=n\left(  y_{i}+z_{i}\right)  ^{n-1}$ and
$\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}=n\left(
y_{i}+z_{i}\right)  ^{n-1}$). Now, by the Leibniz rule,%
\begin{align*}
\dfrac{\partial}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)   &  =\underbrace{\left(  \dfrac{\partial
}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \right)  \right)
}_{=0=\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)
\right)  }\cdot\left(  y_{i}+z_{i}\right)  ^{n}+p_{n}\left(  y+z\right)
\cdot\underbrace{\dfrac{\partial}{\partial y_{i}}\left(  y_{i}+z_{i}\right)
^{n}}_{=\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}}\\
&  =\left(  \dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)
\right)  \right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}+p_{n}\left(
y+z\right)  \cdot\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)
^{n}.
\end{align*}
Compared with%
\[
\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)  =\left(  \dfrac{\partial}{\partial z_{i}%
}\left(  p_{n}\left(  y+z\right)  \right)  \right)  \cdot\left(  y_{i}%
+z_{i}\right)  ^{n}+p_{n}\left(  y+z\right)  \cdot\dfrac{\partial}{\partial
z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}%
\]
(this follows from the Leibniz rule), this yields%
\begin{equation}
\dfrac{\partial}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)  =\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)  .
\label{pf.hirota.y+z.2}%
\end{equation}
\par
Now, forget that we fixed $n\in\mathbb{N}$. We have shown that every
$n\in\mathbb{N}$ satisfies (\ref{pf.hirota.y+z.2}). Now, since $P=\sum
\limits_{n\in\mathbb{N}}p_{n}w_{i}^{n}$, we have $P\left(  y+z\right)
=\sum\limits_{n\in\mathbb{N}}p_{n}\left(  y+z\right)  \cdot\left(  y_{i}%
+z_{i}\right)  ^{n}$, so that%
\begin{align*}
&  \left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  \left(  P\left(  y+z\right)  \right) \\
&  =\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  \left(  \sum\limits_{n\in\mathbb{N}}p_{n}\left(  y+z\right)
\cdot\left(  y_{i}+z_{i}\right)  ^{n}\right) \\
&  =\sum\limits_{n\in\mathbb{N}}\underbrace{\dfrac{\partial}{\partial y_{i}%
}\left(  p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)
^{n}\right)  }_{\substack{=\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)
\\\text{(by (\ref{pf.hirota.y+z.2}))}}}-\sum\limits_{n\in\mathbb{N}}%
\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right) \\
&  =\sum\limits_{n\in\mathbb{N}}\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)
-\sum\limits_{n\in\mathbb{N}}\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)  =0.
\end{align*}
This proves (\ref{pf.hirota.y+z.1}).} Thus,%
\begin{align*}
&  Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(  y+z\right)
\right)  -Q\left(  \widetilde{\partial_{z}}\right)  \left(  P\left(
y+z\right)  \right) \\
&  =\underbrace{\left(  Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  \right)  }_{=\sum\limits_{i>0}d_{i}%
\circ\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  }\left(  P\left(  y+z\right)  \right)  =\sum\limits_{i>0}\left(
d_{i}\circ\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial
z_{i}}\right)  \right)  \left(  P\left(  y+z\right)  \right) \\
&  =\sum\limits_{i>0}d_{i}\underbrace{\left(  \left(  \dfrac{\partial
}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  \left(  P\left(
y+z\right)  \right)  \right)  }_{\substack{=0\\\text{(by
(\ref{pf.hirota.y+z.1}))}}}=\sum\limits_{i>0}\underbrace{d_{i}\left(
0\right)  }_{=0}=0.
\end{align*}
In other words, $Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(
y+z\right)  \right)  =Q\left(  \widetilde{\partial_{z}}\right)  \left(
P\left(  y+z\right)  \right)  $. This proves Lemma \ref{lem.hirota.y+z}.

\textit{Proof of Theorem \ref{thm.hirota}.} We introduce a new family of
indeterminates $\left(  z_{1},z_{2},z_{3},...\right)  $. Denote this family by
$z$. (This $z$ has nothing to do with the element $z$ of $\mathcal{B}$. It is
best to forget about $\mathcal{B}$ here, and only think about $\mathcal{B}%
^{\left(  0\right)  }=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $.)
Denote by $\widetilde{\partial_{z}}$ the sequence $\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial z_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $.

Denote by $\widetilde{\partial_{y}}$ the sequence $\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial y_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial y_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial y_{3}},...\right)  $. Also, let $-2y$
be the sequence $\left(  -2y_{1},-2y_{2},-2y_{3},...\right)  $. Then,%
\begin{align}
\sum\limits_{k=0}^{\infty}S_{k}\left(  -2y\right)  u^{k}  &  =\sum
\limits_{k\geq0}S_{k}\left(  -2y\right)  u^{k}=\exp\left(  \sum\limits_{i\geq
1}-2y_{i}u^{i}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.schur.sk.genfun}), with
}-2y\text{ substituted for }x\text{ and }u\text{ substituted for }z\right)
\nonumber\\
&  =\exp\left(  \sum\limits_{j\geq1}-2y_{j}u^{j}\right)  =\exp\left(
-2\sum\limits_{j>0}u^{j}y_{j}\right)  \label{pf.hirota.29}%
\end{align}
and%
\begin{align}
\sum\limits_{k=0}^{\infty}S_{k}\left(  \widetilde{\partial_{y}}\right)
u^{-k}  &  =\sum\limits_{k\geq0}S_{k}\left(  \widetilde{\partial_{y}}\right)
u^{-k}=\exp\left(  \sum\limits_{i\geq1}\dfrac{1}{i}\dfrac{\partial}{\partial
y_{i}}u^{-i}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.schur.sk.genfun}), with
}\widetilde{\partial_{y}}\text{ substituted for }x\text{ and }u^{-1}\text{
substituted for }z\right) \nonumber\\
&  =\exp\left(  \sum\limits_{j\geq1}\dfrac{1}{j}\dfrac{\partial}{\partial
y_{j}}u^{-j}\right)  =\exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  . \label{pf.hirota.30}%
\end{align}


Applying Lemma \ref{lem.hirota.newton} to $K=\left(  \mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  \right)  \left[  x_{1},x_{2}%
,x_{3},...\right]  $ and $P=\tau\left(  x+z\right)  \tau\left(  x-z\right)  $,
we obtain%
\begin{equation}
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
\left(  \tau\left(  x+z\right)  \tau\left(  x-z\right)  \right)  =\tau\left(
x+y+z\right)  \tau\left(  x-y-z\right)  . \label{pf.hirota.31}%
\end{equation}


Now,%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  u\exp\left(  -2\sum\limits_{j>0}%
u^{j}y_{j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  \left(  \tau\left(  x-y\right)
\tau\left(  x+y\right)  \right)  \right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  u\underbrace{\exp\left(
-2\sum\limits_{j>0}u^{j}y_{j}\right)  }_{\substack{=\sum\limits_{k=0}^{\infty
}S_{k}\left(  -2y\right)  u^{k}\\\text{(by (\ref{pf.hirota.29}))}%
}}\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial
}{\partial y_{j}}\right)  }_{\substack{=\sum\limits_{k=0}^{\infty}S_{k}\left(
\widetilde{\partial_{y}}\right)  u^{-k}\\\text{(by (\ref{pf.hirota.30}))}%
}}\left(  \tau\left(  x+y+z\right)  \tau\left(  x-y-z\right)  \right)
\right)  \mid_{z=0}\\
&  =\operatorname*{CT}\nolimits_{u}\left(  u\left(  \sum\limits_{k=0}^{\infty
}S_{k}\left(  -2y\right)  u^{k}\right)  \left(  \sum\limits_{k=0}^{\infty
}S_{k}\left(  \widetilde{\partial_{y}}\right)  u^{-k}\right)  \left(
\tau\left(  x+y+z\right)  \tau\left(  x-y-z\right)  \right)  \right)
\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  \underbrace{S_{j+1}%
\left(  \widetilde{\partial_{y}}\right)  \left(  \tau\left(  x+y+z\right)
\tau\left(  x-y-z\right)  \right)  }_{\substack{=S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \left(  \tau\left(  x+y+z\right)  \tau\left(
x-y-z\right)  \right)  \\\text{(by Lemma \ref{lem.hirota.y+z}, applied
to}\\K=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \text{, }P=\tau\left(
x+w\right)  \tau\left(  x-w\right)  \text{ and }Q=S_{j+1}\left(  w\right)
\text{)}}}\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \underbrace{\left(  \tau\left(  x+y+z\right)
\tau\left(  x-y-z\right)  \right)  }_{\substack{=\exp\left(  \sum
\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(
x+z\right)  \tau\left(  x-z\right)  \right)  \\\text{(by (\ref{pf.hirota.31}%
))}}}\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(  x+z\right)
\tau\left(  x-z\right)  \right)  \mid_{z=0}.
\end{align*}
Compared with the fact that (by the definition of $A\left(  \sum
\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}%
\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  ,\tau,\tau\right)  $)
we have%
\begin{align*}
&  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right) \\
&  =\underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  \right)  \left(  \partial_{z}\right)  }_{=\sum\limits_{j=0}%
^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{\partial_{z}%
}\right)  \exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  }\left(  \tau\left(  x+z\right)  \tau\left(  x-z\right)  \right)
\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(  x+z\right)
\tau\left(  x-z\right)  \right)  \mid_{z=0},
\end{align*}
this yields%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  u\exp\left(  -2\sum\limits_{j>0}%
u^{j}y_{j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  \left(  \tau\left(  x-y\right)
\tau\left(  x+y\right)  \right)  \right) \\
&  =A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right)  .
\end{align*}
Hence, (\ref{pf.hirota.thirdrewriting}) rewrites as follows:%
\[
\left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0\right)  \ \Longleftrightarrow\ \left(  A\left(  \sum
\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}%
\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  ,\tau,\tau\right)
=0\right)  .
\]
Since $S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0$ is equivalent to $\sigma\left(  \tau\right)  \in\Omega$ (by
Theorem \ref{thm.plu.inf} \textbf{(b)}, applied to $\sigma\left(  \tau\right)
$ instead of $\tau$), this rewrites as follows:%
\[
\left(  \sigma\left(  \tau\right)  \in\Omega\right)  \ \Longleftrightarrow
\ \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  =0\right)  .
\]
This proves Theorem \ref{thm.hirota}.

Theorem \ref{thm.hirota} tells us that a nonzero $\tau\in\mathcal{B}^{\left(
0\right)  }$ satisfies $\sigma\left(  \tau\right)  \in\Omega$ if and only if
it satisfies the equation (\ref{thm.hirota.eqn}). The left hand side of this
equation is a power series with respect to the variables $y_{1},y_{2}%
,y_{3},...$. A power series is $0$ if and only if each of its coefficients is
$0$. Hence, the equation (\ref{thm.hirota.eqn}) holds if and only if for each
monomial in $y_{1},y_{2},y_{3},...$, the coefficient of the left hand side of
(\ref{thm.hirota.eqn}) in front of this monomial is $0$. Thus, the equation
(\ref{thm.hirota.eqn}) is equivalent to \textbf{a system of infinitely many
equations}, one for each monomial in $y_{1},y_{2},y_{3},...$. We don't know of
a good way to describe these equations (without using the variables
$y_{1},y_{2},y_{3},...$), but we can describe the equations corresponding to
the simplest among our monomials: the monomials of degree $0$ and those of
degree $1$.

In the following, we consider $\left(  \mathbb{C}\left[  \left[  y_{1}%
,y_{2},y_{3},...\right]  \right]  \right)  \left[  x_{1},x_{2},x_{3}%
,...\right]  $ as a subring of \newline$\left(  \mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  \right)  \left[  \left[  y_{1},y_{2},y_{3}%
,...\right]  \right]  $. For every commutative ring $K$, every element $T$ of
$K\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  $ and any
monomial\footnote{When we say ``monomial'', we mean a monomial without
coefficient.} $\mathfrak{m}$ in the variables $y_{1},y_{2},y_{3},...$, we
denote by $T\left[  \mathfrak{m}\right]  $ the coefficient of the monomial
$\mathfrak{m}$ in the power series $T$. (For example, $\left(  \exp\left(
x_{2}y_{2}\right)  \right)  \left[  y_{2}^{3}\right]  =\dfrac{x_{2}^{3}}{6}$;
note that $K=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ in this
example, so that $x_{2}$ counts as a constant!)

For every $P\in\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]  $ and
every monomial $\mathfrak{m}$ in the variables $y_{1},y_{2},y_{3},...$, we
have
\begin{equation}
\left(  A\left(  P,\tau,\tau\right)  \right)  \left[  \mathfrak{m}\right]
=A\left(  P\left[  \mathfrak{m}\right]  ,\tau,\tau\right)  .
\label{pf.hirota.66}%
\end{equation}
\footnote{\textit{Proof.} We have $P=\sum\limits_{\substack{\mathfrak{n}\text{
is a monomial}\\\text{in }y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]
\cdot\mathfrak{n}$. Since the map%
\begin{align*}
\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]
\right)  \left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]   &
\rightarrow\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  x_{1},x_{2},x_{3},...\right]  ,\\
Q  &  \mapsto A\left(  Q,\tau,\tau\right)
\end{align*}
is $\mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  $-linear,
we have
\[
A\left(  \sum\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in
}y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]  \cdot\mathfrak{n}%
,\tau,\tau\right)  =\sum\limits_{\substack{\mathfrak{n}\text{ is a
monomial}\\\text{in }y_{1},y_{2},y_{3},...}}A\left(  P\left[  \mathfrak{n}%
\right]  ,\tau,\tau\right)  \cdot\mathfrak{n}.
\]
But $P=\sum\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in
}y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]  \cdot\mathfrak{n}$ shows
that
\[
A\left(  P,\tau,\tau\right)  =A\left(  \sum\limits_{\substack{\mathfrak{n}%
\text{ is a monomial}\\\text{in }y_{1},y_{2},y_{3},...}}P\left[
\mathfrak{n}\right]  \cdot\mathfrak{n},\tau,\tau\right)  =\sum
\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in }y_{1}%
,y_{2},y_{3},...}}A\left(  P\left[  \mathfrak{n}\right]  ,\tau,\tau\right)
\cdot\mathfrak{n},
\]
so that the coefficient of $A\left(  P,\tau,\tau\right)  $ before
$\mathfrak{m}$ equals $A\left(  P\left[  \mathfrak{m}\right]  ,\tau
,\tau\right)  $. Since we denoted the coefficient of $A\left(  P,\tau
,\tau\right)  $ before $\mathfrak{m}$ by $\left(  A\left(  P,\tau,\tau\right)
\right)  \left[  \mathfrak{m}\right]  $, this rewrites as $\left(  A\left(
P,\tau,\tau\right)  \right)  \left[  \mathfrak{m}\right]  =A\left(  P\left[
\mathfrak{m}\right]  ,\tau,\tau\right)  $, qed.}

Now, let us describe the equations that are obtained from
(\ref{thm.hirota.eqn}) by taking coefficients before monomials of degree $0$
and $1$:

\textbf{Monomials of degree }$0$\textbf{:} The only monomial of degree $0$ in
$y_{1},y_{2},y_{3},...$ is $1$. We have%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  1\right] \\
&  =A\left(  \underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum
\limits_{s>0}y_{s}w_{s}\right)  \right)  \left[  1\right]  }_{=S_{1}\left(
\widetilde{w}\right)  =w_{1}},\tau,\tau\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.66}), applied to
}P=\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  \text{
and }\mathfrak{m}=1\right) \\
&  =A\left(  w_{1},\tau,\tau\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
Corollary \ref{cor.hirota.odd}, since }w_{1}\text{ is odd}\right)  .
\end{align*}
Therefore, if we take coefficients with respect to the monomial $1$ in the
equation (\ref{pf.hirota.66}), we obtain a tautology.

\textbf{Monomials of degree }$1$\textbf{:} This will be more interesting. The
monomials of degree $1$ in $y_{1},y_{2},y_{3},...$ are $y_{1},y_{2},y_{3}%
,...$. Let $r$ be a positive integer. We have%
\begin{align}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{r}\right] \nonumber\\
&  =A\left(  \underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum
\limits_{s>0}y_{s}w_{s}\right)  \right)  \left[  y_{r}\right]  }%
_{\substack{=-2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r}\\\text{(by
easy computations)}}},\tau,\tau\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.66}), applied to
}P=\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  \text{
and }\mathfrak{m}=y_{r}\right) \nonumber\\
&  =A\left(  -2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r},\tau
,\tau\right)  . \label{pf.hirota.69}%
\end{align}
Denote the polynomial $-2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r}$ by
$T_{r}\left(  w\right)  $. Then, (\ref{pf.hirota.69}) rewrites as%
\begin{equation}
\left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{r}\right]  =A\left(
T_{r}\left(  w\right)  ,\tau,\tau\right)  . \label{pf.hirota.70}%
\end{equation}
We have $T_{1}\left(  w\right)  =w_{2}$, $T_{2}\left(  w\right)
=-\dfrac{w_{1}^{3}}{3}-\dfrac{2w_{3}}{3}$ and $T_{3}\left(  w\right)
=\dfrac{w_{1}w_{3}}{3}-\dfrac{w_{4}}{2}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}%
}{12}-\dfrac{w_{1}^{2}w_{2}}{2}$. Since $T_{1}\left(  w\right)  $ and
$T_{2}\left(  w\right)  $ are odd, we have $A\left(  T_{1}\left(  w\right)
,\tau,\tau\right)  =0$ and $A\left(  T_{2}\left(  w\right)  ,\tau,\tau\right)
=0$ (by Corollary \ref{cor.hirota.odd}). Therefore, taking coefficients with
respect to the monomials $y_{1}$ and $y_{2}$ in the equation
(\ref{pf.hirota.66}) yields tautologies. However, $T_{3}\left(  w\right)  $ is
\textbf{not odd}. Applying (\ref{pf.hirota.70}) to $r=3$, we obtain%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{3}\right] \\
&  =A\left(  T_{3}\left(  w\right)  ,\tau,\tau\right)  =A\left(  \dfrac
{w_{1}w_{3}}{3}-\dfrac{w_{4}}{2}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12}-\dfrac{w_{1}^{2}w_{2}}{2},\tau,\tau\right) \\
&  =A\left(  \dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12},\tau,\tau\right)  +\underbrace{A\left(  -\dfrac{w_{4}}{2}-\dfrac
{w_{1}^{2}w_{2}}{2},\tau,\tau\right)  }_{\substack{=0\\\text{(by Corollary
\ref{cor.hirota.odd}, since}\\-\dfrac{w_{4}}{2}-\dfrac{w_{1}^{2}w_{2}}%
{2}\text{ is odd)}}}\\
&  =A\left(  \dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12},\tau,\tau\right)  =\left(  \left(  \dfrac{\dfrac{\partial}{\partial
z_{1}}\dfrac{\partial}{\partial z_{3}}}{3}-\dfrac{\left(  \dfrac{\partial
}{\partial z_{2}}\right)  ^{2}}{4}-\dfrac{\left(  \dfrac{\partial}{\partial
z_{1}}\right)  ^{4}}{12}\right)  \left(  \tau\left(  x-z\right)  \tau\left(
x+z\right)  \right)  \right)  \mid_{z=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }A\left(
\dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}{12},\tau
,\tau\right)  \right) \\
&  =\dfrac{1}{12}\left(  \left(  4\dfrac{\partial}{\partial z_{1}}%
\dfrac{\partial}{\partial z_{3}}-3\left(  \dfrac{\partial}{\partial z_{2}%
}\right)  ^{2}-\left(  \dfrac{\partial}{\partial z_{1}}\right)  ^{4}\right)
\left(  \tau\left(  x-z\right)  \tau\left(  x+z\right)  \right)  \right)
\mid_{z=0}\\
&  =\dfrac{1}{12}\left(  \left(  4\dfrac{\partial}{\partial w_{1}}%
\dfrac{\partial}{\partial w_{3}}-3\left(  \dfrac{\partial}{\partial w_{2}%
}\right)  ^{2}-\left(  \dfrac{\partial}{\partial w_{1}}\right)  ^{4}\right)
\left(  \tau\left(  x-w\right)  \tau\left(  x+w\right)  \right)  \right)
\mid_{w=0}.
\end{align*}
Since $\dfrac{\partial}{\partial w_{j}}=\partial_{w_{j}}$ for every $j$, we
rewrite this as%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{3}\right] \\
&  =\dfrac{1}{12}\left(  \left(  4\partial_{w_{1}}\partial_{w_{3}}%
-3\partial_{w_{2}}^{2}-\partial_{w_{1}}^{4}\right)  \left(  \tau\left(
x-w\right)  \tau\left(  x+w\right)  \right)  \right)  \mid_{w=0}.
\end{align*}
Hence, taking coefficients with respect to the monomial $y_{3}$ in the
equation (\ref{thm.hirota.eqn}), we obtain%
\[
\dfrac{1}{12}\left(  \left(  4\partial_{w_{1}}\partial_{w_{3}}-3\partial
_{w_{2}}^{2}-\partial_{w_{1}}^{4}\right)  \left(  \tau\left(  x-w\right)
\tau\left(  x+w\right)  \right)  \right)  \mid_{w=0}=0.
\]
In other words,
\begin{equation}
\left(  \partial_{w_{1}}^{4}+3\partial_{w_{2}}^{2}-4\partial_{w_{1}}%
\partial_{w_{3}}\right)  \left(  \tau\left(  x-w\right)  \tau\left(
x+w\right)  \right)  \mid_{w=0}=0. \label{KdV.star}%
\end{equation}
This does not yet look like a PDE in any usual form. We will now transform it
into one.

We make the substitution $x_{1}=x$, $x_{2}=y$, $x_{3}=t$, $x_{m}=c_{m}$ for
$m\geq4$. Here, $x$, $y$, $t$ and $c_{m}$ (for $m\geq4$) are new symbols (in
particularly, $x$ and $y$ no longer denote the sequences $\left(  x_{1}%
,x_{2},x_{3},...\right)  $ and $\left(  y_{1},y_{2},y_{3},...\right)  $). Let
$u=2\partial_{x}^{2}\log\tau$.

\begin{proposition}
\label{prop.KdV.computation}The polynomial $\tau\left(  x,y,t,c_{4}%
,c_{5},...\right)  $ satisfies (\ref{KdV.star}) if and only if the function
$u$ satisfies the KP equation
\[
\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(  \partial_{t}u-\dfrac{3}%
{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)
\]
(where $c_{4}$, $c_{5}$, $c_{6}$, $...$ are considered as constants).
\end{proposition}

\textit{Proof of Proposition \ref{prop.KdV.computation}.} Optional homework exercise.

Thus, we know that any element $\tau$ of $\Omega$ gives rise to a solution of
the KP equation (namely, the solution is $2\partial_{x}^{2}\log\left(
\sigma^{-1}\left(  \tau\right)  \right)  $). Two elements of $\Omega$
differing from each other by a scalar factor yield one and the same solution
of the KP equation. Hence, any element of $\operatorname*{Gr}$ gives rise to a
solution of the KP equation. Since we know how to produce elements of
$\operatorname*{Gr}$, we thus know how to produce solutions of the KP equation!

This does not give \textbf{all} solutions, and in fact we cannot even hope to
find all solutions explicitly (since they depend on boundary conditions, and
these can be arbitrarily nonexplicit), but we will use this to find a dense
subset of them (in an appropriate sense).

The KP equation is not the KdV (Korteweg-de Vries) equation; but if we have a
solution of the KP equation which does not depend on $y$, then this solution
satisfies $\partial_{t}u-\dfrac{3}{2}u\partial_{x}u-\dfrac{1}{4}\partial
_{x}^{3}u=\operatorname*{const}$, and with some work it gives rise to a
solution of the KdV equation (under appropriate decay-at-infinity conditions).

The equations corresponding to the coefficients of the monomials $y_{4}$,
$y_{5}$, $...$ in (\ref{pf.hirota.66}) correspond to the \textit{KP hierarchy}
of higher-order PDEs. There is no point in writing them up explicitly; they
become more and more complicated.

\begin{corollary}
\label{cor.KdV.schursols}Let $\lambda$ be a partition. Then, $2\partial
_{x}^{2}\log\left(  S_{\lambda}\left(  x,y,t,c_{4},c_{5},...\right)  \right)
$ is a solution of the KP equation (and of the whole KP hierarchy), where
$c_{4}$, $c_{5}$, $c_{6}$, $...$ are considered as constants.
\end{corollary}

\textit{Proof of Corollary \ref{cor.KdV.schursols}.} Write $\lambda$ in the
form $\lambda=\left(  \lambda_{0},\lambda_{1},\lambda_{2},...\right)  $. Let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be the sequence defined by
$i_{k}=\lambda_{k}-k$ for every $k\in\mathbb{N}$. Then, $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ is a $0$-degression, and we know that the
elementary semiinfinite wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ is in $\Omega$. But Theorem \ref{thm.schur} yields $\sigma
^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=S_{\lambda}\left(  x\right)  $ (since $\lambda=\left(  i_{0}+0,i_{1}%
+1,i_{2}+2,...\right)  $), so that $\sigma\left(  S_{\lambda}\left(  x\right)
\right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\in\Omega$. Hence,
the function $2\partial_{x}^{2}\log\left(  S_{\lambda}\left(  x,y,t,c_{4}%
,c_{5},...\right)  \right)  $ satisfies the KP equation (and the whole KP
hierarchy). This proves Corollary \ref{cor.KdV.schursols}.

\subsubsection{\textbf{[unfinished]} \texorpdfstring{$n$}{n}-soliton solutions
of KdV}

Now we will construct other solutions of the KdV equations (which are called
multisoliton solutions).

We will identify the $\mathcal{A}$-modules $\mathcal{B}^{\left(  0\right)  }$
and $\mathcal{F}^{\left(  0\right)  }$ along the Boson-Fermion correspondence
$\sigma$.

\begin{definition}
Define a quantum field $\Gamma\left(  u,v\right)  \in\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  0\right)  }\right)  \right)
\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]  $ by%
\begin{equation}
\Gamma\left(  u,v\right)  =\exp\left(  \sum\limits_{j\geq1}\dfrac{u^{j}-v^{j}%
}{j}a_{-j}\right)  \exp\left(  -\sum\limits_{j\geq1}\dfrac{u^{-j}-v^{-j}}%
{j}a_{j}\right)  . \label{n-soliton.Gamma(u,v).def1}%
\end{equation}

\end{definition}

It is possible to rewrite the equality (\ref{n-soliton.Gamma(u,v).def1}) in
the following form:%

\begin{equation}
\Gamma\left(  u,v\right)  =u\left.  :\Gamma\left(  u\right)  \Gamma^{\ast
}\left(  v\right)  :\right.  . \label{n-soliton.Gamma(u,v).def2}%
\end{equation}
However, before we can make sense of this equality
(\ref{n-soliton.Gamma(u,v).def2}), we need to explain what we mean by $\left.
:\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  :\right.  $. Theorem
\ref{thm.euler} (applied to $m=-1$ and to $m=0$) yields that%
\begin{equation}
\Gamma\left(  u\right)  =z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  -1\right)  }
\label{pf.n-soliton.0}%
\end{equation}
and%
\begin{equation}
\Gamma^{\ast}\left(  u\right)  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
0\right)  }. \label{pf.n-soliton.1}%
\end{equation}
Renaming $u$ as $v$ in (\ref{pf.n-soliton.1}), we obtain%
\begin{equation}
\Gamma^{\ast}\left(  v\right)  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}v^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}v^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
0\right)  }. \label{pf.n-soliton.2}%
\end{equation}
If we now extend the ``normal ordered product'' which we have defined on
$U\left(  \mathcal{A}\right)  $ to a ``normal ordered multiplication map''
$U\left(  \mathcal{A}\right)  \left[  z\right]  \left[  \left[  u,u^{-1}%
\right]  \right]  \times U\left(  \mathcal{A}\right)  \left[  z\right]
\left[  \left[  v,v^{-1}\right]  \right]  \rightarrow U\left(  \mathcal{A}%
\right)  \left[  z\right]  \left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]
$

[...] [This isn't really that easy to formalize, and this formalization is wrong.]

[According to Etingof, one can put these power series on a firm footing by
defining a series $\gamma\in\left(  \operatorname*{Hom}\left(  A,B\right)
\right)  \left[  \left[  u,u^{-1}\right]  \right]  $ (where $A$ and $B$ are
two \textbf{graded} vector spaces) to be ``sampled-rational'' if every
homogeneous $w\in A$ and every homogeneous $f\in B^{\ast}$ satisfy
$\left\langle f,\gamma w\right\rangle \in\mathbb{C}\left(  u\right)  $.
Sampled-rational power series form a torsion-free $\mathbb{C}\left(  u\right)
$-module\footnote{But I don't think the composition of any two
sampled-rational power series is sampled-rational. Ideas?}. And limits are
defined sample-wise (see below). But it probably needs some explanations how
$\mathbb{C}\left(  u\right)  $ is embedded in $\mathbb{C}\left[  \left[
u,u^{-1}\right]  \right]  $ (or what it means for an element of $\mathbb{C}%
\left[  \left[  u,u^{-1}\right]  \right]  $ to be a rational function).]

We will use the following notation, generalizing Definition \ref{def.OMEGA}:

\begin{definition}
Let $A$ and $B$ be two $\mathbb{C}$-vector spaces, and let $\left(
u_{1},u_{2},...,u_{\ell}\right)  $ be a sequence of distinct symbols. For
every $\ell$-tuple $\mathbf{i}\in\mathbb{Z}^{\ell}$, define a monomial
$\mathbf{u}^{\mathbf{i}}\in\mathbb{C}\left(  \left(  u_{1},u_{2},...,u_{\ell
}\right)  \right)  $ by $\mathbf{u}^{\mathbf{i}}=u_{1}^{i_{1}}u_{2}^{i_{2}%
}...u_{\ell}^{i_{\ell}}$, where the $\ell$-tuple $\mathbf{i}$ is written in
the form $\mathbf{i}=\left(  i_{1},i_{2},...,i_{\ell}\right)  $. Then, the map%
\begin{align*}
A\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  \times B\left(
\left(  u_{1},u_{2},...,u_{\ell}\right)  \right)   &  \rightarrow\left(
A\otimes B\right)  \left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)
,\\
\left(  \sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}a_{\mathbf{i}}%
\mathbf{u}^{\mathbf{i}},\sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}%
}b_{\mathbf{i}}\mathbf{u}^{\mathbf{i}}\right)   &  \mapsto\sum
\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}\left(  \sum\limits_{\mathbf{j}%
\in\mathbb{Z}^{\ell}}a_{\mathbf{j}}\otimes b_{\mathbf{i}-\mathbf{j}}\right)
\mathbf{u}^{\mathbf{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{\mathbf{i}}\text{ lie in
}A\text{ and all }b_{\mathbf{i}}\text{ lie in }B\right)
\end{align*}
is well-defined (in fact, it is easy to see that for any Laurent series
$\sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}a_{\mathbf{i}}\mathbf{u}%
^{\mathbf{i}}\in A\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  $
with all $a_{\mathbf{i}}$ lying in $A$, any Laurent series $\sum
\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}b_{\mathbf{i}}\mathbf{u}^{\mathbf{i}%
}\in B\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  $ with all
$b_{\mathbf{i}}$ lying in $B$, and any $\ell$-tuple $\mathbf{i}\in
\mathbb{Z}^{\ell}$, the sum $\sum\limits_{\mathbf{j}\in\mathbb{Z}^{\ell}%
}a_{\mathbf{j}}\otimes b_{\mathbf{i}-\mathbf{j}}$ has only finitely many
addends and vanishes if any coordinate of $\mathbf{i}$ is small enough) and
$\mathbb{C}$-bilinear. Hence, it induces a $\mathbb{C}$-linear map%
\begin{align*}
A\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  \otimes B\left(
\left(  u_{1},u_{2},...,u_{\ell}\right)  \right)   &  \rightarrow\left(
A\otimes B\right)  \left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)
,\\
\left(  \sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}a_{\mathbf{i}}%
\mathbf{u}^{\mathbf{i}}\right)  \otimes\left(  \sum\limits_{\mathbf{i}%
\in\mathbb{Z}^{\ell}}b_{\mathbf{i}}\mathbf{u}^{\mathbf{i}}\right)   &
\mapsto\sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}\left(  \sum
\limits_{\mathbf{j}\in\mathbb{Z}^{\ell}}a_{\mathbf{j}}\otimes b_{\mathbf{i}%
-\mathbf{j}}\right)  \mathbf{u}^{\mathbf{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{\mathbf{i}}\text{ lie in
}A\text{ and all }b_{\mathbf{i}}\text{ lie in }B\right)  .
\end{align*}
This map will be denoted by $\Omega_{A,B,\left(  u_{1},u_{2},...,u_{\ell
}\right)  }$. Clearly, when $\ell=1$, this map $\Omega_{A,B,\left(
u_{1}\right)  }$ is identical with the map $\Omega_{A,B,u_{1}}$ defined in
Definition \ref{def.OMEGA}.
\end{definition}

\begin{proposition}
\label{prop.KdV.grassm}If $\tau\in\Omega$ and $a\in\mathbb{C}$, then%
\[
\left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\in\Omega_{u,v},
\]
where%
\[
\Omega_{u,v}=\left\{  \tau\in\mathcal{B}^{\left(  0\right)  }\left(  \left(
u,v\right)  \right)  \ \mid\ S\left(  \tau\otimes\tau\right)  =0\right\}  .
\]
(Here, the $S$ really means not the map $S:\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }\rightarrow\mathcal{B}^{\left(
1\right)  }\otimes\mathcal{B}^{\left(  -1\right)  }$ itself, but rather the
map $\left(  \mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }\right)  \left(  \left(  u,v\right)  \right)  \rightarrow\left(
\mathcal{B}^{\left(  1\right)  }\otimes\mathcal{B}^{\left(  -1\right)
}\right)  \left(  \left(  u,v\right)  \right)  $ it induces. And $\tau
\otimes\tau$ means not $\tau\otimes\tau\in\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }$ but rather $\Omega_{\mathcal{B}%
^{\left(  0\right)  },\mathcal{B}^{\left(  0\right)  },\left(  u,v\right)
}\left(  \tau\otimes\tau\right)  \in\left(  \mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(  \left(  u,v\right)
\right)  $.)
\end{proposition}

\begin{corollary}
For any $a^{(1)},a^{(2)},...,a^{(n)}\in\mathbb{C}$, we have
\begin{align*}
&  \left(  1+a^{(1)}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(
1+a^{(2)}\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a^{(n)}%
\Gamma\left(  u_{n},v_{n}\right)  \right)  \mathbf{1}\\
&  \in\Omega
\end{align*}
(in fact, in an appropriate $\Omega_{u_{1},v_{1},u_{2},v_{2},...}$ rather than
in $\Omega$ itself).
\end{corollary}

\textit{Idea of proof of Proposition.} We will prove $\Gamma\left(
u,v\right)  ^{2}=0$, but we will have to make sense of a term like
$\Gamma\left(  u,v\right)  ^{2}$ in order to define this. Thus, $1+a\Gamma
\left(  u,v\right)  $ will become $\exp\left(  a\Gamma\left(  u,v\right)
\right)  $.

We will formalize this proof later.

But first, here is the punchline of this:

\begin{proposition}
Let $a^{(1)},a^{(2)},...,a^{(n)}\in\mathbb{C}$. If $\tau=\left(
1+a^{(1)}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(  1+a^{(2)}%
\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a^{(n)}\Gamma\left(
u_{n},v_{n}\right)  \right)  \mathbf{1}$, then $2\partial_{x}^{2}\log\tau$ is
given by a convergent series and defines a solution of KP depending on the
parameters $a^{(i)}$, $u_{i}$ and $v_{i}$.
\end{proposition}

This solution is called an $n$\textit{-soliton solution}.

For $n=1$, we have%
\[
\tau=\left(  1+a\Gamma\left(  u,v\right)  \right)  \mathbf{1}=1+a\exp\left(
\left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)  y+\left(  u^{3}%
-v^{3}\right)  t+\left(  u^{4}-v^{4}\right)  c_{4}+...\right)  .
\]
Absorb the $c_{i}$ parameters into a single constant $c$, which can be
absorbed into $a$. So we get%
\[
\tau=1+a\exp\left(  \left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)
y+\left(  u^{3}-v^{3}\right)  t\right)  .
\]
This $\tau$ satisfies
\[
2\partial_{x}^{2}\log\tau=\dfrac{\left(  u-v\right)  ^{2}}{2}\dfrac{1}%
{\cosh^{2}\left(  \dfrac{1}{2}\left(  \left(  u-v\right)  x+\left(
u^{2}-v^{2}\right)  y+\left(  u^{3}-v^{3}\right)  \tau\right)  \right)  }.
\]
Call this function $U$. To make it independent of $y$ (so we get a solution of
KdV equation), we set $v=-u$, and this becomes%
\[
U=\dfrac{2u^{2}}{\cosh^{2}\left(  ux+u^{3}t\right)  }.
\]
This is exactly the soliton solution of KdV.

But let us now give the promised proof of Proposition \ref{prop.KdV.grassm}.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that
$\Gamma\left(  u,v\right)  =u\left.  :\Gamma\left(  u\right)  \Gamma^{\ast
}\left(  v\right)  :\right.  $. We can show:

\begin{lemma}
\label{lem.KdV.GG}We have%
\[
\Gamma\left(  u\right)  \Gamma\left(  v\right)  =\left(  u-v\right)
\cdot\left.  :\Gamma\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\left(
u-v\right)  \cdot\left.  :\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(
v\right)  :\right.  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.KdV.GG}.} We have%
\[
...\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \exp\left(
\sum\limits_{k>0}\dfrac{a_{-k}}{k}v^{k}\right)  ...
\]
and we have to switch these two terms. We get something like%
\[
\exp\left(  -\log\left(  1-\dfrac{v}{u}\right)  \right)  =\dfrac{1}%
{1-\dfrac{v}{u}}=\dfrac{u}{u-v}.
\]
Etc.

We can generalize this: If $\varepsilon=1$ or $\varepsilon=-1$, we can define
$\Gamma_{\varepsilon}$ by $\Gamma_{+1}=\Gamma$ and $\Gamma_{-1}=\Gamma^{\ast}%
$. Then,

\begin{proposition}
We have%
\[
\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}}\left(
u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)  =\prod
\limits_{i<j}\left(  u_{i}-u_{j}\right)  ^{\varepsilon_{i}\varepsilon_{j}%
}\left.  :\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon
_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
:\right.  .
\]
Here, series are being expanded in the region where $\left\vert u_{1}%
\right\vert >\left\vert u_{2}\right\vert >...>\left\vert u_{n}\right\vert $.
\end{proposition}

\begin{corollary}
The matrix elements of $\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)
\Gamma_{\varepsilon_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}%
}\left(  u_{n}\right)  $ (this means expressions of the form $\left(  w^{\ast
},\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}%
}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
w\right)  $ with $w\in\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}%
\in\mathcal{B}^{\left(  0\right)  \ast}$ (where $^{\ast}$ means restricted
dual); a priori, these are only series) are series which converge to rational
functions of the form%
\[
P\left(  u\right)  \cdot\prod\limits_{i<j}\left(  u_{i}-u_{j}\right)
^{\varepsilon_{i}\varepsilon_{j}},\ \ \ \ \ \ \ \ \ \ \text{where }%
P\in\mathbb{C}\left[  u_{1}^{\pm1},u_{2}^{\pm1},...,u_{n}^{\pm1}\right]  .
\]

\end{corollary}

This follows from the Proposition since matrix elements of normal ordered
products are Laurent polynomials.

\begin{corollary}
We have $\Gamma\left(  u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)
=\dfrac{\left(  u^{\prime}-u\right)  \left(  v^{\prime}-v\right)  }{\left(
v^{\prime}-u\right)  \left(  u^{\prime}-v\right)  }\left.  :\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  :\right.  $.
\end{corollary}

Here, we cancelled $u-v$ and $u^{\prime}-v^{\prime}$ which is okay because our
rational functions lie in an integral domain.

As a corollary of this corollary, we have:

\begin{corollary}
If $u\neq v$, then $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  =0$. By which we mean that for any $w\in
\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}\in\mathcal{B}^{\left(
0\right)  \ast}$, we have $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\left(  w^{\ast},\Gamma\left(  u^{\prime
},v^{\prime}\right)  \Gamma\left(  u,v\right)  w\right)  =0$ as a rational function.
\end{corollary}

Informally, this can be written $\left(  \Gamma\left(  u,v\right)  \right)
^{2}=0$. But this does not really make sense in a formal sense since we are
not supposed to take squares of such power series.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that our idea was
to use $1+a\Gamma=\exp\left(  a\Gamma\right)  $ since $\Gamma^{2}=0$. But this
is not rigorous since we cannot speak of $\Gamma^{2}$. So here is the actual proof:

We have (abbreviating $\Gamma\left(  u,v\right)  $ by $\Gamma$ occasionally)%
\begin{align*}
&  S\left(  \left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\otimes\left(
1+a\Gamma\left(  u,v\right)  \right)  \tau\right) \\
&  =\underbrace{S\left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}\tau\in\Omega\text{)}}}+a\underbrace{S\left(  \Gamma\otimes1+1\otimes
\Gamma\right)  \left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}S\text{ commutes with }\mathfrak{gl}_{\infty}\text{,}\\\text{and coefficients
of }\Gamma\text{ are in }\mathfrak{gl}_{\infty}\text{,}\\\text{and }S\left(
\tau\otimes\tau\right)  =0\text{)}}}+a^{2}S\left(  \Gamma\otimes\Gamma\right)
\left(  \tau\otimes\tau\right) \\
&  =a^{2}S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right)
.
\end{align*}
Remains to prove that $S\left(  \Gamma\otimes\Gamma\right)  \left(
\tau\otimes\tau\right)  =0$.

We have
\begin{align*}
&  S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right) \\
&  =\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime}\rightarrow
v}}\dfrac{1}{2}S\left(  \Gamma\left(  u,v\right)  \tau\otimes\Gamma\left(
u^{\prime},v^{\prime}\right)  \tau+\Gamma\left(  u^{\prime},v^{\prime}\right)
\tau\otimes\Gamma\left(  u,v\right)  \tau\right) \\
&  =\dfrac{1}{2}\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime
}\rightarrow v}}\underbrace{S\left(  \Gamma\left(  u^{\prime},v^{\prime
}\right)  \otimes1+1\otimes\Gamma\left(  u^{\prime},v^{\prime}\right)
\right)  \left(  \Gamma\left(  u,v\right)  \otimes1+1\otimes\Gamma\left(
u,v\right)  \right)  \left(  \tau\otimes\tau\right)  }%
_{\substack{=0\\\text{(since }S\text{ commutes with these things)}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2}\lim\limits_{\substack{u^{\prime
}\rightarrow u;\\v^{\prime}\rightarrow v}}S\left(  \underbrace{\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  }_{\rightarrow
0}\otimes1+1\otimes\underbrace{\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  }_{\rightarrow0}\right)  \left(  \tau\otimes
\tau\right) \\
&  =0.
\end{align*}
This proves Proposition \ref{prop.KdV.grassm}.

\subsection{\textbf{[unfinished]} Representations of
\texorpdfstring{$\operatorname*{Vir}$}{Vir} revisited}

We now come back to the representation theory of the Virasoro algebra
$\operatorname*{Vir}$.

Recall that to every pair $\lambda=\left(  c,h\right)  $, we can attach a
Verma module $M_{\lambda}^{+}=M_{c,h}^{+}$ over $\operatorname*{Vir}$. We will
denote this module by $M_{\lambda}=M_{c,h}$, and its $v_{\lambda}^{+}$ by
$v_{\lambda}$.

This module $M_{\lambda}$ has a symmetric bilinear form $\left(  \cdot
,\cdot\right)  :M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$
such that $\left(  v_{\lambda},v_{\lambda}\right)  =1$ and $\left(
L_{n}v,w\right)  =\left(  v,L_{-n}w\right)  $ for all $n\in\mathbb{Z}$, $v\in
M_{\lambda}$ and $w\in M_{\lambda}$. This form is called the
\textit{Shapovalov form}, and is obtained from the invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ by means of the
involution on $\operatorname*{Vir}$.

Also, if $\lambda\in\mathbb{R}^{2}$, the module $M_{\lambda}^{+}$ has a
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ satisfying the same conditions.

We recall that $M_{\lambda}$ has a unique irreducible quotient $L_{\lambda}$.
We have asked questions about when it is unitary, etc.. We will try to answer
some of these questions today.

\begin{Convention}
Let us change the grading of the Virasoro algebra $\operatorname*{Vir}$ to
$\deg L_{i}=-i$. Correspondingly, $M_{\lambda}$ becomes $M_{\lambda}%
=\bigoplus\limits_{n\geq0}M_{\lambda}\left[  n\right]  $.
\end{Convention}

For any $n\geq0$, we have the polynomial $\det\nolimits_{n}\left(  c,h\right)
$ which is the determinant of the contravariant form $\left(  \cdot
,\cdot\right)  $ in degree $n$. This polynomial is defined up to a constant
scalar. Let us recall how it is defined:

Let $\left(  w_{j}\right)  $ be a basis of $U\left(  \operatorname*{Vir}%
\nolimits_{-}\right)  \left[  n\right]  $ (where $\operatorname*{Vir}%
\nolimits_{-}$ is $\left\langle L_{-1},L_{-2},L_{-3},...\right\rangle $; this
is now the \textbf{positive} part of $\operatorname*{Vir}$). Then,
$\det\nolimits_{n}\left(  c,h\right)  =\det\left(  \left(  w_{I}v_{\lambda
},w_{J}v_{\lambda}\right)  _{I,J}\right)  $. If we change the basis by a
matrix $S$, the determinant multiplies by $\left(  \det S\right)  ^{2}$.

For a Hermitian form, we can do the same when $\left(  c,h\right)  $ is real,
but then $\det\nolimits_{n}\left(  c,h\right)  $ is defined up to a
\textbf{positive} scalar, because now the determinant multiplies by
$\left\vert \det S\right\vert ^{2}$. Hence it makes sense to say that
$\det\nolimits_{n}\left(  c,h\right)  >0$.

\begin{proposition}
We have $\det\nolimits_{n}\left(  c,h\right)  =0$ if and only if there exists
a singular vector $w\neq0$ in $M_{c,h}$ of degree $\leq n$ and $>0$.

In particular, if $\det\nolimits_{n}\left(  c,h\right)  =0$, then
$\det\nolimits_{n+1}\left(  c,h\right)  =0$.
\end{proposition}

In fact, we will see that $\det\nolimits_{n+1}$ is divisible by $\det
\nolimits_{n}$.

\textit{Proof of Proposition.} Apparently this is supposed to follow from
something we did.

We recall examples:%
\begin{align*}
\det\nolimits_{1}  &  =2h,\\
\det\nolimits_{2}  &  =2h\left(  16h^{2}+2hc-10h+c\right)  .
\end{align*}
Also recall that $M_{c,h}$ is irreducible if and only if every positive $n$
satisfies $\det\nolimits_{n}\left(  c,h\right)  \neq0$.

\begin{proposition}
Let $\left(  c,h\right)  \in\mathbb{R}^{2}$. If $M_{c,h}$ is unitary, then
$\det\nolimits_{n}\left(  c,h\right)  >0$ for all positive $n$.

More generally, if $L_{c,h}\left[  n\right]  \cong M_{c,h}\left[  n\right]  $
for some positive $n$, and $L_{c,h}$ is unitary, then $\det\nolimits_{n}%
\left(  c,h\right)  >0$.
\end{proposition}

\textit{Proof of Proposition.} A positive-definite Hermitian matrix has
positive determinant.

\begin{theorem}
\label{thm.kac.leader}Fix $c$. Regard $\det\nolimits_{m}\left(  c,h\right)  $
as a polynomial in $h$. Then,%
\[
\det\nolimits_{m}\left(  c,h\right)  =K\cdot h^{\sum\limits_{\substack{r,s\geq
1;\\rs\leq m}}p\left(  m-rs\right)  }+\left(  \text{lower terms}\right)
\]
for some nonzero constant $K$ (which depends on the choice of the basis).
\end{theorem}

\textit{Proof.} We computed before the leading term of $\det\nolimits_{m}$ for
any graded Lie algebra.

$\left(  L_{-k}^{m_{k}}...L_{-1}^{m_{1}}v_{\lambda},L_{-k}^{n_{k}}%
...L_{-1}^{n_{1}}v_{\lambda}\right)  $: the main contribution to the leading
term comes from diagonal.

What degree in $h$ do we get?

If $\mu$ is a partition of $m$, we can write $m=1k_{1}\left(  \mu\right)
+2k_{2}\left(  \mu\right)  +...$, where $k_{i}\left(  \mu\right)  $ is the
number of times $i$ occurs in $\mu$.

$\left(  L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v,L_{-\ell}^{k_{\ell}}%
...L_{-1}^{k_{1}}v\right)  =\left(  v,L_{1}^{k_{1}}...L_{\ell}^{k_{\ell}%
}L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v\right)  $.

So $\mu$ contributes $k_{1}+...+k_{\ell}$ to the exponent of $h$.

So we conclude that the total exponent of $h$ is $\sum\limits_{\mu\vdash
m}\sum\limits_{i}k_{i}\left(  \mu\right)  $.

The rest is easy combinatorics:

Let $m\left(  r,s\right)  $ denote the number of partitions of $m$ in which
$r$ occurs exactly $s$ times. Then, $m\left(  r,s\right)  =p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  $. Thus, with $m$ and
$r$ fixed,%
\begin{align*}
\sum\limits_{s}sm\left(  r,s\right)   &  =\sum\limits_{s}s\left(  p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  \right) \\
&  =\sum\limits_{s}sp\left(  m-rs\right)  -\sum\limits_{s}sp\left(  m-r\left(
s+1\right)  \right) \\
&  =\sum\limits_{s}sp\left(  m-rs\right)  -\sum\limits_{s}\left(  s-1\right)
p\left(  m-rs\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }s-1\text{ for
}s\text{ in the second sum}\right) \\
&  =\sum\limits_{s}\underbrace{\left(  s-\left(  s-1\right)  \right)  }%
_{=1}p\left(  m-rs\right)  =\sum\limits_{s}p\left(  m-rs\right)  .
\end{align*}


So our job is to show that $\sum\limits_{\mu\vdash m}\sum\limits_{i}%
k_{i}\left(  \mu\right)  =\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}sm\left(  r,s\right)  $. But $\sum\limits_{\substack{s\geq1;\\s\leq
m}}sm\left(  r,s\right)  $ is the total number of occurrences of $r$ in all
partitions of $m$. Summed over $r$, it yields the total number of parts of all
partitions of $m$. But this is also $\sum\limits_{\mu\vdash m}\sum
\limits_{i}k_{i}\left(  \mu\right)  $, qed.

We now quote a theorem which was proved independently by Kac and Feigin-Fuchs:

\begin{theorem}
Suppose $rs\leq m$. Then, if%
\[
h=h_{r,s}\left(  c\right)  :=\dfrac{1}{48}\left(  \left(  13-c\right)  \left(
r^{2}+s^{2}\right)  +\sqrt{\left(  c-1\right)  \left(  c-25\right)  }\left(
r^{2}-s^{2}\right)  -24rs-2+2c\right)  ,
\]
then $\det\nolimits_{m}\left(  c,h\right)  =0$. (This is true for any of the
branches of the square root.)
\end{theorem}

\begin{theorem}
\label{thm.kac.thm1}If $h=h_{r,s}\left(  c\right)  $, then $M_{c,h}$ has a
nonzero singular vector in degree $1\leq d\leq rs$.
\end{theorem}

\begin{theorem}
[Kac, also proved by Feigin-Fuchs]\label{thm.kac.thm2}We have%
\[
\det\nolimits_{m}\left(  c,h\right)  =K_{m}\cdot\prod
\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(  c\right)
\right)  ^{p\left(  m-rs\right)  },
\]
where $K_{m}$ is some constant. Note that we should choose the same branch of
the square root in $\sqrt{\left(  c-1\right)  \left(  c-25\right)  }$ for
$h_{r,s}$ and $h_{s,r}$. The square roots ``cancel out'' and give way to a
polynomial in $h$ and $c$.
\end{theorem}

To prove these, we will use the following lemma:

\begin{lemma}
\label{lem.kac.linalg}Let $A\left(  t\right)  $ be a polynomial in one
variable $t$ with values in $\operatorname*{End}V$, where $V$ is a
finite-dimensional vector space. Suppose that $\dim\operatorname*{Ker}\left(
A\left(  0\right)  \right)  \geq n$. Then, $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.kac.linalg}.} Pick a basis $e_{1}%
,e_{2},...,e_{m}$ of $V$ such that the first $n$ vectors $e_{1},e_{2}%
,...,e_{n}$ are in $\operatorname*{Ker}\left(  A\left(  0\right)  \right)  $.
Then, the matrix of $A\left(  t\right)  $ in this basis has first $n$ columns
divisible by $t$, so that its determinant $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.

\textit{Proof of Theorem \ref{thm.kac.thm2}.} Let $A=A\left(  h\right)  $ be
the matrix of the contravariant form in degree $m$, considered as a polynomial
in $h$. If $h=h_{r,s}\left(  c\right)  $, we have a singular vector $w$ in
degree $1\leq d\leq rs$ (by Theorem \ref{thm.kac.thm1}), which generates a
Verma submodule $M_{c,h^{\prime}}\subseteq M_{c,h}$ (by Homework Set 3 problem
1) (the $c$ is the same since $c$ is central and thus acts by the same number
on all vectors).

So $M_{c,h}\left[  m\right]  \supseteq M_{c,h^{\prime}}\left[  m-d\right]  $.
We also have $\dim\left(  M_{c,h^{\prime}}\left[  m-d\right]  \right)
=p\left(  m-d\right)  \geq p\left(  m-rs\right)  $ (since $d\leq rs$) and
$M_{c,h^{\prime}}\left[  m-d\right]  \subseteq\operatorname*{Ker}\left(
\cdot,\cdot\right)  $ (when $h=h_{r,s}$). Hence, $\dim\left(
\operatorname*{Ker}\left(  \cdot,\cdot\right)  \right)  \geq p\left(
m-rs\right)  $. By Lemma \ref{lem.kac.linalg}, this yields that $\det
\nolimits_{m}\left(  c,h\right)  $ is divisible by $\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$.

But it is easy to see that for Weil-generic $c$, the $h-h_{r,s}\left(
c\right)  $ are different, so that $\det\nolimits_{m}\left(  c,h\right)  $ is
divisible by $\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(
h-h_{r,s}\left(  c\right)  \right)  ^{p\left(  m-rs\right)  }$. But by Theorem
\ref{thm.kac.leader}, the leading term of $\det\nolimits_{m}\left(
c,h\right)  $ is $K\cdot h^{\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}p\left(  m-rs\right)  }$, which has exactly the same degree. So
$\det\nolimits_{m}\left(  c,h\right)  $ is a constant multiple of
$\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$. Theorem \ref{thm.kac.thm2} is proven.

We will not prove Theorem \ref{thm.kac.thm1}, since we do not have the tools
for that.

\begin{corollary}
The module $M_{c,h}$ is irreducible if and only if $\left(  c,h\right)  $ does
not lie on the lines
\[
h-h_{r,r}\left(  c\right)  =0\ \Longleftrightarrow\ h+\left(  r^{2}-1\right)
\left(  c-1\right)  /24=0
\]
and quadrics (in fact, hyperbolas if we are over $\mathbb{R}$)%
\begin{align*}
&  \ \left(  h-h_{r,s}\left(  c\right)  \right)  \left(  h-h_{s,r}\left(
c\right)  \right)  =0\\
&  \Longleftrightarrow\ \left(  h-\dfrac{\left(  r-s\right)  ^{2}}{4}\right)
^{2}+\dfrac{h}{24}\left(  c-1\right)  \left(  r^{2}+s^{2}-2\right)  +\dfrac
{1}{576}\left(  r^{2}-1\right)  \left(  s^{2}-1\right)  \left(  c-1\right)
^{2}\\
&  \ \ \ \ \ \ \ \ \ \ +\dfrac{1}{48}\left(  c-1\right)  \left(  r-s\right)
^{2}\left(  rs+1\right)  =0.
\end{align*}

\end{corollary}

\begin{corollary}
\label{cor.kac.irred}\textbf{(1)} Let $h\geq0$ and $c\geq1$. Then, $L_{c,h}$
is unitary.

\textbf{(2)} Let $h>0$ and $c>1$. Then, $M_{c,h}\cong L_{c,h}$, so that
$M_{c,h}$ is irreducible.
\end{corollary}

\textit{Proof of Corollary \ref{cor.kac.irred}.} \textbf{(2)} Lines and
hyperbolas do not pass through the region.

For part \textbf{(1)} we need a lemma:

\begin{lemma}
Let $\mathfrak{g}$ be a graded Lie algebra (with $\dim\mathfrak{g}_{i}%
\neq\infty$) with a real structure $\dag$. Let $U\subseteq\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ be the set of all $\lambda$ such that $L_{\lambda}$ is
unitary. Then, $U$ is closed in the usual metric.

[\textbf{Note:} This lemma possibly needs additional assumptions, like the
assumption that the map $\dag$ reverses the degree (i. e., every
$j\in\mathbb{Z}$ satisfies $\dag\left(  \mathfrak{g}_{j}\right)
\subseteq\mathfrak{g}_{-j}$) and that $\mathfrak{g}_{0}$ is an abelian Lie algebra.]
\end{lemma}

\textit{Proof of Lemma.} It follows from the fact that if $\left(
A_{n}\right)  $ is a sequence of positive definite Hermitian matrices, and
$\lim\limits_{n\rightarrow\infty}A_{n}=A_{\infty}$, then $A_{\infty}$ is
nonnegative definite.

Okay, sorry, we are not going to use this lemma; we will derive the special
case we need.

Now I claim that if $h>0$ and $c>1$, then $L_{c,h}=M_{c,h}$ is unitary. We
know this is true for some points of this region (namely, the ones ``above the
zigzag line''). Then everything follows from the fact that if $A\left(
t\right)  $ is a continuous family of nondegenerate Hermitian matrices
parametrized by $t\in\left[  0,1\right]  $ such that $A\left(  0\right)  >0$,
then $A\left(  t\right)  >0$ for every $t$. (This fact is because the
signature of a nondegenerate Hermitian matrix is a continuous map to a
discrete set, and thus constant on connected components.)

e. g., consider $M_{1,h}$ as a limit of $M_{1+\dfrac{1}{n},h}$ (this is
irreducible for large $n$).

So the matrix of the form in $M_{1,h}\left[  m\right]  $ is a limit of the
matrices for $M_{1+\dfrac{1}{n},h}\left[  m\right]  $. So the matrix for
$M_{1,h}\left[  m\right]  $ is $\geq0$. But kernel lies in $J_{1,h}\left[
m\right]  $, so the form on $L_{1,h}\left[  m\right]  =\left(  M_{1,h}\diagup
J_{1,h}\right)  \left[  m\right]  $ is strictly positive.

By analyzing the Kac curves, we can show (although \textit{we} will
\textit{not} show) that in the region $0\leq c<1$, there are only countably
many points where we possibly can have unitarity:

$c\left(  m\right)  =1-\dfrac{6}{\left(  m+2\right)  \left(  m+3\right)  };$

$h_{r,s}\left(  m\right)  =\dfrac{\left(  \left(  m+3\right)  r-\left(
m+2\right)  s\right)  ^{2}-1}{4\left(  m+2\right)  \left(  m+3\right)  }$ with
$1\leq r\leq s\leq m+1$.

for $m\geq0$.

In fact we will show that at these points we indeed have unitary representations.

\begin{proposition}
\textbf{(1)} If $c\geq0$ and $L_{c,h}$ is unitary, then $h=0$.

\textbf{(2)} We have $L_{0,h}=M_{0,h}$ if and only if $h\neq\dfrac{m^{2}%
-1}{24}$ for all $m\geq0$.

\textbf{(3)} We have $L_{1,h}=M_{1,h}$ if and only if $h\neq\dfrac{m^{2}}{24}$
for all $m\geq0$.
\end{proposition}

\textit{Proof.} \textbf{(2)} and \textbf{(3)} follow immediately from the Kac
determinant formula. For \textbf{(1)}, just compute $\det\left(
\begin{array}
[c]{cc}%
\left(  L_{-N}^{2}v,L_{-N}^{2}v\right)  & \left(  L_{-N}^{2}v,L_{-2N}v\right)
\\
\left(  L_{-2N}v,L_{-N}^{2}v\right)  & \left(  L_{-2N}v,L_{-2N}v\right)
\end{array}
\right)  =4N^{3}h^{2}\left(  8h-5N\right)  $ (this is $<0$ for high enough $N$
as long as $h\neq0$), so that the only possibility for unitarity is $h=0$.

\section{Affine Lie algebras}

\subsection{Introducing
\texorpdfstring{$\protect\widehat{\mathfrak{gl}_{n}}$}{gl-n-hat}}

\begin{definition}
\label{def.glnhat}Let $V$ denote the vector representation of $\mathfrak{gl}%
_{\infty}$ defined in Definition \ref{def.glinf.V}.

Let $n$ be a positive integer. Consider $L\mathfrak{gl}_{n}=\mathfrak{gl}%
_{n}\left[  t,t^{-1}\right]  $; this is the loop algebra of the Lie algebra
$\mathfrak{gl}_{n}$. This loop algebra clearly acts on $\mathbb{C}^{n}\left[
t,t^{-1}\right]  $ (by $\left(  At^{i}\right)  \rightharpoonup\left(
wt^{j}\right)  =Awt^{i+j}$ for all $A\in\mathfrak{gl}_{n}$, $w\in
\mathbb{C}^{n}$, $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$). But we can identify
the vector space $\mathbb{C}^{n}\left[  t,t^{-1}\right]  $ with $V$ as
follows: Let $\left(  e_{1},e_{2},...,e_{n}\right)  $ be the standard basis of
$\mathbb{C}^{n}$. Then we identify $e_{i}t^{k}\in\mathbb{C}^{n}\left[
t,t^{-1}\right]  $ with $v_{i-kn}\in V$ for every $i\in\left\{
1,2,...,n\right\}  $ and $k\in\mathbb{Z}$. The action of $L\mathfrak{gl}_{n}$
on $\mathbb{C}^{n}\left[  t,t^{-1}\right]  $ now becomes an action of
$L\mathfrak{gl}_{n}$ on $V$. Hence, $L\mathfrak{gl}_{n}$ maps into
$\operatorname*{End}V$. More precisely, $L\mathfrak{gl}_{n}$ maps into
$\overline{\mathfrak{a}_{\infty}}\subseteq\operatorname*{End}V$. Here is a
direct way to construct this mapping:

Let $a\left(  t\right)  \in L\mathfrak{gl}_{n}$ be a Laurent polynomial with
coefficients in $\mathfrak{gl}_{n}$. Write $a\left(  t\right)  $ in the form
$a\left(  t\right)  =\sum\limits_{k\in\mathbb{Z}}a_{k}t^{k}$ with all $a_{k}$
lying in $\mathfrak{gl}_{n}$. Then, let $\operatorname*{Toep}\nolimits_{n}%
\left(  a\left(  t\right)  \right)  $ be the matrix%
\[
\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  \in\overline{\mathfrak{a}_{\infty}}.
\]
Formally speaking, this matrix is defined as the matrix whose $\left(
ni+\alpha,nj+\beta\right)  $-th entry equals the $\left(  \alpha,\beta\right)
$-th entry of the $n\times n$ matrix $a_{j-i}$ for all $i\in\mathbb{Z}$,
$j\in\mathbb{Z}$, $\alpha\in\left\{  1,2,...,n\right\}  $ and $\beta
\in\left\{  1,2,...,n\right\}  $. In other words, this is the block matrix
consisting of infinitely many $n\times n$-blocks such that the ``$i$-th block
diagonal'' is filled with $a_{i}$'s for every $i\in\mathbb{Z}$.

We thus have defined a map $\operatorname*{Toep}\nolimits_{n}:L\mathfrak{gl}%
_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$. This map
$\operatorname*{Toep}\nolimits_{n}$ is injective, and is exactly the map
$L\mathfrak{gl}_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$ we obtain from
the above action of $L\mathfrak{gl}_{n}$ on $V$. In particular, this map
$\operatorname*{Toep}\nolimits_{n}$ is a Lie algebra homomorphism.

In the following, we will often regard the injective map $\operatorname*{Toep}%
\nolimits_{n}$ as an inclusion, i. e., we will identify any $a\left(
t\right)  \in L\mathfrak{gl}_{n}$ with its image $\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \right)  \in\overline{\mathfrak{a}%
_{\infty}}$.
\end{definition}

Note that I chose the notation $\operatorname*{Toep}\nolimits_{n}$ because of
the notion of Toeplitz matrices. For any $a\left(  t\right)  \in
L\mathfrak{gl}_{n}$, the matrix $\operatorname*{Toep}\nolimits_{n}\left(
a\left(  t\right)  \right)  $ can be called an infinite ``block-Toeplitz''
matrix. If $n=1$, then $\operatorname*{Toep}\nolimits_{1}\left(  a\left(
t\right)  \right)  $ is an actual infinite Toeplitz matrix.

\begin{example}
Since $\mathfrak{gl}_{1}$ is a $1$-dimensional abelian Lie algebra, we can
identify $L\mathfrak{gl}_{1}$ with the Lie algebra $\overline{\mathcal{A}}$.
The image $\operatorname*{Toep}\nolimits_{1}\left(  L\mathfrak{gl}_{1}\right)
$ is the abelian Lie subalgebra $\left\langle T^{j}\ \mid\ j\in\mathbb{Z}%
\right\rangle $ of $\overline{\mathfrak{a}_{\infty}}$ (where $T$ is the shift
operator) and is isomorphic to $\overline{\mathcal{A}}$.
\end{example}

It is easy to see that:

\begin{proposition}
\label{prop.Toep.alg}Let $n$ be a positive integer. Define an associative
algebra structure on $L\mathfrak{gl}_{n}=\mathfrak{gl}_{n}\left[
t,t^{-1}\right]  $ by%
\[
\left(  at^{i}\right)  \cdot\left(  bt^{j}\right)  =abt^{i+j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{gl}_{n}\text{, }%
b\in\mathfrak{gl}_{n}\text{, }i\in\mathbb{Z}\text{ and }j\in\mathbb{Z}.
\]
Then, $\operatorname*{Toep}\nolimits_{n}$ is not only a Lie algebra
homomorphism, but also a homomorphism of associative algebras.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Toep.alg}.} Let $a\left(  t\right)  \in
L\mathfrak{gl}_{n}$ and $b\left(  t\right)  \in L\mathfrak{gl}_{n}$. Write
$a\left(  t\right)  $ in the form $a\left(  t\right)  =\sum\limits_{k\in
\mathbb{Z}}a_{k}t^{k}$ with all $a_{k}$ lying in $\mathfrak{gl}_{n}$. Write
$b\left(  t\right)  $ in the form $b\left(  t\right)  =\sum\limits_{k\in
\mathbb{Z}}b_{k}t^{k}$ with all $b_{k}$ lying in $\mathfrak{gl}_{n}$. By the
definition of $\operatorname*{Toep}\nolimits_{n}$, we have%
\begin{align*}
\operatorname*{Toep}\nolimits_{n}\left(  a\left(  t\right)  \right)   &
=\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \\
\text{and}\ \ \ \ \ \ \ \ \ \ \operatorname*{Toep}\nolimits_{n}\left(
b\left(  t\right)  \right)   &  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  .
\end{align*}
Hence,%
\begin{align}
&  \operatorname*{Toep}\nolimits_{n}\left(  a\left(  t\right)  \right)
\cdot\operatorname*{Toep}\nolimits_{n}\left(  b\left(  t\right)  \right)
\nonumber\\
&  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \nonumber\\
&  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k-\left(  -1\right)  }b_{-1-k} &
\sum\limits_{k\in\mathbb{Z}}a_{k-\left(  -1\right)  }b_{0-k} & \sum
\limits_{k\in\mathbb{Z}}a_{k-\left(  -1\right)  }b_{1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k-0}b_{-1-k} & \sum\limits_{k\in
\mathbb{Z}}a_{k-0}b_{0-k} & \sum\limits_{k\in\mathbb{Z}}a_{k-0}b_{1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k-1}b_{-1-k} & \sum\limits_{k\in
\mathbb{Z}}a_{k-1}b_{0-k} & \sum\limits_{k\in\mathbb{Z}}a_{k-1}b_{1-k} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the rule for multiplying block
matrices}\right) \nonumber\\
&  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +\left(
-1\right)  -k} & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +0-k}
& \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{0+1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{1+1-k} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \label{pf.Toep.alg.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since any }\left(  i,j\right)
\in\mathbb{Z}^{2}\text{ satisfies }\sum\limits_{k\in\mathbb{Z}}a_{k-i}%
b_{j-k}=\sum\limits_{k\in\mathbb{Z}}a_{k}b_{i+j-k}\right)  .\nonumber
\end{align}
On the other hand, multiplying $a\left(  t\right)  =\sum\limits_{k\in
\mathbb{Z}}a_{k}t^{k}$ and $b\left(  t\right)  =\sum\limits_{k\in\mathbb{Z}%
}b_{k}t^{k}$, we obtain%
\[
a\left(  t\right)  \cdot b\left(  t\right)  =\left(  \sum\limits_{k\in
\mathbb{Z}}a_{k}t^{k}\right)  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}%
b_{k}t^{k}\right)  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{k\in
\mathbb{Z}}a_{k}b_{i-k}\right)  t^{i}%
\]
(by the definition of the product of two Laurent polynomials), so that%
\[
\operatorname*{Toep}\nolimits_{n}\left(  a\left(  t\right)  \cdot b\left(
t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +\left(
-1\right)  -k} & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +0-k}
& \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{0+1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{1+1-k} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)
\]
(by the definition of $\operatorname*{Toep}\nolimits_{n}$). Compared with
(\ref{pf.Toep.alg.1}), this yields $\operatorname*{Toep}\nolimits_{n}\left(
a\left(  t\right)  \right)  \cdot\operatorname*{Toep}\nolimits_{n}\left(
b\left(  t\right)  \right)  =\operatorname*{Toep}\nolimits_{n}\left(  a\left(
t\right)  \cdot b\left(  t\right)  \right)  $.

Now forget that we fixed $a\left(  t\right)  $ and $b\left(  t\right)  $. We
thus have proven that every $a\left(  t\right)  \in L\mathfrak{gl}_{n}$ and
$b\left(  t\right)  \in L\mathfrak{gl}_{n}$ satisfy $\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \right)  \cdot\operatorname*{Toep}%
\nolimits_{n}\left(  b\left(  t\right)  \right)  =\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \cdot b\left(  t\right)  \right)  $.
Combined with the fact that $\operatorname*{Toep}\nolimits_{n}\left(
1\right)  =\operatorname*{id}$ (this is very easy to prove), this yields that
$\operatorname*{Toep}\nolimits_{n}$ is a homomorphism of associative algebras.
Hence, $\operatorname*{Toep}\nolimits_{n}$ is also a homomorphism of Lie
algebras. Proposition \ref{prop.Toep.alg} is proven.

Recall that the Lie algebra $\overline{\mathfrak{a}_{\infty}}$ has a central
extension $\mathfrak{a}_{\infty}$, which equals $\overline{\mathfrak{a}%
_{\infty}}\oplus\mathbb{C}K$ as a vector space but has its Lie bracket defined
using the cocycle $\alpha$.

\begin{proposition}
\label{prop.ainf.alphaomega}Let $\alpha:\overline{\mathfrak{a}_{\infty}}%
\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ be the Japanese cocycle.

Let $n\in\mathbb{N}$. Let $\omega:L\mathfrak{gl}_{n}\times L\mathfrak{gl}%
_{n}\rightarrow\mathbb{C}$ be the $2$-cocycle on $L\mathfrak{gl}_{n}$ which is
defined by%
\begin{equation}
\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  =\sum
\limits_{k\in\mathbb{Z}}k\operatorname*{Tr}\left(  a_{k}b_{-k}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\left(  t\right)  \in L\mathfrak{gl}%
_{n}\text{ and }b\left(  t\right)  \in L\mathfrak{gl}_{n}
\label{prop.ainf.alphaomega.form}%
\end{equation}
(where we write $a\left(  t\right)  $ in the form $a\left(  t\right)
=\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$ with $a_{i}\in\mathfrak{gl}_{n}$, and
where we write $b\left(  t\right)  $ in the form $b\left(  t\right)
=\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ with $b_{i}\in\mathfrak{gl}_{n}$).

Then, the restriction of the Japanese cocycle $\alpha:\overline{\mathfrak{a}%
_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ to
$L\mathfrak{gl}_{n}\times L\mathfrak{gl}_{n}$ is the $2$-cocycle $\omega$.
\end{proposition}

\begin{remark}
The $2$-cocycle $\omega$ in Proposition \ref{prop.ainf.alphaomega} coincides
with the cocycle $\omega$ defined in Definition \ref{def.loop} in the case
when $\mathfrak{g}=\mathfrak{gl}_{n}$ and $\left(  \cdot,\cdot\right)  $ is
the form $\mathfrak{gl}_{n}\times\mathfrak{gl}_{n}\rightarrow\mathbb{C}%
,\ \left(  a,b\right)  \mapsto\operatorname*{Tr}\left(  ab\right)  $. The
$1$-dimensional central extension $\widehat{\mathfrak{gl}_{n}}_{\omega}$
induced by this $2$-cocycle $\omega$ (by the procedure shown in Definition
\ref{def.loop}) will be denoted by $\widehat{\mathfrak{gl}_{n}}$ in the
following. Note that $\widehat{\mathfrak{gl}_{n}}=L\mathfrak{gl}_{n}%
\oplus\mathbb{C}K$ as a vector space.

Note that the equality (\ref{prop.ainf.alphaomega.form}) can be rewritten in
the suggestive form%
\[
\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\operatorname*{Res}\nolimits_{t=0}\operatorname*{Tr}\left(  da\left(
t\right)  b\left(  t\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a\left(  t\right)  \in L\mathfrak{gl}_{n}\text{ and }b\left(  t\right)  \in
L\mathfrak{gl}_{n}%
\]
(as long as the ``matrix-valued differential form'' $da\left(  t\right)
b\left(  t\right)  $ is understood correctly).
\end{remark}

\textit{Proof of Proposition \ref{prop.ainf.alphaomega}.} We need to prove
that $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  $ for any
$a\left(  t\right)  \in L\mathfrak{gl}_{n}$ and $b\left(  t\right)  \in
L\mathfrak{gl}_{n}$ (where, of course, we consider $a\left(  t\right)  $ and
$b\left(  t\right)  $ as elements of $\overline{\mathfrak{a}_{\infty}}$ in the
term $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  $).

Write $a\left(  t\right)  $ in the form $a\left(  t\right)  =\sum
\limits_{k\in\mathbb{Z}}a_{k}t^{k}$ with all $a_{k}$ lying in $\mathfrak{gl}%
_{n}$. Write $b\left(  t\right)  $ in the form $b\left(  t\right)
=\sum\limits_{k\in\mathbb{Z}}b_{k}t^{k}$ with all $b_{k}$ lying in
$\mathfrak{gl}_{n}$.

In the following, for any integers $u$ and $v$, the $\left(  u,v\right)  $-th
\textit{block} of a matrix will mean the submatrix obtained by leaving only
the rows numbered $un+1$, $un+2$, $...$, $\left(  u+1\right)  n$ and the
columns numbered $vn+1$, $vn+2$, $...$, $\left(  v+1\right)  n$. (This, of
course, makes sense only when the matrix has such rows and such columns.)

By the definition of our embedding $\operatorname*{Toep}\nolimits_{n}\left(
a\left(  t\right)  \right)  :L\mathfrak{gl}_{n}\rightarrow\overline
{\mathfrak{a}_{\infty}}$, we have%
\begin{align*}
a\left(  t\right)   &  =\operatorname*{Toep}\nolimits_{n}\left(  a\left(
t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
b\left(  t\right)   &  =\operatorname*{Toep}\nolimits_{n}\left(  b\left(
t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  ,
\end{align*}
where the matrices $\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  $ are understood as block matrices made of $n\times n$ blocks.

In order to compute $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)
\right)  $, let us write these two infinite matrices $a\left(  t\right)  $ and
$b\left(  t\right)  $ as $2\times2$ block matrices \textbf{made of infinite
blocks each}, where the blocks are separated as follows:

- The left blocks contain the $j$-th columns for all $j\leq0$; the right
blocks contain the $j$-th columns for all $j>0$.

- The upper blocks contain the $i$-th rows for all $i\leq0$; the lower blocks
contain the $i$-th rows for all $i>0$.

Written like this, the matrix $a\left(  t\right)  $ takes the form $\left(
\begin{array}
[c]{cc}%
A_{11} & A_{12}\\
A_{21} & A_{22}%
\end{array}
\right)  $ with%
\begin{align*}
A_{11}  &  =\left(
\begin{array}
[c]{cccc}%
... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2}\\
... & a_{-1} & a_{0} & a_{1}\\
... & a_{-2} & a_{-1} & a_{0}%
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ A_{12}=\left(
\begin{array}
[c]{cccc}%
... & ... & ... & ...\\
a_{3} & a_{4} & a_{5} & ...\\
a_{2} & a_{3} & a_{4} & ...\\
a_{1} & a_{2} & a_{3} & ...
\end{array}
\right)  ,\\
A_{21}  &  =\left(
\begin{array}
[c]{cccc}%
... & a_{-3} & a_{-2} & a_{-1}\\
... & a_{-4} & a_{-3} & a_{-2}\\
... & a_{-5} & a_{-4} & a_{-3}\\
... & ... & ... & ...
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ A_{22}=\left(
\begin{array}
[c]{cccc}%
a_{0} & a_{1} & a_{2} & ...\\
a_{-1} & a_{0} & a_{1} & ...\\
a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ...
\end{array}
\right)  ,
\end{align*}
and the matrix $b\left(  t\right)  $ takes the form $\left(
\begin{array}
[c]{cc}%
B_{11} & B_{12}\\
B_{21} & B_{22}%
\end{array}
\right)  $ with similarly-defined blocks $B_{11}$, $B_{12}$, $B_{21}$ and
$B_{22}$.

By the definition of $\alpha$ given in Theorem \ref{thm.japan}, we now have
$\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}B_{21}\right)  $. We now need
to compute $B_{12}A_{21}$ and $A_{12}B_{21}$ in order to simplify this.

Now, since $B_{12}=\left(
\begin{array}
[c]{cccc}%
... & ... & ... & ...\\
b_{3} & b_{4} & b_{5} & ...\\
b_{2} & b_{3} & b_{4} & ...\\
b_{1} & b_{2} & b_{3} & ...
\end{array}
\right)  $ and $A_{21}=\left(
\begin{array}
[c]{cccc}%
... & a_{-3} & a_{-2} & a_{-1}\\
... & a_{-4} & a_{-3} & a_{-2}\\
... & a_{-5} & a_{-4} & a_{-3}\\
... & ... & ... & ...
\end{array}
\right)  $, the matrix $B_{12}A_{21}$ is a matrix whose rows and columns are
indexed by nonpositive integers, and whose $\left(  i,j\right)  $-th block
equals $\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k-\left(  i+1\right)
}a_{-k+\left(  j+1\right)  }$ for any pair of negative integers $i$ and $j$.
Similarly, the matrix $A_{12}B_{21}$ is a matrix whose rows and columns are
indexed by nonpositive integers, and whose $\left(  i,j\right)  $-th block
equals $\sum\limits_{k\in\mathbb{Z};\ k>0}a_{k-\left(  i+1\right)
}b_{-k+\left(  j+1\right)  }$ for any pair of negative integers $i$ and $j$.
Thus, the matrix $-B_{12}A_{21}+A_{12}B_{21}$ is a matrix whose rows and
columns are indexed by nonpositive integers, and whose $\left(  i,j\right)
$-th block equals $-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k-\left(  i+1\right)
}a_{-k+\left(  j+1\right)  }+\sum\limits_{k\in\mathbb{Z};\ k>0}a_{k-\left(
i+1\right)  }b_{-k+\left(  j+1\right)  }$ for any pair of negative integers
$i$ and $j$. But since $\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}%
B_{21}\right)  $ is clearly the sum of the traces of the $\left(  i,i\right)
$-th blocks of the matrix $-B_{12}A_{21}+A_{12}B_{21}$ over all negative
integers $i$, we thus have%
\begin{align*}
\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}B_{21}\right)   &
=\sum\limits_{i\in\mathbb{Z};\ i<0}\operatorname*{Tr}\left(  -\sum
\limits_{k\in\mathbb{Z};\ k>0}b_{k-\left(  i+1\right)  }a_{-k+\left(
i+1\right)  }+\sum\limits_{k\in\mathbb{Z};\ k>0}a_{k-\left(  i+1\right)
}b_{-k+\left(  i+1\right)  }\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\leq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k-i}a_{-k+i}+\sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k-i}b_{-k+i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}i+1\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}+\sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k+i}b_{-k-i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}-i\text{ in the first sum}\right)  .
\end{align*}


We are now going to split the first sum on the right hand side and get the
$\operatorname*{Tr}$ out of it. To see that this is allowed, we notice that
each of the infinite sums $\sum\limits_{\substack{\left(  i,k\right)
\in\mathbb{Z}^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}$ and $\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}a_{k+i}b_{-k-i}$ converges with respect to the discrete
topology\footnote{\textit{Proof.} Since $\sum\limits_{k\in\mathbb{Z}}%
a_{k}t^{k}=a\left(  t\right)  \in L\mathfrak{gl}_{n}$, only finitely many
$k\in\mathbb{Z}$ satisfy $a_{k}\neq0$. Hence, there exists some $N\in
\mathbb{Z}$ such that every $\nu\in\mathbb{Z}$ satisfying $\nu<N$ satisfies
$a_{\nu}=0$. Consider this $N$. Any pair $\left(  i,k\right)  \in
\mathbb{Z}^{2}$ such that $k+i>-N$ satisfies $-k-i=-\underbrace{\left(
k+i\right)  }_{>-N}<N$ and thus $a_{-k-i}=0$ (because we know that every
$\nu\in\mathbb{Z}$ satisfying $\nu<N$ satisfies $a_{\nu}=0$) and thus
$b_{k+i}a_{-k-i}=0$. Thus, all but finitely many pairs $\left(  i,k\right)
\in\mathbb{Z}^{2}$ such that $i\geq0$ and $k>0$ satisfy $b_{k+i}a_{-k-i}=0$
(because it is clear that all but finitely many pairs $\left(  i,k\right)
\in\mathbb{Z}^{2}$ such that $i\geq0$ and $k>0$ satisfy $k+i>-N$). In other
words, the sum $\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}%
^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}$ converges with respect to the discrete
topology. A similar argument shows that the sum $\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}a_{k+i}b_{-k-i}$ converges with respect to the discrete topology.}.
Hence, we can transform these sums as we please: For example,%
\begin{align}
&  \sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}b_{k+i}a_{-k-i}\nonumber\\
&  =\sum\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}}\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0;\\k+i=\ell}}\underbrace{b_{k+i}}_{\substack{=b_{\ell}\\\text{(since
}k+i=\ell\text{)}}}\underbrace{a_{-k-i}}_{\substack{=a_{-\left(  k+i\right)
}=a_{-\ell}\\\text{(since }k+i=\ell\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }k+i>0\text{ for all }i\geq0\text{ and }k>0\right) \nonumber\\
&  =\sum\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}}\underbrace{\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0;\\k+i=\ell}}b_{\ell}a_{-\ell}}_{\substack{=\ell b_{\ell}a_{-\ell
}\\\text{(since there exist exactly }\ell\text{ pairs }\left(  i,k\right)
\in\mathbb{Z}^{2}\\\text{satisfying }i\geq0\text{, }k>0\text{ and }%
k+i=\ell\text{)}}}=\sum\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}}\ell
b_{\ell}a_{-\ell}=\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}kb_{k}a_{-k}
\label{pf.ainf.alphaomega.sum1}%
\end{align}
(here, we renamed the summation index $\ell$ as $k$) and similarly
\begin{equation}
\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}a_{k+i}b_{-k-i}=\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}%
}ka_{k}b_{-k}. \label{pf.ainf.alphaomega.sum2}%
\end{equation}
The equality (\ref{pf.ainf.alphaomega.sum1}) becomes%
\begin{align*}
\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}b_{k+i}a_{-k-i}  &  =\sum\limits_{\substack{k\in\mathbb{Z}%
;\\k>0}}kb_{k}a_{-k}=\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}}\left(
-k\right)  b_{-k}a_{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k\text{ for
}-k\text{ in the first sum}\right) \\
&  =-\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}}kb_{-k}a_{k},
\end{align*}
so that
\begin{equation}
\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}}kb_{-k}a_{k}=-\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}b_{k+i}a_{-k-i}. \label{pf.ainf.alphaomega.sum3}%
\end{equation}


But
\begin{align*}
&  \omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right) \\
&  =\sum\limits_{k\in\mathbb{Z}}k\operatorname*{Tr}\left(  a_{k}b_{-k}\right)
=\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}}k\underbrace{\operatorname*{Tr}%
\left(  a_{k}b_{-k}\right)  }_{=\operatorname*{Tr}\left(  b_{-k}a_{k}\right)
}+\underbrace{0\operatorname*{Tr}\left(  a_{0}b_{-0}\right)  }_{=0}%
+\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}k\operatorname*{Tr}\left(
a_{k}b_{-k}\right) \\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}%
}k\operatorname*{Tr}\left(  b_{-k}a_{k}\right)  }_{=\operatorname*{Tr}\left(
\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}}kb_{-k}a_{k}\right)
}+\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}k\operatorname*{Tr}%
\left(  a_{k}b_{-k}\right)  }_{=\operatorname*{Tr}\left(  \sum
\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{k}b_{-k}\right)  }\\
&  =\operatorname*{Tr}\left(  \underbrace{\sum\limits_{\substack{k\in
\mathbb{Z};\\k<0}}kb_{-k}a_{k}}_{\substack{=-\sum\limits_{\substack{\left(
i,k\right)  \in\mathbb{Z}^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}\\\text{(by
(\ref{pf.ainf.alphaomega.sum3}))}}}\right)  +\operatorname*{Tr}\left(
\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{k}b_{-k}%
}_{\substack{=\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}%
^{2};\\i\geq0;\ k>0}}a_{k+i}b_{-k-i}\\\text{(by (\ref{pf.ainf.alphaomega.sum2}%
))}}}\right) \\
&  =\operatorname*{Tr}\left(  -\sum\limits_{\substack{\left(  i,k\right)
\in\mathbb{Z}^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}\right)  +\operatorname*{Tr}%
\left(  \sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2}%
;\\i\geq0;\ k>0}}a_{k+i}b_{-k-i}\right) \\
&  =\operatorname*{Tr}\left(  -\sum\limits_{i\in\mathbb{Z};\ i\geq0}%
\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}\right)  +\operatorname*{Tr}%
\left(  \sum\limits_{i\in\mathbb{Z};\ i\geq0}\sum\limits_{k\in\mathbb{Z}%
;\ k>0}a_{k+i}b_{-k-i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have unfolded our single sums
into double sums}\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}\right)  +\sum
\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(  \sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k+i}b_{-k-i}\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}+\sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k+i}b_{-k-i}\right)  =\operatorname*{Tr}\left(
-B_{12}A_{21}+A_{12}B_{21}\right) \\
&  =\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  .
\end{align*}
Thus, $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  $ is proven, so
we have verified Proposition \ref{prop.ainf.alphaomega}.

Note that Proposition \ref{prop.ainf.alphaomega} gives a new proof of
Proposition \ref{prop.japan.nontr}. This proof (whose details are left to the
reader) uses two easy facts:

\begin{itemize}
\item If $\sigma:\mathfrak{g}\times\mathfrak{g}\rightarrow\mathbb{C}$ is a
$2$-coboundary on a Lie algebra $\mathfrak{g}$, and $\mathfrak{h}$ is a Lie
subalgebra of $\mathfrak{g}$, then $\sigma\mid_{\mathfrak{h}\times
\mathfrak{h}}$ must be a $2$-coboundary on $\mathfrak{h}$.

\item For any positive integer $n$, the $2$-cocycle $\omega$ of Proposition
\ref{prop.ainf.alphaomega} is not a $2$-coboundary.
\end{itemize}

But if we look closely at this argument, we see that it is not a completely
new proof; it is a direct generalization of the proof of Proposition
\ref{prop.japan.nontr} that we gave above. In fact, in the particular case
when $n=1$, our embedding of $L\mathfrak{gl}_{n}$ into $\overline
{\mathfrak{a}_{\infty}}$ becomes the canonical injection of the abelian Lie
subalgebra $\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $ into
$\overline{\mathfrak{a}_{\infty}}$ (where $T$ is as in the proof of
Proposition \ref{prop.japan.nontr}), and we see that what we just did was
generalizing that abelian Lie subalgebra.

\begin{definition}
Due to Proposition \ref{prop.ainf.alphaomega}, the restriction of the
$2$-cocycle $\alpha$ to $L\mathfrak{gl}_{n}\times L\mathfrak{gl}_{n}$ is the
$2$-cocycle $\omega$. Thus, the $1$-dimensional central extension of
$L\mathfrak{gl}_{n}$ determined by the $2$-cocycle $\omega$ canonically
injects into the $1$-dimensional central extension of $\overline
{\mathfrak{a}_{\infty}}$ determined by the $2$-cocycle $\alpha$. If we recall
that the $1$-dimensional central extension of $L\mathfrak{gl}_{n}$ by the
$2$-cocycle $\omega$ is $\widehat{\mathfrak{gl}_{n}}$ whereas the
$1$-dimensional central extension of $\overline{\mathfrak{a}_{\infty}}$
determined by the $2$-cocycle $\alpha$ is $\mathfrak{a}_{\infty}$, we can
rewrite this as follows: We have an injection $\widehat{\mathfrak{gl}_{n}%
}\rightarrow\mathfrak{a}_{\infty}$ which lifts the inclusion $L\mathfrak{gl}%
_{n}\subseteq\overline{\mathfrak{a}_{\infty}}$ and sends $K$ to $K$. We denote
this inclusion map $\widehat{\mathfrak{gl}_{n}}\rightarrow\mathfrak{a}%
_{\infty}$ by $\widehat{\operatorname*{Toep}\nolimits_{n}}$, but we will often
consider it as an inclusion.

Similarly, we can get an inclusion $\widehat{\mathfrak{sl}_{n}}\subseteq
\mathfrak{a}_{\infty}$ which lifts the inclusion $L\mathfrak{sl}_{n}%
\subseteq\overline{\mathfrak{a}_{\infty}}$.

So $\mathcal{B}^{\left(  m\right)  }\cong\mathcal{F}^{\left(  m\right)  }$ is
a module over $\widehat{\mathfrak{gl}_{n}}$ and $\widehat{\mathfrak{sl}_{n}}$
at level $1$ (this means that $K$ acts as $1$).
\end{definition}

\begin{corollary}
\label{cor.glnhat.div}There is a Lie algebra isomorphism $\widehat{\phi
}:\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{1}}$ which sends $K$ to $K$
and sends $a_{m}$ to $T^{m}\in\widehat{\mathfrak{gl}_{1}}$ for every
$m\in\mathbb{Z}$. (Here, we are considering the injection
$\widehat{\mathfrak{gl}_{1}}\rightarrow\mathfrak{a}_{\infty}$ as an inclusion,
so that $\widehat{\mathfrak{gl}_{1}}$ is identified with the image of this inclusion.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.glnhat.div}.} There is an obvious Lie
algebra isomorphism $\phi:\overline{\mathcal{A}}\rightarrow L\mathfrak{gl}%
_{1}$ which sends $a_{m}$ to $t^{m}\in L\mathfrak{gl}_{1}$ for every
$m\in\mathbb{Z}$. This isomorphism $\phi$ is easily seen to satisfy%
\begin{equation}
\omega\left(  \phi\left(  x\right)  ,\phi\left(  y\right)  \right)
=\omega^{\prime}\left(  x,y\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
x\in\overline{\mathcal{A}}\text{ and }y\in\overline{\mathcal{A}}\text{,}
\label{pf.glinhat.div.1}%
\end{equation}
where $\omega:L\mathfrak{gl}_{1}\times L\mathfrak{gl}_{1}\rightarrow
\mathbb{C}$ is the $2$-cocycle on $L\mathfrak{gl}_{1}$ defined in Proposition
\ref{prop.ainf.alphaomega}, and $\omega^{\prime}:\overline{\mathcal{A}}%
\times\overline{\mathcal{A}}\rightarrow\mathbb{C}$ is the $2$-cocycle on
$\overline{\mathcal{A}}$ defined by%
\[
\omega^{\prime}\left(  a_{k},a_{\ell}\right)  =k\delta_{k,-\ell}%
\ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\text{ and }\ell\in
\mathbb{Z}.
\]
Thus, the Lie algebra isomorphism $\phi:\overline{\mathcal{A}}\rightarrow
L\mathfrak{gl}_{1}$ gives rise to an isomorphism $\widehat{\phi}$ from the
extension of $\overline{\mathcal{A}}$ defined by the $2$-cocycle
$\omega^{\prime}$ to the extension of $L\mathfrak{gl}_{1}$ defined by the
$2$-cocycle $\omega$. Since the extension of $\overline{\mathcal{A}}$ defined
by the $2$-cocycle $\omega^{\prime}$ is $\mathcal{A}$, while the extension of
$L\mathfrak{gl}_{1}$ defined by the $2$-cocycle $\omega$ is
$\widehat{\mathfrak{gl}_{1}}$, this rewrites as follows: The Lie algebra
isomorphism $\phi:\overline{\mathcal{A}}\rightarrow L\mathfrak{gl}_{1}$ gives
rise to an isomorphism $\widehat{\phi}:\mathcal{A}\rightarrow
\widehat{\mathfrak{gl}_{1}}$. This isomorphism $\widehat{\phi}$ sends $K$ to
$K$, and sends $a_{m}$ to $t^{m}\in\widehat{\mathfrak{gl}_{1}}$ for every
$m\in\mathbb{Z}$. Since $t^{m}$ corresponds to $T^{m}$ under our inclusion
$\widehat{\mathfrak{gl}_{1}}\rightarrow\mathfrak{a}_{\infty}$ (in fact,
$\operatorname*{Toep}\nolimits_{1}\left(  t^{m}\right)  =T^{m}$), this shows
that $\widehat{\phi}$ sends $a_{m}$ to $T^{m}\in\widehat{\mathfrak{gl}_{1}}$
for every $m\in\mathbb{Z}$. Corollary \ref{cor.glnhat.div} is thus proven.

\begin{proposition}
\label{prop.glnhat.T}Let $n$ be a positive integer. Consider the shift
operator $T$. Let us regard the injections $\overline{\mathfrak{a}_{\infty}%
}\rightarrow\mathfrak{a}_{\infty}$, $L\mathfrak{gl}_{n}\rightarrow
\overline{\mathfrak{a}_{\infty}}$ and $\widehat{\mathfrak{gl}_{n}}%
\rightarrow\mathfrak{a}_{\infty}$ as inclusions, so that $L\mathfrak{gl}_{n}$,
$\widehat{\mathfrak{gl}_{n}}$ and $\mathfrak{a}_{\infty}$ all become subspaces
of $\mathfrak{a}_{\infty}$.

\textbf{(a)} For every $m\in\mathbb{Z}$, we have $T^{m}\in L\mathfrak{gl}%
_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$.

\textbf{(b)} We have $\widehat{\mathfrak{gl}_{1}}\subseteq
\widehat{\mathfrak{gl}_{n}}$. Hence, the Lie algebra isomorphism
$\widehat{\phi}:\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{1}}$ constructed
in Corollary \ref{cor.glnhat.div} induces a Lie algebra injection
$\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{n}}$ (which sends every
$a\in\mathcal{A}$ to $\widehat{\phi}\left(  a\right)  \in
\widehat{\mathfrak{gl}_{n}}$). The restriction of the $\widehat{\mathfrak{gl}%
_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$ by means of this injection is
the $\mathcal{A}$-module $\mathcal{F}^{\left(  m\right)  }$ that we know.
\end{proposition}

\textit{First proof of Proposition \ref{prop.glnhat.T}.} \textbf{(a)} We
recall that $T=\left(
\begin{array}
[c]{cccccc}%
... & ... & ... & ... & ... & ...\\
... & 0 & 1 & 0 & 0 & ...\\
... & 0 & 0 & 1 & 0 & ...\\
... & 0 & 0 & 0 & 1 & ...\\
... & 0 & 0 & 0 & 0 & ...\\
... & ... & ... & ... & ... & ...
\end{array}
\right)  $ (this is the matrix which has $1$'s on the $1$-st diagonal and
$0$'s everywhere else). Clearly, $T\in\overline{\mathfrak{a}_{\infty}}$. We
want to prove that $T$ lies in $L\mathfrak{gl}_{n}\subseteq\overline
{\mathfrak{a}_{\infty}}$.

Let $a_{0}=\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & ... & 0\\
0 & 0 & 1 & ... & 0\\
0 & 0 & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & 0
\end{array}
\right)  $ (this is the $n\times n$ matrix which has $1$'s on the $1$-st
diagonal and $0$'s everywhere else).

Let $a_{1}=\left(
\begin{array}
[c]{ccccc}%
0 & 0 & 0 & ... & 0\\
0 & 0 & 0 & ... & 0\\
0 & 0 & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & 0\\
1 & 0 & 0 & ... & 0
\end{array}
\right)  $ (this is the $n\times n$ matrix which has a $1$ in its lowermost
leftmost corner, and $0$'s everywhere else).

Then, $T=\operatorname*{Toep}\nolimits_{n}\left(  a_{0}+ta_{1}\right)  $.
Thus, for every $m\in\mathbb{N}$, we have%
\begin{align*}
T^{m}  &  =\left(  \operatorname*{Toep}\nolimits_{n}\left(  a_{0}%
+ta_{1}\right)  \right)  ^{m}=\operatorname*{Toep}\nolimits_{n}\left(  \left(
a_{0}+ta_{1}\right)  ^{m}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{because of
Proposition \ref{prop.Toep.alg}}\right) \\
&  \in\operatorname*{Toep}\nolimits_{n}\left(  L\mathfrak{gl}_{n}\right)
=L\mathfrak{gl}_{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since we regard
}\operatorname*{Toep}\nolimits_{n}\text{ as an inclusion}\right)  .
\end{align*}
Since it is easy to see that $T^{-1}\in L\mathfrak{gl}_{n}$ as
well\footnote{This is analogous to $T\in L\mathfrak{gl}_{n}$ (because $T^{-1}$
is the matrix which has $1$'s on the $\left(  -1\right)  $-st diagonal and
$0$'s everywhere else).}, a similar argument yields that $\left(
T^{-1}\right)  ^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{N}$. In other
words, $T^{-m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{N}$. In other
words, $T^{m}\in\mathfrak{gl}_{n}$ for all nonpositive integers $m$. Combined
with the fact that $T^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{N}$,
this yields that $T^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{Z}$. Since
$L\mathfrak{gl}_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$, we thus have
$T^{m}\in L\mathfrak{gl}_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$ for all
$m\in\mathbb{Z}$. This proves Proposition \ref{prop.glnhat.T} \textbf{(a)}.

\textbf{(b)} For every $a\left(  t\right)  \in L\mathfrak{gl}_{1}$, we have
$\operatorname*{Toep}\nolimits_{1}\left(  a\left(  t\right)  \right)
\in\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $%
\ \ \ \ \footnote{\textit{Proof.} Let $a\left(  t\right)  \in L\mathfrak{gl}%
_{1}$. Write $a\left(  t\right)  $ in the form $\sum\limits_{i\in\mathbb{Z}%
}a_{i}t^{i}$ with $a_{i}\in\mathfrak{gl}_{1}$. Then, of course, the $a_{i}$
are scalars (since $\mathfrak{gl}_{1}=\mathbb{C}$). By the definition of
$\operatorname*{Toep}\nolimits_{1}$, we have%
\[
\operatorname*{Toep}\nolimits_{1}\left(  a\left(  t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  =\sum\limits_{i\in\mathbb{Z}}a_{i}T^{i}\in\left\langle T^{j}%
\ \mid\ j\in\mathbb{Z}\right\rangle ,
\]
qed.}. Thus, $\operatorname*{Toep}\nolimits_{1}\left(  L\mathfrak{gl}%
_{1}\right)  \subseteq\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $.
Since we are considering $\operatorname*{Toep}\nolimits_{1}$ as an inclusion,
this becomes $L\mathfrak{gl}_{1}\subseteq\left\langle T^{j}\ \mid
\ j\in\mathbb{Z}\right\rangle $. Combined with $\left\langle T^{j}\ \mid
\ j\in\mathbb{Z}\right\rangle \subseteq L\mathfrak{gl}_{n}$ (because every
$m\in\mathbb{Z}$ satisfies $T^{m}\in L\mathfrak{gl}_{n}$ (according to
Proposition \ref{prop.glnhat.T} \textbf{(a)})), this yields $L\mathfrak{gl}%
_{1}\subseteq L\mathfrak{gl}_{n}$. Thus, $\widehat{\mathfrak{gl}_{1}}%
\subseteq\widehat{\mathfrak{gl}_{n}}$.

Hence, the Lie algebra isomorphism $\widehat{\phi}:\mathcal{A}\rightarrow
\widehat{\mathfrak{gl}_{1}}$ constructed in Corollary \ref{cor.glnhat.div}
induces a Lie algebra injection $\mathcal{A}\rightarrow\widehat{\mathfrak{gl}%
_{n}}$ (which sends every $a\in\mathcal{A}$ to $\widehat{\phi}\left(
a\right)  \in\widehat{\mathfrak{gl}_{n}}$). This injection is exactly the
embedding $\mathcal{A}\rightarrow\mathfrak{a}_{\infty}$ constructed in
Definition \ref{def.ainf.A} (apart from the fact that its target is
$\widehat{\mathfrak{gl}_{n}}$ rather than $\mathfrak{a}_{\infty}$). Hence, the
restriction of the $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(
m\right)  }$ by means of this injection is the $\mathcal{A}$-module
$\mathcal{F}^{\left(  m\right)  }$ that we know\footnote{because both the
$\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$ and
the $\mathcal{A}$-module $\mathcal{F}^{\left(  m\right)  }$ were defined as
restrictions of the $\mathfrak{a}_{\infty}$-module $\mathcal{F}^{\left(
m\right)  }$}. This proves Proposition \ref{prop.glnhat.T} \textbf{(b)}.

Our inclusions $L\mathfrak{gl}_{n}\subseteq\overline{\mathfrak{a}_{\infty}}$
and $\widehat{\mathfrak{gl}_{n}}\subseteq\mathfrak{a}_{\infty}$ can be
somewhat refined: For any positive integers $n$ and $N$ satisfying $n\mid N$,
we have $L\mathfrak{gl}_{n}\subseteq L\mathfrak{gl}_{N}$ and
$\widehat{\mathfrak{gl}_{n}}\subseteq\widehat{\mathfrak{gl}_{N}}$. Let us
formulate this more carefully without abuse of notation:

\begin{proposition}
\label{prop.glnhat.div}Let $n$ and $N$ be positive integers such that $n\mid
N$. Then, the inclusion $\operatorname*{Toep}\nolimits_{n}:L\mathfrak{gl}%
_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$ factors through the inclusion
$\operatorname*{Toep}\nolimits_{N}:L\mathfrak{gl}_{N}\rightarrow
\overline{\mathfrak{a}_{\infty}}$. More precisely:

Let $d=\dfrac{N}{n}$. Let $\operatorname*{Toep}\nolimits_{n,N}:L\mathfrak{gl}%
_{n}\rightarrow L\mathfrak{gl}_{N}$ be the map which sends every $a\left(
t\right)  \in L\mathfrak{gl}_{n}$ to%
\[
\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left(
\begin{array}
[c]{ccccc}%
a_{\left(  j-i\right)  d} & a_{\left(  j-i\right)  d+1} & a_{\left(
j-i\right)  d+2} & ... & a_{\left(  j-i\right)  d+\left(  d-1\right)  }\\
a_{\left(  j-i\right)  d-1} & a_{\left(  j-i\right)  d} & a_{\left(
j-i\right)  d+1} & ... & a_{\left(  j-i\right)  d+\left(  d-2\right)  }\\
a_{\left(  j-i\right)  d-2} & a_{\left(  j-i\right)  d-1} & a_{\left(
j-i\right)  d} & ... & a_{\left(  j-i\right)  d+\left(  d-3\right)  }\\
... & ... & ... & ... & ...\\
a_{\left(  j-i\right)  d-\left(  d-1\right)  } & a_{\left(  j-i\right)
d-\left(  d-2\right)  } & a_{\left(  j-i\right)  d-\left(  d-3\right)  } &
... & a_{\left(  j-i\right)  d}%
\end{array}
\right)  }_{\substack{\text{this is an }N\times N\text{-matrix constructed as
a }d\times d\text{-block matrix}\\\text{consisting of }n\times n\text{-blocks;
one can formally define this matrix}\\\text{as the }N\times N\text{-matrix
whose }\left(  nI+\alpha,nJ+\beta\right)  \text{-th entry equals}\\\text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a_{\left(  j-i\right)
d+J-I}\text{ for all }I\in\left\{  0,1,...,d-1\right\}  \text{,}\\J\in\left\{
0,1,...,d-1\right\}  \text{, }\alpha\in\left\{  1,2,...,n\right\}  \text{ and
}\beta\in\left\{  1,2,...,n\right\}  }}t^{\ell}\in L\mathfrak{gl}_{N}%
\]
(where we write $a\left(  t\right)  $ in the form $a\left(  t\right)
=\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$ with $a_{i}\in\mathfrak{gl}_{n}$).

\textbf{(a)} We have $\operatorname*{Toep}\nolimits_{N}\circ
\operatorname*{Toep}\nolimits_{n,N}=\operatorname*{Toep}\nolimits_{n}$. In
other words, we can regard $\operatorname*{Toep}\nolimits_{n,N}$ as an
inclusion map $L\mathfrak{gl}_{n}\rightarrow L\mathfrak{gl}_{N}$ which forms a
commutative triangle with the inclusion maps $\operatorname*{Toep}%
\nolimits_{n}:L\mathfrak{gl}_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$
and $\operatorname*{Toep}\nolimits_{N}:L\mathfrak{gl}_{N}\rightarrow
\overline{\mathfrak{a}_{\infty}}$. In other words, if we consider
$L\mathfrak{gl}_{n}$ and $L\mathfrak{gl}_{N}$ as Lie subalgebras of
$\overline{\mathfrak{a}_{\infty}}$ (by means of the injections
$\operatorname*{Toep}\nolimits_{n}:L\mathfrak{gl}_{n}\rightarrow
\overline{\mathfrak{a}_{\infty}}$ and $\operatorname*{Toep}\nolimits_{N}%
:L\mathfrak{gl}_{N}\rightarrow\overline{\mathfrak{a}_{\infty}}$), then
$L\mathfrak{gl}_{n}\subseteq L\mathfrak{gl}_{N}$.

\textbf{(b)} If we consider $\operatorname*{Toep}\nolimits_{n,N}$ as an
inclusion map $L\mathfrak{gl}_{n}\rightarrow L\mathfrak{gl}_{N}$, then the
$2$-cocycle $\omega:L\mathfrak{gl}_{n}\times L\mathfrak{gl}_{n}\rightarrow
\mathbb{C}$ defined in Proposition \ref{prop.ainf.alphaomega} is the
restriction of the similarly-defined $2$-cocycle $\omega:L\mathfrak{gl}%
_{N}\times L\mathfrak{gl}_{N}\rightarrow\mathbb{C}$ (we also call it $\omega$
because it is constructed similarly) to $L\mathfrak{gl}_{n}\times
L\mathfrak{gl}_{n}$. As a consequence, the inclusion map $\operatorname*{Toep}%
\nolimits_{n,N}:L\mathfrak{gl}_{n}\rightarrow L\mathfrak{gl}_{N}$ induces a
Lie algebra injection $\widehat{\operatorname*{Toep}\nolimits_{n,N}%
}:\widehat{\mathfrak{gl}_{n}}\rightarrow\widehat{\mathfrak{gl}_{N}}$ which
satisfies $\widehat{\operatorname*{Toep}\nolimits_{N}}\circ
\widehat{\operatorname*{Toep}\nolimits_{n,N}}=\widehat{\operatorname*{Toep}%
\nolimits_{n}}$. Thus, this injection $\widehat{\operatorname*{Toep}%
\nolimits_{n,N}}$ forms a commutative triangle with the inclusion maps
$\widehat{\operatorname*{Toep}\nolimits_{n}}:\widehat{\mathfrak{gl}_{n}%
}\rightarrow\mathfrak{a}_{\infty}$ and $\widehat{\operatorname*{Toep}%
\nolimits_{N}}:\widehat{\mathfrak{gl}_{N}}\rightarrow\mathfrak{a}_{\infty}$.
In other words, if we consider $\widehat{\mathfrak{gl}_{n}}$ and
$\widehat{\mathfrak{gl}_{N}}$ as Lie subalgebras of $\mathfrak{a}_{\infty}$
(by means of the injections $\widehat{\operatorname*{Toep}\nolimits_{n}%
}:\widehat{\mathfrak{gl}_{n}}\rightarrow\mathfrak{a}_{\infty}$ and
$\widehat{\operatorname*{Toep}\nolimits_{N}}:\widehat{\mathfrak{gl}_{N}%
}\rightarrow\mathfrak{a}_{\infty}$), then $\widehat{\mathfrak{gl}_{n}%
}\subseteq\widehat{\mathfrak{gl}_{N}}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glnhat.div}.} \textbf{(a)} The proof of
Proposition \ref{prop.glnhat.div} \textbf{(a)} is completely straightforward.
(One has to show that the $\left(  Ni+nI+\alpha,Nj+nJ+\beta\right)  $-th entry
of $\left(  \operatorname*{Toep}\nolimits_{N}\circ\operatorname*{Toep}%
\nolimits_{n,N}\right)  \left(  a\left(  t\right)  \right)  $ equals the
$\left(  Ni+nI+\alpha,Nj+nJ+\beta\right)  $-th entry of $\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \right)  $ for every $a\left(
t\right)  \in L\mathfrak{gl}_{n}$, every $i\in\mathbb{Z}$, every
$j\in\mathbb{Z}$, every $I\in\left\{  0,1,...,d-1\right\}  $, $J\in\left\{
0,1,...,d-1\right\}  $, $\alpha\in\left\{  1,2,...,n\right\}  $ and $\beta
\in\left\{  1,2,...,n\right\}  $.)

\textbf{(b)} The $2$-cocycle $\omega:L\mathfrak{gl}_{n}\times L\mathfrak{gl}%
_{n}\rightarrow\mathbb{C}$ defined in Proposition \ref{prop.ainf.alphaomega}
is the restriction of the similarly-defined $2$-cocycle $\omega:L\mathfrak{gl}%
_{N}\times L\mathfrak{gl}_{N}\rightarrow\mathbb{C}$ to $L\mathfrak{gl}%
_{n}\times L\mathfrak{gl}_{n}$. (This is because both of these $2$-cocycles
are restrictions of the Japanese cocycle $\alpha:\overline{\mathfrak{a}%
_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$, as
shown in Proposition \ref{prop.ainf.alphaomega}.) This proves Proposition
\ref{prop.glnhat.div}.

Note that Proposition \ref{prop.glnhat.div} can be used to derive Proposition
\ref{prop.glnhat.T}:

\textit{Second proof of Proposition \ref{prop.glnhat.T}.} \textbf{(a)} For
every $m\in\mathbb{Z}$, we have $T^{m}\in\widehat{\mathfrak{gl}_{1}}$ (because
the Lie algebra isomorphism $\widehat{\phi}$ constructed in Corollary
\ref{cor.glnhat.div} satisfies $\phi\left(  a_{m}\right)  =T^{m}$, so that
$T^{m}\in\phi\left(  a_{m}\right)  \in\widehat{\mathfrak{gl}_{1}}$). Thus, for
every $m\in\mathbb{Z}$, we have $T^{m}\in\widehat{\mathfrak{gl}_{1}}%
\cap\overline{\mathfrak{a}_{\infty}}=L\mathfrak{gl}_{1}$.

Due to Proposition \ref{prop.glnhat.div} \textbf{(a)}, we have $L\mathfrak{gl}%
_{1}\subseteq L\mathfrak{gl}_{n}$ (since $1\mid n$). Thus, for every
$m\in\mathbb{Z}$, we have $T^{m}\in L\mathfrak{gl}_{1}\subseteq L\mathfrak{gl}%
_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$. This proves Proposition
\ref{prop.glnhat.T} \textbf{(a)}.

\textbf{(b)} Due to Proposition \ref{prop.glnhat.div} \textbf{(b)}, we have
$\widehat{\mathfrak{gl}_{1}}\subseteq\widehat{\mathfrak{gl}_{n}}$ (since
$1\mid n$). Hence, the Lie algebra isomorphism $\widehat{\phi}:\mathcal{A}%
\rightarrow\widehat{\mathfrak{gl}_{1}}$ constructed in Corollary
\ref{cor.glnhat.div} induces a Lie algebra injection $\mathcal{A}%
\rightarrow\widehat{\mathfrak{gl}_{n}}$ (which sends every $a\in\mathcal{A}$
to $\widehat{\phi}\left(  a\right)  \in\widehat{\mathfrak{gl}_{n}}$). Formally
speaking, this injection is the map $\widehat{\operatorname*{Toep}%
\nolimits_{1,n}}\circ\widehat{\phi}:\mathcal{A}\rightarrow
\widehat{\mathfrak{gl}_{n}}$ (because the injection $\widehat{\mathfrak{gl}%
_{1}}\rightarrow\widehat{\mathfrak{gl}_{n}}$ is $\widehat{\operatorname*{Toep}%
\nolimits_{1,n}}$). Therefore, the restriction of the $\widehat{\mathfrak{gl}%
_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$ by means of this injection is%
\begin{align*}
&  \left(  \text{the restriction of the }\widehat{\mathfrak{gl}_{n}%
}\text{-module }\mathcal{F}^{\left(  m\right)  }\text{ by means of the
injection }\widehat{\operatorname*{Toep}\nolimits_{1,n}}\circ\widehat{\phi
}:\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{n}}\right) \\
&  =\left(  \text{the restriction of the }\mathfrak{a}_{\infty}\text{-module
}\mathcal{F}^{\left(  m\right)  }\text{ by means of the injection
}\widehat{\operatorname*{Toep}\nolimits_{n}}\circ\widehat{\operatorname*{Toep}%
\nolimits_{1,n}}\circ\widehat{\phi}:\mathcal{A}\rightarrow\mathfrak{a}%
_{\infty}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the }\widehat{\mathfrak{gl}_{n}}\text{-module }\mathcal{F}%
^{\left(  m\right)  }\text{ itself was the restriction of the }\mathfrak{a}%
_{\infty}\text{-module }\mathcal{F}^{\left(  m\right)  }\\
\text{by means of the injection }\widehat{\operatorname*{Toep}\nolimits_{n}%
}:\widehat{\mathfrak{gl}_{n}}\rightarrow\mathfrak{a}_{\infty}%
\end{array}
\right) \\
&  =\left(  \text{the restriction of the }\mathfrak{a}_{\infty}\text{-module
}\mathcal{F}^{\left(  m\right)  }\text{ by means of the injection
}\widehat{\operatorname*{Toep}\nolimits_{1}}\circ\widehat{\phi}:\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\widehat{\operatorname*{Toep}\nolimits_{n}}\circ
\widehat{\operatorname*{Toep}\nolimits_{1,n}}=\widehat{\operatorname*{Toep}%
\nolimits_{1}}\\
\text{(by Proposition \ref{prop.glnhat.div} \textbf{(b)}, applied to }n\text{
and }1\text{ instead of }N\text{ and }n\text{)}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the restriction of the }\mathfrak{a}_{\infty}\text{-module }%
\mathcal{F}^{\left(  m\right)  }\text{ by means of the}\\
\text{embedding }\mathcal{A}\rightarrow\mathfrak{a}_{\infty}\text{ constructed
in Definition \ref{def.ainf.A}}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }\widehat{\operatorname*{Toep}\nolimits_{1}}\circ\widehat{\phi
}:\mathcal{A}\rightarrow\mathfrak{a}_{\infty}\text{ is exactly the}\\
\text{embedding }\mathcal{A}\rightarrow\mathfrak{a}_{\infty}\text{ constructed
in Definition \ref{def.ainf.A}}%
\end{array}
\right) \\
&  =\left(  \text{the }\mathcal{A}\text{-module }\mathcal{F}^{\left(
m\right)  }\text{ that we know}\right)  .
\end{align*}
This proves Proposition \ref{prop.glnhat.T} \textbf{(b)}.

\subsection{The semidirect product
\texorpdfstring{$\protect\widetilde{\mathfrak{gl}_{n}}$}{gl-n-tilde} and its
representation theory}

\subsubsection{Extending affine Lie algebras by derivations}

Now we give a definition pertaining to general affine Lie algebras:

\begin{definition}
\label{def.gwave}If $\widehat{\mathfrak{g}}=L\mathfrak{g}\oplus\mathbb{C}K$ is
an affine Lie algebra (the $\oplus$ sign here only means a direct sum of
vector spaces, not a direct sum of Lie algebras), then there exists a unique
linear map $d:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$ such
that $d\left(  a\left(  t\right)  \right)  =ta^{\prime}\left(  t\right)  $ for
every $a\left(  t\right)  \in L\mathfrak{g}$ (so that $d\left(  at^{\ell
}\right)  =\ell at^{\ell}$ for every $a\in\mathfrak{g}$ and $\ell\in
\mathbb{N}$) and $d\left(  K\right)  =0$. This linear map $d$ is a derivation
(as can be easily checked). Thus, the abelian Lie algebra $\mathbb{C}d$ (a
one-dimensional Lie algebra) acts on the Lie algebra $\widehat{\mathfrak{g}}$
by derivations (in the obvious way, with $d$ acting as $d$). Thus, a
semidirect product $\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$ is well-defined
(according to Definition \ref{def.semidir.lielie}).

Set $\widetilde{\mathfrak{g}}=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$.
Clearly, $\widetilde{\mathfrak{g}}=\mathbb{C}d\oplus\widehat{\mathfrak{g}}$ as
vector space. The Lie algebra $\widetilde{\mathfrak{g}}$ is graded by taking
the grading of $\widehat{\mathfrak{g}}$ and additionally giving $d$ the degree
$0$.
\end{definition}

One can wonder which $\widehat{\mathfrak{g}}$-modules can be extended to
$\widetilde{\mathfrak{g}}$-modules. This can't be generally answered, but here
is a partial uniqueness result:

\begin{lemma}
\label{lem.gwave.uniqueder}Let $\mathfrak{g}$ be a Lie algebra, and $d$ be the
unique derivation $\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$
constructed in Definition \ref{def.gwave}. Let $M$ be a $\widehat{\mathfrak{g}%
}$-module, and $v$ an element of $M$ such that $M$ is generated by $v$ as a
$\widehat{\mathfrak{g}}$-module. Then, there exists \textbf{at most one}
extension of the $\widehat{\mathfrak{g}}$-representation on $M$ to
$\widetilde{\mathfrak{g}}$ such that $dv=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.gwave.uniqueder}.} Let $\rho_{1}%
:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}M$ and $\rho
_{2}:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}M$ be two
extensions of the $\widehat{\mathfrak{g}}$-representation on $M$ to
$\widetilde{\mathfrak{g}}$ such that $\rho_{1}\left(  d\right)  v=0$ and
$\rho_{2}\left(  d\right)  v=0$. If we succeed in showing that $\rho_{1}%
=\rho_{2}$, then Lemma \ref{lem.gwave.uniqueder} will be proven.

Let $U$ be the subset $\left\{  u\in M\ \mid\ \rho_{1}\left(  d\right)
u=\rho_{2}\left(  d\right)  u\right\}  $ of $M$. Clearly, $U$ is a vector
subspace of $M$. Also, $v\in U$ (since $\rho_{1}\left(  d\right)  v=0=\rho
_{2}\left(  d\right)  v$). We will now show that $U$ is a
$\widehat{\mathfrak{g}}$-submodule of $M$.

In fact, since $\rho_{1}$ is an action of $\widetilde{\mathfrak{g}}$ on $M$,
every $m\in M$ and every $\alpha\in\widehat{\mathfrak{g}}$ satisfy%
\[
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \rho_{1}\left(
\alpha\right)  m\right)  -\left(  \rho_{1}\left(  \alpha\right)  \right)
\left(  \rho_{1}\left(  d\right)  m\right)  =\rho_{1}\left(  \left[
d,\alpha\right]  \right)  m.
\]
Since $\rho_{1}\left(  \alpha\right)  m=\alpha\rightharpoonup m$ (because the
action $\rho_{1}$ extends the $\widehat{\mathfrak{g}}$-representation on $M$)
and $\left[  d,\alpha\right]  =d\left(  \alpha\right)  $ (by the definition of
the Lie bracket on the semidirect product $\widetilde{\mathfrak{g}}%
=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$), this rewrites as follows: Every
$m\in M$ and every $\alpha\in\widehat{\mathfrak{g}}$ satisfy%
\[
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  -\left(  \rho_{1}\left(  \alpha\right)  \right)  \left(  \rho
_{1}\left(  d\right)  m\right)  =\rho_{1}\left(  d\left(  \alpha\right)
\right)  m.
\]
Since $\left(  \rho_{1}\left(  \alpha\right)  \right)  \left(  \rho_{1}\left(
d\right)  m\right)  =\alpha\rightharpoonup\left(  \rho_{1}\left(  d\right)
m\right)  $ (again because the action $\rho_{1}$ extends the
$\widehat{\mathfrak{g}}$-representation on $M$) and $\rho_{1}\left(  d\left(
\alpha\right)  \right)  m=\left(  d\left(  \alpha\right)  \right)
\rightharpoonup m$ (for the same reason), this further rewrites as follows:
Every $m\in M$ and every $\alpha\in\widehat{\mathfrak{g}}$ satisfy%
\begin{equation}
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  -\alpha\rightharpoonup\left(  \rho_{1}\left(  d\right)  m\right)
=\left(  d\left(  \alpha\right)  \right)  \rightharpoonup m.
\label{pf.gwave.uniqueder.1}%
\end{equation}


Now, let $m\in U$ and $\alpha\in\widehat{\mathfrak{g}}$ be arbitrary. Then,
$\rho_{1}\left(  d\right)  m=\rho_{2}\left(  d\right)  m$ (by the definition
of $U$, since $m\in U$), but we have%
\[
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  =\alpha\rightharpoonup\left(  \rho_{1}\left(  d\right)  m\right)
+\left(  d\left(  \alpha\right)  \right)  \rightharpoonup m
\]
(by (\ref{pf.gwave.uniqueder.1})) and%
\[
\left(  \rho_{2}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  =\alpha\rightharpoonup\left(  \rho_{2}\left(  d\right)  m\right)
+\left(  d\left(  \alpha\right)  \right)  \rightharpoonup m
\]
(similarly). Hence,%
\begin{align*}
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)   &  =\alpha\rightharpoonup\underbrace{\left(  \rho_{1}\left(
d\right)  m\right)  }_{=\rho_{2}\left(  d\right)  m}+\left(  d\left(
\alpha\right)  \right)  \rightharpoonup m\\
&  =\alpha\rightharpoonup\left(  \rho_{2}\left(  d\right)  m\right)  +\left(
d\left(  \alpha\right)  \right)  \rightharpoonup m=\left(  \rho_{2}\left(
d\right)  \right)  \left(  \alpha\rightharpoonup m\right)  ,
\end{align*}
so that $\alpha\rightharpoonup m\in U$ (by the definition of $U$).

Now forget that we fixed $m\in U$ and $\alpha\in\widehat{\mathfrak{g}}$. We
thus have showed that $\alpha\rightharpoonup m\in U$ for every $m\in U$ and
$\alpha\in\widehat{\mathfrak{g}}$. In other words, $U$ is a
$\widehat{\mathfrak{g}}$-submodule of $M$. Since $v\in U$, this yields that
$U$ is a $\widehat{\mathfrak{g}}$-submodule of $M$ containing $v$, and thus
must be the whole $M$ (since $M$ is generated by $v$ as a
$\widehat{\mathfrak{g}}$-module). Thus, $M=U=\left\{  u\in M\ \mid\ \rho
_{1}\left(  d\right)  u=\rho_{2}\left(  d\right)  u\right\}  $. Hence, every
$u\in M$ satisfies $\rho_{1}\left(  d\right)  u=\rho_{2}\left(  d\right)  u$.
Thus, $\rho_{1}\left(  d\right)  =\rho_{2}\left(  d\right)  $.

Combining $\rho_{1}\mid_{\widehat{\mathfrak{g}}}=\rho_{2}\mid
_{\widehat{\mathfrak{g}}}$ (because both $\rho_{1}$ and $\rho_{2}$ are
extensions of the $\widehat{\mathfrak{g}}$-representation on $M$, and thus
coincide on $\widehat{\mathfrak{g}}$) and $\rho_{1}\mid_{\mathbb{C}d}=\rho
_{2}\mid_{\mathbb{C}d}$ (because $\rho_{1}\left(  d\right)  =\rho_{2}\left(
d\right)  $), we obtain $\rho_{1}=\rho_{2}$ (because the vector space
$\widetilde{\mathfrak{g}}=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$ is
generated by $\mathbb{C}d$ and $\widehat{\mathfrak{g}}$, and thus two linear
maps which coincide on $\mathbb{C}d$ and on $\widehat{\mathfrak{g}}$ must be
identical). Thus, as we said above, Lemma \ref{lem.gwave.uniqueder} is proven.

\subsubsection{\texorpdfstring{$\protect\widetilde{\mathfrak{gl}_{n}}$}
{gl-n-tilde}}

Applying Definition \ref{def.gwave} to $\mathfrak{g}=\mathfrak{gl}_{n}$, we
obtain a Lie algebra $\widetilde{\mathfrak{gl}_{n}}$. We want to study its
highest weight theory.

\begin{Convention}
For the sake of disambiguation, let us, in the following, use $E_{i,j}%
^{\mathfrak{gl}_{n}}$ to denote the elementary matrices of $\mathfrak{gl}_{n}$
(these are defined for $\left(  i,j\right)  \in\left\{  1,2,...,n\right\}
^{2}$), and use $E_{i,j}^{\mathfrak{gl}_{\infty}}$ to denote the elementary
matrices of $\mathfrak{gl}_{\infty}$ (these are defined for $\left(
i,j\right)  \in\mathbb{Z}^{2}$).
\end{Convention}

\begin{definition}
We can make $L\mathfrak{gl}_{n}$ into a graded Lie algebra by setting $\deg
E_{i,j}^{\mathfrak{gl}_{n}}=j-i$ (this, so far, is the standard grading on
$\mathfrak{gl}_{n}$) and $\deg t=n$. Consequently, $\widehat{\mathfrak{gl}%
_{n}}=\mathbb{C}K\oplus\mathfrak{gl}_{n}$ (this is just a direct sum of vector
spaces) becomes a graded Lie algebra with $\deg K=0$, and
$\widetilde{\mathfrak{gl}_{n}}=\mathbb{C}d\oplus\widehat{\mathfrak{gl}_{n}}$
(again, this is only a direct sum of vector spaces) becomes a graded Lie
algebra with $\deg d=0$.
\end{definition}

The triangular decomposition of $\widetilde{\mathfrak{gl}_{n}}$ is
$\widetilde{\mathfrak{gl}_{n}}=\widetilde{\mathfrak{n}_{-}}\oplus
\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}_{+}}$. Here,
$\widetilde{\mathfrak{h}}=\mathbb{C}K\oplus\mathbb{C}d\oplus\mathfrak{h}$
where $\mathfrak{h}$ is the Lie algebra of diagonal $n\times n$ matrices (in
other words, $\mathfrak{h}=\left\langle E_{1,1}^{\mathfrak{gl}_{n}}%
,E_{2,2}^{\mathfrak{gl}_{n}},...,E_{n,n}^{\mathfrak{gl}_{n}}\right\rangle $).
Further, $\widetilde{\mathfrak{n}_{+}}=\mathfrak{n}_{+}\oplus t\mathfrak{gl}%
_{n}\left[  t\right]  $ (where $\mathfrak{n}_{+}$ is the Lie algebra of
strictly upper-triangular matrices) and $\widetilde{\mathfrak{n}_{-}%
}=\mathfrak{n}_{-}\oplus t^{-1}\mathfrak{gl}_{n}\left[  t^{-1}\right]  $
(where $\mathfrak{n}_{-}$ is the Lie algebra of strictly lower-triangular matrices).

\begin{definition}
For every $m\in\mathbb{Z}$, define the weight $\widetilde{\omega}_{m}%
\in\widetilde{\mathfrak{h}}^{\ast}$ by%
\begin{align*}
\widetilde{\omega}_{m}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right)   &
=\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\leq\overline{m};\\
0,\text{ if }i>\overline{m}%
\end{array}
\right.  +\dfrac{m-\overline{m}}{n}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  ;\\
\widetilde{\omega}_{m}\left(  K\right)   &  =1;\\
\widetilde{\omega}_{m}\left(  d\right)   &  =0,
\end{align*}
where $\overline{m}$ is the remainder of $m$ modulo $n$ (that is, the element
of $\left\{  0,1,...,n-1\right\}  $ satisfying $m\equiv\overline
{m}\operatorname{mod}n$).
\end{definition}

Note that we can rewrite the definition of $\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  $ as%
\begin{align*}
&  \widetilde{\omega}_{m}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right) \\
&  =\left\{
\begin{array}
[c]{c}%
\left(  \text{the number of all }j\in\mathbb{Z}\text{ such that }j\equiv
i\operatorname{mod}n\text{ and }1\leq j\leq m\right)
,\ \ \ \ \ \ \ \ \ \ \text{if }m\geq0;\\
-\left(  \text{the number of all }j\in\mathbb{Z}\text{ such that }j\equiv
i\operatorname{mod}n\text{ and }m<j\leq0\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if
}m\leq0
\end{array}
\right.  .
\end{align*}


\subsubsection{The \texorpdfstring{$\protect\widetilde{\mathfrak{gl}_{n}}$}
{gl-n-tilde}-module
\texorpdfstring{$\mathcal{F}^{\left(  m\right)  }$}{structure on the
semi-infinite wedge space}}

A natural question to ask about representations of $\widehat{\mathfrak{g}}$ is
when and how they can be extended to representations of
$\widetilde{\mathfrak{g}}$. Here is an answer for $\mathfrak{g}%
=\widehat{\mathfrak{gl}_{n}}$ and the representation $\mathcal{F}^{\left(
m\right)  }$:

\begin{proposition}
\label{prop.glwave.F}Let $m\in\mathbb{Z}$. Let $\psi_{m}$ be the element
$v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\in\mathcal{F}^{\left(  m\right)
}$.

There exists a unique extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{F}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$. The action of $d$ in
this extension is given by%
\[
d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(
\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{proposition}

Note that the infinite sum $\sum\limits_{k\geq0}\left(  \left\lceil
\dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)
$ in Proposition \ref{prop.glwave.F} is well-defined\footnote{In fact,
$\left(  i_{0},i_{1},i_{2},...\right)  $ is an $m$-degression. Hence, every
sufficiently high $k\geq0$ satisfies $i_{k}+k=m$ and thus $m-k=i_{k}$ and thus
$\left\lceil \dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}%
{n}\right\rceil =0$. Thus, all but finitely many addends of the infinite sum
$\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  $ are zero, so that this
sum is well-defined, qed.}.

\textit{Proof of Proposition \ref{prop.glwave.F}.} \textit{Uniqueness:} Let us
prove that there exists \textbf{at most one} extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

By Proposition \ref{prop.glnhat.T} \textbf{(b)}, the $\mathcal{A}$-module
$\mathcal{F}^{\left(  m\right)  }$ is a restriction of the
$\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$. As a
consequence, $\mathcal{F}^{\left(  m\right)  }$ is generated by $\psi_{m}$ as
a $\widehat{\mathfrak{gl}_{n}}$-module (since $\mathcal{F}^{\left(  m\right)
}$ is generated by $\psi_{m}$ as an $\mathcal{A}$-module). Hence, by Lemma
\ref{lem.gwave.uniqueder} (applied to $\mathfrak{g}=\mathfrak{gl}_{n}$,
$M=\mathcal{F}^{\left(  m\right)  }$ and $v=\psi_{m}$), there exists
\textbf{at most one} extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{F}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

\textit{Existence:} Let us now show that there exists an extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

In fact, let us construct this extension. In order to do so, it is clearly
enough to define the action of $d$ on $\mathcal{F}^{\left(  m\right)  }$
(because an action of $\widehat{\mathfrak{gl}_{n}}$ on $\mathcal{F}^{\left(
m\right)  }$ is already defined), and then show that every $A\in
\widehat{\mathfrak{gl}_{n}}$ satisfies%
\begin{equation}
\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }},A\mid_{\mathcal{F}^{\left(
m\right)  }}\right]  =\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}%
\mid_{\mathcal{F}^{\left(  m\right)  }}. \label{pf.glwave.F.1}%
\end{equation}
\footnote{Here, for every $\xi\in\widetilde{\mathfrak{gl}_{n}}$, we denote by
$\xi\mid_{\mathcal{F}^{\left(  m\right)  }}$ the action of $\xi$ on
$\mathcal{F}^{\left(  m\right)  }$. Besides, $\left[  d,A\right]
_{\widetilde{\mathfrak{gl}_{n}}}$ means the Lie bracket of $d$ and $A$ in the
Lie algebra $\widetilde{\mathfrak{gl}_{n}}$.}

Let us define the action of $d$ on $\mathcal{F}^{\left(  m\right)  }$ by
stipulating that
\begin{equation}
d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(
\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge... \label{pf.glwave.F.2}%
\end{equation}
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. (This is
extended by linearity to the whole of $\mathcal{F}^{\left(  m\right)  }$,
since $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
_{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ is
a basis of $\mathcal{F}^{\left(  m\right)  }$.)

It is rather clear that (\ref{pf.glwave.F.2}) holds not only for every
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, but also for every
straying $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $%
.\ \ \ \ \footnote{In fact, if $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
straying $m$-degression with no two equal elements, and $\pi$ is its
straightening permutation, then $\sum\limits_{k\geq0}\left(  \left\lceil
\dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)
=\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{\pi^{-1}\left(  k\right)  }}{n}\right\rceil \right)  $,
and this readily yields (\ref{pf.glwave.F.2}). If $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $m$-degression with two equal elements,
then (\ref{pf.glwave.F.2}) is even more obvious (since both sides of
(\ref{pf.glwave.F.2}) are zero in this case).} Renaming $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ as $\left(  j_{0},j_{1},j_{2},...\right)  $ and
renaming the summation index $k$ as $p$, we can rewrite this as follows: We
have%
\begin{equation}
d\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  =\left(
\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  \right)  \cdot v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge... \label{pf.glwave.F.2stray}%
\end{equation}
for every straying $m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $.

We now need to prove that every $A\in\widehat{\mathfrak{gl}_{n}}$ satisfies
(\ref{pf.glwave.F.1}). Since this equation (\ref{pf.glwave.F.1}) is linear in
$A$, we need to check it only in the case when $A=K$ and in the case when
$A=at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and some $\ell\in\mathbb{Z}$
(because the vector space $\widehat{\mathfrak{gl}_{n}}$ is generated by $K$
and all elements of the form $at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and
some $\ell\in\mathbb{Z}$). But checking the equation (\ref{pf.glwave.F.1}) in
the case when $A=K$ is trivial\footnote{In fact, $K\mid_{\mathcal{F}^{\left(
m\right)  }}=\operatorname*{id}$, so that $\left[  d\mid_{\mathcal{F}^{\left(
m\right)  }},K\mid_{\mathcal{F}^{\left(  m\right)  }}\right]  =\left[
d\mid_{\mathcal{F}^{\left(  m\right)  }},\operatorname*{id}\right]  =0$, and
by the definition of a semidirect product of Lie algebras we have $\left[
d,K\right]  _{\widetilde{\mathfrak{gl}_{n}}}=d\left(  K\right)  =0$, so that
both sides of (\ref{pf.glwave.F.1}) are zero in the case $A=K$, so that
(\ref{pf.glwave.F.1}) trivially holds in the case when $A=K$.}. Hence, it only
remains to check the equation (\ref{pf.glwave.F.1}) in the case when
$A=at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and some $\ell\in\mathbb{Z}$.

So let $a\in\mathfrak{gl}_{n}$ and $\ell\in\mathbb{Z}$ be arbitrary. We can
WLOG assume that if $\ell=0$, then the diagonal entries of the matrix $a$ are
zero\footnote{Here is why this assumption is allowed:
\par
We must prove that every $a\in\mathfrak{gl}_{n}$ and $\ell\in\mathbb{Z}$
satisfy the equation (\ref{pf.glwave.F.1}) for $A=at^{\ell}$. In other words,
we must prove that every $a\in\mathfrak{gl}_{n}$ and $\ell\in\mathbb{Z}$
satisfy $\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }},\left(  at^{\ell
}\right)  \mid_{\mathcal{F}^{\left(  m\right)  }}\right]  =\left[  d,\left(
at^{\ell}\right)  \right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}%
^{\left(  m\right)  }}$. If $\ell\neq0$, then our assumption (that if $\ell
=0$, then the diagonal entries of the matrix $a$ are zero) is clearly allowed
(because it only makes a statement about the case $\ell=0$). So we only need
to consider the case $\ell=0$. In this case, the equation which we must prove
(this is the equation $\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }%
},\left(  at^{\ell}\right)  \mid_{\mathcal{F}^{\left(  m\right)  }}\right]
=\left[  d,\left(  at^{\ell}\right)  \right]  _{\widetilde{\mathfrak{gl}_{n}}%
}\mid_{\mathcal{F}^{\left(  m\right)  }}$) simplifies to $\left[
d\mid_{\mathcal{F}^{\left(  m\right)  }},a\mid_{\mathcal{F}^{\left(  m\right)
}}\right]  =\left[  d,a\right]  _{\widetilde{\mathfrak{gl}_{n}}}%
\mid_{\mathcal{F}^{\left(  m\right)  }}$. This equation is clearly linear in
$a$. Hence, we can WLOG\ assume that either the matrix $a$ is diagonal, or all
diagonal entries of the matrix $a$ are zero (because every $n\times n$ matrix
can be decomposed as a sum of a diagonal matrix with a matrix all of whose
diagonal entries are zero). But in the case when the matrix $a$ is diagonal,
the equation $\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }},a\mid
_{\mathcal{F}^{\left(  m\right)  }}\right]  =\left[  d,a\right]
_{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(  m\right)  }}$ is
very easy to check (the details of this are left to the reader). Hence, it is
enough to only consider the case when the diagonal entries of the matrix $a$
are $0$. Of course, our assumption is justified in this case.
\par
Thus, we are allowed to make the assumption that if $\ell=0$, then the
diagonal entries of the matrix $a$ are zero.}. Let us assume this. (The
purpose of this assumption is to ensure that we can apply Proposition
\ref{prop.glinf.ainfact} to $at^{\ell}$ in lieu of $a$.)

Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an $m$-degression.

We recall that, when we embedded $L\mathfrak{gl}_{n}$ into $\overline
{\mathfrak{a}_{\infty}}$, we identified the element $at^{\ell}\in
L\mathfrak{gl}_{n}$ with the matrix $\operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  $ whose $\left(  ni+\alpha,nj+\beta\right)  $-th entry
equals%
\[
\left\{
\begin{array}
[c]{l}%
\text{the }\left(  \alpha,\beta\right)  \text{-th entry of }%
a,\ \ \ \ \ \ \ \ \ \ \text{if }j-i=\ell;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }j-i\neq\ell
\end{array}
\right.
\]
for all $i\in\mathbb{Z}$, $j\in\mathbb{Z}$, $\alpha\in\left\{
1,2,...,n\right\}  $ and $\beta\in\left\{  1,2,...,n\right\}  $. Hence, for
every $j\in\mathbb{Z}$ and $\beta\in\left\{  1,2,...,n\right\}  $, we have%
\begin{align}
&  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{nj+\beta}\nonumber\\
&  =\sum\limits_{i\in\mathbb{Z}}\sum\limits_{\alpha\in\left\{
1,2,...,n\right\}  }\left\{
\begin{array}
[c]{l}%
\text{the }\left(  \alpha,\beta\right)  \text{-th entry of }%
a,\ \ \ \ \ \ \ \ \ \ \text{if }j-i=\ell;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }j-i\neq\ell
\end{array}
\right.  v_{ni+\alpha}\nonumber\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\underbrace{\sum
\limits_{i\in\mathbb{Z}}\left\{
\begin{array}
[c]{l}%
\text{the }\left(  \alpha,\beta\right)  \text{-th entry of }%
a,\ \ \ \ \ \ \ \ \ \ \text{if }j-i=\ell;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }j-i\neq\ell
\end{array}
\right.  v_{ni+\alpha}}_{\substack{=\left(  \text{the }\left(  \alpha
,\beta\right)  \text{-th entry of }a\right)  v_{n\left(  j-\ell\right)
+\alpha}\\\text{(since there is precisely one }i\in\mathbb{Z}\text{ satisfying
}j-i=\ell\text{, namely }i=j-\ell\text{)}}}\nonumber\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right)  v_{n\left(
j-\ell\right)  +\alpha}. \label{pf.glwave.F.3}%
\end{align}


Note that the matrix $\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  $ has the property that, for every integer $i\leq0$, the $\left(
i,i\right)  $-th entry of $\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  $ is $0$. (This is due to our assumption that if $\ell=0$, then the
diagonal entries of the matrix $a$ are zero.) As a consequence, we can apply
Proposition \ref{prop.glinf.ainfact} to $\operatorname*{Toep}\nolimits_{n}%
\left(  at^{\ell}\right)  $ and $v_{i_{k}}$ instead of $a$ and $b_{k}$, and
obtain%
\begin{align}
&  \left(  \widehat{\rho}\left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.glwave.F.4}%
\end{align}


Now, we can check that, for every $k\geq0$, we have%
\begin{align}
&  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\right) \nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge.... \label{pf.glwave.F.4a}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.glwave.F.4a}):} Let $k\geq0$. Write the
integer $i_{k}$ in the form $nj+\beta$ for some $j\in\mathbb{Z}$ and $\beta
\in\left\{  1,2,...,n\right\}  $. Then,
\[
\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}=\left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \rightharpoonup v_{nj+\beta}=\sum\limits_{\alpha
\in\left\{  1,2,...,n\right\}  }\left(  \text{the }\left(  \alpha
,\beta\right)  \text{-th entry of }a\right)  v_{n\left(  j-\ell\right)
+\alpha}%
\]
due to (\ref{pf.glwave.F.3}). Hence,%
\begin{align}
&  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
\underbrace{\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  }_{=\sum\limits_{\alpha
\in\left\{  1,2,...,n\right\}  }\left(  \text{the }\left(  \alpha
,\beta\right)  \text{-th entry of }a\right)  v_{n\left(  j-\ell\right)
+\alpha}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(  j-\ell\right)
+\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge.... \label{pf.glwave.F.5}%
\end{align}
\par
Now, fix $\alpha\in\left\{  1,2,...,n\right\}  $. Let $\left(  j_{0}%
,j_{1},j_{2},...\right)  $ be the straying $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...,i_{k-1},n\left(  j-\ell\right)  +\alpha,i_{k+1}%
,i_{k+2},...\right)  $. Then, $j_{p}=i_{p}$ for every $p\geq0$ satisfying
$p\neq k$.
\par
Comparing $\left\lceil \dfrac{i_{k}}{n}\right\rceil =j+1$ (since
$i_{k}=nj+\beta$ with $\beta\in\left\{  1,2,...,n\right\}  $) with
$\left\lceil \dfrac{j_{k}}{n}\right\rceil =j-\ell+1$ (since $j_{k}=n\left(
j-\ell\right)  +\alpha$ with $\alpha\in\left\{  1,2,...,n\right\}  $), we get
$\left\lceil \dfrac{j_{k}}{n}\right\rceil =\left\lceil \dfrac{i_{k}}%
{n}\right\rceil -\ell$.
\par
Since $\left\lceil \dfrac{j_{p}}{n}\right\rceil =\left\lceil \dfrac{i_{p}}%
{n}\right\rceil $ for every $p\geq0$ satisfying $p\neq k$ (because every
$p\geq0$ satisfying $p\neq k$ satisfies $j_{p}=i_{p}$), the two sums
$\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  $ and $\sum\limits_{p\geq
0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{i_{p}}%
{n}\right\rceil \right)  $ differ only in their $k$-th addends. Since the
$k$-th addends differ in $\ell$ (because $\left\lceil \dfrac{j_{k}}%
{n}\right\rceil =\left\lceil \dfrac{i_{k}}{n}\right\rceil -\ell$), this yields
$\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  =\sum\limits_{p\geq
0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{i_{p}}%
{n}\right\rceil \right)  +\ell$.
\par
But since $\left(  i_{0},i_{1},i_{2},...,i_{k-1},n\left(  j-\ell\right)
+\alpha,i_{k+1},i_{k+2},...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  $,
we have
\par%
\begin{align}
&  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\right) \nonumber\\
&  =d\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
=\underbrace{\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}%
{n}\right\rceil -\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  \right)
}_{=\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell}\cdot
\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...}%
_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\\\text{(since }\left(  j_{0},j_{1},j_{2},...\right)  =\left(
i_{0},i_{1},i_{2},...,i_{k-1},n\left(  j-\ell\right)  +\alpha,i_{k+1}%
,i_{k+2},...\right)  \text{)}}}\nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \cdot
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(
j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glwave.F.6}%
\end{align}
Now forget that we fixed $\alpha$. Now, applying $d$ to the equality
(\ref{pf.glwave.F.5}), we get%
\begin{align*}
&  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\right) \\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{d\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(  j-\ell\right)  +\alpha}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right)  }_{\substack{=\left(
\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \cdot
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(
j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge
...\\\text{(by (\ref{pf.glwave.F.6}))}}}\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\left(  \sum\limits_{p\geq0}\left(  \left\lceil
\dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)
+\ell\right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\sum\limits_{\alpha\in\left\{
1,2,...,n\right\}  }\left(  \text{the }\left(  \alpha,\beta\right)  \text{-th
entry of }a\right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...}_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\\text{(by (\ref{pf.glwave.F.5}))}}}\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \cdot
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(
\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...,
\end{align*}
so that (\ref{pf.glwave.F.4a}) is proven.}

Since $A=at^{\ell}$, we have $A\mid_{\mathcal{F}^{\left(  m\right)  }}=\left(
at^{\ell}\right)  \mid_{\mathcal{F}^{\left(  m\right)  }}=\widehat{\rho
}\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)  $
(because the element $at^{\ell}\in L\mathfrak{gl}_{n}$ was identified with the
matrix $\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  $ and this
matrix acts on $\mathcal{F}^{\left(  m\right)  }$ via $\widehat{\rho}$). Thus,
we can rewrite (\ref{pf.glwave.F.4}) as%
\begin{align}
&  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.glwave.F.4b}%
\end{align}
Applying $d$ to this equality, we get%
\begin{align}
&  d\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \nonumber\\
&  =\sum\limits_{k\geq0}\underbrace{d\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}%
\nolimits_{n}\left(  at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right)  }%
_{\substack{=\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}%
{n}\right\rceil -\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)
+\ell\right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}%
\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\\text{(by (\ref{pf.glwave.F.4a}))}}}\nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\sum\limits_{k\geq0}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}%
\nolimits_{n}\left(  at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{\substack{=\left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \\\text{(by (\ref{pf.glwave.F.4b}%
))}}}\nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  \right)  \left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }%
}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.glwave.F.10}%
\end{align}
Since%
\begin{align*}
&  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\underbrace{\left(  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  }_{\substack{=\left(  \sum\limits_{p\geq0}\left(
\left\lceil \dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{j_{p}}%
{n}\right\rceil \right)  \right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\\\text{(by (\ref{pf.glwave.F.2stray}), applied to }\left(
j_{0},j_{1},j_{2},...\right)  =\left(  i_{0},i_{1},i_{2},...\right)  \text{)}%
}}\\
&  =\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  \left(
\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  \right)  \left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\end{align*}
and%
\begin{align*}
&  \left(  \left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid
_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left(  \ell A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since, by the definition of the Lie bracket on the semidirect product}\\
\widetilde{\mathfrak{gl}_{n}}=\mathbb{C}d\ltimes\widehat{\mathfrak{gl}_{n}%
}\text{, we have }\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}%
}=d\underbrace{\left(  A\right)  }_{=at^{\ell}}=d\left(  at^{\ell}\right)
=\ell\underbrace{at^{\ell}}_{=A}=\ell A
\end{array}
\right) \\
&  =\ell\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\end{align*}
we can rewrite (\ref{pf.glwave.F.10}) as%
\begin{align*}
&  d\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\underbrace{\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}%
{n}\right\rceil -\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  \right)
\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=\left(  A\mid
_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  d\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\ell\left(  A\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  }_{=\left(  \left[  d,A\right]  _{\widetilde{\mathfrak{gl}%
_{n}}}\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }\\
&  =\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  d\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  +\left(
\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  .
\end{align*}
In other words,%
\begin{align*}
&  \left(  \left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  +\left(  \left[
d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  +\left[
d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  .
\end{align*}
Since this holds for every $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $, this yields that $\left(  d\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \circ\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }%
}\right)  =\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  +\left[
d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}$ (because $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression}}$ is a basis of $\mathcal{F}^{\left(  m\right)  }$). In
other words,%
\[
\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}=\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  -\left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \circ\left(  d\mid
_{\mathcal{F}^{\left(  m\right)  }}\right)  =\left[  d\mid_{\mathcal{F}%
^{\left(  m\right)  }},A\mid_{\mathcal{F}^{\left(  m\right)  }}\right]  .
\]
In other words, (\ref{pf.glwave.F.1}) holds.

We have thus checked the equation (\ref{pf.glwave.F.1}) in the case when
$A=at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and some $\ell\in\mathbb{Z}$.
As explained above, this completes the proof of the equation
(\ref{pf.glwave.F.1}) for every $A\in\widehat{\mathfrak{gl}_{n}}$. Hence, we
have constructed an action of $d$ on $\mathcal{F}^{\left(  m\right)  }$. This
action clearly satisfies $d\psi_{m}=0$ (because $\psi_{m}=v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...$, so that%
\begin{align*}
d\psi_{m}  &  =d\left(  v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\right) \\
&  =\left(  \sum\limits_{k\geq0}\underbrace{\left(  \left\lceil \dfrac{m-k}%
{n}\right\rceil -\left\lceil \dfrac{m-k}{n}\right\rceil \right)  }%
_{=0}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glwave.F.2}), applied to
}i_{k}=m-k\right) \\
&  =0
\end{align*}
). Hence, we have proven the existence of an extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

Altogether, we have now proven both the uniqueness and the existence of an
extension of the $\widehat{\mathfrak{gl}_{n}}$-representation on
$\mathcal{F}^{\left(  m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such
that $d\psi_{m}=0$. Moreover, in the proof of the existence, we have showed
that the action of $d$ in this extension is given by%
\[
d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(
\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ (because we
defined this extension using (\ref{pf.glwave.F.2})). This completes the proof
of Proposition \ref{prop.glwave.F}.

Next, an irreducibility result:

\begin{proposition}
\label{prop.glwave.F.irr}Let $m\in\mathbb{Z}$. Let $\psi_{m}$ be the element
$v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\in\mathcal{F}^{\left(  m\right)
}$.

\textbf{(a)} The $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(
m\right)  }$ is irreducible.

\textbf{(b)} Let $\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}%
}:\widetilde{\mathfrak{gl}_{n}}\rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ denote the unique extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$. (This
is well-defined due to Proposition \ref{prop.glwave.F}.)

The $\widetilde{\mathfrak{gl}_{n}}$-module $\left(  \mathcal{F}^{\left(
m\right)  },\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}}\right)  $ is
irreducible with highest weight $\widetilde{\omega}_{m}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glwave.F.irr}.} \textbf{(a)} By
Proposition \ref{prop.F.irrep}, we know that $F$ is an irreducible
$\mathcal{A}_{0}$-module. In other words, $\mathcal{B}^{\left(  m\right)  }$
is an irreducible $\mathcal{A}_{0}$-module (since $\mathcal{B}^{\left(
m\right)  }=F_{m}=F$ as $\mathcal{A}_{0}$-modules). Hence, $\mathcal{B}%
^{\left(  m\right)  }$ is also an irreducible $\mathcal{A}$-module (since the
$\mathcal{A}_{0}$-module $\mathcal{B}^{\left(  m\right)  }$ is a restriction
of the $\mathcal{A}$-module $\mathcal{B}^{\left(  m\right)  }$).

Since the Boson-Fermion correspondence $\sigma_{m}:\mathcal{B}^{\left(
m\right)  }\rightarrow\mathcal{F}^{\left(  m\right)  }$ is an $\mathcal{A}%
$-module isomorphism, we have $\mathcal{B}^{\left(  m\right)  }\cong%
\mathcal{F}^{\left(  m\right)  }$ as $\mathcal{A}$-modules. Since
$\mathcal{B}^{\left(  m\right)  }$ is an irreducible $\mathcal{A}$-module,
this yields that $\mathcal{F}^{\left(  m\right)  }$ is an irreducible
$\mathcal{A}$-module.

By Proposition \ref{prop.glnhat.T} \textbf{(b)}, the $\mathcal{A}$-module
$\mathcal{F}^{\left(  m\right)  }$ is a restriction of the
$\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$. Since
the $\mathcal{A}$-module $\mathcal{F}^{\left(  m\right)  }$ is irreducible,
this yields that the $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}%
^{\left(  m\right)  }$ is irreducible. Proposition \ref{prop.glwave.F.irr}
\textbf{(a)} is proven.

\textbf{(b)} It is easy to check that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$
and $x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)  \psi_{m}$ for every
$x\in\widetilde{\mathfrak{h}}$.

\textit{Proof.} Proving that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$ is easy,
since $\widetilde{\mathfrak{n}_{+}}$ embeds into $\mathfrak{a}_{\infty}$ as
strictly upper-triangular matrices (and $\mathcal{F}^{\left(  m\right)  }$ is
a graded $\mathfrak{a}_{\infty}$-module).

In order to prove that $x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)
\psi_{m}$ for every $x\in\widetilde{\mathfrak{h}}$, we must show that
$E_{i,i}^{\mathfrak{gl}_{n}}\psi_{m}=\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  \psi_{m}$ for every $i\in\left\{
1,2,...,n\right\}  $. (In fact, this is enough, because the relations
$K\psi_{m}=\widetilde{\omega}_{m}\left(  K\right)  \psi_{m}$ and $d\psi
_{m}=\widetilde{\omega}_{m}\left(  d\right)  \psi_{m}$ follow directly from
$\widehat{\rho}\left(  K\right)  =\operatorname*{id}$ and $d\psi_{m}=0$.)

Let $i\in\left\{  1,2,...,n\right\}  $. Use $\operatorname*{Toep}%
\nolimits_{n}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right)  =\sum\limits_{j\equiv
i\operatorname{mod}n}E_{j,j}^{\mathfrak{gl}_{\infty}}$ to conclude that
\[
\widehat{\rho}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right)  \psi_{m}%
=\underbrace{\left(  \left\{
\begin{array}
[c]{c}%
1,\text{ if }i\leq\overline{m};\\
0,\text{ if }i>\overline{m}%
\end{array}
\right.  +\dfrac{m-\overline{m}}{n}\right)  }_{=\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  }\psi_{m}=\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  \psi_{m},
\]
where $\overline{m}$ is the element of $\left\{  0,1,...,n-1\right\}  $
satisfying $m\equiv\overline{m}\operatorname{mod}n$.

Thus, we have checked that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$ and
$x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)  \psi_{m}$ for every
$x\in\widetilde{\mathfrak{h}}$. Thus, $\psi_{m}$ is a singular vector of
weight $\widetilde{\omega}_{m}$. In other words, $\psi_{m}\in
\operatorname*{Sing}\nolimits_{\widetilde{\omega}_{m}}\left(  \mathcal{F}%
^{\left(  m\right)  }\right)  $. By Lemma \ref{lem.singvec}, we thus have a
canonical isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\widetilde{\mathfrak{gl}_{n}}}\left(
M_{\widetilde{\omega}_{m}}^{+},\mathcal{F}^{\left(  m\right)  }\right)   &
\rightarrow\operatorname*{Sing}\nolimits_{\widetilde{\omega}_{m}}\left(
\mathcal{F}^{\left(  m\right)  }\right)  ,\\
\phi &  \mapsto\phi\left(  v_{\widetilde{\omega}_{m}}^{+}\right)  .
\end{align*}
Thus, since $\psi_{m}\in\operatorname*{Sing}\nolimits_{\widetilde{\omega}_{m}%
}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $, there exists a
$\widetilde{\mathfrak{gl}_{n}}$-module homomorphism $\phi:M_{\widetilde{\omega
}_{m}}^{+}\rightarrow\mathcal{F}^{\left(  m\right)  }$ such that $\phi\left(
v_{\widetilde{\omega}_{m}}^{+}\right)  =\psi_{m}$. Consider this $\phi$.

Since $\mathcal{F}^{\left(  m\right)  }$ is generated by $\psi_{m}$ as a
$\widehat{\mathfrak{gl}_{n}}$-module (this was proven in the proof of
Proposition \ref{prop.glwave.F}), it is clear that $\mathcal{F}^{\left(
m\right)  }$ is generated by $\psi_{m}$ as a $\widetilde{\mathfrak{gl}_{n}}%
$-module as well. Thus, $\phi$ must be surjective (because $\psi_{m}%
=\phi\left(  v_{\widetilde{\omega}_{m}}^{+}\right)  \in\phi\left(
M_{\widetilde{\omega}_{m}}^{+}\right)  $). Hence, $\mathcal{F}^{\left(
m\right)  }$ is (isomorphic to) a quotient of the $\widetilde{\mathfrak{gl}%
_{n}}$-module $M_{\widetilde{\omega}_{m}}^{+}$. In other words, $\mathcal{F}%
^{\left(  m\right)  }$ is a highest-weight module with highest weight
$\widetilde{\omega}_{m}$. Combined with the irreducibility of $\mathcal{F}%
^{\left(  m\right)  }$, this proves Proposition \ref{prop.glwave.F.irr}.

\subsubsection{The
\texorpdfstring{$\protect\widetilde{\mathfrak{gl}_{n}}$}{gl-n-tilde}-module
\texorpdfstring{$\mathcal{B}^{\left(  m\right)  }$}{structure on the bosonic
Fock space}}

By applying the Boson-Fermion correspondence $\sigma$ to Proposition
\ref{prop.glwave.F}, we obtain:

\begin{proposition}
\label{prop.glwave.B}Let $m\in\mathbb{Z}$. Let $\psi_{m}^{\prime}$ be the
element $\sigma^{-1}\left(  v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\right)
\in\mathcal{B}^{\left(  m\right)  }$ (the highest-weight vector of
$\mathcal{B}^{\left(  m\right)  }$).

There exists a unique extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{B}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}^{\prime}=0$. The action
of $d$ in this extension is given by%
\[
d\left(  \sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  =\left(  \sum\limits_{k\geq0}\left(  \left\lceil
\dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)
\right)  \cdot\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{proposition}

By applying the Boson-Fermion correspondence $\sigma$ to Proposition
\ref{prop.glwave.F.irr}, we obtain:

\begin{proposition}
Let $m\in\mathbb{Z}$. Let $\psi_{m}^{\prime}$ be the element $\sigma
^{-1}\left(  v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\right)  \in
\mathcal{B}^{\left(  m\right)  }$ (the highest-weight vector of $\mathcal{B}%
^{\left(  m\right)  }$).

\textbf{(a)} The $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{B}^{\left(
m\right)  }$ is irreducible.

\textbf{(b)} Let $\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}%
}:\widetilde{\mathfrak{gl}_{n}}\rightarrow\operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  $ denote the unique extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{B}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}^{\prime
}=0$. (This is well-defined due to Proposition \ref{prop.glwave.B}.)

The $\widetilde{\mathfrak{gl}_{n}}$-module $\left(  \mathcal{B}^{\left(
m\right)  },\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}}\right)  $ is
irreducible with highest weight $\widetilde{\omega}_{m}$.
\end{proposition}

\subsubsection{\texorpdfstring{$\protect\widetilde{\mathfrak{sl}_{n}}$}
{sl-n-tilde} and its action on
\texorpdfstring{$\mathcal{B}^{\left(  m\right)  }$}{the bosonic
Fock space}}

We have $\left[  I_{n}t,\widehat{\mathfrak{sl}_{n}}\right]  =0$ in the Lie
algebra $\widehat{\mathfrak{gl}_{n}}$ (this is because $\left[  I_{n}%
t,L\mathfrak{sl}_{n}\right]  =0$ in the Lie algebra $L\mathfrak{gl}_{n}$, and
because $\omega\left(  I_{n}t,L\mathfrak{sl}_{n}\right)  =0$ where the
$2$-cocycle $\omega$ is the one defined in Proposition
\ref{prop.ainf.alphaomega}). Since $I_{n}t\in\widehat{\mathfrak{gl}_{n}}$ acts
on $\mathcal{F}$ by the operator $\widehat{\operatorname*{Toep}\nolimits_{n}%
}\left(  I_{n}t\right)  =T^{n}$ (more precisely, by the action of $T^{n}$ on
$\mathcal{F}$, but let us abbreviate this by $T^{n}$ here), this yields that
the action of $T^{n}$ on $\mathcal{F}$ is an $\widehat{\mathfrak{sl}_{n}}%
$-module homomorphism. Thus, the action of $T^{n}$ on $\mathcal{B}$ also is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism. As a consequence, the
restriction to $\widehat{\mathfrak{sl}_{n}}$ of the representation
$\mathcal{B}^{\left(  m\right)  }$ is not irreducible.

But $\psi_{m}^{\prime}$ is still a highest-weight vector with highest weight
$\widetilde{\omega}_{m}$. Let us look at how this representation
$\mathcal{B}^{\left(  m\right)  }$ decomposes.

\begin{definition}
Let $h_{i}=E_{i,i}^{\mathfrak{gl}_{n}}-E_{i+1,i+1}^{\mathfrak{gl}_{n}}$ for
$i\in\left\{  1,2,...,n-1\right\}  $, and let $h_{0}=K-h_{1}-h_{2}%
-...-h_{n-1}$. Then, $\left(  h_{0},h_{1},...,h_{n-1},d\right)  $ is a basis
of $\widetilde{\mathfrak{h}}\cap\widetilde{\mathfrak{sl}_{n}}$ (which is the
$0$-th homogeneous component of $\widetilde{\mathfrak{sl}_{n}}$).
\end{definition}

\begin{definition}
For every $m\in\mathbb{Z}$, define the weight $\omega_{m}\in\left(
\widetilde{\mathfrak{h}}\cap\widetilde{\mathfrak{sl}_{n}}\right)  ^{\ast}$ to
be the restriction $\widetilde{\omega}_{m}\mid_{\widetilde{\mathfrak{h}}%
\cap\widetilde{\mathfrak{sl}_{n}}}$ of $\widetilde{\omega}_{m}$ to the $0$-th
homogeneous component of $\widetilde{\mathfrak{sl}_{n}}$.
\end{definition}

This weight $\omega_{m}$ does not depend on $m$ but only depends on the
residue class of $m$ modulo $n$. In fact, it satisfies%
\begin{align*}
\omega_{m}\left(  h_{i}\right)   &  =\widetilde{\omega}_{m}\left(
h_{i}\right)  =\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\equiv m\operatorname{mod}n;\\
0,\text{ if }i\not \equiv m\operatorname{mod}n
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  0,1,...,n-1\right\}
;\\
\omega_{m}\left(  d\right)   &  =\widetilde{\omega}_{m}\left(  d\right)  =0.
\end{align*}


\begin{definition}
Let $\mathcal{A}^{\left(  n\right)  }$ be the Lie subalgebra $\left\langle
K\right\rangle +\left\langle a_{ni}\ \mid\ i\in\mathbb{Z}\right\rangle $ of
$\mathcal{A}$.
\end{definition}

Note that the map%
\begin{align*}
\mathcal{A}  &  \rightarrow\mathcal{A}^{\left(  n\right)  },\\
a_{i}  &  \mapsto a_{ni}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{Z},\\
K  &  \mapsto nK
\end{align*}
is a Lie algebra isomorphism. But we still consider $\mathcal{A}^{\left(
n\right)  }$ as a Lie subalgebra of $\mathcal{A}$, and we won't identify it
with $\mathcal{A}$ via this isomorphism.

Since $\mathcal{A}^{\left(  n\right)  }$ is a Lie subalgebra of $\mathcal{A}$,
both $\mathcal{A}$-modules $\mathcal{F}$ and $\mathcal{B}$ become
$\mathcal{A}^{\left(  n\right)  }$-modules.

Let us consider the direct sum $\widehat{\mathfrak{sl}_{n}}\oplus
\mathcal{A}^{\left(  n\right)  }$ of Lie algebras. Let us denote by $K_{1}$
the element $\left(  K,0\right)  $ of $\widehat{\mathfrak{sl}_{n}}%
\oplus\mathcal{A}^{\left(  n\right)  }$ (where the $K$ means the element $K$
of $\widehat{\mathfrak{sl}_{n}}$), and let us denote by $K_{2}$ the element
$\left(  0,K\right)  $ of $\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}%
^{\left(  n\right)  }$ (where the $K$ means the element $K$ of $\mathcal{A}%
^{\left(  n\right)  }$). Note that both elements $K_{1}=\left(  K,0\right)  $
and $K_{2}=\left(  0,K\right)  $ lie in the center of $\widehat{\mathfrak{sl}%
_{n}}\oplus\mathcal{A}^{\left(  n\right)  }$; hence, so does their difference
$K_{1}-K_{2}=\left(  K,-K\right)  $. Thus, $\left\langle K_{1}-K_{2}%
\right\rangle $ (the $\mathbb{C}$-linear span of the set $\left\{  K_{1}%
-K_{2}\right\}  $) is an ideal of $\widehat{\mathfrak{sl}_{n}}\oplus
\mathcal{A}^{\left(  n\right)  }$. Thus, $\left(  \widehat{\mathfrak{sl}_{n}%
}\oplus\mathcal{A}^{\left(  n\right)  }\right)  \diagup\left(  K_{1}%
-K_{2}\right)  $ is a Lie algebra.

\begin{proposition}
The Lie algebras $\widehat{\mathfrak{gl}_{n}}$ and $\left(
\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)  }\right)
\diagup\left(  K_{1}-K_{2}\right)  $ are isomorphic. More precisely, the maps%
\begin{align*}
\left(  \widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)
}\right)  \diagup\left(  K_{1}-K_{2}\right)   &  \rightarrow
\widehat{\mathfrak{gl}_{n}},\\
\overline{\left(  At^{\ell},0\right)  }  &  \mapsto At^{\ell}%
\ \ \ \ \ \ \ \ \ \ \text{for every }A\in\mathfrak{sl}_{n}\text{ and }\ell
\in\mathbb{Z},\\
\overline{\left(  0,a_{n\ell}\right)  }  &  \mapsto\operatorname*{id}%
\nolimits_{n}t^{\ell}\ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{Z},\\
\overline{K_{1}}=\overline{K_{2}}  &  \mapsto K
\end{align*}
and%
\begin{align*}
\widehat{\mathfrak{gl}_{n}}  &  \rightarrow\left(  \widehat{\mathfrak{sl}_{n}%
}\oplus\mathcal{A}^{\left(  n\right)  }\right)  \diagup\left(  K_{1}%
-K_{2}\right)  ,\\
At^{\ell}  &  \mapsto\overline{\left(  \left(  A-\dfrac{1}{n}\left(
\operatorname*{Tr}A\right)  \cdot\operatorname*{id}\nolimits_{n}\right)
t^{\ell},\left(  \dfrac{1}{n}\operatorname*{Tr}A\right)  a_{n\ell}\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }A\in\mathfrak{gl}_{n}\text{ and }\ell
\in\mathbb{Z},\\
K  &  \mapsto\overline{K_{1}}=\overline{K_{2}}.
\end{align*}
are mutually inverse isomorphisms of Lie algebras.
\end{proposition}

The proof of this proposition is left to the reader (it is completely
straightforward). This isomorphism $\widehat{\mathfrak{gl}_{n}}\cong\left(
\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)  }\right)
\diagup\left(  K_{1}-K_{2}\right)  $ allows us to consider any
$\widehat{\mathfrak{gl}_{n}}$-module as an $\left(  \widehat{\mathfrak{sl}%
_{n}}\oplus\mathcal{A}^{\left(  n\right)  }\right)  \diagup\left(  K_{1}%
-K_{2}\right)  $-module, i. e., as an $\widehat{\mathfrak{sl}_{n}}%
\oplus\mathcal{A}^{\left(  n\right)  }$-module on which $K_{1}$ and $K_{2}$
act the same way. In particular, $\mathcal{F}$ and $\mathcal{B}$ become
$\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)  }$-modules.
Of course, the actions of the two addends $\widehat{\mathfrak{sl}_{n}}$ and
$\mathcal{A}^{\left(  n\right)  }$ on $\mathcal{F}$ and $\mathcal{B}$ are
exactly the actions of $\widehat{\mathfrak{sl}_{n}}$ and $\mathcal{A}^{\left(
n\right)  }$ on $\mathcal{F}$ and $\mathcal{B}$ that result from the canonical
inclusions $\widehat{\mathfrak{sl}_{n}}\subseteq\widehat{\mathfrak{gl}_{n}%
}\subseteq\mathfrak{a}_{\infty}$ and $\mathcal{A}^{\left(  n\right)
}\subseteq\mathcal{A}\cong\widehat{\mathfrak{gl}_{1}}\subseteq\mathfrak{a}%
_{\infty}$. (This is clear for the action of $\widehat{\mathfrak{sl}_{n}}$,
and is very easy to see for the action of $\mathcal{A}^{\left(  n\right)  }$.)

We checked above that the action of $T^{n}$ on $\mathcal{B}$ is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism. This easily generalizes:
For every integer $i$, the action of $T^{ni}$ on $\mathcal{B}$ is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism.\footnote{\textit{Proof.}
Let $i$ be an integer. We have $\left[  I_{n}t^{i},\widehat{\mathfrak{sl}_{n}%
}\right]  =0$ in the Lie algebra $\widehat{\mathfrak{gl}_{n}}$ (this is
because $\left[  I_{n}t^{i},L\mathfrak{sl}_{n}\right]  =0$ in the Lie algebra
$L\mathfrak{gl}_{n}$, and because $\omega\left(  I_{n}t^{i},L\mathfrak{sl}%
_{n}\right)  =0$ where the $2$-cocycle $\omega$ is the one defined in
Proposition \ref{prop.ainf.alphaomega}). Since $I_{n}t^{i}\in
\widehat{\mathfrak{gl}_{n}}$ acts on $\mathcal{F}$ by the operator
$\widehat{\operatorname*{Toep}\nolimits_{n}}\left(  I_{n}t^{i}\right)
=T^{ni}$ (more precisely, by the action of $T^{ni}$ on $\mathcal{F}$, but let
us abbreviate this by $T^{ni}$ here), this yields that the action of $T^{ni}$
on $\mathcal{F}$ is an $\widehat{\mathfrak{sl}_{n}}$-module homomorphism.
Thus, the action of $T^{ni}$ on $\mathcal{B}$ also is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism.} Thus, the subspace
$\mathcal{B}_{0}^{\left(  m\right)  }=\left\{  v\in\mathcal{B}^{\left(
m\right)  }\ \mid\ T^{ni}v=0\text{ for all }i>0\right\}  $ of $\mathcal{B}%
^{\left(  m\right)  }$ is an $\widehat{\mathfrak{sl}_{n}}$-submodule.
Recalling that $\mathcal{B}^{\left(  m\right)  }=\mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  $, with $T^{ni}$ acting as $ni\dfrac{\partial
}{\partial x_{ni}}$, we have $\mathcal{B}_{0}^{\left(  m\right)  }%
\cong\mathbb{C}\left[  x_{j}\ \mid\ n\nmid j\right]  $.

\begin{theorem}
\label{thm.B0m}This $\mathcal{B}_{0}^{\left(  m\right)  }$ is an irreducible
$\widehat{\mathfrak{sl}_{n}}$-module (or $\widetilde{\mathfrak{sl}_{n}}%
$-module; this doesn't matter) with highest weight $\omega_{\overline{m}}$
(this means that $\mathcal{B}_{0}^{\left(  m\right)  }\cong L_{\omega
_{\overline{m}}}$) and depends only on $\overline{m}$ (the remainder of $m$
modulo $n$) rather than on $m$. Moreover, $\mathcal{B}^{\left(  m\right)
}\cong\mathcal{B}_{0}^{\left(  m\right)  }\otimes\widetilde{F}_{m}$, where
$\widetilde{F}_{m}$ is the appropriate Fock module over $\mathcal{A}^{\left(
n\right)  }$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.B0m}.} We clearly have such a decomposition
as vector spaces, $\widetilde{F}_{m}=\mathbb{C}\left[  x_{n},x_{2n}%
,x_{3n},...\right]  $. Each of the two Lie algebras acts in its own factor:
$\mathcal{A}^{\left(  n\right)  }$ acts in $\widetilde{F}_{m}$, and
$\widehat{\mathfrak{gl}_{n}}$ commutes with $\mathcal{A}^{\left(  n\right)  }%
$. Since the tensor product is irreducible, each factor is irreducible, so
that $\mathcal{B}_{0}^{\left(  m\right)  }$ is irreducible.

\subsubsection{\textbf{[unfinished]} Classification of unitary highest-weight
\texorpdfstring{$\protect\widehat{\mathfrak{sl}_{n}}$}{sl-n-hat}-modules}

We can now classify unitary highest-weight representations of
$\widehat{\mathfrak{sl}_{n}}$:

\begin{proposition}
The highest-weight representation $L_{\omega_{m}}$ is unitary for each
$m\in\left\{  0,1,...,n-1\right\}  $.
\end{proposition}

\textit{Proof.} The contravariant Hermitian form on $L_{\omega_{m}}$ is the
restriction of the form on $\mathcal{B}^{\left(  m\right)  }$.

\begin{corollary}
If $k_{0},k_{1},...,k_{n-1}$ are nonnegative integers, then $L_{k_{0}%
\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}}$ is unitary (of level
$k_{0}+k_{1}+...+k_{n-1}$).
\end{corollary}

\textit{Proof.} The tensor product $L_{\omega_{0}}^{\otimes k_{0}}\otimes
L_{\omega_{1}}^{\otimes k_{1}}\otimes...\otimes L_{\omega_{n-1}}^{\otimes
k_{n-1}}$ is unitary (being a tensor product of unitary representations), and
thus is a direct sum of irreducible representations. Clearly, $L_{k_{0}%
\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}}$ is a summand of this
module, and thus also unitary, qed.

\begin{theorem}
\label{thm.sln.unitaries}These $L_{k_{0}\omega_{0}+k_{1}\omega_{1}%
+...+k_{n-1}\omega_{n-1}}$ (with $k_{0},k_{1},...,k_{n-1}$ being nonnegative
integers) are the only unitary highest-weight representations of
$\widehat{\mathfrak{sl}_{n}}$.
\end{theorem}

To prove this, first a lemma:

\begin{lemma}
\label{lem.sl2.unitaries}Consider the antilinear $\mathbb{R}$-antiinvolution
$\dag:\mathfrak{sl}_{2}\rightarrow\mathfrak{sl}_{2}$ defined by $e^{\dag}=f$,
$f^{\dag}=e$ and $h^{\dag}=h$. Let $\lambda\in\mathfrak{h}^{\ast}$. We
identify the function $\lambda\in\mathfrak{h}^{\ast}$ with the value
$\lambda\left(  h\right)  \in\mathbb{C}$. Then, $L_{\lambda}$ is a unitary
representation of $\mathfrak{sl}_{2}$ if and only if $\lambda\in\mathbb{Z}%
_{+}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sl2.unitaries}.} Assume that $L_{\lambda}$ is
a unitary representation of $\mathfrak{sl}_{2}$. Let $v_{\lambda}=v_{\lambda
}^{+}$. Since $L_{\lambda}$ is unitary, the form $\left(  \cdot,\cdot\right)
$ is positive definite, so that $\left(  v_{\lambda},v_{\lambda}\right)  >0$.

Every $n\in\mathbb{N}$ satisfies
\[
\left(  f^{n}v_{\lambda},f^{n}v_{\lambda}\right)  =n!\overline{\lambda}\left(
\overline{\lambda}-1\right)  ...\left(  \overline{\lambda}-n+1\right)  \left(
v_{\lambda},v_{\lambda}\right)
\]
(the proof of this is analogous to the proof of (\ref{exa.sl2.bilinform}), but
uses $e^{\dag}=f$). Since $\left(  \cdot,\cdot\right)  $ is positive definite,
we must have $\left(  f^{n}v_{\lambda},f^{n}v_{\lambda}\right)  \geq0$ for
every $n\in\mathbb{N}$. Thus, every $n\in\mathbb{N}$ satisfies $n!\overline
{\lambda}\left(  \overline{\lambda}-1\right)  ...\left(  \overline{\lambda
}-n+1\right)  \left(  v_{\lambda},v_{\lambda}\right)  =\left(  f^{n}%
v_{\lambda},f^{n}v_{\lambda}\right)  \geq0$, so that $\overline{\lambda
}\left(  \overline{\lambda}-1\right)  ...\left(  \overline{\lambda
}-n+1\right)  \geq0$ (since $\left(  v_{\lambda},v_{\lambda}\right)  >0$).
Applied to $n=1$, this yields $\overline{\lambda}\geq0$, so that
$\overline{\lambda}\in\mathbb{R}$ and thus $\lambda\in\mathbb{R}$. Hence,
$\overline{\lambda}\geq0$ becomes $\lambda\geq0$.

Every $n\in\mathbb{N}$ satisfies $\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  =\overline{\lambda}\left(  \overline{\lambda}-1\right)
...\left(  \overline{\lambda}-n+1\right)  \geq0$. Thus, $\lambda\in
\mathbb{Z}_{+}$ (otherwise, $\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  $ would alternate in sign for each sufficiently large $n$).

This proves one direction of Lemma \ref{lem.sl2.unitaries}. The converse
direction is classical and easy. Lemma \ref{lem.sl2.unitaries} is proven.

\begin{corollary}
Let $\lambda\in\mathbb{C}$. If $\mathfrak{g}$ is a Lie algebra with antilinear
$\mathbb{R}$-antiinvolution $\dag$ and $\mathfrak{sl}_{2}$ is a Lie subalgebra
of $\mathfrak{g}$, and if $\dag\mid_{\mathfrak{sl}_{2}}$ sends $e,f,h$ to
$f,e,h$, and if $V$ is a unitary representation of $\mathfrak{g}$, and if some
$v\in V$ satisfies $ev=0$ and $hv=\lambda v$, then $\lambda\in\mathbb{Z}_{+}$.
\end{corollary}

\textit{Proof of Theorem \ref{thm.sln.unitaries}.} For every $i\in\left\{
0,1,...,n-1\right\}  $, we have an $\mathfrak{sl}_{2}$-subalgebra:%
\begin{align*}
h_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i,i}-E_{i+1,i+1},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
K+E_{n,n}-E_{1,1},\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.  ,\\
e_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i,i+1},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
E_{n,1}t,\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.  ;\\
f_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i+1,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
E_{1,n}t^{-1},\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.
\end{align*}
\footnote{Here, $E_{i,j}$ means $E_{i,j}^{\mathfrak{gl}_{n}}$.} (these form an
$\mathfrak{sl}_{2}$-triple, as can be easily checked). These satisfy
$e_{i}^{\dag}=f_{i}$, $f_{i}^{\dag}=e_{i}$ and $h_{i}^{\dag}=h_{i}$. Thus, if
$L_{\lambda}$ is a unitary representation of $\widehat{\mathfrak{sl}_{n}}$,
then $\lambda\left(  h_{i}\right)  \in\mathbb{Z}_{+}$. But $\omega_{i}$ are a
basis for the weights, and namely the dual basis to the basis of the $h_{i}$.
Thus, $\lambda=\sum\limits_{i=0}^{n-1}\lambda\left(  h_{i}\right)  \omega_{i}%
$. Hence, $\lambda=\sum\limits_{i=0}^{n-1}k_{i}\omega_{i}$ with $k_{i}%
\in\mathbb{Z}_{+}$. Qed.

\begin{remark}
Relation between $\widehat{\mathfrak{sl}_{n}}$-modules and $\mathfrak{sl}_{n}$-modules:

Let $L_{\lambda}$ be a unitary $\widehat{\mathfrak{sl}_{n}}$-module, with
$\lambda=k_{0}\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}$.

Then, $U\left(  \mathfrak{sl}_{n}\right)  v_{\lambda}=L_{\overline{\lambda}}$
where $\overline{\lambda}=k_{1}\omega_{1}+k_{2}\omega_{2}+...+k_{n-1}%
\omega_{n-1}$ is a weight for $\mathfrak{sl}_{n}$. And if the level of
$L_{\lambda}$ was $k$, then we must have $k_{1}+k_{2}+...+k_{n-1}\leq k$.
\end{remark}

\subsection{The Sugawara construction}

We will now study the Sugawara construction. It constructs a
$\operatorname*{Vir}$ action on a $\widehat{\mathfrak{g}}$-module (under some
conditions), and it generalizes the action of $\operatorname*{Vir}$ on the
$\mu$-Fock representation $F_{\mu}$ (that was constructed in Proposition
\ref{prop.fockvir.answer2}).

\begin{definition}
\label{def.sugawara}Let $\mathfrak{g}$ be a finite-dimensional $\mathbb{C}%
$-Lie algebra equipped with a $\mathfrak{g}$-invariant symmetric bilinear form
$\left(  \cdot,\cdot\right)  $. (This form needs not be nondegenerate; it is
even allowed to be $0$.)

Consider the $2$-cocycle $\omega:\mathfrak{g}\left[  t,t^{-1}\right]
\times\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$ defined by%
\[
\omega\left(  a,b\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
a^{\prime},b\right)  dt\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}%
\left[  t,t^{-1}\right]  \text{ and }b\in\mathfrak{g}\left[  t,t^{-1}\right]
.
\]
(This is the $2$-cocycle $\omega$ in Definition \ref{def.loop}. We just
slightly rewrote the definition.) Also consider the affine Lie algebra
$\widehat{\mathfrak{g}}=\mathfrak{g}\left[  t,t^{-1}\right]  \oplus
\mathbb{C}K$ defined through this cocycle $\omega$.

Let $\operatorname*{Kil}$ denote the Killing form on $\mathfrak{g}$, defined
by
\[
\operatorname*{Kil}\left(  a,b\right)  =\operatorname*{Tr}\left(
\operatorname*{ad}\left(  a\right)  \cdot\operatorname*{ad}\left(  b\right)
\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in\mathfrak{g}.
\]


An element $k\in\mathbb{C}$ is said to be \textit{non-critical for }$\left(
\mathfrak{g},\left(  \cdot,\cdot\right)  \right)  $ if and only if the form
$k\cdot\left(  \cdot,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$ is nondegenerate.
\end{definition}

\begin{definition}
\label{def.sugawara.M}Let $M$ be a $\widehat{\mathfrak{g}}$-module.

We say that $M$ is \textit{admissible} if for every $v\in M$, there exists
some $N\in\mathbb{N}$ such that every integer $n\geq N$ and every
$a\in\mathfrak{g}$ satisfy $at^{n}\cdot v=0$.

If $k\in\mathbb{C}$, then we say that $M$ is \textit{of level }$k$ if
$K\mid_{M}=k\cdot\operatorname*{id}$.
\end{definition}

\begin{proposition}
\label{prop.WtoDerg}Let $\mathfrak{g}$ be a finite-dimensional $\mathbb{C}%
$-Lie algebra equipped with a $\mathfrak{g}$-invariant symmetric bilinear form
$\left(  \cdot,\cdot\right)  $. Consider the affine Lie algebra
$\widehat{\mathfrak{g}}$ defined as in Definition \ref{def.sugawara}.

\textbf{(a)} Then, there is a natural homomorphism $\eta
_{\widehat{\mathfrak{g}}}:W\rightarrow\operatorname*{Der}\widehat{\mathfrak{g}%
}$ of Lie algebras given by
\[
\left(  \eta_{\widehat{\mathfrak{g}}}\left(  f\partial\right)  \right)
\left(  g,\alpha\right)  =\left(  fg^{\prime},0\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[  t,t^{-1}\right]
\text{, }g\in\mathfrak{g}\left[  t,t^{-1}\right]  \text{ and }\alpha
\in\mathbb{C}.
\]


\textbf{(b)} There also is a natural homomorphism $\widetilde{\eta
}_{\widehat{\mathfrak{g}}}:\operatorname*{Vir}\rightarrow\operatorname*{Der}%
\widehat{\mathfrak{g}}$ of Lie algebras given by
\[
\left(  \widetilde{\eta}_{\widehat{\mathfrak{g}}}\left(  f\partial+\lambda
K\right)  \right)  \left(  g,\alpha\right)  =\left(  fg^{\prime},0\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[  t,t^{-1}\right]
\text{, }g\in\mathfrak{g}\left[  t,t^{-1}\right]  \text{, }\lambda
\in\mathbb{C}\text{ and }\alpha\in\mathbb{C}.
\]
This homomorphism $\widetilde{\eta}_{\widehat{\mathfrak{g}}}$ is simply the
extension of the homomorphism $\eta_{\widehat{\mathfrak{g}}}:W\rightarrow
\operatorname*{Der}\widehat{\mathfrak{g}}$ to $\operatorname*{Vir}$ by means
of requiring that $\widetilde{\eta}_{\widehat{\mathfrak{g}}}\left(  K\right)
=0$.

This homomorphism $\widetilde{\eta}_{\widehat{\mathfrak{g}}}$ makes
$\widehat{\mathfrak{g}}$ a $\operatorname*{Vir}$-module on which
$\operatorname*{Vir}$ acts by derivations. Therefore, a Lie algebra
$\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$ is defined (according to
Definition \ref{def.semidir.lielie}).
\end{proposition}

The proof of Proposition \ref{prop.WtoDerg} is left to the reader. (A proof of
Proposition \ref{prop.WtoDerg} \textbf{(a)} can be obtained by carefully
generalizing the proof of Lemma \ref{lem.WtoDerA}. Actually, Proposition
\ref{prop.WtoDerg} \textbf{(a)} generalizes Lemma \ref{lem.WtoDerA}, since (as
we will see in Remark \ref{rmk.sugawara.fockvir}) the Lie algebra
$\widehat{\mathfrak{g}}$ generalizes $\mathcal{A}$.)

The following theorem is one of the most important facts about affine Lie algebras:

\begin{theorem}
[Sugawara construction]\label{thm.sugawara}Let us work in the situation of
Definition \ref{def.sugawara}.

Let $k\in\mathbb{C}$ be non-critical for $\left(  \mathfrak{g},\left(
\cdot,\cdot\right)  \right)  $. Let $M$ be an admissible
$\widehat{\mathfrak{g}}$-module of level $k$. Let $B\subseteq\mathfrak{g}$ be
a basis orthonormal with respect to the form $k\left(  \cdot,\cdot\right)
+\dfrac{1}{2}\operatorname*{Kil}$.

For every $x\in\mathfrak{g}$ and $n\in\mathbb{Z}$, let us denote by $x_{n}$
the element $xt^{n}\in\widehat{\mathfrak{g}}$.

For every $x\in\mathfrak{g}$, every $m\in\mathbb{Z}$ and $\ell\in\mathbb{Z}$,
define the ``normal ordered product'' $\left.  :x_{m}x_{\ell}:\right.  $ in
$U\left(  \widehat{\mathfrak{g}}\right)  $ by%
\[
\left.  :x_{m}x_{\ell}:\right.  =\left\{
\begin{array}
[c]{c}%
x_{m}x_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq\ell;\\
x_{\ell}x_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>\ell
\end{array}
\right.  .
\]


For every $n\in\mathbb{Z}$, define an endomorphism $L_{n}$ of $M$ by%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.  .
\]


\textbf{(a)} This endomorphism $L_{n}$ is indeed well-defined. In other words,
for every $n\in\mathbb{Z}$, every $a\in B$ and every $v\in M$, the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  v$ converges in
the discrete topology (i. e., has only finitely many nonzero addends).

\textbf{(b)} For every $n\in\mathbb{Z}$, the endomorphism $L_{n}$ does not
depend on the choice of the orthonormal basis $B$.

\textbf{(c)} The endomorphisms $L_{n}$ for $n\in\mathbb{Z}$ give rise to a
$\operatorname*{Vir}$-representation on $M$ with central charge%
\[
c=k\cdot\sum\limits_{a\in B}\left(  a,a\right)  .
\]


\textbf{(d)} These formulas (for $L_{n}$ and $c$) extend the action of
$\widehat{\mathfrak{g}}$ on $M$ to an action of $\operatorname*{Vir}%
\ltimes\widehat{\mathfrak{g}}$, so they satisfy $\left[  L_{n},a_{m}\right]
=-ma_{n+m}$ and $\left[  L_{n},K\right]  =0$.

\textbf{(e)} We have $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$ for any
$a\in\mathfrak{g}$ and any integers $n$ and $m$.
\end{theorem}

\begin{remark}
\label{rmk.sugawara.fockvir}We have already encountered an example of this
construction: namely, the example where $\mathfrak{g}$ is the trivial Lie
algebra $\mathbb{C}$, where $\left(  \cdot,\cdot\right)  :\mathfrak{g}%
\times\mathfrak{g}\rightarrow\mathbb{C}$ is the bilinear form $\left(
x,y\right)  \mapsto xy$, where $k=1$, and where $M$ is the
$\widehat{\mathfrak{g}}$-module $F_{\mu}$. (To make sense of this, notice that
when $\mathfrak{g}$ is the trivial Lie algebra $\mathbb{C}$, the affine Lie
algebra $\widehat{\mathfrak{g}}$ is canonically isomorphic to the Heisenberg
algebra $\mathcal{A}$, through an isomorphism $\widehat{\mathfrak{g}%
}\rightarrow\mathcal{A}$ which takes $t^{n}$ to $a_{n}$ and $K$ to $K$.) In
this example, the operators $L_{n}$ defined in Theorem \ref{thm.sugawara} are
exactly the operators $L_{n}$ defined in Definition \ref{def.fockvir}.
\end{remark}

Before we prove Theorem \ref{thm.sugawara}, we formulate a number of lemmas.
First, an elementary lemma on Killing forms of finite-dimensional Lie algebras:

\begin{lemma}
\label{lem.sugawara.Kil}Let $\mathfrak{g}$ be a finite-dimensional Lie
algebra. Denote by $\operatorname*{Kil}$ the Killing form of $\mathfrak{g}$.
Let $n\in\mathbb{N}$ and $p_{1},p_{2},...,p_{n}\in\mathfrak{g}$ and
$q_{1},q_{2},...,q_{n}\in\mathfrak{g}$ be such that the tensor $\sum
\limits_{i=1}^{n}p_{i}\otimes q_{i}\in\mathfrak{g}\otimes\mathfrak{g}$ is
$\mathfrak{g}$-invariant. Then, $\sum\limits_{i=1}^{n}\left[  \left[
b,p_{i}\right]  ,q_{i}\right]  =\sum\limits_{i=1}^{n}\operatorname*{Kil}%
\left(  b,p_{i}\right)  q_{i}$ for every $b\in\mathfrak{g}$.
\end{lemma}

Here, we are using the following notation:

\begin{remark}
Let $\mathfrak{g}$ be a Lie algebra. An element $m$ of a $\mathfrak{g}$-module
$M$ is said to be $\mathfrak{g}$\textit{-invariant} if and only if it
satisfies $\left(  x\rightharpoonup m=0\text{ for every }x\in\mathfrak{g}%
\right)  $. We regard $\mathfrak{g}$ as a $\mathfrak{g}$-module by means of
the adjoint action of $\mathfrak{g}$ (that is, we set $x\rightharpoonup
m=\left[  x,m\right]  $ for every $x\in\mathfrak{g}$ and $m\in\mathfrak{g}$);
thus, $\mathfrak{g}\otimes\mathfrak{g}$ becomes a $\mathfrak{g}$-module as
well. Explicitly, the action of $\mathfrak{g}$ on $\mathfrak{g}\otimes
\mathfrak{g}$ is given by%
\[
x\rightharpoonup\left(  \sum\limits_{i=1}^{n}p_{i}\otimes q_{i}\right)
=\sum\limits_{i=1}^{n}\left[  x,p_{i}\right]  \otimes q_{i}+\sum
\limits_{i=1}^{n}p_{i}\otimes\left[  x,q_{i}\right]
\]
for every tensor $\sum\limits_{i=1}^{n}p_{i}\otimes q_{i}\in\mathfrak{g}%
\otimes\mathfrak{g}$. Hence, a tensor $\sum\limits_{i=1}^{n}p_{i}\otimes
q_{i}\in\mathfrak{g}\otimes\mathfrak{g}$ is $\mathfrak{g}$-invariant if and
only if every $x\in\mathfrak{g}$ satisfies $\sum\limits_{i=1}^{n}\left[
x,p_{i}\right]  \otimes q_{i}+\sum\limits_{i=1}^{n}p_{i}\otimes\left[
x,q_{i}\right]  =0$. In other words, a tensor $\sum\limits_{i=1}^{n}%
p_{i}\otimes q_{i}\in\mathfrak{g}\otimes\mathfrak{g}$ is $\mathfrak{g}%
$-invariant if and only if every $x\in\mathfrak{g}$ satisfies $\sum
\limits_{i=1}^{n}\left[  p_{i},x\right]  \otimes q_{i}=-\sum\limits_{i=1}%
^{n}p_{i}\otimes\left[  q_{i},x\right]  $.
\end{remark}

\textit{Proof of Lemma \ref{lem.sugawara.Kil}.} Let $\left(  c_{1}%
,c_{2},...,c_{m}\right)  $ be a basis of the vector space $\mathfrak{g}$, and
let $\left(  c_{1}^{\ast},c_{2}^{\ast},...,c_{m}^{\ast}\right)  $ be the dual
basis of $\mathfrak{g}^{\ast}$. Then, every $i\in\left\{  1,2,...,n\right\}  $
satisfies
\[
\operatorname*{Kil}\left(  b,p_{i}\right)  =\operatorname*{Tr}\left(  \left(
\operatorname*{ad}b\right)  \circ\left(  \operatorname*{ad}p_{i}\right)
\right)  =\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left(  \left(
\operatorname*{ad}b\right)  \circ\left(  \operatorname*{ad}p_{i}\right)
\right)  \left(  c_{j}\right)  \right)  =\sum\limits_{j=1}^{m}c_{j}^{\ast
}\left(  \left[  b,\left[  p_{i},c_{j}\right]  \right]  \right)  .
\]
Hence,%
\begin{align*}
\sum\limits_{i=1}^{n}\operatorname*{Kil}\left(  b,p_{i}\right)  q_{i}  &
=\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left[
b,\left[  p_{i},c_{j}\right]  \right]  \right)  q_{i}=\sum\limits_{j=1}%
^{m}\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[  b,\left[  p_{i}%
,c_{j}\right]  \right]  \right)  q_{i}\\
&  =-\sum\limits_{j=1}^{m}\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  \left[  q_{i},c_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sum\limits_{i=1}^{n}p_{i}\otimes q_{i}\text{ is } \mathfrak{g}%
\text{-invariant, so that}\\
\sum\limits_{i=1}^{n}\left[  p_{i},c_{j}\right]  \otimes q_{i}=-\sum
\limits_{i=1}^{n}p_{i}\otimes\left[  q_{i},c_{j}\right]  \text{ for every
}j\in\left\{  1,2,...,m\right\}  \text{, and thus}\\
\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[  b,\left[  p_{i},c_{j}\right]
\right]  \right)  q_{i}=-\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  \left[  q_{i},c_{j}\right]  \text{ for every }%
j\in\left\{  1,2,...,m\right\}
\end{array}
\right) \\
&  =-\sum\limits_{j=1}^{m}\sum\limits_{i=1}^{n}\left[  q_{i},c_{j}^{\ast
}\left(  \left[  b,p_{i}\right]  \right)  c_{j}\right]  =-\sum\limits_{i=1}%
^{n}\left[  q_{i},\underbrace{\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  c_{j}}_{\substack{=\left[  b,p_{i}\right]
\\\text{(since }\left(  c_{1}^{\ast},c_{2}^{\ast},...,c_{m}^{\ast}\right)
\text{ is the dual basis}\\\text{to the basis }\left(  c_{1},c_{2}%
,...,c_{m}\right)  \text{)}}}\right] \\
&  =-\sum\limits_{i=1}^{n}\left[  q_{i},\left[  b,p_{i}\right]  \right]
=\sum\limits_{i=1}^{n}\left[  \left[  b,p_{i}\right]  ,q_{i}\right]  ,
\end{align*}
which proves Lemma \ref{lem.sugawara.Kil}.

Here comes another lemma on $\mathfrak{g}$-invariant bilinear forms:

\begin{lemma}
\label{lem.sugawara.Kil2}Let $\mathfrak{g}$ be a finite-dimensional
$\mathbb{C}$-Lie algebra equipped with a $\mathfrak{g}$-invariant symmetric
bilinear form $\left\langle \cdot,\cdot\right\rangle $. Let $B\subseteq
\mathfrak{g}$ be a basis orthonormal with respect to the form $\left\langle
\cdot,\cdot\right\rangle $.

\textbf{(a)} Then, the tensor $\sum\limits_{a\in B}a\otimes a$ is
$\mathfrak{g}$-invariant in $\mathfrak{g}\otimes\mathfrak{g}$.

\textbf{(b)} Let $B^{\prime}$ also be a basis of $\mathfrak{g}$ orthonormal
with respect to the form $\left\langle \cdot,\cdot\right\rangle $. Then,
$\sum\limits_{a\in B}a\otimes a=\sum\limits_{a\in B^{\prime}}a\otimes a$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sugawara.Kil2}.} The bilinear form
$\left\langle \cdot,\cdot\right\rangle $ is nondegenerate (since it has an
orthonormal basis).

\textbf{(a)} For every $v\in\mathfrak{g}$, let $v^{\ast}:\mathfrak{g}%
\rightarrow\mathbb{C}$ be the $\mathbb{C}$-linear map which sends every
$w\in\mathfrak{g}$ to $\left\langle v,w\right\rangle $. Then, $\mathfrak{g}%
^{\ast}=\left\{  v^{\ast}\ \mid\ v\in\mathfrak{g}\right\}  $ (since the form
$\left\langle \cdot,\cdot\right\rangle $ is nondegenerate).

Let $b\in\mathfrak{g}$. We will now prove that $h\left(  \sum\limits_{a\in
B}\left(  \left[  b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)
\right)  =0$ for every $h\in\left(  \mathfrak{g}\otimes\mathfrak{g}\right)
^{\ast}$.

In fact, let $h\in\left(  \mathfrak{g}\otimes\mathfrak{g}\right)  ^{\ast}$.
Since $\mathfrak{g}$ is finite-dimensional, we have $\left(  \mathfrak{g}%
\otimes\mathfrak{g}\right)  ^{\ast}=\mathfrak{g}^{\ast}\otimes\mathfrak{g}%
^{\ast}$, so that $h\in\mathfrak{g}^{\ast}\otimes\mathfrak{g}^{\ast}$. We can
WLOG assume that $h=f_{1}\otimes f_{2}$ for some $f_{1}\in\mathfrak{g}^{\ast}$
and $f_{2}\in\mathfrak{g}^{\ast}$ (because every tensor in $\mathfrak{g}%
^{\ast}\otimes\mathfrak{g}^{\ast}$ is a $\mathbb{C}$-linear combination of
pure tensors, and the assertion which we want to prove (namely, the equality
$h\left(  \sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  \right)  =0$) is $\mathbb{C}$-linear in
$h$). Assume this.

Since $f_{1}\in\mathfrak{g}^{\ast}=\left\{  v^{\ast}\ \mid\ v\in
\mathfrak{g}\right\}  $, there exists some $v_{1}\in\mathfrak{g}$ such that
$f_{1}=v_{1}^{\ast}$. Consider this $v_{1}$.

Since $f_{2}\in\mathfrak{g}^{\ast}=\left\{  v^{\ast}\ \mid\ v\in
\mathfrak{g}\right\}  $, there exists some $v_{2}\in\mathfrak{g}$ such that
$f_{2}=v_{2}^{\ast}$. Consider this $v_{2}$.

Since $B$ is an orthonormal basis with respect to $\left\langle \cdot
,\cdot\right\rangle $, we have $\sum\limits_{a\in B}a\left\langle \left[
b,v_{2}\right]  ,a\right\rangle =\left[  b,v_{2}\right]  $ and $\sum
\limits_{a\in B}\left\langle \left[  b,v_{1}\right]  ,a\right\rangle a=\left[
b,v_{1}\right]  $.

Now, $h=\underbrace{f_{1}}_{=v_{1}^{\ast}}\otimes\underbrace{f_{2}}%
_{=v_{2}^{\ast}}=v_{1}^{\ast}\otimes v_{2}^{\ast}$, so that%
\begin{align*}
&  h\left(  \sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  \right) \\
&  =\left(  v_{1}^{\ast}\otimes v_{2}^{\ast}\right)  \left(  \sum\limits_{a\in
B}\left(  \left[  b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)
\right) \\
&  =\sum\limits_{a\in B}\left(  \underbrace{v_{1}^{\ast}\left(  \left[
b,a\right]  \right)  }_{\substack{=\left\langle v_{1},\left[  b,a\right]
\right\rangle \\\text{(by the definition of }v_{1}^{\ast}\text{)}}%
}\cdot\underbrace{v_{2}^{\ast}\left(  a\right)  }_{\substack{=\left\langle
v_{2},a\right\rangle \\\text{(by the definition of }v_{2}^{\ast}\text{)}%
}}+\underbrace{v_{1}^{\ast}\left(  a\right)  }_{\substack{=\left\langle
v_{1},a\right\rangle \\\text{(by the definition of }v_{1}^{\ast}\text{)}%
}}\cdot\underbrace{v_{2}^{\ast}\left(  \left[  b,a\right]  \right)
}_{\substack{=\left\langle v_{2},\left[  b,a\right]  \right\rangle \\\text{(by
the definition of }v_{2}^{\ast}\text{)}}}\right) \\
&  =\sum\limits_{a\in B}\left(  \underbrace{\left\langle v_{1},\left[
b,a\right]  \right\rangle }_{\substack{=-\left\langle \left[  b,v_{1}\right]
,a\right\rangle \\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{
is invariant)}}}\cdot\left\langle v_{2},a\right\rangle +\left\langle
v_{1},a\right\rangle \cdot\underbrace{\left\langle v_{2},\left[  b,a\right]
\right\rangle }_{\substack{=-\left\langle \left[  b,v_{2}\right]
,a\right\rangle \\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{
is invariant)}}}\right) \\
&  =\sum\limits_{a\in B}\left(  -\left\langle \left[  b,v_{1}\right]
,a\right\rangle \cdot\left\langle v_{2},a\right\rangle -\left\langle
v_{1},a\right\rangle \cdot\left\langle \left[  b,v_{2}\right]  ,a\right\rangle
\right) \\
&  =-\underbrace{\sum\limits_{a\in B}\left\langle \left[  b,v_{1}\right]
,a\right\rangle \cdot\left\langle v_{2},a\right\rangle }_{=\left\langle
v_{2},\sum\limits_{a\in B}\left\langle \left[  b,v_{1}\right]  ,a\right\rangle
a\right\rangle }-\underbrace{\sum\limits_{a\in B}\left\langle v_{1}%
,a\right\rangle \cdot\left\langle \left[  b,v_{2}\right]  ,a\right\rangle
}_{=\left\langle v_{1},\sum\limits_{a\in B}a\left\langle \left[
b,v_{2}\right]  ,a\right\rangle \right\rangle }\\
&  =-\left\langle v_{2},\underbrace{\sum\limits_{a\in B}\left\langle \left[
b,v_{1}\right]  ,a\right\rangle a}_{=\left[  b,v_{1}\right]  }\right\rangle
-\left\langle v_{1},\underbrace{\sum\limits_{a\in B}a\left\langle \left[
b,v_{2}\right]  ,a\right\rangle }_{=\left[  b,v_{2}\right]  }\right\rangle \\
&  =-\underbrace{\left\langle v_{2},\left[  b,v_{1}\right]  \right\rangle
}_{\substack{=\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle
\\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{ is symmetric)}%
}}-\underbrace{\left\langle v_{1},\left[  b,v_{2}\right]  \right\rangle
}_{\substack{=-\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle
\\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{ is invariant)}%
}}=-\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle -\left(
-\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle \right)  =0.
\end{align*}


We thus have proven that $h\left(  \sum\limits_{a\in B}\left(  \left[
b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)  \right)  =0$ for
every $h\in\left(  \mathfrak{g}\otimes\mathfrak{g}\right)  ^{\ast}$.
Consequently, $\sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  =0$.

Hence, we have shown that $\sum\limits_{a\in B}\left(  \left[  b,a\right]
\otimes a+a\otimes\left[  b,a\right]  \right)  =0$ for every $b\in
\mathfrak{g}$. In other words, the tensor $\sum\limits_{a\in B}a\otimes a$ is
$\mathfrak{g}$-invariant. Lemma \ref{lem.sugawara.Kil2} \textbf{(a)} is proven.

\textbf{(b)} For every $a\in B$ and $b\in B^{\prime}$, let $\xi_{a,b}$ be the
$b$-coordinate of $a$ with respect to the basis $B^{\prime}$. Then, every
$a\in B$ satisfies $a=\sum\limits_{b\in B^{\prime}}\xi_{a,b}b$. Thus, $\left(
\xi_{a,b}\right)  _{\left(  a,b\right)  \in B\times B^{\prime}}$ (this is a
matrix whose rows and columns are indexed by elements of $B$ and $B^{\prime}$,
respectively) is the matrix which represents the change of bases from
$B^{\prime}$ to $B$ (or from $B$ to $B^{\prime}$, depending on how you define
the matrix representing a change of basis). Since both $B$ and $B^{\prime}$
are two orthonormal bases with respect to the same bilinear form $\left\langle
\cdot,\cdot\right\rangle $, this matrix must thus be orthogonal. Hence, every
$b\in B^{\prime}$ and $b^{\prime}\in B^{\prime}$ satisfy $\sum\limits_{a\in
B}\xi_{a,b}\xi_{a,b^{\prime}}=\delta_{b,b^{\prime}}$ (where $\delta
_{b,b^{\prime}}$ is the Kronecker delta of $b$ and $b^{\prime}$). Now, since
every $a\in B$ satisfies $a=\sum\limits_{b\in B^{\prime}}\xi_{a,b}b$ and
$a=\sum\limits_{b\in B^{\prime}}\xi_{a,b}b=\sum\limits_{b^{\prime}\in
B^{\prime}}\xi_{a,b^{\prime}}b^{\prime}$ (here, we renamed $b$ as $b^{\prime}$
in the sum), we have%
\begin{align*}
&  \sum\limits_{a\in B}\underbrace{a}_{=\sum\limits_{b\in B^{\prime}}\xi
_{a,b}b}\otimes\underbrace{a}_{=\sum\limits_{b^{\prime}\in B^{\prime}}%
\xi_{a,b^{\prime}}b^{\prime}}\\
&  =\sum\limits_{a\in B}\left(  \sum\limits_{b\in B^{\prime}}\xi
_{a,b}b\right)  \otimes\left(  \sum\limits_{b^{\prime}\in B^{\prime}}%
\xi_{a,b^{\prime}}b^{\prime}\right)  =\sum\limits_{a\in B}\sum\limits_{b\in
B^{\prime}}\sum\limits_{b^{\prime}\in B^{\prime}}\xi_{a,b}\xi_{a,b^{\prime}%
}b\otimes b^{\prime}\\
&  =\sum\limits_{b\in B^{\prime}}\sum\limits_{b^{\prime}\in B^{\prime}%
}\underbrace{\sum\limits_{a\in B}\xi_{a,b}\xi_{a,b^{\prime}}}_{=\delta
_{b,b^{\prime}}}b\otimes b^{\prime}=\sum\limits_{b\in B^{\prime}%
}\underbrace{\sum\limits_{b^{\prime}\in B^{\prime}}\delta_{b,b^{\prime}%
}b\otimes b^{\prime}}_{=b\otimes b}=\sum\limits_{b\in B^{\prime}}b\otimes b\\
&  =\sum\limits_{a\in B^{\prime}}a\otimes a\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we renamed }b\text{ as }a\text{ in the sum}\right)  .
\end{align*}
This proves Lemma \ref{lem.sugawara.Kil2} \textbf{(b)}.

As a consequence of this lemma, we get:

\begin{lemma}
\label{lem.sugawara.Kil3}Let $\mathfrak{g}$ be a finite-dimensional
$\mathbb{C}$-Lie algebra equipped with a $\mathfrak{g}$-invariant symmetric
bilinear form $\left(  \cdot,\cdot\right)  $. Denote by $\operatorname*{Kil}$
the Killing form of $\mathfrak{g}$. Let $B\subseteq\mathfrak{g}$ be a basis
orthonormal with respect to the form $k\left(  \cdot,\cdot\right)  +\dfrac
{1}{2}\operatorname*{Kil}$. Let $b\in\mathfrak{g}$.

\textbf{(a)} We have $\sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  =0$.

\textbf{(b)} We have $\dfrac{1}{2}\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(  b,a\right)  a=b$.

\textbf{(c)} We have $\left(  \left[  b,a\right]  ,a\right)  =0$ for every
$a\in\mathfrak{g}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sugawara.Kil3}.} The basis $B$ is orthonormal
with respect to a symmetric $\mathfrak{g}$-invariant bilinear form (namely,
the form $k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$). As
a consequence, the tensor $\sum\limits_{a\in B}a\otimes a$ is $\mathfrak{g}%
$-invariant in $\mathfrak{g}\otimes\mathfrak{g}$ (by Lemma
\ref{lem.sugawara.Kil2} \textbf{(a)}, applied to $\left\langle \cdot
,\cdot\right\rangle =k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}%
\operatorname*{Kil}$). In other words, $\sum\limits_{a\in B}\left(  \left[
b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)  =0$. This proves
Lemma \ref{lem.sugawara.Kil3} \textbf{(a)}.

\textbf{(b)} If $\left\langle \cdot,\cdot\right\rangle $ is any nondegenerate
inner product\footnote{By ``inner product'', we mean a symmetric bilinear
form.} on a finite-dimensional vector space $V$ and $B$ is an orthonormal
basis with respect to that product, then any vector $b\in V$ is equal to
$\sum\limits_{a\in B}\left\langle b,a\right\rangle a$. Applying this fact to
the inner product $\left\langle \cdot,\cdot\right\rangle =k\left(  \cdot
,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$ on the vector space
$V=\mathfrak{g}$, we conclude that $b=k\sum\limits_{a\in B}\left(  b,a\right)
a+\dfrac{1}{2}\sum\limits_{a\in B}\operatorname*{Kil}\left(  b,a\right)  a$.

Now, applying Lemma \ref{lem.sugawara.Kil} to the $\mathfrak{g}$-invariant
tensor $\sum\limits_{a\in B}a\otimes a$ in lieu of $\sum\limits_{i=1}^{n}%
p_{i}\otimes q_{i}$, we see that $\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  =\sum\limits_{a\in B}\operatorname*{Kil}\left(
b,a\right)  a$. Hence,%
\[
b=k\sum\limits_{a\in B}\left(  b,a\right)  a+\dfrac{1}{2}\underbrace{\sum
\limits_{a\in B}\operatorname*{Kil}\left(  b,a\right)  a}_{=\sum\limits_{a\in
B}\left[  \left[  b,a\right]  ,a\right]  }=\dfrac{1}{2}\sum\limits_{a\in
B}\left[  \left[  b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(
b,a\right)  a.
\]
This proves Lemma \ref{lem.sugawara.Kil3} \textbf{(b)}.

\begin{vershort}
\textbf{(c)} Every $c\in\mathfrak{g}$ satisfies $\left(  \left[  a,b\right]
,c\right)  +\left(  b,\left[  a,c\right]  \right)  =0$ (due to the
$\mathfrak{g}$-invariance of $\left(  \cdot,\cdot\right)  $). Applying this to
$c=a$, we obtain $\left(  \left[  a,b\right]  ,a\right)  +\left(  b,\left[
a,a\right]  \right)  =0$. Since $\left[  a,a\right]  =0$ and $\left[
a,b\right]  =-\left[  b,a\right]  $, this rewrites as $\left(  -\left[
b,a\right]  ,a\right)  +\left(  b,0\right)  =0$. This simplifies to $-\left(
\left[  b,a\right]  ,a\right)  =0$. Thus, $\left(  \left[  b,a\right]
,a\right)  =0$. This proves Lemma \ref{lem.sugawara.Kil3} \textbf{(c)}.
\end{vershort}

\begin{verlong}
\textbf{(c)} Every $c\in\mathfrak{g}$ satisfies $\left(  \left[  a,b\right]
,c\right)  +\left(  b,\left[  a,c\right]  \right)  =0$ (due to the
$\mathfrak{g}$-invariance of $\left(  \cdot,\cdot\right)  $). Applying this to
$c=a$, we obtain $\left(  \left[  a,b\right]  ,a\right)  +\left(  b,\left[
a,a\right]  \right)  =0$. Thus,%
\begin{align*}
0  &  =\left(  \underbrace{\left[  a,b\right]  }_{\substack{=-\left[
b,a\right]  \\\text{(since the Lie bracket}\\\text{is antisymmetric)}%
}},a\right)  +\left(  b,\underbrace{\left[  a,a\right]  }%
_{\substack{=0\\\text{(since the Lie bracket}\\\text{is antisymmetric)}%
}}\right)  =\underbrace{\left(  -\left[  b,a\right]  ,a\right)  }%
_{\substack{=-\left(  \left[  b,a\right]  ,a\right)  \\\text{(since the form
}\left(  \cdot,\cdot\right)  \\\text{is bilinear)}}}+\underbrace{\left(
b,0\right)  }_{\substack{=0\\\text{(since the form }\left(  \cdot
,\cdot\right)  \\\text{is bilinear)}}}\\
&  =-\left(  \left[  b,a\right]  ,a\right)  +0=-\left(  \left[  b,a\right]
,a\right)  .
\end{align*}
Adding $\left(  \left[  b,a\right]  ,a\right)  $ to this equality, we obtain
$\left(  \left[  b,a\right]  ,a\right)  =0$. This proves Lemma
\ref{lem.sugawara.Kil3} \textbf{(c)}.
\end{verlong}

\begin{noncompile}
Here is a \textit{different proof of Lemma \ref{lem.sugawara.Kil3}
\textbf{(c)} (which I had written before I found the trivial proof above):}
Every $c\in\mathfrak{g}$ satisfies $\left(  \left[  b,a\right]  ,c\right)
+\left(  a,\left[  b,c\right]  \right)  =0$ (due to the $\mathfrak{g}%
$-invariance of $\left(  \cdot,\cdot\right)  $). Applying this to $c=a$, we
obtain $\left(  \left[  b,a\right]  ,a\right)  +\left(  a,\left[  b,a\right]
\right)  =0$. Since the form $\left(  \cdot,\cdot\right)  $ is symmetric, this
rewrites as $2\left(  \left[  b,a\right]  ,a\right)  =0$. Thus, $\left(
\left[  b,a\right]  ,a\right)  =0$. This proves Lemma \ref{lem.sugawara.Kil3}
\textbf{(c)}.
\end{noncompile}

Next, we formulate the analogue of Remark \ref{rmk.fockvir.normal.mn}:

\begin{remark}
\label{rmk.sugawara.normal.mn}Let $x\in\mathfrak{g}$. If $m$ and $n$ are
integers such that $m\neq-n$, then $\left.  :x_{m}x_{n}:\right.  =x_{m}x_{n}$.
(This is because $\left[  x_{m},x_{n}\right]  =0$ in $\widehat{\mathfrak{g}}$
when $m\neq-n$.)
\end{remark}

In analogy to Remark \ref{rmk.fockvir.normal.comm} \textbf{(a)}, we have
commutativity of normal ordered products:

\begin{remark}
\label{rmk.sugawara.normal.comm}Let $x\in\mathfrak{g}$. Any $m\in\mathbb{Z}$
and $n\in\mathbb{Z}$ satisfy $\left.  :x_{m}x_{n}:\right.  =\left.
:x_{n}x_{m}:\right.  $.
\end{remark}

Also, here is a simple way to rewrite the definition of $\left.  :x_{m}%
x_{n}:\right.  $:

\begin{remark}
\label{rmk.sugawara.normal.max}Let $x\in\mathfrak{g}$. Any $m\in\mathbb{Z}$
and $n\in\mathbb{Z}$ satisfy $\left.  :x_{m}x_{n}:\right.  =x_{\min\left\{
m,n\right\}  }x_{\max\left\{  m,n\right\}  }$.
\end{remark}

Generalizing Remark \ref{rmk.fockvir.normal.K}, we have:

\begin{remark}
\label{rmk.sugawara.normal.K}Let $x\in\mathfrak{g}$. Let $m$ and $n$ be integers.

\textbf{(a)} Then, $\left.  :x_{m}x_{n}:\right.  =x_{m}x_{n}+n\left[
m>0\right]  \delta_{m,-n}\left(  x,x\right)  K$. Here, when $\mathfrak{A}$ is
an assertion, we denote by $\left[  \mathfrak{A}\right]  $ the truth value of
$\mathfrak{A}$ (that is, the number $\left\{
\begin{array}
[c]{c}%
1\text{, if }\mathfrak{A}\text{ is true;}\\
0\text{, if }\mathfrak{A}\text{ is false }%
\end{array}
\right.  $).

\textbf{(b)} For any $y\in U\left(  \widehat{\mathfrak{g}}\right)  $, we have
$\left[  y,\left.  :x_{m}x_{n}:\right.  \right]  =\left[  y,x_{m}x_{n}\right]
$ in $U\left(  \widehat{\mathfrak{g}}\right)  $ (where $\left[  \cdot
,\cdot\right]  $ denotes the commutator in $U\left(  \widehat{\mathfrak{g}%
}\right)  $).
\end{remark}

The proof of this is left to the reader (it follows very quickly from the definitions).

Next, here is a completely elementary lemma:

\begin{lemma}
\label{lem.telescope}Let $G$ be an abelian group (written additively).
Whenever $\left(  u_{m}\right)  _{m\in\mathbb{Z}}\in G^{\mathbb{Z}}$ is a
family of elements of $G$, and $\mathcal{A}\left(  m\right)  $ is an assertion
for every $m\in\mathbb{Z}$, let us abbreviate the sum $\sum
\limits_{\substack{m\in\mathbb{Z};\\\mathcal{A}\left(  m\right)  }}u_{m}$ (if
this sum is well-defined) by $\sum\limits_{\mathcal{A}\left(  m\right)  }%
u_{m}$. (For instance, we will abbreviate the sum $\sum\limits_{\substack{m\in
\mathbb{Z};\\3\leq m\leq7}}u_{m}$ by $\sum\limits_{3\leq m\leq7}u_{m}$.)

For any integers $\alpha$ and $\beta$ such that $\alpha\leq\beta$, for any
nonnegative integer $N$, and for any family $\left(  u_{m}\right)
_{m\in\mathbb{Z}}\in G^{\mathbb{Z}}$ of elements of $G$, we have%
\[
\sum\limits_{\left\vert m-\beta\right\vert \leq N}u_{m}-\sum
\limits_{\left\vert m-\alpha\right\vert \leq N}u_{m}=-\sum\limits_{\alpha
-N\leq m<\beta-N}u_{m}+\sum\limits_{\alpha+N<m\leq\beta+N}u_{m}.
\]

\end{lemma}

The proof of Lemma \ref{lem.telescope} (which is merely an easy generalization
of the telescope principle) is left to the reader.

\textit{Proof of Theorem \ref{thm.sugawara}.} Let us use the notation
$\sum\limits_{\mathcal{A}\left(  m\right)  }u_{m}$ defined in Lemma
\ref{lem.telescope}.

In the following, we will consider the topology on $\operatorname*{End}M$
defined as follows: Endow $M$ with the discrete topology, endow $M^{M}$ with
the product topology, and endow $\operatorname*{End}M$ with a topology by
viewing $\operatorname*{End}M$ as a subset of the set $M^{M}$. Clearly, in
this topology, a net $\left(  a_{s}\right)  _{s\in S}$ of elements of
$\operatorname*{End}M$ converges if and only if for every $v\in M$, the net
$\left(  a_{s}v\right)  _{s\in S}$ of elements of $M$ converges (in the
discrete topology). As a consequence, whenever $\left(  u_{m}\right)
_{m\in\mathbb{Z}}$ is a family of elements of $\operatorname*{End}M$ indexed
by integers, the sum $\sum\limits_{m\in\mathbb{Z}}u_{m}$ converges with
respect to the topology which we defined on $\operatorname*{End}M$ if and only
if for every $v\in M$, the sum $\sum\limits_{m\in\mathbb{Z}}u_{m}v$ converges
in the discrete topology (i. e., has only finitely many nonzero addends).
Consequently, the convergence of an infinite sum with respect to the topology
which we defined on $\operatorname*{End}M$ is equivalent to the convergence of
this sum in the meaning in which we used the word ``convergence'' in Theorem
\ref{thm.sugawara}.

Note that addition, composition, and scalar multiplication (in the sense of:
multiplication by scalars) of maps in $\operatorname*{End}M$ are continuous
maps with respect to this topology.

We will use the notation $\lim\limits_{N\rightarrow\infty}$ for limits with
respect to the topology on $\operatorname*{End}M$. Note that, if $\left(
u_{m}\right)  _{m\in\mathbb{Z}}$ is a family of elements of
$\operatorname*{End}M$ indexed by integers, and if the sum $\sum
\limits_{m\in\mathbb{Z}}u_{m}$ converges with respect to the topology which we
defined on $\operatorname*{End}M$, then $\sum\limits_{m\in\mathbb{Z}}%
u_{m}=\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\alpha
\right\vert \leq N}u_{m}$ for every $\alpha\in\mathbb{R}$.

In the following, $\left[  \cdot,\cdot\right]  _{L\mathfrak{g}}$ will mean the
Lie bracket of $L\mathfrak{g}$, whereas the notation $\left[  \cdot
,\cdot\right]  $ without a subscript will mean either the Lie bracket of
$\widehat{\mathfrak{g}}$ or the Lie bracket of $\mathfrak{g}$. Note that the
use of the same notation for the Lie bracket of $\widehat{\mathfrak{g}}$ and
for the Lie bracket of $\mathfrak{g}$ will not lead to conflicts, since the
Lie bracket of $\mathfrak{g}$ is the restriction of the Lie bracket of
$\widehat{\mathfrak{g}}$ to $\mathfrak{g}\times\mathfrak{g}$ (this follows
quickly from $\omega\left(  \mathfrak{g},\mathfrak{g}\right)  =0$).

Note that any $x\in\mathfrak{g}$, $y\in\mathfrak{g}$, $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$ satisfy%
\begin{equation}
\left[  x_{n},y_{m}\right]  =\left[  x,y\right]  _{n+m}+K\omega\left(
x_{n},y_{m}\right)  \label{pf.sugawara.lie}%
\end{equation}
\footnote{This is because%
\begin{align*}
\left[  \underbrace{x_{n}}_{=xt^{n}},\underbrace{y_{m}}_{=yt^{m}}\right]   &
=\left[  xt^{n},yt^{m}\right]  =\left(  \underbrace{\left[  xt^{n}%
,yt^{m}\right]  _{L\mathfrak{g}}}_{\substack{=\left[  x,y\right]
t^{n+m}\\\text{(by the definition of the Lie}\\\text{algebra structure on
}L\mathfrak{g}\text{)}}},\omega\left(  \underbrace{xt^{n}}_{=x_{n}%
},\underbrace{yt^{m}}_{=y_{m}}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the Lie bracket on
}\widehat{\mathfrak{g}}\right) \\
&  =\left(  \underbrace{\left[  x,y\right]  t^{n+m}}_{=\left[  x,y\right]
_{n+m}},\omega\left(  x_{n},y_{m}\right)  \right)  =\left(  \left[
x,y\right]  _{n+m},\omega\left(  x_{n},y_{m}\right)  \right)  =\left[
x,y\right]  _{n+m}+K\omega\left(  x_{n},y_{m}\right)  .
\end{align*}
}.

\textbf{(a)} Let $n\in\mathbb{Z}$ and $v\in M$. We must prove that for every
$a\in B$, the sum $\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.
v$ converges in the discrete topology. We will prove a slightly more general
statement: We will prove that for every $x\in\mathfrak{g}$, the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$ converges in
the discrete topology.

In fact, let $x\in\mathfrak{g}$. We must prove that the sum $\sum
\limits_{m\in\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$ converges in the
discrete topology.

Recall the definition of an admissible module. With slightly modified
notations, it looks as follows: A $\widehat{\mathfrak{g}}$-module $P$ is said
to be \textit{admissible} if for every $w\in P$, there exists some
$\mathbf{M}\in\mathbb{N}$ such that every integer $\mathbf{m}\geq\mathbf{M}$
and every $a\in\mathfrak{g}$ satisfy $at^{\mathbf{m}}\cdot w=0$. Hence, for
every $w\in M$, there exists some $\mathbf{M}\in\mathbb{N}$ such that every
integer $\mathbf{m}\geq\mathbf{M}$ and every $a\in\mathfrak{g}$ satisfy
$at^{\mathbf{m}}\cdot w=0$ (because $M$ is admissible). Applying this to
$w=v$, we see that there exists some $\mathbf{M}\in\mathbb{N}$ such that every
integer $\mathbf{m}\geq\mathbf{M}$ and every $a\in\mathfrak{g}$ satisfy
$at^{\mathbf{m}}\cdot v=0$. Fix this $\mathbf{M}$. Every integer
$m\geq\mathbf{M}$ satisfies%
\begin{equation}
\underbrace{x_{m}}_{=xt^{m}}v=xt^{m}\cdot v=0 \label{pf.sugawara.a.1}%
\end{equation}
(by the equality $at^{\mathbf{m}}\cdot v=0$, applied to $a=x$ and
$\mathbf{m}=m$). Now, every integer $m$ such that $\max\left\{  m,n-m\right\}
\geq\mathbf{M}$ satisfies%
\[
\underbrace{\left.  :x_{m}x_{n-m}:\right.  }_{\substack{=x_{\min\left\{
m,n-m\right\}  }x_{\max\left\{  m,n-m\right\}  }\\\text{(by Remark
\ref{rmk.sugawara.normal.max}, applied}\\\text{to }\ell=n-m\text{)}}%
}v=x_{\min\left\{  m,n-m\right\}  }\underbrace{x_{\max\left\{  m,n-m\right\}
}v}_{\substack{=0\\\text{(by (\ref{pf.sugawara.a.1}), applied to }\max\left\{
m,n-m\right\}  \\\text{instead of }m\text{ (since }\max\left\{  m,n-m\right\}
\geq\mathbf{M}\text{))}}}=x_{\min\left\{  m,n-m\right\}  }0=0.
\]
Since all but finitely many integers $m$ satisfy $\max\left\{  m,n-m\right\}
\geq\mathbf{M}$ (this is obvious), this shows that all but finitely many
integers $m$ satisfy $\left.  :x_{m}x_{n-m}:\right.  v=0$. In other words, all
but finitely many addends of the sum $\sum\limits_{m\in\mathbb{Z}}\left.
:x_{m}x_{n-m}:\right.  v$ are zero. Hence, the sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$ converges in the discrete
topology. This proves Theorem \ref{thm.sugawara} \textbf{(a)}.

Note that, during the proof of Theorem \ref{thm.sugawara} \textbf{(a)}, we
have shown that for every $n\in\mathbb{Z}$, $x\in\mathfrak{g}$ and $v\in M$,
the sum $\sum\limits_{m\in\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$
converges in the discrete topology. In other words, for every $n\in\mathbb{Z}$
and $x\in\mathfrak{g}$, the sum $\sum\limits_{m\in\mathbb{Z}}\left.
:x_{m}x_{n-m}:\right.  $ converges in the topology which we defined on
$\operatorname*{End}M$.

\textbf{(b)} Let $n\in\mathbb{Z}$. Let $B^{\prime}$ be an orthonormal basis of
$\mathfrak{g}$ with respect to the form $k\left(  \cdot,\cdot\right)
+\dfrac{1}{2}\operatorname*{Kil}$. We are going to prove that%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  \label{pf.sugawara.basisind.1}%
\end{equation}
(where $L_{n}$ still denotes the operator $\dfrac{1}{2}\sum\limits_{a\in
B}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  $ defined in
Theorem \ref{thm.sugawara} using the orthonormal basis $B$, not the
orthonormal basis $B^{\prime}$). Once (\ref{pf.sugawara.basisind.1}) is
proven, it will follow that $L_{n}$ does not depend on $B$, and thus Theorem
\ref{thm.sugawara} \textbf{(b)} will be proven.

Applying Lemma \ref{lem.sugawara.Kil2} \textbf{(b)} to $\left\langle
\cdot,\cdot\right\rangle =k\left(  \cdot,\cdot\right)  +\dfrac{1}%
{2}\operatorname*{Kil}$, we obtain $\sum\limits_{a\in B}a\otimes
a=\sum\limits_{a\in B^{\prime}}a\otimes a$. Thus,%
\begin{equation}
\sum\limits_{a\in B}a_{u}a_{v}=\sum\limits_{a\in B^{\prime}}a_{u}%
a_{v}\ \ \ \ \ \ \ \ \ \ \text{for any }u\in\mathbb{Z}\text{ and }%
v\in\mathbb{Z} \label{pf.sugawara.basisind.pf.1}%
\end{equation}
\footnote{This follows from applying the linear map
\begin{align*}
\mathfrak{g}\otimes\mathfrak{g}  &  \rightarrow\operatorname*{End}M,\\
x\otimes y  &  \mapsto x_{u}y_{v}%
\end{align*}
to the equality $\sum\limits_{a\in B}a\otimes a=\sum\limits_{a\in B^{\prime}%
}a\otimes a$.}.

Thus, every $m\in\mathbb{Z}$ satisfies $\sum\limits_{a\in B}\left.
:a_{m}a_{n-m}:\right.  =\sum\limits_{a\in B^{\prime}}\left.  :a_{m}%
a_{n-m}:\right.  $\ \ \ \ \footnote{\textit{Proof.} We distinguish between two
cases:
\par
\textit{Case 1:} We have $m\leq n-m$.
\par
\textit{Case 2:} We have $m>n-m$.
\par
Let us first consider Case 1. In this case, $m\leq n-m$. Hence, every
$a\in\mathfrak{g}$ satisfies $\left.  :a_{m}a_{n-m}:\right.  =a_{m}a_{n-m}$.
Thus,
\begin{align*}
\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.   &  =\sum\limits_{a\in
B}a_{m}a_{n-m}=\sum\limits_{a\in B^{\prime}}\underbrace{a_{m}a_{n-m}%
}_{=\left.  :a_{m}a_{n-m}:\right.  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.sugawara.basisind.pf.1}), applied to }u=m\text{ and }v=n-m\right) \\
&  =\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  .
\end{align*}
This proves $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  =\sum
\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ in Case 1.
\par
Let us now consider Case 2. In this case, $m>n-m$. Hence, every $a\in
\mathfrak{g}$ satisfies $\left.  :a_{m}a_{n-m}:\right.  =a_{n-m}a_{m}$. Thus,
\begin{align*}
\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.   &  =\sum\limits_{a\in
B}a_{n-m}a_{m}=\sum\limits_{a\in B^{\prime}}\underbrace{a_{n-m}a_{m}%
}_{=\left.  :a_{m}a_{n-m}:\right.  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.sugawara.basisind.pf.1}), applied to }u=n-m\text{ and }v=m\right) \\
&  =\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  .
\end{align*}
This proves $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  =\sum
\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ in Case 2.
\par
Hence, $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  =\sum\limits_{a\in
B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ is proven in each of the cases 1
and 2. Thus, $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.
=\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ always holds
(since cases 1 and 2 cover all possibilities), qed.}. Hence,%
\begin{align*}
L_{n}  &  =\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}%
\underbrace{\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  }%
_{=\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  }=\dfrac{1}%
{2}\sum\limits_{m\in\mathbb{Z}}\sum\limits_{a\in B^{\prime}}\left.
:a_{m}a_{n-m}:\right. \\
&  =\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  .
\end{align*}
Thus, (\ref{pf.sugawara.basisind.1}) is proven. As we said, this completes the
proof of Theorem \ref{thm.sugawara} \textbf{(b)}.

\textbf{(c)} \textit{1st step:} Let us first show that%
\begin{equation}
\left[  b_{r},L_{n}\right]  =rb_{n+r}\ \ \ \ \ \ \ \ \ \ \text{for every }%
b\in\mathfrak{g}\text{ and any integers }r\text{ and }n.
\label{pf.sugawara.step1}%
\end{equation}


\textit{Proof of (\ref{pf.sugawara.step1}):} Let $b\in\mathfrak{g}$,
$r\in\mathbb{Z}$ and $n\in\mathbb{Z}$.

We must be careful here with infinite sums, since not even formal algebra
allows us to manipulate infinite sums like $\sum\limits_{m\in\mathbb{Z}%
}\left[  b,a\right]  _{r+m}a_{n-m}$ (for good reasons: these are divergent in
every meaning of this word). While we were working in the Heisenberg algebra
$\mathcal{A}$ (which can be written as $\widehat{\mathfrak{g}}$ for
$\mathfrak{g}$ being the trivial Lie algebra $\mathbb{C}$), these infinite
sums made sense due to all of their addends being $0$ (since $\left[
b,a\right]  =0$ for all $a$ and $b$ lying in the trivial Lie algebra
$\mathbb{C}$). But this was an exception rather than the rule, and now we need
to take care.

Let us first assume that $r\geq0$.

Since%
\begin{align*}
L_{n}  &  =\dfrac{1}{2}\sum\limits_{a\in B}\underbrace{\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  }_{=\lim\limits_{N\rightarrow\infty
}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left.
:a_{m}a_{n-m}:\right.  }=\dfrac{1}{2}\sum\limits_{a\in B}\lim
\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert
\leq N}\left.  :a_{m}a_{n-m}:\right. \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left.  :a_{m}%
a_{n-m}:\right.  ,
\end{align*}
we have%
\begin{align}
&  \left[  b_{r},L_{n}\right] \nonumber\\
&  =\left[  b_{r},\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum
\limits_{a\in B}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}\left.  :a_{m}a_{n-m}:\right.  \right] \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\underbrace{\left[
b_{r},\left.  :a_{m}a_{n-m}:\right.  \right]  }%
_{\substack{_{\substack{=\left[  b_{r},a_{m}a_{n-m}\right]  }}\\\text{(by
Remark \ref{rmk.sugawara.normal.K} \textbf{(b)}, applied to}\\a\text{, }%
b_{r}\text{ and }n-m\text{ instead of }x\text{, }y\text{ and }n\text{)}%
}}\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\underbrace{\left[
b_{r},a_{m}a_{n-m}\right]  }_{=\left[  b_{r},a_{m}\right]  a_{n-m}%
+a_{m}\left[  b_{r},a_{n-m}\right]  }\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\underbrace{\left[  b_{r},a_{m}\right]  }_{\substack{=\left[  b,a\right]
_{r+m}+K\omega\left(  b_{r},a_{m}\right)  \\\text{(by (\ref{pf.sugawara.lie}%
))}}}a_{n-m}+a_{m}\underbrace{\left[  b_{r},a_{n-m}\right]  }%
_{\substack{=\left[  b,a\right]  _{n+r-m}+K\omega\left(  b_{r},a_{n-m}\right)
\\\text{(by (\ref{pf.sugawara.lie}))}}}\right) \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}%
+a_{m}\left[  b,a\right]  _{n+r-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)
\right) \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
. \label{pf.sugawara.b.1}%
\end{align}


Now fix $a\in B$. We now notice that for any $N\in\mathbb{N}$, the sum
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}$ (in $\operatorname*{End}M$) has at most one
nonzero addend (because $\omega\left(  b_{r},a_{m}\right)  $ can be nonzero
for at most one integer $m$ (namely, for $m=-r$)). Hence, this sum
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}$ converges for any $N\in\mathbb{N}$. For
sufficiently high $N$, this sum does have an addend for $m=-r$, and all other
addends of this sum are $0$ (since $\omega\left(  b_{r},a_{m}\right)  =0$
whenever $m\neq-r$), so that the value of this sum is $\underbrace{K}%
_{\substack{=k\\\text{(since }K\text{ acts as}\\k\cdot\operatorname*{id}\text{
on }M\text{)}}}\underbrace{\omega\left(  b_{r},a_{-r}\right)  }%
_{\substack{=r\left(  b,a\right)  \\\text{(by the definition of }%
\omega\text{)}}}\underbrace{a_{n-\left(  -r\right)  }}_{=a_{n+r}}=kr\left(
b,a\right)  a_{n+r}$. We thus have shown that the sum $\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(  b_{r},a_{m}\right)  a_{n-m}$
converges for all $N\in\mathbb{N}$, and satisfies%
\begin{equation}
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}=kr\left(  b,a\right)  a_{n+r}%
\ \ \ \ \ \ \ \ \ \ \text{for sufficiently high }N\text{.}
\label{pf.sugawara.b.2a}%
\end{equation}
Similarly, we see that the sum $\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)  $ converges
for all $N\in\mathbb{N}$, and satisfies%
\begin{equation}
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}a_{m}K\omega\left(
b_{r},a_{n-m}\right)  =a_{n+r}kr\left(  b,a\right)
\ \ \ \ \ \ \ \ \ \ \text{for sufficiently high }N\text{.}
\label{pf.sugawara.b.2b}%
\end{equation}
Finally, for all $N\in\mathbb{N}$, the sum $\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[  b,a\right]  _{r+m}%
a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $
converges\footnote{\textit{Proof.} Let $N\in\mathbb{N}$. The sum
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
$ converges (because it appears on the right hand side of
(\ref{pf.sugawara.b.1})), and the sums $\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}K\omega\left(  b_{r},a_{m}\right)  a_{n-m}$ and
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}a_{m}K\omega\left(
b_{r},a_{n-m}\right)  $ converge (as we have just seen). Hence, the sum%
\begin{align*}
&  \sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}%
+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r}%
,a_{n-m}\right)  \right)  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  -K\omega\left(  b_{r},a_{m}\right)
a_{n-m}-a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
\end{align*}
converges as well (since it is obtained by subtracting the latter two sums
from the former sum componentwise). But this sum clearly simplifies to
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $. Hence,
the sum $\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $
converges, qed.}.

Since the sums $\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}K\omega\left(  b_{r},a_{m}\right)  a_{n-m}$, $\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)  $
and \newline$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $
converge for every $N\in\mathbb{N}$, we have%
\begin{align*}
&  \sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)
\end{align*}
for every $N\in\mathbb{N}$. Hence, for every sufficiently high $N\in
\mathbb{N}$, we have%
\begin{align}
&  \sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
\nonumber\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+\underbrace{\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}%
K\omega\left(  b_{r},a_{m}\right)  a_{n-m}}_{\substack{=kr\left(  b,a\right)
a_{n+r}\text{ for sufficiently high }N\\\text{(by (\ref{pf.sugawara.b.2a}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)  }%
_{\substack{=a_{n+r}kr\left(  b,a\right)  \text{ for sufficiently high
}N\\\text{(by (\ref{pf.sugawara.b.2a}))}}}\nonumber\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+\underbrace{kr\left(  b,a\right)  a_{n+r}+a_{n+r}kr\left(  b,a\right)
}_{=2rk\cdot\left(  b,a\right)  a_{n+r}}\nonumber\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}. \label{pf.sugawara.b.3.sufficiently}%
\end{align}


Now, forget that we fixed $a$. The equality (\ref{pf.sugawara.b.1}) becomes%
\begin{align}
&  \left[  b_{r},L_{n}\right] \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in
B}\underbrace{\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}%
+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r}%
,a_{n-m}\right)  \right)  }_{\substack{=\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[
b,a\right]  _{n+r-m}\right)  +2rk\cdot\left(  b,a\right)  a_{n+r}\\\text{for
sufficiently high }N\text{ (by (\ref{pf.sugawara.b.3.sufficiently}))}%
}}\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}\right) \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}+\sum\limits_{a\in B}a_{m}\left[  b,a\right]
_{n+r-m}\right)  +rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}.
\label{pf.sugawara.b.6}%
\end{align}
But since $\sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  =0$ (by Lemma \ref{lem.sugawara.Kil3}
\textbf{(a)}), we have \newline$\sum\limits_{a\in B}\left(  \left[
b,a\right]  _{\ell}\otimes a_{s}+a_{\ell}\otimes\left[  b,a\right]
_{s}\right)  =0$ for any two integers $\ell$ and $s$. In particular, every
$m\in\mathbb{Z}$ satisfies $\sum\limits_{a\in B}\left(  \left[  b,a\right]
_{m}\otimes a_{n+r-m}+a_{m}\otimes\left[  b,a\right]  _{n+r-m}\right)  =0$.
Hence, every $m\in\mathbb{Z}$ satisfies $\sum\limits_{a\in B}\left(  \left[
b,a\right]  _{m}a_{n+r-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  =0$, so
that $\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}+\sum\limits_{a\in
B}a_{m}\left[  b,a\right]  _{n+r-m}=0$ and thus $\sum\limits_{a\in B}%
a_{m}\left[  b,a\right]  _{n+r-m}=-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}$. Hence, (\ref{pf.sugawara.b.6}) becomes%
\begin{align}
&  \left[  b_{r},L_{n}\right] \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}+\underbrace{\sum\limits_{a\in B}a_{m}\left[
b,a\right]  _{n+r-m}}_{=-\sum\limits_{a\in B}\left[  b,a\right]  _{m}%
a_{n+r-m}}\right)  +rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}%
\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}\right)  +rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}.
\label{pf.sugawara.b.9}%
\end{align}
We will now transform the limit in this equation: In fact,%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\underbrace{\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}\right)  }_{=\sum\limits_{a\in B}\left(  \sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]  _{r+m}a_{n-m}%
-\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]
_{m}a_{n+r-m}\right)  }\\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left[
b,a\right]  _{r+m}a_{n-m}}_{\substack{=\sum\limits_{\left\vert m-r-\dfrac
{n}{2}\right\vert \leq N}\left[  b,a\right]  _{m}a_{n+r-m}\\\text{(here, we
substituted }m-r\text{ for }m\text{ in the sum)}}}-\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\underbrace{\left(
\sum\limits_{\left\vert m-r-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]
_{m}a_{n+r-m}-\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left[
b,a\right]  _{m}a_{n+r-m}\right)  }_{\substack{=-\sum\limits_{\dfrac{n}%
{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}+\sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  b,a\right]
_{m}a_{n+r-m}\\\text{(by Lemma \ref{lem.telescope}, applied to }u_{m}=\left[
b,a\right]  _{m}a_{n+r-m}\text{,}\\\alpha=\dfrac{n}{2}\text{ and }\beta
=\dfrac{n}{2}+r\text{)}}}\\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(  -\sum
\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]
_{m}a_{n+r-m}+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[
b,a\right]  _{m}a_{n+r-m}\right)  .
\end{align*}
Since every $a\in B$ satisfies $\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}%
{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}\rightarrow0$ for $N\rightarrow
\infty$\ \ \ \ \footnote{\textit{Proof.} Let $a\in B$.
\par
Let $w\in M$. From the proof of Theorem \ref{thm.sugawara} \textbf{(a)},
recall the fact that for every $w\in M$, there exists some $\mathbf{M}%
\in\mathbb{N}$ such that every integer $\mathbf{m}\geq\mathbf{M}$ and every
$a\in\mathfrak{g}$ satisfy $at^{\mathbf{m}}\cdot w=0$. Applied to $w=a$, this
yields that there exists some $\mathbf{M}\in\mathbb{N}$ such that%
\begin{equation}
\text{every integer }\mathbf{m}\geq\mathbf{M}\text{ satisfies }at^{\mathbf{m}%
}\cdot w=0. \label{pf.sugawara.foot.1}%
\end{equation}
Consider this $\mathbf{M}$.
\par
Let $N$ be an integer such that $N\geq\mathbf{M}-\dfrac{n}{2}-r$. Then,
$\dfrac{n}{2}+r+N\geq\mathbf{M}$. Now, every integer $m$ such that $\dfrac
{n}{2}-N\leq m<\dfrac{n}{2}+r-N$ must satisfy $n+r-\underbrace{m}_{\geq
\dfrac{n}{2}-N}\leq n+r-\left(  \dfrac{n}{2}-N\right)  =\dfrac{n}{2}%
+r+N\geq\mathbf{M}$ and thus $at^{n+r-m}\cdot w=0$ (by
(\ref{pf.sugawara.foot.1}), applied to $\mathbf{m}=n+r-m$), thus $\left[
b,a\right]  _{m}\underbrace{a_{n+r-m}}_{=at^{n+r-m}}w=\left[  b,a\right]
_{m}\cdot\underbrace{at^{n+r-m}\cdot w}_{=0}=0$. Hence, $\sum\limits_{\dfrac
{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\underbrace{\left[  b,a\right]  _{m}%
a_{n+r-m}w}_{=0}=\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}0=0$.
\par
Now forget that we fixed $N$. We thus have showed that $\sum\limits_{\dfrac
{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}w=0$ for
every integer $N$ such that $N\geq\mathbf{M}-\dfrac{n}{2}-r$. Hence,
$\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]
_{m}a_{n+r-m}w=0$ for every sufficiently large $N$. Thus, $\sum\limits_{\dfrac
{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}%
w\rightarrow0$ for $N\rightarrow\infty$. Since this holds for every $w\in M$,
we thus obtain $\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}\rightarrow0$ for $N\rightarrow\infty$, qed.}, this
becomes%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[  b,a\right]
_{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
-\underbrace{\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}}_{\rightarrow0\text{ for }N\rightarrow\infty}%
+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  b,a\right]
_{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\underbrace{\left[  b,a\right]  _{m}a_{n+r-m}%
}_{=a_{n+r-m}\left[  b,a\right]  _{m}+\left[  \left[  b,a\right]
_{m},a_{n+r-m}\right]  }\\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  a_{n+r-m}\left[  b,a\right]
_{m}+\left[  \left[  b,a\right]  _{m},a_{n+r-m}\right]  \right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(  \sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]
_{m}+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  \left[
b,a\right]  _{m},a_{n+r-m}\right]  \right)  .
\end{align*}
Since every $a\in B$ satisfies $\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}%
{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}\rightarrow0$ for $N\rightarrow
\infty$\ \ \ \ \footnote{\textit{Proof.} Let $w\in M$. From the proof of
Theorem \ref{thm.sugawara} \textbf{(a)}, recall the fact that for every $w\in
M$, there exists some $\mathbf{M}\in\mathbb{N}$ such that every integer
$\mathbf{m}\geq\mathbf{M}$ and every $a\in\mathfrak{g}$ satisfy
$at^{\mathbf{m}}\cdot w=0$. Consider this $\mathbf{M}$. Thus,%
\begin{equation}
\text{every integer }\mathbf{m}\geq\mathbf{M}\text{ and every }a\in
\mathfrak{g}\text{ satisfy }at^{\mathbf{m}}\cdot w=0.
\label{pf.sugawara.foot.2a}%
\end{equation}
\par
Let $a\in B$.
\par
Let $N$ be an integer such that $N\geq\mathbf{M}-\dfrac{n}{2}$. Then,
$\dfrac{n}{2}+N\geq\mathbf{M}$. Now, every integer $m$ such that $\dfrac{n}%
{2}+N<m\leq\dfrac{n}{2}+r+N$ must satisfy $m>\dfrac{n}{2}+N\geq\mathbf{M}$ and
thus $\left[  b,a\right]  t^{m}\cdot w=0$ (by (\ref{pf.sugawara.foot.2a}),
applied to $m$ and $\left[  b,a\right]  $ instead of $\mathbf{m}$ and $a$),
thus $a_{n+r-m}\underbrace{\left[  b,a\right]  _{m}}_{=\left[  b,a\right]
t^{m}}w=a_{n+r-m}\underbrace{\left[  b,a\right]  t^{m}\cdot w}_{=0}=0$. Hence,
$\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\underbrace{a_{n+r-m}%
\left[  b,a\right]  _{m}w}_{=0}=\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}%
{2}+r+N}0=0$.
\par
Now forget that we fixed $N$. We thus have showed that $\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}w=0$ for every
integer $N$ such that $N\geq\mathbf{M}-\dfrac{n}{2}$. Hence, $\sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]
_{m}w=0$ for every sufficiently large $N$. Thus, $\sum\limits_{\dfrac{n}%
{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}w\rightarrow0$
for $N\rightarrow\infty$. Since this holds for every $w\in M$, we thus obtain
$\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[
b,a\right]  _{m}\rightarrow0$ for $N\rightarrow\infty$, qed.}, this becomes%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[  b,a\right]
_{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[
b,a\right]  _{m}}_{\rightarrow0\text{ for }N\rightarrow\infty}+\sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  \left[  b,a\right]
_{m},a_{n+r-m}\right]  \right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\underbrace{\left[  \left[  b,a\right]
_{m},a_{n+r-m}\right]  }_{\substack{=\left[  \left[  b,a\right]  ,a\right]
_{n+r}+K\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)  \\\text{(by
(\ref{pf.sugawara.lie}), applied to }\left[  b,a\right]  \text{, }a\text{,
}m\text{ and }n+r-m\\\text{instead of }x\text{, }y\text{, }n\text{, }%
m\text{)}}}
\end{align*}%
\begin{align*}
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  \left[  \left[  b,a\right]  ,a\right]
_{n+r}+\underbrace{K}_{\substack{=k\\\text{(since }K\text{ acts on }M\text{ as
}k\cdot\operatorname*{id}\text{)}}}\omega\left(  \left[  b,a\right]
_{m},a_{n+r-m}\right)  \right) \\
&  =\lim\limits_{N\rightarrow\infty}\underbrace{\sum\limits_{a\in B}%
\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  \left[  \left[
b,a\right]  ,a\right]  _{n+r}+k\omega\left(  \left[  b,a\right]
_{m},a_{n+r-m}\right)  \right)  }_{=\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac
{n}{2}+r+N}\sum\limits_{a\in B}\left[  \left[  b,a\right]  ,a\right]
_{n+r}+k\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\sum\limits_{a\in
B}\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)  }\\
&  =\lim\limits_{N\rightarrow\infty}\left(  \underbrace{\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}}_{=r\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}}+k\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}%
\sum\limits_{a\in B}\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)
\right) \\
&  =\lim\limits_{N\rightarrow\infty}\left(  r\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  _{n+r}+k\sum\limits_{\dfrac{n}{2}+N<m\leq
\dfrac{n}{2}+r+N}\sum\limits_{a\in B}\omega\left(  \left[  b,a\right]
_{m},a_{n+r-m}\right)  \right)  .
\end{align*}
Since every integer $m$ and every $a\in B$ satisfy $\omega\left(  \left[
b,a\right]  _{m},a_{n+r-m}\right)  =0$\ \ \ \ \footnote{\textit{Proof.} Let
$m$ be an integer, and let $a\in B$. From Lemma \ref{lem.sugawara.Kil3}
\textbf{(c)}, we have $\left(  \left[  b,a\right]  ,a\right)  =0$, so that
$m\left(  \left[  b,a\right]  ,a\right)  =0$. But by the definition of
$\omega$, we have
\begin{align*}
\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)   &  =\left\{
\begin{array}
[c]{c}%
m\left(  \left[  b,a\right]  ,a\right)  ,\text{ if }m=-\left(  n+r-m\right)
;\\
0,\text{ if }m\neq-\left(  n+r-m\right)
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
0,\text{ if }m=-\left(  n+r-m\right)  ;\\
0,\text{ if }m\neq-\left(  n+r-m\right)
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\left(  \left[  b,a\right]
,a\right)  =0\right) \\
&  =0,
\end{align*}
qed.}, this simplifies to%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[  b,a\right]
_{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\left(  r\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  _{n+r}+k\sum\limits_{\dfrac{n}{2}+N<m\leq
\dfrac{n}{2}+r+N}\sum\limits_{a\in B}\underbrace{\omega\left(  \left[
b,a\right]  _{m},a_{n+r-m}\right)  }_{=0}\right) \\
&  =\lim\limits_{N\rightarrow\infty}r\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  _{n+r}=r\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}.
\end{align*}
Thus, (\ref{pf.sugawara.b.9}) becomes%
\begin{align*}
&  \left[  b_{r},L_{n}\right] \\
&  =\dfrac{1}{2}\underbrace{\lim\limits_{N\rightarrow\infty}\sum
\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in
B}\left[  b,a\right]  _{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}\right)  }_{=r\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}}+rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}r\sum\limits_{a\in B}\underbrace{\left[  \left[  b,a\right]
,a\right]  _{n+r}}_{=\left[  \left[  b,a\right]  ,a\right]  t^{n+r}}%
+rk\sum\limits_{a\in B}\left(  b,a\right)  \underbrace{a_{n+r}}_{=at^{n+r}}\\
&  =rt^{n+r}\underbrace{\left(  \dfrac{1}{2}\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(  b,a\right)
a\right)  }_{\substack{=b\\\text{(by Lemma \ref{lem.sugawara.Kil3}
\textbf{(b)})}}}=r\underbrace{t^{n+r}b}_{=b_{n+r}}=rb_{n+r}.
\end{align*}
This proves (\ref{pf.sugawara.step1}) in the case when $r\geq0$. The case when
$r\leq0$ is handled analogously (except that this time we have to apply Lemma
\ref{lem.telescope} to $u_{m}=\left[  b,a\right]  _{m}a_{n+r-m}$,
$\alpha=\dfrac{n}{2}+r$ and $\beta=\dfrac{n}{2}$ instead of applying it to
$u_{m}=\left[  b,a\right]  _{m}a_{n+r-m}$, $\alpha=\dfrac{n}{2}$ and
$\beta=\dfrac{n}{2}+r$). Altogether, the proof of (\ref{pf.sugawara.step1}) is
thus complete.

\textit{2nd step:} It is clear that%
\begin{equation}
\left[  L_{n},a_{m}\right]  =-ma_{n+m}\ \ \ \ \ \ \ \ \ \ \text{for any }%
a\in\mathfrak{g}\text{ and any integers }n\text{ and }m
\label{pf.sugawara.step2.am}%
\end{equation}
(since (\ref{pf.sugawara.step1}) (applied to $r=m$ and $a=b$) yields $\left[
a_{m},L_{n}\right]  =ma_{n+m}$, so that $\left[  L_{n},a_{m}\right]
=-\underbrace{\left[  a_{m},L_{n}\right]  }_{=ma_{n+m}}=-ma_{n+m}$). Also, it
is clear that%
\begin{equation}
\left[  L_{n},K\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for any integer }n
\label{pf.sugawara.step2.K}%
\end{equation}
(since $K$ acts as a scalar on $M$).

\textit{3rd step:} Now, we will prove that%
\begin{equation}
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}-n}%
{12}\delta_{n,-m}k\cdot\sum\limits_{a\in B}\left(  a,a\right)
\ \ \ \ \ \ \ \ \ \ \text{for any integers }n\text{ and }m
\label{pf.sugawara.step3}%
\end{equation}
(as an identity in $\operatorname*{End}M$).

\textit{Proof of (\ref{pf.sugawara.step3}):} We know that every $n\in
\mathbb{Z}$ satisfies%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.  =\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  \label{pf.sugawara.step3.pf.1}%
\end{equation}
(here, we substituted $-m$ for $m$ in the second sum).

Repeat the Second Proof of Proposition \ref{prop.fockvir.answer2}, with the
following changes:

\begin{itemize}
\item Reprove Lemma \ref{lem.fockvir.welldef} with $F_{\mu}$ replaced by $M$
and with an additional ``Let $a\in\mathfrak{g}$ be arbitrary.'' condition.
(The proof will be slightly different from the proof of the original Lemma
\ref{lem.fockvir.welldef} because $M$ is no longer a polynomial ring, but this
time we can use the admissibility of $M$ instead.)

\item Replace every $F_{\mu}$ by $M$.

\item Instead of the equality (\ref{def.fockvir.def}), use the equality
(\ref{pf.sugawara.step3.pf.1}) (which differs from the equality
(\ref{def.fockvir.def}) only in the presence of a $\sum\limits_{a\in B}$
sign). As a consequence, $\sum\limits_{a\in B}$ signs need to be dragged along
through the computations (but they don't complicate the calculation).

\item Instead of using Remark \ref{rmk.fockvir.normal.mn}, use Remark
\ref{rmk.sugawara.normal.mn}.

\item Instead of using Remark \ref{rmk.fockvir.normal.comm} \textbf{(a)}, use
Remark \ref{rmk.sugawara.normal.comm}.

\item Instead of using Remark \ref{rmk.fockvir.normal.K}, use Remark
\ref{rmk.sugawara.normal.K}.

\item Instead of using Proposition \ref{prop.fockvir.answer1}, use
(\ref{pf.sugawara.step2.am}).

\item Instead of the equality $a_{m-\ell}a_{n+\ell}=\left.  :a_{m-\ell
}a_{n+\ell}:\right.  -\left(  n+\ell\right)  \left[  \ell<m\right]
\delta_{m,-n}\operatorname*{id}$, check the equality $a_{m-\ell}a_{n+\ell
}=\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n+\ell\right)  \left[
\ell<m\right]  \delta_{m,-n}\left(  a,a\right)  k$ for every $a\in B$.

\item Instead of the equality $a_{-\ell}a_{m+n+\ell}=\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  -\ell\left[  \ell<0\right]  \delta_{m,-n}%
\operatorname*{id}$, check the equality $a_{-\ell}a_{m+n+\ell}=\left.
:a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[  \ell<0\right]  \delta
_{m,-n}\left(  a,a\right)  k$ for every $a\in B$.
\end{itemize}

Once these changes (most of which are automatic) are made, we have obtained a
proof of (\ref{pf.sugawara.step3}).

\textit{4th step:} From (\ref{pf.sugawara.step3}), it is clear that the
endomorphisms $L_{n}$ for $n\in\mathbb{Z}$ give rise to a $\operatorname*{Vir}%
$-representation on $M$ with central charge%
\[
c=k\cdot\sum\limits_{a\in B}\left(  a,a\right)  .
\]
This proves Theorem \ref{thm.sugawara} \textbf{(c)}.

\textbf{(d)} From (\ref{pf.sugawara.step2.am}) and (\ref{pf.sugawara.step2.K}%
), it follows that the formulas for $L_{n}$ and $c$ we have given in Theorem
\ref{thm.sugawara} extend the action of $\widehat{\mathfrak{g}}$ on $M$ to an
action of $\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$. Theorem
\ref{thm.sugawara} \textbf{(d)} thus is proven.

\begin{noncompile}
Here is the old proof of (\ref{pf.sugawara.step3}). Note that I don't
understand it and I am pretty sure it is partly wrong.

Let $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Now we must prove the Virasoro
relations $\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\left(
\text{something}\right)  $.

Let $R_{n,m}=\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$. Since
$\left[  L_{n},a_{r}\right]  =-ra_{n+r}$, it is clear that $R_{n,m}$ commutes
with $a_{r}$ for all $a$ and $r$. In particular, this yields that $R_{n,m}$
commutes with $L_{0}$ (since $L_{0}$ is made out of $a_{r}$'s). But $\left[
L_{0},R_{n,m}\right]  =\left(  n+m\right)  R_{n,m}$ (since $\left[
a_{r},L_{0}\right]  =ra_{r}$ and thus $\left[  L_{n},L_{0}\right]  =nL_{n}$).
Hence, $R_{n,m}=0$ whenever $n+m\neq0$.

The only case that remains to be checked is $n+m=0$. So we need to compute
$\left[  L_{n},L_{-n}\right]  -2nL_{0}$.

We write%
\begin{align*}
&  \left[  L_{n},L_{-n}\right]  -2nL_{0}=R_{n,-n}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left(
-m\right)  a_{n+m}a_{-n-m}+\left(  n+m\right)  a_{m}a_{-m}\right)  -2nL_{0}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{3n}{2}\right\vert \leq N}\left(  -m+n\right)
a_{m}a_{-m}+\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
n+m\right)  a_{m}a_{-m}\right)  -2nL_{0}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{3n}{2}-N}\left(
m+n\right)  a_{m}a_{-m}}_{\rightarrow0}+\sum\limits_{\dfrac{n}{2}+N\leq
m<\dfrac{3n}{2}+N}\left(  -m+n\right)  a_{m}a_{-m}+\sum\limits_{1\leq
m\leq\dfrac{n}{2}+N}2nmk\left(  a,a\right)  \right) \\
&  =\operatorname*{constant}.
\end{align*}
We calculate this constant: It is enough to calculate it for $n=1$ and for
$n=2$. For $n=1$, it is $\dfrac{1}{2}\left(  -N\left(  N+1\right)  +N\left(
N+1\right)  \right)  \cdot k\sum\limits_{a\in B}\left(  a,a\right)  =0$. For
$n=2$, it is
\[
\dfrac{1}{2}\underbrace{\left(  -N\left(  N+2\right)  -\left(  N+1\right)
\left(  N+3\right)  +2\left(  N+2\right)  \left(  N+1\right)  \right)  }%
_{=1}\cdot k\sum\limits_{a\in B}\left(  a,a\right)  =\dfrac{1}{2}\cdot
k\sum\limits_{a\in B}\left(  a,a\right)  .
\]
Thus, $R_{1,-1}=0$ and $R_{2,-2}=\dfrac{1}{2}\cdot k\sum\limits_{a\in
B}\left(  a,a\right)  $. As a consequence, $R_{n,-n}=\dfrac{n^{3}-n}{12}%
k\sum\limits_{a\in B}\left(  a,a\right)  $. (We could have also obtained this
by direct computation.) This proves that, with $c=k\sum\limits_{a\in B}\left(
a,a\right)  $, we get a representation of $\operatorname*{Vir}$.
\end{noncompile}

\textbf{(e)} Theorem \ref{thm.sugawara} \textbf{(e)} follows immediately from
(\ref{pf.sugawara.step2.am}).

Thus, the proof of Theorem \ref{thm.sugawara} is complete.

We are now going to specialize these results to the case of $\mathfrak{g}$
being simple. In this case, the so-called \textit{dual Coxeter number} of the
simple Lie algebra $\mathfrak{g}$ comes into play. Let us explain what this is:

\begin{definition}
\label{def.dualcox}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. Let $\theta$ be the maximal root of $\mathfrak{g}$. (In other words,
let $\theta$ be the highest weight of the adjoint representation of
$\mathfrak{g}$.) Let $\rho=\dfrac{1}{2}\sum\limits_{\substack{\alpha\text{
root of }\mathfrak{g}\text{;}\\\alpha>0}}\alpha$ be the half-sum of all
positive roots. The \textit{dual Coxeter number} $h^{\vee}$ of $\mathfrak{g}$
is defined by $h^{\vee}=1+\left(  \theta,\rho\right)  $. It is easy to show
that $h^{\vee}$ is a positive integer.
\end{definition}

\begin{definition}
\label{def.standform}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. The \textit{standard form} on $\mathfrak{g}$ will mean the scalar
multiple of the Killing form under which $\left(  \alpha,\alpha\right)  $
(under the inverse form on $\mathfrak{g}^{\ast}$) equals $2$ for long roots
$\alpha$. (We do not care to define what a long root is, but it is enough to
say that the maximal root $\theta$ is a long root, and this is clearly enough
to define the standard form.)

(The \textit{inverse form} of a nondegenerate bilinear form $\left(
\cdot,\cdot\right)  $ on $\mathfrak{g}$ means the bilinear form on
$\mathfrak{g}^{\ast}=\mathfrak{h}^{\ast}\oplus\mathfrak{n}_{+}^{\ast}%
\oplus\mathfrak{n}_{-}^{\ast}$ obtained by dualizing the bilinear form
$\left(  \cdot,\cdot\right)  $ on $\mathfrak{g}=\mathfrak{h}\oplus
\mathfrak{n}_{+}\oplus\mathfrak{n}_{-}$ using itself.)

We are going to denote the standard form by $\left(  \cdot,\cdot\right)  $.
\end{definition}

\begin{lemma}
\label{lem.dualcox}Let $B$ be an orthonormal basis of $\mathfrak{g}$ with
respect to the standard form. Let $C=\sum\limits_{a\in B}a^{2}\in U\left(
\mathfrak{g}\right)  $. This element $C$ is known to be central in $U\left(
\mathfrak{g}\right)  $ (this is easily checked), and is called the
\textit{quadratic Casimir}.

Then:

\textbf{(1)} For every $\lambda\in\mathfrak{h}^{\ast}$, the element $C\in
U\left(  \mathfrak{g}\right)  $ acts on $L_{\lambda}$ by $\left(
\lambda,\lambda+2\rho\right)  \cdot\operatorname*{id}$. (Here, $L_{\lambda}$
means $L_{\lambda}^{+}$, but actually can be replaced by any highest-weight
module with highest weight $\lambda$.)

\textbf{(2)} The element $C\in U\left(  \mathfrak{g}\right)  $ acts on the
adjoint representation $\mathfrak{g}$ by $2h^{\vee}\cdot\operatorname*{id}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.dualcox}.} If $\left(  b_{i}\right)  _{i\in
I}$ is any basis of $\mathfrak{g}$, and $\left(  b_{i}^{\ast}\right)  _{i\in
I}$ is the dual basis of $\mathfrak{g}$ with respect to the standard form
$\left(  \cdot,\cdot\right)  $, then%
\begin{equation}
C=\sum\limits_{i\in I}b_{i}b_{i}^{\ast}. \label{pf.dualcox.Csum}%
\end{equation}
\footnote{This is a well-known property of the quadratic Casimir.}

\textbf{(1)} Let $\lambda\in\mathfrak{h}^{\ast}$.

Let us refine the triangular decomposition $\mathfrak{g}=\mathfrak{h}%
\oplus\mathfrak{n}_{+}\oplus\mathfrak{n}_{-}$ to the weight space
decomposition $\mathfrak{g}=\mathfrak{h}\oplus\left(  \bigoplus\limits_{\alpha
>0}\mathfrak{g}_{\alpha}\right)  \oplus\left(  \bigoplus\limits_{\alpha
<0}\mathfrak{g}_{\alpha}\right)  $, where $\mathfrak{g}_{\alpha}%
=\mathbb{C}e_{\alpha}$ for roots $\alpha>0$, and $\mathfrak{g}_{-\alpha
}=\mathbb{C}f_{\alpha}$ for roots $\alpha>0$. (This is standard theory of
simple Lie algebras.) Normalize the $f_{\alpha}$ in such a way that $\left(
e_{\alpha},f_{\alpha}\right)  =1$. As usual, denote $h_{\alpha}=\left[
e_{\alpha},f_{\alpha}\right]  $ for every root $\alpha>0$.

Fix an orthonormal basis $\left(  x_{i}\right)  _{i\in\left\{
1,2,...,r\right\}  }$ of $\mathfrak{h}$. Clearly, $\left(  x_{i}\right)
_{i\in\left\{  1,2,...,r\right\}  }\cup\left(  e_{\alpha}\right)  _{\alpha
>0}\cup\left(  f_{\alpha}\right)  _{\alpha>0}$ (where the index $\alpha$ runs
over positive roots only) is a basis of $\mathfrak{g}$. Since%
\begin{align*}
\left(  e_{\alpha},x_{i}\right)   &  =\left(  f_{\alpha},x_{i}\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,...,r\right\}  \text{
and roots }\alpha>0;\\
\left(  e_{\alpha},f_{\beta}\right)   &  =0\ \ \ \ \ \ \ \ \ \ \text{for any
two distinct roots }\alpha>0\text{ and }\beta>0;\\
\left(  e_{\alpha},e_{\gamma}\right)   &  =\left(  f_{\alpha},f_{\gamma
}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for any roots }\alpha>0\text{ and
}\gamma>0;\\
\left(  x_{i},x_{j}\right)   &  =\delta_{i,j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i\in\left\{  1,2,...,r\right\}  \text{ and }j\in\left\{  1,2,...,r\right\}
;\\
\left(  e_{\alpha},f_{\alpha}\right)   &  =\left(  f_{\alpha},e_{\alpha
}\right)  =1\ \ \ \ \ \ \ \ \ \ \text{for any root }\alpha>0\text{,}%
\end{align*}
we see that $\left(  x_{i}\right)  _{i\in\left\{  1,2,...,r\right\}  }%
\cup\left(  f_{\alpha}\right)  _{\alpha>0}\cup\left(  e_{\alpha}\right)
_{\alpha>0}$ is the dual basis to this basis $\left(  x_{i}\right)
_{i\in\left\{  1,2,...,r\right\}  }\cup\left(  e_{\alpha}\right)  _{\alpha
>0}\cup\left(  f_{\alpha}\right)  _{\alpha>0}$ with respect to the standard
form $\left(  \cdot,\cdot\right)  $. Thus, (\ref{pf.dualcox.Csum}) yields%
\[
C=\sum\limits_{i=1}^{r}x_{i}^{2}+\sum\limits_{\alpha>0}\left(  f_{\alpha
}e_{\alpha}+e_{\alpha}f_{\alpha}\right)  ,
\]
so that (denoting $v_{\lambda}^{+}$ by $v_{\lambda}$) we have%
\begin{align*}
Cv_{\lambda}  &  =\sum\limits_{i=1}^{r}\underbrace{x_{i}^{2}v_{\lambda}%
}_{=\lambda\left(  x_{i}\right)  ^{2}v_{\lambda}}+\sum\limits_{\alpha
>0}\left(  f_{\alpha}e_{\alpha}+e_{\alpha}f_{\alpha}\right)  v_{\lambda
}=\underbrace{\sum\limits_{i=1}^{r}\lambda\left(  x_{i}\right)  ^{2}%
}_{=\left(  \lambda,\lambda\right)  }v_{\lambda}+\sum\limits_{\alpha>0}\left(
f_{\alpha}\underbrace{e_{\alpha}v_{\lambda}}_{=0}+\underbrace{e_{\alpha
}f_{\alpha}}_{=f_{\alpha}e_{\alpha}+\left[  e_{\alpha},f_{\alpha}\right]
}v_{\lambda}\right) \\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha>0}\left(
f_{\alpha}e_{\alpha}+\left[  e_{\alpha},f_{\alpha}\right]  \right)
v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}f_{\alpha}\underbrace{e_{\alpha}v_{\lambda}}_{=0}+\sum\limits_{\alpha
>0}\underbrace{\left[  e_{\alpha},f_{\alpha}\right]  }_{=h_{\alpha}}%
v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}\underbrace{h_{\alpha}v_{\lambda}}_{=\lambda\left(  h_{\alpha}\right)
v_{\lambda}}=\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}\underbrace{\lambda\left(  h_{\alpha}\right)  }_{=\left(  \lambda
,\alpha\right)  }v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha>0}\left(
\lambda,\alpha\right)  v_{\lambda}=\underbrace{\left(  \left(  \lambda
,\lambda\right)  +\sum\limits_{\alpha>0}\left(  \lambda,\alpha\right)
\right)  }_{\substack{=\left(  \lambda,\lambda+\sum\limits_{\alpha>0}%
\alpha\right)  =\left(  \lambda,\lambda+2\rho\right)  \\\text{(since }%
\sum\limits_{\alpha>0}\alpha=2\rho\text{)}}}v_{\lambda}=\left(  \lambda
,\lambda+2\rho\right)  v_{\lambda}.
\end{align*}
Thus, every $a\in U\left(  \mathfrak{g}\right)  $ satisfies
\begin{align*}
Cav_{\lambda}  &  =a\underbrace{Cv_{\lambda}}_{=\left(  \lambda,\lambda
+2\rho\right)  v_{\lambda}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }C\text{ is
central in }U\left(  \mathfrak{g}\right)  \right) \\
&  =\left(  \lambda,\lambda+2\rho\right)  av_{\lambda}.
\end{align*}
Hence, $C$ acts as $\left(  \lambda,\lambda+2\rho\right)  \cdot
\operatorname*{id}$ on $L_{\lambda}$ (because every element of $L_{\lambda}$
has the form $av_{\lambda}$ for some $a\in U\left(  \mathfrak{g}\right)  $).
This proves Lemma \ref{lem.dualcox} \textbf{(1)}.

\textbf{(2)} We have $\mathfrak{g}=L_{\theta}$, and thus Lemma
\ref{lem.dualcox} \textbf{(1)} yields%
\[
C\mid_{L_{\theta}}=\left(  \theta,\theta+2\rho\right)  =\underbrace{\left(
\theta,\theta\right)  }_{=2}+2\left(  \theta,\rho\right)  =2+2\left(
\theta,\rho\right)  =2h^{\vee}.
\]
This proves Lemma \ref{lem.dualcox} \textbf{(2)}.

Here is a little table of dual Coxeter numbers, depending on the root system
type of $\mathfrak{g}$:

For $A_{n-1}$, we have $h^{\vee}=n$.

For $B_{n}$, we have $h^{\vee}=2n-1$.

For $C_{n}$, we have $h^{\vee}=n+1$.

For $D_{n}$, we have $h^{\vee}=2n-2$.

For $E_{6}$, we have $h^{\vee}=12$.

For $E_{7}$, we have $h^{\vee}=18$.

For $E_{8}$, we have $h^{\vee}=30$.

For $F_{4}$, we have $h^{\vee}=9$.

For $G_{2}$, we have $h^{\vee}=4$.

Every Lie theorist is supposed to remember these by heart.

\begin{lemma}
\label{lem.dualcox.kil}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. Then,%
\[
\operatorname*{Kil}\left(  a,b\right)  =2h^{\vee}\cdot\left(  a,b\right)
\ \ \ \ \ \ \ \ \ \ \text{for any }a,b\in\mathfrak{g}.
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.dualcox.kil}.} Let $B$ be an orthonormal basis
of $\mathfrak{g}$ with respect to the standard form. Define the quadratic
Casimir $C=\sum\limits_{a\in B}a^{2}$ as in Lemma \ref{lem.dualcox}. Then,%
\[
\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  C\right)  =\sum\limits_{a\in
B}\underbrace{\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  a^{2}\right)
}_{=\operatorname*{Tr}\left(  \left(  \operatorname*{ad}a\right)  \circ\left(
\operatorname*{ad}a\right)  \right)  =\operatorname*{Kil}\left(  a,a\right)
}=\sum\limits_{a\in B}\operatorname*{Kil}\left(  a,a\right)  .
\]
Comparing this with%
\begin{align*}
\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  C\right)   &  =2h^{\vee
}\underbrace{\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(
\operatorname*{id}\right)  }_{=\dim\mathfrak{g}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }C\mid_{\mathfrak{g}}=2h^{\vee}\operatorname*{id}\text{ by Lemma
\ref{lem.dualcox} \textbf{(2)}}\right) \\
&  =2h^{\vee}\underbrace{\dim\mathfrak{g}}_{\substack{=\left\vert B\right\vert
=\sum\limits_{a\in B}1=\sum\limits_{a\in B}\left(  a,a\right)  \\\text{(since
every }a\in B\text{ satisfies }\left(  a,a\right)  =1\text{)}}}=2h^{\vee}%
\sum\limits_{a\in B}\left(  a,a\right)  ,
\end{align*}
we obtain $\sum\limits_{a\in B}\operatorname*{Kil}\left(  a,a\right)
=2h^{\vee}\sum\limits_{a\in B}\left(  a,a\right)  $. Since
$\operatorname*{Kil}$ is a scalar multiple of $\left(  \cdot,\cdot\right)  $
(because there is only one $\mathfrak{g}$-invariant symmetric bilinear form on
$\mathfrak{g}$ up to scaling), this yields $\operatorname*{Kil}=2h^{\vee}%
\cdot\left(  \cdot,\cdot\right)  $ (because $\sum\limits_{a\in B}%
\underbrace{\left(  a,a\right)  }_{=1}=\sum\limits_{a\in B}1=\left\vert
B\right\vert \neq0$). Lemma \ref{lem.dualcox.kil} is proven.

So let us now look at the Sugawara construction when $\mathfrak{g}$ is simple
finite-dimensional. First of all, $k$ is non-critical if and only if
$k\neq-h^{\vee}$. (The value $k=-h^{\vee}$ is called the \textit{critical
level}.)

If $B^{\prime}$ is an orthonormal basis under $\left(  \cdot,\cdot\right)  $
(rather than under $k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}%
\operatorname*{Kil}=\left(  k+h^{\vee}\right)  \left(  \cdot,\cdot\right)  $),
then we have%
\begin{align}
L_{n}  &  =\dfrac{1}{2\left(  k+h^{\vee}\right)  }\sum\limits_{a\in B^{\prime
}}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.
\ \ \ \ \ \ \ \ \ \ \text{and}\nonumber\\
c  &  =\dfrac{k}{k+h^{\vee}}\underbrace{\sum\limits_{a\in B^{\prime}}\left(
a,a\right)  }_{\substack{=\left\vert B^{\prime}\right\vert \\\text{(since
}\left(  a,a\right)  =1\text{ for every }a\in B^{\prime}\text{)}}}=\dfrac
{k}{k+h^{\vee}}\underbrace{\left\vert B^{\prime}\right\vert }_{=\dim
\mathfrak{g}}=\dfrac{k\dim\mathfrak{g}}{k+h^{\vee}}.
\label{thm.sugawara.simple.c}%
\end{align}
In particular, this induces an internal grading on any $\widehat{\mathfrak{g}%
}$-module which is a quotient of $M_{\lambda}^{+}$ by eigenvalues of $L_{0}$,
whenever $\lambda$ is a weight of $\widehat{\mathfrak{g}}$. This is a grading
by complex numbers, since eigenvalues of $L_{0}$ are not necessarily integers.
(Note that this does not work for general admissible modules in lieu of
quotients of $M_{\lambda}^{+}$.)

What happens at the critical level $k=-h^{\vee}$ ? The above formulas with
$k+h^{\vee}$ in the denominators clearly don't work at this level anymore. We
can, however, remove the denominators, i. e., consider the operators%
\[
T_{n}=\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  .
\]
Then, the same calculations as we did in the proof of Theorem
\ref{thm.sugawara} tell us that these $T_{n}$ satisfy $\left[  T_{n}%
,a_{m}\right]  =0$ and $\left[  T_{n},T_{m}\right]  =0$; they are thus central
``elements'' of $U\left(  \widehat{\mathfrak{g}}\right)  $ (except that they
are not actually elements of $U\left(  \widehat{\mathfrak{g}}\right)  $, but
of some completion of $U\left(  \widehat{\mathfrak{g}}\right)  $ acting on
admissible modules).

For any complex numbers $\gamma_{1},\gamma_{2},\gamma_{3},...$, we can
construct a $\widehat{\mathfrak{g}}$-module $M_{\lambda}\diagup\left(
\sum\limits_{m\geq1}\left(  \left(  T_{m}-\gamma_{m}\right)  M_{\lambda
}\right)  \right)  $, which does not have a grading. So, at the critical
level, we do not automatically get gradings on quotients of $M_{\lambda}$
anymore. This is one reason why representations at the critical level are
considered more difficult than those at non-critical levels.

\subsection{The Sugawara construction and unitarity}

We now will show that the Sugawara construction preserves unitarity:

\begin{proposition}
Consider the situation of Theorem \ref{thm.sugawara}. If $M$ is a unitary
admissible module for $\widehat{\mathfrak{g}}$, then $M$ is a unitary
$\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$-module. (We recall that the
Virasoro algebra had its unitary structure given by $L_{n}^{\dag}=L_{-n}$.)
\end{proposition}

But for $M$ to be unitary for $\widehat{\mathfrak{g}}$, we need $k\in
\mathbb{Z}_{+}$ (this is easy to prove; we proved it for $\mathfrak{sl}_{n}$,
and the general case is similar). Since for $k=0$, there is only the trivial
representation, we really must require $k\geq1$ to get something interesting.
And since $c=\dfrac{k\dim\mathfrak{g}}{k+h^{\vee}}$, the $c$ is then $\geq1$,
since $\dim\mathfrak{g}\geq1+h^{\vee}$. These modules are already known to us
to be unitary, so this construction does not help us in constructing new
unitary modules.

But there is a way to amend this by a variation of the Sugawara construction:
the Goddard-Kent-Olive construction.

\subsection{The Goddard-Kent-Olive construction (a.k.a. the coset
construction)}

\begin{definition}
\label{def.goddardkentolive}Let $\mathfrak{g}$ and $\mathfrak{p}$ be two
finite-dimensional Lie algebras such that $\mathfrak{g}\supseteq\mathfrak{p}$.
Let $\left(  \cdot,\cdot\right)  $ be a $\mathfrak{g}$-invariant form
(possibly degenerate) on $\mathfrak{g}$. We can restrict this form to
$\mathfrak{p}$, and obtain a $\mathfrak{p}$-invariant form on $\mathfrak{p}$.
Construct an affine Lie algebra $\widehat{\mathfrak{g}}$ as in Definition
\ref{def.sugawara} using the $\mathfrak{g}$-invariant form $\left(
\cdot,\cdot\right)  $ on $\mathfrak{g}$, and similarly construct an affine Lie
algebra $\widehat{\mathfrak{p}}$ using the restriction of this form to
$\mathfrak{p}$. Then, $\widehat{\mathfrak{g}}\supseteq\widehat{\mathfrak{p}}$.
Choose a level $k$ which is non-critical for both $\mathfrak{g}$ and
$\mathfrak{p}$.

Let $M$ be an admissible $\widehat{\mathfrak{g}}$-module at level $k$. Then,
$M$ automatically becomes an admissible $\widehat{\mathfrak{p}}$-module at
level $k$. Hence, on $M$, we have two Virasoro actions: one which is obtained
from the $\widehat{\mathfrak{g}}$-action, and one which is obtained from the
$\widehat{\mathfrak{p}}$-action. We will denote these actions by $\left(
L_{i}^{\mathfrak{g}}\right)  _{i\in\mathbb{Z}}$ and $\left(  L_{i}%
^{\mathfrak{p}}\right)  _{i\in\mathbb{Z}}$, respectively (that is, for every
$i\in\mathbb{Z}$, we denote by $L_{i}^{\mathfrak{g}}$ the action of $L_{i}%
\in\operatorname*{Vir}$ obtained from the $\widehat{\mathfrak{g}}$-module
structure on $M$, and we denote by $L_{i}^{\mathfrak{p}}$ the action of
$L_{i}\in\operatorname*{Vir}$ obtained from the $\widehat{\mathfrak{p}}%
$-module structure on $M$), and we will denote their central charges by
$c_{\mathfrak{g}}$ and $c_{\mathfrak{p}}$, respectively.
\end{definition}

\begin{theorem}
\label{thm.goddardkentolive}Consider the situation of Definition
\ref{def.goddardkentolive}. Let $L_{i}=L_{i}^{\mathfrak{g}}-L_{i}%
^{\mathfrak{p}}$ for all $i\in\mathbb{Z}$.

\textbf{(a)} Then, $\left(  L_{i}\right)  _{i\in\mathbb{Z}}$ is a
$\operatorname*{Vir}$-action on $M$ with central charge $c=c_{\mathfrak{g}%
}-c_{\mathfrak{p}}$.

\textbf{(b)} Also, $\left[  L_{n},\widehat{p}\right]  =0$ for all
$\widehat{p}\in\widehat{\mathfrak{p}}$ and $n\in\mathbb{Z}$.

\textbf{(c)} Moreover, $\left[  L_{n},L_{m}^{\mathfrak{p}}\right]  =0$ for all
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.goddardkentolive}.} \textbf{(b)} Let
$n\in\mathbb{Z}$. Every $p\in\mathfrak{p}$ and $m\in\mathbb{Z}$ satisfy%
\[
\left[  \underbrace{L_{n}}_{=L_{n}^{\mathfrak{g}}-L_{n}^{\mathfrak{p}}}%
,p_{m}\right]  =\underbrace{\left[  L_{n}^{\mathfrak{g}},p_{m}\right]
}_{\substack{=-mp_{n+m}\\\text{(by Theorem \ref{thm.sugawara} \textbf{(e)}%
,}\\\text{applied to }p\text{ instead of }a\text{)}}}-\underbrace{\left[
L_{n}^{\mathfrak{p}},p_{m}\right]  }_{\substack{=-mp_{n+m}\\\text{(by Theorem
\ref{thm.sugawara} \textbf{(e)},}\\\text{applied to }p\text{ and }%
\mathfrak{p}\text{ instead of }a\text{ and }\mathfrak{g}\text{)}}}=\left(
-mp_{m+n}\right)  -\left(  -mp_{m+n}\right)  =0.
\]
Combined with the fact that $\left[  L_{n},K\right]  =0$ (this is trivial,
since $K$ acts as $k\cdot\operatorname*{id}$ on $M$), this yields that
$\left[  L_{n},\widehat{p}\right]  =0$ for all $\widehat{p}\in
\widehat{\mathfrak{p}}$ and $n\in\mathbb{Z}$ (because every $\widehat{p}%
\in\widehat{\mathfrak{p}}$ is a $\mathbb{C}$-linear combination of terms of
the form $p_{m}$ (with $p\in\mathfrak{p}$ and $m\in\mathbb{Z}$) and $K$).
Thus, Theorem \ref{thm.goddardkentolive} \textbf{(b)} is proven.

\textbf{(c)} Let $n\in\mathbb{Z}$. We recall that $L_{n}^{\mathfrak{p}}$ was
defined by $L_{n}^{\mathfrak{p}}=\dfrac{1}{2}\sum\limits_{a\in B}%
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  $, where $B$ is an
orthonormal basis of $\mathfrak{p}$ with respect to a certain bilinear form on
$\mathfrak{p}$. Thus, $L_{n}^{\mathfrak{p}}$ is a sum of products of elements
of $\widehat{\mathfrak{p}}$ (or, more precisely, their actions on $M$).

Now, let $m\in\mathbb{Z}$. We have just seen that $L_{n}^{\mathfrak{p}}$ is a
sum of products of elements of $\widehat{\mathfrak{p}}$ (or, more precisely,
their actions on $M$). Similarly, $L_{m}^{\mathfrak{p}}$ is a sum of products
of elements of $\widehat{\mathfrak{p}}$ (or, more precisely, their actions on
$M$). Since we know that $L_{n}$ commutes with every element of
$\widehat{\mathfrak{p}}$ (due to Theorem \ref{thm.goddardkentolive}
\textbf{(b)}), this yields that $L_{n}$ commutes with $L_{m}^{\mathfrak{p}}$.
In other words, $\left[  L_{n},L_{m}^{\mathfrak{p}}\right]  =0$. Theorem
\ref{thm.goddardkentolive} \textbf{(c)} is thus established.

\textbf{(a)} Any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfy%
\begin{align*}
&  \left[  L_{n},\underbrace{L_{m}}_{=L_{m}^{\mathfrak{g}}-L_{m}%
^{\mathfrak{p}}}\right] \\
&  =\left[  L_{n},L_{m}^{\mathfrak{g}}-L_{m}^{\mathfrak{p}}\right]  =\left[
L_{n},L_{m}^{\mathfrak{g}}\right]  -\underbrace{\left[  L_{n},L_{m}%
^{\mathfrak{p}}\right]  }_{\substack{=0\\\text{(by Theorem
\ref{thm.goddardkentolive} \textbf{(c)})}}}=\left[  \underbrace{L_{n}}%
_{=L_{n}^{\mathfrak{g}}-L_{n}^{\mathfrak{p}}},L_{m}^{\mathfrak{g}}\right] \\
&  =\left[  L_{n}^{\mathfrak{g}}-L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}%
}\right]  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
-\underbrace{\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}}\right]
}_{\substack{=\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}}-L_{m}%
^{\mathfrak{p}}\right]  +\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}%
}\right]  \\\text{(since }L_{m}^{\mathfrak{g}}=\left(  L_{m}^{\mathfrak{g}%
}-L_{m}^{\mathfrak{p}}\right)  +L_{m}^{\mathfrak{p}}\text{)}}}\\
&  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]  -\left[
L_{n}^{\mathfrak{p}},\underbrace{L_{m}^{\mathfrak{g}}-L_{m}^{\mathfrak{p}}%
}_{=L_{m}}\right]  -\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}}\right]
\\
&  =\underbrace{\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
}_{\substack{=\left(  n-m\right)  L_{n+m}^{\mathfrak{g}}-\dfrac{n^{3}-n}%
{12}c_{\mathfrak{g}}\delta_{n,-m}\\\text{(by Theorem \ref{thm.sugawara}
\textbf{(c)})}}}-\underbrace{\left[  L_{n}^{\mathfrak{p}},L_{m}\right]
}_{=-\left[  L_{m},L_{n}^{\mathfrak{p}}\right]  }-\underbrace{\left[
L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}}\right]  }_{\substack{=\left(
n-m\right)  L_{n+m}^{\mathfrak{p}}-\dfrac{n^{3}-n}{12}c_{\mathfrak{p}}%
\delta_{n,-m}\\\text{(by Theorem \ref{thm.sugawara} \textbf{(c)}%
,}\\\text{applied to }\mathfrak{p}\text{ instead of }\mathfrak{g}\text{)}}}\\
&  =\left(  \left(  n-m\right)  L_{n+m}^{\mathfrak{g}}-\dfrac{n^{3}-n}%
{12}c_{\mathfrak{g}}\delta_{n,-m}\right)  +\left[  L_{m},L_{n}^{\mathfrak{p}%
}\right]  -\left(  \left(  n-m\right)  L_{n+m}^{\mathfrak{p}}-\dfrac{n^{3}%
-n}{12}c_{\mathfrak{p}}\delta_{n,-m}\right) \\
&  =\left(  n-m\right)  \underbrace{\left(  L_{n+m}^{\mathfrak{g}}%
-L_{n+m}^{\mathfrak{p}}\right)  }_{=L_{n+m}}-\dfrac{n^{3}-n}{12}\left(
c_{\mathfrak{g}}-c_{\mathfrak{p}}\right)  \delta_{n,-m}+\underbrace{\left[
L_{m},L_{n}^{\mathfrak{p}}\right]  }_{\substack{=0\\\text{(by Theorem
\ref{thm.goddardkentolive} \textbf{(c)},}\\\text{applied to }m\text{ and
}n\text{ instead of }n\text{ and }m\text{)}}}\\
&  =\left(  n-m\right)  L_{n+m}-\dfrac{n^{3}-n}{12}\left(  c_{\mathfrak{g}%
}-c_{\mathfrak{p}}\right)  \delta_{n,-m}.
\end{align*}
Hence, $\left(  L_{i}\right)  _{i\in\mathbb{Z}}$ is a $\operatorname*{Vir}%
$-action on $M$ with central charge $c=c_{\mathfrak{g}}-c_{\mathfrak{p}}$.
Theorem \ref{thm.goddardkentolive} \textbf{(a)} is thus proven. This completes
the proof of Theorem \ref{thm.goddardkentolive}.

\begin{example}
Let $\mathfrak{a}$ be a simple finite-dimensional Lie algebra. Let
$\mathfrak{g}=\mathfrak{a}\oplus\mathfrak{a}$, and let $\mathfrak{p}%
=\mathfrak{a}_{\operatorname*{diag}}\subseteq\mathfrak{a}\oplus\mathfrak{a}$
(where $\mathfrak{a}_{\operatorname*{diag}}$ denotes the Lie subalgebra
$\left\{  \left(  x,x\right)  \ \mid\ x\in\mathfrak{a}\right\}  $ of
$\mathfrak{a}\oplus\mathfrak{a}$). Consider the standard form $\left(
\cdot,\cdot\right)  $ on $\mathfrak{a}$. Define a symmetric bilinear form on
$\mathfrak{a}\oplus\mathfrak{a}$ as the direct sum of the standard forms on
$\mathfrak{a}$ and $\mathfrak{a}$.

Let $V^{\prime}$ and $V^{\prime\prime}$ be admissible $\widehat{\mathfrak{a}}%
$-modules at levels $k^{\prime}$ and $k^{\prime\prime}$. Theorem
\ref{thm.sugawara} endows these vector spaces $V^{\prime}$ and $V^{\prime
\prime}$ with $\operatorname*{Vir}$-module structures. These
$\operatorname*{Vir}$-module structures have central charges $c_{\mathfrak{a}%
}^{\prime}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}}$ and
$c_{\mathfrak{a}}^{\prime\prime}=\dfrac{k^{\prime\prime}\dim\mathfrak{a}%
}{k^{\prime\prime}+h^{\vee}}$, respectively (by (\ref{thm.sugawara.simple.c}%
)). Let $\left(  L_{i}^{\prime}\right)  _{i\in\mathbb{Z}}$ and $\left(
L_{i}^{\prime\prime}\right)  _{i\in\mathbb{Z}}$ denote the actions of
$\operatorname*{Vir}$ on these modules.

Then, $V^{\prime}\otimes V^{\prime\prime}$ is an admissible
$\widehat{\mathfrak{g}}$-module at level $k^{\prime}+k^{\prime\prime}$. Thus,
by Theorem \ref{thm.sugawara}, this vector space $V^{\prime}\otimes
V^{\prime\prime}$ becomes a $\operatorname*{Vir}$-module. The action $\left(
L_{i}^{\mathfrak{g}}\right)  _{i\in\mathbb{Z}}$ of $\operatorname*{Vir}$ on
this $\operatorname*{Vir}$-module $V^{\prime}\otimes V^{\prime\prime}$ is
given by $L_{i}^{\mathfrak{g}}=L_{i}^{\prime}+L_{i}^{\prime\prime}$ (or, more
precisely, $L_{i}^{\mathfrak{g}}=L_{i}^{\prime}\otimes\operatorname*{id}%
+\operatorname*{id}\otimes L_{i}^{\prime\prime}$). The central charge
$c_{\mathfrak{g}}$ of this $\operatorname*{Vir}$-module $V^{\prime}\otimes
V^{\prime\prime}$ is
\[
c_{\mathfrak{g}}=c_{\mathfrak{a}}^{\prime}+c_{\mathfrak{a}}^{\prime\prime
}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}}+\dfrac
{k^{\prime\prime}\dim\mathfrak{a}}{k^{\prime\prime}+h^{\vee}}.
\]


Since $\widehat{\mathfrak{p}}=\widehat{\mathfrak{a}}$ acts on $V^{\prime
}\otimes V^{\prime\prime}$ by diagonal action, we also get a
$\operatorname*{Vir}$-module structure $\left(  L_{i}^{\mathfrak{p}}\right)
_{i\in\mathbb{Z}}$ on $V^{\prime}\otimes V^{\prime\prime}$ by applying Theorem
\ref{thm.sugawara} to $\mathfrak{p}$ instead of $\mathfrak{g}$. The central
charge of this $\operatorname*{Vir}$-module is
\[
c_{\mathfrak{p}}=\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime}+k^{\prime
\prime}+h^{\vee}}\dim\mathfrak{a}%
\]
(since the level of the $\widehat{\mathfrak{p}}$-module $V^{\prime}\otimes
V^{\prime\prime}$ is $k^{\prime}+k^{\prime\prime}$).

Thus, the central charge $c$ of the $\operatorname*{Vir}$-action on
$V^{\prime}\otimes V^{\prime\prime}$ given by Theorem
\ref{thm.goddardkentolive} is%
\begin{align*}
c  &  =c_{\mathfrak{a}}^{\prime}+c_{\mathfrak{a}}^{\prime\prime}%
-c_{\mathfrak{p}}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}%
}+\dfrac{k^{\prime\prime}\dim\mathfrak{a}}{k^{\prime\prime}+h^{\vee}}%
-\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime}+k^{\prime\prime}+h^{\vee}}%
\dim\mathfrak{a}\\
&  =\left(  \dfrac{k^{\prime}}{k^{\prime}+h^{\vee}}+\dfrac{k^{\prime\prime}%
}{k^{\prime\prime}+h^{\vee}}-\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime
}+k^{\prime\prime}+h^{\vee}}\right)  \dim\mathfrak{a}.
\end{align*}


We can use this construction to obtain, for every positive integer $m$, a
unitary representation of $\operatorname*{Vir}$ with central charge
$1-\dfrac{6}{\left(  m+2\right)  \left(  m+3\right)  }$: In fact, let
$\mathfrak{a}=\mathfrak{sl}_{2}$, so that $h^{\vee}=2$, and let $k^{\prime}=1$
and $k^{\prime\prime}=m$. Then,%
\[
c=3\left(  \dfrac{1}{3}+\dfrac{m}{m+2}-\dfrac{m+1}{m+3}\right)  =1-\dfrac
{6}{\left(  m+2\right)  \left(  m+3\right)  }.
\]
So we get unitary representations of $\operatorname*{Vir}$ with central charge
$c$ for these values of $c$.
\end{example}

\subsection{\label{subsect.prelims}Preliminaries to simple and Kac-Moody Lie
algebras}

Our next goal is defining and studying the Kac-Moody Lie algebras. Before we
do this, however, we will recollect some properties of simple
finite-dimensional Lie algebras (which are, in some sense, the prototypical
Kac-Moody Lie algebras); and yet before that, we show some general results
from the theory of Lie algebras which will be used in our later proofs.

[This whole Section \ref{subsect.prelims} is written by Darij and aims at
covering the gap between introductory courses in Lie algebras and Etingof's
class. It states some folklore facts about Lie algebras which will be used later.]

\subsubsection{A basic property of
\texorpdfstring{$\mathfrak{sl}_{2}$}{sl-2}-modules}

We begin with a lemma from the representation theory of $\mathfrak{sl}_{2}$:

\begin{lemma}
\label{lem.serre-gen.sl2}Let $e$, $f$ and $h$ mean the classical basis
elements of $\mathfrak{sl}_{2}$. Let $\lambda\in\mathbb{C}$. We consider any
$\mathfrak{sl}_{2}$-module as a $U\left(  \mathfrak{sl}_{2}\right)  $-module.

\textbf{(a)} Let $V$ be an $\mathfrak{sl}_{2}$-module. Let $x\in V$ be such
that $ex=0$ and $hx=\lambda x$. Then, every $n\in\mathbb{N}$ satisfies
$e^{n}f^{n}x=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x$.

\textbf{(b)} Let $V$ be an $\mathfrak{sl}_{2}$-module. Let $x\in V$ be such
that $fx=0$ and $hx=\lambda x$. Then, every $n\in\mathbb{N}$ satisfies
$f^{n}e^{n}x=n!\lambda\left(  \lambda+1\right)  ...\left(  \lambda+n-1\right)
x$.

\textbf{(c)} Let $V$ be a finite-dimensional $\mathfrak{sl}_{2}$-module. Let
$x$ be a nonzero element of $V$ satisfying $ex=0$ and $hx=\lambda x$. Then,
$\lambda\in\mathbb{N}$ and $f^{\lambda+1}x=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.serre-gen.sl2}.} \textbf{(a)} \textit{1st
step:} We will see that%
\begin{equation}
hf^{m}x=\left(  \lambda-2m\right)  f^{m}x\ \ \ \ \ \ \ \ \ \ \text{for every
}m\in\mathbb{N}. \label{pf.serre-gen.sl2.1}%
\end{equation}


\textit{Proof of (\ref{pf.serre-gen.sl2.1}):} We will prove
(\ref{pf.serre-gen.sl2.1}) by induction over $m$:

\textit{Induction base:} For $m=0$, we have $hf^{m}x=hf^{0}x=hx=\lambda x$ and
$\left(  \lambda-2m\right)  f^{m}x=\left(  \lambda-2\cdot0\right)
f^{0}x=\lambda x$, so that $hf^{m}x=\left(  \lambda-2m\right)  f^{m}x$ holds
for $m=0$. In other words, (\ref{pf.serre-gen.sl2.1}) holds for $m=0$. This
completes the induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$. Assume that
(\ref{pf.serre-gen.sl2.1}) holds for $m=M$. We must then prove that
(\ref{pf.serre-gen.sl2.1}) holds for $m=M+1$ as well.

Since (\ref{pf.serre-gen.sl2.1}) holds for $m=M$, we have $hf^{M}x=\left(
\lambda-2M\right)  f^{M}x$. Now,%
\begin{align*}
h\underbrace{f^{M+1}}_{=ff^{M}}x  &  =\underbrace{hf}_{=fh+\left[  h,f\right]
}f^{M}x=\left(  fh+\left[  h,f\right]  \right)  f^{M}x=f\underbrace{hf^{M}%
x}_{=\left(  \lambda-2M\right)  f^{M}x}+\underbrace{\left[  h,f\right]
}_{=-2f}f^{M}x\\
&  =\left(  \lambda-2M\right)  \underbrace{ff^{M}}_{=f^{M+1}}%
x-2\underbrace{ff^{M}}_{=f^{M+1}}x=\left(  \lambda-2M\right)  f^{M+1}%
x-2f^{M+1}x\\
&  =\underbrace{\left(  \lambda-2M-2\right)  }_{=\lambda-2\left(  M+1\right)
}f^{M+1}x=\left(  \lambda-2\left(  M+1\right)  \right)  f^{M+1}x.
\end{align*}
Thus, (\ref{pf.serre-gen.sl2.1}) holds for $m=M+1$ as well. This completes the
induction step. The induction proof of (\ref{pf.serre-gen.sl2.1}) is thus complete.

\textit{2nd step:} We will see that%
\begin{equation}
ef^{m}x=m\left(  \lambda-m+1\right)  f^{m-1}x\ \ \ \ \ \ \ \ \ \ \text{for
every positive }m\in\mathbb{N}. \label{pf.serre-gen.sl2.2}%
\end{equation}


\textit{Proof of (\ref{pf.serre-gen.sl2.2}):} We will prove
(\ref{pf.serre-gen.sl2.2}) by induction over $m$:

\textit{Induction base:} For $m=1$, we have%
\[
ef^{m}x=\underbrace{ef^{1}}_{=ef=\left[  e,f\right]  +fe}x=\left(  \left[
e,f\right]  +fe\right)  x=\underbrace{\left[  e,f\right]  }_{=h}%
x+f\underbrace{ex}_{=0}=hx+f0=hx=\lambda x
\]
and $m\left(  \lambda-m+1\right)  f^{m-1}x=1\underbrace{\left(  \lambda
-1+1\right)  }_{=\lambda}\underbrace{f^{1-1}}_{=1}x=\lambda x$, so that
$ef^{m}x=m\left(  \lambda-m+1\right)  f^{m-1}x$ holds for $m=1$. In other
words, (\ref{pf.serre-gen.sl2.2}) holds for $m=1$. This completes the
induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$ be positive. Assume that
(\ref{pf.serre-gen.sl2.2}) holds for $m=M$. We must then prove that
(\ref{pf.serre-gen.sl2.2}) holds for $m=M+1$ as well.

Since (\ref{pf.serre-gen.sl2.2}) holds for $m=M$, we have $ef^{M}x=M\left(
\lambda-M+1\right)  f^{M-1}x$. Now,%
\begin{align*}
e\underbrace{f^{M+1}}_{=ff^{M}}x  &  =\underbrace{ef}_{=fe+\left[  e,f\right]
}f^{M}x=\left(  fe+\left[  e,f\right]  \right)  f^{M}x=f\underbrace{ef^{M}%
}_{=M\left(  \lambda-M+1\right)  f^{M-1}x}x+\underbrace{\left[  e,f\right]
}_{=h}f^{M}x\\
&  =M\left(  \lambda-M+1\right)  \underbrace{ff^{M-1}}_{=f^{M}}%
x+\underbrace{hf^{M}x}_{\substack{=\left(  \lambda-2M\right)  f^{M}%
x\\\text{(by (\ref{pf.serre-gen.sl2.1}), applied to }m=M\text{)}}}\\
&  =M\left(  \lambda-M+1\right)  f^{M}x+\left(  \lambda-2M\right)
f^{M}x=\underbrace{\left(  M\left(  \lambda-M+1\right)  +\left(
\lambda-2M\right)  \right)  }_{=\left(  M+1\right)  \left(  \lambda-\left(
M+1\right)  +1\right)  }f^{M}x\\
&  =\left(  M+1\right)  \left(  \lambda-\left(  M+1\right)  +1\right)  f^{M}x.
\end{align*}
Thus, (\ref{pf.serre-gen.sl2.2}) holds for $m=M+1$ as well. This completes the
induction step. The induction proof of (\ref{pf.serre-gen.sl2.2}) is thus complete.

\textit{3rd step:} We will see that%
\begin{equation}
e^{n}f^{n}x=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{N}.
\label{pf.serre-gen.sl2.3}%
\end{equation}


\textit{Proof of (\ref{pf.serre-gen.sl2.3}):} We will prove
(\ref{pf.serre-gen.sl2.3}) by induction over $n$:

\textit{Induction base:} For $n=0$, we have $e^{n}f^{n}x=e^{0}f^{0}x=x$ and
$n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x=\underbrace{0!}_{=1}\underbrace{\lambda\left(  \lambda-1\right)  ...\left(
\lambda-0+1\right)  }_{=\left(  \text{empty product}\right)  =1}x=x$, so that
$e^{n}f^{n}x=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x$ holds for $n=0$. In other words, (\ref{pf.serre-gen.sl2.3}) holds for
$n=0$. This completes the induction base.

\textit{Induction step:} Let $N\in\mathbb{N}$. Assume that
(\ref{pf.serre-gen.sl2.3}) holds for $n=N$. We must then prove that
(\ref{pf.serre-gen.sl2.3}) holds for $n=N+1$ as well.

Since (\ref{pf.serre-gen.sl2.3}) holds for $n=N$, we have $e^{N}%
f^{N}x=N!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-N+1\right)  x$.
Now,%
\begin{align*}
\underbrace{e^{N+1}}_{=e^{N}e}f^{N+1}x  &  =e^{N}\underbrace{ef^{N+1}%
x}_{\substack{=\left(  N+1\right)  \left(  \lambda-\left(  N+1\right)
+1\right)  f^{\left(  N+1\right)  -1}x\\\text{(by (\ref{pf.serre-gen.sl2.2}),
applied to }m=N+1\text{)}}}=\left(  N+1\right)  \left(  \lambda-\left(
N+1\right)  +1\right)  e^{N}\underbrace{f^{\left(  N+1\right)  -1}}_{=f^{N}%
}x\\
&  =\left(  N+1\right)  \left(  \lambda-\left(  N+1\right)  +1\right)
\underbrace{e^{N}f^{N}x}_{=N!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-N+1\right)  x}\\
&  =\left(  N+1\right)  \left(  \lambda-\left(  N+1\right)  +1\right)  \cdot
N!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-N+1\right)  x\\
&  =\underbrace{\left(  \left(  N+1\right)  \cdot N!\right)  }_{=\left(
N+1\right)  !}\cdot\underbrace{\left(  \lambda\left(  \lambda-1\right)
...\left(  \lambda-N+1\right)  \right)  \cdot\left(  \lambda-\left(
N+1\right)  +1\right)  }_{=\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  N+1\right)  +1\right)  }x\\
&  =\left(  N+1\right)  !\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  N+1\right)  +1\right)  x.
\end{align*}
Thus, (\ref{pf.serre-gen.sl2.3}) holds for $n=N+1$ as well. This completes the
induction step. The induction proof of (\ref{pf.serre-gen.sl2.3}) is thus complete.

Lemma \ref{lem.serre-gen.sl2} \textbf{(a)} immediately follows from
(\ref{pf.serre-gen.sl2.3}).

\textbf{(b)} The proof of Lemma \ref{lem.serre-gen.sl2} \textbf{(b)} is
analogous to the proof of Lemma \ref{lem.serre-gen.sl2} \textbf{(a)}.

\textbf{(c)} By assumption, $\dim V<\infty$. Now, the endomorphism $h\mid_{V}$
of $V$ has at most $\dim V$ distinct eigenvalues (since an endomorphism of any
finite-dimensional vector space $W$ has at most $\dim W$ distinct
eigenvalues). From this, it is easy to conclude that $f^{\dim V}%
x=0$\ \ \ \ \footnote{\textit{Proof.} Assume the opposite. Then, $f^{\dim
V}x\neq0$.
\par
Now, let $m\in\left\{  0,1,...,\dim V\right\}  $ be arbitrary. We will prove
that $\lambda-2m$ is an eigenvalue of $h\mid_{V}$.
\par
In fact, $m\leq\dim V$, so that $f^{\dim V-m}\left(  f^{m}x\right)  =f^{\dim
V-m+m}x=f^{\dim V}x\neq0$ and thus $f^{m}x\neq0$. Since $hf^{m}x=\left(
\lambda-2m\right)  f^{m}x$ (by (\ref{pf.serre-gen.sl2.1})), this yields that
$f^{m}x$ is a nonzero eigenvector of $h\mid_{V}$ with eigenvalue $\lambda-2m$.
Thus, $\lambda-2m$ is an eigenvalue of $h\mid_{V}$.
\par
Now forget that we fixed $m$. Thus, we have proven that $\lambda-2m$ is an
eigenvalue of $h\mid_{V}$ for every $m\in\left\{  0,1,...,\dim V\right\}  $.
Thus we have found $\dim V+1$ pairwise distinct eigenvalues of $h\mid_{V}$.
This contradicts the fact that $h\mid_{V}$ has at most $\dim V$ distinct
eigenvalues. This contradiction shows that our assumption was wrong, qed.}.
Thus, there exists a smallest $m\in\mathbb{N}$ satisfying $f^{m}x=0$. Denote
this $m$ by $u$. Then, $f^{u}x=0$. Since $f^{0}x=x\neq0$, this $u$ is $\neq0$,
so that $f^{u-1}x$ is well-defined. Moreover, $f^{u-1}x\neq0$ (since $u$ is
the smallest $m\in\mathbb{N}$ satisfying $f^{m}x=0$).

Lemma \ref{lem.serre-gen.sl2} \textbf{(a)} (applied to $n=u$) yields
$e^{u}f^{u}x=u!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-u+1\right)
x$. Since $e^{u}\underbrace{f^{u}x}_{=0}=0$, this rewrites as $u!\lambda
\left(  \lambda-1\right)  ...\left(  \lambda-u+1\right)  x=0$. Since
$\operatorname*{char}\mathbb{C}=0$, we can divide this equation by $u!$, and
obtain $\lambda\left(  \lambda-1\right)  ...\left(  \lambda-u+1\right)  x=0$.
Since $x\neq0$, this yields $\lambda\left(  \lambda-1\right)  ...\left(
\lambda-u+1\right)  =0$. Thus, one of the numbers $\lambda$, $\lambda-1$,
$...$, $\lambda-u+1$ must be $0$. In other words, $\lambda\in\left\{
0,1,...,u-1\right\}  $. Hence, $\lambda\in\mathbb{N}$ and $\lambda\leq u-1$.

Applying (\ref{pf.serre-gen.sl2.1}) to $m=u-1$, we obtain $hf^{u-1}x=\left(
\lambda-2\left(  u-1\right)  \right)  f^{u-1}x$. Denote $\lambda-2\left(
u-1\right)  $ by $\mu$. Then, $hf^{u-1}x=\underbrace{\left(  \lambda-2\left(
u-1\right)  \right)  }_{=\mu}f^{u-1}x=\mu f^{u-1}x$. Also, $ff^{u-1}%
x=f^{u}x=0$. Thus, we can apply Lemma \ref{lem.serre-gen.sl2} \textbf{(b)} to
$\mu$, $f^{u-1}x$ and $u-1$ instead of $\lambda$, $x$ and $n$. Thus, we obtain%
\[
f^{u-1}e^{u-1}f^{u-1}x=\left(  u-1\right)  !\mu\left(  \mu+1\right)
...\left(  \mu+\left(  u-1\right)  -1\right)  f^{u-1}x.
\]
But $\mu=\underbrace{\lambda}_{\leq u-1}-2\left(  u-1\right)  \leq\left(
u-1\right)  -2\left(  u-1\right)  =-\left(  u-1\right)  $, so that each of the
integers $\mu$, $\mu+1$, $...$, $\mu+\left(  u-1\right)  -1$ is nonzero. Thus,
their product $\mu\left(  \mu+1\right)  ...\left(  \mu+\left(  u-1\right)
-1\right)  $ also is $\neq0$. Combined with $\left(  u-1\right)  !\neq0$, this
yields $\left(  u-1\right)  !\mu\left(  \mu+1\right)  ...\left(  \mu+\left(
u-1\right)  -1\right)  \neq0$. Combined with $f^{u-1}x\neq0$, this yields
$\left(  u-1\right)  !\mu\left(  \mu+1\right)  ...\left(  \mu+\left(
u-1\right)  -1\right)  f^{u-1}x\neq0$. Thus,%
\[
f^{u-1}e^{u-1}f^{u-1}x=\left(  u-1\right)  !\mu\left(  \mu+1\right)
...\left(  \mu+\left(  u-1\right)  -1\right)  f^{u-1}x\neq0,
\]
so that $e^{u-1}f^{u-1}x\neq0$.

But Lemma \ref{lem.serre-gen.sl2} \textbf{(a)} (applied to $n=u-1$) yields
$e^{u-1}f^{u-1}x=\left(  u-1\right)  !\lambda\left(  \lambda-1\right)
...\left(  \lambda-\left(  u-1\right)  +1\right)  x$. Thus,
\[
\left(  u-1\right)  !\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  u-1\right)  +1\right)  x=e^{u-1}f^{u-1}x\neq0.
\]
Hence, $\lambda\left(  \lambda-1\right)  ...\left(  \lambda-\left(
u-1\right)  +1\right)  \neq0$. Hence, $\dbinom{\lambda}{u-1}=\dfrac{1}{\left(
u-1\right)  !}\underbrace{\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  u-1\right)  +1\right)  }_{\neq0}\neq0$, so that $u-1\leq
\lambda$ (because otherwise, we would have $\dbinom{\lambda}{u-1}=0$,
contradicting $\dbinom{\lambda}{u-1}\neq0$). Combined with $u-1\geq\lambda$,
this yields $u-1=\lambda$. Thus, $u=\lambda+1$. Hence, $f^{u}x=0$ rewrites as
$f^{\lambda+1}x=0$. This proves Lemma \ref{lem.serre-gen.sl2} \textbf{(c)}.

\subsubsection{\texorpdfstring{$Q$}{Q}-graded Lie algebras}

The following generalization of the standard definition of a $\mathbb{Z}%
$-graded Lie algebra suggests itself:

\begin{definition}
\label{def.Q-graded.lie}Let $Q$ be an abelian group, written additively.

\textbf{(a)} A $Q$\textit{-graded vector space} will mean a vector space $V$
equipped with a family $\left(  V\left[  \alpha\right]  \right)  _{\alpha\in
Q}$ of vector subspaces $V\left[  \alpha\right]  $ of $V$ (indexed by elements
of $Q$) satisfying $V=\bigoplus\limits_{\alpha\in Q}V\left[  \alpha\right]  $.
For every $\alpha\in Q$, the subspace $V\left[  \alpha\right]  $ is called the
$\alpha$\textit{-th homogeneous component} of the $Q$-graded vector space $V$.
The family $\left(  V\left[  \alpha\right]  \right)  _{\alpha\in Q}$ is called
a $Q$\textit{-grading} on the vector space $V$.

\textbf{(b)} A $Q$\textit{-graded Lie algebra} will mean a Lie algebra
$\mathfrak{g}$ equipped with a family $\left(  \mathfrak{g}\left[
\alpha\right]  \right)  _{\alpha\in Q}$ of vector subspaces $\mathfrak{g}%
\left[  \alpha\right]  $ of $\mathfrak{g}$ (indexed by elements of $Q$)
satisfying $\mathfrak{g}=\bigoplus\limits_{\alpha\in Q}\mathfrak{g}\left[
\alpha\right]  $ and satisfying
\[
\left[  \mathfrak{g}\left[  \alpha\right]  ,\mathfrak{g}\left[  \beta\right]
\right]  \subseteq\mathfrak{g}\left[  \alpha+\beta\right]
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha,\beta\in Q\text{.}%
\]
In this case, $Q$ is called the \textit{root lattice} of this $Q$-graded Lie
algebra $\mathfrak{g}$. (This does not mean that $Q$ actually has to be a
lattice of roots of $\mathfrak{g}$, or that $Q$ must be related in any way to
the roots of $\mathfrak{g}$.) Clearly, any $Q$-graded Lie algebra is a
$Q$-graded vector space. Thus, the notion of the $\alpha$-th homogeneous
component of a $Q$-graded Lie algebra makes sense for every $\alpha\in Q$.
\end{definition}

\begin{Convention}
Whenever $Q$ is an abelian group, $\alpha$ is an element of $Q$, and $V$ is a
$Q$-graded vector space or a $Q$-graded Lie algebra, we will denote the
$\alpha$-th homogeneous component of $V$ by $V\left[  \alpha\right]  $.
\end{Convention}

In the context of a $Q$-graded vector space (or Lie algebra) $V$, one often
writes $V_{\alpha}$ instead of $V\left[  \alpha\right]  $ for the $\alpha$-th
homogeneous component of $V$. This notation, however, can sometimes be misunderstood.

When a group homomorphism from $Q$ to $\mathbb{Z}$ is given, a $Q$-graded Lie
algebra canonically becomes a $\mathbb{Z}$-graded Lie algebra:

\begin{proposition}
\label{prop.Q-graded.principal}Let $Q$ be an abelian group. Let $\ell
:Q\rightarrow\mathbb{Z}$ be a group homomorphism. Let $\mathfrak{g}$ be a
$Q$-graded Lie algebra.

\textbf{(a)} For every $m\in\mathbb{Z}$, the internal direct sum
$\bigoplus\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)
=m}}\mathfrak{g}\left[  \alpha\right]  $ is well-defined.

\textbf{(b)} Denote this internal direct sum $\bigoplus
\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m}}\mathfrak{g}%
\left[  \alpha\right]  $ by $\mathfrak{g}_{\left[  m\right]  }$. Then, the Lie
algebra $\mathfrak{g}$ equipped with the grading $\left(  \mathfrak{g}%
_{\left[  m\right]  }\right)  _{m\in\mathbb{Z}}$ is a $\mathbb{Z}$-graded Lie algebra.

(This grading $\left(  \mathfrak{g}_{\left[  m\right]  }\right)
_{m\in\mathbb{Z}}$ is called the \textit{principal grading} on $\mathfrak{g}$
induced by the given $Q$-grading on $\mathfrak{g}$ and the map $\ell$.)
\end{proposition}

\begin{vershort}
The proof of this proposition is straightforward and left to the reader.
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.Q-graded.principal}.} \textbf{(a)}
Since $\mathfrak{g}$ is $Q$-graded, we have
\[
\mathfrak{g}=\bigoplus\limits_{\alpha\in Q}\mathfrak{g}\left[  \alpha\right]
=\bigoplus\limits_{m\in\mathbb{Z}}\bigoplus\limits_{\substack{\alpha\in
Q;\\\ell\left(  \alpha\right)  =m}}\mathfrak{g}\left[  \alpha\right]  .
\]
Thus, the internal direct sum $\bigoplus\limits_{\substack{\alpha\in
Q;\\\ell\left(  \alpha\right)  =m}}\mathfrak{g}\left[  \alpha\right]  $ is
defined for every $m\in\mathbb{Z}$. This proves Proposition
\ref{prop.Q-graded.principal} \textbf{(a)}.

\textbf{(b)} We have
\[
\mathfrak{g}=\bigoplus\limits_{m\in\mathbb{Z}}\underbrace{\bigoplus
\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m}}\mathfrak{g}%
\left[  \alpha\right]  }_{=\mathfrak{g}_{\left[  m\right]  }}=\bigoplus
\limits_{m\in\mathbb{Z}}\mathfrak{g}_{\left[  m\right]  }.
\]
Also, every $m_{1}\in\mathbb{Z}$ and every $m_{2}\in\mathbb{Z}$ satisfy%
\[
\mathfrak{g}_{\left[  m_{1}\right]  }\mathfrak{g}_{\left[  m_{2}\right]
}\subseteq\mathfrak{g}_{\left[  m_{1}+m_{2}\right]  }%
\]
\footnote{\textit{Proof.} Let $m_{1}\in\mathbb{Z}$ and $m_{2}\in\mathbb{Z}$.
Then, the definition of $\mathfrak{g}_{\left[  m_{1}\right]  }$ yields
\begin{align*}
\mathfrak{g}_{\left[  m_{1}\right]  }  &  =\bigoplus\limits_{\substack{\alpha
\in Q;\\\ell\left(  \alpha\right)  =m_{1}}}\mathfrak{g}\left[  \alpha\right]
=\sum\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m_{1}%
}}\mathfrak{g}\left[  \alpha\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
direct sums are sums}\right) \\
&  =\sum\limits_{\substack{\beta\in Q;\\\ell\left(  \beta\right)  =m_{1}%
}}\mathfrak{g}\left[  \beta\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
renamed the index }\alpha\text{ as }\beta\right)  .
\end{align*}
Also, the definition of $\mathfrak{g}_{\left[  m_{2}\right]  }$ yields%
\begin{align*}
\mathfrak{g}_{\left[  m_{2}\right]  }  &  =\bigoplus\limits_{\substack{\alpha
\in Q;\\\ell\left(  \alpha\right)  =m_{2}}}\mathfrak{g}\left[  \alpha\right]
=\sum\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m_{2}%
}}\mathfrak{g}\left[  \alpha\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
direct sums are sums}\right) \\
&  =\sum\limits_{\substack{\gamma\in Q;\\\ell\left(  \gamma\right)  =m_{2}%
}}\mathfrak{g}\left[  \gamma\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{here,
we renamed the index }\alpha\text{ as }\gamma\right)  .
\end{align*}
Finally, the definition of $\mathfrak{g}_{\left[  m_{1}+m_{2}\right]  }$
yields%
\[
\mathfrak{g}_{\left[  m_{1}+m_{2}\right]  }=\bigoplus\limits_{\substack{\alpha
\in Q;\\\ell\left(  \alpha\right)  =m_{1}+m_{2}}}\mathfrak{g}\left[
\alpha\right]  =\sum\limits_{\substack{\alpha\in Q;\\\ell\left(
\alpha\right)  =m_{1}+m_{2}}}\mathfrak{g}\left[  \alpha\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since direct sums are sums}\right)  .
\]
\par
Every $\beta\in Q$ and $\gamma\in Q$ such that $\ell\left(  \beta\right)
=m_{1}$ and $\ell\left(  \gamma\right)  =m_{2}$ satisfy%
\begin{align*}
\ell\left(  \beta+\gamma\right)   &  =\underbrace{\ell\left(  \beta\right)
}_{=m_{1}}+\underbrace{\ell\left(  \gamma\right)  }_{=m_{2}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\text{ is a }\mathbb{Z}%
\text{-module homomorphism}\right) \\
&  =m_{1}+m_{2}.
\end{align*}
Thus, every $\beta\in Q$ and $\gamma\in Q$ such that $\ell\left(
\beta\right)  =m_{1}$ and $\ell\left(  \gamma\right)  =m_{2}$ satisfy%
\begin{equation}
\mathfrak{g}\left[  \beta+\gamma\right]  \subseteq\sum
\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m_{1}+m_{2}%
}}\mathfrak{g}\left[  \alpha\right]  =\mathfrak{g}_{\left[  m_{1}%
+m_{2}\right]  }. \label{pf.Q-graded.principal.1}%
\end{equation}
\par
Since $\mathfrak{g}_{\left[  m_{1}\right]  }=\sum\limits_{\substack{\beta\in
Q;\\\ell\left(  \beta\right)  =m_{1}}}\mathfrak{g}\left[  \beta\right]  $ and
$\mathfrak{g}_{\left[  m_{2}\right]  }=\sum\limits_{\substack{\gamma\in
Q;\\\ell\left(  \gamma\right)  =m_{2}}}\mathfrak{g}\left[  \gamma\right]  $,
we have%
\begin{align*}
\left[  \mathfrak{g}_{\left[  m_{1}\right]  },\mathfrak{g}_{\left[
m_{2}\right]  }\right]   &  =\left[  \sum\limits_{\substack{\beta\in
Q;\\\ell\left(  \beta\right)  =m_{1}}}\mathfrak{g}\left[  \beta\right]
,\sum\limits_{\substack{\gamma\in Q;\\\ell\left(  \gamma\right)  =m_{2}%
}}\mathfrak{g}\left[  \gamma\right]  \right]  =\sum\limits_{\substack{\beta\in
Q;\\\ell\left(  \beta\right)  =m_{1}}}\sum\limits_{\substack{\gamma\in
Q;\\\ell\left(  \gamma\right)  =m_{2}}}\underbrace{\left[  \mathfrak{g}\left[
\beta\right]  ,\mathfrak{g}\left[  \gamma\right]  \right]  }%
_{\substack{\subseteq\mathfrak{g}\left[  \beta+\gamma\right]  \\\text{(since
}\mathfrak{g}\text{ is a }Q\text{-graded Lie algebra)}}}\\
&  \subseteq\sum\limits_{\substack{\beta\in Q;\\\ell\left(  \beta\right)
=m_{1}}}\sum\limits_{\substack{\gamma\in Q;\\\ell\left(  \gamma\right)
=m_{2}}}\underbrace{\mathfrak{g}\left[  \beta+\gamma\right]  }%
_{\substack{\subseteq\mathfrak{g}_{\left[  m_{1}+m_{2}\right]  }\\\text{(by
(\ref{pf.Q-graded.principal.1}), since}\\\ell\left(  \beta\right)
=m_{1}\text{ and }\ell\left(  \gamma\right)  =m_{2}\text{)}}}\subseteq
\sum\limits_{\substack{\beta\in Q;\\\ell\left(  \beta\right)  =m_{1}}%
}\sum\limits_{\substack{\gamma\in Q;\\\ell\left(  \gamma\right)  =m_{2}%
}}\mathfrak{g}_{\left[  m_{1}+m_{2}\right]  }\\
&  \subseteq\mathfrak{g}_{\left[  m_{1}+m_{2}\right]  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}_{\left[  m_{1}%
+m_{2}\right]  }\text{ is a vector space}\right)  ,
\end{align*}
qed.}. Combined with the fact that $\mathfrak{g}=\bigoplus\limits_{m\in
\mathbb{Z}}\mathfrak{g}_{\left[  m\right]  }$, this yields that the Lie
algebra $\mathfrak{g}$ equipped with the family $\left(  \mathfrak{g}_{\left[
m\right]  }\right)  _{m\in\mathbb{Z}}$ is a $\mathbb{Z}$-graded Lie algebra.
This proves Proposition \ref{prop.Q-graded.principal}.
\end{verlong}

\subsubsection{A few lemmas on generating subspaces of Lie algebras}

We proceed with some facts about generating sets of Lie algebras (free or not):

\begin{lemma}
\label{lem.generation.1}Let $\mathfrak{g}$ be a Lie algebra, and let $T$ be a
vector subspace of $\mathfrak{g}$. Assume that $\mathfrak{g}$ is generated by
$T$ as a Lie algebra.

Let $U$ be a vector subspace of $\mathfrak{g}$ such that $T\subseteq U$ and
$\left[  T,U\right]  \subseteq U$. Then, $U=\mathfrak{g}$.
\end{lemma}

Notice that Lemma \ref{lem.generation.1} is not peculiar to Lie algebras. A
similar result holds (for instance) if ``Lie algebra'' is replaced by
``commutative nonunital algebra'' and ``$\left[  T,U\right]  $'' is replaced
by ``$TU$''.

The following proof is written merely for the sake of completeness;
intuitively, Lemma \ref{lem.generation.1} should be obvious from the
observation that all iterated Lie brackets of elements of $T$ can be written
as linear combinations of Lie brackets of the form $\left[  t_{1},\left[
t_{2},\left[  ...,\left[  t_{k-1},t_{k}\right]  \right]  \right]  \right]  $
(with $t_{1},t_{2},...,t_{k}\in T$) by applying the Jacobi identity iteratively.

\textit{Proof of Lemma \ref{lem.generation.1}.} Define a sequence $\left(
T_{n}\right)  _{n\geq1}$ of vector subspaces of $\mathfrak{g}$ recursively as
follows: Let $T_{1}=T$, and for every positive integer $n$, set $T_{n+1}%
=\left[  T,T_{n}\right]  $.

We have%
\begin{equation}
\left[  T_{i},T_{j}\right]  \subseteq T_{i+j}\ \ \ \ \ \ \ \ \ \ \text{for any
positive integers }i\text{ and }j\text{.} \label{pf.generation.1.additivity}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.generation.1.additivity}):} We will prove
(\ref{pf.generation.1.additivity}) by induction over $i$.
\par
\textit{Induction base:} For any positive integer $j$, we have $T_{j+1}%
=\left[  T,T_{j}\right]  $ (by the definition of $T_{j+1}$) and thus $\left[
\underbrace{T_{1}}_{=T},T_{j}\right]  =\left[  T,T_{j}\right]  =T_{j+1}%
=T_{1+j}$. In other words, (\ref{pf.generation.1.additivity}) holds for $i=1$.
This completes the induction base.
\par
\textit{Induction step:} Let $k$ be a positive integer. Assume that
(\ref{pf.generation.1.additivity}) is proven for $i=k$. We now will prove
(\ref{pf.generation.1.additivity}) for $i=k+1$.
\par
Since (\ref{pf.generation.1.additivity}) is proven for $i=k$, we have%
\begin{equation}
\left[  T_{k},T_{j}\right]  \subseteq T_{k+j}\ \ \ \ \ \ \ \ \ \ \text{for any
positive integer }j\text{.} \label{pf.generation.1.additivity.2}%
\end{equation}
\par
Now, let $j$ be a positive integer. Then, $T_{k+j+1}=\left[  T,T_{k+j}\right]
$ (by the definition of $T_{k+j+1}$) and $T_{j+1}=\left[  T,T_{j}\right]  $
(by the definition of $T_{j+1}$). Now, any $x\in T$, $y\in T_{k}$ and $z\in
T_{j}$ satisfy%
\begin{align*}
\left[  \left[  x,y\right]  ,z\right]   &  =-\underbrace{\left[  \left[
y,z\right]  ,x\right]  }_{=-\left[  x,\left[  y,z\right]  \right]  }-\left[
\underbrace{\left[  z,x\right]  }_{=-\left[  x,z\right]  },y\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the Jacobi identity}\right) \\
&  =\underbrace{-\left(  -\left[  x,\left[  y,z\right]  \right]  \right)
}_{=\left[  x,\left[  y,z\right]  \right]  }-\underbrace{\left[  -\left[
x,z\right]  ,y\right]  }_{=-\left[  \left[  x,z\right]  ,y\right]  =\left[
y,\left[  x,z\right]  \right]  }=\left[  \underbrace{x}_{\in T},\left[
\underbrace{y}_{\in T_{k}},\underbrace{z}_{\in T_{j}}\right]  \right]
-\left[  \underbrace{y}_{\in T_{k}},\left[  \underbrace{x}_{\in T}%
,\underbrace{z}_{\in T_{j}}\right]  \right] \\
&  \in\left[  T,\underbrace{\left[  T_{k},T_{j}\right]  }_{\substack{\subseteq
T_{k+j}\\\text{(by (\ref{pf.generation.1.additivity.2}))}}}\right]  +\left[
T_{k},\underbrace{\left[  T,T_{j}\right]  }_{=T_{j+1}}\right]  \subseteq
\underbrace{\left[  T,T_{k+j}\right]  }_{=T_{k+j+1}}+\underbrace{\left[
T_{k},T_{j+1}\right]  }_{\substack{\subseteq T_{k+j+1}\\\text{(by
(\ref{pf.generation.1.additivity.2}), applied to}\\j+1\text{ instead of
}j\text{)}}}\\
&  \subseteq T_{k+j+1}+T_{k+j+1}\subseteq T_{k+j+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }T_{k+j+1}\text{ is a vector space}\right) \\
&  =T_{\left(  k+1\right)  +j}.
\end{align*}
Hence, $\left[  \left[  T,T_{k}\right]  ,T_{j}\right]  \subseteq T_{\left(
k+1\right)  +j}$ (since $T_{\left(  k+1\right)  +j}$ is a vector space). Since
$\left[  T,T_{k}\right]  =T_{k+1}$ (by the definition of $T_{k+1}$), this
rewrites as $\left[  T_{k+1},T_{j}\right]  \subseteq T_{\left(  k+1\right)
+j}$. Since we have proven this for every positive integer $j$, we have thus
proven (\ref{pf.generation.1.additivity}) for $i=k+1$. The induction step is
thus complete. This finishes the proof of (\ref{pf.generation.1.additivity}).}
Now, let $S$ be the vector subspace $\sum\limits_{i\geq1}T_{i}$ of
$\mathfrak{g}$. Then, every positive integer $k$ satisfies $T_{k}\subseteq S$.
In particular, $T_{1}\subseteq S$. Since $S=\sum\limits_{i\geq1}T_{i}$ and
$S=\sum\limits_{i\geq1}T_{i}=\sum\limits_{j\geq1}T_{j}$, we have%
\[
\left[  S,S\right]  =\left[  \sum\limits_{i\geq1}T_{i},\sum\limits_{j\geq
1}T_{j}\right]  =\sum\limits_{i\geq1}\sum\limits_{j\geq1}\underbrace{\left[
T_{i},T_{j}\right]  }_{\substack{\subseteq T_{i+j}\subseteq S\\\text{(since
every positive}\\\text{integer }k\text{ satisfies }T_{k}\subseteq S\text{)}%
}}\subseteq\sum\limits_{i\geq1}\sum\limits_{j\geq1}S\subseteq S
\]
(since $S$ is a vector space). Thus, $S$ is a Lie subalgebra of $\mathfrak{g}%
$. Since $T=T_{1}\subseteq S$, this yields that $S$ is a Lie subalgebra of
$\mathfrak{g}$ containing $T$ as a subset. Since the smallest Lie subalgebra
of $\mathfrak{g}$ containing $T$ as a subset is $\mathfrak{g}$ itself (because
$\mathfrak{g}$ is generated by $T$ as a Lie algebra), this yields that
$S\supseteq\mathfrak{g}$. In other words, $S=\mathfrak{g}$.

Now, it is easy to see that%
\begin{equation}
T_{i}\subseteq U\text{ for every positive integer }i. \label{pf.generation.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.generation.2}):} We will prove
(\ref{pf.generation.2}) by induction over $i$.
\par
\textit{Induction base:} We have $T_{1}=T\subseteq U$. Thus,
(\ref{pf.generation.2}) holds for $i=1$. This completes the induction base.
\par
\textit{Induction step:} Let $k$ be a positive integer. Assume that
(\ref{pf.generation.2}) holds for $i=k$. We now will prove
(\ref{pf.generation.2}) for $i=k+1$.
\par
Since (\ref{pf.generation.2}) holds for $i=k$, we have $T_{k}\subseteq U$.
Since $T_{k+1}=\left[  T,T_{k}\right]  $ (by the definition of $T_{k+1}$), we
have $T_{k+1}=\left[  T,\underbrace{T_{k}}_{\subseteq U}\right]
\subseteq\left[  T,U\right]  \subseteq U$. In other words,
(\ref{pf.generation.2}) holds for $i=k+1$. This completes the induction step.
Thus, (\ref{pf.generation.2}) is proven.} Hence,%
\[
\mathfrak{g}=S=\sum\limits_{i\geq1}\underbrace{T_{i}}_{\subseteq U}%
\subseteq\sum\limits_{i\geq1}U\subseteq U
\]
(since $U$ is a vector space). Thus, $U=\mathfrak{g}$, and this proves Lemma
\ref{lem.generation.1}.

The next result is related:

\begin{theorem}
\label{thm.FreeLie.grading1}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie
algebra. Let $T$ be a vector subspace of $\mathfrak{g}\left[  1\right]  $ such
that $\mathfrak{g}$ is generated by $T$ as a Lie algebra. Then,
$T=\mathfrak{g}\left[  1\right]  $.
\end{theorem}

\begin{vershort}
The proof of this theorem proceeds by defining the sequence $\left(
T_{n}\right)  _{n\geq1}$ as in the proof of Lemma \ref{lem.generation.1}, and
showing that $T_{i}\subseteq\mathfrak{g}\left[  i\right]  $ for every positive
integer $i$. The details are left to the reader.
\end{vershort}

\begin{verlong}
\textit{Proof of Theorem \ref{thm.FreeLie.grading1}.} Define a sequence
$\left(  T_{n}\right)  _{n\geq1}$ of vector subspaces of $\mathfrak{g}$
recursively as follows: Let $T_{1}=T$, and for every positive integer $n$, set
$T_{n+1}=\left[  T,T_{n}\right]  $.

Let $S$ be the vector subspace $\sum\limits_{i\geq1}T_{i}$ of $\mathfrak{g}$.
Just as in the proof of Lemma \ref{lem.generation.1}, we can see that
$S=\mathfrak{g}$.

Let $\pi_{1}$ be the canonical projection from the graded vector space
$\mathfrak{g}$ on its $1$-th homogeneous component $\mathfrak{g}\left[
1\right]  $. Then, $\pi_{1}$ sends every homogeneous component of
$\mathfrak{g}$ other than $\mathfrak{g}\left[  1\right]  $ to $0$. In other
words, $\pi_{1}$ sends $\mathfrak{g}\left[  i\right]  $ to $0$ for every
integer $i\neq1$. In other words,%
\begin{equation}
\text{every integer }i\neq1\text{ satisfies }\pi_{1}\left(  \mathfrak{g}%
\left[  i\right]  \right)  =0. \label{pf.FreeLie.grading1}%
\end{equation}


On the other hand, since $\pi_{1}$ is a projection on $\mathfrak{g}\left[
1\right]  $, we have $\mathfrak{g}\left[  1\right]  =\pi_{1}\left(
\mathfrak{g}\right)  $.

Since $\pi_{1}$ is a projection on $\mathfrak{g}\left[  1\right]  $, we have%
\begin{equation}
\pi_{1}\left(  x\right)  =x\ \ \ \ \ \ \ \ \ \ \text{for every }%
x\in\mathfrak{g}\left[  1\right]  . \label{pf.FreeLie.grading0}%
\end{equation}


Now, it is easy to see that%
\begin{equation}
T_{i}\subseteq\mathfrak{g}\left[  i\right]  \ \ \ \ \ \ \ \ \ \ \text{for
every positive integer }i. \label{pf.FreeLie.grading2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.FreeLie.grading2}):} We will prove
(\ref{pf.FreeLie.grading2}) by induction over $i$.
\par
\textit{Induction base:} We have $T_{1}=T\subseteq\mathfrak{g}\left[
1\right]  $. Thus, (\ref{pf.FreeLie.grading2}) is proven for $i=1$. This
completes the induction base.
\par
\textit{Induction step:} Let $n$ be a positive integer. Assume that
(\ref{pf.FreeLie.grading2}) holds for $i=n$. We now must prove that
(\ref{pf.FreeLie.grading2}) also holds for $i=n+1$.
\par
Since (\ref{pf.FreeLie.grading2}) holds for $i=n$, we have $T_{n}%
\subseteq\mathfrak{g}\left[  n\right]  $. By the definition of $T_{n+1}$, we
have
\begin{align*}
T_{n+1}  &  =\left[  \underbrace{T}_{\subseteq\mathfrak{g}\left[  1\right]
},\underbrace{T_{n}}_{\subseteq\mathfrak{g}\left[  n\right]  }\right]
\subseteq\left[  \mathfrak{g}\left[  1\right]  ,\mathfrak{g}\left[  n\right]
\right]  \subseteq\mathfrak{g}\left[  1+n\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathfrak{g}\text{ is a }\mathbb{Z}\text{-graded Lie
algebra}\right) \\
&  =\mathfrak{g}\left[  n+1\right]  .
\end{align*}
Thus, (\ref{pf.FreeLie.grading2}) also holds for $i=n+1$. This completes the
induction step. The induction proof of (\ref{pf.FreeLie.grading2}) is thus
complete.} Hence, for every positive integer $i\geq2$, we have%
\[
\pi_{1}\left(  \underbrace{T_{i}}_{\subseteq\mathfrak{g}\left[  i\right]
}\right)  \subseteq\pi_{1}\left(  \mathfrak{g}\left[  i\right]  \right)  =0
\]
(by (\ref{pf.FreeLie.grading1}), since $i\neq1$). Hence,%
\begin{align*}
\mathfrak{g}\left[  1\right]   &  =\pi_{1}\left(  \underbrace{\mathfrak{g}%
}_{=S=\sum\limits_{i\geq1}T_{i}}\right)  =\pi_{1}\left(  \sum\limits_{i\geq
1}T_{i}\right)  =\sum\limits_{i\geq1}\pi_{1}\left(  T_{i}\right)  =\pi
_{1}\left(  T_{1}\right)  +\sum\limits_{i\geq2}\underbrace{\pi_{1}\left(
T_{i}\right)  }_{\substack{=0\\\text{(since }i\geq2\text{)}}}\\
&  =\pi_{1}\left(  T_{1}\right)  +\underbrace{\sum\limits_{i\geq2}0}_{=0}%
=\pi_{1}\left(  T_{1}\right)  =\left\{  \underbrace{\pi_{1}\left(  x\right)
}_{\substack{=x\\\text{(by (\ref{pf.FreeLie.grading0}), since}\\x\in
T_{1}=T\subseteq\mathfrak{g}\left[  1\right]  \text{)}}}\ \mid\ x\in
T_{1}\right\}  =\left\{  x\ \mid\ x\in T_{1}\right\}  =T_{1}=T.
\end{align*}
This proves Theorem \ref{thm.FreeLie.grading1}.
\end{verlong}

Generating subspaces can help in proving that Lie algebra homomorphisms are
$Q$-graded:

\begin{proposition}
\label{prop.generation.Q-gr}Let $\mathfrak{g}$ and $\mathfrak{h}$ be two
$Q$-graded Lie algebras. Let $T$ be a $Q$-graded vector subspace of
$\mathfrak{g}$. Assume that $\mathfrak{g}$ is generated by $T$ as a Lie algebra.

Let $f:\mathfrak{g}\rightarrow\mathfrak{h}$ be a Lie algebra homomorphism.
Assume that $f\mid_{T}:T\rightarrow\mathfrak{h}$ is a $Q$-graded map.

Then, the map $f$ is $Q$-graded.
\end{proposition}

\begin{vershort}
The proof of this is left to the reader.
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.generation.Q-gr}.} For every $\alpha\in
Q$, let $P_{\alpha}$ be the vector subspace $\left(  \mathfrak{g}\left[
\alpha\right]  \right)  \cap f^{-1}\left(  \mathfrak{h}\left[  \alpha\right]
\right)  $ of $\mathfrak{g}$. Then,%
\begin{equation}
\left[  P_{\alpha},P_{\beta}\right]  \subseteq P_{\alpha+\beta}%
\ \ \ \ \ \ \ \ \ \ \text{for any }\alpha\in Q\text{ and }\beta\in Q.
\label{pf.generation.Q-gr.PaPb}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.generation.Q-gr.PaPb}):} Let $\alpha\in Q$
and $\beta\in Q$. Let $x\in P_{\alpha}$ and $y\in P_{\beta}$. Then, $x\in
P_{\alpha}=\left(  \mathfrak{g}\left[  \alpha\right]  \right)  \cap
f^{-1}\left(  \mathfrak{h}\left[  \alpha\right]  \right)  $ (by the definition
of $P_{\alpha}$), so that $x\in\mathfrak{g}\left[  \alpha\right]  $ and $x\in
f^{-1}\left(  \mathfrak{h}\left[  \alpha\right]  \right)  $. Also, $y\in
P_{\beta}=\left(  \mathfrak{g}\left[  \beta\right]  \right)  \cap
f^{-1}\left(  \mathfrak{h}\left[  \beta\right]  \right)  $ (by the definition
of $P_{\beta}$), so that $y\in\mathfrak{g}\left[  \beta\right]  $ and $y\in
f^{-1}\left(  \mathfrak{h}\left[  \beta\right]  \right)  $. From $x\in
f^{-1}\left(  \mathfrak{h}\left[  \alpha\right]  \right)  $, we obtain
$f\left(  x\right)  \in\mathfrak{h}\left[  \alpha\right]  $. From $y\in
f^{-1}\left(  \mathfrak{h}\left[  \beta\right]  \right)  $, we get $f\left(
y\right)  \in\mathfrak{h}\left[  \beta\right]  $. Now,%
\begin{align*}
f\left(  \left[  x,y\right]  \right)   &  =\left[  \underbrace{f\left(
x\right)  }_{\in\mathfrak{h}\left[  \alpha\right]  },\underbrace{f\left(
y\right)  }_{\in\mathfrak{h}\left[  \beta\right]  }\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }f\text{ is a Lie algebra
homomorphism}\right) \\
&  \in\left[  \mathfrak{h}\left[  \alpha\right]  ,\mathfrak{h}\left[
\beta\right]  \right]  \subseteq\mathfrak{h}\left[  \alpha+\beta\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{h}\text{ is a
}Q\text{-graded Lie algebra}\right)  ,
\end{align*}
and thus $\left[  x,y\right]  \in f^{-1}\left(  \mathfrak{h}\left[
\alpha+\beta\right]  \right)  $. Combined with%
\[
\left[  \underbrace{x}_{\in\mathfrak{g}\left[  \alpha\right]  },\underbrace{y}%
_{\in\mathfrak{g}\left[  \beta\right]  }\right]  \in\left[  \mathfrak{g}%
\left[  \alpha\right]  ,\mathfrak{g}\left[  \beta\right]  \right]
\subseteq\mathfrak{g}\left[  \alpha+\beta\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathfrak{g}\text{ is a }Q\text{-graded Lie algebra}\right)  ;
\]
this yields $\left[  x,y\right]  \in\left(  \mathfrak{g}\left[  \alpha
+\beta\right]  \right)  \cap f^{-1}\left(  \mathfrak{h}\left[  \alpha
+\beta\right]  \right)  $. But since $P_{\alpha+\beta}=\left(  \mathfrak{g}%
\left[  \alpha+\beta\right]  \right)  \cap f^{-1}\left(  \mathfrak{h}\left[
\alpha+\beta\right]  \right)  $ (by the definition of $P_{\alpha+\beta}$),
this rewrites as $\left[  x,y\right]  \in P_{\alpha+\beta}$.
\par
Now forget that we fixed $x$ and $y$. We thus have proven that every $x\in
P_{\alpha}$ and $y\in P_{\beta}$ satisfy $\left[  x,y\right]  \in
P_{\alpha+\beta}$. Since $P_{\alpha+\beta}$ is a vector space, this yields
$\left[  P_{\alpha},P_{\beta}\right]  \subseteq P_{\alpha+\beta}$. This proves
(\ref{pf.generation.Q-gr.PaPb}).}

Now, let $P$ be the vector subspace $\sum\limits_{\alpha\in Q}P_{\alpha}$ of
$\mathfrak{g}$. Then,%
\begin{equation}
P_{\alpha}\subseteq P\text{ for every }\alpha\in Q\text{.}
\label{pf.generation.Q-gr.tauto}%
\end{equation}


But since $P=\sum\limits_{\alpha\in Q}P_{\alpha}$ and $P=\sum\limits_{\alpha
\in Q}P_{\alpha}=\sum\limits_{\beta\in Q}P_{\beta}$ (here, we renamed the
summation index $\alpha$ as $\beta$), we have%
\begin{align*}
\left[  P,P\right]   &  =\left[  \sum\limits_{\alpha\in Q}P_{\alpha}%
,\sum\limits_{\beta\in Q}P_{\beta}\right]  =\sum\limits_{\alpha\in Q}%
\sum\limits_{\beta\in Q}\underbrace{\left[  P_{\alpha},P_{\beta}\right]
}_{\substack{\subseteq P_{\alpha+\beta}\\\text{(by
(\ref{pf.generation.Q-gr.PaPb}))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the
Lie bracket is bilinear}\right) \\
&  \subseteq\sum\limits_{\alpha\in Q}\sum\limits_{\beta\in Q}%
\underbrace{P_{\alpha+\beta}}_{\substack{\subseteq P\\\text{(by
(\ref{pf.generation.Q-gr.tauto}), applied to}\\\alpha+\beta\text{ instead of
}\alpha\text{)}}}\subseteq\sum\limits_{\alpha\in Q}\sum\limits_{\beta\in
Q}P\subseteq P\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\text{ is a vector
space}\right)  .
\end{align*}
As a consequence, $P$ is a Lie subalgebra of $\mathfrak{g}$.

Since $T$ is a $Q$-graded vector subspace, we have $T=\bigoplus\limits_{\alpha
\in Q}T\left[  \alpha\right]  $, and every $\alpha\in Q$ satisfies $T\left[
\alpha\right]  \subseteq\mathfrak{g}\left[  \alpha\right]  $.

Now, $T\subseteq P$\ \ \ \ \footnote{\textit{Proof.} Let $\alpha\in Q$. Let
$x\in T\left[  \alpha\right]  $. Then, $\left(  f\mid_{T}\right)  \left(
x\right)  \in\mathfrak{h}\left[  \alpha\right]  $ (since $f\mid_{T}$ is
$Q$-graded). Thus, $f\left(  x\right)  =\left(  f\mid_{T}\right)  \left(
x\right)  \in\mathfrak{h}\left[  \alpha\right]  $, so that $x\in f^{-1}\left(
\mathfrak{h}\left[  \alpha\right]  \right)  $. Combined with $x\in T\left[
\alpha\right]  \subseteq\mathfrak{g}\left[  \alpha\right]  $, this yields
$x\in\left(  \mathfrak{g}\left[  \alpha\right]  \right)  \cap f^{-1}\left(
\mathfrak{h}\left[  \alpha\right]  \right)  =P_{\alpha}$.
\par
Now forget that we fixed $x$. We thus have proven that every $x\in T\left[
\alpha\right]  $ satisfies $x\in P_{\alpha}$. In other words, $T\left[
\alpha\right]  \subseteq P_{\alpha}$.
\par
Now forget that we fixed $\alpha$. We thus have proven that $T\left[
\alpha\right]  \subseteq P_{\alpha}$ for every $\alpha\in Q$. But
\begin{align*}
T  &  =\bigoplus\limits_{\alpha\in Q}T\left[  \alpha\right]  =\sum
\limits_{\alpha\in Q}\underbrace{T\left[  \alpha\right]  }_{\subseteq
P_{\alpha}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since direct sums are
sums}\right) \\
&  \subseteq\sum\limits_{\alpha\in Q}P_{\alpha}=P,
\end{align*}
qed.}. Hence, $P$ is a Lie subalgebra of $\mathfrak{g}$ containing $T$ as a
subset. But since every Lie subalgebra of $\mathfrak{g}$ containing $T$ as a
subset must be $\mathfrak{g}$ (because $\mathfrak{g}$ is generated by $T$ as a
Lie algebra), this yields that $P=\mathfrak{g}$.

Now, let $\beta\in Q$ be arbitrary. Let $x\in\mathfrak{g}\left[  \beta\right]
$. Then, $x\in\mathfrak{g}\left[  \beta\right]  \subseteq\mathfrak{g}%
=P=\sum\limits_{\alpha\in Q}P_{\alpha}=P_{\beta}+\sum\limits_{\substack{\alpha
\in Q;\\\alpha\neq\beta}}P_{\alpha}$. Hence, there exist some $y\in P_{\beta}$
and some $z\in\sum\limits_{\substack{\alpha\in Q;\\\alpha\neq\beta}}P_{\alpha
}$ such that $x=y+z$. Consider these $y$ and $z$. From $x=y+z$, we obtain
$x-y=z$. But since $\mathfrak{g}$ is $Q$-graded, we have
\[
\mathfrak{g}=\bigoplus\limits_{\alpha\in Q}\mathfrak{g}\left[  \alpha\right]
=\left(  \mathfrak{g}\left[  \beta\right]  \right)  \oplus
\underbrace{\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha\neq\beta
}}\mathfrak{g}\left[  \alpha\right]  }_{\substack{=\sum
\limits_{\substack{\alpha\in Q;\\\alpha\neq\beta}}\mathfrak{g}\left[
\alpha\right]  \\\text{(since direct sums are sums)}}}=\left(  \mathfrak{g}%
\left[  \beta\right]  \right)  \oplus\left(  \sum\limits_{\substack{\alpha\in
Q;\\\alpha\neq\beta}}\mathfrak{g}\left[  \alpha\right]  \right)  .
\]
Thus, the internal direct sum $\left(  \mathfrak{g}\left[  \beta\right]
\right)  \oplus\left(  \sum\limits_{\substack{\alpha\in Q;\\\alpha\neq\beta
}}\mathfrak{g}\left[  \alpha\right]  \right)  $ is well-defined, so that
$\left(  \mathfrak{g}\left[  \beta\right]  \right)  \cap\left(  \sum
\limits_{\substack{\alpha\in Q;\\\alpha\neq\beta}}\mathfrak{g}\left[
\alpha\right]  \right)  =0$.

By the definition of $P_{\beta}$, we have $P_{\beta}=\left(  \mathfrak{g}%
\left[  \beta\right]  \right)  \cap f^{-1}\left(  \mathfrak{h}\left[
\beta\right]  \right)  \subseteq\mathfrak{g}\left[  \beta\right]  $. Hence,
$y\in P_{\beta}\subseteq\mathfrak{g}\left[  \beta\right]  $. Combined with
$x\in\mathfrak{g}\left[  \beta\right]  $, this yields $x-y\in\mathfrak{g}%
\left[  \beta\right]  -\mathfrak{g}\left[  \beta\right]  \subseteq
\mathfrak{g}\left[  \beta\right]  $ (since $\mathfrak{g}\left[  \beta\right]
$ is a vector space). Since $x-y=z$, this rewrites as $z\in\mathfrak{g}\left[
\beta\right]  $. Combined with $z\in\sum\limits_{\substack{\alpha\in
Q;\\\alpha\neq\beta}}\underbrace{P_{\alpha}}_{\substack{\subseteq
\mathfrak{g}\left[  \alpha\right]  \\\text{(this is proven in the
same}\\\text{way as we showed }P_{\beta}\subseteq\mathfrak{g}\left[
\beta\right]  \text{)}}}\subseteq\sum\limits_{\substack{\alpha\in
Q;\\\alpha\neq\beta}}\mathfrak{g}\left[  \alpha\right]  $, this yields
$z\in\left(  \mathfrak{g}\left[  \beta\right]  \right)  \cap\left(
\sum\limits_{\substack{\alpha\in Q;\\\alpha\neq\beta}}\mathfrak{g}\left[
\alpha\right]  \right)  =0$. Hence, $z=0$. Thus, $x-y=z=0$, so that $x=y\in
P_{\beta}=\left(  \mathfrak{g}\left[  \beta\right]  \right)  \cap
f^{-1}\left(  \mathfrak{h}\left[  \beta\right]  \right)  \subseteq
f^{-1}\left(  \mathfrak{h}\left[  \beta\right]  \right)  $, hence $f\left(
x\right)  \in\mathfrak{h}\left[  \beta\right]  $.

Now forget that we fixed $x$. We thus have proven that $f\left(  x\right)
\in\mathfrak{h}\left[  \beta\right]  $ for every $x\in\mathfrak{g}\left[
\beta\right]  $. In other words, $f\left(  \mathfrak{g}\left[  \beta\right]
\right)  \subseteq\mathfrak{h}\left[  \beta\right]  $.

Now forget that we fixed $\beta$. We thus have shown that $f\left(
\mathfrak{g}\left[  \beta\right]  \right)  \subseteq\mathfrak{h}\left[
\beta\right]  $ for every $\beta\in Q$.\ In other words, the map $f$ is
$Q$-graded. This proves Proposition \ref{prop.generation.Q-gr}.
\end{verlong}

Next, a result on free Lie algebras:

\begin{proposition}
\label{prop.Ufree}Let $V$ be a vector space. We let $\operatorname*{FreeLie}V$
denote the free Lie algebra on the vector space $V$ (not on the set $V$), and
let $T\left(  V\right)  $ denote the tensor algebra of $V$. Then, there exists
a canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}V\right)
\rightarrow T\left(  V\right)  $, which commutes with the canonical injections
of $V$ into $U\left(  \operatorname*{FreeLie}V\right)  $ and into $T\left(
V\right)  $.
\end{proposition}

We are going to prove Proposition \ref{prop.Ufree} by combining the universal
properties of the universal enveloping algebra, the free Lie algebra, and the
tensor algebra. Let us first formulate these properties. First, the universal
property of the universal enveloping algebra:

\begin{theorem}
\label{thm.universal.U}Let $\mathfrak{g}$ be a Lie algebra. We denote by
$\iota_{\mathfrak{g}}^{U}:\mathfrak{g}\rightarrow U\left(  \mathfrak{g}%
\right)  $ the canonical map from $\mathfrak{g}$ into $U\left(  \mathfrak{g}%
\right)  $. (This map $\iota_{\mathfrak{g}}^{U}$ is injective by the
Poincar\'{e}-Birkhoff-Witt theorem, but this is not relevant to the current
theorem.) For any algebra $B$ and any Lie algebra homomorphism $f:\mathfrak{g}%
\rightarrow B$ (where the Lie algebra structure on $B$ is defined by the
commutator of the multiplication of $B$), there exists a unique algebra
homomorphism $F:U\left(  \mathfrak{g}\right)  \rightarrow B$ satisfying
$f=F\circ\iota_{\mathfrak{g}}^{U}$.
\end{theorem}

Next, the universal property of the free Lie algebra:

\begin{theorem}
\label{thm.universal.FreeLie}Let $V$ be a vector space. We denote by
$\iota_{V}^{\operatorname*{FreeLie}}:V\rightarrow\operatorname*{FreeLie}V$ the
canonical map from $V$ into $\operatorname*{FreeLie}V$. (The construction of
$\operatorname*{FreeLie}V$ readily shows that this map $\iota_{V}%
^{\operatorname*{FreeLie}}$ is injective.) For any Lie algebra $\mathfrak{h}$
and any linear map $f:V\rightarrow\mathfrak{h}$, there exists a unique Lie
algebra homomorphism $F:\operatorname*{FreeLie}V\rightarrow\mathfrak{h}$
satisfying $f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$.
\end{theorem}

Finally, the universal property of the tensor algebra:

\begin{theorem}
\label{thm.universal.tensor}Let $V$ be a vector space. We denote by $\iota
_{V}^{T}:V\rightarrow T\left(  V\right)  $ the canonical map from $V$ into
$T\left(  V\right)  $. (This map $\iota_{V}^{T}$ is known to be injective.)
For any algebra $B$ and any linear map $f:V\rightarrow B$, there exists a
unique algebra homomorphism $F:T\left(  V\right)  \rightarrow B$ satisfying
$f=F\circ\iota_{V}^{T}$.
\end{theorem}

\textit{Proof of Proposition \ref{prop.Ufree}.} The algebra $T\left(
V\right)  $ canonically becomes a Lie algebra (by defining the Lie bracket on
$T\left(  V\right)  $ as the commutator of the multiplication). Similarly, the
algebra $U\left(  \operatorname*{FreeLie}V\right)  $ becomes a Lie algebra.

Applying Theorem \ref{thm.universal.FreeLie} to $\mathfrak{h}=T\left(
V\right)  $ and $f=\iota_{V}^{T}$, we obtain that there exists a unique Lie
algebra homomorphism $F:\operatorname*{FreeLie}V\rightarrow T\left(  V\right)
$ satisfying $\iota_{V}^{T}=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. Denote
this Lie algebra homomorphism $F$ by $h$. Then, $h:\operatorname*{FreeLie}%
V\rightarrow T\left(  V\right)  $ is a Lie algebra homomorphism satisfying
$\iota_{V}^{T}=h\circ\iota_{V}^{\operatorname*{FreeLie}}$.

Applying Theorem \ref{thm.universal.U} to $\mathfrak{g}%
=\operatorname*{FreeLie}V$, $B=T\left(  V\right)  $ and $f=h$, we obtain that
there exists a unique algebra homomorphism $F:U\left(  \operatorname*{FreeLie}%
V\right)  \rightarrow T\left(  V\right)  $ satisfying $h=F\circ\iota
_{\operatorname*{FreeLie}V}^{U}$. Denote this algebra homomorphism $F$ by
$\alpha$. Then, $\alpha:U\left(  \operatorname*{FreeLie}V\right)  \rightarrow
T\left(  V\right)  $ is an algebra homomorphism satisfying $h=\alpha\circ
\iota_{\operatorname*{FreeLie}V}^{U}$.

Applying Theorem \ref{thm.universal.tensor} to $B=U\left(
\operatorname*{FreeLie}V\right)  $ and $f=\iota_{\operatorname*{FreeLie}V}%
^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}$, we obtain that there exists a
unique algebra homomorphism $F:T\left(  V\right)  \rightarrow U\left(
\operatorname*{FreeLie}V\right)  $ satisfying $\iota_{\operatorname*{FreeLie}%
V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}=F\circ\iota_{V}^{T}$. Denote
this algebra homomorphism $F$ by $\beta$. Then, $\beta:T\left(  V\right)
\rightarrow U\left(  \operatorname*{FreeLie}V\right)  $ is an algebra
homomorphism satisfying $\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota
_{V}^{\operatorname*{FreeLie}}=\beta\circ\iota_{V}^{T}$.

Both $\alpha$ and $\beta$ are algebra homomorphisms, and therefore Lie algebra
homomorphisms. Also, $\iota_{\operatorname*{FreeLie}V}^{U}$ is a Lie algebra homomorphism.

We have%
\[
\beta\circ\underbrace{\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}}%
_{=h}\circ\iota_{V}^{\operatorname*{FreeLie}}=\beta\circ\underbrace{h\circ
\iota_{V}^{\operatorname*{FreeLie}}}_{=\iota_{V}^{T}}=\beta\circ\iota_{V}%
^{T}=\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}%
^{\operatorname*{FreeLie}}%
\]
and%
\[
\alpha\circ\underbrace{\beta\circ\iota_{V}^{T}}_{=\iota
_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
}=\underbrace{\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}}_{=h}\circ
\iota_{V}^{\operatorname*{FreeLie}}=h\circ\iota_{V}^{\operatorname*{FreeLie}%
}=\iota_{V}^{T}.
\]


Now, applying Theorem \ref{thm.universal.FreeLie} to $\mathfrak{h}=U\left(
\operatorname*{FreeLie}V\right)  $ and $f=\iota_{\operatorname*{FreeLie}V}%
^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}$, we obtain that there exists a
unique Lie algebra homomorphism $F:\operatorname*{FreeLie}V\rightarrow
U\left(  \operatorname*{FreeLie}V\right)  $ satisfying $\iota
_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. Thus, any two Lie algebra
homomorphisms $F:\operatorname*{FreeLie}V\rightarrow U\left(
\operatorname*{FreeLie}V\right)  $ satisfying $\iota_{\operatorname*{FreeLie}%
V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}=F\circ\iota_{V}%
^{\operatorname*{FreeLie}}$ must be equal. Since $\beta\circ\alpha\circ
\iota_{\operatorname*{FreeLie}V}^{U}$ and $\iota_{\operatorname*{FreeLie}%
V}^{U}$ are two such Lie algebra homomorphisms (because we know that
$\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota
_{V}^{\operatorname*{FreeLie}}=\iota_{\operatorname*{FreeLie}V}^{U}\circ
\iota_{V}^{\operatorname*{FreeLie}}$ and clearly $\iota
_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
=\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
$), this yields that $\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}%
V}^{U}$ and $\iota_{\operatorname*{FreeLie}V}^{U}$ must be equal. In other
words,%
\[
\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}=\iota
_{\operatorname*{FreeLie}V}^{U}.
\]


Next, applying Theorem \ref{thm.universal.U} to $\mathfrak{g}%
=\operatorname*{FreeLie}V$, $B=U\left(  \operatorname*{FreeLie}V\right)  $ and
$f=\iota_{\operatorname*{FreeLie}V}^{U}$, we obtain that there exists a unique
algebra homomorphism $F:U\left(  \operatorname*{FreeLie}V\right)  \rightarrow
U\left(  \operatorname*{FreeLie}V\right)  $ satisfying $\iota
_{\operatorname*{FreeLie}V}^{U}=F\circ\iota_{\operatorname*{FreeLie}V}^{U}$.
Thus, any two algebra homomorphisms $F:U\left(  \operatorname*{FreeLie}%
V\right)  \rightarrow U\left(  \operatorname*{FreeLie}V\right)  $ satisfying
$\iota_{\operatorname*{FreeLie}V}^{U}=F\circ\iota_{\operatorname*{FreeLie}%
V}^{U}$ must be equal. Since $\beta\circ\alpha$ and $\operatorname*{id}%
\nolimits_{U\left(  \operatorname*{FreeLie}V\right)  }$ are two such algebra
homomorphisms (because $\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}%
V}^{U}=\iota_{\operatorname*{FreeLie}V}^{U}$ and $\operatorname*{id}%
\nolimits_{U\left(  \operatorname*{FreeLie}V\right)  }\circ\iota
_{\operatorname*{FreeLie}V}^{U}=\iota_{\operatorname*{FreeLie}V}^{U}$), this
yields that $\beta\circ\alpha$ and $\operatorname*{id}\nolimits_{U\left(
\operatorname*{FreeLie}V\right)  }$ must be equal. Thus,%
\[
\beta\circ\alpha=\operatorname*{id}\nolimits_{U\left(  \operatorname*{FreeLie}%
V\right)  }.
\]


On the other hand, applying Theorem \ref{thm.universal.tensor} to $B=T\left(
V\right)  $ and $f=\iota_{V}^{T}$, we obtain that there exists a unique
algebra homomorphism $F:T\left(  V\right)  \rightarrow T\left(  V\right)  $
satisfying $\iota_{V}^{T}=F\circ\iota_{V}^{T}$. Therefore, any two algebra
homomorphisms $F:T\left(  V\right)  \rightarrow T\left(  V\right)  $
satisfying $\iota_{V}^{T}=F\circ\iota_{V}^{T}$ must be equal. Since
$\alpha\circ\beta$ and $\operatorname*{id}\nolimits_{T\left(  V\right)  }$ are
two such algebra homomorphisms (because we know that $\alpha\circ\beta
\circ\iota_{V}^{T}=\iota_{V}^{T}$ and $\operatorname*{id}\nolimits_{T\left(
V\right)  }\circ\iota_{V}^{T}=\iota_{V}^{T}$), this yields that $\alpha
\circ\beta$ and $\operatorname*{id}\nolimits_{T\left(  V\right)  }$ must be
equal. In other words, $\alpha\circ\beta=\operatorname*{id}\nolimits_{T\left(
V\right)  }$. Combined with $\beta\circ\alpha=\operatorname*{id}%
\nolimits_{U\left(  \operatorname*{FreeLie}V\right)  }$, this yields that
$\alpha$ and $\beta$ are mutually inverse, and thus $\alpha$ and $\beta$ are
algebra isomorphisms. Hence, $\alpha:U\left(  \operatorname*{FreeLie}V\right)
\rightarrow T\left(  V\right)  $ is a canonical algebra isomorphism. Also,
$\alpha$ commutes with the canonical injections of $V$ into $U\left(
\operatorname*{FreeLie}V\right)  $ and into $T\left(  V\right)  $, because%
\[
\underbrace{\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}}_{=h}\circ
\iota_{V}^{\operatorname*{FreeLie}}=h\circ\iota_{V}^{\operatorname*{FreeLie}%
}=\iota_{V}^{T}.
\]
Hence, there exists a canonical algebra isomorphism $U\left(
\operatorname*{FreeLie}V\right)  \rightarrow T\left(  V\right)  $, which
commutes with the canonical injections of $V$ into $U\left(
\operatorname*{FreeLie}V\right)  $ and into $T\left(  V\right)  $ (namely,
$\alpha$). Proposition \ref{prop.Ufree} is proven.

\begin{verlong}
There is a special version of Proposition \ref{prop.Ufree} available for
graded vector spaces:

\begin{proposition}
\label{prop.Ufree.gr}Let $Q$ be an abelian group. Let $V$ be a $Q$-graded
vector space. Let us use the notations of Proposition \ref{prop.Ufree}. Then,
the canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}V\right)
\rightarrow T\left(  V\right)  $ constructed in Proposition \ref{prop.Ufree}
is an isomorphism of $Q$\textbf{-graded} algebras.
\end{proposition}

One way to prove Proposition \ref{prop.Ufree.gr} is to scatter the word
``graded'' across the proof of Proposition \ref{prop.Ufree}; of course, we
would need the graded analogues of Theorems \ref{thm.universal.tensor},
\ref{thm.universal.U} and \ref{thm.universal.FreeLie} for this to work. Here
is a slightly different way, which will require only the graded version of
Theorem \ref{thm.universal.tensor}:

\begin{theorem}
\label{thm.universal.tensor.gr}Let $Q$ be an abelian group. Let $V$ be a
$Q$-graded vector space. We denote by $\iota_{V}^{T}:V\rightarrow T\left(
V\right)  $ the canonical map from $V$ into $T\left(  V\right)  $. (This map
$\iota_{V}^{T}$ is known to be injective.) Let $B$ be any $Q$-graded algebra,
and $f:V\rightarrow B$ be any $Q$-graded linear map. According to Theorem
\ref{thm.universal.tensor}, there exists a unique algebra homomorphism
$F:T\left(  V\right)  \rightarrow B$ satisfying $f=F\circ\iota_{V}^{T}$. This
homomorphism $F$ is $Q$-graded.
\end{theorem}

\textit{Proof of Theorem \ref{thm.universal.tensor.gr}.} Consider the unique
algebra homomorphism $F:T\left(  V\right)  \rightarrow B$ satisfying
$f=F\circ\iota_{V}^{T}$. We need to show that this $F$ is $Q$-graded.

For every $n\in\mathbb{N}$ and every $\left(  q_{1},q_{2},...,q_{n}\right)
\in Q^{n}$ and every $w\in\left(  V\left[  q_{1}\right]  \right)
\otimes\left(  V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(
V\left[  q_{n}\right]  \right)  $, we have%
\begin{equation}
F\left(  w\right)  \in B\left[  q_{1}+q_{2}+...+q_{n}\right]  .
\label{pf.universal.tensor.gr.1}%
\end{equation}


\textit{Proof of (\ref{pf.universal.tensor.gr.1}):} We will treat the map
$\iota_{V}^{T}$ as an inclusion map, so that $\iota_{V}^{T}\left(  x\right)
=x$ for every $x\in V$.

Let $n\in\mathbb{N}$ and $\left(  q_{1},q_{2},...,q_{n}\right)  \in Q^{n}$.

We need to prove the relation (\ref{pf.universal.tensor.der.gr.1}) for all
$w\in\left(  V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[
q_{2}\right]  \right)  \otimes...\otimes\left(  V\left[  q_{n}\right]
\right)  $. In order to achieve this, it is enough to prove the relation
(\ref{pf.universal.tensor.der.gr.1}) for all pure tensors $w\in\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)  $ (because every
tensor is a linear combination of pure tensors, but the relation
(\ref{pf.universal.tensor.gr.1}) is linear in $w$). Thus, we can assume WLOG
that $w$ is a pure tensor. Assume this. Then, there exists a $\left(
v_{1},v_{2},...,v_{n}\right)  \in\left(  V\left[  q_{1}\right]  \right)
\times\left(  V\left[  q_{2}\right]  \right)  \times...\times\left(  V\left[
q_{n}\right]  \right)  $ such that $w=v_{1}\otimes v_{2}\otimes...\otimes
v_{n}$. Consider this $\left(  v_{1},v_{2},...,v_{n}\right)  $. We have
$w=v_{1}\otimes v_{2}\otimes...\otimes v_{n}=v_{1}v_{2}...v_{n}$ (since the
multiplication on the algebra $T\left(  V\right)  $ is given by the tensor
product). On the other hand, every $i\in\left\{  1,2,...,n\right\}  $
satisfies $v_{i}\in V\left[  q_{i}\right]  \subseteq V$, and thus $\iota
_{V}^{T}\left(  v_{i}\right)  =v_{i}$ (since $\iota_{V}^{T}\left(  x\right)
=x$ for every $x\in V$), so that $\underbrace{f}_{=F\circ\iota_{V}^{T}}\left(
v_{i}\right)  =\left(  F\circ\iota_{V}^{T}\right)  \left(  v_{i}\right)
=F\left(  \underbrace{\iota_{V}^{T}\left(  v_{i}\right)  }_{=v_{i}}\right)
=F\left(  v_{i}\right)  $. Thus, $\left(  f\left(  v_{1}\right)  ,f\left(
v_{2}\right)  ,...,f\left(  v_{n}\right)  \right)  =\left(  F\left(
v_{1}\right)  ,F\left(  v_{2}\right)  ,...,F\left(  v_{n}\right)  \right)  $,
so that $f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(
v_{n}\right)  =F\left(  v_{1}\right)  F\left(  v_{2}\right)  ...F\left(
v_{n}\right)  $. Hence,%
\begin{align*}
F\left(  w\right)   &  =F\left(  v_{1}v_{2}...v_{n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }w=v_{1}v_{2}...v_{n}\right) \\
&  =F\left(  v_{1}\right)  F\left(  v_{2}\right)  ...F\left(  v_{n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }F\text{ is an algebra homomorphism}%
\right) \\
&  =f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(  v_{n}\right)  .
\end{align*}
But every $i\in\left\{  1,2,...,n\right\}  $ satisfies $f\left(  v_{i}\right)
\in B\left[  q_{i}\right]  $ (because $v_{i}\in V\left[  q_{i}\right]  $ and
since $f$ is $Q$-graded). Hence, $\left(  f\left(  v_{1}\right)  ,f\left(
v_{2}\right)  ,...,f\left(  v_{n}\right)  \right)  \in\left(  B\left[
q_{1}\right]  \right)  \times\left(  B\left[  q_{2}\right]  \right)
\times...\times\left(  B\left[  q_{n}\right]  \right)  $. Thus,
\[
f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(  v_{n}\right)
\in\left(  B\left[  q_{1}\right]  \right)  \left(  B\left[  q_{2}\right]
\right)  ...\left(  B\left[  q_{n}\right]  \right)  \subseteq B\left[
q_{1}+q_{2}+...+q_{n}\right]
\]
(since $B$ is a graded algebra). Altogether, we now have%
\[
F\left(  w\right)  =f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(
v_{n}\right)  \in B\left[  q_{1}+q_{2}+...+q_{n}\right]  .
\]
This proves (\ref{pf.universal.tensor.gr.1}).

Now, let $q\in Q$. By the definition of the grading on a tensor product, we
have%
\[
V^{\otimes n}\left[  q\right]  =\bigoplus\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)
\]
for every $n\in\mathbb{N}$. On the other hand, $T\left(  V\right)
=\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}$ (where the direct sum is a
direct sum of graded vector spaces), so that%
\begin{align*}
\left(  T\left(  V\right)  \right)  \left[  q\right]   &  =\left(
\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}\right)  \left[  q\right]
=\bigoplus\limits_{n\in\mathbb{N}}\underbrace{V^{\otimes n}\left[  q\right]
}_{=\bigoplus\limits_{\substack{\left(  q_{1},q_{2},...,q_{n}\right)  \in
Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(  V\left[  q_{1}\right]  \right)
\otimes\left(  V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(
V\left[  q_{n}\right]  \right)  }\\
&  =\bigoplus\limits_{n\in\mathbb{N}}\bigoplus\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right) \\
&  =\sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(  q_{1}%
,q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(  V\left[
q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)
\end{align*}
(since direct sums are sums). Thus,%
\begin{align*}
&  F\left(  \left(  T\left(  V\right)  \right)  \left[  q\right]  \right)
=F\left(  \sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)  \right) \\
&  =\sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(  q_{1}%
,q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}%
}\underbrace{F\left(  \left(  V\left[  q_{1}\right]  \right)  \otimes\left(
V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(  V\left[
q_{n}\right]  \right)  \right)  }_{\substack{\subseteq B\left[  q_{1}%
+q_{2}+...+q_{n}\right]  \\\text{(since (\ref{pf.universal.tensor.gr.1})
yields that }F\left(  w\right)  \in B\left[  q_{1}+q_{2}+...+q_{n}\right]
\\\text{for every }w\in\left(  V\left[  q_{1}\right]  \right)  \otimes\left(
V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(  V\left[
q_{n}\right]  \right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }F\text{ is linear}\right) \\
&  \subseteq\sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}B\left[
\underbrace{q_{1}+q_{2}+...+q_{n}}_{=q}\right]  =\sum\limits_{n\in\mathbb{N}%
}\sum\limits_{\substack{\left(  q_{1},q_{2},...,q_{n}\right)  \in
Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}B\left[  q\right]  \subseteq B\left[
q\right]
\end{align*}
(since $B\left[  q\right]  $ is a vector space).

Now forget that we fixed $q$. We thus have shown that $F\left(  \left(
T\left(  V\right)  \right)  \left[  q\right]  \right)  \subseteq B\left[
q\right]  $. The map $F$ therefore is $Q$-graded. Theorem
\ref{thm.universal.tensor.gr} is thus proven.

\textit{Proof of Proposition \ref{prop.Ufree.gr}.} Define the maps $\alpha$
and $\beta$ as in the proof of Proposition \ref{prop.Ufree}. Then, $\beta$ is
the unique algebra homomorphism $F:T\left(  V\right)  \rightarrow U\left(
\operatorname*{FreeLie}V\right)  $ satisfying $\iota_{\operatorname*{FreeLie}%
V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}=F\circ\iota_{V}^{T}$. Hence,
Theorem \ref{thm.universal.tensor.gr} (applied to $B=U\left(
\operatorname*{FreeLie}V\right)  $ and $f=\iota_{\operatorname*{FreeLie}V}%
^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}$) yields that this homomorphism
$\beta$ is $Q$-graded. Also, we have seen in the proof of Proposition
\ref{prop.Ufree} that $\beta$ is an algebra isomorphism, and that $\alpha$ and
$\beta$ are mutually inverse.

Since $\alpha$ and $\beta$ are mutually inverse, we have $\alpha=\beta^{-1}$,
so that $\alpha$ is the inverse of a $Q$-graded algebra isomorphism (because
$\beta$ is a $Q$-graded algebra isomorphism). Thus, $\alpha$ itself is
$Q$-graded (because any inverse of a $Q$-graded isomorphism must itself be
$Q$-graded). Since $\alpha$ is the canonical algebra isomorphism $U\left(
\operatorname*{FreeLie}V\right)  \rightarrow T\left(  V\right)  $ constructed
in Proposition \ref{prop.Ufree}, this yields that the canonical algebra
isomorphism $U\left(  \operatorname*{FreeLie}V\right)  \rightarrow T\left(
V\right)  $ constructed in Proposition \ref{prop.Ufree} is $Q$-graded. Hence,
the canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}V\right)
\rightarrow T\left(  V\right)  $ constructed in Proposition \ref{prop.Ufree}
is an isomorphism of $Q$-graded algebras. Proposition \ref{prop.Ufree.gr} is proven.
\end{verlong}

\subsubsection{Universality of the tensor algebra with respect to derivations}

Next, let us notice that the universal property of the tensor algebra (Theorem
\ref{thm.universal.tensor}) has an analogue for derivations in lieu of algebra homomorphisms:

\begin{theorem}
\label{thm.universal.tensor.der}Let $V$ be a vector space. We denote by
$\iota_{V}^{T}:V\rightarrow T\left(  V\right)  $ the canonical map from $V$
into $T\left(  V\right)  $. (This map $\iota_{V}^{T}$ is known to be
injective.) For any $T\left(  V\right)  $-bimodule $M$ and any linear map
$f:V\rightarrow M$, there exists a unique derivation $F:T\left(  V\right)
\rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$.
\end{theorem}

It should be noticed that ``derivation'' means ``$\mathbb{C}$-linear
derivation'' here.

Before we prove this theorem, let us extend its uniqueness part a bit:

\begin{proposition}
\label{prop.derivation.unique}Let $A$ be an algebra. Let $M$ be an
$A$-bimodule, and $d:A\rightarrow M$ and $e:A\rightarrow M$ two derivations.
Let $S$ be a subset of $A$ which generates $A$ as an algebra. Assume that
$d\mid_{S}=e\mid_{S}$. Then, $d=e$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.derivation.unique}.} Let $U$ be the
subset $\operatorname*{Ker}\left(  d-e\right)  $ of $A$. Clearly, $U$ is a
vector space (since $d-e$ is a linear map (since $d$ and $e$ are linear)).

It is known that any derivation $f:A\rightarrow M$ satisfies $f\left(
1\right)  =0$. Applying this to $f=d$, we get $d\left(  1\right)  =0$.
Similarly, $e\left(  1\right)  =0$. Thus, $\left(  d-e\right)  \left(
1\right)  =\underbrace{d\left(  1\right)  }_{=0}-\underbrace{e\left(
1\right)  }_{=0}=0$, so that $1\in\operatorname*{Ker}\left(  d-e\right)  =U$.

Now let $b\in U$ and $c\in U$. Since $b\in U=\operatorname*{Ker}\left(
d-e\right)  $, we have $\left(  d-e\right)  \left(  b\right)  =0$. Thus,
$d\left(  b\right)  -e\left(  b\right)  =\left(  d-e\right)  \left(  b\right)
=0$, so that $d\left(  b\right)  =e\left(  b\right)  $. Similarly, $d\left(
c\right)  =e\left(  c\right)  $.

Now, since $d$ is a derivation, the Leibniz formula yields $d\left(
bc\right)  =d\left(  b\right)  \cdot c+b\cdot d\left(  c\right)  $. Similarly,
$e\left(  bc\right)  =e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)  $.
Hence,%
\begin{align*}
\left(  d-e\right)  \left(  bc\right)   &  =\underbrace{d\left(  bc\right)
}_{=d\left(  b\right)  \cdot c+b\cdot d\left(  c\right)  }%
-\underbrace{e\left(  bc\right)  }_{=e\left(  b\right)  \cdot c+b\cdot
e\left(  c\right)  }=\left(  \underbrace{d\left(  b\right)  }_{=e\left(
b\right)  }\cdot c+b\cdot\underbrace{d\left(  c\right)  }_{=e\left(  c\right)
}\right)  -\left(  e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)
\right) \\
&  =\left(  e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)  \right)
-\left(  e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)  \right)  =0.
\end{align*}
In other words, $bc\in\operatorname*{Ker}\left(  d-e\right)  =U$.

Now forget that we fixed $b$ and $c$. We have thus showed that any $b\in U$
and $c\in U$ satisfy $bc\in U$. Combined with the fact that $U$ is a vector
space and that $1\in U$, this yields that $U$ is a subalgebra of $A$. Since
$S\subseteq U$ (because every $s\in S$ satisfies%
\[
\left(  d-e\right)  \left(  s\right)  =\underbrace{d\left(  s\right)
}_{=\left(  d\mid_{S}\right)  \left(  s\right)  }-\underbrace{e\left(
s\right)  }_{=\left(  e\mid_{S}\right)  \left(  s\right)  }%
=\underbrace{\left(  d\mid_{S}\right)  }_{=e\mid_{S}}\left(  s\right)
-\left(  e\mid_{S}\right)  \left(  s\right)  =\left(  e\mid_{S}\right)
\left(  s\right)  -\left(  e\mid_{S}\right)  \left(  s\right)  =0
\]
and thus $s\in\operatorname*{Ker}\left(  d-e\right)  =U$), this yields that
$U$ is a subalgebra of $A$ containing $S$ as a subset. But since the smallest
subalgebra of $A$ containing $S$ as a subset is $A$ itself (because $S$
generates $A$ as an algebra), this yields that $U\supseteq A$. Hence,
$A\subseteq U=\operatorname*{Ker}\left(  d-e\right)  $, so that $d-e=0$ and
thus $d=e$. Proposition \ref{prop.derivation.unique} is proven.

\textit{Proof of Theorem \ref{thm.universal.tensor.der}.} For any
$n\in\mathbb{N}$, we can define a linear map $\Phi_{n}:V^{\otimes
n}\rightarrow M$ by the equation%
\begin{equation}
\left(
\begin{array}
[c]{r}%
\Phi_{n}\left(  v_{1}\otimes v_{2}\otimes...\otimes v_{n}\right)
=\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\\
\text{for all }v_{1},v_{2},...,v_{n}\in V
\end{array}
\right)  \label{pf.universal.tensor.der.Phin}%
\end{equation}
(by the universal property of the tensor product, since the term
$\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}$ is clearly
multilinear in $v_{1}$, $v_{2}$, $...$, $v_{n}$). Define this map $\Phi_{n}$.
Let $\Phi$ be the map $\bigoplus\limits_{n\in\mathbb{N}}\Phi_{n}%
:\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}\rightarrow M$. Then, every
$n\in\mathbb{N}$ and every $v_{1},v_{2},...,v_{n}$ satisfy%
\begin{align}
\Phi\left(  v_{1}\otimes v_{2}\otimes...\otimes v_{n}\right)   &  =\Phi
_{n}\left(  v_{1}\otimes v_{2}\otimes...\otimes v_{n}\right) \nonumber\\
&  =\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}.
\label{pf.universal.tensor.der.Phi}%
\end{align}


Since $\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}=T\left(  V\right)  $,
the map $\Phi$ is a map from $T\left(  V\right)  $ to $M$. We will now prove
that $\Phi$ is a derivation. In fact, in order to prove this, we must show
that%
\begin{equation}
\Phi\left(  ab\right)  =\Phi\left(  a\right)  \cdot b+a\cdot\Phi\left(
b\right)  \ \ \ \ \ \ \ \ \ \ \text{for any }a\in T\left(  V\right)  \text{
and }b\in T\left(  V\right)  . \label{pf.universal.tensor.der.Phider}%
\end{equation}


\textit{Proof of (\ref{pf.universal.tensor.der.Phider}):} Every element of
$T\left(  V\right)  $ is a linear combination of elements of $V^{\otimes n}$
for various $n\in\mathbb{N}$ (because $T\left(  V\right)  =\bigoplus
\limits_{n\in\mathbb{N}}V^{\otimes n}$). Meanwhile, every element of
$V^{\otimes n}$ for any $n\in\mathbb{N}$ is a linear combination of pure
tensors. Combining these two observations, we see that every element of
$T\left(  V\right)  $ is a linear combination of pure tensors.

We need to prove the equation (\ref{pf.universal.tensor.der.Phider}) for all
$a\in T\left(  V\right)  $ and $b\in T\left(  V\right)  $. Since this equation
is linear in each of $a$ and $b$, we can WLOG assume that $a$ and $b$ are pure
tensors (since every element of $T\left(  V\right)  $ is a linear combination
of pure tensors). Assume this. Then, $a$ is a pure tensor, so that there
exists an $n\in\mathbb{N}$ and some $v_{1},v_{2},...,v_{n}\in V$ satisfying
$a=v_{1}\otimes v_{2}\otimes...\otimes v_{n}$. Consider this $n$ and these
$v_{1},v_{2},...,v_{n}$. Also, $b$ is a pure tensor, so that there exists an
$m\in\mathbb{N}$ and some $w_{1},w_{2},...,w_{m}\in V$ satisfying
$b=w_{1}\otimes w_{2}\otimes...\otimes w_{m}$. Consider this $m$ and these
$w_{1},w_{2},...,w_{m}$.

By (\ref{pf.universal.tensor.der.Phi}) (applied to $m$ and $w_{1}%
,w_{2},...,w_{m}$ instead of $n$ and $v_{1},v_{2},...,v_{n}$), we have%
\begin{align*}
\Phi\left(  w_{1}\otimes w_{2}\otimes...\otimes w_{m}\right)   &
=\sum\limits_{k=1}^{m}w_{1}\cdot w_{2}\cdot...\cdot w_{k-1}\cdot f\left(
w_{k}\right)  \cdot w_{k+1}\cdot w_{k+2}\cdot...\cdot w_{m}\\
&  =\sum\limits_{k=n+1}^{n+m}w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot
f\left(  w_{k-n}\right)  \cdot w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}%
\end{align*}
(here, we substituted $k-n$ for $k$ in the sum).

Let $\left(  u_{1},u_{2},...,u_{n+m}\right)  $ be the $\left(  n+m\right)
$-tuple $\left(  v_{1},v_{2},...,v_{n},w_{1},w_{2},...,w_{m}\right)  $. Then,
\[
u_{1}\otimes u_{2}\otimes...\otimes u_{n+m}=\underbrace{v_{1}\otimes
v_{2}\otimes...\otimes v_{n}}_{=a}\otimes\underbrace{w_{1}\otimes w_{2}%
\otimes...\otimes w_{m}}_{=b}=a\otimes b=ab.
\]


By (\ref{pf.universal.tensor.der.Phi}) (applied to $n+m$ and $u_{1}%
,u_{2},...,u_{n+m}$ instead of $n$ and $v_{1},v_{2},...,v_{n}$), we have%
\begin{align*}
&  \Phi\left(  u_{1}\otimes u_{2}\otimes...\otimes u_{n+m}\right) \\
&  =\sum\limits_{k=1}^{n+m}u_{1}\cdot u_{2}\cdot...\cdot u_{k-1}\cdot f\left(
u_{k}\right)  \cdot u_{k+1}\cdot u_{k+2}\cdot...\cdot u_{n+m}\\
&  =\sum\limits_{k=1}^{n}\underbrace{u_{1}\cdot u_{2}\cdot...\cdot
u_{k-1}\cdot f\left(  u_{k}\right)  \cdot u_{k+1}\cdot u_{k+2}\cdot...\cdot
u_{n+m}}_{\substack{=v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\cdot w_{1}\cdot
w_{2}\cdot...\cdot w_{m}\\\text{(since }\left(  u_{1},u_{2},...,u_{n+m}%
\right)  =\left(  v_{1},v_{2},...,v_{n},w_{1},w_{2},...,w_{m}\right)  \text{
and }k\leq n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k=n+1}^{n+m}\underbrace{u_{1}\cdot
u_{2}\cdot...\cdot u_{k-1}\cdot f\left(  u_{k}\right)  \cdot u_{k+1}\cdot
u_{k+2}\cdot...\cdot u_{n+m}}_{\substack{=v_{1}\cdot v_{2}\cdot...\cdot
v_{n}\cdot w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot f\left(
w_{k-n}\right)  \cdot w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}%
\\\text{(since }\left(  u_{1},u_{2},...,u_{n+m}\right)  =\left(  v_{1}%
,v_{2},...,v_{n},w_{1},w_{2},...,w_{m}\right)  \text{ and }k>n\text{)}}}\\
&  =\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\cdot
\underbrace{w_{1}\cdot w_{2}\cdot...\cdot w_{m}}_{=w_{1}\otimes w_{2}%
\otimes...\otimes w_{m}=b}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k=n+1}^{n+m}\underbrace{v_{1}\cdot
v_{2}\cdot...\cdot v_{n}}_{=v_{1}\otimes v_{2}\otimes...\otimes v_{n}=a}\cdot
w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot f\left(  w_{k-n}\right)  \cdot
w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}\\
&  =\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\cdot b\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k=n+1}^{n+m}a\cdot w_{1}\cdot w_{2}%
\cdot...\cdot w_{k-n-1}\cdot f\left(  w_{k-n}\right)  \cdot w_{k-n+1}\cdot
w_{k-n+2}\cdot...\cdot w_{m}\\
&  =\underbrace{\left(  \sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot
v_{k-1}\cdot f\left(  v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot
v_{n}\right)  }_{\substack{=\Phi\left(  v_{1}\otimes v_{2}\otimes...\otimes
v_{n}\right)  \\\text{(by (\ref{pf.universal.tensor.der.Phi}))}}}\cdot b\\
&  \ \ \ \ \ \ \ \ \ \ +a\cdot\underbrace{\left(  \sum\limits_{k=n+1}%
^{n+m}w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot f\left(  w_{k-n}\right)
\cdot w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}\right)  }_{=\Phi\left(
w_{1}\otimes w_{2}\otimes...\otimes w_{m}\right)  }\\
&  =\Phi\left(  \underbrace{v_{1}\otimes v_{2}\otimes...\otimes v_{n}}%
_{=a}\right)  \cdot b+a\cdot\Phi\left(  \underbrace{w_{1}\otimes w_{2}%
\otimes...\otimes w_{m}}_{=b}\right)  =\Phi\left(  a\right)  \cdot
b+a\cdot\Phi\left(  b\right)  .
\end{align*}
Thus, (\ref{pf.universal.tensor.der.Phider}) is proven.

Now that we know that $\Phi$ satisfies (\ref{pf.universal.tensor.der.Phider}),
we conclude that $\Phi$ is a derivation.

Next, notice that every $v\in V$ satisfies $\iota_{V}^{T}\left(  v\right)  =v$
(since $\iota_{V}^{T}$ is just the inclusion map). Hence, every $v\in V$
satisfies%
\begin{align*}
\left(  \Phi\circ\iota_{V}^{T}\right)  \left(  v\right)   &  =\Phi\left(
\underbrace{\iota_{V}^{T}\left(  v\right)  }_{\substack{=v}}\right)
=\Phi\left(  v\right) \\
&  =\sum\limits_{k=1}^{1}f\left(  v\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.universal.tensor.der.Phi}), applied to }n=1\text{ and }%
v_{1}=v\right) \\
&  =f\left(  v\right)  .
\end{align*}
Thus, $\Phi\circ\iota_{V}^{T}=f$.

So we know that $\Phi$ is a derivation satisfying $f=\Phi\circ\iota_{V}^{T}$.
Thus, we have shown that there exists a derivation $F:T\left(  V\right)
\rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$ (namely, $F=\Phi$). In order
to complete the proof of Theorem \ref{thm.universal.tensor.der}, we only need
to check that this derivation is unique. In other words, we need to check that
whenever a derivation $F:T\left(  V\right)  \rightarrow M$ satisfies
$f=F\circ\iota_{V}^{T}$, we must have $F=\Phi$. Let us prove this now. Let
$F:T\left(  V\right)  \rightarrow M$ be any derivation satisfying
$f=F\circ\iota_{V}^{T}$. Then, every $v\in V$ satisfies%
\begin{align*}
\left(  F\mid_{V}\right)  \left(  v\right)   &  =F\left(  \underbrace{v}%
_{=\iota_{V}^{T}\left(  v\right)  }\right)  =F\left(  \iota_{V}^{T}\left(
v\right)  \right)  =\underbrace{\left(  F\circ\iota_{V}^{T}\right)  }%
_{=f=\Phi\circ\iota_{V}^{T}}\left(  v\right)  =\left(  \Phi\circ\iota_{V}%
^{T}\right)  \left(  v\right) \\
&  =\Phi\left(  \underbrace{\iota_{V}^{T}\left(  v\right)  }_{=v}\right)
=\Phi\left(  v\right)  =\left(  \Phi\mid_{V}\right)  \left(  v\right)  .
\end{align*}
Thus, $F\mid_{V}=\Phi\mid_{V}$. Proposition \ref{prop.derivation.unique}
(applied to $A=T\left(  V\right)  $, $d=F$, $e=\Phi$ and $S=V$) thus yields
$F=\Phi$ (since $V$ generates $T\left(  V\right)  $ as an algebra). This
completes the proof of Theorem \ref{thm.universal.tensor.der} (as we have seen above).

\begin{verlong}
We record a graded version of Theorem \ref{thm.universal.tensor.der}:

\begin{theorem}
\label{thm.universal.tensor.der.gr}Let $Q$ be an abelian group. Let $V$ be a
$Q$-graded vector space. We denote by $\iota_{V}^{T}:V\rightarrow T\left(
V\right)  $ the canonical map from $V$ into $T\left(  V\right)  $. (This map
$\iota_{V}^{T}$ is known to be injective and $Q$-graded.) For any $Q$-graded
$T\left(  V\right)  $-bimodule $M$ and any $Q$-graded linear map
$f:V\rightarrow M$, there exists a unique $Q$-graded derivation $F:T\left(
V\right)  \rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.universal.tensor.der.gr}.} We will treat the
map $\iota_{V}^{T}$ as an inclusion map, so that $\iota_{V}^{T}\left(
x\right)  =x$ for every $x\in V$.

Define the map $\Phi$ as in the proof of Theorem
\ref{thm.universal.tensor.der}. As we have seen in the proof of Theorem
\ref{thm.universal.tensor.der}, this map $\Phi$ is a derivation $T\left(
V\right)  \rightarrow M$ satisfying $f=\Phi\circ\iota_{V}^{T}$. We will now
prove that $\Phi$ is $Q$-graded.

For every $Q$-graded vector space $W$ and every $q\in Q$, let $\pi_{q}^{W}$ be
the canonical projection from $W$ to the $q$-th homogeneous component
$W\left[  q\right]  $. Of course, for every $Q$-graded vector space $W$ and
every $w\in W$, we have%
\begin{equation}
w=\sum\limits_{q\in Q}\pi_{q}^{W}\left(  w\right)  .
\label{pf.universal.tensor.der.gr.0}%
\end{equation}
Let us notice that for every $Q$-graded vector space $W$ and any two distinct
elements $p$ and $q$ of $Q$, we have%
\begin{equation}
\pi_{q}^{W}\left(  W\left[  p\right]  \right)  =0
\label{pf.universal.tensor.der.gr.0a}%
\end{equation}
(because $\pi_{q}^{W}$ is the projection of the $Q$-graded vector space $W$
onto its $q$-th homogeneous component $W\left[  q\right]  $, while $W\left[
p\right]  $ is a homogeneous component of $W$ distinct from $W\left[
q\right]  $).

For every $q\in Q$, let $\Phi_{q}$ be the linear map $\left(  T\left(
V\right)  \right)  \left[  q\right]  \rightarrow M\left[  q\right]  $ defined
by%
\[
\left(  \Phi_{q}\left(  x\right)  =\pi_{q}^{M}\left(  \Phi\left(  x\right)
\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }x\in\left(  T\left(  V\right)
\right)  \left[  q\right]  \right)  .
\]
Then, the direct sum $\bigoplus\limits_{q\in Q}\Phi_{q}:\bigoplus\limits_{q\in
Q}\left(  T\left(  V\right)  \right)  \left[  q\right]  \rightarrow
\bigoplus\limits_{q\in Q}M\left[  q\right]  $ is a $Q$-graded linear map from
$T\left(  V\right)  $ to $M$ (since $\bigoplus\limits_{q\in Q}\left(  T\left(
V\right)  \right)  \left[  q\right]  =T\left(  V\right)  $ and $\bigoplus
\limits_{q\in Q}M\left[  q\right]  =M$). Denote this map $\bigoplus
\limits_{q\in Q}\Phi_{q}$ by $\Phi^{\prime}$.

Every $r\in Q$ and every $x\in\left(  T\left(  V\right)  \right)  \left[
r\right]  $ satisfy%
\begin{align}
\Phi^{\prime}\left(  x\right)   &  =\left(  \bigoplus\limits_{q\in Q}%
\Phi\left[  q\right]  \right)  \left(  x\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Phi^{\prime}=\bigoplus\limits_{q\in Q}\Phi\left[  q\right]
\right) \nonumber\\
&  =\Phi_{r}\left(  x\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
x\in\left(  T\left(  V\right)  \right)  \left[  r\right]  \right) \nonumber\\
&  =\pi_{r}^{M}\left(  \Phi\left(  x\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi_{r}\right)  .
\label{pf.universal.tensor.der.gr.1}%
\end{align}


Now, every $r\in Q$ and every $x\in V\left[  r\right]  $ satisfy%
\begin{align*}
\left(  \Phi^{\prime}\circ\iota_{V}^{T}\right)  \left(  x\right)   &
=\Phi^{\prime}\left(  \underbrace{\iota_{V}^{T}\left(  x\right)  }%
_{=x}\right)  =\Phi^{\prime}\left(  x\right) \\
&  =\pi_{r}^{M}\left(  \Phi\left(  \underbrace{x}_{=\iota_{V}^{T}\left(
x\right)  }\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.universal.tensor.der.gr.1}) (since }x\in V\left[  r\right]
\subseteq\left(  T\left(  V\right)  \right)  \left[  r\right]  \text{)}\right)
\\
&  =\pi_{r}^{M}\left(  \underbrace{\Phi\left(  \iota_{V}^{T}\left(  x\right)
\right)  }_{=\left(  \Phi\circ\iota_{V}^{T}\right)  \left(  x\right)
}\right)  =\pi_{r}^{M}\left(  \underbrace{\left(  \Phi\circ\iota_{V}%
^{T}\right)  }_{=f}\left(  x\right)  \right)  =\pi_{r}^{M}\left(  f\left(
x\right)  \right)  =f\left(  x\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }f\left(  x\right)  \in M\left[  r\right]  \text{ (because
}f\text{ is }Q\text{-graded and }x\in V\left[  r\right]  \text{), and thus}\\
\text{the projection }\pi_{r}^{M}\text{ onto }M\left[  r\right]  \text{ leaves
}f\left(  x\right)  \text{ invariant}%
\end{array}
\right)  .
\end{align*}
Thus, $\Phi^{\prime}\circ\iota_{V}^{T}=f$.

We will next show that $\Phi^{\prime}$ is a derivation. Indeed, in order to
prove this, we must show that%
\begin{equation}
\Phi^{\prime}\left(  ab\right)  =\Phi^{\prime}\left(  a\right)  \cdot
b+a\cdot\Phi^{\prime}\left(  b\right)  \ \ \ \ \ \ \ \ \ \ \text{for any }a\in
T\left(  V\right)  \text{ and }b\in T\left(  V\right)  .
\label{pf.universal.tensor.der.gr.2}%
\end{equation}


\textit{Proof of (\ref{pf.universal.tensor.der.gr.2}):} We need to prove the
equation (\ref{pf.universal.tensor.der.gr.2}) for all $a\in T\left(  V\right)
$ and $b\in T\left(  V\right)  $. Since this equation is linear in each of $a$
and $b$, we can WLOG assume that $a$ and $b$ are homogeneous (since every
element of $T\left(  V\right)  $ is a linear combination of homogeneous
elements). Assume this. Then, $a$ is homogeneous, so there exists a $p\in Q$
such that $a\in\left(  T\left(  V\right)  \right)  \left[  p\right]  $.
Consider this $p$. Since $b$ is homogeneous, there exists an $r\in Q$ such
that $b\in\left(  T\left(  V\right)  \right)  \left[  r\right]  $. Consider
this $r$. Since $a\in\left(  T\left(  V\right)  \right)  \left[  p\right]  $
and $b\in\left(  T\left(  V\right)  \right)  \left[  r\right]  $, we have
$ab\in\left(  T\left(  V\right)  \right)  \left[  p+r\right]  $ (since
$T\left(  V\right)  $ is a $Q$-graded algebra), so that%
\begin{align*}
\Phi^{\prime}\left(  ab\right)   &  =\pi_{p+r}^{M}\left(  \underbrace{\Phi
\left(  ab\right)  }_{\substack{=\Phi\left(  a\right)  \cdot b+a\cdot
\Phi\left(  b\right)  \\\text{(since }\Phi\text{ is a derivation)}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.universal.tensor.der.gr.1}),
applied to }ab\text{ and }p+r\text{ instead of }x\text{ and }r\right) \\
&  =\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b+a\cdot\Phi\left(
b\right)  \right)  =\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)
+\pi_{p+r}^{M}\left(  a\cdot\Phi\left(  b\right)  \right)  .
\end{align*}
Now, it is easy to see that $\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot
b\right)  =\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot
b$\ \ \ \ \footnote{\textit{Proof.} Applying
(\ref{pf.universal.tensor.der.gr.0}) to $W=M$ and $w=\Phi\left(  a\right)  $,
we obtain $\Phi\left(  a\right)  =\sum\limits_{q\in Q}\pi_{q}^{M}\left(
\Phi\left(  a\right)  \right)  $, so that%
\begin{align*}
\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)   &  =\pi_{p+r}%
^{M}\left(  \sum\limits_{q\in Q}\pi_{q}^{M}\left(  \Phi\left(  a\right)
\right)  \cdot b\right)  =\sum\limits_{q\in Q}\pi_{p+r}^{M}\left(  \pi_{q}%
^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right) \\
&  =\pi_{p+r}^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)
\cdot b\right)  +\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(
\pi_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  .
\end{align*}
\par
Now, for every $q\in Q$, we have $\underbrace{\pi_{q}^{M}\left(  \Phi\left(
a\right)  \right)  }_{\substack{\in M\left[  q\right]  \\\text{(since }\pi
_{q}^{M}\text{ is a}\\\text{projection on }M\left[  q\right]  \text{)}}%
}\cdot\underbrace{b}_{\in\left(  T\left(  V\right)  \right)  \left[  r\right]
}\in\left(  M\left[  q\right]  \right)  \cdot\left(  \left(  T\left(
V\right)  \right)  \left[  r\right]  \right)  \subseteq M\left[  q+r\right]  $
(since $M$ is a graded $T\left(  V\right)  $-bimodule). For every $q\in Q$
satisfying $q+r\neq p+r$, we have $\pi_{p+r}^{W}\left(  M\left[  q+r\right]
\right)  =0$ (by (\ref{pf.universal.tensor.der.gr.0a}), applied to $M$, $p+r$
and $q+r$ instead of $W$, $q$ and $p$). Thus,%
\[
\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(  \underbrace{\pi
_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b}_{\in M\left[
q+r\right]  }\right)  \in\sum\limits_{\substack{q\in Q;\\q\neq p}%
}\underbrace{\pi_{p+r}^{M}\left(  M\left[  q+r\right]  \right)  }%
_{\substack{=0\\\text{(since }q+r\neq p+r\\\text{(because }q\neq p\text{))}%
}}=\sum\limits_{\substack{q\in Q;\\q\neq p}}0=0,
\]
so that $\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(  \pi
_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  =0$. Hence,%
\[
\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)  =\pi_{p+r}%
^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)
+\underbrace{\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(
\pi_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  }_{=0}%
=\pi_{p+r}^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot
b\right)  .
\]
On the other hand, $\underbrace{\pi_{p}^{M}\left(  \Phi\left(  a\right)
\right)  }_{\substack{\in M\left[  p\right]  \\\text{(since }\pi_{p}^{M}\text{
is a}\\\text{projection on }M\left[  p\right]  \text{)}}}\cdot\underbrace{b}%
_{\in\left(  T\left(  V\right)  \right)  \left[  r\right]  }\in\left(
M\left[  p\right]  \right)  \cdot\left(  \left(  T\left(  V\right)  \right)
\left[  r\right]  \right)  \subseteq M\left[  p+r\right]  $ (since $M$ is a
graded $T\left(  V\right)  $-bimodule). Thus, $\pi_{p+r}^{M}\left(  \pi
_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  =\pi_{p}%
^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b$ (because $\pi_{p+r}^{M}$
is a projection on $M\left[  p+r\right]  $ and thus leaves every element in
$M\left[  p+r\right]  $ fixed). Hence,%
\[
\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)  =\pi_{p+r}%
^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)
=\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b,
\]
qed.}. Since $\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  =\Phi^{\prime
}\left(  a\right)  $ (because (\ref{pf.universal.tensor.der.gr.1}) (applied to
$a$ and $p$ instead of $x$ and $r$) yields $\Phi^{\prime}\left(  a\right)
=\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  $), this rewrites as
$\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)  =\Phi^{\prime
}\left(  a\right)  \cdot b$. The same argument (but with the right action of
$T\left(  V\right)  $ on $M$ replaced by left action) shows that $\pi
_{p+r}^{M}\left(  b\cdot\Phi\left(  a\right)  \right)  =b\cdot\Phi^{\prime
}\left(  a\right)  $. If we apply this equality to $a$, $b$, $p$ and $r$ in
lieu of $b$, $a$, $r$ and $p$, we obtain $\pi_{r+p}^{M}\left(  a\cdot
\Phi\left(  b\right)  \right)  =a\cdot\Phi^{\prime}\left(  b\right)  $. In
other words, $\pi_{p+r}^{M}\left(  a\cdot\Phi\left(  b\right)  \right)
=a\cdot\Phi^{\prime}\left(  b\right)  $. Thus,%
\begin{align*}
\Phi^{\prime}\left(  ab\right)   &  =\underbrace{\pi_{p+r}^{M}\left(
\Phi\left(  a\right)  \cdot b\right)  }_{=\Phi^{\prime}\left(  a\right)  \cdot
b}+\underbrace{\pi_{p+r}^{M}\left(  a\cdot\Phi\left(  b\right)  \right)
}_{=a\cdot\Phi^{\prime}\left(  b\right)  }\\
&  =\Phi^{\prime}\left(  a\right)  \cdot b+a\cdot\Phi^{\prime}\left(
b\right)  .
\end{align*}
This proves (\ref{pf.universal.tensor.der.gr.2}).

From (\ref{pf.universal.tensor.der.gr.2}), it becomes clear that $\Phi
^{\prime}$ is a derivation. Since $\Phi^{\prime}$ also is $Q$-graded and
satisfies $f=\Phi^{\prime}\circ\iota_{V}^{T}$, we thus conclude that there
exists a $Q$-graded derivation $F:T\left(  V\right)  \rightarrow M$ satisfying
$f=F\circ\iota_{V}^{T}$ (namely, $F=\Phi$). Combining this with the fact that
there exists \textbf{at most one} $Q$-graded derivation $F:T\left(  V\right)
\rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$%
\ \ \ \ \footnote{\textit{Proof.} Theorem \ref{thm.universal.tensor.der}
yields that there exists a unique derivation $F:T\left(  V\right)  \rightarrow
M$ satisfying $f=F\circ\iota_{V}^{T}$. Hence, there exists at most one
derivation $F:T\left(  V\right)  \rightarrow M$ satisfying $f=F\circ\iota
_{V}^{T}$. In particular, there exists at most one $Q$-graded derivation
$F:T\left(  V\right)  \rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$,
qed.}, we conclude that there exists \textbf{a unique} $Q$-graded derivation
$F:T\left(  V\right)  \rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$.
Theorem \ref{thm.universal.tensor.der.gr} is proven.
\end{verlong}

We will later use a corollary of Proposition \ref{prop.derivation.unique}:

\begin{corollary}
\label{cor.derivation.unique.ihg}Let $A$ be an algebra. Let $B$ be a
subalgebra of $A$. Let $C$ be a subalgebra of $B$. Let $d:A\rightarrow A$ be a
derivation of the algebra $A$. Let $S$ be a subset of $C$ which generates $C$
as an algebra. Assume that $d\left(  S\right)  \subseteq B$. Then, $d\left(
C\right)  \subseteq B$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.derivation.unique.ihg}.} Since $C\subseteq
B\subseteq A$, the vector spaces $A$ and $B$ become $C$-modules.

Let $\pi:A\rightarrow A\diagup B$ be the canonical projection. Clearly, $\pi$
is a $C$-module homomorphism, and satisfies $\operatorname*{Ker}\pi=B$. Let
$d^{\prime}:C\rightarrow A\diagup B$ be the restriction of the map $\pi\circ
d:A\rightarrow A\diagup B$ to $C$. It is easy to see that $d^{\prime
}:C\rightarrow A\diagup B$ is a derivation\footnote{\textit{Proof.} Every
$x\in C$ and $y\in C$ satisfy%
\begin{align*}
d^{\prime}\left(  xy\right)   &  =\left(  \pi\circ d\right)  \left(
xy\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }d^{\prime}\text{ is the
restriction of }\pi\circ d\text{ to }C\right) \\
&  =\pi\left(  \underbrace{d\left(  xy\right)  }_{\substack{=d\left(
x\right)  \cdot y+x\cdot d\left(  y\right)  \\\text{(since }d\text{ is a
derivation)}}}\right)  =\pi\left(  d\left(  x\right)  \cdot y+x\cdot d\left(
y\right)  \right) \\
&  =\underbrace{\pi\left(  d\left(  x\right)  \cdot y\right)  }%
_{\substack{=\pi\left(  d\left(  x\right)  \right)  \cdot y\\\text{(since }%
\pi\text{ is a }C\text{-module}\\\text{homomorphism)}}}+\underbrace{\pi\left(
x\cdot d\left(  y\right)  \right)  }_{\substack{=x\cdot\pi\left(  d\left(
y\right)  \right)  \\\text{(since }\pi\text{ is a }C\text{-module}%
\\\text{homomorphism)}}}\\
&  =\underbrace{\pi\left(  d\left(  x\right)  \right)  }_{\substack{=\left(
\pi\circ d\right)  \left(  x\right)  =d^{\prime}\left(  x\right)
\\\text{(since }d^{\prime}\text{ is the restriction of}\\\pi\circ d\text{ to
}C\text{, and since }x\in C\text{)}}}\cdot y+x\cdot\underbrace{\pi\left(
d\left(  y\right)  \right)  }_{\substack{=\left(  \pi\circ d\right)  \left(
y\right)  =d^{\prime}\left(  y\right)  \\\text{(since }d^{\prime}\text{ is the
restriction of}\\\pi\circ d\text{ to }C\text{, and since }y\in C\text{)}%
}}=d^{\prime}\left(  x\right)  \cdot y+x\cdot d^{\prime}\left(  y\right)  .
\end{align*}
Thus, $d^{\prime}$ is a derivation, qed.}. On the other hand, $0:C\rightarrow
A\diagup B$ is a derivation as well. Every $s\in S$ satisfies%
\begin{align*}
\left(  d^{\prime}\mid_{S}\right)  \left(  s\right)   &  =d^{\prime}\left(
s\right)  =\left(  \pi\circ d\right)  \left(  s\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }d^{\prime}\text{ is the restriction
of }\pi\circ d\text{ to }C\right) \\
&  =\pi\left(  d\left(  s\right)  \right)  =0\ \ \ \ \ \ \ \ \ \ \left(
\text{since }d\left(  \underbrace{s}_{\in S}\right)  \in d\left(  S\right)
\subseteq B=\operatorname*{Ker}\pi\right) \\
&  =0\left(  s\right)  =\left(  0\mid_{S}\right)  \left(  s\right)  .
\end{align*}
Thus, $d^{\prime}\mid_{S}=0\mid_{S}$. Proposition \ref{prop.derivation.unique}
(applied to $C$, $A\diagup M$, $d^{\prime}$ and $0$ instead of $A$, $M$, $d$
and $e$) therefore yields that $d^{\prime}=0$ on $C$. But since $d^{\prime}$
is the restriction of $\pi\circ d$ to $C$, we have $d^{\prime}=\left(
\pi\circ d\right)  \mid_{C}$. Thus, $\left(  \pi\circ d\right)  \mid
_{C}=d^{\prime}=0$, so that $\left(  \pi\circ d\right)  \left(  C\right)  =0$.
Thus, $\pi\left(  d\left(  C\right)  \right)  =\left(  \pi\circ d\right)
\left(  C\right)  =0$, so that $d\left(  C\right)  \subseteq
\operatorname*{Ker}\pi=B$. Corollary \ref{cor.derivation.unique.ihg} is
therefore proven.

\begin{corollary}
\label{cor.derivation.Lie.semidir}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a vector space equipped with both a Lie algebra structure
and a $\mathfrak{g}$-module structure. Assume that $\mathfrak{g}$ acts on
$\mathfrak{h}$ by derivations. Consider the semidirect product $\mathfrak{g}%
\ltimes\mathfrak{h}$ defined as in Definition \ref{def.semidir.lielie}
\textbf{(b)}. Consider $\mathfrak{g}$ as a Lie subalgebra of $\mathfrak{g}%
\ltimes\mathfrak{h}$. Consider $\mathfrak{g}\ltimes\mathfrak{h}$ as a Lie
subalgebra of $U\left(  \mathfrak{g}\ltimes\mathfrak{h}\right)  $ (where the
Lie bracket on $U\left(  \mathfrak{g}\ltimes\mathfrak{h}\right)  $ is defined
as the commutator of the multiplication). Consider $\mathfrak{h}$ as a Lie
subalgebra of $\mathfrak{g}\ltimes\mathfrak{h}$, whence $U\left(
\mathfrak{h}\right)  $ becomes a subalgebra of $U\left(  \mathfrak{g}%
\ltimes\mathfrak{h}\right)  $.

Then, $\left[  \mathfrak{g},U\left(  \mathfrak{h}\right)  \right]  \subseteq
U\left(  \mathfrak{h}\right)  $ (as subsets of $U\left(  \mathfrak{g}%
\ltimes\mathfrak{h}\right)  $).
\end{corollary}

\textit{Proof of Corollary \ref{cor.derivation.Lie.semidir}.} Let
$x\in\mathfrak{g}$. Define a map $\xi:U\left(  \mathfrak{g}\ltimes
\mathfrak{h}\right)  \rightarrow U\left(  \mathfrak{g}\ltimes\mathfrak{h}%
\right)  $ by%
\[
\left(  \xi\left(  y\right)  =\left[  x,y\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }y\in U\left(  \mathfrak{g}\ltimes
\mathfrak{h}\right)  \right)  .
\]
Then, $\xi$ is clearly a derivation of the algebra $U\left(  \mathfrak{g}%
\ltimes\mathfrak{h}\right)  $.

We are identifying $\mathfrak{g}$ with a Lie subalgebra of $\mathfrak{g}%
\ltimes\mathfrak{h}$. Clearly, $x\in\mathfrak{g}$ corresponds to $\left(
x,0\right)  \in\mathfrak{g}\ltimes\mathfrak{h}$ under this identification.

We are also identifying $\mathfrak{h}$ with a Lie subalgebra of $\mathfrak{g}%
\ltimes\mathfrak{h}$. Every $y\in\mathfrak{h}$ corresponds to $\left(
0,y\right)  \in\mathfrak{g}\ltimes\mathfrak{h}$ under this identification.

Thus, every $y\in\mathfrak{h}$ satisfies%
\begin{align*}
\left[  \underbrace{x}_{=\left(  x,0\right)  },\underbrace{y}_{=\left(
0,y\right)  }\right]   &  =\left[  \left(  x,0\right)  ,\left(  0,y\right)
\right]  =\left(  \underbrace{\left[  x,0\right]  }_{=0},\underbrace{\left[
0,y\right]  }_{=0}+x\rightharpoonup y-\underbrace{0\rightharpoonup0}%
_{=0}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the Lie bracket on
}\mathfrak{g}\ltimes\mathfrak{h}\right) \\
&  =\left(  0,x\rightharpoonup y\right)  =x\rightharpoonup y\in\mathfrak{h}.
\end{align*}
Hence, $\xi\left(  y\right)  =\left[  x,y\right]  \in\mathfrak{h}$ for every
$y\in\mathfrak{h}$. Thus, $\xi\left(  \mathfrak{h}\right)  \subseteq
\mathfrak{h}\subseteq U\left(  \mathfrak{h}\right)  $.

Now, we notice that the subset $\mathfrak{h}$ of $U\left(  \mathfrak{h}%
\right)  $ generates $U\left(  \mathfrak{h}\right)  $ as an algebra. Thus,
Corollary \ref{cor.derivation.unique.ihg} (applied to $A=U\left(
\mathfrak{g}\ltimes\mathfrak{h}\right)  $, $B=U\left(  \mathfrak{h}\right)  $,
$C=U\left(  \mathfrak{h}\right)  $, $d=\xi$ and $S=\mathfrak{h}$) yields
$\xi\left(  U\left(  \mathfrak{h}\right)  \right)  \subseteq U\left(
\mathfrak{h}\right)  $. Hence, every $u\in U\left(  \mathfrak{h}\right)  $
satisfies $\xi\left(  u\right)  \in U\left(  \mathfrak{h}\right)  $. But since
$\xi\left(  u\right)  =\left[  x,u\right]  $ (by the definition of $\xi$),
this yields that every $u\in U\left(  \mathfrak{h}\right)  $ satisfies
$\left[  x,u\right]  \in U\left(  \mathfrak{h}\right)  $.

Now forget that we fixed $x$. We thus have shown that every $x\in\mathfrak{g}$
and every $u\in U\left(  \mathfrak{h}\right)  $ satisfy $\left[  x,u\right]
\in U\left(  \mathfrak{h}\right)  $. Thus, $\left[  \mathfrak{g},U\left(
\mathfrak{h}\right)  \right]  \subseteq U\left(  \mathfrak{h}\right)  $ (since
$U\left(  \mathfrak{h}\right)  $ is a vector space). This proves Corollary
\ref{cor.derivation.Lie.semidir}.

\subsubsection{Universality of the free Lie algebra with respect to
derivations}

Both Theorem \ref{thm.universal.tensor.der} and Proposition
\ref{prop.derivation.unique} have analogues pertaining to Lie algebras in lieu
of (associative) algebras.\footnote{Notice that the Lie-algebraic analogue of
a derivation from an algebra $A$ into an $A$-bimodule is a $1$-cocycle from a
Lie algebra $\mathfrak{g}$ into a $\mathfrak{g}$-module.} We are going to
formulate both of these analogues, but we start with that of Proposition
\ref{prop.derivation.unique}, since it is the one we will find utile in our
study of Kac-Moody Lie algebras:

\begin{proposition}
\label{prop.derivation.Lie.unique}Let $\mathfrak{g}$ be a Lie algebra. Let $M$
be a $\mathfrak{g}$-module, and $d:\mathfrak{g}\rightarrow M$ and
$e:\mathfrak{g}\rightarrow M$ two $1$-cocycles. Let $S$ be a subset of
$\mathfrak{g}$ which generates $\mathfrak{g}$ as a Lie algebra. Assume that
$d\mid_{S}=e\mid_{S}$. Then, $d=e$.
\end{proposition}

\begin{vershort}
The proof of Proposition \ref{prop.derivation.Lie.unique} is analogous to that
of Proposition \ref{prop.derivation.unique}.
\end{vershort}

\begin{verlong}
The proof of Proposition \ref{prop.derivation.Lie.unique} is analogous to that
of Proposition \ref{prop.derivation.unique}. Here are its details:

\textit{Proof of Proposition \ref{prop.derivation.Lie.unique}.} Let $U$ be the
subset $\operatorname*{Ker}\left(  d-e\right)  $ of $\mathfrak{g}$. Clearly,
$U$ is a vector space (since $d-e$ is a linear map (since $d$ and $e$ are linear)).

Let $b\in U$ and $c\in U$. Since $b\in U=\operatorname*{Ker}\left(
d-e\right)  $, we have $\left(  d-e\right)  \left(  b\right)  =0$. Thus,
$d\left(  b\right)  -e\left(  b\right)  =\left(  d-e\right)  \left(  b\right)
=0$, so that $d\left(  b\right)  =e\left(  b\right)  $. Similarly, $d\left(
c\right)  =e\left(  c\right)  $.

Now, since $d$ is a $1$-cocycle, we have $d\left(  \left[  b,c\right]
\right)  =\left[  d\left(  b\right)  ,c\right]  +\left[  b,d\left(  c\right)
\right]  $ (by the definition of $1$-cocycles). Similarly, $e\left(  \left[
b,c\right]  \right)  =\left[  e\left(  b\right)  ,c\right]  +\left[
b,e\left(  c\right)  \right]  $. Hence,%
\begin{align*}
\left(  d-e\right)  \left(  \left[  b,c\right]  \right)   &
=\underbrace{d\left(  \left[  b,c\right]  \right)  }_{=\left[  d\left(
b\right)  ,c\right]  +\left[  b,d\left(  c\right)  \right]  }%
-\underbrace{e\left(  \left[  b,c\right]  \right)  }_{=\left[  e\left(
b\right)  ,c\right]  +\left[  b,e\left(  c\right)  \right]  }=\left(  \left[
\underbrace{d\left(  b\right)  }_{=e\left(  b\right)  },c\right]  +\left[
b,\underbrace{d\left(  c\right)  }_{=e\left(  c\right)  }\right]  \right)
-\left(  \left[  e\left(  b\right)  ,c\right]  +\left[  b,e\left(  c\right)
\right]  \right) \\
&  =\left(  \left[  e\left(  b\right)  ,c\right]  +\left[  b,e\left(
c\right)  \right]  \right)  -\left(  \left[  e\left(  b\right)  ,c\right]
+\left[  b,e\left(  c\right)  \right]  \right)  =0.
\end{align*}
In other words, $\left[  b,c\right]  \in\operatorname*{Ker}\left(  d-e\right)
=U$.

Now forget that we fixed $b$ and $c$. We have thus showed that any $b\in U$
and $c\in U$ satisfy $\left[  b,c\right]  \in U$. Combined with the fact that
$U$ is a vector space, this yields that $U$ is a Lie subalgebra of
$\mathfrak{g}$. Since $S\subseteq U$ (because every $s\in S$ satisfies%
\[
\left(  d-e\right)  \left(  s\right)  =\underbrace{d\left(  s\right)
}_{=\left(  d\mid_{S}\right)  \left(  s\right)  }-\underbrace{e\left(
s\right)  }_{=\left(  e\mid_{S}\right)  \left(  s\right)  }%
=\underbrace{\left(  d\mid_{S}\right)  }_{=e\mid_{S}}\left(  s\right)
-\left(  e\mid_{S}\right)  \left(  s\right)  =\left(  e\mid_{S}\right)
\left(  s\right)  -\left(  e\mid_{S}\right)  \left(  s\right)  =0
\]
and thus $s\in\operatorname*{Ker}\left(  d-e\right)  =U$), this yields that
$U$ is a Lie subalgebra of $\mathfrak{g}$ containing $S$ as a subset. But
since the smallest Lie subalgebra of $\mathfrak{g}$ containing $S$ as a subset
is $\mathfrak{g}$ itself (because $S$ generates $\mathfrak{g}$ as a Lie
algebra), this yields that $U\supseteq\mathfrak{g}$. Hence, $\mathfrak{g}%
\subseteq U=\operatorname*{Ker}\left(  d-e\right)  $, so that $d-e=0$ and thus
$d=e$. Proposition \ref{prop.derivation.Lie.unique} is proven.
\end{verlong}

\begin{verlong}
An analogue of Theorem \ref{thm.universal.tensor.der.gr} for Lie algebras can
also be given, and is left to the reader.
\end{verlong}

We record a corollary of Proposition \ref{prop.derivation.Lie.unique}:

\begin{corollary}
\label{cor.derivation.Lie.unique.ihg}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a Lie subalgebra of $\mathfrak{g}$. Let $\mathfrak{i}$ be a
Lie subalgebra of $\mathfrak{h}$. Let $d:\mathfrak{g}\rightarrow\mathfrak{g}$
be a derivation of the Lie algebra $\mathfrak{g}$. Let $S$ be a subset of
$\mathfrak{i}$ which generates $\mathfrak{i}$ as a Lie algebra. Assume that
$d\left(  S\right)  \subseteq\mathfrak{h}$. Then, $d\left(  \mathfrak{i}%
\right)  \subseteq\mathfrak{h}$.
\end{corollary}

\begin{vershort}
This corollary is analogous to Corollary \ref{cor.derivation.unique.ihg}, and
proven accordingly.
\end{vershort}

\begin{verlong}
This corollary is analogous to Corollary \ref{cor.derivation.unique.ihg}, and
proven accordingly:

\textit{Proof of Corollary \ref{cor.derivation.Lie.unique.ihg}.} We regard
$\mathfrak{g}$ as a $\mathfrak{g}$-module by means of the adjoint action.
Since $\mathfrak{i}\subseteq\mathfrak{h}\subseteq\mathfrak{g}$, the
$\mathfrak{g}$-module $\mathfrak{g}$ thus becomes an $\mathfrak{i}$-module.

We also regard $\mathfrak{h}$ as an $\mathfrak{h}$-module by means of the
adjoint action. Since $\mathfrak{i}\subseteq\mathfrak{h}$, the $\mathfrak{h}%
$-module $\mathfrak{h}$ thus becomes an $\mathfrak{i}$-module.

Let $\pi:\mathfrak{g}\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ be the
canonical projection. Clearly, $\pi$ is an $\mathfrak{i}$-module homomorphism,
and satisfies $\operatorname*{Ker}\pi=\mathfrak{h}$. Let $d^{\prime
}:\mathfrak{i}\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ be the restriction
of the map $\pi\circ d:\mathfrak{g}\rightarrow\mathfrak{g}\diagup\mathfrak{h}$
to $\mathfrak{i}$. It is easy to see that $d^{\prime}:\mathfrak{i}%
\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ is a $1$%
-cocycle\footnote{\textit{Proof.} Every $x\in\mathfrak{i}$ and $y\in
\mathfrak{i}$ satisfy%
\begin{align*}
d^{\prime}\left(  \left[  x,y\right]  \right)   &  =\left(  \pi\circ d\right)
\left(  \left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}d^{\prime}\text{ is the restriction of }\pi\circ d\text{ to }\mathfrak{i}%
\right) \\
&  =\pi\left(  \underbrace{d\left(  \left[  x,y\right]  \right)
}_{\substack{=\left[  d\left(  x\right)  ,y\right]  +\left[  x,d\left(
y\right)  \right]  \\\text{(since }d\text{ is a derivation)}}}\right)
=\pi\left(  \underbrace{\left[  d\left(  x\right)  ,y\right]  }_{=-\left[
y,d\left(  x\right)  \right]  }+\left[  x,d\left(  y\right)  \right]  \right)
\\
&  =\pi\left(  -\underbrace{\left[  y,d\left(  x\right)  \right]
}_{\substack{=y\rightharpoonup\left(  d\left(  x\right)  \right)
\\\text{(since }\mathfrak{g}\text{ is a }\mathfrak{g}\text{-module}\\\text{by
the adjoint action)}}}+\underbrace{\left[  x,d\left(  y\right)  \right]
}_{\substack{=x\rightharpoonup\left(  d\left(  y\right)  \right)
\\\text{(since }\mathfrak{g}\text{ is a }\mathfrak{g}\text{-module}\\\text{by
the adjoint action)}}}\right) \\
&  =\pi\left(  -y\rightharpoonup\left(  d\left(  x\right)  \right)
+x\rightharpoonup\left(  d\left(  y\right)  \right)  \right) \\
&  =-\underbrace{\pi\left(  y\rightharpoonup\left(  d\left(  x\right)
\right)  \right)  }_{\substack{=y\rightharpoonup\left(  \pi\left(  d\left(
x\right)  \right)  \right)  \\\text{(since }\pi\text{ is an }\mathfrak{i}%
\text{-module homomorphism)}}}+\underbrace{\pi\left(  x\rightharpoonup\left(
d\left(  y\right)  \right)  \right)  }_{\substack{=x\rightharpoonup\left(
\pi\left(  d\left(  y\right)  \right)  \right)  \\\text{(since }\pi\text{ is
an }\mathfrak{i}\text{-module homomorphism)}}}\\
&  =-y\rightharpoonup\left(  \underbrace{\pi\left(  d\left(  x\right)
\right)  }_{\substack{=\left(  \pi\circ d\right)  \left(  x\right)
=d^{\prime}\left(  x\right)  \\\text{(since }d^{\prime}\text{ is the
restriction of}\\\pi\circ d\text{ to }\mathfrak{i}\text{, and since }%
x\in\mathfrak{i}\text{)}}}\right)  +x\rightharpoonup\left(  \underbrace{\pi
\left(  d\left(  y\right)  \right)  }_{\substack{=\left(  \pi\circ d\right)
\left(  y\right)  =d^{\prime}\left(  y\right)  \\\text{(since }d^{\prime
}\text{ is the restriction of}\\\pi\circ d\text{ to }\mathfrak{i}\text{, and
since }y\in\mathfrak{i}\text{)}}}\right) \\
&  =-y\rightharpoonup\left(  d^{\prime}\left(  x\right)  \right)
+x\rightharpoonup\left(  d^{\prime}\left(  y\right)  \right)
=x\rightharpoonup\left(  d^{\prime}\left(  y\right)  \right)
-y\rightharpoonup\left(  d^{\prime}\left(  x\right)  \right)  .
\end{align*}
Thus, $d^{\prime}$ is a $1$-cocycle, qed.}. On the other hand, $0:\mathfrak{i}%
\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ is a $1$-cocycle as well. Every
$s\in S$ satisfies%
\begin{align*}
\left(  d^{\prime}\mid_{S}\right)  \left(  s\right)   &  =d^{\prime}\left(
s\right)  =\left(  \pi\circ d\right)  \left(  s\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }d^{\prime}\text{ is the restriction
of }\pi\circ d\text{ to }\mathfrak{i}\right) \\
&  =\pi\left(  d\left(  s\right)  \right)  =0\ \ \ \ \ \ \ \ \ \ \left(
\text{since }d\left(  \underbrace{s}_{\in S}\right)  \in d\left(  S\right)
\subseteq\mathfrak{h}=\operatorname*{Ker}\pi\right) \\
&  =0\left(  s\right)  =\left(  0\mid_{S}\right)  \left(  s\right)  .
\end{align*}
Thus, $d^{\prime}\mid_{S}=0\mid_{S}$. Proposition
\ref{prop.derivation.Lie.unique} (applied to $\mathfrak{i}$, $\mathfrak{g}%
\diagup\mathfrak{h}$, $d^{\prime}$ and $0$ instead of $\mathfrak{g}$, $M$, $d$
and $e$) therefore yields that $d^{\prime}=0$ on $\mathfrak{i}$. But since
$d^{\prime}$ is the restriction of $\pi\circ d$ to $\mathfrak{i}$, we have
$d^{\prime}=\left(  \pi\circ d\right)  \mid_{\mathfrak{i}}$. Thus, $\left(
\pi\circ d\right)  \mid_{\mathfrak{i}}=d^{\prime}=0$, so that $\left(
\pi\circ d\right)  \left(  \mathfrak{i}\right)  =0$. Thus, $\pi\left(
d\left(  \mathfrak{i}\right)  \right)  =\left(  \pi\circ d\right)  \left(
\mathfrak{i}\right)  =0$, so that $d\left(  \mathfrak{i}\right)
\subseteq\operatorname*{Ker}\pi=\mathfrak{h}$. Corollary
\ref{cor.derivation.Lie.unique.ihg} is therefore proven.
\end{verlong}

Let us now state the analogue of Proposition \ref{prop.derivation.unique} in
the Lie-algebraic setting:

\begin{theorem}
\label{thm.universal.FreeLie.der}Let $V$ be a vector space. We denote by
$\iota_{V}^{\operatorname*{FreeLie}}:V\rightarrow\operatorname*{FreeLie}V$ the
canonical map from $V$ into $\operatorname*{FreeLie}V$. (This map $\iota
_{V}^{\operatorname*{FreeLie}}$ is easily seen to be injective.) For any
$\operatorname*{FreeLie}V$-module $M$ and any linear map $f:V\rightarrow M$,
there exists a unique $1$-cocycle $F:\operatorname*{FreeLie}V\rightarrow M$
satisfying $f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$.
\end{theorem}

\begin{vershort}
Although we will not use this theorem anywhere in the following, let us
briefly discuss how it is proven. Theorem \ref{thm.universal.FreeLie.der}
cannot be proven as directly as we proved Theorem
\ref{thm.universal.tensor.der}. Instead, a way to prove Theorem
\ref{thm.universal.FreeLie.der} is by using the following lemma:
\end{vershort}

\begin{verlong}
Although we will not use this theorem anywhere in the following, let us
discuss how it is proven. Theorem \ref{thm.universal.FreeLie.der} cannot be
proven as directly as we proved Theorem \ref{thm.universal.tensor.der}.
Instead, a way to prove Theorem \ref{thm.universal.FreeLie.der} is by using
the following lemma:
\end{verlong}

\begin{lemma}
\label{lem.semidir.coc-to-deriv}Let $\mathfrak{g}$ be a Lie algebra. Let $M$
be a $\mathfrak{g}$-module. Define the semidirect product $\mathfrak{g}\ltimes
M$ as in Definition \ref{def.semidir}. Let $\varphi:\mathfrak{g}\rightarrow M$
be a linear map. Then, $\varphi:\mathfrak{g}\rightarrow M$ is a $1$-cocycle if
and only if the map%
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)
\]
is a Lie algebra homomorphism.
\end{lemma}

\begin{vershort}
This lemma helps reducing Theorem \ref{thm.universal.FreeLie.der} to Theorem
\ref{thm.universal.FreeLie}. We leave the details of this proof (both of the
lemma and of Theorem \ref{thm.universal.FreeLie.der}) to the reader.
\end{vershort}

\begin{verlong}
We will use this lemma to reduce Theorem \ref{thm.universal.FreeLie.der} to
Theorem \ref{thm.universal.FreeLie}; let us, however, first establish the
lemma itself:

\textit{Proof of Lemma \ref{lem.semidir.coc-to-deriv}.} Let $\Phi$ denote the
map
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)  .
\]
Clearly, the map $\Phi$ is linear.

We will now prove the following two assertions:

\textit{Assertion }$\mathfrak{K}_{1}$\textit{:} If $\varphi:\mathfrak{g}%
\rightarrow M$ is a $1$-cocycle, then $\Phi$ is a Lie algebra homomorphism.

\textit{Assertion }$\mathfrak{K}_{2}$\textit{:} If $\Phi$ is a Lie algebra
homomorphism, then $\varphi:\mathfrak{g}\rightarrow M$ is a $1$-cocycle.

\textit{Proof of Assertion }$\mathfrak{K}_{1}$\textit{:} Assume that
$\varphi:\mathfrak{g}\rightarrow M$ is a $1$-cocycle. By the definition of
``$1$-cocycle'', this means that
\[
\varphi\left(  \left[  a,b\right]  \right)  =a\rightharpoonup\varphi\left(
b\right)  -b\rightharpoonup\varphi\left(  a\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{ and }b\in
\mathfrak{g}.
\]


Let $a\in\mathfrak{g}$ and $b\in\mathfrak{g}$. By the definition of $\Phi$, we
have $\Phi\left(  a\right)  =\left(  a,\varphi\left(  a\right)  \right)  $ and
$\Phi\left(  b\right)  =\left(  b,\varphi\left(  b\right)  \right)  $. Thus,%
\[
\left[  \underbrace{\Phi\left(  a\right)  }_{=\left(  a,\varphi\left(
a\right)  \right)  },\underbrace{\Phi\left(  b\right)  }_{=\left(
b,\varphi\left(  b\right)  \right)  }\right]  =\left[  \left(  a,\varphi
\left(  a\right)  \right)  ,\left(  b,\varphi\left(  b\right)  \right)
\right]  =\left(  \left[  a,b\right]  ,a\rightharpoonup\varphi\left(
b\right)  -b\rightharpoonup\varphi\left(  a\right)  \right)
\]
(by the definition of the semidirect product $\mathfrak{g}\ltimes M$). On the
other hand, the definition of $\Phi$ yields
\[
\Phi\left(  \left[  a,b\right]  \right)  =\left(  \left[  a,b\right]
,\underbrace{\varphi\left(  \left[  a,b\right]  \right)  }_{=a\rightharpoonup
\varphi\left(  b\right)  -b\rightharpoonup\varphi\left(  a\right)  }\right)
=\left(  \left[  a,b\right]  ,a\rightharpoonup\varphi\left(  b\right)
-b\rightharpoonup\varphi\left(  a\right)  \right)  =\left[  \Phi\left(
a\right)  ,\Phi\left(  b\right)  \right]  .
\]


Now forget that we fixed $a$ and $b$. We thus have shown that every
$a\in\mathfrak{g}$ and $b\in\mathfrak{g}$ satisfy $\Phi\left(  \left[
a,b\right]  \right)  =\left[  \Phi\left(  a\right)  ,\Phi\left(  b\right)
\right]  $. Since $\Phi$ is linear, this yields that $\Phi$ is a Lie algebra
homomorphism. This proves Assertion $\mathfrak{K}_{1}$.

\textit{Proof of Assertion }$\mathfrak{K}_{2}$\textit{:} Assume that $\Phi$ is
a Lie algebra homomorphism. Thus,%
\[
\Phi\left(  \left[  a,b\right]  \right)  =\left[  \Phi\left(  a\right)
,\Phi\left(  b\right)  \right]  \ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}.
\]


Now let $a\in\mathfrak{g}$ and $b\in\mathfrak{g}$. By the definition of $\Phi
$, we have the three equalities $\Phi\left(  a\right)  =\left(  a,\varphi
\left(  a\right)  \right)  $, $\Phi\left(  b\right)  =\left(  b,\varphi\left(
b\right)  \right)  $ and $\Phi\left(  \left[  a,b\right]  \right)  =\left(
\left[  a,b\right]  ,\varphi\left(  \left[  a,b\right]  \right)  \right)  $.
Thus,%
\begin{align*}
\left(  \left[  a,b\right]  ,\varphi\left(  \left[  a,b\right]  \right)
\right)   &  =\Phi\left(  \left[  a,b\right]  \right)  =\left[
\underbrace{\Phi\left(  a\right)  }_{=\left(  a,\varphi\left(  a\right)
\right)  },\underbrace{\Phi\left(  b\right)  }_{=\left(  b,\varphi\left(
b\right)  \right)  }\right] \\
&  =\left[  \left(  a,\varphi\left(  a\right)  \right)  ,\left(
b,\varphi\left(  b\right)  \right)  \right]  =\left(  \left[  a,b\right]
,a\rightharpoonup\varphi\left(  b\right)  -b\rightharpoonup\varphi\left(
a\right)  \right)
\end{align*}
(by the definition of the semidirect product $\mathfrak{g}\ltimes M$). Hence,
$\varphi\left(  \left[  a,b\right]  \right)  =a\rightharpoonup\varphi\left(
b\right)  -b\rightharpoonup\varphi\left(  a\right)  $.

Now forget that we fixed $a$ and $b$. We thus have shown that%
\[
\varphi\left(  \left[  a,b\right]  \right)  =a\rightharpoonup\varphi\left(
b\right)  -b\rightharpoonup\varphi\left(  a\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{ and }b\in
\mathfrak{g}.
\]
By the definition of ``$1$-cocycle'', this means exactly that $\varphi
:\mathfrak{g}\rightarrow M$ is a $1$-cocycle (since we already know that
$\varphi$ is linear). We have therefore shown that $\varphi:\mathfrak{g}%
\rightarrow M$ is a $1$-cocycle. This proves Assertion $\mathfrak{K}_{2}$.

Combining Assertion $\mathfrak{K}_{1}$ with Assertion $\mathfrak{K}_{2}$, we
conclude that $\varphi:\mathfrak{g}\rightarrow M$ is a $1$-cocycle if and only
if $\Phi$ is a Lie algebra homomorphism. Since $\Phi$ is the map%
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)  ,
\]
this rewrites as follows: $\varphi:\mathfrak{g}\rightarrow M$ is a $1$-cocycle
if and only if the map%
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)
\]
is a Lie algebra homomorphism. This proves Lemma
\ref{lem.semidir.coc-to-deriv}.

\textit{Proof of Theorem \ref{thm.universal.FreeLie.der}.} Fix a
$\operatorname*{FreeLie}V$-module $M$ and any linear map $f:V\rightarrow M$.

Let $\mathfrak{g}=\operatorname*{FreeLie}V$. Let $\pi_{1}:\mathfrak{g}\ltimes
M\rightarrow\mathfrak{g}$ be the canonical projection on the first addend.
Then, $\pi_{1}$ is known to be a Lie algebra homomorphism. Let $\pi
_{2}:\mathfrak{g}\ltimes M\rightarrow M$ be the canonical projection on the
second addend.

Regard the canonical injection $\iota_{V}^{\operatorname*{FreeLie}%
}:V\rightarrow\operatorname*{FreeLie}V=\mathfrak{g}$ as an inclusion. Let
$\psi$ be the map%
\[
V\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto\left(
x,f\left(  x\right)  \right)  .
\]
This map $\psi$ is clearly linear. Thus, Theorem \ref{thm.universal.FreeLie}
(applied to $\mathfrak{g}\ltimes M$ and $\psi$ instead of $\mathfrak{h}$ and
$f$) yields that there exists a unique Lie algebra homomorphism
$F:\operatorname*{FreeLie}V\rightarrow\mathfrak{g}\ltimes M$ satisfying
$\psi=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. Denote this $F$ by $\Psi$.
Thus, $\Psi:\operatorname*{FreeLie}V\rightarrow\mathfrak{g}\ltimes M$ is a Lie
algebra homomorphism satisfying $\psi=\Psi\circ\iota_{V}%
^{\operatorname*{FreeLie}}$.

Since $\pi_{1}$ and $\Psi$ are Lie algebra homomorphisms, their composition
$\pi_{1}\circ\Psi:\operatorname*{FreeLie}V\rightarrow\mathfrak{g}$ must also
be a Lie algebra homomorphism. Note that $\operatorname*{id}%
\nolimits_{\mathfrak{g}}$ also is a Lie algebra homomorphism from
$\operatorname*{FreeLie}V$ to $\mathfrak{g}$ (since $\mathfrak{g}%
=\operatorname*{FreeLie}V$).

Every $x\in V$ satisfies $\iota_{V}^{\operatorname*{FreeLie}}\left(  x\right)
=x$ (since we regard the map $\iota_{V}^{\operatorname*{FreeLie}}$ as an
inclusion). But every $x\in V$ satisfies%
\[
\left(  \pi_{1}\circ\psi\right)  \left(  x\right)  =\pi_{1}\left(
\underbrace{\psi\left(  x\right)  }_{\substack{=\left(  x,f\left(  x\right)
\right)  \\\text{(by the definition of }\psi\text{)}}}\right)  =\pi_{1}\left(
x,f\left(  x\right)  \right)  =x
\]
(since $\pi_{1}:\mathfrak{g}\ltimes M\rightarrow\mathfrak{g}$ was defined as
the canonical projection on the first addend). Thus, every $x\in V$ satisfies
$\left(  \pi_{1}\circ\psi\right)  \left(  x\right)  =x=\iota_{V}%
^{\operatorname*{FreeLie}}\left(  x\right)  $. Hence, $\pi_{1}\circ\psi
=\iota_{V}^{\operatorname*{FreeLie}}$.

Applying Theorem \ref{thm.universal.FreeLie} to $\mathfrak{g}$ and $\pi
_{1}\circ\psi$ instead of $\mathfrak{h}$ and $f$, we conclude that there
exists a unique Lie algebra homomorphism $F:\operatorname*{FreeLie}%
V\rightarrow\mathfrak{g}$ satisfying $\pi_{1}\circ\psi=F\circ\iota
_{V}^{\operatorname*{FreeLie}}$. Hence, if $F_{1}$ and $F_{2}$ are two Lie
algebra homomorphisms $\operatorname*{FreeLie}V\rightarrow\mathfrak{g}$
satisfying $\pi_{1}\circ\psi=F_{1}\circ\iota_{V}^{\operatorname*{FreeLie}}$
and $\pi_{1}\circ\psi=F_{2}\circ\iota_{V}^{\operatorname*{FreeLie}}$, then we
must have $F_{1}=F_{2}$. Applying this to $F_{1}=\pi_{1}\circ\Psi$ and
$F_{2}=\operatorname*{id}\nolimits_{\mathfrak{g}}$ (since $\pi_{1}%
\circ\underbrace{\psi}_{=\Psi\circ\iota_{V}^{\operatorname*{FreeLie}}}=\pi
_{1}\circ\Psi\circ\iota_{V}^{\operatorname*{FreeLie}}$ and $\pi_{1}\circ
\psi=\iota_{V}^{\operatorname*{FreeLie}}=\operatorname*{id}%
\nolimits_{\mathfrak{g}}\circ\iota_{V}^{\operatorname*{FreeLie}}$), we obtain
$\pi_{1}\circ\Psi=\operatorname*{id}\nolimits_{\mathfrak{g}}$.

Now, let $\varphi$ denote the linear map $\pi_{2}\circ\Psi
:\operatorname*{FreeLie}V\rightarrow M$. Since $\operatorname*{FreeLie}%
V=\mathfrak{g}$, this map $\varphi$ is a linear map $\mathfrak{g}\rightarrow
M$. We shall now show that this map $\varphi$ is a $1$-cocycle satisfying
$f=\varphi\circ\iota_{V}^{\operatorname*{FreeLie}}$.

Indeed, every $x\in\mathfrak{g}$ satisfies%
\[
\Psi\left(  x\right)  =\left(  x,\varphi\left(  x\right)  \right)
\]
\footnote{\textit{Proof.} Let $x\in\mathfrak{g}$. Then, $x\in\mathfrak{g}%
=\operatorname*{FreeLie}V$, so that $\Psi\left(  x\right)  $ is an element of
$\mathfrak{g}\ltimes M$. Hence, we can write $\Psi\left(  x\right)  $ in the
form $\Psi\left(  x\right)  =\left(  y,z\right)  $ for some $y\in\mathfrak{g}$
and $z\in M$ (by the definition of $\mathfrak{g}\ltimes M$). Consider these
$y$ and $z$. Since $\Psi\left(  x\right)  =\left(  y,z\right)  $, we have
$\pi_{1}\left(  \Psi\left(  x\right)  \right)  =\pi_{1}\left(  y,z\right)  =y$
(since $\pi_{1}:\mathfrak{g}\ltimes M\rightarrow\mathfrak{g}$ is the canonical
projection on the first addend) and $\pi_{2}\left(  \Psi\left(  x\right)
\right)  =\pi_{2}\left(  y,z\right)  =z$ (since $\pi_{2}:\mathfrak{g}\ltimes
M\rightarrow M$ is the canonical projection on the second addend). But now,
$y=\pi_{1}\left(  \Psi\left(  x\right)  \right)  =\underbrace{\left(  \pi
_{1}\circ\Psi\right)  }_{=\operatorname*{id}\nolimits_{\mathfrak{g}}}\left(
x\right)  =x$ and $z=\pi_{2}\left(  \Psi\left(  x\right)  \right)
=\underbrace{\left(  \pi_{2}\circ\Psi\right)  }_{=\varphi}\left(  x\right)  $,
so we have $\Psi\left(  x\right)  =\left(  \underbrace{y}_{=x},\underbrace{z}%
_{=\varphi\left(  x\right)  }\right)  =\left(  x,\varphi\left(  x\right)
\right)  $, qed.}. In other words, $\Psi$ is the map
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)  .
\]
Since we know that $\Psi$ is a Lie algebra homomorphism, we thus conclude that
the map
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)
\]
is a Lie algebra homomorphism. Hence, $\varphi:\mathfrak{g}\rightarrow M$ is a
$1$-cocycle (because Lemma \ref{lem.semidir.coc-to-deriv} yields that
$\varphi:\mathfrak{g}\rightarrow M$ is a $1$-cocycle if and only if the map%
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)
\]
is a Lie algebra homomorphism). In other words, $\varphi
:\operatorname*{FreeLie}V\rightarrow M$ is a $1$-cocycle (since $\mathfrak{g}%
=\operatorname*{FreeLie}V$).

Every $x\in V$ satisfies%
\begin{align*}
\left(  \underbrace{\varphi}_{=\pi_{2}\circ\Psi}\circ\iota_{V}%
^{\operatorname*{FreeLie}}\right)  \left(  x\right)   &  =\left(  \pi_{2}%
\circ\underbrace{\Psi\circ\iota_{V}^{\operatorname*{FreeLie}}}_{=\psi}\right)
\left(  x\right)  =\left(  \pi_{2}\circ\psi\right)  \left(  x\right)  =\pi
_{2}\left(  \underbrace{\psi\left(  x\right)  }_{\substack{=\left(  x,f\left(
x\right)  \right)  \\\text{(by the definition of }\psi\text{)}}}\right) \\
&  =\pi_{2}\left(  x,f\left(  x\right)  \right)  =f\left(  x\right)
\end{align*}
(since $\pi_{2}:\mathfrak{g}\ltimes M\rightarrow M$ was defined as the
canonical projection on the second addend). Thus, $\varphi\circ\iota
_{V}^{\operatorname*{FreeLie}}=f$.

Altogether, we know that $\varphi:\operatorname*{FreeLie}V\rightarrow M$ is a
$1$-cocycle satisfying $f=\varphi\circ\iota_{V}^{\operatorname*{FreeLie}}$. We
thus conclude that there exists a $1$-cocycle $F:\operatorname*{FreeLie}%
V\rightarrow M$ satisfying $f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$
(namely, $F=\varphi$).

Now, let us prove the uniqueness of such an $F$.

Let $F$ be a $1$-cocycle $\operatorname*{FreeLie}V\rightarrow M$ satisfying
$f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. We will prove that $F=\varphi$.

Recall that the free Lie algebra $\operatorname*{FreeLie}V$ is generated by
its subset $V$ as a Lie algebra. Every $x\in V$ satisfies%
\[
\underbrace{f}_{=F\circ\iota_{V}^{\operatorname*{FreeLie}}}\left(  x\right)
=\left(  F\circ\iota_{V}^{\operatorname*{FreeLie}}\right)  \left(  x\right)
=F\left(  \underbrace{\iota_{V}^{\operatorname*{FreeLie}}\left(  x\right)
}_{=x}\right)  =F\left(  x\right)  =\left(  F\mid_{V}\right)  \left(
x\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }x\in V\right)
\]
and%
\[
\underbrace{f}_{=\varphi\circ\iota_{V}^{\operatorname*{FreeLie}}}\left(
x\right)  =\left(  \varphi\circ\iota_{V}^{\operatorname*{FreeLie}}\right)
\left(  x\right)  =\varphi\left(  \underbrace{\iota_{V}%
^{\operatorname*{FreeLie}}\left(  x\right)  }_{=x}\right)  =\varphi\left(
x\right)  =\left(  \varphi\mid_{V}\right)  \left(  x\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }x\in V\right)  .
\]
Thus, every $x\in V$ satisfies $\left(  F\mid_{V}\right)  \left(  x\right)
=f\left(  x\right)  =\left(  \varphi\mid_{V}\right)  \left(  x\right)  $.
Hence, $F\mid_{V}=\varphi\mid_{V}$. Thus, Proposition
\ref{prop.derivation.Lie.unique} (applied to $\operatorname*{FreeLie}V$, $F$,
$\varphi$ and $V$ instead of $\mathfrak{g}$, $d$, $e$ and $S$) yields that
$F=\varphi$.

Now forget that we fixed $F$. We thus have proven that every $1$-cocycle
$F:\operatorname*{FreeLie}V\rightarrow M$ satisfying $f=F\circ\iota
_{V}^{\operatorname*{FreeLie}}$ must satisfy $F=\varphi$. Hence, there exists
at most one $1$-cocycle $F:\operatorname*{FreeLie}V\rightarrow M$ satisfying
$f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. Combined with the fact that
there exists a $1$-cocycle $F:\operatorname*{FreeLie}V\rightarrow M$
satisfying $f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$ (this fact was proven
above), this yields that there exists a unique $1$-cocycle
$F:\operatorname*{FreeLie}V\rightarrow M$ satisfying $f=F\circ\iota
_{V}^{\operatorname*{FreeLie}}$. This proves Theorem
\ref{thm.universal.FreeLie.der}.
\end{verlong}

An alternative way to prove Theorem \ref{thm.universal.FreeLie.der} is the
following: Apply Theorem \ref{thm.universal.tensor.der} to construct a
derivation $F:T\left(  V\right)  \rightarrow M$ (of algebras) satisfying
$f=F\circ\iota_{V}^{T}$, and then identify $\operatorname*{FreeLie}V$ with a
Lie subalgebra of $T\left(  V\right)  $ (because Proposition \ref{prop.Ufree}
$U\left(  \operatorname*{FreeLie}V\right)  \cong T\left(  V\right)  $, and
because the Poincar\'{e}-Birkhoff-Witt theorem entails an injection
$\operatorname*{FreeLie}V\rightarrow U\left(  \operatorname*{FreeLie}V\right)
$). Then, restricting the derivation $F:T\left(  V\right)  \rightarrow M$ to
$\operatorname*{FreeLie}V$, we obtain a $1$-cocycle $\operatorname*{FreeLie}%
V\rightarrow M$ with the required properties. The uniqueness part of Theorem
\ref{thm.universal.FreeLie.der} is easy (and follows from Proposition
\ref{prop.derivation.Lie.unique} below). This proof of Theorem
\ref{thm.universal.FreeLie.der} has the disadvantage that it makes use of the
Poincar\'{e}-Birkhoff-Witt theorem, which does not generalize to the case of
Lie algebras over rings (whereas Theorem \ref{thm.universal.FreeLie.der} does
generalize to this case).

\subsubsection{Derivations from grading}

The following simple lemma will help us defining derivations on Lie algebras:

\begin{lemma}
\label{lem.deriv.grading}Let $Q$ be an abelian group. Let $s\in
\operatorname*{Hom}\left(  Q,\mathbb{C}\right)  $ be a group homomorphism. Let
$\mathfrak{n}$ be a $Q$-graded Lie algebra. Let $\eta:\mathfrak{n}%
\rightarrow\mathfrak{n}$ be a linear map satisfying%
\begin{equation}
\eta\left(  x\right)  =s\left(  w\right)  \cdot x\ \ \ \ \ \ \ \ \ \ \text{for
every }w\in Q\text{ and every }x\in\mathfrak{n}\left[  w\right]  .
\label{lem.deriv.grading.1}%
\end{equation}
Then, $\eta$ is a derivation (of Lie algebras).
\end{lemma}

\textit{Proof of Lemma \ref{lem.deriv.grading}.} In order to prove that $\eta$
is a derivation, we need to check that
\begin{equation}
\eta\left(  \left[  a,b\right]  \right)  =\left[  \eta\left(  a\right)
,b\right]  +\left[  a,\eta\left(  b\right)  \right]
\ \ \ \ \ \ \ \ \ \ \text{for any }a\in\mathfrak{n}\text{ and }b\in
\mathfrak{n}. \label{pf.deriv.grading.1}%
\end{equation}
Let us prove the equation (\ref{pf.deriv.grading.1}). Since this equation is
linear in each of $a$ and $b$, we can WLOG assume that $a$ and $b$ are
homogeneous (because any element of $\mathfrak{n}$ is a sum of homogeneous
elements). So, assume this. We will write the binary operation of the group
$Q$ as addition. Since $a$ is homogeneous, we have $a\in\mathfrak{n}\left[
u\right]  $ for some $u\in Q$. Consider this $u$. Since $b$ is homogeneous, we
have $b\in\mathfrak{n}\left[  v\right]  $ for some $v\in Q$. Fix this $v$.
Thus, $\left[  a,b\right]  \in\mathfrak{n}\left[  u+v\right]  $ (since
$a\in\mathfrak{n}\left[  u\right]  $ and $b\in\mathfrak{n}\left[  v\right]  $
and since $\mathfrak{n}$ is $Q$-graded). Thus, (\ref{lem.deriv.grading.1})
(applied to $x=a+b$ and $w=u+v$) yields $\eta\left(  \left[  a,b\right]
\right)  =\underbrace{s\left(  u+v\right)  }_{\substack{=s\left(  u\right)
+s\left(  v\right)  \\\text{(since }s\text{ is a group}\\\text{homomorphism)}%
}}\cdot\left[  a,b\right]  =\left(  s\left(  u\right)  +s\left(  v\right)
\right)  \cdot\left[  a,b\right]  $. On the other hand,
(\ref{lem.deriv.grading.1}) (applied to $x=a$ and $w=u$) yields $\eta\left(
a\right)  =s\left(  u\right)  \cdot a$. Also, (\ref{lem.deriv.grading.1})
(applied to $x=b$ and $y=v$) yields $\eta\left(  b\right)  =s\left(  v\right)
\cdot b$. Now,%
\[
\left[  \underbrace{\eta\left(  a\right)  }_{=s\left(  u\right)  \cdot
a},b\right]  +\left[  a,\underbrace{\eta\left(  b\right)  }_{=s\left(
v\right)  \cdot b}\right]  =s\left(  u\right)  \cdot\left[  a,b\right]
+s\left(  v\right)  \cdot\left[  a,b\right]  =\left(  s\left(  u\right)
+s\left(  v\right)  \right)  \cdot\left[  a,b\right]  =\eta\left(  \left[
a,b\right]  \right)  .
\]
This proves (\ref{pf.deriv.grading.1}). Now that (\ref{pf.deriv.grading.1}) is
proven, we conclude that $\eta$ is a derivation. Lemma \ref{lem.deriv.grading}
is proven.

\subsubsection{The commutator of derivations}

The following proposition is the classical analogue of Proposition
\ref{prop.commutator.derivs} for algebras in lieu of Lie algebras:

\begin{proposition}
\label{prop.commutator.derivs.alg}Let $A$ be an algebra. Let $f:A\rightarrow
A$ and $g:A\rightarrow A$ be two derivations of $A$. Then, $\left[
f,g\right]  $ is a derivation of $A$. (Here, the Lie bracket is to be
understood as the Lie bracket on $\operatorname*{End}A$, so that we have
$\left[  f,g\right]  =f\circ g-g\circ f$.)
\end{proposition}

The proof of this is completely analogous to that of Proposition
\ref{prop.commutator.derivs}. Moreover, by the same argument, the following
slight generalization of Proposition \ref{prop.commutator.derivs.alg} can be shown:

\begin{proposition}
\label{prop.commutator.derivs.alg.2}Let $A$ be a subalgebra of an algebra $B$.
Let $f:A\rightarrow B$ and $g:B\rightarrow B$ be two derivations such that
$g\left(  A\right)  \subseteq A$. Then, $f\circ\left(  g\mid_{A}\right)
-g\circ f:A\rightarrow B$ is a derivation.
\end{proposition}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.commutator.derivs.alg.2}.} Let $a\in A$
and $b\in A$. Since $f$ is a derivation, we have $f\left(  ab\right)
=f\left(  a\right)  \cdot b+a\cdot f\left(  b\right)  $. Thus,%
\begin{align*}
\left(  g\circ f\right)  \left(  ab\right)   &  =g\left(  \underbrace{f\left(
ab\right)  }_{=f\left(  a\right)  \cdot b+a\cdot f\left(  b\right)  }\right)
=g\left(  f\left(  a\right)  \cdot b+a\cdot f\left(  b\right)  \right) \\
&  =\underbrace{g\left(  f\left(  a\right)  \cdot b\right)  }%
_{\substack{=g\left(  f\left(  a\right)  \right)  \cdot b+f\left(  a\right)
\cdot g\left(  b\right)  \\\text{(since }g\text{ is a derivation)}%
}}+\underbrace{g\left(  a\cdot f\left(  b\right)  \right)  }%
_{\substack{=g\left(  a\right)  \cdot f\left(  b\right)  +a\cdot g\left(
f\left(  b\right)  \right)  \\\text{(since }g\text{ is a derivation)}}}\\
&  =\underbrace{g\left(  f\left(  a\right)  \right)  }_{=\left(  g\circ
f\right)  \left(  a\right)  }\cdot b+f\left(  a\right)  \cdot g\left(
b\right)  +g\left(  a\right)  \cdot f\left(  b\right)  +a\cdot
\underbrace{g\left(  f\left(  b\right)  \right)  }_{=\left(  g\circ f\right)
\left(  b\right)  }\\
&  =\left(  g\circ f\right)  \left(  a\right)  \cdot b+f\left(  a\right)
\cdot g\left(  b\right)  +g\left(  a\right)  \cdot f\left(  b\right)
+a\cdot\left(  g\circ f\right)  \left(  b\right)  .
\end{align*}
Let us notice that $f\left(  g\left(  a\right)  \right)  $ and $f\left(
g\left(  b\right)  \right)  $ are well-defined (since $g\left(  \underbrace{a}%
_{\in A}\right)  \in g\left(  A\right)  \subseteq A$ and $g\left(
\underbrace{b}_{\in A}\right)  \in g\left(  A\right)  \subseteq A$). Since $g$
is a derivation, we have $g\left(  ab\right)  =g\left(  a\right)  \cdot
b+a\cdot g\left(  b\right)  $. Thus,%
\begin{align*}
\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  ab\right)   &
=f\left(  \underbrace{\left(  g\mid_{A}\right)  \left(  ab\right)
}_{=g\left(  ab\right)  =g\left(  a\right)  \cdot b+a\cdot g\left(  b\right)
}\right)  =f\left(  g\left(  a\right)  \cdot b+a\cdot g\left(  b\right)
\right) \\
&  =\underbrace{f\left(  g\left(  a\right)  \cdot b\right)  }%
_{\substack{=f\left(  g\left(  a\right)  \right)  \cdot b+g\left(  a\right)
\cdot f\left(  b\right)  \\\text{(since }g\text{ is a derivation)}%
}}+\underbrace{f\left(  a\cdot g\left(  b\right)  \right)  }%
_{\substack{=f\left(  a\right)  \cdot g\left(  b\right)  +a\cdot f\left(
g\left(  b\right)  \right)  \\\text{(since }g\text{ is a derivation)}}}\\
&  =f\left(  \underbrace{g\left(  a\right)  }_{=\left(  g\mid_{A}\right)
\left(  a\right)  }\right)  \cdot b+g\left(  a\right)  \cdot f\left(
b\right)  +f\left(  a\right)  \cdot g\left(  b\right)  +a\cdot f\left(
\underbrace{g\left(  b\right)  }_{=\left(  g\mid_{A}\right)  \left(  b\right)
}\right) \\
&  =\underbrace{f\left(  \left(  g\mid_{A}\right)  \left(  a\right)  \right)
}_{=\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  a\right)  }\cdot
b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(  a\right)  \cdot
g\left(  b\right)  +a\cdot\underbrace{f\left(  \left(  g\mid_{A}\right)
\left(  b\right)  \right)  }_{=\left(  f\circ\left(  g\mid_{A}\right)
\right)  \left(  b\right)  }\\
&  =\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  a\right)  \cdot
b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(  a\right)  \cdot
g\left(  b\right)  +a\cdot\left(  f\circ\left(  g\mid_{A}\right)  \right)
\left(  b\right)  .
\end{align*}


Thus,%
\begin{align*}
&  \left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(  ab\right)
\\
&  =\underbrace{\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
ab\right)  }_{=\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
a\right)  \cdot b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(
a\right)  \cdot g\left(  b\right)  +a\cdot\left(  f\circ\left(  g\mid
_{A}\right)  \right)  \left(  b\right)  }-\underbrace{\left(  g\circ f\right)
\left(  ab\right)  }_{=\left(  g\circ f\right)  \left(  a\right)  \cdot
b+f\left(  a\right)  \cdot g\left(  b\right)  +g\left(  a\right)  \cdot
f\left(  b\right)  +a\cdot\left(  g\circ f\right)  \left(  b\right)  }\\
&  =\left(  \left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  a\right)
\cdot b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(  a\right)  \cdot
g\left(  b\right)  +a\cdot\left(  f\circ\left(  g\mid_{A}\right)  \right)
\left(  b\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ -\left(  \left(  g\circ f\right)  \left(  a\right)
\cdot b+f\left(  a\right)  \cdot g\left(  b\right)  +g\left(  a\right)  \cdot
f\left(  b\right)  +a\cdot\left(  g\circ f\right)  \left(  b\right)  \right)
\\
&  =\underbrace{\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
a\right)  \cdot b-\left(  g\circ f\right)  \left(  a\right)  \cdot
b}_{=\left(  \left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
a\right)  -\left(  g\circ f\right)  \left(  a\right)  \right)  \cdot
b}+\underbrace{a\cdot\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
b\right)  -a\cdot\left(  g\circ f\right)  \left(  b\right)  }_{=a\cdot\left(
\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  b\right)  -\left(
g\circ f\right)  \left(  b\right)  \right)  }\\
&  =\underbrace{\left(  \left(  f\circ\left(  g\mid_{A}\right)  \right)
\left(  a\right)  -\left(  g\circ f\right)  \left(  a\right)  \right)
}_{=\left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(
a\right)  }\cdot b+a\cdot\underbrace{\left(  \left(  f\circ\left(  g\mid
_{A}\right)  \right)  \left(  b\right)  -\left(  g\circ f\right)  \left(
b\right)  \right)  }_{=\left(  f\circ\left(  g\mid_{A}\right)  -g\circ
f\right)  \left(  b\right)  }\\
&  =\left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(
a\right)  \cdot b+a\cdot\left(  f\circ\left(  g\mid_{A}\right)  -g\circ
f\right)  \left(  b\right)  .
\end{align*}
We have thus proven that any $a\in A$ and $b\in A$ satisfy $\left(
f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(  ab\right)  =\left(
f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(  a\right)  \cdot
b+a\cdot\left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(
b\right)  $. In other words, $f\circ\left(  g\mid_{A}\right)  -g\circ f$ is a
derivation. This proves Proposition \ref{prop.commutator.derivs.alg.2}.

\textit{Proof of Proposition \ref{prop.commutator.derivs.alg}.} Applying
Proposition \ref{prop.commutator.derivs.alg.2} to $B=A$, we obtain that
$f\circ\left(  g\mid_{A}\right)  -g\circ f:A\rightarrow A$ is a derivation
(since $g\left(  A\right)  \subseteq A$). In other words, $\left[  f,g\right]
$ is a derivation (since $f\circ\underbrace{\left(  g\mid_{A}\right)  }%
_{=g}-g\circ f=f\circ g-g\circ f=\left[  f,g\right]  $). Hence, Proposition
\ref{prop.commutator.derivs.alg} is proven.
\end{verlong}

\protect\begin{noncompile}
\subsubsection{Extending degree-zero invariant forms on graded Lie algebras}

In this subsection we will collect some results on how to construct a
degree-$0$ invariant bilinear form on a graded Lie algebra if we are given its
values on the first few homogeneous components and we know that these
homogeneous components generate the whole Lie algebra. Our main results will
be the following two theorems:

\begin{theorem}
[...][ext]
\end{theorem}

\begin{theorem}
[...][symmetry gives symmetry]
\end{theorem}

These two theorems will be later applied to Kac-Moody algebras, on which it
will allow the construction of an invariant form. Nevertheless I (Darij) am
going to prove them in full generality (at least, the greatest generality
known to me) and full detail, seeing that most references known to me offer
neither. However, I don't know of any application of these theorems beyond the
case of Kac-Moody algebras.

We start with the easiest part: the uniqueness of the extended form. We state
it in probably the most general form:

\begin{proposition}
\label{prop.bilext.uni1}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie
algebra over a field $k$. Let $M$ and $N$ be two $\mathbb{Z}$-graded
$\mathfrak{g}$-modules. Let $K\in\mathbb{N}$ be a nonnegative integer. Assume
that every integer $n>K$ satisfies%
\begin{equation}
N\left[  -n\right]  =\sum\limits_{i=1}^{n-1}\mathfrak{g}\left[  -i\right]
\rightharpoonup N\left[  -n+i\right]  . \label{prop.bilext.uni1.gen1}%
\end{equation}
\footnotemark\ For every $n\in\left\{  1,2,...,K\right\}  $, let $\beta_{n}$
be a $k$-bilinear form $M\left[  n\right]  \times N\left[  -n\right]
\rightarrow k$. Then, there exists \textbf{at most} one sequence $\left(
\gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps satisfying the
following three properties \ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2
and \ref{prop.bilext.uni1}.3:

\textit{Property \ref{prop.bilext.uni1}.1:} For every positive integer $n$,
the map $\gamma_{n}$ is a $k$-bilinear form $M\left[  n\right]  \times
N\left[  -n\right]  \rightarrow k$.

\textit{Property \ref{prop.bilext.uni1}.2:} We have $\gamma_{n}=\beta_{n}$ for
every $n\in\left\{  1,2,...,K\right\}  $.

\textit{Property \ref{prop.bilext.uni1}.3:} Every positive integer $n$, every
$i\in\left\{  1,2,...,n-1\right\}  $, every $x\in\mathfrak{g}\left[
-i\right]  $, every $a\in M\left[  n\right]  $ and every $b\in N\left[
-n+i\right]  $ satisfy%
\begin{equation}
\gamma_{n-i}\left(  x\rightharpoonup a,b\right)  +\gamma_{n}\left(
a,x\rightharpoonup b\right)  =0. \label{prop.bilext.uni1.main}%
\end{equation}

\end{proposition}

\footnotetext{Here and in the following, we are using the notation introduced
in Definition \ref{def.liesubspace}.}\ This proposition is rather
straightforward to prove by induction:

\begin{vershort}
\textit{Proof of Proposition \ref{prop.bilext.uni1}.} Let $\left(  \delta
_{1},\delta_{2},\delta_{3},...\right)  $ and $\left(  \varepsilon
_{1},\varepsilon_{2},\varepsilon_{3},...\right)  $ be two sequences $\left(
\gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps satisfying the
properties \ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and
\ref{prop.bilext.uni1}.3. All we need to prove is that $\left(  \delta
_{1},\delta_{2},\delta_{3},...\right)  =\left(  \varepsilon_{1},\varepsilon
_{2},\varepsilon_{3},...\right)  $. That is, all we need to prove is that
$\delta_{n}=\varepsilon_{n}$ for every positive integer $n$.

We are going to prove this by strong induction over $n$. This means that we
fix a positive integer $n$ and are going to show that $\delta_{n}%
=\varepsilon_{n}$ assuming that $\delta_{n^{\prime}}=\varepsilon_{n^{\prime}}$
holds for every positive integer $n^{\prime}<n$.

First of all, if $n\leq K$, then $n\in\left\{  1,2,...,K\right\}  $. Hence, if
$n\leq K$, then Property \ref{prop.bilext.uni1}.2 (which by assumption holds
both for $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  =\left(
\delta_{1},\delta_{2},\delta_{3},...\right)  $ and for $\left(  \gamma
_{1},\gamma_{2},\gamma_{3},...\right)  =\left(  \varepsilon_{1},\varepsilon
_{2},\varepsilon_{3},...\right)  $) yields $\delta_{n}=\beta_{n}$ and
$\varepsilon_{n}=\beta_{n}$. Thus, if $n\leq K$, we have $\delta_{n}=\beta
_{n}=\varepsilon_{n}$, which means that our induction step is complete in this
case. Thus, for the rest of this proof, we will WLOG assume that $n>K$. This
entails that (\ref{prop.bilext.uni1.gen1}) holds.

In order to prove that $\delta_{n}=\varepsilon_{n}$, we have to show that
$\delta_{n}\left(  a,b\right)  =\varepsilon_{n}\left(  a,b\right)  $ for every
$a\in M\left[  n\right]  $ and $b\in N\left[  -n\right]  $. So let $a\in
M\left[  n\right]  $ and $b\in N\left[  -n\right]  $ be arbitrary. Then,%
\[
b\in N\left[  -n\right]  =\sum\limits_{i=1}^{n-1}\mathfrak{g}\left[
-i\right]  \rightharpoonup N\left[  -n+i\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{prop.bilext.uni1.gen1})}\right)
\]
In other words, $b$ is a sum of elements of $\mathfrak{g}\left[  -i\right]
\rightharpoonup N\left[  -n+i\right]  $ for varying $i\in\left\{
1,2,...,n-1\right\}  $. Since the identity that we want to prove (that is,
$\delta_{n}\left(  a,b\right)  =\varepsilon_{n}\left(  a,b\right)  $) is
$k$-linear in $b$, we can therefore WLOG assume that $b\in\mathfrak{g}\left[
-i\right]  \rightharpoonup N\left[  -n+i\right]  $. Assume this. Then, $b$ is
a $k$-linear combination of elements of the form $z\rightharpoonup u$ with
$z\in\mathfrak{g}\left[  -i\right]  $ and $u\in N\left[  -n+i\right]  $.
Again, by linearity, we can WLOG assume that $b$ is such an element (rather
than only a $k$-linear combination). Assume this. So we have
$b=z\rightharpoonup u$ for some $z\in\mathfrak{g}\left[  -i\right]  $ and
$u\in N\left[  -n+i\right]  $. Considering this $z$ and this $u$, we have
$\delta_{n}\left(  a,b\right)  =\delta_{n}\left(  a,z\rightharpoonup u\right)
$.

But we know that $\left(  \delta_{1},\delta_{2},\delta_{3},...\right)  $ is a
sequence $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps
satisfying Property \ref{prop.bilext.uni1}.3. Hence, we can apply Property
\ref{prop.bilext.uni1}.3 to $\delta_{j}$, $z$ and $u$ instead of $\gamma_{j}$,
$x$ and $b$, and this yields $\delta_{n-i}\left(  z\rightharpoonup a,u\right)
+\delta_{n}\left(  a,z\rightharpoonup u\right)  =0$. Thus, $\delta_{n}\left(
a,z\rightharpoonup u\right)  =-\delta_{n-i}\left(  z\rightharpoonup
a,u\right)  $.

Altogether, $\delta_{n}\left(  a,b\right)  =\delta_{n}\left(
a,z\rightharpoonup u\right)  =-\delta_{n-i}\left(  z\rightharpoonup
a,u\right)  $. The same argument, applied to $\varepsilon_{j}$ instead of
$\delta_{j}$, shows that $\varepsilon_{n}\left(  a,b\right)  =-\varepsilon
_{n-i}\left(  z\rightharpoonup a,u\right)  $. But recall the induction
hypothesis saying that $\delta_{n^{\prime}}=\varepsilon_{n^{\prime}}$ holds
for every positive integer $n^{\prime}<n$. This yields $\delta_{n-i}%
=\varepsilon_{n-i}$ (since $n-i<n$). Thus,%
\[
\delta_{n}\left(  a,b\right)  =-\underbrace{\delta_{n-i}}_{=\varepsilon_{n-i}%
}\left(  z\rightharpoonup a,u\right)  =-\varepsilon_{n-i}\left(
z\rightharpoonup a,u\right)  =\varepsilon_{n}\left(  a,b\right)  .
\]
This is exactly what we needed to prove to complete the induction step. Hence,
we have shown that $\delta_{n}=\varepsilon_{n}$ for every positive integer
$n$, and the proof of Proposition \ref{prop.bilext.uni1} is complete.
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.bilext.uni1}.} Let $\left(  \delta
_{1},\delta_{2},\delta_{3},...\right)  $ and $\left(  \varepsilon
_{1},\varepsilon_{2},\varepsilon_{3},...\right)  $ be two sequences $\left(
\gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps satisfying the
properties \ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and
\ref{prop.bilext.uni1}.3. We are going to prove that $\left(  \delta
_{1},\delta_{2},\delta_{3},...\right)  =\left(  \varepsilon_{1},\varepsilon
_{2},\varepsilon_{3},...\right)  $.

We know that $\left(  \delta_{1},\delta_{2},\delta_{3},...\right)  $ is a
sequence $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps
satisfying the property \ref{prop.bilext.uni1}.3. Hence, we can apply Property
\ref{prop.bilext.uni1}.3 to $\left(  \gamma_{1},\gamma_{2},\gamma
_{3},...\right)  =\left(  \delta_{1},\delta_{2},\delta_{3},...\right)  $. As a
result, we conclude that every positive integer $n$, every $i\in\left\{
1,2,...,n-1\right\}  $, every $x\in\mathfrak{g}\left[  -i\right]  $, every
$a\in M\left[  n\right]  $ and every $b\in N\left[  -n+i\right]  $ satisfy%
\[
\delta_{n-i}\left(  x\rightharpoonup a,b\right)  +\delta_{n}\left(
a,x\rightharpoonup b\right)  =0.
\]
In other words, every positive integer $n$, every $i\in\left\{
1,2,...,n-1\right\}  $, every $x\in\mathfrak{g}\left[  -i\right]  $, every
$a\in M\left[  n\right]  $ and every $b\in N\left[  -n+i\right]  $ satisfy%
\begin{equation}
\delta_{n}\left(  a,x\rightharpoonup b\right)  =-\delta_{n-i}\left(
x\rightharpoonup a,b\right)  . \label{prop.bilext.uni1.prop3d}%
\end{equation}


The same argument (but with $\delta_{j}$ replaced by $\varepsilon_{j}$) yields
that every positive integer $n$, every $i\in\left\{  1,2,...,n-1\right\}  $,
every $x\in\mathfrak{g}\left[  -i\right]  $, every $a\in M\left[  n\right]  $
and every $b\in N\left[  -n+i\right]  $ satisfy%
\begin{equation}
\varepsilon_{n}\left(  a,x\rightharpoonup b\right)  =-\varepsilon_{n-i}\left(
x\rightharpoonup a,b\right)  . \label{prop.bilext.uni1.prop3e}%
\end{equation}


Now, we will show that
\begin{equation}
\delta_{n}=\varepsilon_{n}\ \ \ \ \ \ \ \ \ \ \text{for every positive integer
}n\text{.} \label{pf.bilext.uni1.claim}%
\end{equation}


\textit{Proof of (\ref{pf.bilext.uni1.claim}):} We will prove
(\ref{pf.bilext.uni1.claim}) by strong induction over $n$:

\textit{Induction step:}\footnote{A strong induction needs no induction base.}
Let $\mathbf{n}$ be a positive integer. Assume that
(\ref{pf.bilext.uni1.claim}) holds whenever $n<\mathbf{n}$. We now will prove
that (\ref{pf.bilext.uni1.claim}) holds when $n=\mathbf{n}$.

We will show that $\delta_{\mathbf{n}}=\varepsilon_{\mathbf{n}}$.

We distinguish between two cases:

\textit{Case 1:} We have $\mathbf{n}\leq K$.

\textit{Case 2:} We have $\mathbf{n}>K$.

Let us consider Case 1 first. In this case, $\mathbf{n}\leq K$. Thus,
$\mathbf{n}\in\left\{  1,2,...,K\right\}  $. Hence, we can apply Property
\ref{prop.bilext.uni1}.2 to $\left(  \gamma_{1},\gamma_{2},\gamma
_{3},...\right)  =\left(  \delta_{1},\delta_{2},\delta_{3},...\right)  $ and
$n=\mathbf{n}$ (because we know that $\left(  \delta_{1},\delta_{2},\delta
_{3},...\right)  $ is a sequence $\left(  \gamma_{1},\gamma_{2},\gamma
_{3},...\right)  $ of maps satisfying Property \ref{prop.bilext.uni1}.2). As a
consequence, we obtain $\delta_{\mathbf{n}}=\beta_{\mathbf{n}}$. The same
argument (but with $\delta_{i}$ replaced by $\varepsilon_{i}$) yields that
$\varepsilon_{\mathbf{n}}=\beta_{\mathbf{n}}$. Hence, $\delta_{\mathbf{n}%
}=\beta_{\mathbf{n}}=\varepsilon_{\mathbf{n}}$.

We have thus proven $\delta_{\mathbf{n}}=\varepsilon_{\mathbf{n}}$ in Case 1.

Now, let us consider Case 2. In this case, $\mathbf{n}>K$.

Let $a\in M\left[  \mathbf{n}\right]  $. We are first going to show that
\begin{equation}
\delta_{\mathbf{n}}\left(  a,b\right)  =\varepsilon_{\mathbf{n}}\left(
a,b\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,...,\mathbf{n}-1\right\}  \text{ and }b\in\mathfrak{g}\left[  -i\right]
\rightharpoonup N\left[  -\mathbf{n}+i\right]  \text{.}
\label{pf.bilext.uni1.1}%
\end{equation}


\textit{Proof of (\ref{pf.bilext.uni1.1}):} Let $i\in\left\{
1,2,...,\mathbf{n}-1\right\}  $ and $b\in\mathfrak{g}\left[  -i\right]
\rightharpoonup N\left[  -\mathbf{n}+i\right]  $. By the definition of
$\mathfrak{g}\left[  -i\right]  \rightharpoonup N\left[  -\mathbf{n}+i\right]
$, we know that $\mathfrak{g}\left[  -i\right]  \rightharpoonup N\left[
-\mathbf{n}+i\right]  $ is the $k$-linear span of all elements of the form
$y\rightharpoonup u$ with $y\in\mathfrak{g}\left[  -i\right]  $ and $u\in
N\left[  -\mathbf{n}+i\right]  $. Hence, the elements of $\mathfrak{g}\left[
-i\right]  \rightharpoonup N\left[  -\mathbf{n}+i\right]  $ are $k$-linear
combinations of elements of the form $z\rightharpoonup u$ with $z\in
\mathfrak{g}\left[  -i\right]  $ and $u\in N\left[  -\mathbf{n}+i\right]  $.
Since $b$ is an element of $\mathfrak{g}\left[  -i\right]  \rightharpoonup
N\left[  -\mathbf{n}+i\right]  $, this yields that $b$ is a $k$-linear
combination of elements of the form $z\rightharpoonup u$ with $z\in
\mathfrak{g}\left[  -i\right]  $ and $u\in M\left[  -\mathbf{n}+i\right]  $.
In other words, there exist an $L\in\mathbb{N}$, some elements $\lambda_{1}$,
$\lambda_{2}$, $...$, $\lambda_{L}$ of $k$, some elements $z_{1}$, $z_{2}$,
$...$, $z_{L}$ of $\mathfrak{g}\left[  -i\right]  $ and some elements $u_{1}$,
$u_{2}$, $...$, $u_{L}$ of $M\left[  -\mathbf{n}+i\right]  $ such that
$b=\sum\limits_{\ell=1}^{L}\lambda_{\ell}z_{\ell}\rightharpoonup u_{\ell}$.
Consider this $L$, these $\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{L}$,
these $z_{1}$, $z_{2}$, $...$, $z_{L}$, and these $u_{1}$, $u_{2}$, $...$,
$u_{L}$.

Since $b=\sum\limits_{\ell=1}^{L}\lambda_{\ell}z_{\ell}\rightharpoonup
u_{\ell}$, we have
\begin{align}
\delta_{\mathbf{n}}\left(  a,b\right)   &  =\delta_{\mathbf{n}}\left(
a,\sum\limits_{\ell=1}^{L}\lambda_{\ell}z_{\ell}\rightharpoonup u_{\ell
}\right)  =\sum\limits_{\ell=1}^{L}\lambda_{\ell}\underbrace{\delta
_{\mathbf{n}}\left(  a,z_{\ell}\rightharpoonup u_{\ell}\right)  }%
_{\substack{=-\delta_{\mathbf{n}-i}\left(  z_{\ell}\rightharpoonup a,u_{\ell
}\right)  \\\text{(by (\ref{pf.bilext.uni1.prop1delta}), applied to
}\mathbf{n}\text{, }i\text{, }z_{\ell}\text{, }a\text{ and }u_{\ell
}\\\text{instead of }n\text{, }i\text{, }x\text{, }a\text{ and }b\text{)}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{\mathbf{n}}\text{ is
}k\text{-bilinear}\right)  .\nonumber\\
&  =\sum\limits_{\ell=1}^{L}\left(  -\delta_{\mathbf{n}-i}\left(  z_{\ell
}\rightharpoonup a,u_{\ell}\right)  \right)  . \label{pf.bilext.uni1.3}%
\end{align}
The same argument (but with $\delta_{j}$ replaced by $\varepsilon_{j}$) yields%
\begin{equation}
\varepsilon_{\mathbf{n}}\left(  a,b\right)  =\sum\limits_{\ell=1}^{L}\left(
-\varepsilon_{\mathbf{n}-i}\left(  z_{\ell}\rightharpoonup a,u_{\ell}\right)
\right)  . \label{pf.bilext.uni1.4}%
\end{equation}
But since $i\in\left\{  1,2,...,\mathbf{n}-1\right\}  $, we have $i>0$, so
that $\mathbf{n}-i<\mathbf{n}$. Hence, (\ref{pf.bilext.uni1.claim}) holds for
$n=\mathbf{n}-i$ (because we have assumed that (\ref{pf.bilext.uni1.claim})
holds whenever $n<\mathbf{n}$). In other words, $\delta_{\mathbf{n}%
-i}=\varepsilon_{\mathbf{n}-i}$. Now, (\ref{pf.bilext.uni1.3}) becomes%
\[
\delta_{\mathbf{n}}\left(  a,b\right)  =\sum\limits_{\ell=1}^{L}\left(
-\underbrace{\delta_{\mathbf{n}-i}}_{=\varepsilon_{\mathbf{n}-i}}\left(
z_{\ell}\rightharpoonup a,u_{\ell}\right)  \right)  =\sum\limits_{\ell=1}%
^{L}\left(  -\varepsilon_{\mathbf{n}-i}\left(  z_{\ell}\rightharpoonup
a,u_{\ell}\right)  \right)  =\varepsilon_{\mathbf{n}}\left(  a,b\right)
\]
(by (\ref{pf.bilext.uni1.4})). This proves (\ref{pf.bilext.uni1.1}).

Now, let $b$ be any element of $N\left[  -\mathbf{n}\right]  $. Since
$\mathbf{n}>K$, we can apply (\ref{prop.bilext.uni1.gen1}) to $n=\mathbf{n}$.
As a result, we obtain%
\[
N\left[  -\mathbf{n}\right]  =\sum\limits_{i=1}^{\mathbf{n}-1}\mathfrak{g}%
\left[  -i\right]  \rightharpoonup N\left[  -\mathbf{n}+i\right]  .
\]
Hence,%
\[
b\in N\left[  -\mathbf{n}\right]  =\sum\limits_{i=1}^{\mathbf{n}%
-1}\mathfrak{g}\left[  -i\right]  \rightharpoonup N\left[  -\mathbf{n}%
+i\right]  .
\]
Thus, there exists an $\left(  \mathbf{n}-1\right)  $-tuple $\left(
b_{1},b_{2},...,b_{\mathbf{n}-1}\right)  $ of elements of $N$ such that
$\left(  \text{every }i\in\left\{  1,2,...,\mathbf{n}-1\right\}  \text{
satisfies }b_{i}\in\mathfrak{g}\left[  -i\right]  \rightharpoonup N\left[
-\mathbf{n}+i\right]  \right)  $ and $b=\sum\limits_{i=1}^{\mathbf{n}-1}b_{i}%
$. Consider this $\left(  \mathbf{n}-1\right)  $-tuple $\left(  b_{1}%
,b_{2},...,b_{\mathbf{n}-1}\right)  $. Since $b=\sum\limits_{i=1}%
^{\mathbf{n}-1}b_{i}$, we have%
\begin{align*}
\delta_{\mathbf{n}}\left(  a,b\right)   &  =\delta_{\mathbf{n}}\left(
a,\sum\limits_{i=1}^{\mathbf{n}-1}b_{i}\right)  =\sum\limits_{i=1}%
^{\mathbf{n}-1}\underbrace{\delta_{\mathbf{n}}\left(  a,b_{i}\right)
}_{\substack{=\varepsilon_{\mathbf{n}}\left(  a,b_{i}\right)  \\\text{(by
(\ref{pf.bilext.uni1.1}) (applied to }b_{i}\text{ instead of }b\text{)}%
\\\text{(since }b_{i}\in\mathfrak{g}\left[  -i\right]  \rightharpoonup
N\left[  -\mathbf{n}+i\right]  \text{))}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\delta_{\mathbf{n}}\text{ is }k\text{-linear}\right) \\
&  =\sum\limits_{i=1}^{\mathbf{n}-1}\varepsilon_{\mathbf{n}}\left(
a,b_{i}\right)  =\varepsilon_{\mathbf{n}}\left(  a,\underbrace{\sum
\limits_{i=1}^{\mathbf{n}-1}b_{i}}_{=b}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\varepsilon_{\mathbf{n}}\text{ is }k\text{-linear}\right) \\
&  =\varepsilon_{\mathbf{n}}\left(  a,b\right)  .
\end{align*}


Now, forget that we fixed $a$ and $b$. We thus have shown that $\delta
_{\mathbf{n}}\left(  a,b\right)  =\varepsilon_{\mathbf{n}}\left(  a,b\right)
$ for every $a\in M\left[  \mathbf{n}\right]  $ and $b\in N\left[
-\mathbf{n}\right]  $. In other words, $\delta_{\mathbf{n}}=\varepsilon
_{\mathbf{n}}$. We have thus proven $\delta_{\mathbf{n}}=\varepsilon
_{\mathbf{n}}$ in Case 2.

Hence, $\delta_{\mathbf{n}}=\varepsilon_{\mathbf{n}}$ is proven in each of the
Cases 1 and 2. Since the Cases 1 and 2 cover all possibilities, this yields
that $\delta_{\mathbf{n}}=\varepsilon_{\mathbf{n}}$ always holds. In other
words, (\ref{pf.bilext.uni1.claim}) holds when $n=\mathbf{n}$. This completes
the induction step. The induction proof of (\ref{pf.bilext.uni1.claim}) is
thus complete.

From (\ref{pf.bilext.uni1.claim}), we immediately obtain $\left(  \delta
_{1},\delta_{2},\delta_{3},...\right)  =\left(  \varepsilon_{1},\varepsilon
_{2},\varepsilon_{3},...\right)  $.

Now, forget that we fixed $\left(  \delta_{1},\delta_{2},\delta_{3}%
,...\right)  $ and $\left(  \varepsilon_{1},\varepsilon_{2},\varepsilon
_{3},...\right)  $. We thus have shown that if $\left(  \delta_{1},\delta
_{2},\delta_{3},...\right)  $ and $\left(  \varepsilon_{1},\varepsilon
_{2},\varepsilon_{3},...\right)  $ are two sequences $\left(  \gamma
_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps satisfying the properties
\ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}%
.3, then $\left(  \delta_{1},\delta_{2},\delta_{3},...\right)  =\left(
\varepsilon_{1},\varepsilon_{2},\varepsilon_{3},...\right)  $. In other words,
any two sequences $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of
maps satisfying the properties \ref{prop.bilext.uni1}.1,
\ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}.3 must be equal. In other
words, there exists at most one sequence $\left(  \gamma_{1},\gamma_{2}%
,\gamma_{3},...\right)  $ of maps satisfying the properties
\ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}%
.3. This proves Proposition \ref{prop.bilext.uni1}.
\end{verlong}

We will now supplement this proposition with a corresponding existence
statement, albeit one that requires more assumptions:

\begin{proposition}
\label{prop.bilext.1}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra
over a field $k$. Let $M$ and $N$ be two $\mathbb{Z}$-graded $\mathfrak{g}%
$-modules. Let $K\in\mathbb{N}$ be a nonnegative integer. Assume that every
integer $n>K$ satisfies%
\begin{equation}
N\left[  -n\right]  =\sum\limits_{i=1}^{n-1}\mathfrak{g}\left[  -i\right]
\rightharpoonup N\left[  -n+i\right]  . \label{prop.bilext.1.gen1}%
\end{equation}
Assume further that every integer $n>K$ satisfies%
\begin{equation}
M\left[  n\right]  =\sum\limits_{i=1}^{n-1}\mathfrak{g}\left[  i\right]
\rightharpoonup M\left[  n-i\right]  . \label{prop.bilext.1.gen2}%
\end{equation}
For every $i\in\left\{  1,2,...,K\right\}  $, let $\beta_{i}$ be a
$k$-bilinear form $M\left[  i\right]  \times N\left[  -i\right]  \rightarrow
k$. Assume that every $n\in\left\{  1,2,...,K\right\}  $, every $i\in\left\{
1,2,...,n-1\right\}  $, every $x\in\mathfrak{g}\left[  -i\right]  $, every
$a\in M\left[  n\right]  $ and every $b\in N\left[  -n+i\right]  $ satisfy%
\begin{equation}
\beta_{n-i}\left(  x\rightharpoonup a,b\right)  +\beta_{n}\left(
a,x\rightharpoonup b\right)  =0. \label{prop.bilext.1.main}%
\end{equation}


Then, there exists \textbf{one and only one} sequence $\left(  \gamma
_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps satisfying the three
properties \ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and
\ref{prop.bilext.uni1}.3. (See Proposition \ref{prop.bilext.uni1} for the
statements of these three properties.)
\end{proposition}

Before we prove this, we will need two lemmas:

\begin{lemma}
\label{lem.bilext.1.1}Let $p\in\mathbb{N}$. Let $k$ be a field. Let $W$ be a
$k$-vector space. Let $V_{1}$, $V_{2}$, $...$, $V_{p}$ be $k$-vector spaces.
For every $i\in\left\{  1,2,...,p\right\}  $, let $f_{i}:V_{i}\rightarrow W$
be a $k$-linear map. We denote by $\sum\limits_{i=1}^{p}f_{i}$ the map
$\bigoplus\limits_{i=1}^{p}V_{i}\rightarrow W$ obtained from the maps
$f_{i}:V_{i}\rightarrow W$ through the universal property of the direct sum
$\bigoplus\limits_{i=1}^{p}V_{i}$. (This is the map which sends every $\left(
v_{i}\right)  _{i\in\left\{  1,2,...,p\right\}  }\in\bigoplus\limits_{i=1}%
^{p}V_{i}$ to $\sum\limits_{i=1}^{p}f_{i}\left(  v_{i}\right)  $.) Then,%
\[
\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus\limits_{i=1}%
^{p}V_{i}\right)  =\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  .
\]

\end{lemma}

Lemma \ref{lem.bilext.1.1} is just a basic fact from linear algebra.

\begin{verlong}
\textit{Proof of Lemma \ref{lem.bilext.1.1}.} Let $w\in\left(  \sum
\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus\limits_{i=1}^{p}V_{i}\right)
$. Then, there exists a $v\in\bigoplus\limits_{i=1}^{p}V_{i}$ such that
$w=\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  v\right)  $. Consider
this $v$.

Since $v\in\bigoplus\limits_{i=1}^{p}V_{i}$, we know that $v$ can be written
in the form $v=\left(  v_{i}\right)  _{i\in\left\{  1,2,...,p\right\}  }$,
where $v_{1}$, $v_{2}$, $...$, $v_{p}$ are vectors in $V_{1}$, $V_{2}$, $...$,
$V_{p}$, respectively. Consider these $v_{1}$, $v_{2}$, $...$, $v_{p}$.

Now,%
\begin{align*}
w  &  =\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  v\right)  =\left(
\sum\limits_{i=1}^{p}f_{i}\right)  \left(  \left(  v_{i}\right)
_{i\in\left\{  1,2,...,p\right\}  }\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }v=\left(  v_{i}\right)  _{i\in\left\{  1,2,...,p\right\}
}\right) \\
&  =\sum\limits_{i=1}^{p}f_{i}\left(  \underbrace{v_{i}}_{\in V_{i}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\sum\limits_{i=1}%
^{p}f_{i}\right) \\
&  \in\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  .
\end{align*}


Now, forget that we fixed $w$. We thus have shown that $\left(  \sum
\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus\limits_{i=1}^{p}V_{i}\right)
\subseteq\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  $.

On the other hand, let $u\in\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  $.
Then, there exists a family $\left(  u_{i}\right)  _{i\in\left\{
1,2,...,p\right\}  }$ of elements of $W$ which satisfies $u=\sum
\limits_{i=1}^{p}u_{i}$, $\left(  u_{i}\in f_{i}\left(  V_{i}\right)  \text{
for every }i\in\left\{  1,2,...,p\right\}  \right)  $ and $\left(  \text{all
but finitely many }i\in\left\{  1,2,...,p\right\}  \text{ satisfy }%
u_{i}=0\right)  $\ \ \ \ \footnote{The third of these conditions is a
tautology, of course.}. Consider such a family $\left(  u_{i}\right)
_{i\in\left\{  1,2,...,p\right\}  }$. We know that for every $i\in\left\{
1,2,...,p\right\}  $, there exists an $x\in V_{i}$ such that $u_{i}%
=f_{i}\left(  x\right)  $ (because $u_{i}\in f_{i}\left(  V_{i}\right)  $).
Denote this $x$ by $x_{i}$. Then, for every $i\in\left\{  1,2,...,p\right\}
$, we have $x_{i}\in V_{i}$ and $u_{i}=f_{i}\left(  x_{i}\right)  $.

But $\bigoplus\limits_{i=1}^{p}V_{i}=\prod\limits_{i=1}^{p}V_{i}$ (because a
direct sum of finitely many $k$-vector spaces is the same as their direct
product) and $\left(  x_{i}\right)  _{i\in\left\{  1,2,...,p\right\}  }%
\in\prod\limits_{i=1}^{p}V_{i}$ (since $x_{i}\in V_{i}$ for every
$i\in\left\{  1,2,...,p\right\}  $). Hence,
\[
\left(  x_{i}\right)  _{i\in\left\{  1,2,...,p\right\}  }\in\prod
\limits_{i=1}^{p}V_{i}=\bigoplus\limits_{i=1}^{p}V_{i},
\]
so that $\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  \left(
x_{i}\right)  _{i\in\left\{  1,2,...,p\right\}  }\right)  $ is well-defined.
By the definition of $\sum\limits_{i=1}^{p}f_{i}$, we have%
\[
\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  \left(  x_{i}\right)
_{i\in\left\{  1,2,...,p\right\}  }\right)  =\sum\limits_{i=1}^{p}f_{i}\left(
x_{i}\right)  ,
\]
so that%
\[
u=\sum\limits_{i=1}^{p}\underbrace{u_{i}}_{=f_{i}\left(  x_{i}\right)  }%
=\sum\limits_{i=1}^{p}f_{i}\left(  x_{i}\right)  =\left(  \sum\limits_{i=1}%
^{p}f_{i}\right)  \left(  \left(  x_{i}\right)  _{i\in\left\{
1,2,...,p\right\}  }\right)  \in\left(  \sum\limits_{i=1}^{p}f_{i}\right)
\left(  \bigoplus\limits_{i=1}^{p}V_{i}\right)  .
\]


Now, forget that we fixed $u$. We thus have shown that every $u\in
\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  $ satisfies $u\in\left(
\sum\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus\limits_{i=1}^{p}%
V_{i}\right)  $. In other words, $\sum\limits_{i=1}^{p}f_{i}\left(
V_{i}\right)  \subseteq\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(
\bigoplus\limits_{i=1}^{p}V_{i}\right)  $. Combining this with $\left(
\sum\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus\limits_{i=1}^{p}%
V_{i}\right)  \subseteq\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  $, we
obtain $\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus
\limits_{i=1}^{p}V_{i}\right)  =\sum\limits_{i=1}^{p}f_{i}\left(
V_{i}\right)  $. This proves Lemma \ref{lem.bilext.1.1}.
\end{verlong}

\begin{lemma}
\label{lem.bilext.1.2}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra
over a field $k$. Let $M$ and $N$ be two $\mathbb{Z}$-graded $\mathfrak{g}%
$-modules. Let $\mathbf{n}$ be a nonnegative integer. Let $\left(  \gamma
_{1},\gamma_{2},...,\gamma_{\mathbf{n}-1}\right)  $ be a sequence of maps such
that the two properties \ref{prop.bilext.uni1}.1 and \ref{prop.bilext.uni1}.3
are satisfied for all $n\leq\mathbf{n}-1$.

[...]
\end{lemma}

\textit{Proof of Lemma \ref{lem.bilext.1.2}.} [...]

\textit{Proof of Proposition \ref{prop.bilext.1}.} From Proposition
\ref{prop.bilext.uni1}, we know that there exists \textbf{at most} one
sequence $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  $ of maps
satisfying the three properties \ref{prop.bilext.uni1}.1,
\ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}.3. It thus remains to show
that there exists \textbf{at least} one such sequence. So let us construct
such a sequence.

We will construct a sequence $\left(  \gamma_{1},\gamma_{2},\gamma
_{3},...\right)  $ of maps satisfying the three properties
\ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}%
.3 recursively, more precisely by strong induction. This means that we will
assume that $\mathbf{n}$ is a positive integer and we are given a sequence
$\left(  \gamma_{1},\gamma_{2},...,\gamma_{\mathbf{n}-1}\right)  $ of maps
satisfying the three properties \ref{prop.bilext.uni1}.1,
\ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}.3 for all $n\leq
\mathbf{n}-1$\ \ \ \ \footnote{When I say ``for all $n\leq\mathbf{n}-1$'', I
don't mean to remove the conditions imposed on $n$ in the properties
\ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}%
.3, but I just mean to add the extra condition that $n\leq\mathbf{n}-1$ to the
existing conditions. So, for example, saying that Property
\ref{prop.bilext.uni1}.2 holds for all $n\leq\mathbf{n}-1$ is equivalent to
saying that $\gamma_{n}=\beta_{n}$ for every $n\in\left\{  1,2,...,K\right\}
$ satisfying $n\leq\mathbf{n}-1$, but not to saying that $\gamma_{n}=\beta
_{n}$ for all positive integers $n$ whatsoever satisfying $n\leq\mathbf{n}%
-1$.}, and we will show how to construct a new map $\gamma_{\mathbf{n}}$ which
extends this sequence to a sequence $\left(  \gamma_{1},\gamma_{2}%
,...,\gamma_{\mathbf{n}}\right)  $ of maps satisfying the three properties
\ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}%
.3 for all $n\leq\mathbf{n}$. This will allow recursively constructing maps
$\gamma_{1}$, $\gamma_{2}$, $\gamma_{3}$, $...$ in such a way that the
resulting sequence $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  $
satisfies the three properties \ref{prop.bilext.uni1}.1,
\ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}.3 for all $n$. This will
show that there exists at least one such sequence.

So, let us perform the induction step in the recursive construction of the
sequence $\left(  \gamma_{1},\gamma_{2},\gamma_{3},...\right)  $. Let
$\mathbf{n}$ be a positive integer. Assume that we are given a sequence
$\left(  \gamma_{1},\gamma_{2},...,\gamma_{\mathbf{n}-1}\right)  $ of maps
satisfying the three properties \ref{prop.bilext.uni1}.1,
\ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}.3 for all $n\leq
\mathbf{n}-1$. We need to construct a new map $\gamma_{\mathbf{n}}$ which
extends this sequence to a sequence $\left(  \gamma_{1},\gamma_{2}%
,...,\gamma_{\mathbf{n}}\right)  $ of maps satisfying the three properties
\ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and \ref{prop.bilext.uni1}%
.3 for all $n\leq\mathbf{n}$.

Let us define the map $\gamma_{\mathbf{n}}:M\left[  \mathbf{n}\right]  \times
N\left[  -\mathbf{n}\right]  \rightarrow k$ as follows:

Let us distinguish between two cases:

\textit{Case 1:} We have $\mathbf{n}\in\left\{  1,2,...,K\right\}  $.

\textit{Case 2:} We have $\mathbf{n}\notin\left\{  1,2,...,K\right\}  $.

Let us consider Case 1 first. In this case, $\mathbf{n}\in\left\{
1,2,...,K\right\}  $. Thus, $\beta_{\mathbf{n}}$ is a well-defined map
$M\left[  \mathbf{n}\right]  \times N\left[  -\mathbf{n}\right]  \rightarrow
k$. Hence, we can just define $\gamma_{\mathbf{n}}$ to be $\beta_{\mathbf{n}}%
$. Define $\gamma_{\mathbf{n}}$ this way. Thus, $\gamma_{\mathbf{n}}$ is
defined in Case 1. (We will later prove that the sequence $\left(  \gamma
_{1},\gamma_{2},...,\gamma_{\mathbf{n}}\right)  $ satisfies the three
properties \ref{prop.bilext.uni1}.1, \ref{prop.bilext.uni1}.2 and
\ref{prop.bilext.uni1}.3 for all $n\leq\mathbf{n}$.)

Let us now consider Case 2. In this case, $\mathbf{n}\notin\left\{
1,2,...,K\right\}  $, so that $\mathbf{n}>K$ (since $\mathbf{n}$ is a positive integer).

Let $i\in\left\{  1,2,...,\mathbf{n}-1\right\}  $. Then, $\mathbf{n}%
-i\in\left\{  1,2,...,\mathbf{n}-1\right\}  $. In other words, $\mathbf{n}-i$
is a positive integer satisfying $\mathbf{n}-i\leq\mathbf{n}-1$. Hence,
Property \ref{prop.bilext.uni1}.1 is satisfied for $n=\mathbf{n}-i$ (because
we have assumed that Property \ref{prop.bilext.uni1}.1 is satisfied for all
$n\leq\mathbf{n}-1$). Hence, we can apply Property \ref{prop.bilext.uni1}.1 to
$n=\mathbf{n}-i$. We thus conclude that the map $\gamma_{\mathbf{n}-i}$ is a
$k$-bilinear form $M\left[  \mathbf{n}-i\right]  \times N\left[  -\left(
\mathbf{n}-i\right)  \right]  \rightarrow k$. In other words, the map
$\gamma_{\mathbf{n}-i}$ is a $k$-bilinear form $M\left[  \mathbf{n}-i\right]
\times N\left[  -\mathbf{n}+i\right]  \rightarrow k$.

Define a map $\xi_{i}:\mathfrak{g}\left[  -i\right]  \times N\left[
-\mathbf{n}+i\right]  \rightarrow\operatorname*{Hom}\left(  M\left[
\mathbf{n}\right]  ,k\right)  $ as follows: For every $\left(  x,b\right)
\in\mathfrak{g}\left[  -i\right]  \times N\left[  -\mathbf{n}+i\right]  $,
define $\xi_{i}\left(  x,b\right)  $ to be the map%
\begin{align*}
M\left[  \mathbf{n}\right]   &  \rightarrow k,\\
a  &  \mapsto\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b\right)  .
\end{align*}
This is well-defined\footnote{\textit{Proof.} For every $\left(  x,b\right)
\in\mathfrak{g}\left[  -i\right]  \times N\left[  -\mathbf{n}+i\right]  $, the
map%
\begin{align*}
M\left[  \mathbf{n}\right]   &  \rightarrow k,\\
a  &  \mapsto\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b\right)
\end{align*}
is $k$-linear (since $\gamma_{\mathbf{n}-i}$ and the action of $\mathfrak{g}$
on $M$ are both $k$-bilinear), and hence is an element of $\operatorname*{Hom}%
\left(  M\left[  \mathbf{n}\right]  ,k\right)  $. Hence, $\xi_{i}$ is
well-defined.}. Thus, we have defined a map $\xi_{i}$.

For every $\left(  x,b\right)  \in\mathfrak{g}\left[  -i\right]  \times
N\left[  -\mathbf{n}+i\right]  $, we know that $\xi_{i}\left(  x,b\right)  $
is the map%
\begin{align*}
M\left[  \mathbf{n}\right]   &  \rightarrow k,\\
a  &  \mapsto\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b\right)
\end{align*}
(because this is how we have defined $\xi_{i}\left(  x,b\right)  $). In other
words, for every $\left(  x,b\right)  \in\mathfrak{g}\left[  -i\right]  \times
N\left[  -\mathbf{n}+i\right]  $, we have%
\begin{equation}
\left(  \xi_{i}\left(  x,b\right)  \right)  \left(  a\right)  =\gamma
_{\mathbf{n}-i}\left(  x\rightharpoonup a,b\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }a\in M\left[  \mathbf{n}\right]  .
\label{pf.bilext.1.xi_i}%
\end{equation}


The map $\xi_{i}$ is $k$-bilinear\footnote{\textit{Proof.} \textbf{a)} Let
$x_{1}$ and $x_{2}$ be elements of $\mathfrak{g}\left[  -i\right]  $. Let
$\lambda_{1}$ and $\lambda_{2}$ be elements of $k$. Let $b$ be an element of
$N\left[  -\mathbf{n}+i\right]  $. Since $x_{1}\in\mathfrak{g}\left[
-i\right]  $ and $x_{2}\in\mathfrak{g}\left[  -i\right]  $, we have
$\lambda_{1}x_{1}+\lambda_{2}x_{2}\in\mathfrak{g}\left[  -i\right]  $, so that
$\left(  \lambda_{1}x_{1}+\lambda_{2}x_{2},b\right)  \in\mathfrak{g}\left[
-i\right]  \times N\left[  -\mathbf{n}+i\right]  $. Thus, every $a\in M\left[
\mathbf{n}\right]  $ satisfies%
\begin{align}
&  \left(  \xi_{i}\left(  \lambda_{1}x_{1}+\lambda_{2}x_{2},b\right)  \right)
\left(  a\right) \nonumber\\
&  =\gamma_{\mathbf{n}-i}\left(  \underbrace{\left(  \lambda_{1}x_{1}%
+\lambda_{2}x_{2}\right)  \rightharpoonup a}_{=\lambda_{1}x_{1}\rightharpoonup
a+\lambda_{2}x_{2}\rightharpoonup a},b\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.bilext.1.xi_i}), applied to }x=\lambda_{1}x_{1}+\lambda
_{2}x_{2}\right) \nonumber\\
&  =\gamma_{\mathbf{n}-i}\left(  \lambda_{1}x_{1}\rightharpoonup a+\lambda
_{2}x_{2}\rightharpoonup a,b\right)  =\lambda_{1}\gamma_{\mathbf{n}-i}\left(
x_{1}\rightharpoonup a,b\right)  +\lambda_{2}\gamma_{\mathbf{n}-i}\left(
x_{2}\rightharpoonup a,b\right) \label{pf.bilext.1.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\gamma_{\mathbf{n}-i}\text{ is a
}k\text{-bilinear map}\right)  .\nonumber
\end{align}
On the other hand, every $a\in M\left[  \mathbf{n}\right]  $ satisfies
\begin{equation}
\left(  \xi_{i}\left(  x_{1},b\right)  \right)  \left(  a\right)
=\gamma_{\mathbf{n}-i}\left(  x_{1}\rightharpoonup a,b\right)
\label{pf.bilext.1.1b}%
\end{equation}
(by (\ref{pf.bilext.1.xi_i}), applied to $x=x_{1}$) and
\begin{equation}
\left(  \xi_{i}\left(  x_{2},b\right)  \right)  \left(  a\right)
=\gamma_{\mathbf{n}-i}\left(  x_{2}\rightharpoonup a,b\right)
\label{pf.bilext.1.1c}%
\end{equation}
(by (\ref{pf.bilext.1.xi_i}), applied to $x=x_{2}$). Hence, every $a\in
M\left[  \mathbf{n}\right]  $ satisfies
\begin{align*}
&  \left(  \xi_{i}\left(  \lambda_{1}x_{1}+\lambda_{2}x_{2},b\right)  \right)
\left(  a\right) \\
&  =\lambda_{1}\underbrace{\gamma_{\mathbf{n}-i}\left(  x_{1}\rightharpoonup
a,b\right)  }_{\substack{=\left(  \xi_{i}\left(  x_{1},b\right)  \right)
\left(  a\right)  \\\text{(by (\ref{pf.bilext.1.1b}))}}}+\lambda
_{2}\underbrace{\gamma_{\mathbf{n}-i}\left(  x_{2}\rightharpoonup a,b\right)
}_{\substack{=\left(  \xi_{i}\left(  x_{2},b\right)  \right)  \left(
a\right)  \\\text{(by (\ref{pf.bilext.1.1c}))}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.bilext.1.1})}\right) \\
&  =\lambda_{1}\left(  \xi_{i}\left(  x_{1},b\right)  \right)  \left(
a\right)  +\lambda_{2}\left(  \xi_{i}\left(  x_{2},b\right)  \right)  \left(
a\right)  =\left(  \lambda_{1}\xi_{i}\left(  x_{1},b\right)  +\lambda_{2}%
\xi_{i}\left(  x_{2},b\right)  \right)  \left(  a\right)  .
\end{align*}
In other words, $\xi_{i}\left(  \lambda_{1}x_{1}+\lambda_{2}x_{2},b\right)
=\lambda_{1}\xi_{i}\left(  x_{1},b\right)  +\lambda_{2}\xi_{i}\left(
x_{2},b\right)  $.
\par
Now, forget that we fixed $x_{1}$, $x_{2}$, $\lambda_{1}$, $\lambda_{2}$ and
$b$. We thus have proven that $\xi_{i}\left(  \lambda_{1}x_{1}+\lambda
_{2}x_{2},b\right)  =\lambda_{1}\xi_{i}\left(  x_{1},b\right)  +\lambda_{2}%
\xi_{i}\left(  x_{2},b\right)  $ for all $x_{1}\in\mathfrak{g}\left[
-i\right]  $, $x_{2}\in\mathfrak{g}\left[  -i\right]  $, $\lambda_{1}\in k$,
$\lambda_{2}\in k$ and $b\in N\left[  -\mathbf{n}+i\right]  $. In other words,
the map $\xi_{i}$ is $k$-linear in its first argument.
\par
\textbf{b)} Now, let $b_{1}$ and $b_{2}$ be elements of $N\left[
-\mathbf{n}+i\right]  $. Let $\mu_{1}$ and $\mu_{2}$ be elements of $k$. Let
$x$ be an element of $\mathfrak{g}\left[  -i\right]  $. Since $b_{1}\in
N\left[  -\mathbf{n}+i\right]  $ and $b_{2}\in N\left[  -\mathbf{n}+i\right]
$, we have $\mu_{1}b_{1}+\mu_{2}b_{2}\in N\left[  -\mathbf{n}+i\right]  $, so
that $\left(  x,\mu_{1}b_{1}+\mu_{2}b_{2}\right)  \in\mathfrak{g}\left[
-i\right]  \times N\left[  -\mathbf{n}+i\right]  $. Thus, every $a\in M\left[
\mathbf{n}\right]  $ satisfies%
\begin{align}
&  \left(  \xi_{i}\left(  x,\mu_{1}b_{1}+\mu_{2}b_{2}\right)  \right)  \left(
a\right) \nonumber\\
&  =\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,\mu_{1}b_{1}+\mu_{2}%
b_{2}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.bilext.1.xi_i}),
applied to }b=\mu_{1}b_{1}+\mu_{2}b_{2}\right) \nonumber\\
&  =\mu_{1}\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b_{1}\right)
+\mu_{2}\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b_{2}\right)
\label{pf.bilext.1.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\gamma_{\mathbf{n}-i}\text{ is a
}k\text{-bilinear map}\right)  .\nonumber
\end{align}
On the other hand, every $a\in M\left[  \mathbf{n}\right]  $ satisfies
\begin{equation}
\left(  \xi_{i}\left(  x,b_{1}\right)  \right)  \left(  a\right)
=\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b_{1}\right)
\label{pf.bilext.1.2b}%
\end{equation}
(by (\ref{pf.bilext.1.xi_i}), applied to $b=b_{1}$) and
\begin{equation}
\left(  \xi_{i}\left(  x,b_{2}\right)  \right)  \left(  a\right)
=\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b_{2}\right)
\label{pf.bilext.1.2c}%
\end{equation}
(by (\ref{pf.bilext.1.xi_i}), applied to $b=b_{2}$). Hence, every $a\in
M\left[  \mathbf{n}\right]  $ satisfies
\begin{align*}
&  \left(  \xi_{i}\left(  x,\mu_{1}b_{1}+\mu_{2}b_{2}\right)  \right)  \left(
a\right) \\
&  =\mu_{1}\underbrace{\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup
a,b_{1}\right)  }_{\substack{=\left(  \xi_{i}\left(  x,b_{1}\right)  \right)
\left(  a\right)  \\\text{(by (\ref{pf.bilext.1.2b}))}}}+\mu_{2}%
\underbrace{\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup a,b_{2}\right)
}_{\substack{=\left(  \xi_{i}\left(  x,b_{2}\right)  \right)  \left(
a\right)  \\\text{(by (\ref{pf.bilext.1.2c}))}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.bilext.1.2})}\right) \\
&  =\mu_{1}\left(  \xi_{i}\left(  x,b_{1}\right)  \right)  \left(  a\right)
+\mu_{2}\left(  \xi_{i}\left(  x,b_{2}\right)  \right)  \left(  a\right)
=\left(  \mu_{1}\xi_{i}\left(  x,b_{1}\right)  +\mu_{2}\xi_{i}\left(
x,b_{2}\right)  \right)  \left(  a\right)  .
\end{align*}
In other words, $\xi_{i}\left(  x,\mu_{1}b_{1}+\mu_{2}b_{2}\right)  =\mu
_{1}\xi_{i}\left(  x,b_{1}\right)  +\mu_{2}\xi_{i}\left(  x,b_{2}\right)  $.
\par
Now, forget that we fixed $b_{1}$, $b_{2}$, $\mu_{1}$, $\mu_{2}$ and $x$. We
thus have proven that $\xi_{i}\left(  x,\mu_{1}b_{1}+\mu_{2}b_{2}\right)
=\mu_{1}\xi_{i}\left(  x,b_{1}\right)  +\mu_{2}\xi_{i}\left(  x,b_{2}\right)
$ for all $b_{1}\in N\left[  -\mathbf{n}+i\right]  $, $b_{2}\in N\left[
-\mathbf{n}+i\right]  $, $\mu_{1}\in k$, $\mu_{2}\in k$ and $x\in
\mathfrak{g}\left[  -i\right]  $. In other words, the map $\xi_{i}$ is
$k$-linear in its second argument.
\par
\textbf{c)} Now, we know that the map $\xi_{i}$ is $k$-linear in its first
argument and $k$-linear in its second argument. Hence, the map $\xi_{i}$ is
$k$-bilinear, qed.}. Hence, by the universal property of the tensor product,
this map $\xi_{i}:\mathfrak{g}\left[  -i\right]  \times N\left[
-\mathbf{n}+i\right]  \rightarrow\operatorname*{Hom}\left(  M\left[
\mathbf{n}\right]  ,k\right)  $ gives rise to a $k$-linear map $\Xi
_{i}:\mathfrak{g}\left[  -i\right]  \otimes N\left[  -\mathbf{n}+i\right]
\rightarrow\operatorname*{Hom}\left(  M\left[  \mathbf{n}\right]  ,k\right)  $
satisfying%
\[
\left(  \Xi_{i}\left(  x\otimes b\right)  =\xi_{i}\left(  x,b\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  x,b\right)  \in\mathfrak{g}%
\left[  -i\right]  \times N\left[  -\mathbf{n}+i\right]  \right)  .
\]
Consider this map $\Xi_{i}$. Then, every $x\in\mathfrak{g}\left[  -i\right]  $
and $b\in N\left[  -\mathbf{n}+i\right]  $ satisfy%
\[
\Xi_{i}\left(  x\otimes b\right)  =\xi_{i}\left(  x,b\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  x,b\right)  \in
\mathfrak{g}\left[  -i\right]  \times N\left[  -\mathbf{n}+i\right]  \text{
(since }x\in\mathfrak{g}\left[  -i\right]  \text{ and }b\in N\left[
-\mathbf{n}+i\right]  \text{)}\right)  .
\]
Thus, every $x\in\mathfrak{g}\left[  -i\right]  $, $b\in N\left[
-\mathbf{n}+i\right]  $ and $a\in M\left[  \mathbf{n}\right]  $ satisfy%
\begin{equation}
\left(  \underbrace{\Xi_{i}\left(  x\otimes b\right)  }_{=\xi_{i}\left(
x,b\right)  }\right)  \left(  a\right)  =\left(  \xi_{i}\left(  x,b\right)
\right)  \left(  a\right)  =\gamma_{\mathbf{n}-i}\left(  x\rightharpoonup
a,b\right)  \label{pf.bilext.1.Xi_i}%
\end{equation}
(by (\ref{pf.bilext.1.xi_i})).

Now, forget that we fixed $i$. We thus have defined a $k$-linear map $\Xi
_{i}:\mathfrak{g}\left[  -i\right]  \otimes N\left[  -\mathbf{n}+i\right]
\rightarrow\operatorname*{Hom}\left(  M\left[  \mathbf{n}\right]  ,k\right)  $
for every $i\in\left\{  1,2,...,\mathbf{n}-1\right\}  $. Consider the map
$\sum\limits_{i=1}^{\mathbf{n}-1}\Xi_{i}$ (defined in the same way as the map
$\sum\limits_{i=1}^{p}f_{i}$ in Lemma \ref{lem.bilext.1.1}). Denote this map
by $\Xi$. Thus, $\Xi=\sum\limits_{i=1}^{\mathbf{n}-1}\Xi_{i}$.

On the other hand, let $i\in\left\{  1,2,...,\mathbf{n}-1\right\}  $ again.
Define a map $\omega_{i}:\mathfrak{g}\left[  -i\right]  \times N\left[
-\mathbf{n}+i\right]  \rightarrow N\left[  -\mathbf{n}\right]  $ by%
\begin{equation}
\left(  \omega_{i}\left(  x,b\right)  =x\rightharpoonup
b\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  x,b\right)  \in\mathfrak{g}\left[
-i\right]  \times N\left[  -\mathbf{n}+i\right]  \right)  .
\label{pf.bilext.1.omega_i}%
\end{equation}
This is well-defined\footnote{\textit{Proof.} For every $\left(  x,b\right)
\in\mathfrak{g}\left[  -i\right]  \times N\left[  -\mathbf{n}+i\right]  $, the
element $x\rightharpoonup b$ lies in $N\left[  -\mathbf{n}\right]  $ (because
we have $\left(  x,b\right)  \in\mathfrak{g}\left[  -i\right]  \times N\left[
-\mathbf{n}+i\right]  $, so that $x\in\mathfrak{g}\left[  -i\right]  $ and
$b\in N\left[  -\mathbf{n}+i\right]  $, so that
\begin{align*}
\underbrace{x}_{\in\mathfrak{g}\left[  -i\right]  }\rightharpoonup
\underbrace{b}_{\in N\left[  -\mathbf{n}+i\right]  }  &  \in\mathfrak{g}%
\left[  -i\right]  \rightharpoonup N\left[  -\mathbf{n}+i\right]  \subseteq
N\left[  \underbrace{\left(  -i\right)  +\left(  -\mathbf{n}+i\right)
}_{=-\mathbf{n}}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }N\text{ is a }\mathbb{Z}%
\text{-graded }\mathfrak{g}\text{-module}\right) \\
&  =N\left[  -\mathbf{n}\right]
\end{align*}
). Hence, $\omega_{i}$ is well-defined, qed.}. We have thus defined a map
$\omega_{i}$.

[...]

According to Lemma \ref{lem.bilext.1.1} (applied to $p=\mathbf{n}-1$,
$W=\operatorname*{Hom}\left(  M\left[  \mathbf{n}\right]  ,k\right)  $,
$V_{i}=\mathfrak{g}\left[  -i\right]  \otimes N\left[  -\mathbf{n}+i\right]  $
and $f_{i}=\Xi_{i}$), we have%
\[
\left(  \sum\limits_{i=1}^{\mathbf{n}-1}\Xi_{i}\right)  \left(  \bigoplus
\limits_{i=1}^{\mathbf{n}-1}V_{i}\right)  =\sum\limits_{i=1}^{\mathbf{n}-1}%
\Xi_{i}\left(  V_{i}\right)
\]


[Quote]Let $p\in\mathbb{N}$. Let $k$ be a field. Let $W$ be a $k$-vector
space. Let $V_{1}$, $V_{2}$, $...$, $V_{p}$ be $k$-vector spaces. For every
$i\in\left\{  1,2,...,p\right\}  $, let $f_{i}:V_{i}\rightarrow W$ be a
$k$-linear map. We denote by $\sum\limits_{i=1}^{p}f_{i}$ the map
$\bigoplus\limits_{i=1}^{p}V_{i}\rightarrow W$ obtained from the maps
$f_{i}:V_{i}\rightarrow W$ through the universal property of the direct sum
$\bigoplus\limits_{i=1}^{p}V_{i}$. (This is the map which sends every $\left(
v_{i}\right)  _{i\in\left\{  1,2,...,p\right\}  }\in\bigoplus\limits_{i=1}%
^{p}V_{i}$ to $\sum\limits_{i=1}^{p}f_{i}\left(  v_{i}\right)  $.) Then,%
\[
\left(  \sum\limits_{i=1}^{p}f_{i}\right)  \left(  \bigoplus\limits_{i=1}%
^{p}V_{i}\right)  =\sum\limits_{i=1}^{p}f_{i}\left(  V_{i}\right)  .
\]


[...]

Let $a\in M\left[  \mathbf{n}\right]  $ and $b\in N\left[  -\mathbf{n}\right]
$.

[...]

[...] [...]
\end{noncompile}

\subsection{Simple Lie algebras: a recollection}

The Kac-Moody Lie algebras form a class of Lie algebras which contains all
simple finite-dimensional and all affine Lie algebras, but also many more.
Before we start studying them, let us recall some facts about simple Lie algebras:

Let $\mathfrak{g}$ be a finite-dimensional simple Lie algebra over
$\mathbb{C}$. A \textit{Cartan subalgebra} of $\mathfrak{g}$ means a maximal
commutative Lie subalgebra which consists of semisimple\footnote{An element of
a Lie algebra is said to be \textit{semisimple} if and only if its action on
the adjoint representation is a semisimple operator.} elements. There are
usually many Cartan subalgebras of $\mathfrak{g}$, but they are all conjugate
under the action of the corresponding Lie group $G$ (which satisfies
$\mathfrak{g}=\operatorname*{Lie}G$, and can be defined as the connected
component of the identity in the group $\operatorname*{Aut}\mathfrak{g}$).
Thus, there is no loss of generality in picking one such subalgebra. So pick a
Cartan subalgebra $\mathfrak{h}$ of $\mathfrak{g}$. We denote the dimension
$\dim\mathfrak{h}$ by $n$ and also by $\operatorname*{rank}\mathfrak{g}$. This
dimension $\dim\mathfrak{h}=\operatorname*{rank}\mathfrak{g}$ is called the
\textit{rank} of $\mathfrak{g}$. The restriction of the Killing form on
$\mathfrak{g}$ to $\mathfrak{h}\times\mathfrak{h}$ is a nondegenerate
symmetric bilinear form on $\mathfrak{h}$.

For every $\alpha\in\mathfrak{h}^{\ast}$, we can define a vector subspace
$\mathfrak{g}_{\alpha}$ of $\mathfrak{g}$ by
\[
\mathfrak{g}_{\alpha}=\left\{  a\in\mathfrak{g}\ \mid\ \left[  h,a\right]
=\alpha\left(  h\right)  a\text{ for all }h\in\mathfrak{h}\right\}  .
\]
It can be shown that $\mathfrak{g}_{0}=\mathfrak{h}$. Now we let $\Delta$ be
the finite subset $\left\{  \alpha\in\mathfrak{h}^{\ast}\diagdown\left\{
0\right\}  \ \mid\ \mathfrak{g}_{\alpha}\neq0\right\}  $ of $\mathfrak{h}%
^{\ast}\diagdown\left\{  0\right\}  $. Then, $\mathfrak{g}=\mathfrak{h}%
\oplus\bigoplus\limits_{\alpha\in\Delta}\mathfrak{g}_{\alpha}$ (as a direct
sum of vector spaces). The subset $\Delta$ is called the \textit{root system}
of $\mathfrak{g}$. The elements of $\Delta$ are called the \textit{roots} of
$\mathfrak{g}$. It is known that for each $\alpha\in\Delta$, the vector space
$\mathfrak{g}_{\alpha}$ is one-dimensional and can be written as
$\mathfrak{g}_{\alpha}=\mathbb{C}e_{\alpha}$ for some particular $e_{\alpha
}\in\mathfrak{g}_{\alpha}$.

We want to use the decomposition $\mathfrak{g}=\mathfrak{h}\oplus
\bigoplus\limits_{\alpha\in\Delta}\mathfrak{g}_{\alpha}$ in order to construct
a triangular decomposition of $\mathfrak{g}$. This can be done with the
grading which we constructed in Proposition \ref{prop.grad.g}, but let us do
it again now, with more elementary means: Fix an $\overline{h}\in\mathfrak{h}$
such that every $\alpha\in\Delta$ satisfies $\alpha\left(  \overline
{h}\right)  \in\mathbb{R}\diagdown\left\{  0\right\}  $ (it can be seen that
such $\overline{h}$ exists). Define $\Delta_{+}=\left\{  \alpha\in\Delta
\ \mid\ \alpha\left(  \overline{h}\right)  >0\right\}  $ and $\Delta
_{-}=\left\{  \alpha\in\Delta\ \mid\ \alpha\left(  \overline{h}\right)
<0\right\}  $. Then, $\Delta$ is the union of two disjoint subsets $\Delta
_{+}$ and $\Delta_{-}$, and we have $\Delta_{+}=-\Delta_{-}$. The triangular
decomposition of $\mathfrak{g}$ is now defined as $\mathfrak{g}=\mathfrak{n}%
_{-}\oplus\mathfrak{h}\oplus\mathfrak{n}_{+}$, where $\mathfrak{n}%
_{-}=\bigoplus\limits_{\alpha\in\Delta_{-}}\mathfrak{g}_{\alpha}$ and
$\mathfrak{n}_{+}=\bigoplus\limits_{\alpha\in\Delta_{+}}\mathfrak{g}_{\alpha}%
$. This decomposition depends on the choice of $\overline{h}$ (and
$\mathfrak{h}$, of course). The elements of $\Delta_{+}$ are called
\textit{positive roots} of $\mathfrak{g}$, and the elements of $\Delta_{-}$
are called \textit{negative roots} of $\mathfrak{g}$. If $\alpha$ is a root of
$\mathfrak{g}$, then we write $\alpha>0$ if $\alpha$ is a positive root, and
we write $\alpha<0$ if $\alpha$ is a negative root.

Let us now construct the grading on $\mathfrak{g}$ which yields this
triangular decomposition $\mathfrak{g}=\mathfrak{n}_{-}\oplus\mathfrak{h}%
\oplus\mathfrak{n}_{+}$. This grading was already constructed in Proposition
\ref{prop.grad.g}, but now we are going to do this in detail:

We define the \textit{simple roots} of $\mathfrak{g}$ as the elements of
$\Delta_{+}$ which cannot be written as sums of more than one element of
$\Delta_{+}$. It can be shown that there are exactly $n$ of these simple
roots, and they form a basis of $\mathfrak{h}^{\ast}$. Denote these simple
roots as $\alpha_{1}$, $\alpha_{2}$, $...$, $\alpha_{n}$. Every root
$\alpha\in\Delta_{+}$ can now be written in the form $\alpha=\sum
\limits_{i=1}^{n}k_{i}\left(  \alpha\right)  \alpha_{i}$ for a unique
$n$-tuple $\left(  k_{1}\left(  \alpha\right)  ,k_{2}\left(  \alpha\right)
,...,k_{n}\left(  \alpha\right)  \right)  $ of nonnegative integers.

For all $\alpha,\beta\in\Delta$ with $\alpha+\beta\notin\Delta\cup\left\{
0\right\}  $, we have $\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{\beta
}\right]  =0$. For all $\alpha,\beta\in\mathfrak{h}^{\ast}$, we have $\left[
\mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}\right]  \subseteq\mathfrak{g}%
_{\alpha+\beta}$. In particular, for every $\alpha\in\mathfrak{h}^{\ast}$, we
have $\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{-\alpha}\right]
\subseteq\mathfrak{h}$. Better yet, we can show that for every $\alpha
\in\Delta$, there exists some nonzero $h_{\alpha}\in\mathfrak{h}$ such that
$\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{-\alpha}\right]  =\mathbb{C}%
h_{\alpha}$.

For every $i\in\left\{  1,2,...,n\right\}  $, pick a generator $e_{i}$ of the
vector space $\mathfrak{g}_{\alpha_{i}}$ and a generator $f_{i}$ of the vector
space $\mathfrak{g}_{-\alpha_{i}}$.

It is possible to normalize $e_{i}$ and $f_{i}$ in such a way that $\left[
h_{i},e_{i}\right]  =2e_{i}$ and $\left[  h_{i},f_{i}\right]  =-2f_{i}$, where
$h_{i}=\left[  e_{i},f_{i}\right]  $. This $h_{i}$ will, of course, lie in
$\mathfrak{h}$ and be a scalar multiple of $h_{\alpha_{i}}$. We can normalize
$h_{\alpha_{i}}$ in such a way that $h_{i}=h_{\alpha_{i}}$. We suppose that
all these normalizations are done. Then:

\begin{proposition}
\label{prop.serre-gen.1}With the notations introduced above, we have:

\textbf{(a)} The family $\left(  h_{1},h_{2},...,h_{n}\right)  $ is a basis of
$\mathfrak{h}$.

\textbf{(b)} For any $i$ and $j$ in $\left\{  1,2,...,n\right\}  $, denote
$\alpha_{j}\left(  h_{i}\right)  $ by $a_{i,j}$. The Lie algebra
$\mathfrak{g}$ is generated (as a Lie algebra) by the elements $e_{i}$,
$f_{i}$ and $h_{i}$ with $i\in\left\{  1,2,...,n\right\}  $ (a total of $3n$
elements), and the following relations hold:%
\begin{align*}
\left[  h_{i},h_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  h_{i},e_{j}\right]   &  =\alpha_{j}\left(  h_{i}\right)  e_{j}%
=a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{
1,2,...,n\right\}  ;\\
\left[  h_{i},f_{j}\right]   &  =-\alpha_{j}\left(  h_{i}\right)
f_{j}=a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{
1,2,...,n\right\}  ;\\
\left[  e_{i},f_{j}\right]   &  =\delta_{i,j}h_{i}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{  1,2,...,n\right\}  .
\end{align*}
(This does not mean that no more relations hold. In fact, additional
relations, the so-called Serre relations, do hold in $\mathfrak{g}$; we will
see these relations later, in Theorem \ref{thm.serre-gen.2}.)

The $n\times n$ matrix $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ is
called the \textit{Cartan matrix} of $\mathfrak{g}$.

Let $\left(  \cdot,\cdot\right)  $ denote the standard form on $\mathfrak{g}$
(defined in Definition \ref{def.standform}). Then, $\left(  \cdot
,\cdot\right)  $ is a nonzero scalar multiple of the Killing form on
$\mathfrak{g}$ (since any two nonzero invariant symmetric bilinear forms on
$\mathfrak{g}$ are scalar multiples of each other). Hence, the restriction of
$\left(  \cdot,\cdot\right)  $ to $\mathfrak{h}\times\mathfrak{h}$ is
nondegenerate (since the restriction of the Killing form to $\mathfrak{h}%
\times\mathfrak{h}$ is nondegenerate). Thus, this restriction gives rise to a
vector space isomorphism $\mathfrak{h}\rightarrow\mathfrak{h}^{\ast}$. This
isomorphism sends $h_{i}$ to $\alpha_{i}^{\vee}=\dfrac{2\alpha_{i}}{\left(
\alpha_{i},\alpha_{i}\right)  }$ for every $i$ (where we denote by $\left(
\cdot,\cdot\right)  $ not only the standard form, but also the inverse form of
its restriction to $\mathfrak{h}$). Thus, $a_{i,j}=\alpha_{j}\left(
h_{i}\right)  =\dfrac{2\left(  \alpha_{j},\alpha_{i}\right)  }{\left(
\alpha_{i},\alpha_{i}\right)  }$ for all $i$ and $j$. (Note that the latter
equality would still hold if $\left(  \cdot,\cdot\right)  $ would mean the
Killing form rather than the standard form.)

The elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ are called \textit{Chevalley
generators} of $\mathfrak{g}$.

\textbf{Properties of the matrix }$A$\textbf{:}

\textbf{1)} We have $a_{i,i}=2$ for all $i\in\left\{  1,2,...,n\right\}  $.

\textbf{2)} Any two distinct $i\in\left\{  1,2,...,n\right\}  $ and
$j\in\left\{  1,2,...,n\right\}  $ satisfy $a_{i,j}\leq0$ and $a_{i,j}%
\in\mathbb{Z}$. Also, $a_{i,j}=0$ if and only if $a_{j,i}=0$.

\textbf{3)} The matrix $A$ is indecomposable (i. e., if conjugation of $A$ by
a permutation matrix brings $A$ into a block-diagonal form $\left(
\begin{array}
[c]{cc}%
A_{1} & 0\\
0 & A_{2}%
\end{array}
\right)  $, then either $A_{1}$ or $A_{2}$ is a $0\times0$ matrix).

\textbf{4)} The matrix $A$ is positive. Here is what we mean by this: There
exists a diagonal $n\times n$ matrix $D$ with positive diagonal entries such
that $DA$ is a symmetric and positive definite matrix.
\end{proposition}

\begin{theorem}
An $n\times n$ matrix $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ satisfies
the four properties \textbf{1)}, \textbf{2)}, \textbf{3)} and \textbf{4)} of
Proposition \ref{prop.serre-gen.1} if and only if it is a Cartan matrix of a
simple Lie algebra.
\end{theorem}

Such matrices (and thus, simple finite-dimensional Lie algebras) can be
encoded by so-called \textit{Dynkin diagrams}. The \textit{Dynkin diagram} of
a simple Lie algebra $\mathfrak{g}$ is defined as the graph with vertex set
$\left\{  1,2,...,n\right\}  $, and the following rules for drawing
edges\footnote{The notion of a graph we are using here is slightly different
from the familiar notions of a graph in graph theory, since this graph can
have both directed and undirected edges.}:

\begin{itemize}
\item If $a_{i,j}=0$, then the vertices $i$ and $j$ are not connected by any
edge (directed or undirected).

\item If $a_{i,j}=a_{j,i}=-1$, then the vertices $i$ and $j$ are connected by
exactly one edge, and this edge is undirected.

\item If $a_{i,j}=-2$ and $a_{j,i}=-1$, then the vertices $i$ and $j$ are
connected by two directed edges from $j$ to $i$ (and no other edges).

\item If $a_{i,j}=-3$ and $a_{j,i}=-1$, then the vertices $i$ and $j$ are
connected by three directed edges from $j$ to $i$ (and no other edges).
\end{itemize}

Here is a classification of simple finite-dimensional Lie algebras by their
Dynkin diagrams:

$A_{n}=\mathfrak{sl}\left(  n+1\right)  $ for $n\geq1$; the Dynkin diagram is
$%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$ (with $n$ nodes).

$B_{n}=\mathfrak{so}\left(  2n+1\right)  $ for $n\geq2$; the Dynkin diagram is
$%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{=>}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{=>}[r] & \circ}%
%EndExpansion
$ (with $n$ nodes, only the last edge being directed and double). (Note that
$\mathfrak{so}\left(  3\right)  \cong\mathfrak{sl}\left(  2\right)  $.)

$C_{n}=\mathfrak{sp}\left(  2n\right)  $ for $n\geq2$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{<=}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{<=}[r] & \circ}%
%EndExpansion
$ (with $n$ nodes, only the last edge being directed and double). (Note that
$\mathfrak{sp}\left(  2\right)  \cong\mathfrak{sl}\left(  2\right)  $ and
$\mathfrak{sp}\left(  4\right)  \cong\mathfrak{so}\left(  5\right)  $.)

$D_{n}=\mathfrak{so}\left(  2n\right)  $ for $n\geq4$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & & & & \circ\\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{-}[ru] \ar@{-}[rd] \\
%& & & & & \circ}}}%
%BeginExpansion
\xymatrix{
& & & & & \circ\\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{-}[ru] \ar@{-}[rd] \\
& & & & & \circ}%
%EndExpansion
$ (with $n$ nodes). (Note that $\mathfrak{so}\left(  4\right)  \cong%
\mathfrak{sl}\left(  2\right)  \oplus\mathfrak{sl}\left(  2\right)  $ and
$\mathfrak{so}\left(  6\right)  \cong\mathfrak{sl}\left(  4\right)  $.)

Exceptional Lie algebras:

$E_{6}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & \circ\ar@{-}[d] & & \\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ
%}}}%
%BeginExpansion
\xymatrix{
& & \circ\ar@{-}[d] & & \\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$E_{7}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & \circ\ar@{-}[d] & & & \\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
%[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
& & \circ\ar@{-}[d] & & & \\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$E_{8}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & \circ\ar@{-}[d] & & & & \\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
%[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
& & \circ\ar@{-}[d] & & & & \\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$F_{4}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{=>}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{=>}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$G_{2}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ& \circ\ar@3{->}[l]
%}}}%
%BeginExpansion
\xymatrix{
\circ& \circ\ar@3{->}[l]
}%
%EndExpansion
$.

Now to the Serre relations, which we have not yet written down:

\begin{theorem}
\label{thm.serre-gen.2}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. Use the notations introduced in Proposition \ref{prop.serre-gen.1}.

\textbf{(a)} Let $i$ and $j$ be two distinct elements of $\left\{
1,2,...,n\right\}  $. Then, in $\mathfrak{g}$, we have $\left(
\operatorname*{ad}\left(  e_{i}\right)  \right)  ^{1-a_{i,j}}e_{j}=0$ and
$\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}%
=0$. These relations (totalling up to $2n\left(  n-1\right)  $ relations,
because there are $n\left(  n-1\right)  $ pairs $\left(  i,j\right)  $ of
distinct elements of $\left\{  1,2,...,n\right\}  $) are called the
\textit{Serre relations} for $\mathfrak{g}$.

\textbf{(b)} Combined with the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  h_{i},h_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  h_{i},e_{j}\right]  =a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  e_{i},f_{j}\right]  =\delta_{i,j}h_{i}\ \ \ \ \ \ \ \ \ \ \text{for
all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{nonserre-relations}%
\end{equation}
of Proposition \ref{prop.serre-gen.1}, the Serre relations form a set of
defining relations for $\mathfrak{g}$. This means that, if
$\widetilde{\mathfrak{g}}$ denotes the quotient Lie algebra
\[
\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  ,
\]
then $\widetilde{\mathfrak{g}}\diagup\left(  \text{Serre relations}\right)
\cong\mathfrak{g}$. (Here, $\operatorname*{FreeLie}\left(  h_{i},f_{i}%
,e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ denotes the free Lie
algebra with $3n$ generators $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$.)
\end{theorem}

\begin{remark}
If $\mathfrak{g}\cong\mathfrak{sl}_{2}$, then $\mathfrak{g}$ has no Serre
relations (because $n=1$), and thus the claim of Theorem \ref{thm.serre-gen.2}
\textbf{(b)} rewrites as $\widetilde{\mathfrak{g}}\cong\mathfrak{g}$ (where
$\widetilde{\mathfrak{g}}$ is defined as in Theorem \ref{thm.serre-gen.2}).
But in all other cases, the Lie algebra $\widetilde{\mathfrak{g}}$ is
infinite-dimensional, and while it clearly projects onto $\mathfrak{g}$, it is
much bigger than $\mathfrak{g}$.
\end{remark}

We will give a partial proof of Theorem \ref{thm.serre-gen.2}: We will only
prove part \textbf{(a)}.

\textit{Proof of Theorem \ref{thm.serre-gen.2} \textbf{(a)}.} Define a
$\mathbb{C}$-linear map%
\begin{align*}
\Phi_{i}:\mathfrak{sl}_{2}  &  \rightarrow\mathfrak{g},\\
e  &  \mapsto e_{i},\\
f  &  \mapsto f_{i},\\
h  &  \mapsto h_{i}.
\end{align*}
Since $\left[  e_{i},f_{i}\right]  =h_{i}$, $\left[  h_{i},e_{i}\right]
=2e_{i}$ and $\left[  h_{i},f_{i}\right]  =-2f_{i}$, this map $\Phi_{i}$ is a
Lie algebra homomorphism.

But $\mathfrak{g}$ is a $\mathfrak{g}$-module (by the adjoint representation
of $\mathfrak{g}$), and thus becomes an $\mathfrak{sl}_{2}$-module by means of
$\Phi_{i}:\mathfrak{sl}_{2}\rightarrow\mathfrak{g}$. This $\mathfrak{sl}_{2}%
$-module satisfies%
\[
ef_{j}=\underbrace{\left(  \Phi_{i}\left(  e\right)  \right)  }_{=e_{i}}%
f_{j}=\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)  f_{j}=\left[
e_{i},f_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq j\right)
\]
and%
\[
hf_{j}=\underbrace{\left(  \Phi_{i}\left(  h\right)  \right)  }_{=h_{i}}%
f_{j}=\left(  \operatorname*{ad}\left(  h_{i}\right)  \right)  f_{j}=\left[
h_{i},f_{j}\right]  =-a_{i,j}f_{j}.
\]
Hence, Lemma \ref{lem.serre-gen.sl2} \textbf{(c)} (applied to $V=\mathfrak{g}%
$, $\lambda=-a_{i,j}$ and $x=f_{j}$) yields that $-a_{i,j}\in\mathbb{N}$ and
$f^{-a_{i,j}+1}f_{j}=0$. Since%
\[
f^{-a_{i,j}+1}f_{j}=f^{1-a_{i,j}}f_{j}=\left(  \underbrace{\Phi_{i}\left(
f\right)  }_{=f_{i}}\right)  ^{1-a_{i,j}}f_{j}=\left(  \operatorname*{ad}%
\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j},
\]
this rewrites as $\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}=0$. Similarly, $\left(  \operatorname*{ad}\left(
e_{i}\right)  \right)  ^{1-a_{i,j}}e_{j}=0$. Theorem \ref{thm.serre-gen.2}
\textbf{(a)} is thus proven.

As we said, we are not going to prove Theorem \ref{thm.serre-gen.2}
\textbf{(b)} here.

\subsection{\textbf{[unfinished]} Kac-Moody Lie algebras: definition and
construction}

Now forget about our simple Lie algebra $\mathfrak{g}$. Let us first define
the notion of contragredient Lie algebras by axioms; we will construct these
algebras later.

\begin{definition}
\label{def.contragredient}Suppose that $A=\left(  a_{i,j}\right)  _{1\leq
i,j\leq n}$ is any $n\times n$ matrix of complex numbers.

Let $Q$ be the free abelian group generated by $n$ symbols $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{n}$ (that is, $Q=\mathbb{Z}\alpha_{1}%
\oplus\mathbb{Z}\alpha_{2}\oplus...\oplus\mathbb{Z}\alpha_{n}$). These symbols
are just symbols, not weights of any Lie algebra (at the moment). We write the
group $Q$ additively.

A \textit{contragredient Lie algebra} corresponding to $A$ is a $Q$-graded
$\mathbb{C}$-Lie algebra $\mathfrak{g}$ which is (as a Lie algebra) generated
by some elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ which satisfy the following three conditions:

\textbf{(1)} These elements satisfy the relations (\ref{nonserre-relations}).

\textbf{(2)} The vector space $\mathfrak{g}\left[  0\right]  $ has $\left(
h_{1},h_{2},...,h_{n}\right)  $ as a $\mathbb{C}$-vector space basis, and we
have $\mathfrak{g}\left[  \alpha_{i}\right]  =\mathbb{C}e_{i}$ and
$\mathfrak{g}\left[  -\alpha_{i}\right]  =\mathbb{C}f_{i}$ for all
$i\in\left\{  1,2,...,n\right\}  $.

\textbf{(3)} Every nonzero $Q$-graded ideal in $\mathfrak{g}$ has a nonzero
intersection with $\mathfrak{g}\left[  0\right]  $.

(Here, we are using the notation $\mathfrak{g}\left[  \alpha\right]  $ for the
$\alpha$-th homogeneous component of the $Q$-graded Lie algebra $\mathfrak{g}%
$, just as in Definition \ref{def.Q-graded.lie}.)

Just as in the case of $\mathbb{Z}$-graded Lie algebras, we will denote
$\mathfrak{g}\left[  0\right]  $ by $\mathfrak{h}$.
\end{definition}

Note that the condition \textbf{(3)} is satisfied for simple
finite-dimensional Lie algebras $\mathfrak{g}$ (graded by their weight spaces,
where $Q$ is the root lattice\footnote{in the meaning which this word has in
the theory of simple Lie algebras} of $\mathfrak{g}$, and $A$ is the Cartan
matrix); hence, simple finite-dimensional Lie algebras (graded by their weight
spaces) are contragredient.

\begin{theorem}
\label{thm.g(A).exuni}Let $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ be a
(fixed) $n\times n$ matrix of complex numbers.

\textbf{(a)} Then, there exists a unique (up to $Q$-graded isomorphism
respecting the generators $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$,
$...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$) contragredient Lie algebra
$\mathfrak{g}$ corresponding to $A$.

\textbf{(b)} If $A$ is a Cartan matrix, then the contragredient Lie algebra
$\mathfrak{g}$ corresponding to $A$ is finite-dimensional and simple.
\end{theorem}

\begin{definition}
Let $A$ be an $n\times n$ matrix of complex numbers. Then, the unique (up to
isomorphism) contragredient Lie algebra $\mathfrak{g}$ corresponding to $A$ is
denoted by $\mathfrak{g}\left(  A\right)  $.
\end{definition}

The proof of Theorem \ref{thm.g(A).exuni} rests upon the following fact:

\begin{theorem}
\label{thm.gtilde}Let $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ be an
$n\times n$ matrix of complex numbers. Let $e_{1}$, $e_{2}$, $...$, $e_{n}$,
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ be $3n$
distinct symbols (which are, a priori, new and unrelated to the vectors
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ in Definition \ref{def.contragredient}). Let
$\widetilde{\mathfrak{g}}$ be the quotient Lie algebra
\[
\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  .
\]
(Here, $\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  $ denotes the free Lie algebra with $3n$
generators $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$,
$h_{1}$, $h_{2}$, $...$, $h_{n}$.)

By abuse of notation, we will denote the projections of the elements $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$ onto the quotient Lie algebra $\widetilde{\mathfrak{g}}$ by the
same letters $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$.

Let $Q$ be the free abelian group generated by $n$ symbols $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{n}$ (that is, $Q=\mathbb{Z}\alpha_{1}%
\oplus\mathbb{Z}\alpha_{2}\oplus...\oplus\mathbb{Z}\alpha_{n}$). These symbols
are just symbols, not weights of any Lie algebra (at the moment).

\textbf{(a)} We can make $\widetilde{\mathfrak{g}}$ uniquely into a $Q$-graded
Lie algebra by setting%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]


\textbf{(b)} Let $\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(
e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ (this means the free
Lie algebra with $n$ generators $e_{1}$, $e_{2}$, $...$, $e_{n}$).

Let $\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  f_{i}%
\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ (this means the free Lie
algebra with $n$ generators $f_{1}$, $f_{2}$, $...$, $f_{n}$).

Let $\widetilde{\mathfrak{h}}$ be the free vector space with basis
$h_{1},h_{2},...,h_{n}$. Consider $\widetilde{\mathfrak{h}}$ as an abelian Lie algebra.

Then, we have well-defined canonical Lie algebra homomorphisms $\iota
_{+}:\widetilde{\mathfrak{n}}_{+}\rightarrow\widetilde{\mathfrak{g}}$ and
$\iota_{-}:\widetilde{\mathfrak{n}}_{-}\rightarrow\widetilde{\mathfrak{g}}$
given by sending the generators $e_{1}$, $e_{2}$, $...$, $e_{n}$ (in the case
of $\iota_{+}$), respectively, $f_{1}$, $f_{2}$, $...$, $f_{n}$ (in the case
of $\iota_{-}$) to the corresponding generators $e_{1}$, $e_{2}$, $...$,
$e_{n}$ (in the case of $\iota_{+}$), respectively, $f_{1}$, $f_{2}$, $...$,
$f_{n}$ (in the case of $\iota_{-}$). Moreover, we have a well-defined linear
map $\iota_{0}:\widetilde{\mathfrak{h}}\rightarrow\widetilde{\mathfrak{g}}$
given by sending the generators $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $h_{1}$,
$h_{2}$, $...$, $h_{n}$, respectively.

These maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ are injective Lie algebra homomorphisms.

\textbf{(c)} We have $\widetilde{\mathfrak{g}}=\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $.

\textbf{(d)} Both $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
\oplus\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ and $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ are Lie subalgebras of
$\widetilde{\mathfrak{g}}$.

\textbf{(e)} The $0$-th homogeneous component of $\widetilde{\mathfrak{g}}$
(in the $Q$-grading) is $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $.
That is, $\widetilde{\mathfrak{g}}\left[  0\right]  =\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $. Moreover,%
\[
\bigoplus_{\substack{\alpha\text{ is a }\mathbb{Z}\text{-linear combination}%
\\\text{of }\alpha_{1}\text{, }\alpha_{2}\text{, }...\text{, }\alpha_{n}\text{
with nonnegative}\\\text{coefficients; }\alpha\neq0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  =\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
\]
and%
\[
\bigoplus_{\substack{\alpha\text{ is a }\mathbb{Z}\text{-linear combination}%
\\\text{of }\alpha_{1}\text{, }\alpha_{2}\text{, }...\text{, }\alpha_{n}\text{
with nonpositive}\\\text{coefficients; }\alpha\neq0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  =\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
.
\]


\textbf{(f)} There exists an involutive Lie algebra automorphism of
$\widetilde{\mathfrak{g}}$ which sends $e_{1}$, $e_{2}$, $...$, $e_{n}$,
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$, $-h_{2}$,
$...$, $-h_{n}$, respectively.

\textbf{(g)} Every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  =\mathbb{C}e_{i}$ and
$\widetilde{\mathfrak{g}}\left[  -\alpha_{i}\right]  =\mathbb{C}f_{i}$.

\textbf{(h)} Let $I$ be the sum of all $Q$-graded ideals in
$\widetilde{\mathfrak{g}}$ which have zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $. Then, $I$ itself is a $Q$-graded ideal in
$\widetilde{\mathfrak{g}}$ which has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $.

\textbf{(i)} Let $\mathfrak{g}=\widetilde{\mathfrak{g}}\diagup I$. Clearly,
$\mathfrak{g}$ is a $Q$-graded Lie algebra. The projections of the elements
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{g}}$ on the quotient Lie
algebra $\widetilde{\mathfrak{g}}\diagup I=\mathfrak{g}$ will still be denoted
by $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}%
$, $h_{2}$, $...$, $h_{n}$. Then, $\mathfrak{g}$ is a contragredient Lie
algebra corresponding to $A$.
\end{theorem}

\begin{definition}
Let $A$ be an $n\times n$ matrix of complex numbers. Then, the Lie algebra
$\widetilde{\mathfrak{g}}$ defined in Theorem \ref{thm.gtilde} is denoted by
$\widetilde{\mathfrak{g}}\left(  A\right)  $.
\end{definition}

\textit{Proof of Theorem \ref{thm.gtilde}.} First of all, for the sake of
clarity, let us make a convention: In the following proof, the word ``Lie
derivation'' will always mean ``derivation of Lie algebras'', whereas the word
``derivation'' without the word ``Lie'' directly in front of it will always
mean ``derivation of algebras''. The only exception to this will be the
formulation ``$\mathfrak{a}$ acts on $\mathfrak{b}$ by derivations'' where
$\mathfrak{a}$ and $\mathfrak{b}$ are two Lie algebras; this formulation has
been defined in Definition \ref{def.semidir.lielie} \textbf{(a)}.

\bigskip

\begin{vershort}
\textbf{(f)} The relations%
\[
\left\{
\begin{array}
[c]{l}%
\left[  -h_{i},-h_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  -h_{i},f_{j}\right]  =a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  -h_{i},e_{j}\right]  =-a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  f_{i},e_{j}\right]  =\delta_{i,j}\left(  -h_{i}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.
\]
are satisfied in $\widetilde{\mathfrak{g}}$ (since they are easily seen to be
equivalent to the relations (\ref{nonserre-relations}), and the relations
(\ref{nonserre-relations}) are satisfied in $\widetilde{\mathfrak{g}}$ by the
definition of $\widetilde{\mathfrak{g}}$). Hence, we can define a Lie algebra
homomorphism
\[
\omega:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  \rightarrow\widetilde{\mathfrak{g}}%
\]
by requiring%
\[
\left\{
\begin{array}
[c]{c}%
\omega\left(  e_{i}\right)  =f_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\omega\left(  f_{i}\right)  =e_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\omega\left(  h_{i}\right)  =-h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  .
\]
Consider this $\omega$. Since $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)
=\widetilde{\mathfrak{g}}$, this homomorphism $\omega$ is a Lie algebra
endomorphism of $\widetilde{\mathfrak{g}}$. It is easy to see that the Lie
algebra homomorphisms $\omega^{2}$ and $\operatorname*{id}$ are equal on the
generators $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$,
$h_{1}$, $h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{g}}$. Hence, these
must be identical, i. e., we have $\omega^{2}=\operatorname*{id}$. Thus,
$\omega$ is an involutive Lie algebra automorphism of $\widetilde{\mathfrak{g}%
}$, and as we know from its definition, it sends $e_{1}$, $e_{2}$, $...$,
$e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$,
$-h_{2}$, $...$, $-h_{n}$, respectively. This proves Theorem \ref{thm.gtilde}
\textbf{(f)}.
\end{vershort}

\begin{verlong}
\textbf{(f)} Let us notice that the relations (\ref{nonserre-relations}) are
equivalent to the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  -h_{i},-h_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  -h_{i},f_{j}\right]  =a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  -h_{i},e_{j}\right]  =-a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  f_{i},e_{j}\right]  =\delta_{i,j}\left(  -h_{i}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{nonserre-relations2}%
\end{equation}
\footnote{\textit{Proof.} We will show that the assertion
\begin{equation}
\left(  \left[  e_{i},f_{j}\right]  =\delta_{i,j}h_{i}\text{ for all }%
i,j\in\left\{  1,2,...,n\right\}  \right)  \label{pf.gtilde.equiv.1}%
\end{equation}
is equivalent to the assertion
\begin{equation}
\left(  \left[  f_{i},e_{j}\right]  =\delta_{i,j}\left(  -h_{i}\right)
\ \text{for all }i,j\in\left\{  1,2,...,n\right\}  \right)  .
\label{pf.gtilde.equiv.2}%
\end{equation}
\par
If (\ref{pf.gtilde.equiv.1}) holds, then (\ref{pf.gtilde.equiv.2}) holds as
well (because if (\ref{pf.gtilde.equiv.1}) holds, then any $i,j\in\left\{
1,2,...,n\right\}  $ satisfy%
\begin{align*}
-\left[  f_{i},e_{j}\right]   &  =\left[  e_{j},f_{i}\right]
=\underbrace{\delta_{j,i}}_{=\delta_{i,j}=\left\{
\begin{array}
[c]{c}%
1,\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  }h_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.gtilde.equiv.1}),
applied to }i\text{ and }j\text{ instead of }j\text{ and }i\right) \\
&  =\left\{
\begin{array}
[c]{c}%
1,\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  h_{j}=\left\{
\begin{array}
[c]{c}%
h_{j},\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
h_{i},\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=i\text{ in the case when
}i=j\right) \\
&  =\underbrace{\left\{
\begin{array}
[c]{c}%
1,\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  }_{=\delta_{i,j}}h_{i}=\delta_{i,j}h_{i}%
\end{align*}
and thus $\left[  f_{i},e_{j}\right]  =-\delta_{i,j}h_{i}=\delta_{i,j}\left(
-h_{i}\right)  $). Similarly, if (\ref{pf.gtilde.equiv.2}) holds, then
(\ref{pf.gtilde.equiv.1}) holds as well. Thus, the assertion
(\ref{pf.gtilde.equiv.1}) is equivalent to the assertion
(\ref{pf.gtilde.equiv.2}). In other words, the fourth of the four relations
(\ref{nonserre-relations}) is equivalent to the fourth of the four relations
(\ref{nonserre-relations2}). But it is easy to see that the second of the four
relations (\ref{nonserre-relations}) is equivalent to the third of the four
relations (\ref{nonserre-relations2}). Similarly, the third of the four
relations (\ref{nonserre-relations}) is equivalent to the second of the four
relations (\ref{nonserre-relations2}). Finally, the first of the four
relations (\ref{nonserre-relations}) is equivalent to the first of the four
relations (\ref{nonserre-relations2}). Altogether, we thus conclude that the
relations (\ref{nonserre-relations}) are equivalent to the relations
(\ref{nonserre-relations2}), qed.}. Hence,
\begin{align*}
\widetilde{\mathfrak{g}}  &  =\operatorname*{FreeLie}\left(  h_{i},f_{i}%
,e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right) \\
&  =\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations2})}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the relations
(\ref{nonserre-relations}) are equivalent to the relations
(\ref{nonserre-relations2})}\right)  .
\end{align*}
Hence, the relations (\ref{nonserre-relations2}) are satisfied in
$\widetilde{\mathfrak{g}}$.

Now, we can define a Lie algebra homomorphism
\[
\Omega:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \rightarrow\widetilde{\mathfrak{g}}%
\]
by requiring%
\begin{equation}
\left\{
\begin{array}
[c]{c}%
\Omega\left(  e_{i}\right)  =f_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\Omega\left(  f_{i}\right)  =e_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\Omega\left(  h_{i}\right)  =-h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{pf.gtilde.OMEGA}%
\end{equation}
(because we can define a Lie algebra homomorphism from a Lie algebra by
arbitrarily choosing its values on the free generators). Define this $\Omega$.
Then, $\Omega$ sends the relations (\ref{nonserre-relations}) to the relations
(\ref{nonserre-relations2}). Since the relations (\ref{nonserre-relations2})
are satisfied in $\widetilde{\mathfrak{g}}$, this yields that the Lie algebra
homomorphism $\Omega$ factors through the factor Lie algebra%
\[
\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  .
\]
In other words, there exists a unique Lie algebra homomorphism%
\[
\omega:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  \rightarrow\widetilde{\mathfrak{g}}%
\]
satisfying $\omega\circ\pi=\Omega$, where
\begin{align*}
\pi:  &  \operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right) \\
&  \rightarrow\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)
\end{align*}
is the canonical projection. Consider this $\omega$. Since \newline%
$\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  =\widetilde{\mathfrak{g}}$, this $\omega$
is a Lie algebra homomorphism from $\widetilde{\mathfrak{g}}$ to
$\widetilde{\mathfrak{g}}$. Due to $\omega\circ\pi=\Omega$ and because of
(\ref{pf.gtilde.OMEGA}), we have%
\begin{equation}
\left\{
\begin{array}
[c]{c}%
\omega\left(  e_{i}\right)  =f_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\omega\left(  f_{i}\right)  =e_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\omega\left(  h_{i}\right)  =-h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  . \label{pf.gtilde.omega}%
\end{equation}
Thus, $\omega$ sends $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$,
$...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$, $-h_{2}$, $...$, $-h_{n}%
$, respectively.

The elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ generate $\widetilde{\mathfrak{g}}$
as a Lie algebra\footnote{This is because $\widetilde{\mathfrak{g}%
}=\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  $.}. In other words, the subset $\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $
generates $\widetilde{\mathfrak{g}}$ as a Lie algebra.

The maps $\omega^{2}$ and $\operatorname*{id}$ are equal to each other on the
set $\left\{  e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2}%
,...,h_{n}\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let $x\in\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $.
We will prove that $\omega^{2}\left(  x\right)  =\operatorname*{id}\left(
x\right)  $.
\par
Indeed, since $x\in\left\{  e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n}%
,h_{1},h_{2},...,h_{n}\right\}  =\left\{  e_{1},e_{2},...,e_{n}\right\}
\cup\left\{  f_{1},f_{2},...,f_{n}\right\}  \cup\left\{  h_{1},h_{2}%
,...,h_{n}\right\}  $, we must be in one of the three following three cases:
\par
\textit{Case 1:} We have $x\in\left\{  e_{1},e_{2},...,e_{n}\right\}  $.
\par
\textit{Case 2:} We have $x\in\left\{  f_{1},f_{2},...,f_{n}\right\}  $.
\par
\textit{Case 3:} We have $x\in\left\{  h_{1},h_{2},...,h_{n}\right\}  $.
\par
Let us first consider Case 1. In this case, $x\in\left\{  e_{1},e_{2}%
,...,e_{n}\right\}  $. Thus, there exists an $i\in\left\{  1,2,...,n\right\}
$ such that $x=e_{i}$. Consider this $i$. From $x=e_{i}$, we obtain
$\omega\left(  x\right)  =\omega\left(  e_{i}\right)  =f_{i}$ and thus
$\omega^{2}\left(  x\right)  =\omega\left(  \underbrace{\omega\left(
x\right)  }_{=f_{i}}\right)  =\omega\left(  f_{i}\right)  =e_{i}%
=x=\operatorname*{id}\left(  x\right)  $. Thus, $\omega^{2}\left(  x\right)
=\operatorname*{id}\left(  x\right)  $ is proven in Case 1.
\par
Let us next consider Case 2. In this case, $x\in\left\{  f_{1},f_{2}%
,...,f_{n}\right\}  $. Thus, there exists an $i\in\left\{  1,2,...,n\right\}
$ such that $x=f_{i}$. Consider this $i$. From $x=f_{i}$, we obtain
$\omega\left(  x\right)  =\omega\left(  f_{i}\right)  =e_{i}$ and thus
$\omega^{2}\left(  x\right)  =\omega\left(  \underbrace{\omega\left(
x\right)  }_{=e_{i}}\right)  =\omega\left(  e_{i}\right)  =f_{i}%
=x=\operatorname*{id}\left(  x\right)  $. Thus, $\omega^{2}\left(  x\right)
=\operatorname*{id}\left(  x\right)  $ is proven in Case 2.
\par
Let us first consider Case 3. In this case, $x\in\left\{  h_{1},h_{2}%
,...,h_{n}\right\}  $. Thus, there exists an $i\in\left\{  1,2,...,n\right\}
$ such that $x=h_{i}$. Consider this $i$. From $x=h_{i}$, we obtain
$\omega\left(  x\right)  =\omega\left(  h_{i}\right)  =-h_{i}$ and thus
$\omega^{2}\left(  x\right)  =\omega\left(  \underbrace{\omega\left(
x\right)  }_{=-h_{i}}\right)  =\omega\left(  -h_{i}\right)
=-\underbrace{\omega\left(  h_{i}\right)  }_{=-h_{i}}=-\left(  -h_{i}\right)
=h_{i}=x=\operatorname*{id}\left(  x\right)  $. Thus, $\omega^{2}\left(
x\right)  =\operatorname*{id}\left(  x\right)  $ is proven in Case 3.
\par
Hence, the equality $\omega^{2}\left(  x\right)  =\operatorname*{id}\left(
x\right)  $ is proven in each of the three Cases 1, 2 and 3. Since these three
cases cover all possibilities, this yields that $\omega^{2}\left(  x\right)
=\operatorname*{id}\left(  x\right)  $ always holds.
\par
Now forget that we fixed $x$. Thus, we have shown that $\omega^{2}\left(
x\right)  =\operatorname*{id}\left(  x\right)  $ for every $x\in\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $.
In other words, the maps $\omega^{2}$ and $\operatorname*{id}$ are equal to
each other on the set $\left\{  e_{1},e_{2},...,e_{n},f_{1},f_{2}%
,...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $, qed.}. Since this set $\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $
generates $\widetilde{\mathfrak{g}}$ as a Lie algebra, this yields that the
maps $\omega^{2}$ and $\operatorname*{id}$ are equal to each other on a
generating set of the Lie algebra $\widetilde{\mathfrak{g}}$. We also know
that $\omega^{2}$ and $\operatorname*{id}$ are Lie algebra homomorphisms
(since $\omega$ is a Lie algebra homomorphism).

Now, it is well-known that if two Lie algebra homomorphisms from a Lie algebra
$\mathfrak{i}$ to another Lie algebra are equal to each other on a generating
set of the Lie algebra $\mathfrak{i}$, then these two homomorphisms must be
identical. Applied to our two Lie algebra homomorphisms $\omega^{2}$ and
$\operatorname*{id}$ (which, as we know, are equal to each other on a
generating set of the Lie algebra $\widetilde{\mathfrak{g}}$), we conclude
that the two homomorphisms $\omega^{2}$ and $\operatorname*{id}$ must be
identical. In other words, $\omega^{2}=\operatorname*{id}$. Hence, $\omega$ is
an involutive automorphism of the Lie algebra $\widetilde{\mathfrak{g}}$.

Thus, there exists an involutive Lie algebra automorphism of
$\widetilde{\mathfrak{g}}$ which sends $e_{1}$, $e_{2}$, $...$, $e_{n}$,
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$, $-h_{2}$,
$...$, $-h_{n}$, respectively (namely, $\omega$). This proves Theorem
\ref{thm.gtilde} \textbf{(f)}.
\end{verlong}

\bigskip

\textbf{(a)} In order to define a $Q$-grading on a free Lie algebra, it is
enough to choose the degrees of its free generators. Thus, we can define a
$Q$-grading on the Lie algebra $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ by setting%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]
The relations (\ref{nonserre-relations}) are homogeneous with respect to this
$Q$-grading; hence, the quotient Lie algebra $\operatorname*{FreeLie}\left(
h_{i},f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)
\diagup\left(  \text{the relations (\ref{nonserre-relations})}\right)  $
inherits the $Q$-grading from $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $. Since this
quotient Lie algebra is $\widetilde{\mathfrak{g}}$, we thus have constructed a
$Q$-grading on $\widetilde{\mathfrak{g}}$ which satisfies%
\begin{equation}
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  . \label{pf.gtilde.a.1}%
\end{equation}
Since this grading is clearly the only one to satisfy (\ref{pf.gtilde.a.1})
(because $\widetilde{\mathfrak{g}}$ is generated as a Lie algebra by $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$), this proves Theorem \ref{thm.gtilde} \textbf{(a)}.

\bigskip

\textbf{(b)} \underline{\textit{1st step: Definitions and identifications.}}

Let $N_{+}$ be the free vector space with basis $e_{1},e_{2},...,e_{n}$. Since
$\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(  e_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $, we then have a canonical
isomorphism $\widetilde{\mathfrak{n}}_{+}\cong\operatorname*{FreeLie}\left(
N_{+}\right)  $ (where $\operatorname*{FreeLie}\left(  N_{+}\right)  $ means
the free Lie algebra over the vector space (not the set) $N_{+}$). We identify
$\widetilde{\mathfrak{n}}_{+}$ with $\operatorname*{FreeLie}\left(
N_{+}\right)  $ along this isomorphism. Due to the construction of the free
Lie algebra, we have a canonical injection $N_{+}\rightarrow
\operatorname*{FreeLie}\left(  N_{+}\right)  =\widetilde{\mathfrak{n}}_{+}$.
We will regard this injection as an inclusion (so that $N_{+}\subseteq
\widetilde{\mathfrak{n}}_{+}$).

By Proposition \ref{prop.Ufree} (applied to $V=N_{+}$), there exists a
canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}\left(
N_{+}\right)  \right)  \rightarrow T\left(  N_{+}\right)  $. We identify
$U\left(  \widetilde{\mathfrak{n}}_{+}\right)  =U\left(
\operatorname*{FreeLie}\left(  N_{+}\right)  \right)  $ with $T\left(
N_{+}\right)  $ along this isomorphism.

Let $N_{-}$ be the free vector space with basis $f_{1},f_{2},...,f_{n}$. Since
$\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  f_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $, we then have a canonical
isomorphism $\widetilde{\mathfrak{n}}_{-}\cong\operatorname*{FreeLie}\left(
N_{-}\right)  $ (where $\operatorname*{FreeLie}\left(  N_{-}\right)  $ means
the free Lie algebra over the vector space (not the set) $N_{-}$). We identify
$\widetilde{\mathfrak{n}}_{-}$ with $\operatorname*{FreeLie}\left(
N_{-}\right)  $ along this isomorphism. Due to the construction of the free
Lie algebra, we have a canonical injection $N_{-}\rightarrow
\operatorname*{FreeLie}\left(  N_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$.
We will regard this injection as an inclusion (so that $N_{-}\subseteq
\widetilde{\mathfrak{n}}_{-}$).

By Proposition \ref{prop.Ufree} (applied to $V=N_{-}$), there exists a
canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}\left(
N_{-}\right)  \right)  \rightarrow T\left(  N_{-}\right)  $. We identify
$U\left(  \widetilde{\mathfrak{n}}_{-}\right)  =U\left(
\operatorname*{FreeLie}\left(  N_{-}\right)  \right)  $ with $T\left(
N_{-}\right)  $ along this isomorphism.

A consequence of the Poincar\'{e}-Birkhoff-Witt theorem says that for any Lie
algebra $\mathfrak{i}$, the canonical map $\mathfrak{i}\rightarrow U\left(
\mathfrak{i}\right)  $ is injective. Thus, the canonical map
$\widetilde{\mathfrak{n}}_{+}\rightarrow U\left(  \widetilde{\mathfrak{n}}%
_{+}\right)  $ and the canonical map $\widetilde{\mathfrak{n}}_{-}\rightarrow
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ are injective. We will
therefore regard these maps as inclusions.

Let us identify the group $Q$ with $\mathbb{Z}^{n}$ by means of identifying
$\alpha_{i}$ with the column vector $e_{i}=\left(  \underbrace{0,0,...,0}%
_{i-1\text{ zeroes}},1,\underbrace{0,0,...,0}_{n-i\text{ zeroes}}\right)
^{T}$ for every $i\in\left\{  1,2,...,n\right\}  $. As a consequence, for
every $i\in\left\{  1,2,...,n\right\}  $, the row vector $e_{i}^{T}A$ is an
element of the group $\operatorname*{Hom}\left(  Q,\mathbb{C}\right)  $ of
group homomorphisms from $Q$ to $\mathbb{C}$. Thus, for every $w\in Q$ and
every $i\in\left\{  1,2,...,n\right\}  $, the product $e_{i}^{T}Aw$ is a
complex number.

\bigskip

\underline{\textit{2nd step: Defining a }$Q$\textit{-grading on }%
$\widetilde{\mathfrak{n}}_{-}$\textit{.}}

Let us define a $Q$-grading on the vector space $N_{-}$ by setting%
\[
\deg\left(  f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  .
\]
(This is well-defined since $\left(  f_{1},f_{2},...,f_{n}\right)  $ is a
basis of $N_{-}$.) Then, the free Lie algebra $\operatorname*{FreeLie}\left(
N_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$ canonically becomes a $Q$-graded
Lie algebra, and the grading on this Lie algebra also satisfies%
\[
\deg\left(  f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  .
\]
(This grading clearly makes the map $\iota_{-}$ graded. We will not use this
fact, however.) We will later use this grading to define certain Lie
derivations $\eta_{1}$, $\eta_{2}$, $...$, $\eta_{n}$ of the Lie algebra
$\widetilde{\mathfrak{n}}_{-}$.

\bigskip

\underline{\textit{3rd step: Defining an }$\widetilde{\mathfrak{h}}%
$\textit{-module }$\widetilde{\mathfrak{n}}_{-}$\textit{.}}

For every $i\in\left\{  1,2,...,n\right\}  $, let us define a linear map
$\eta_{i}:\widetilde{\mathfrak{n}}_{-}\rightarrow\widetilde{\mathfrak{n}}_{-}$
by setting%
\begin{equation}
\left(  \eta_{i}\left(  x\right)  =\left(  e_{i}^{T}Aw\right)  \cdot
x\ \ \ \ \ \ \ \ \ \ \text{for every }w\in Q\text{ and every }x\in
\widetilde{\mathfrak{n}}_{-}\left[  w\right]  \right)  .
\label{pf.gtilde.b.eta.def}%
\end{equation}
This map $\eta_{i}$ is well-defined (because in order to define a linear map
from a $Q$-graded vector space, it is enough to define it linearly on every
homogeneous component) and graded (because it multiplies any homogeneous
element of $\widetilde{\mathfrak{n}}_{-}$ by a scalar). Actually, $\eta_{i}$
acts as a scalar on each homogeneous component of $\widetilde{\mathfrak{n}%
}_{-}$. Moreover, for every $i\in\left\{  1,2,...,n\right\}  $, Lemma
\ref{lem.deriv.grading} (applied to $s=e_{i}^{T}A$, $\mathfrak{n}%
=\widetilde{\mathfrak{n}}_{-}$ and $\eta=\eta_{i}$) yields that $\eta_{i}$ is
a Lie derivation. That is, $\eta_{i}\in\operatorname*{Der}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. One can directly see that%
\begin{equation}
\eta_{i}\left(  f_{j}\right)  =-a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for any
}i\in\left\{  1,2,...,n\right\}  \text{ and }j\in\left\{  1,2,...,n\right\}
\label{pf.gtilde.b.eta.fj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gtilde.b.eta.fj}):} Let $i\in\left\{
1,2,...,n\right\}  $ and $j\in\left\{  1,2,...,n\right\}  $. By the definition
of our grading on $\widetilde{\mathfrak{n}}_{-}$, we have $\deg\left(
f_{j}\right)  =-\underbrace{\alpha_{j}}_{=e_{j}}=-e_{j}$, so that $f_{j}%
\in\widetilde{\mathfrak{n}}_{-}\left[  -e_{j}\right]  $. Hence,
(\ref{pf.gtilde.b.eta.def}) (applied to $x=f_{j}$ and $w=-\alpha_{j}$) yields
$\eta_{i}\left(  f_{j}\right)  =\left(  e_{i}^{T}A\left(  -e_{j}\right)
\right)  \cdot f_{j}=-\underbrace{\left(  e_{i}^{T}Ae_{j}\right)  }_{=a_{i,j}%
}\cdot f_{j}=-a_{i,j}f_{j}$. This proves (\ref{pf.gtilde.b.eta.fj}).}.

[Note that, while we defined the $\eta_{i}$ using the grading, there is also
an alternative way to define them, by applying Theorem
\ref{thm.universal.FreeLie.der}.]

It is easy to see that
\begin{equation}
\left[  \eta_{i},\eta_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  \text{ and }j\in\left\{  1,2,...,n\right\}
\label{pf.gtilde.b.eta.commute}%
\end{equation}
(since each of the maps $\eta_{i}$ and $\eta_{j}$ acts as a scalar on each
homogeneous component of $\widetilde{\mathfrak{n}}_{-}$).

Define a linear map $\Xi:\widetilde{\mathfrak{h}}\rightarrow
\operatorname*{Der}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  \Xi\left(  h_{i}\right)  =\eta_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}  \right)
\]
(this map is well-defined, since $\left(  h_{1},h_{2},...,h_{n}\right)  $ is a
basis of $\widetilde{\mathfrak{h}}$). Then, $\Xi$ is a Lie algebra
homomorphism (this follows from (\ref{pf.gtilde.b.eta.commute})), and thus
makes $\widetilde{\mathfrak{n}}_{-}$ into an $\widetilde{\mathfrak{h}}$-module
on which $\widetilde{\mathfrak{h}}$ acts by derivations. Thus, a Lie algebra
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$ is well-defined
(according to Definition \ref{def.semidir.lielie}). Both Lie algebras
$\widetilde{\mathfrak{h}}$ and $\widetilde{\mathfrak{n}}_{-}$ canonically
inject (by Lie algebra homomorphisms) into this Lie algebra
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$. Therefore, both
$\widetilde{\mathfrak{h}}$ and $\widetilde{\mathfrak{n}}_{-}$ will be
considered as Lie subalgebras of $\widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}$.

In the Lie algebra $\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}$, every $i\in\left\{  1,2,...,n\right\}  $ and $j\in\left\{
1,2,...,n\right\}  $ satisfy%
\begin{align}
\left[  h_{i},f_{j}\right]   &  =h_{i}\rightharpoonup f_{j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{where }\rightharpoonup\text{ denotes the
action of }\widetilde{\mathfrak{h}}\text{ on }\widetilde{\mathfrak{n}}%
_{-}\right) \nonumber\\
&  =\underbrace{\left(  \Xi\left(  h_{i}\right)  \right)  }_{=\eta_{i}}\left(
f_{j}\right)  =\eta_{i}\left(  f_{j}\right)  =-a_{i,j}f_{j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.gtilde.b.eta.fj})}\right)  .
\label{pf.gtilde.b.semidir.ij}%
\end{align}
From (\ref{nonserre-relations}), we see that the same relation is satisfied in
the Lie algebra $\widetilde{\mathfrak{g}}$.

Since $\widetilde{\mathfrak{n}}_{-}$ is a Lie subalgebra of
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$, the universal
enveloping algebra $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ is a
subalgebra of $U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $. This makes $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ into a $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $-bimodule. Since $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  =T\left(  N_{-}\right)  $, this means
that $U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  $ is a $T\left(  N_{-}\right)  $-bimodule.

\bigskip

\underline{\textit{4th step: Defining an action of }$\widetilde{\mathfrak{g}}$
\textit{on }$U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $\textit{.}}

We are going to construct an action of the Lie algebra
$\widetilde{\mathfrak{g}}$ on $U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ (but not by derivations). First,
let us define some further maps.

Let $\iota_{N_{-}}^{T}:N_{-}\rightarrow T\left(  N_{-}\right)  $ be the
canonical inclusion map. Notice that we are regarding $\iota_{N_{-}}^{T}$ as
an inclusion.

\begin{vershort}
For every $i\in\left\{  1,2,...,n\right\}  $, let us define a
derivation\footnote{Here, by ``derivation'', we mean a derivation of algebras,
not of Lie algebras.} $\varepsilon_{i}:U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by requiring that%
\[
\left(  \varepsilon_{i}\left(  f_{j}\right)  =\delta_{i,j}h_{i}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,...,n\right\}  \right)
.
\]
\footnote{Why is this well-defined? We know that $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  =T\left(  N_{-}\right)  $. Hence, (by
Theorem \ref{thm.universal.tensor.der}, applied to $V=N_{-}$ and $M=U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $) we can
lift any linear map $f:N_{-}\rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ to a derivation $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $. Taking
$f$ equal to the linear map $N_{-}\rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ which sends every $f_{j}$ to
$\delta_{i,j}h_{i}$, we obtain a derivation $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ which sends every $f_{j}$ to
$\delta_{i,j}h_{i}$. This is why $\varepsilon_{i}$ is well-defined.}
\end{vershort}

\begin{verlong}
For every $i\in\left\{  1,2,...,n\right\}  $, let $\varepsilon_{i}^{\prime
}:N_{-}\rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ be the linear map defined by%
\[
\left(  \varepsilon_{i}^{\prime}\left(  f_{j}\right)  =\delta_{i,j}%
h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,...,n\right\}
\right)
\]
(this map is well-defined, since $\left(  f_{1},f_{2},...,f_{n}\right)  $ is a
basis of $N_{-}$).

For every $i\in\left\{  1,2,...,n\right\}  $, there exists a unique
derivation\footnote{Here, by ``derivation'', we mean a derivation of algebras,
not of Lie algebras.} $F:T\left(  N_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $
satisfying $\varepsilon_{i}^{\prime}=F\circ\iota_{N_{-}}^{T}$ (according to
Theorem \ref{thm.universal.tensor.der}, applied to $V=N_{-}$, $M=U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ and
$f=\varepsilon_{i}^{\prime}$). Denote this derivation by $\varepsilon_{i}$.
Thus, for every $i\in\left\{  1,2,...,n\right\}  $, the map $\varepsilon
_{i}:T\left(  N_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ is a derivation satisfying
$\varepsilon_{i}^{\prime}=\varepsilon_{i}\circ\iota_{N_{-}}^{T}$. Since
$T\left(  N_{-}\right)  =U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $, this
map $\varepsilon_{i}$ is thus a derivation $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $. Clearly, every $i\in\left\{
1,2,...,n\right\}  $ and $j\in\left\{  1,2,...,n\right\}  $ satisfy%
\begin{equation}
\varepsilon_{i}\left(  f_{j}\right)  =\delta_{i,j}h_{i}
\label{pf.gtilde.b.epsilon.fj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gtilde.b.epsilon.fj}):} Let $i\in\left\{
1,2,...,n\right\}  $ and $j\in\left\{  1,2,...,n\right\}  $. Then,
$\iota_{N_{-}}^{T}\left(  f_{j}\right)  =f_{j}$ (since we regard $\iota
_{N_{-}}^{T}$ as an inclusion). But since $\varepsilon_{i}^{\prime
}=\varepsilon_{i}\circ\iota_{N_{-}}^{T}$, we have $\varepsilon_{i}^{\prime
}\left(  f_{j}\right)  =\left(  \varepsilon_{i}\circ\iota_{N_{-}}^{T}\right)
\left(  f_{j}\right)  =\varepsilon_{i}\left(  \underbrace{\iota_{N_{-}}%
^{T}\left(  f_{j}\right)  }_{=f_{j}}\right)  =\varepsilon_{i}\left(
f_{j}\right)  $. Thus, $\varepsilon_{i}\left(  f_{j}\right)  =\varepsilon
_{i}^{\prime}\left(  f_{j}\right)  =\delta_{i,j}h_{i}$. This proves
(\ref{pf.gtilde.b.epsilon.fj}).}.
\end{verlong}

Let $\rho:U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(
\widetilde{\mathfrak{h}}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ be the vector space
homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \text{ and }\beta\in U\left(  \widetilde{\mathfrak{h}}\right)
\]
(this is clearly well-defined). Since $\widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}=\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}$ as vector spaces, Corollary \ref{cor.U(X)U} (applied
to $k=\mathbb{C}$, $\mathfrak{c}=\widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}$, $\mathfrak{a}=\widetilde{\mathfrak{n}}_{-}$ and
$\mathfrak{b}=\widetilde{\mathfrak{h}}$) yields that $\rho$ is an isomorphism
of filtered vector spaces, of left $U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $-modules and of right $U\left(  \widetilde{\mathfrak{h}}\right)
$-modules. Thus, $\rho^{-1}$ also is an isomorphism of filtered vector spaces,
of left $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $-modules and of right
$U\left(  \widetilde{\mathfrak{h}}\right)  $-modules.

\begin{vershort}
For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$E_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\begin{equation}
\left(  E_{i}\left(  u_{-}u_{0}\right)  =\varepsilon_{i}\left(  u_{-}\right)
u_{0}\ \ \ \ \ \ \ \ \ \ \text{for every }u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  \right)  . \label{pf.gtilde.b.Ei.short}%
\end{equation}
\footnote{Why is this well-defined? Since $\rho$ is an isomorphism, we have
$U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)
\cong U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(
\widetilde{\mathfrak{h}}\right)  $. In order to define a linear map $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(  \widetilde{\mathfrak{h}%
}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $, we just need to take a bilinear map
$U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \times U\left(
\widetilde{\mathfrak{h}}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ and apply the universal
property of the tensor product. Taking%
\[
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \times U\left(
\widetilde{\mathfrak{h}}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  ,\ \ \ \ \ \ \ \ \ \ \left(
u_{-},u_{0}\right)  \mapsto\varepsilon_{i}\left(  u_{-}\right)  u_{0}%
\]
as this bilinear map, we obtain (by the universal property) a linear map
$U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(
\widetilde{\mathfrak{h}}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ which sends $u_{-}\otimes
u_{0}$ to $\varepsilon_{i}\left(  u_{-}\right)  u_{0}$ for every $u_{-}\in
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  $. Composing this map with the isomorphism
$\rho^{-1}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\otimes U\left(  \widetilde{\mathfrak{h}}\right)  $, we obtain a linear map
$U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)
\rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $ which sends $u_{-}u_{0}$ to $\varepsilon_{i}\left(
u_{-}\right)  u_{0}$ for every $u_{-}\in U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $ and $u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $.
Therefore, $E_{i}$ is well-defined.}
\end{vershort}

\begin{verlong}
Let $\mu:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \otimes U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ be the
multiplication map of the algebra $U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $. We consider $U\left(
\widetilde{\mathfrak{h}}\right)  $ as a subalgebra of $U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ (since
$\widetilde{\mathfrak{h}}$ is a Lie subalgebra of $\widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}$).

For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$E_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by $E_{i}=\mu\circ\left(
\varepsilon_{i}\otimes\operatorname*{id}\right)  \circ\rho^{-1}$. Then,
$E_{i}$ is a right $U\left(  \widetilde{\mathfrak{h}}\right)  $-module
homomorphism (because all of $\mu$, $\varepsilon_{i}\otimes\operatorname*{id}$
and $\rho^{-1}$ are right $U\left(  \widetilde{\mathfrak{h}}\right)  $-module
homomorphisms). Also, every $u_{-}\in U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $ and $u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $
satisfy%
\begin{equation}
E_{i}\left(  u_{-}u_{0}\right)  =\varepsilon_{i}\left(  u_{-}\right)  u_{0}
\label{pf.gtilde.b.Ei}%
\end{equation}
\footnote{\textit{Proof.} Let $u_{-}\in U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $ and $u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $.
Since $E_{i}=\mu\circ\left(  \varepsilon_{i}\otimes\operatorname*{id}\right)
\circ\rho^{-1}$, we have
\begin{align*}
E_{i}\left(  u_{-}u_{0}\right)   &  =\left(  \mu\circ\left(  \varepsilon
_{i}\otimes\operatorname*{id}\right)  \circ\rho^{-1}\right)  \left(
u_{-}u_{0}\right)  =\left(  \mu\circ\left(  \varepsilon_{i}\otimes
\operatorname*{id}\right)  \right)  \left(  \underbrace{\rho^{-1}\left(
u_{-}u_{0}\right)  }_{\substack{=u_{-}\otimes u_{0}\\\text{(since the
definition of }\rho\text{ yields}\\\rho\left(  u_{-}\otimes u_{0}\right)
=u_{-}u_{0}\text{)}}}\right) \\
&  =\left(  \mu\circ\left(  \varepsilon_{i}\otimes\operatorname*{id}\right)
\right)  \left(  u_{-}\otimes u_{0}\right)  =\mu\left(  \underbrace{\left(
\varepsilon_{i}\otimes\operatorname*{id}\right)  \left(  u_{-}\otimes
u_{0}\right)  }_{=\varepsilon_{i}\left(  u_{-}\right)  \otimes u_{0}}\right)
=\mu\left(  \varepsilon_{i}\left(  u_{-}\right)  \otimes u_{0}\right)
=\varepsilon_{i}\left(  u_{-}\right)  u_{0}%
\end{align*}
(since $\mu$ is the multiplication map), qed.}.
\end{verlong}

For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$F_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  F_{i}\left(  u\right)  =f_{i}u\ \ \ \ \ \ \ \ \ \ \text{for every
}u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  .
\]
Clearly, $F_{i}$ is a right $U\left(  \widetilde{\mathfrak{h}}\right)
$-module homomorphism.

For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$H_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  H_{i}\left(  u\right)  =h_{i}u\ \ \ \ \ \ \ \ \ \ \text{for every
}u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  .
\]
Clearly, $H_{i}$ is a right $U\left(  \widetilde{\mathfrak{h}}\right)
$-module homomorphism.

Our next goal is to prove the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  H_{i},H_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  E_{i},F_{j}\right]  =\delta_{i,j}H_{i}\ \ \ \ \ \ \ \ \ \ \text{for
all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{pf.gtilde.NONSERRE}%
\end{equation}
in $\operatorname*{End}\left(  U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $. Once these relations
are proven, it will follow that a Lie algebra homomorphism
$\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}\left(  U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $
mapping $h_{i}$, $e_{i}$, $f_{i}$ to $H_{i}$, $E_{i}$, $F_{i}$ for all $i$
exists (and is unique), and this map will make $U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ into a
$\widetilde{\mathfrak{g}}$-module. This $\widetilde{\mathfrak{g}}$-module
structure will then yield Theorem \ref{thm.gtilde} \textbf{(b)} by a rather
simple argument. But we must first verify (\ref{pf.gtilde.NONSERRE}).

\bigskip

\underline{\textit{5th step: Verifying the relations (\ref{pf.gtilde.NONSERRE}%
).}}

We will verify the four relations (\ref{pf.gtilde.NONSERRE}) one after the other:

\bigskip

\textit{Proof of the relation }$\left[  H_{i},H_{j}\right]  =0$ \textit{for
all } $i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Every $u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $
satisfies%
\begin{align*}
\underbrace{\left[  H_{i},H_{j}\right]  }_{=H_{i}\circ H_{j}-H_{j}\circ H_{i}%
}u  &  =\left(  H_{i}\circ H_{j}-H_{j}\circ H_{i}\right)  \left(  u\right)
=H_{i}\underbrace{\left(  H_{j}u\right)  }_{\substack{=h_{j}u\\\text{(by the
definition}\\\text{of }H_{j}\text{)}}}-H_{j}\underbrace{\left(  H_{i}u\right)
}_{\substack{=h_{i}u\\\text{(by the definition}\\\text{of }H_{i}\text{)}}}\\
&  =\underbrace{H_{i}\left(  h_{j}u\right)  }_{\substack{=h_{i}\left(
h_{j}u\right)  \\\text{(by the definition}\\\text{of }H_{i}\text{)}%
}}-\underbrace{H_{j}\left(  h_{i}u\right)  }_{\substack{=h_{j}\left(
h_{i}u\right)  \\\text{(by the definition}\\\text{of }H_{j}\text{)}}}\\
&  =h_{i}\left(  h_{j}u\right)  -h_{j}\left(  h_{i}u\right)
=\underbrace{\left(  h_{i}h_{j}-h_{j}h_{i}\right)  }_{\substack{=\left[
h_{i},h_{j}\right]  =0\\\text{in }U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \\\text{(since }\left[
h_{i},h_{j}\right]  =0\text{ in }\widetilde{\mathfrak{h}}\text{)}}}u=0.
\end{align*}
Thus, $\left[  H_{i},H_{j}\right]  =0$.

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  H_{i},H_{j}\right]  =0$ for all $i,j\in\left\{  1,2,...,n\right\}  $.

\bigskip

\textit{Proof of the relation }$\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$
\textit{for all } $i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

This will be the most difficult among the four relations that we must prove.

Applying Corollary \ref{cor.derivation.Lie.semidir} to
$\widetilde{\mathfrak{h}}$ and $\widetilde{\mathfrak{n}}_{-}$ instead of
$\mathfrak{g}$ and $\mathfrak{h}$, we obtain $\left[  \widetilde{\mathfrak{h}%
},U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \right]  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ in $U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $.

Let us consider $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ as
$\widetilde{\mathfrak{n}}_{-}$-module via the adjoint action. Then,
$\widetilde{\mathfrak{n}}_{-}\subseteq U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $ as $\widetilde{\mathfrak{n}}_{-}$-modules.

Let $i$ be any element of $\left\{  1,2,...,n\right\}  $. Define a map
$\zeta_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  \zeta_{i}\left(  u\right)  =\left[  h_{i},u\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  .
\]
Clearly, $\zeta_{i}$ is a derivation of algebras.

Now, using the relation $\left[  \widetilde{\mathfrak{h}},U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right]  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $, it is easy to check that $\zeta
_{i}\left(  U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \right)  \subseteq
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Every $u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ satisfies%
\[
\zeta_{i}\left(  u\right)  =\left[  \underbrace{h_{i}}_{\in
\widetilde{\mathfrak{h}}},\underbrace{u}_{\in U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  }\right]  \in\left[  \widetilde{\mathfrak{h}},U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right]  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  .
\]
In other words, $\zeta_{i}\left(  U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)
$, qed.}.

Now, let $j\in\left\{  1,2,...,n\right\}  $ be arbitrary. Recall that
$\zeta_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ and $\varepsilon_{j}:U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ are
derivations satisfying $\zeta_{i}\left(  U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)
$. Thus, Proposition \ref{prop.commutator.derivs.alg.2} (applied to $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $, $U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $, $\varepsilon_{j}$ and
$\zeta_{i}$ instead of $A$, $B$, $f$ and $g$) yields that $\varepsilon
_{j}\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)
}\right)  -\zeta_{i}\circ\varepsilon_{j}:U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is a derivation (of algebras).

On the other hand, $-a_{i,j}\varepsilon_{j}:U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is also a derivation (of algebras),
since $\varepsilon_{j}$ is a derivation.

We will now prove that%
\begin{equation}
\varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon_{j}=-a_{i,j}\varepsilon
_{j}. \label{pf.gtilde.dercomm1}%
\end{equation}
\footnote{Note that the term $\varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  $ in this
equality is well-defined because $\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  \left(  U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right)  =\zeta_{i}\left(  U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right)  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $.} This will bring us very close to the
proof of the relation $\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$.

\begin{vershort}
\textit{Proof of (\ref{pf.gtilde.dercomm1}):} Every $k\in\left\{
1,2,...,n\right\}  $ satisfies%
\begin{align*}
&  \left(  \left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}\right)  \left(  f_{k}\right) \\
&  =\varepsilon_{j}\left(  \underbrace{\zeta_{i}\left(  f_{k}\right)
}_{\substack{=\left[  h_{i},f_{k}\right]  \\\text{(by the definition of }%
\zeta_{i}\text{)}}}\right)  -\underbrace{\zeta_{i}\left(  \varepsilon
_{j}\left(  f_{k}\right)  \right)  }_{\substack{=\left[  h_{i},\varepsilon
_{j}\left(  f_{k}\right)  \right]  \\\text{(by the definition of }\zeta
_{i}\text{)}}}\\
&  =\varepsilon_{j}\left(  \underbrace{\left[  h_{i},f_{k}\right]
}_{\substack{=-a_{i,k}f_{k}\\\text{(by (\ref{pf.gtilde.b.semidir.ij}), applied
to}\\k\text{ instead of }j\text{)}}}\right)  -\left[  h_{i}%
,\underbrace{\varepsilon_{j}\left(  f_{k}\right)  }_{\substack{=\delta
_{j,k}h_{j}\\\text{(by the definition of }\varepsilon_{j}\text{)}}}\right] \\
&  =\underbrace{\varepsilon_{j}\left(  -a_{i,k}f_{k}\right)  }_{=-a_{i,k}%
\varepsilon_{j}\left(  f_{k}\right)  }-\underbrace{\left[  h_{i},\delta
_{j,k}h_{j}\right]  }_{\substack{=0\\\text{(since }\widetilde{\mathfrak{h}%
}\text{ is an abelian Lie algebra)}}}=-a_{i,k}\underbrace{\varepsilon
_{j}\left(  f_{k}\right)  }_{\substack{=\delta_{j,k}h_{j}\\\text{(by the
definition of }\varepsilon_{j}\text{)}}}\\
&  =-\underbrace{a_{i,k}\delta_{j,k}}_{=a_{i,j}\delta_{j,k}}h_{j}%
=-a_{i,j}\underbrace{\delta_{j,k}h_{j}}_{\substack{=\varepsilon_{j}\left(
f_{k}\right)  \\\text{(by the definition of }\varepsilon_{j}\text{)}%
}}=-a_{i,j}\varepsilon_{j}\left(  f_{k}\right)  =\left(  \left(
-a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}}\right)  \left(  f_{k}\right)  .
\end{align*}
In other words, the maps $\left(  \varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}%
\circ\varepsilon_{j}\right)  \mid_{N_{-}}$ and $\left(  -a_{i,j}%
\varepsilon_{j}\right)  \mid_{N_{-}}$ are equal to each other on each of the
elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of $N_{-}$. Since $\left(
f_{1},f_{2},...,f_{n}\right)  $ is a basis of $N_{-}$, this yields that
\[
\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}=\left(  -a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}}%
\]
(because the maps $\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}$ and $\left(  -a_{i,j}\varepsilon_{j}\right)
\mid_{N_{-}}$ are linear). Hence, since the set $N_{-}$ generates $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ as an algebra (because $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  =T\left(  N_{-}\right)  $), Proposition
\ref{prop.derivation.unique} (applied to $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, $N_{-}$, $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $, $\varepsilon_{j}\circ\left(  \zeta
_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta
_{i}\circ\varepsilon_{j}$ and $-a_{i,j}\varepsilon_{j}$ instead of $A$, $S$,
$M$, $d$ and $e$) yields that $\varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}%
\circ\varepsilon_{j}=-a_{i,j}\varepsilon_{j}$ (since $\varepsilon_{j}%
\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)
}\right)  -\zeta_{i}\circ\varepsilon_{j}$ and $-a_{i,j}\varepsilon_{j}$ are
derivations). This proves (\ref{pf.gtilde.dercomm1}).
\end{vershort}

\begin{verlong}
\textit{Proof of (\ref{pf.gtilde.dercomm1}):} Every $k\in\left\{
1,2,...,n\right\}  $ satisfies%
\begin{align*}
&  \left(  \left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}\right)  \left(  f_{k}\right) \\
&  =\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \left(  f_{k}\right) \\
&  =\varepsilon_{j}\left(  \underbrace{\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  \left(  f_{k}\right)
}_{\substack{=\zeta_{i}\left(  f_{k}\right)  =\left[  h_{i},f_{k}\right]
\\\text{(by the definition of }\zeta_{i}\text{)}}}\right)  -\underbrace{\zeta
_{i}\left(  \varepsilon_{j}\left(  f_{k}\right)  \right)  }%
_{\substack{=\left[  h_{i},\varepsilon_{j}\left(  f_{k}\right)  \right]
\\\text{(by the definition of }\zeta_{i}\text{)}}}\\
&  =\varepsilon_{j}\left(  \underbrace{\left[  h_{i},f_{k}\right]
}_{\substack{=-a_{i,k}f_{k}\\\text{(by (\ref{pf.gtilde.b.semidir.ij}), applied
to}\\k\text{ instead of }j\text{)}}}\right)  -\left[  h_{i}%
,\underbrace{\varepsilon_{j}\left(  f_{k}\right)  }_{\substack{=\delta
_{j,k}h_{j}\\\text{(by (\ref{pf.gtilde.b.epsilon.fj}), applied to}\\j\text{
and }k\text{ instead of }i\text{ and }j\text{)}}}\right] \\
&  =\underbrace{\varepsilon_{j}\left(  -a_{i,k}f_{k}\right)  }_{=-a_{i,k}%
\varepsilon_{j}\left(  f_{k}\right)  }-\underbrace{\left[  h_{i},\delta
_{j,k}h_{j}\right]  }_{\substack{=0\\\text{(since }\widetilde{\mathfrak{h}%
}\text{ is an abelian Lie algebra)}}}=-a_{i,k}\underbrace{\varepsilon
_{j}\left(  f_{k}\right)  }_{\substack{=\delta_{j,k}h_{j}\\\text{(by
(\ref{pf.gtilde.b.epsilon.fj}), applied to}\\j\text{ and }k\text{ instead of
}i\text{ and }j\text{)}}}\\
&  =-a_{i,k}\underbrace{\delta_{j,k}}_{=\left\{
\begin{array}
[c]{c}%
1,\text{ if }j=k\\
0,\text{ if }j\neq k
\end{array}
\right.  }h_{j}=-\underbrace{a_{i,k}\left\{
\begin{array}
[c]{c}%
1,\text{ if }j=k\\
0,\text{ if }j\neq k
\end{array}
\right.  }_{=\left\{
\begin{array}
[c]{c}%
a_{i,k}\cdot1,\text{ if }j=k\\
a_{i,k}\cdot0,\text{ if }j\neq k
\end{array}
\right.  }h_{j}\\
&  =-\left\{
\begin{array}
[c]{c}%
a_{i,k}\cdot1,\text{ if }j=k\\
a_{i,k}\cdot0,\text{ if }j\neq k
\end{array}
\right.  h_{j}=-\left\{
\begin{array}
[c]{c}%
a_{i,j}\cdot1,\text{ if }j=k\\
a_{i,k}\cdot0,\text{ if }j\neq k
\end{array}
\right.  h_{j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{i,k}=a_{i,j}\text{ if
}j=k\text{ (because }k=j\text{ if }j=k\text{)}\right) \\
&  =-\underbrace{\left\{
\begin{array}
[c]{c}%
a_{i,j}\cdot1,\text{ if }j=k\\
a_{i,j}\cdot0,\text{ if }j\neq k
\end{array}
\right.  }_{=a_{i,j}\cdot\left\{
\begin{array}
[c]{c}%
1,\text{ if }j=k\\
0,\text{ if }j\neq k
\end{array}
\right.  }h_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{i,k}\cdot
0=a_{i,j}\cdot0\text{ if }j\neq k\right) \\
&  =-a_{i,j}\cdot\underbrace{\left\{
\begin{array}
[c]{c}%
1,\text{ if }j=k\\
0,\text{ if }j\neq k
\end{array}
\right.  }_{=\delta_{j,k}}h_{j}=-a_{i,j}\delta_{j,k}h_{j}%
\end{align*}
and%
\[
\left(  \left(  -a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}}\right)  \left(
f_{k}\right)  =\left(  -a_{i,j}\varepsilon_{j}\right)  \left(  f_{k}\right)
=-a_{i,j}\underbrace{\varepsilon_{j}\left(  f_{k}\right)  }_{\substack{=\delta
_{j,k}h_{j}\\\text{(by (\ref{pf.gtilde.b.epsilon.fj}), applied to}\\j\text{
and }k\text{ instead of }i\text{ and }j\text{)}}}=-a_{i,j}\delta_{j,k}h_{j}.
\]
Hence, every $k\in\left\{  1,2,...,n\right\}  $ satisfies%
\[
\left(  \left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}\right)  \left(  f_{k}\right)  =-a_{i,j}\delta
_{j,k}h_{j}=\left(  \left(  -a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}%
}\right)  \left(  f_{k}\right)  .
\]
In other words, the maps $\left(  \varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}%
\circ\varepsilon_{j}\right)  \mid_{N_{-}}$ and $\left(  -a_{i,j}%
\varepsilon_{j}\right)  \mid_{N_{-}}$ are equal to each other on each of the
elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of $N_{-}$. Since $\left(
f_{1},f_{2},...,f_{n}\right)  $ is a basis of $N_{-}$, this yields that
\[
\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}=\left(  -a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}}%
\]
(because the maps $\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}$ and $\left(  -a_{i,j}\varepsilon_{j}\right)
\mid_{N_{-}}$ are linear). Hence, since the set $N_{-}$ generates $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ as an algebra (because $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  =T\left(  N_{-}\right)  $), Proposition
\ref{prop.derivation.unique} (applied to $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, $N_{-}$, $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $, $\varepsilon_{j}\circ\left(  \zeta
_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta
_{i}\circ\varepsilon_{j}$ and $-a_{i,j}\varepsilon_{j}$ instead of $A$, $S$,
$M$, $d$ and $e$) yields that $\varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}%
\circ\varepsilon_{j}=-a_{i,j}\varepsilon_{j}$ (since $\varepsilon_{j}%
\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)
}\right)  -\zeta_{i}\circ\varepsilon_{j}$ and $-a_{i,j}\varepsilon_{j}$ are
derivations). This proves (\ref{pf.gtilde.dercomm1}).
\end{verlong}

Now, we will show that
\begin{equation}
\left[  h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  =a_{i,j}\varepsilon
_{j}\left(  u_{-}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }u_{-}\in
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  . \label{pf.gtilde.dercomm2}%
\end{equation}


\textit{Proof of (\ref{pf.gtilde.dercomm2}):} Let $u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. Then,%
\begin{align*}
\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \left(  u_{-}\right)   &  =\varepsilon_{j}\left(
\underbrace{\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  }\right)  \left(  u_{-}\right)  }_{\substack{=\zeta_{i}\left(
u_{-}\right)  =\left[  h_{i},u_{-}\right]  \\\text{(by the definition of
}\zeta_{i}\text{)}}}\right)  -\underbrace{\zeta_{i}\left(  \varepsilon
_{j}\left(  u_{-}\right)  \right)  }_{\substack{=\left[  h_{i},\varepsilon
_{j}\left(  u_{-}\right)  \right]  \\\text{(by the definition of }\zeta
_{i}\text{)}}}\\
&  =\varepsilon_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  -\left[
h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  .
\end{align*}
Comparing this with%
\[
\underbrace{\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  }_{\substack{=-a_{i,j}\varepsilon_{j}\\\text{(by
(\ref{pf.gtilde.dercomm1}))}}}\left(  u_{-}\right)  =-a_{i,j}\varepsilon
_{j}\left(  u_{-}\right)  ,
\]
we obtain $-a_{i,j}\varepsilon_{j}\left(  u_{-}\right)  =\varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  -\left[  h_{i},\varepsilon
_{j}\left(  u_{-}\right)  \right]  $. In other words, $\left[  h_{i}%
,\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  =a_{i,j}\varepsilon_{j}\left(  u_{-}\right)  $.
This proves (\ref{pf.gtilde.dercomm2}).

Now, let us finally prove that $\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$.

\begin{vershort}
Indeed, let $u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and
$u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $. Then, $\left[
\underbrace{h_{i}}_{\in\mathfrak{h}},\underbrace{u_{-}}_{\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right]  \in\left[
\widetilde{\mathfrak{h}},U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\right]  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $. Thus,
(\ref{pf.gtilde.b.Ei.short}) (applied to $\left[  h_{i},u_{-}\right]  $ and
$j$ instead of $u_{-}$ and $i$) yields%
\[
E_{j}\left(  \left[  h_{i},u_{-}\right]  u_{0}\right)  =\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  u_{0}.
\]
On the other hand, $\underbrace{h_{i}}_{\in\widetilde{\mathfrak{h}}%
}\underbrace{u_{0}}_{\in U\left(  \widetilde{\mathfrak{h}}\right)  }%
\in\widetilde{\mathfrak{h}}U\left(  \widetilde{\mathfrak{h}}\right)  \subseteq
U\left(  \widetilde{\mathfrak{h}}\right)  $. Hence,
(\ref{pf.gtilde.b.Ei.short}) (applied to $h_{i}u_{0}$ and $j$ instead of
$u_{0}$ and $i$) yields%
\[
E_{j}\left(  u_{-}h_{i}u_{0}\right)  =\varepsilon_{j}\left(  u_{-}\right)
h_{i}u_{0}.
\]
But $\underbrace{h_{i}u_{-}}_{=\left[  h_{i},u_{-}\right]  +u_{-}h_{i}}%
u_{0}=\left[  h_{i},u_{-}\right]  u_{0}+u_{-}h_{i}u_{0}$, so that
\begin{align*}
E_{j}\left(  h_{i}u_{-}u_{0}\right)   &  =E_{j}\left(  \left[  h_{i}%
,u_{-}\right]  u_{0}+u_{-}h_{i}u_{0}\right)  =\underbrace{E_{j}\left(  \left[
h_{i},u_{-}\right]  u_{0}\right)  }_{=\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  u_{0}}+\underbrace{E_{j}\left(  u_{-}h_{i}%
u_{0}\right)  }_{=\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}}\\
&  =\varepsilon_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  u_{0}%
+\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}.
\end{align*}


On the other hand,%
\begin{align*}
&  \underbrace{\left[  H_{i},E_{j}\right]  }_{=H_{i}\circ E_{j}-E_{j}\circ
H_{i}}\left(  u_{-}u_{0}\right)  =\left(  H_{i}\circ E_{j}-E_{j}\circ
H_{i}\right)  \left(  u_{-}u_{0}\right) \\
&  =H_{i}\left(  \underbrace{E_{j}\left(  u_{-}u_{0}\right)  }%
_{\substack{=\varepsilon_{j}\left(  u_{-}\right)  u_{0}\\\text{(by
(\ref{pf.gtilde.b.Ei.short}), applied}\\\text{to }j\text{ instead of
}i\text{)}}}\right)  -E_{j}\left(  \underbrace{H_{i}\left(  u_{-}u_{0}\right)
}_{\substack{=h_{i}u_{-}u_{0}\\\text{(by the definition of }H_{i}\text{)}%
}}\right) \\
&  =\underbrace{H_{i}\left(  \varepsilon_{j}\left(  u_{-}\right)
u_{0}\right)  }_{\substack{=h_{i}\varepsilon_{j}\left(  u_{-}\right)
u_{0}\\\text{(by the definition of }H_{i}\text{)}}}-\underbrace{E_{j}\left(
h_{i}u_{-}u_{0}\right)  }_{\substack{=\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  u_{0}+\varepsilon_{j}\left(  u_{-}\right)
h_{i}u_{0}\\\text{(as we saw above)}}}\\
&  =h_{i}\varepsilon_{j}\left(  u_{-}\right)  u_{0}-\left(  \varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  u_{0}+\varepsilon_{j}\left(
u_{-}\right)  h_{i}u_{0}\right)  =h_{i}\varepsilon_{j}\left(  u_{-}\right)
u_{0}-\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}-\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  u_{0}\\
&  =\left(  \underbrace{h_{i}\varepsilon_{j}\left(  u_{-}\right)
-\varepsilon_{j}\left(  u_{-}\right)  h_{i}}_{=\left[  h_{i},\varepsilon
_{j}\left(  u_{-}\right)  \right]  }-\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  \right)  u_{0}=\underbrace{\left(  \left[
h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  \right)  }_{\substack{=a_{i,j}%
\varepsilon_{j}\left(  u_{-}\right)  \\\text{(by (\ref{pf.gtilde.dercomm2}))}%
}}u_{0}\\
&  =a_{i,j}\underbrace{\varepsilon_{j}\left(  u_{-}\right)  u_{0}%
}_{\substack{=E_{j}\left(  u_{-}u_{0}\right)  \\\text{(since
(\ref{pf.gtilde.b.Ei.short}) (applied to }j\text{ instead of }i\text{)}%
\\\text{yields }E_{j}\left(  u_{-}u_{0}\right)  =\varepsilon_{j}\left(
u_{-}\right)  u_{0}\text{)}}}=a_{i,j}E_{j}\left(  u_{-}u_{0}\right)  .
\end{align*}


Now, forget that we fixed $u_{-}$ and $u_{0}$. We thus have proven that%
\[
\left[  H_{i},E_{j}\right]  \left(  u_{-}u_{0}\right)  =\left(  a_{i,j}%
E_{j}\right)  \left(  u_{-}u_{0}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in
U\left(  \widetilde{\mathfrak{h}}\right)  .
\]
Since the vector space $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is generated by products $u_{-}u_{0}$
with $u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in
U\left(  \widetilde{\mathfrak{h}}\right)  $ (this is because $\rho:U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(  \widetilde{\mathfrak{h}%
}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is an isomorphism), this yields that
$\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$.
\end{vershort}

\begin{verlong}
Indeed, let $u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and
$u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $. Then, $\left[
\underbrace{h_{i}}_{\in\mathfrak{h}},\underbrace{u_{-}}_{\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right]  \in\left[
\widetilde{\mathfrak{h}},U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\right]  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $. Thus,
(\ref{pf.gtilde.b.Ei}) (applied to $\left[  h_{i},u_{-}\right]  $ and $j$
instead of $u_{-}$ and $i$) yields%
\begin{equation}
E_{j}\left(  \left[  h_{i},u_{-}\right]  u_{0}\right)  =\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  u_{0}. \label{pf.gtilde.dercomm3}%
\end{equation}
On the other hand, $\underbrace{h_{i}}_{\in\widetilde{\mathfrak{h}}%
}\underbrace{u_{0}}_{\in U\left(  \widetilde{\mathfrak{h}}\right)  }%
\in\widetilde{\mathfrak{h}}U\left(  \widetilde{\mathfrak{h}}\right)  \subseteq
U\left(  \widetilde{\mathfrak{h}}\right)  $. Hence, (\ref{pf.gtilde.b.Ei})
(applied to $h_{i}u_{0}$ and $j$ instead of $u_{0}$ and $i$) yields%
\begin{equation}
E_{j}\left(  u_{-}h_{i}u_{0}\right)  =\varepsilon_{j}\left(  u_{-}\right)
h_{i}u_{0}. \label{pf.gtilde.dercomm4}%
\end{equation}
But $\underbrace{h_{i}u_{-}}_{=\left[  h_{i},u_{-}\right]  +u_{-}h_{i}}%
u_{0}=\left[  h_{i},u_{-}\right]  u_{0}+u_{-}h_{i}u_{0}$, so that
\begin{align}
E_{j}\left(  h_{i}u_{-}u_{0}\right)   &  =E_{j}\left(  \left[  h_{i}%
,u_{-}\right]  u_{0}+u_{-}h_{i}u_{0}\right)  =\underbrace{E_{j}\left(  \left[
h_{i},u_{-}\right]  u_{0}\right)  }_{\substack{=\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  u_{0}\\\text{(by (\ref{pf.gtilde.dercomm3}))}%
}}+\underbrace{E_{j}\left(  u_{-}h_{i}u_{0}\right)  }_{\substack{=\varepsilon
_{j}\left(  u_{-}\right)  h_{i}u_{0}\\\text{(by (\ref{pf.gtilde.dercomm4}))}%
}}\nonumber\\
&  =\varepsilon_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  u_{0}%
+\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}. \label{pf.gtilde.dercomm5}%
\end{align}


On the other hand, $\rho\left(  u_{-}\otimes u_{0}\right)  =u_{-}u_{0}$ (by
the definition of $\rho$), and%
\begin{align*}
&  \left(  \left[  H_{i},E_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right) \\
&  =\underbrace{\left[  H_{i},E_{j}\right]  }_{=H_{i}\circ E_{j}-E_{j}\circ
H_{i}}\left(  \underbrace{\rho\left(  u_{-}\otimes u_{0}\right)  }%
_{=u_{-}u_{0}}\right)  =\left(  H_{i}\circ E_{j}-E_{j}\circ H_{i}\right)
\left(  u_{-}u_{0}\right) \\
&  =H_{i}\left(  \underbrace{E_{j}\left(  u_{-}u_{0}\right)  }%
_{\substack{=\varepsilon_{j}\left(  u_{-}\right)  u_{0}\\\text{(by
(\ref{pf.gtilde.b.Ei}), applied}\\\text{to }j\text{ instead of }i\text{)}%
}}\right)  -E_{j}\left(  \underbrace{H_{i}\left(  u_{-}u_{0}\right)
}_{\substack{=h_{i}u_{-}u_{0}\\\text{(by the definition of }H_{i}\text{)}%
}}\right) \\
&  =\underbrace{H_{i}\left(  \varepsilon_{j}\left(  u_{-}\right)
u_{0}\right)  }_{\substack{=h_{i}\varepsilon_{j}\left(  u_{-}\right)
u_{0}\\\text{(by the definition of }H_{i}\text{)}}}-\underbrace{E_{j}\left(
h_{i}u_{-}u_{0}\right)  }_{\substack{=\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  u_{0}+\varepsilon_{j}\left(  u_{-}\right)
h_{i}u_{0}\\\text{(by (\ref{pf.gtilde.dercomm5}))}}}\\
&  =h_{i}\varepsilon_{j}\left(  u_{-}\right)  u_{0}-\left(  \varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  u_{0}+\varepsilon_{j}\left(
u_{-}\right)  h_{i}u_{0}\right)  =h_{i}\varepsilon_{j}\left(  u_{-}\right)
u_{0}-\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}-\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  u_{0}\\
&  =\left(  \underbrace{h_{i}\varepsilon_{j}\left(  u_{-}\right)
-\varepsilon_{j}\left(  u_{-}\right)  h_{i}}_{=\left[  h_{i},\varepsilon
_{j}\left(  u_{-}\right)  \right]  }-\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  \right)  u_{0}=\underbrace{\left(  \left[
h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  \right)  }_{\substack{=a_{i,j}%
\varepsilon_{j}\left(  u_{-}\right)  \\\text{(by (\ref{pf.gtilde.dercomm2}))}%
}}u_{0}\\
&  =a_{i,j}\underbrace{\varepsilon_{j}\left(  u_{-}\right)  u_{0}%
}_{\substack{=E_{j}\left(  u_{-}u_{0}\right)  \\\text{(since
(\ref{pf.gtilde.b.Ei}) (applied to }j\text{ instead of }i\text{)}%
\\\text{yields }E_{j}\left(  u_{-}u_{0}\right)  =\varepsilon_{j}\left(
u_{-}\right)  u_{0}\text{)}}}=a_{i,j}E_{j}\left(  \underbrace{u_{-}u_{0}%
}_{=\rho\left(  u_{-}\otimes u_{0}\right)  }\right)  =a_{i,j}E_{j}\left(
\rho\left(  u_{-}\otimes u_{0}\right)  \right) \\
&  =\left(  a_{i,j}E_{j}\circ\rho\right)  \left(  u_{-}\otimes u_{0}\right)  .
\end{align*}


Now, forget that we fixed $u_{-}$ and $u_{0}$. We thus have proven that%
\[
\left(  \left[  H_{i},E_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  =\left(  a_{i,j}E_{j}\circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  .
\]
In other words, the two maps $\left[  H_{i},E_{j}\right]  \circ\rho$ and
$a_{i,j}E_{j}\circ\rho$ are equal to each other on each pure tensor in the
tensor product $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(
\widetilde{\mathfrak{h}}\right)  $. Since these two maps are linear, this
yields that these two maps must be identical (because whenever two linear maps
from a tensor product are equal to each other on each pure tensor, these maps
must be identical). In other words, $\left[  H_{i},E_{j}\right]  \circ
\rho=a_{i,j}E_{j}\circ\rho$. Since we can cancel the $\rho$ from this equation
(because $\rho$ is an isomorphism), this yields $\left[  H_{i},E_{j}\right]
=a_{i,j}E_{j}$.
\end{verlong}

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$ for all $i,j\in\left\{
1,2,...,n\right\}  $.

\bigskip

\textit{Proof of the relation }$\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}$
\textit{for all }$i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

\begin{vershort}
The proof of the relation $\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}$ for all
$i,j\in\left\{  1,2,...,n\right\}  $ is analogous to the above-given proof of
the relation $\left[  H_{i},H_{j}\right]  =0$ for all $i,j\in\left\{
1,2,...,n\right\}  $ (except that this time, instead of using the equality
$\left[  h_{i},h_{j}\right]  =0$ in $\widetilde{\mathfrak{h}}$, need to apply
the equality $\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j}$ in
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$; the latter
equality is a consequence of (\ref{pf.gtilde.b.semidir.ij})).
\end{vershort}

\begin{verlong}
Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Every $u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $
satisfies%
\begin{align*}
\underbrace{\left[  H_{i},F_{j}\right]  }_{=H_{i}\circ F_{j}-F_{j}\circ H_{i}%
}u  &  =\left(  H_{i}\circ F_{j}-F_{j}\circ H_{i}\right)  \left(  u\right)
=H_{i}\underbrace{\left(  F_{j}u\right)  }_{\substack{=f_{j}u\\\text{(by the
definition}\\\text{of }F_{j}\text{)}}}-F_{j}\underbrace{\left(  H_{i}u\right)
}_{\substack{=h_{i}u\\\text{(by the definition}\\\text{of }H_{i}\text{)}}}\\
&  =\underbrace{H_{i}\left(  f_{j}u\right)  }_{\substack{=h_{i}\left(
f_{j}u\right)  \\\text{(by the definition}\\\text{of }H_{i}\text{)}%
}}-\underbrace{F_{j}\left(  h_{i}u\right)  }_{\substack{=f_{j}\left(
h_{i}u\right)  \\\text{(by the definition}\\\text{of }F_{j}\text{)}}}\\
&  =h_{i}\left(  f_{j}u\right)  -f_{j}\left(  h_{i}u\right)
=\underbrace{\left(  h_{i}f_{j}-f_{j}h_{i}\right)  }_{\substack{=\left[
h_{i},f_{j}\right]  =-a_{i,j}f_{j}\\\text{in }U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \\\text{(since
(\ref{pf.gtilde.b.semidir.ij}) yields }\left[  h_{i},f_{j}\right]
=-a_{i,j}f_{j}\text{ in }\widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\text{)}}}u\\
&  =-a_{i,j}f_{j}u
\end{align*}
and $-a_{i,j}F_{j}u=-a_{i,j}f_{j}u$ (because the definition of $F_{j}$ yields
$F_{j}u=f_{j}u$). Thus, every $u\in U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ satisfies $\left[  H_{i}%
,F_{j}\right]  u=-a_{i,j}F_{j}u$. Hence, $\left[  H_{i},F_{j}\right]
=-a_{i,j}F_{j}$.

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}$ for all $i,j\in\left\{
1,2,...,n\right\}  $.
\end{verlong}

\bigskip

\textit{Proof of the relation }$\left[  E_{i},F_{j}\right]  =\delta_{i,j}%
H_{i}$ \textit{for all }$i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

\begin{vershort}
Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Let
$u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in
U\left(  \widetilde{\mathfrak{h}}\right)  $. Since $f_{j}\in
\widetilde{\mathfrak{n}}_{-}$ and $u_{-}\in U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, we have $f_{j}u_{-}\in\widetilde{\mathfrak{n}}_{-}\cdot
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. Thus, we can apply
(\ref{pf.gtilde.b.Ei.short}) to $f_{j}u_{-}$ instead of $u_{-}$, and obtain%
\begin{align*}
E_{i}\left(  f_{j}u_{-}u_{0}\right)   &  =\underbrace{\varepsilon_{i}\left(
f_{j}u_{-}\right)  }_{\substack{=\varepsilon_{i}\left(  f_{j}\right)
u_{-}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  \\\text{(since }%
\varepsilon_{i}\text{ is a derivation)}}}u_{0}=\left(  \varepsilon_{i}\left(
f_{j}\right)  u_{-}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  \right)  u_{0}\\
&  =\underbrace{\varepsilon_{i}\left(  f_{j}\right)  }_{\substack{=\delta
_{i,j}h_{i}\\\text{(by the definition of }\varepsilon_{i}\text{)}}}u_{-}%
u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}=\delta_{i,j}h_{i}%
u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}.
\end{align*}

\end{vershort}

\begin{verlong}
Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Let
$u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in
U\left(  \widetilde{\mathfrak{h}}\right)  $. Since $f_{j}\in
\widetilde{\mathfrak{n}}_{-}$ and $u_{-}\in U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, we have $f_{j}u_{-}\in\widetilde{\mathfrak{n}}_{-}\cdot
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. Thus, we can apply
(\ref{pf.gtilde.b.Ei}) to $f_{j}u_{-}$ instead of $u_{-}$, and obtain%
\begin{align}
E_{i}\left(  f_{j}u_{-}u_{0}\right)   &  =\underbrace{\varepsilon_{i}\left(
f_{j}u_{-}\right)  }_{\substack{=\varepsilon_{i}\left(  f_{j}\right)
u_{-}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  \\\text{(since }%
\varepsilon_{i}\text{ is a derivation)}}}u_{0}=\left(  \varepsilon_{i}\left(
f_{j}\right)  u_{-}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  \right)
u_{0}\nonumber\\
&  =\underbrace{\varepsilon_{i}\left(  f_{j}\right)  }_{\substack{=\delta
_{i,j}h_{i}\\\text{(by (\ref{pf.gtilde.b.epsilon.fj}))}}}u_{-}u_{0}%
+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}\nonumber\\
&  =\delta_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)
u_{0}. \label{pf.gtilde.dercomm7}%
\end{align}

\end{verlong}

\begin{vershort}
But%
\begin{align*}
\underbrace{\left[  E_{i},F_{j}\right]  }_{=E_{i}\circ F_{j}-F_{j}\circ E_{i}%
}\left(  u_{-}u_{0}\right)   &  =\left(  E_{i}\circ F_{j}-F_{j}\circ
E_{i}\right)  \left(  u_{-}u_{0}\right) \\
&  =E_{i}\left(  \underbrace{F_{j}\left(  u_{-}u_{0}\right)  }%
_{\substack{=f_{j}u_{-}u_{0}\\\text{(by the definition of }F_{j}\text{)}%
}}\right)  -F_{j}\left(  \underbrace{E_{i}\left(  u_{-}u_{0}\right)
}_{\substack{=\varepsilon_{i}\left(  u_{-}\right)  u_{0}\\\text{(by
(\ref{pf.gtilde.b.Ei.short}))}}}\right) \\
&  =\underbrace{E_{i}\left(  f_{j}u_{-}u_{0}\right)  }_{\substack{=\delta
_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}%
\\\text{(as we have proven above)}}}-\underbrace{F_{j}\left(  \varepsilon
_{i}\left(  u_{-}\right)  u_{0}\right)  }_{\substack{=f_{j}\varepsilon
_{i}\left(  u_{-}\right)  u_{0}\\\text{(by the definition of }F_{j}\text{)}%
}}\\
&  =\delta_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)
u_{0}-f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}=\delta_{i,j}h_{i}%
u_{-}u_{0}\\
&  =\delta_{i,j}\underbrace{h_{i}u_{-}u_{0}}_{\substack{=H_{i}\left(
u_{-}u_{0}\right)  \\\text{(since the definition of }H_{i}\text{
yields}\\H_{i}\left(  u_{-}u_{0}\right)  =h_{i}u_{-}u_{0}\text{)}}%
}=\delta_{i,j}H_{i}\left(  u_{-}u_{0}\right)  .
\end{align*}


Now, forget that we fixed $u_{-}$ and $u_{0}$. We thus have proven that%
\[
\left[  E_{i},F_{j}\right]  \left(  u_{-}u_{0}\right)  =\delta_{i,j}%
H_{i}\left(  u_{-}u_{0}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{-}\in
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  \text{.}%
\]
Since the vector space $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is generated by products $u_{-}u_{0}$
with $u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in
U\left(  \widetilde{\mathfrak{h}}\right)  $ (this is because $\rho:U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(  \widetilde{\mathfrak{h}%
}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is an isomorphism), this yields that
$\left[  E_{i},F_{j}\right]  =\delta_{i,j}H_{i}$.
\end{vershort}

\begin{verlong}
But $\rho\left(  u_{-}\otimes u_{0}\right)  =u_{-}u_{0}$ (by the definition of
$\rho$), and%
\begin{align*}
&  \left(  \left[  E_{i},F_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right) \\
&  =\underbrace{\left[  E_{i},F_{j}\right]  }_{=E_{i}\circ F_{j}-F_{j}\circ
E_{i}}\left(  \underbrace{\rho\left(  u_{-}\otimes u_{0}\right)  }%
_{=u_{-}u_{0}}\right)  =\left(  E_{i}\circ F_{j}-F_{j}\circ E_{i}\right)
\left(  u_{-}u_{0}\right) \\
&  =E_{i}\left(  \underbrace{F_{j}\left(  u_{-}u_{0}\right)  }%
_{\substack{=f_{j}u_{-}u_{0}\\\text{(by the definition of }F_{j}\text{)}%
}}\right)  -F_{j}\left(  \underbrace{E_{i}\left(  u_{-}u_{0}\right)
}_{\substack{=\varepsilon_{i}\left(  u_{-}\right)  u_{0}\\\text{(by
(\ref{pf.gtilde.b.Ei}))}}}\right) \\
&  =\underbrace{E_{i}\left(  f_{j}u_{-}u_{0}\right)  }_{\substack{=\delta
_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}%
\\\text{(by (\ref{pf.gtilde.dercomm7}))}}}-\underbrace{F_{j}\left(
\varepsilon_{i}\left(  u_{-}\right)  u_{0}\right)  }_{\substack{=f_{j}%
\varepsilon_{i}\left(  u_{-}\right)  u_{0}\\\text{(by the definition of }%
F_{j}\text{)}}}\\
&  =\delta_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)
u_{0}-f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}=\delta_{i,j}%
h_{i}\underbrace{u_{-}u_{0}}_{=\rho\left(  u_{-}\otimes u_{0}\right)  }\\
&  =\delta_{i,j}\underbrace{h_{i}\rho\left(  u_{-}\otimes u_{0}\right)
}_{\substack{=H_{i}\left(  \rho\left(  u_{-}\otimes u_{0}\right)  \right)
\\\text{(since the definition of }H_{i}\text{ yields}\\H_{i}\left(
\rho\left(  u_{-}\otimes u_{0}\right)  \right)  =h_{i}\rho\left(  u_{-}\otimes
u_{0}\right)  \text{)}}}=\delta_{i,j}H_{i}\left(  \rho\left(  u_{-}\otimes
u_{0}\right)  \right) \\
&  =\left(  \delta_{i,j}H_{i}\circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  .
\end{align*}


Now, forget that we fixed $u_{-}$ and $u_{0}$. We thus have proven that%
\[
\left(  \left[  E_{i},F_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  =\left(  \delta_{i,j}H_{i}\circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  .
\]
In other words, the two maps $\left[  E_{i},F_{j}\right]  \circ\rho$ and
$\delta_{i,j}H_{i}\circ\rho$ are equal to each other on each pure tensor in
the tensor product $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes
U\left(  \widetilde{\mathfrak{h}}\right)  $. Since these two maps are linear,
this yields that these two maps must be identical (because whenever two linear
maps from a tensor product are equal to each other on each pure tensor, these
maps must be identical). In other words, $\left[  E_{i},F_{j}\right]
\circ\rho=\delta_{i,j}H_{i}\circ\rho$. Since we can cancel the $\rho$ from
this equation (because $\rho$ is an isomorphism), this yields $\left[
E_{i},F_{j}\right]  =\delta_{i,j}H_{i}$.
\end{verlong}

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  E_{i},F_{j}\right]  =\delta_{i,j}H_{i}$ for all $i,j\in\left\{
1,2,...,n\right\}  $.

\bigskip

Altogether, we have thus verified all four relations (\ref{pf.gtilde.NONSERRE}%
). Now, let us define a Lie algebra homomorphism $\xi^{\prime}%
:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \rightarrow\operatorname*{End}\left(  U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $
by the relations%
\[
\left\{
\begin{array}
[c]{c}%
\xi^{\prime}\left(  e_{i}\right)  =E_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}  ;\\
\xi^{\prime}\left(  f_{i}\right)  =F_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}  ;\\
\xi^{\prime}\left(  h_{i}\right)  =H_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  .
\]
This $\xi^{\prime}$ is clearly well-defined (because a Lie algebra
homomorphism from a free Lie algebra over a set can be defined by arbitrarily
choosing its values at the elements of this set). This homomorphism
$\xi^{\prime}$ clearly maps the four relations (\ref{nonserre-relations}) to
the four relations (\ref{pf.gtilde.NONSERRE}). Since we know that the four
relations (\ref{pf.gtilde.NONSERRE}) are satisfied in $\operatorname*{End}%
\left(  U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  $, we conclude that the homomorphism $\xi^{\prime}$
factors through the Lie algebra $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)
=\widetilde{\mathfrak{g}}$. In other words, there exists a Lie algebra
homomorphism $\xi:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}%
\left(  U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  $ such that%
\[
\left\{
\begin{array}
[c]{c}%
\xi\left(  e_{i}\right)  =E_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\xi\left(  f_{i}\right)  =F_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\xi\left(  h_{i}\right)  =H_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  .
\]
Consider this $\xi$. Clearly, the Lie algebra homomorphism $\xi
:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}\left(  U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $
makes the vector space $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ into a $\widetilde{\mathfrak{g}}$-module.

\bigskip

\underline{\textit{6th step: Proving the injectivity of }$\iota_{-}$%
\textit{.}}

We are very close to proving Theorem \ref{thm.gtilde} \textbf{(b)} now.

Let $\xi_{-}$ be the map $\xi\circ\iota_{-}:\widetilde{\mathfrak{n}}%
_{-}\rightarrow\operatorname*{End}\left(  U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $. Then, $\xi_{-}$ is a
Lie algebra homomorphism (since $\xi$ and $\iota_{-}$ are Lie algebra homomorphisms).

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\underbrace{\xi_{-}}%
_{=\xi\circ\iota_{-}}\left(  f_{i}\right)  =\left(  \xi\circ\iota_{-}\right)
\left(  f_{i}\right)  =\xi\left(  \underbrace{\iota_{-}\left(  f_{i}\right)
}_{\substack{=f_{i}\\\text{(by the definition of }\iota_{-}\text{)}}}\right)
=\xi\left(  f_{i}\right)  =F_{i}$ (by the definition of $\xi$).

Let $\mathfrak{s}$ be the subset%
\[
\left\{  s\in\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(  \xi_{-}\left(
s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}
\]
of $\widetilde{\mathfrak{n}}_{-}$. Every $i\in\left\{  1,2,...,n\right\}  $
satisfies%
\[
\underbrace{\left(  \xi_{-}\left(  f_{i}\right)  \right)  }_{=F_{i}}\left(
u\right)  =F_{i}\left(  u\right)  =f_{i}u\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }F_{i}\right)
\]
for all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $, and therefore%
\[
f_{i}\in\left\{  s\in\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(  \xi
_{-}\left(  s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)
\right\}  =\mathfrak{s}.
\]
In other words, $\mathfrak{s}$ contains the elements $f_{1}$, $f_{2}$, $...$,
$f_{n}$.

\begin{vershort}
On the other hand, it is very easy to see that $\mathfrak{s}$ is a Lie
subalgebra of $\widetilde{\mathfrak{n}}_{-}$. (In fact, all that is needed to
prove this is knowing that $\xi_{-}$ is a Lie algebra homomorphism. The
details are left to the reader.)
\end{vershort}

\begin{verlong}
Now, we will show that $\mathfrak{s}$ is a Lie subalgebra of
$\widetilde{\mathfrak{n}}_{-}$.

\textit{Proof that $\mathfrak{s}$ is a Lie subalgebra of }%
$\widetilde{\mathfrak{n}}_{-}$\textit{:}

The reader can easily verify that $\mathfrak{s}$ is a vector subspace of
$\widetilde{\mathfrak{n}}_{-}$ (because for any fixed $u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $, the
equation $\left(  \xi_{-}\left(  s\right)  \right)  \left(  u\right)  =su$ is
linear in $s$). Now, let $s_{1}\in\mathfrak{s}$ and $s_{2}\in\mathfrak{s}$ be
arbitrary. Then,%
\[
s_{1}\in\mathfrak{s}=\left\{  s\in\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(
\xi_{-}\left(  s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)
\right\}  ,
\]
so that%
\begin{equation}
\left(  \xi_{-}\left(  s_{1}\right)  \right)  \left(  u\right)  =s_{1}%
u\ \ \ \ \ \ \ \ \ \ \text{for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  . \label{pf.gtilde.b.step6.1}%
\end{equation}
Similarly,%
\begin{equation}
\left(  \xi_{-}\left(  s_{2}\right)  \right)  \left(  u\right)  =s_{2}%
u\ \ \ \ \ \ \ \ \ \ \text{for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  . \label{pf.gtilde.b.step6.2}%
\end{equation}
Now, since $\xi_{-}$ is a Lie algebra homomorphism, we have $\xi_{-}\left(
\left[  s_{1},s_{2}\right]  \right)  =\left[  \xi_{-}\left(  s_{1}\right)
,\xi_{-}\left(  s_{2}\right)  \right]  =\xi_{-}\left(  s_{1}\right)  \circ
\xi_{-}\left(  s_{2}\right)  -\xi_{-}\left(  s_{2}\right)  \circ\xi_{-}\left(
s_{1}\right)  $. Thus, every $u\in U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ satisfies%
\begin{align*}
\left(  \xi_{-}\left(  \left[  s_{1},s_{2}\right]  \right)  \right)  \left(
u\right)   &  =\left(  \xi_{-}\left(  s_{1}\right)  \circ\xi_{-}\left(
s_{2}\right)  -\xi_{-}\left(  s_{2}\right)  \circ\xi_{-}\left(  s_{1}\right)
\right)  \left(  u\right) \\
&  =\left(  \xi_{-}\left(  s_{1}\right)  \right)  \left(  \underbrace{\left(
\xi_{-}\left(  s_{2}\right)  \right)  \left(  u\right)  }_{\substack{=s_{2}%
u\\\text{(by (\ref{pf.gtilde.b.step6.2}))}}}\right)  -\left(  \xi_{-}\left(
s_{2}\right)  \right)  \left(  \underbrace{\left(  \xi_{-}\left(
s_{1}\right)  \right)  \left(  u\right)  }_{\substack{=s_{1}u\\\text{(by
(\ref{pf.gtilde.b.step6.1}))}}}\right) \\
&  =\underbrace{\left(  \xi_{-}\left(  s_{1}\right)  \right)  \left(
s_{2}u\right)  }_{\substack{=s_{1}s_{2}u\\\text{(by (\ref{pf.gtilde.b.step6.1}%
), applied to}\\s_{2}u\text{ instead of }u\text{)}}}-\underbrace{\left(
\xi_{-}\left(  s_{2}\right)  \right)  \left(  s_{1}u\right)  }%
_{\substack{=s_{2}s_{1}u\\\text{(by (\ref{pf.gtilde.b.step6.2}), applied
to}\\s_{1}u\text{ instead of }u\text{)}}}\\
&  =s_{1}s_{2}u-s_{2}s_{1}u=\underbrace{\left(  s_{1}s_{2}-s_{2}s_{1}\right)
}_{=\left[  s_{1},s_{2}\right]  }u=\left[  s_{1},s_{2}\right]  u.
\end{align*}
Hence,%
\[
\left[  s_{1},s_{2}\right]  \in\left\{  s\in\widetilde{\mathfrak{n}}_{-}%
\ \mid\ \left(  \xi_{-}\left(  s\right)  \right)  \left(  u\right)  =su\text{
for all }u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \right\}  =\mathfrak{s}.
\]
Now, forget that we fixed $s_{1}$ and $s_{2}$. We thus have proven that every
$s_{1}\in\mathfrak{s}$ and $s_{2}\in\mathfrak{s}$ satisfy $\left[  s_{1}%
,s_{2}\right]  \in\mathfrak{s}$. Since $\mathfrak{s}$ is a vector subspace of
$\widetilde{\mathfrak{n}}_{-}$, this yields that $\mathfrak{s}$ is a Lie
subalgebra of $\widetilde{\mathfrak{n}}_{-}$.
\end{verlong}

But now recall that $\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}%
\left(  f_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $. Hence, the
elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ generate $\widetilde{\mathfrak{n}%
}_{-}$ as a Lie algebra. Thus, every Lie subalgebra of
$\widetilde{\mathfrak{n}}_{-}$ which contains the elements $f_{1}$, $f_{2}$,
$...$, $f_{n}$ must be $\widetilde{\mathfrak{n}}_{-}$ itself. Since we know
that $\mathfrak{s}$ is a Lie subalgebra of $\widetilde{\mathfrak{n}}_{-}$ and
contains the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$, this yields that
$\mathfrak{s}$ must be $\widetilde{\mathfrak{n}}_{-}$ itself. In other words,
$\mathfrak{s}=\widetilde{\mathfrak{n}}_{-}$.

Now, let $s^{\prime}\in\widetilde{\mathfrak{n}}_{-}$ be such that $\iota
_{-}\left(  s^{\prime}\right)  =0$. Then, $\underbrace{\xi_{-}}_{=\xi
\circ\iota_{-}}\left(  s^{\prime}\right)  =\left(  \xi\circ\iota_{-}\right)
\left(  s^{\prime}\right)  =\xi\left(  \underbrace{\iota_{-}\left(  s^{\prime
}\right)  }_{=0}\right)  =\xi\left(  0\right)  =0$. But since%
\[
s^{\prime}\in\widetilde{\mathfrak{n}}_{-}=\mathfrak{s}=\left\{  s\in
\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(  \xi_{-}\left(  s\right)  \right)
\left(  u\right)  =su\text{ for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}  ,
\]
we have $\left(  \xi_{-}\left(  s^{\prime}\right)  \right)  \left(  u\right)
=s^{\prime}u$ for all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $. Applied to $u=1$, this yields $\left(
\xi_{-}\left(  s^{\prime}\right)  \right)  \left(  1\right)  =s^{\prime}%
\cdot1=s^{\prime}$. Compared with $\underbrace{\left(  \xi_{-}\left(
s^{\prime}\right)  \right)  }_{=0}\left(  1\right)  =0$, this yields
$s^{\prime}=0$.

Now forget that we fixed $s^{\prime}$. We have thus shown that every
$s^{\prime}\in\widetilde{\mathfrak{n}}_{-}$ such that $\iota_{-}\left(
s^{\prime}\right)  =0$ must satisfy $s^{\prime}=0$. In other words, $\iota
_{-}$ is injective.

\bigskip

\underline{\textit{7th step: Proving the injectivity of }$\iota_{0}$%
\textit{.}}

\begin{vershort}
A similar, but even simpler, argument shows that $\iota_{0}$ is injective.
Again, the reader can fill in the details.
\end{vershort}

\begin{verlong}
The proof of the injectivity of $\iota_{0}$ is similar but even simpler.

Let $\xi_{0}$ be the map $\xi\circ\iota_{0}:\widetilde{\mathfrak{h}%
}\rightarrow\operatorname*{End}\left(  U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $.

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\underbrace{\xi_{0}}%
_{=\xi\circ\iota_{0}}\left(  h_{i}\right)  =\left(  \xi\circ\iota_{0}\right)
\left(  h_{i}\right)  =\xi\left(  \underbrace{\iota_{0}\left(  h_{i}\right)
}_{\substack{=h_{i}\\\text{(by the definition of }\iota_{0}\text{)}}}\right)
=\xi\left(  h_{i}\right)  =H_{i}$ (by the definition of $\xi$).

Let $\mathfrak{t}$ be the subset%
\[
\left\{  s\in\widetilde{\mathfrak{h}}\ \mid\ \left(  \xi_{0}\left(  s\right)
\right)  \left(  u\right)  =su\text{ for all }u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}
\]
of $\widetilde{\mathfrak{h}}$. Every $i\in\left\{  1,2,...,n\right\}  $
satisfies%
\[
\underbrace{\left(  \xi_{0}\left(  h_{i}\right)  \right)  }_{=H_{i}}\left(
u\right)  =H_{i}\left(  u\right)  =h_{i}u\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }H_{i}\right)
\]
for all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $, and therefore%
\[
h_{i}\in\left\{  s\in\widetilde{\mathfrak{h}}\ \mid\ \left(  \xi_{0}\left(
s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}
=\mathfrak{t}.
\]
In other words, $\mathfrak{t}$ contains the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$.

On the other hand, it is easy to see that $\mathfrak{t}$ is a vector subspace
of $\widetilde{\mathfrak{h}}$ (because for any fixed $u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $, the
equation $\left(  \xi_{0}\left(  s\right)  \right)  \left(  u\right)  =su$ is
linear in $s$).

But now recall that $\widetilde{\mathfrak{h}}$ is the free vector space with
basis $h_{1},h_{2},...,h_{n}$. Thus, the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$ span the vector space $\widetilde{\mathfrak{h}}$. Thus, every vector
subspace of $\widetilde{\mathfrak{h}}$ which contains the elements $h_{1}$,
$h_{2}$, $...$, $h_{n}$ must be $\widetilde{\mathfrak{h}}$ itself. Since we
know that $\mathfrak{t}$ is a vector subspace of $\widetilde{\mathfrak{h}}$
and contains the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$, this yields that
$\mathfrak{t}$ must be $\widetilde{\mathfrak{h}}$ itself. In other words,
$\mathfrak{t}=\widetilde{\mathfrak{h}}$.

Now, let $s^{\prime}\in\widetilde{\mathfrak{h}}$ be such that $\iota
_{0}\left(  s^{\prime}\right)  =0$. Then, $\underbrace{\xi_{0}}_{=\xi
\circ\iota_{0}}\left(  s^{\prime}\right)  =\left(  \xi\circ\iota_{0}\right)
\left(  s^{\prime}\right)  =\xi\left(  \underbrace{\iota_{0}\left(  s^{\prime
}\right)  }_{=0}\right)  =\xi\left(  0\right)  =0$. But since%
\[
s^{\prime}\in\widetilde{\mathfrak{h}}=\mathfrak{t}=\left\{  s\in
\widetilde{\mathfrak{h}}\ \mid\ \left(  \xi_{0}\left(  s\right)  \right)
\left(  u\right)  =su\text{ for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}  ,
\]
we have $\left(  \xi_{0}\left(  s^{\prime}\right)  \right)  \left(  u\right)
=s^{\prime}u$ for all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $. Applied to $u=1$, this yields $\left(
\xi_{0}\left(  s^{\prime}\right)  \right)  \left(  1\right)  =s^{\prime}%
\cdot1=s^{\prime}$. Compared with $\underbrace{\left(  \xi_{0}\left(
s^{\prime}\right)  \right)  }_{=0}\left(  1\right)  =0$, this yields
$s^{\prime}=0$.

Now forget that we fixed $s^{\prime}$. We have thus shown that every
$s^{\prime}\in\widetilde{\mathfrak{h}}$ such that $\iota_{0}\left(  s^{\prime
}\right)  =0$ must satisfy $s^{\prime}=0$. In other words, $\iota_{0}$ is injective.
\end{verlong}

\bigskip

\underline{\textit{8th step: Proving the injectivity of }$\iota_{+}$%
\textit{.}}

\begin{vershort}
We have proven the injectivity of the maps $\iota_{-}$ and $\iota_{0}$ above.
The proof of the injectivity of the map $\iota_{+}$ is analogous to the above
proof of the injectivity of the map $\iota_{-}$. (Alternately, the injectivity
of $\iota_{+}$ can be obtained from that of $\iota_{-}$ using the involutive
Lie algebra automorphism constructed in Theorem \ref{thm.gtilde} \textbf{(f)}.)
\end{vershort}

\begin{verlong}
We have proven the injectivity of the maps $\iota_{-}$ and $\iota_{0}$ above.
The proof of the injectivity of the map $\iota_{+}$ is very similar to the
above proof of the injectivity of the map $\iota_{-}$. More precisely, in
order to obtain a proof of the injectivity of the map $\iota_{+}$, it is
enough to apply the following changes to our above proof of the injectivity of
the map $\iota_{-}$:

\begin{itemize}
\item replace every $e_{i}$ by $f_{i}$, and simultaneously replace every
$f_{i}$ by $e_{i}$;

\item replace every $h_{i}$ by $-h_{i}$ (this is allowed since
$\widetilde{\mathfrak{h}}$ is the free vector space with basis $-h_{1}%
,-h_{2},...,-h_{n}$);

\item replace $\widetilde{\mathfrak{n}}_{-}$ by $\widetilde{\mathfrak{n}}_{+}$;

\item replace every reference to (\ref{nonserre-relations}) by a reference to
(\ref{nonserre-relations2}) (this is allowed since we know that the relations
(\ref{nonserre-relations2}) are satisfied in $\widetilde{\mathfrak{g}}$).
\end{itemize}

We have thus proven that the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ are
injective. Moreover, $\iota_{+}$ and $\iota_{-}$ are Lie algebra homomorphisms
(by definition), and $\iota_{0}$ is a Lie algebra homomorphism as well
(because any $i,j\in\left\{  1,2,...,n\right\}  $ satisfy $\left[  h_{i}%
,h_{j}\right]  =0$ in $\widetilde{\mathfrak{h}}$ (since
$\widetilde{\mathfrak{h}}$ is an abelian Lie algebra) and $\left[  h_{i}%
,h_{j}\right]  =0$ in $\widetilde{\mathfrak{g}}$ (due to the relations
(\ref{nonserre-relations}))). This completes the proof of Theorem
\ref{thm.gtilde} \textbf{(b)}.
\end{verlong}

\bigskip

\textbf{(c)} \underline{\textit{1st step: The existence of the direct sum in
question.}}

Define a relation $\leq$ on $Q$ by positing that two $n$-tuples $\left(
\lambda_{1},\lambda_{2},...,\lambda_{n}\right)  \in\mathbb{Z}^{n}$ and
$\left(  \mu_{1},\mu_{2},...,\mu_{n}\right)  \in\mathbb{Z}^{n}$ satisfy
$\lambda_{1}\alpha_{1}+\lambda_{2}\alpha_{2}+...+\lambda_{n}\alpha_{n}\leq
\mu_{1}\alpha_{1}+\mu_{2}\alpha_{2}+...+\mu_{n}\alpha_{n}$ if and only if
every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\lambda_{i}\leq\mu_{i}$.
It is clear that this relation $\leq$ is a non-strict partial order. Define
$\geq$ to be the opposite of $\leq$. Define $>$ and $<$ to be the strict
versions of the relations $\geq$ and $\leq$, respectively; thus, any
$\alpha\in Q$ and $\beta\in Q$ satisfy $\alpha>\beta$ if and only if $\left(
\alpha\neq\beta\text{ and }\alpha\geq\beta\right)  $.

The elements $\alpha$ of $Q$ satisfying $\alpha>0$ are exactly the nonzero
sums $\lambda_{1}\alpha_{1}+\lambda_{2}\alpha_{2}+...+\lambda_{n}\alpha_{n}$
with $\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$ being nonnegative
integers. The elements $\alpha$ of $Q$ satisfying $\alpha<0$ are exactly the
nonzero sums $\lambda_{1}\alpha_{1}+\lambda_{2}\alpha_{2}+...+\lambda
_{n}\alpha_{n}$ with $\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$ being
nonpositive integers.

Let $\widetilde{\mathfrak{g}}\left[  <0\right]  =\bigoplus
\limits_{\substack{\alpha\in Q;\\\alpha<0}}\widetilde{\mathfrak{g}}\left[
\alpha\right]  $ and $\widetilde{\mathfrak{g}}\left[  >0\right]
=\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha>0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  $. Then, $\widetilde{\mathfrak{g}}\left[  0\right]  $,
$\widetilde{\mathfrak{g}}\left[  <0\right]  $ and $\widetilde{\mathfrak{g}%
}\left[  >0\right]  $ are $Q$-graded Lie subalgebras of
$\widetilde{\mathfrak{g}}$ (this is easy to see from the fact that
$\widetilde{\mathfrak{g}}$ is a $Q$-graded Lie algebra).

It is easy to see that the (internal) direct sum $\widetilde{\mathfrak{g}%
}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}}\left[  <0\right]
\oplus\widetilde{\mathfrak{g}}\left[  0\right]  $ is
well-defined.\footnote{\textit{Proof.} We have $\widetilde{\mathfrak{g}%
}=\bigoplus\limits_{\alpha\in Q}\widetilde{\mathfrak{g}}\left[  \alpha\right]
$ (since $\widetilde{\mathfrak{g}}$ is $Q$-graded). But every $\alpha\in Q$
satisfies \textbf{exactly one} of the four assertions $\alpha>0$, $\alpha<0$,
$\alpha=0$ and $\left(  \text{neither }\alpha<0\text{ nor }\alpha>0\text{ nor
}\alpha=0\right)  $. Thus,%
\begin{align*}
\bigoplus\limits_{\alpha\in Q}\widetilde{\mathfrak{g}}\left[  \alpha\right]
&  =\underbrace{\left(  \bigoplus\limits_{\substack{\alpha\in Q;\\\alpha
>0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  }%
_{=\widetilde{\mathfrak{g}}\left[  >0\right]  }\oplus\underbrace{\left(
\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha<0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  \right)  }_{=\widetilde{\mathfrak{g}}\left[
<0\right]  }\oplus\underbrace{\left(  \bigoplus\limits_{\substack{\alpha\in
Q;\\\alpha=0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)
}_{=\widetilde{\mathfrak{g}}\left[  0\right]  }\oplus\left(  \bigoplus
\limits_{\substack{\alpha\in Q;\\\text{neither }\alpha<0\\\text{nor }%
\alpha>0\text{ nor }\alpha=0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]
\right) \\
&  =\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}}\left[  0\right]
\oplus\left(  \bigoplus\limits_{\substack{\alpha\in Q;\\\text{neither }%
\alpha<0\\\text{nor }\alpha>0\text{ nor }\alpha=0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  \right)  .
\end{align*}
Thus, the (internal) direct sum $\widetilde{\mathfrak{g}}\left[  >0\right]
\oplus\widetilde{\mathfrak{g}}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  $ is well-defined (because it is a partial sum of the
direct sum $\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus
\widetilde{\mathfrak{g}}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  \oplus\left(  \bigoplus\limits_{\substack{\alpha\in
Q;\\\text{neither }\alpha<0\\\text{nor }\alpha>0\text{ nor }\alpha
=0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  $).}

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies
\begin{align*}
f_{i}  &  \in\widetilde{\mathfrak{g}}\left[  -\alpha_{i}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\deg\left(  f_{i}\right)
=-\alpha_{i}\right) \\
&  \subseteq\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha<0}%
}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }-\alpha_{i}<0\right) \\
&  =\widetilde{\mathfrak{g}}\left[  <0\right]  .
\end{align*}
Hence, the Lie algebra $\widetilde{\mathfrak{g}}\left[  <0\right]  $ contains
the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$. But now, recall that
$\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  f_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $. Hence, the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{n}}_{-}$ generate
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra. Thus, the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{g}}$ generate $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ as a Lie algebra (because
the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{g}}$
are the images of the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of
$\widetilde{\mathfrak{n}}_{-}$ under the map $\iota_{-}$). Thus, every Lie
subalgebra of $\widetilde{\mathfrak{g}}$ which contains the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ must contain $\iota_{-}\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $ as a subset. Since we know that $\widetilde{\mathfrak{g}%
}\left[  <0\right]  $ is a Lie subalgebra of $\widetilde{\mathfrak{g}}$ and
contains the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$, this yields that
$\widetilde{\mathfrak{g}}\left[  <0\right]  $ must contain $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ as a subset. In other words, $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  <0\right]  $. Similarly (by considering the
elements $e_{1}$, $e_{2}$, $...$, $e_{n}$ instead of $f_{1}$, $f_{2}$, $...$,
$f_{n}$), we can show $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
\subseteq\widetilde{\mathfrak{g}}\left[  >0\right]  $.

\begin{vershort}
A similar argument proves $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
\subseteq\widetilde{\mathfrak{g}}\left[  0\right]  $.
\end{vershort}

\begin{verlong}
Finally, every $i\in\left\{  1,2,...,n\right\}  $ satisfies $h_{i}%
\in\widetilde{\mathfrak{g}}\left[  0\right]  $ (since $\deg\left(
h_{i}\right)  =0$). In other words, the vector space $\widetilde{\mathfrak{g}%
}\left[  0\right]  $ contains the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$.

But now, recall that $\widetilde{\mathfrak{h}}$ is the free vector space with
basis $h_{1},h_{2},...,h_{n}$. Thus, the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$ of $\widetilde{\mathfrak{h}}$ span the vector space
$\widetilde{\mathfrak{h}}$. Consequently, the elements $h_{1}$, $h_{2}$,
$...$, $h_{n}$ of $\widetilde{\mathfrak{g}}$ span the vector space $\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ (because the elements $h_{1}$,
$h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{g}}$ are the images of the
elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{h}}$ under
the map $\iota_{0}$). Hence, every vector subspace of $\widetilde{\mathfrak{g}%
}$ which contains the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ must contain
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ as a subset. Since we
know that $\widetilde{\mathfrak{g}}\left[  0\right]  $ is a vector subspace of
$\widetilde{\mathfrak{g}}$ and contains the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$, this yields that $\widetilde{\mathfrak{g}}\left[  0\right]  $ must
contain $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ as a subset. In
other words, $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  0\right]  $.
\end{verlong}

Since the internal direct sum $\widetilde{\mathfrak{g}}\left[  <0\right]
\oplus\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  $ is well-defined, the internal direct sum $\iota
_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ must also be well-defined (because
$\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  >0\right]  $, $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{g}}\left[
<0\right]  $ and $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
\subseteq\widetilde{\mathfrak{g}}\left[  0\right]  $). We now must prove that
this direct sum is $\widetilde{\mathfrak{g}}$.

\bigskip

\underline{\textit{2nd step: Identifications.}}

Since the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ are injective Lie
algebra homomorphisms, and since their images are linearly disjoint (because
the direct sum $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
\oplus\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ is well-defined), we can regard
these maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ as inclusions of Lie
algebras. Let us do this from now on. Thus, $\widetilde{\mathfrak{n}}_{+}$,
$\widetilde{\mathfrak{n}}_{-}$ and $\widetilde{\mathfrak{h}}$ are Lie
subalgebras of $\widetilde{\mathfrak{g}}$. The identification of
$\widetilde{\mathfrak{n}}_{-}$ with the Lie subalgebra $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ of $\widetilde{\mathfrak{g}}$
eliminates the need of distinguishing between the elements $f_{i}$ of
$\widetilde{\mathfrak{n}}_{-}$ and the elements $f_{i}$ of
$\widetilde{\mathfrak{g}}$ (because for every $i\in\left\{  1,2,...,n\right\}
$, the element $f_{i}$ of $\widetilde{\mathfrak{g}}$ is the image of the
element $f_{i}$ of $\widetilde{\mathfrak{n}}_{-}$ under the map $\iota_{-}$,
and since we regard this map $\iota_{-}$ as inclusion, these two elements
$f_{i}$ are therefore equal). Similarly, we don't have to distinguish between
the elements $e_{i}$ of $\widetilde{\mathfrak{n}}_{+}$ and the elements
$e_{i}$ of $\widetilde{\mathfrak{g}}$, nor is it necessary to distinguish
between the elements $h_{i}$ of $\widetilde{\mathfrak{h}}$ and the elements
$h_{i}$ of $\widetilde{\mathfrak{g}}$.

Since we regard the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ as
inclusions, we have $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
=\widetilde{\mathfrak{n}}_{+}$, $\iota_{-}\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$ and $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{h}}$. Hence, $\iota
_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$. This shows that
the internal direct sums $\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}$ and $\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{h}}$ are well-defined (since they are partial sums of the
direct sum $\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}$).

\bigskip

\underline{\textit{3rd step: Proving that }$\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}$\textit{ is a Lie subalgebra of }%
$\widetilde{\mathfrak{g}}$\textit{.}}

We now will prove part \textbf{(d)} of Theorem \ref{thm.gtilde} before we come
back and finish the proof of part \textbf{(c)}.

\begin{vershort}
Indeed, let us first prove that $\left[  \widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$.

In fact, in order to prove this, it is enough to show that $\left[
h_{i},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq\widetilde{\mathfrak{n}%
}_{-}$ for every $i\in\left\{  1,2,...,n\right\}  $ (since the elements
$h_{1}$, $h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{h}}$ span the vector
space $\widetilde{\mathfrak{h}}$). So let $i\in\left\{  1,2,...,n\right\}  $.
Let $\xi_{i}:\widetilde{\mathfrak{g}}\rightarrow\widetilde{\mathfrak{g}}$ be
the map defined by%
\[
\left(  \xi_{i}\left(  x\right)  =\left[  h_{i},x\right]
\ \ \ \ \ \ \ \ \ \ \text{for any }x\in\widetilde{\mathfrak{g}}\right)  .
\]
Then, $\xi_{i}$ is a Lie derivation of the Lie algebra
$\widetilde{\mathfrak{g}}$. On the other hand, the subset $\left\{
f_{1},f_{2},...,f_{n}\right\}  $ of $\widetilde{\mathfrak{n}}_{-}$ generates
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra (since the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{n}}_{-}$ generate
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra), and we can easily check that
$\xi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)
\subseteq\widetilde{\mathfrak{n}}_{-}$\ \ \ \ \footnote{\textit{Proof.} For
every $j\in\left\{  1,2,...,n\right\}  $, we have%
\begin{align*}
\xi_{i}\left(  f_{j}\right)   &  =\left[  h_{i},f_{j}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\xi_{i}\right) \\
&  =-a_{i,j}\underbrace{f_{j}}_{\in\widetilde{\mathfrak{n}}_{-}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the relations (\ref{nonserre-relations}%
)}\right) \\
&  \in-a_{i,j}\widetilde{\mathfrak{n}}_{-}\subseteq\widetilde{\mathfrak{n}%
}_{-}.
\end{align*}
Thus, $\xi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)
\subseteq\widetilde{\mathfrak{n}}_{-}$, qed.}. Hence, Corollary
\ref{cor.derivation.Lie.unique.ihg} (applied to $\widetilde{\mathfrak{g}}$,
$\widetilde{\mathfrak{n}}_{-}$, $\widetilde{\mathfrak{n}}_{-}$, $\xi_{i}$ and
$\left\{  f_{1},f_{2},...,f_{n}\right\}  $ instead of $\mathfrak{g}$,
$\mathfrak{h}$, $\mathfrak{i}$, $d$ and $S$) yields that $\xi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{n}}_{-}$.
But by the definition of $\xi_{i}$, we have $\xi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  =\left[  h_{i},\widetilde{\mathfrak{n}%
}_{-}\right]  $. Hence, $\left[  h_{i},\widetilde{\mathfrak{n}}_{-}\right]
=\xi_{i}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \subseteq
\widetilde{\mathfrak{n}}_{-}$. Now forget that we fixed $i$. We thus have
proven that $\left[  h_{i},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq
\widetilde{\mathfrak{n}}_{-}$ for every $i\in\left\{  1,2,...,n\right\}  $. As
explained above, this yields $\left[  \widetilde{\mathfrak{h}}%
,\widetilde{\mathfrak{n}}_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$.
\end{vershort}

\begin{verlong}
Indeed, we know that both $\widetilde{\mathfrak{n}}_{-}$ and
$\widetilde{\mathfrak{h}}$ are Lie subalgebras of $\widetilde{\mathfrak{g}}$.
Thus, $\left[  \widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{n}}%
_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$ and $\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{h}}\right]  \subseteq
\widetilde{\mathfrak{h}}$. We will now show that $\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq
\widetilde{\mathfrak{n}}_{-}$.

\textit{Proof of }$\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}%
}_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$\textit{:}

Let $i\in\left\{  1,2,...,n\right\}  $. Let $\xi_{i}:\widetilde{\mathfrak{g}%
}\rightarrow\widetilde{\mathfrak{g}}$ be the map defined by%
\[
\left(  \xi_{i}\left(  x\right)  =\left[  h_{i},x\right]
\ \ \ \ \ \ \ \ \ \ \text{for any }x\in\widetilde{\mathfrak{g}}\right)  .
\]
Then, $\xi_{i}$ is a Lie derivation of the Lie algebra
$\widetilde{\mathfrak{g}}$. On the other hand, the subset $\left\{
f_{1},f_{2},...,f_{n}\right\}  $ of $\widetilde{\mathfrak{n}}_{-}$ generates
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra (since the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{n}}_{-}$ generate
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra), and we can easily check that
$\xi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)
\subseteq\widetilde{\mathfrak{n}}_{-}$\ \ \ \ \footnote{\textit{Proof.} For
every $j\in\left\{  1,2,...,n\right\}  $, we have%
\begin{align*}
\xi_{i}\left(  f_{j}\right)   &  =\left[  h_{i},f_{j}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\xi_{i}\right) \\
&  =-a_{i,j}\underbrace{f_{j}}_{\in\widetilde{\mathfrak{n}}_{-}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the relations (\ref{nonserre-relations}%
)}\right) \\
&  \in-a_{i,j}\widetilde{\mathfrak{n}}_{-}\subseteq\widetilde{\mathfrak{n}%
}_{-}.
\end{align*}
Thus, $\left\{  \xi_{i}\left(  f_{1}\right)  ,\xi_{i}\left(  f_{2}\right)
,...,\xi_{i}\left(  f_{n}\right)  \right\}  \subseteq\widetilde{\mathfrak{n}%
}_{-}$. Since $\left\{  \xi_{i}\left(  f_{1}\right)  ,\xi_{i}\left(
f_{2}\right)  ,...,\xi_{i}\left(  f_{n}\right)  \right\}  =\xi_{i}\left(
\left\{  f_{1},f_{2},...,f_{n}\right\}  \right)  $, this rewrites as $\xi
_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)  \subseteq
\widetilde{\mathfrak{n}}_{-}$, qed.}. Hence, Corollary
\ref{cor.derivation.Lie.unique.ihg} (applied to $\widetilde{\mathfrak{g}}$,
$\widetilde{\mathfrak{n}}_{-}$, $\widetilde{\mathfrak{n}}_{-}$, $\xi_{i}$ and
$\left\{  f_{1},f_{2},...,f_{n}\right\}  $ instead of $\mathfrak{g}$,
$\mathfrak{h}$, $\mathfrak{i}$, $d$ and $S$) yields that $\xi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{n}}_{-}$.
But%
\begin{align*}
\xi_{i}\left(  \widetilde{\mathfrak{n}}_{-}\right)   &  =\left\{
\underbrace{\xi_{i}\left(  x\right)  }_{=\left[  h_{i},x\right]  }\mid
x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left\{  \left[  h_{i},x\right]
\mid x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left[  h_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  =\left[  h_{i},\widetilde{\mathfrak{n}%
}_{-}\right]  \mathbb{C}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  h_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  \text{ is a vector space}\right) \\
&  =\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)  .
\end{align*}
Thus, $\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}$. Now, forget that we fixed $i$. We thus
have shown that $\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}$ for every $i\in\left\{
1,2,...,n\right\}  $.

But the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{h}%
}$ span the vector space $\widetilde{\mathfrak{h}}$. Thus,
$\widetilde{\mathfrak{h}}=\sum\limits_{i=1}^{n}\left(  h_{i}\mathbb{C}\right)
$, so that%
\begin{align*}
\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]   &
=\left[  \sum\limits_{i=1}^{n}\left(  h_{i}\mathbb{C}\right)
,\ \widetilde{\mathfrak{n}}_{-}\right]  =\sum\limits_{i=1}^{n}%
\underbrace{\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]
}_{\subseteq\widetilde{\mathfrak{n}}_{-}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since the Lie bracket is bilinear}\right) \\
&  \subseteq\sum\limits_{i=1}^{n}\widetilde{\mathfrak{n}}_{-}\subseteq
\widetilde{\mathfrak{n}}_{-}.
\end{align*}
This proves $\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}%
_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$.
\end{verlong}

\begin{vershort}
Now, $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
=\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}$, so that%
\begin{align*}
\left[  \widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}\right]   &
=\left[  \widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}%
,\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}\right] \\
&  =\underbrace{\left[  \widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{n}%
}_{-}\right]  }_{\substack{\subseteq\widetilde{\mathfrak{n}}_{-}\\\text{(since
}\widetilde{\mathfrak{n}}_{-}\text{ is a Lie algebra)}}}+\underbrace{\left[
\widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{h}}\right]  }_{=-\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq
\widetilde{\mathfrak{n}}_{-}}+\underbrace{\left[  \widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\right]  }_{\subseteq\widetilde{\mathfrak{n}%
}_{-}}+\underbrace{\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{h}%
}\right]  }_{\substack{\subseteq\widetilde{\mathfrak{h}}\\\text{(since
}\widetilde{\mathfrak{h}}\text{ is a Lie algebra)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)
\\
&  \subseteq\underbrace{\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{n}%
}_{-}+\widetilde{\mathfrak{n}}_{-}}_{\subseteq\widetilde{\mathfrak{n}}_{-}%
}+\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{n}}_{-}%
+\widetilde{\mathfrak{h}}=\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}.
\end{align*}
Thus, $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ is a Lie
subalgebra of $\widetilde{\mathfrak{g}}$.
\end{vershort}

\begin{verlong}
Now, $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
=\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}$ (since direct sums are
sums), so that
\begin{align*}
\left[  \widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}\right]   &
=\left[  \widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}%
,\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}\right]
=\underbrace{\left[  \widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{n}}%
_{-}\right]  }_{\subseteq\widetilde{\mathfrak{n}}_{-}}+\underbrace{\left[
\widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{h}}\right]  }_{=-\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq
\widetilde{\mathfrak{n}}_{-}}+\underbrace{\left[  \widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\right]  }_{\subseteq\widetilde{\mathfrak{n}%
}_{-}}+\underbrace{\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{h}%
}\right]  }_{\subseteq\widetilde{\mathfrak{h}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)
\\
&  \subseteq\underbrace{\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{n}%
}_{-}+\widetilde{\mathfrak{n}}_{-}}_{\subseteq\widetilde{\mathfrak{n}}_{-}%
}+\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{n}}_{-}%
+\widetilde{\mathfrak{h}}=\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}.
\end{align*}
Thus, $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ is a Lie
subalgebra of $\widetilde{\mathfrak{g}}$.
\end{verlong}

(Note that the map $\left(  \iota_{-},\iota_{0}\right)
:\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}\rightarrow
\widetilde{\mathfrak{g}}$ is actually a Lie algebra isomorphism from the
semidirect product $\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}$ (which was constructed during our proof of Theorem \ref{thm.gtilde}
\textbf{(b)}) to $\widetilde{\mathfrak{g}}$. But we will not need this fact,
so we will not prove it either.)

So we have shown that $\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}$ is a Lie subalgebra of $\widetilde{\mathfrak{g}}$. A
similar argument (but with $\widetilde{\mathfrak{n}}_{-}$ replaced by
$\widetilde{\mathfrak{n}}_{+}$, and with $f_{j}$ replaced by $e_{j}$, and with
$-a_{i,j}$ replaced by $a_{i,j}$) shows that $\widetilde{\mathfrak{n}}%
_{+}\oplus\widetilde{\mathfrak{h}}$ is a Lie subalgebra of
$\widetilde{\mathfrak{g}}$.

We now know that $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$
and $\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}$ are Lie
subalgebras of $\widetilde{\mathfrak{g}}$. Since $\widetilde{\mathfrak{n}}%
_{-}=\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $,
$\widetilde{\mathfrak{n}}_{+}=\iota_{+}\left(  \widetilde{\mathfrak{n}}%
_{+}\right)  $ and $\widetilde{\mathfrak{h}}=\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, this rewrites as follows: $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ and $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ are Lie subalgebras of
$\widetilde{\mathfrak{g}}$. This proves Theorem \ref{thm.gtilde} \textbf{(d)}.

\bigskip

\underline{\textit{4th step: Finishing the proof of Theorem \ref{thm.gtilde}
\textbf{(c)}.}}

We know that the internal direct sum $\widetilde{\mathfrak{n}}_{+}%
\oplus\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ makes sense.
Denote this direct sum $\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ as $V$. We know
that $V$ is a vector subspace of $\widetilde{\mathfrak{g}}$. We need to prove
that $V=\widetilde{\mathfrak{g}}$.

\begin{vershort}
Let $N$ be the vector subspace of $\widetilde{\mathfrak{g}}$ spanned by the
$3n$ elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$. Then, $\widetilde{\mathfrak{g}}$ is
generated by $N$ as a Lie algebra (because the elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ generate $\widetilde{\mathfrak{g}}$ as a Lie algebra).
\end{vershort}

\begin{verlong}
Let $N$ be the vector subspace of $\widetilde{\mathfrak{g}}$ spanned by the
$3n$ elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$. Then, $\widetilde{\mathfrak{g}}$ is
generated by $N$ as a Lie algebra (because the elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ generate $\widetilde{\mathfrak{g}}$ as a Lie algebra\footnote{This is
because $\widetilde{\mathfrak{g}}=\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)  $.}).
\end{verlong}

We will now prove that $\left[  N,V\right]  \subseteq V$.

Indeed, since $N=\sum\limits_{i=1}^{n}\left(  e_{i}\mathbb{C}\right)
+\sum\limits_{i=1}^{n}\left(  f_{i}\mathbb{C}\right)  +\sum\limits_{i=1}%
^{n}\left(  h_{i}\mathbb{C}\right)  $ (because $N$ is the vector subspace of
$\widetilde{\mathfrak{g}}$ spanned by the $3n$ elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$) and $V=\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}=\widetilde{\mathfrak{n}}_{+}%
+\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}$ (since direct sums are
sums), we have%
\begin{align}
\left[  N,V\right]   &  =\left[  \sum\limits_{i=1}^{n}\left(  e_{i}%
\mathbb{C}\right)  +\sum\limits_{i=1}^{n}\left(  f_{i}\mathbb{C}\right)
+\sum\limits_{i=1}^{n}\left(  h_{i}\mathbb{C}\right)
,\ \widetilde{\mathfrak{n}}_{+}+\widetilde{\mathfrak{n}}_{-}%
+\widetilde{\mathfrak{h}}\right] \nonumber\\
&  \subseteq\sum\limits_{i=1}^{n}\left[  e_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  +\sum\limits_{i=1}^{n}\left[
e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  +\sum\limits_{i=1}%
^{n}\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right] \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{i=1}^{n}\left[  f_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  +\sum\limits_{i=1}^{n}\left[
f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  +\sum\limits_{i=1}%
^{n}\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right] \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{i=1}^{n}\left[  h_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  +\sum\limits_{i=1}^{n}\left[
h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  +\sum\limits_{i=1}%
^{n}\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]
\label{pf.gtilde.c.1}%
\end{align}
(since the Lie bracket is bilinear).

We will now prove that each summand of each of the nine sums on the right hand
side of (\ref{pf.gtilde.c.1}) is $\subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq
V$\textit{:}

For every $i\in\left\{  1,2,...,n\right\}  $, we have $e_{i}\in
\widetilde{\mathfrak{n}}_{+}$ and thus $e_{i}\mathbb{C}\subseteq
\widetilde{\mathfrak{n}}_{+}$, so that%
\begin{align*}
\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]   &
\subseteq\left[  \widetilde{\mathfrak{n}}_{+},\ \widetilde{\mathfrak{n}}%
_{+}\right]  \subseteq\widetilde{\mathfrak{n}}_{+}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\widetilde{\mathfrak{n}}_{+}\text{ is a Lie algebra}\right) \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}+\widetilde{\mathfrak{n}}%
_{-}+\widetilde{\mathfrak{h}}=V.
\end{align*}
We have thus proven that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq
V$\textit{:}

\begin{vershort}
Let $i\in\left\{  1,2,...,n\right\}  $. Define a map $\psi_{i}%
:\widetilde{\mathfrak{g}}\rightarrow\widetilde{\mathfrak{g}}$ by%
\[
\left(  \psi_{i}\left(  x\right)  =\left[  e_{i},x\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }x\in\widetilde{\mathfrak{g}}\right)  .
\]
Then, $\psi_{i}$ is a Lie derivation of the Lie algebra
$\widetilde{\mathfrak{g}}$. On the other hand, the subset $\left\{
f_{1},f_{2},...,f_{n}\right\}  $ of $\widetilde{\mathfrak{n}}_{-}$ generates
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra (since the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{n}}_{-}$ generate
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra), and we can easily check that
$\psi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)
\subseteq\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
$\ \ \ \ \footnote{\textit{Proof.} For every $j\in\left\{  1,2,...,n\right\}
$, we have%
\begin{align*}
\psi_{i}\left(  f_{j}\right)   &  =\left[  e_{i},f_{j}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\psi_{i}\right) \\
&  =\delta_{i,j}\underbrace{h_{i}}_{\in\widetilde{\mathfrak{h}}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the relations (\ref{nonserre-relations}%
)}\right) \\
&  \in\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}.
\end{align*}
Thus, $\psi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)
\subseteq\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$, qed.}.
Hence, Corollary \ref{cor.derivation.Lie.unique.ihg} (applied to
$\widetilde{\mathfrak{g}}$, $\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}$, $\widetilde{\mathfrak{n}}_{-}$, $\psi_{i}$ and
$\left\{  f_{1},f_{2},...,f_{n}\right\}  $ instead of $\mathfrak{g}$,
$\mathfrak{h}$, $\mathfrak{i}$, $d$ and $S$) yields that $\psi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}$ (since $\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}$ is a Lie subalgebra of
$\widetilde{\mathfrak{g}}$). But by the definition of $\psi_{i}$, we have%
\begin{align*}
\psi_{i}\left(  \widetilde{\mathfrak{n}}_{-}\right)   &  =\left[
e_{i},\widetilde{\mathfrak{n}}_{-}\right]  =\left[  e_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  \mathbb{C}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\left[  e_{i},\widetilde{\mathfrak{n}}_{-}\right]  \text{ is a
vector space}\right) \\
&  =\left[  e_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)  .
\end{align*}
Thus, $\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
\subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}=V$. Now, forget that we fixed $i$. We thus have
shown that $\left[  e_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq V$ for every $i\in\left\{  1,2,...,n\right\}  $.
\end{vershort}

\begin{verlong}
Let $i\in\left\{  1,2,...,n\right\}  $. Define a map $\psi_{i}%
:\widetilde{\mathfrak{g}}\rightarrow\widetilde{\mathfrak{g}}$ by%
\[
\left(  \psi_{i}\left(  x\right)  =\left[  e_{i},x\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }x\in\widetilde{\mathfrak{g}}\right)  .
\]
Then, $\psi_{i}$ is a Lie derivation of the Lie algebra
$\widetilde{\mathfrak{g}}$. On the other hand, the subset $\left\{
f_{1},f_{2},...,f_{n}\right\}  $ of $\widetilde{\mathfrak{n}}_{-}$ generates
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra (since the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{n}}_{-}$ generate
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra), and we can easily check that
$\psi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)
\subseteq\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
$\ \ \ \ \footnote{\textit{Proof.} For every $j\in\left\{  1,2,...,n\right\}
$, we have%
\begin{align*}
\psi_{i}\left(  f_{j}\right)   &  =\left[  e_{i},f_{j}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\psi_{i}\right) \\
&  =\delta_{i,j}\underbrace{h_{i}}_{\in\widetilde{\mathfrak{h}}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the relations (\ref{nonserre-relations}%
)}\right) \\
&  \in\delta_{i,j}\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{h}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{\mathfrak{h}}\text{ is a
vector space}\right) \\
&  \subseteq\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}.
\end{align*}
Thus, $\left\{  \psi_{i}\left(  f_{1}\right)  ,\psi_{i}\left(  f_{2}\right)
,...,\psi_{i}\left(  f_{n}\right)  \right\}  \subseteq\widetilde{\mathfrak{n}%
}_{-}\oplus\widetilde{\mathfrak{h}}$. Since $\left\{  \psi_{i}\left(
f_{1}\right)  ,\psi_{i}\left(  f_{2}\right)  ,...,\psi_{i}\left(
f_{n}\right)  \right\}  =\psi_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}%
\right\}  \right)  $, this rewrites as $\psi_{i}\left(  \left\{  f_{1}%
,f_{2},...,f_{n}\right\}  \right)  \subseteq\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}$, qed.}. Hence, Corollary
\ref{cor.derivation.Lie.unique.ihg} (applied to $\widetilde{\mathfrak{g}}$,
$\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$,
$\widetilde{\mathfrak{n}}_{-}$, $\psi_{i}$ and $\left\{  f_{1},f_{2}%
,...,f_{n}\right\}  $ instead of $\mathfrak{g}$, $\mathfrak{h}$,
$\mathfrak{i}$, $d$ and $S$) yields that $\psi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}$ (since $\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}$ is a Lie subalgebra of
$\widetilde{\mathfrak{g}}$). But%
\begin{align*}
\psi_{i}\left(  \widetilde{\mathfrak{n}}_{-}\right)   &  =\left\{
\underbrace{\psi_{i}\left(  x\right)  }_{=\left[  e_{i},x\right]  }\mid
x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left\{  \left[  e_{i},x\right]
\mid x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left[  e_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  =\left[  e_{i},\widetilde{\mathfrak{n}%
}_{-}\right]  \mathbb{C}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  e_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  \text{ is a vector space}\right) \\
&  =\left[  e_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)  .
\end{align*}
Thus, $\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
\subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}=V$. Now, forget that we fixed $i$. We thus have
shown that $\left[  e_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq V$ for every $i\in\left\{  1,2,...,n\right\}  $.
\end{verlong}

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]  \subseteq
V$\textit{:}

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $e_{i}\mathbb{C}%
\subseteq\widetilde{\mathfrak{n}}_{+}$ (since $e_{i}\in\widetilde{\mathfrak{n}%
}_{+}$). Thus, every $i\in\left\{  1,2,...,n\right\}  $ satisfies%
\begin{align*}
\left[  \underbrace{e_{i}\mathbb{C}}_{\subseteq\widetilde{\mathfrak{n}}%
_{+}\subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}%
},\ \underbrace{\widetilde{\mathfrak{h}}}_{\subseteq\widetilde{\mathfrak{n}%
}_{+}\oplus\widetilde{\mathfrak{h}}}\right]   &  \subseteq\left[
\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}%
,\ \widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}\right] \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{\mathfrak{n}}_{+}%
\oplus\widetilde{\mathfrak{h}}\text{ is a Lie subalgebra of }%
\widetilde{\mathfrak{g}}\right) \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}=V.
\end{align*}


\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq
V$\textit{:}

\begin{vershort}
We have proven above that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq V$.
An analogous argument (or an invocation of the automorphism guaranteed by
Theorem \ref{thm.gtilde} \textbf{(f)}) shows that every $i\in\left\{
1,2,...,n\right\}  $ satisfies $\left[  f_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq V$.
\end{vershort}

\begin{verlong}
We have proven above that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq V$.
A similar argument (but with $\widetilde{\mathfrak{n}}_{-}$ replaced by
$\widetilde{\mathfrak{n}}_{+}$, and with $e_{i}$ replaced by $f_{i}$, and with
$f_{j}$ replaced by $e_{j}$, and with $h_{i}$ replaced by $-h_{i}$, and with
(\ref{nonserre-relations}) replaced by (\ref{nonserre-relations2})) shows that
every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\left[  f_{i}%
\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq V$.
\end{verlong}

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq
V$\textit{:}

We have proven above that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq V$.
A similar argument (but with $\widetilde{\mathfrak{n}}_{+}$ replaced by
$\widetilde{\mathfrak{n}}_{-}$, and with $e_{i}$ replaced by $f_{i}$) shows
that every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\left[
f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]  \subseteq
V$\textit{:}

We have proven above that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]  \subseteq V$. A
similar argument (but with $\widetilde{\mathfrak{n}}_{+}$ replaced by
$\widetilde{\mathfrak{n}}_{-}$, and with $e_{i}$ replaced by $f_{i}$) shows
that every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\left[
f_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]  \subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq
V$\textit{:}

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $h_{i}\mathbb{C}%
\subseteq\widetilde{\mathfrak{h}}$ (since $h_{i}\in\widetilde{\mathfrak{h}}$).
Thus, every $i\in\left\{  1,2,...,n\right\}  $ satisfies%
\begin{align*}
\left[  \underbrace{h_{i}\mathbb{C}}_{\subseteq\widetilde{\mathfrak{n}}%
_{+}\subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}%
},\ \underbrace{\widetilde{\mathfrak{n}}_{+}}_{\subseteq
\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}}\right]   &
\subseteq\left[  \widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}%
},\ \widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}\right] \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{\mathfrak{n}}_{+}%
\oplus\widetilde{\mathfrak{h}}\text{ is a Lie subalgebra of }%
\widetilde{\mathfrak{g}}\right) \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}=V.
\end{align*}


\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq
V$\textit{:}

We have proven above that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq V$.
A similar argument (but with $\widetilde{\mathfrak{n}}_{+}$ replaced by
$\widetilde{\mathfrak{n}}_{-}$) shows that every $i\in\left\{
1,2,...,n\right\}  $ satisfies $\left[  h_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]  \subseteq
V$\textit{:}

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $h_{i}\mathbb{C}%
\subseteq\widetilde{\mathfrak{h}}$ (since $h_{i}\in\widetilde{\mathfrak{h}}$).
Thus, every $i\in\left\{  1,2,...,n\right\}  $ satisfies%
\begin{align*}
\left[  \underbrace{h_{i}\mathbb{C}}_{\subseteq\widetilde{\mathfrak{n}}%
_{+}\subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}%
},\ \underbrace{\widetilde{\mathfrak{h}}}_{\subseteq\widetilde{\mathfrak{n}%
}_{+}\oplus\widetilde{\mathfrak{h}}}\right]   &  \subseteq\left[
\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}%
,\ \widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}}\right] \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{h}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{\mathfrak{n}}_{+}%
\oplus\widetilde{\mathfrak{h}}\text{ is a Lie subalgebra of }%
\widetilde{\mathfrak{g}}\right) \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}=V.
\end{align*}


Thus, we have proven that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
the nine relations $\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}%
_{+}\right]  \subseteq V$, $\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}%
}_{-}\right]  \subseteq V$, $\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{h}%
}\right]  \subseteq V$, $\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}%
}_{+}\right]  \subseteq V$, $\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}%
}_{-}\right]  \subseteq V$, $\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{h}%
}\right]  \subseteq V$, $\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}%
}_{+}\right]  \subseteq V$, $\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}%
}_{-}\right]  \subseteq V$, and $\left[  h_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{h}}\right]  \subseteq V$. Thus, (\ref{pf.gtilde.c.1})
becomes%
\begin{align*}
\left[  N,V\right]   &  \subseteq\sum\limits_{i=1}^{n}\underbrace{\left[
e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  }_{\subseteq V}%
+\sum\limits_{i=1}^{n}\underbrace{\left[  e_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{-}\right]  }_{\subseteq V}+\sum\limits_{i=1}%
^{n}\underbrace{\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]
}_{\subseteq V}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{i=1}^{n}\underbrace{\left[
f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  }_{\subseteq V}%
+\sum\limits_{i=1}^{n}\underbrace{\left[  f_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{-}\right]  }_{\subseteq V}+\sum\limits_{i=1}%
^{n}\underbrace{\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]
}_{\subseteq V}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{i=1}^{n}\underbrace{\left[
h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  }_{\subseteq V}%
+\sum\limits_{i=1}^{n}\underbrace{\left[  h_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{-}\right]  }_{\subseteq V}+\sum\limits_{i=1}%
^{n}\underbrace{\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]
}_{\subseteq V}\\
&  \subseteq\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}%
^{n}V+\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}%
^{n}V+\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}^{n}V\\
&  \subseteq V
\end{align*}
(since $V$ is a vector space). This proves $\left[  N,V\right]  \subseteq V$.

Moreover,
\begin{align*}
N  &  =\sum\limits_{i=1}^{n}\underbrace{\left(  e_{i}\mathbb{C}\right)
}_{\substack{\subseteq V\\\text{(since }e_{i}\in\widetilde{\mathfrak{n}}%
_{+}\subseteq\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}=V\text{)}}}+\sum\limits_{i=1}%
^{n}\underbrace{\left(  f_{i}\mathbb{C}\right)  }_{\substack{\subseteq
V\\\text{(since }f_{i}\in\widetilde{\mathfrak{n}}_{-}\subseteq
\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}=V\text{)}}}+\sum\limits_{i=1}^{n}\underbrace{\left(
h_{i}\mathbb{C}\right)  }_{\substack{\subseteq V\\\text{(since }h_{i}%
\in\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{n}}_{+}%
\oplus\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}=V\text{)}}}\\
&  \subseteq\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}^{n}V+\sum\limits_{i=1}%
^{n}V\subseteq V
\end{align*}
(since $V$ is a vector space).

So we know that $N$ and $V$ are vector subspaces of $\widetilde{\mathfrak{g}}$
such that $\widetilde{\mathfrak{g}}$ is generated by $N$ as a Lie algebra and
such that $N\subseteq V$ and $\left[  N,V\right]  \subseteq V$. Hence, Lemma
\ref{lem.generation.1} (applied to $\widetilde{\mathfrak{g}}$, $N$ and $V$
instead of $\mathfrak{g}$, $T$ and $U$) yields $V=\widetilde{\mathfrak{g}}$.
Thus, $\widetilde{\mathfrak{g}}=V=\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}=\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ (since $\widetilde{\mathfrak{n}}_{-}%
=\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $,
$\widetilde{\mathfrak{n}}_{+}=\iota_{+}\left(  \widetilde{\mathfrak{n}}%
_{+}\right)  $ and $\widetilde{\mathfrak{h}}=\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $). This proves Theorem \ref{thm.gtilde}
\textbf{(c)}.

\bigskip

\textbf{(d)} During the proof of Theorem \ref{thm.gtilde} \textbf{(c)}, we
have already proven Theorem \ref{thm.gtilde} \textbf{(d)}.

\bigskip

\textbf{(e)} We will use the notations we introduced in our proof of Theorem
\ref{thm.gtilde} \textbf{(d)}. During this proof, we have shown that
$\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  >0\right]  $, $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{g}}\left[
<0\right]  $ and $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
\subseteq\widetilde{\mathfrak{g}}\left[  0\right]  $. Also, we know that
$\widetilde{\mathfrak{g}}=\iota_{+}\left(  \widetilde{\mathfrak{n}}%
_{+}\right)  \oplus\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
\oplus\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. Finally, we know
that the internal direct sum $\widetilde{\mathfrak{g}}\left[  >0\right]
\oplus\widetilde{\mathfrak{g}}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  $ is well-defined.

\begin{vershort}
Now, a simple fact from linear algebra says the following: If $U_{1}$, $U_{2}%
$, $U_{3}$, $V_{1}$, $V_{2}$, $V_{3}$ are six vector subspaces of a vector
space $V$ satisfying the four relations $U_{1}\subseteq V_{1}$, $U_{2}%
\subseteq V_{2}$, $U_{3}\subseteq V_{3}$ and $V=U_{1}\oplus U_{2}\oplus U_{3}%
$, and if the internal direct sum $V_{1}\oplus V_{2}\oplus V_{3}$ is
well-defined, then we must have $U_{1}=V_{1}$, $U_{2}=V_{2}$ and $U_{3}=V_{3}$.
\end{vershort}

\begin{verlong}
Now, a simple fact from linear algebra says the following: If $U_{1}$, $U_{2}%
$, $U_{3}$, $V_{1}$, $V_{2}$, $V_{3}$ are six vector subspaces of a vector
space $V$ satisfying the four relations $U_{1}\subseteq V_{1}$, $U_{2}%
\subseteq V_{2}$, $U_{3}\subseteq V_{3}$ and $V=U_{1}\oplus U_{2}\oplus U_{3}%
$, and if the internal direct sum $V_{1}\oplus V_{2}\oplus V_{3}$ is
well-defined, then we must have $U_{1}=V_{1}$, $U_{2}=V_{2}$ and $U_{3}=V_{3}%
$\ \ \ \ \footnote{\textit{Proof.} Let $U_{1}$, $U_{2}$, $U_{3}$, $V_{1}$,
$V_{2}$, $V_{3}$ be six vector subspaces of a vector space $V$ satisfying the
four relations $U_{1}\subseteq V_{1}$, $U_{2}\subseteq V_{2}$, $U_{3}\subseteq
V_{3}$ and $V=V_{1}\oplus V_{2}\oplus V_{3}$. Assume that the internal direct
sum $V_{1}\oplus V_{2}\oplus V_{3}$ is well-defined.
\par
Let $x\in V_{1}$. Then, $x\in V_{1}\subseteq V=U_{1}\oplus U_{2}\oplus U_{3}$.
Hence, there exist $x_{1}\in U_{1}$, $x_{2}\in U_{2}$ and $x_{3}\in U_{3}$
such that $x=x_{1}+x_{2}+x_{3}$. Consider these $x_{1}$, $x_{2}$ and $x_{3}$.
Since $x=x_{1}+x_{2}+x_{3}$, we have $x-x_{1}=\underbrace{x_{2}}_{\in
U_{2}\subseteq V_{2}}+\underbrace{x_{3}}_{\in U_{3}\subseteq V_{3}}\subseteq
V_{2}+V_{3}$. Combined with $\underbrace{x}_{\in V_{1}}-\underbrace{x_{1}%
}_{\in U_{1}\subseteq V_{1}}\in V_{1}-V_{1}\subseteq V_{1}$, this yields
\[
x-x_{1}\in\left(  V_{2}+V_{3}\right)  \cap V_{1}.
\]
\par
On the other hand, the internal direct sum $V_{1}\oplus V_{2}\oplus V_{3}$ is
well-defined. This direct sum rewrites as $V_{1}\oplus V_{2}\oplus
V_{3}=\underbrace{V_{2}\oplus V_{3}}_{\substack{=V_{2}+V_{3}\\\text{(since
direct sums are sums)}}}\oplus V_{1}=\left(  V_{2}+V_{3}\right)  \oplus V_{1}%
$. Hence, the direct sum $\left(  V_{2}+V_{3}\right)  \oplus V_{1}$ is
well-defined. Thus, $\left(  V_{2}+V_{3}\right)  \cap V_{1}=0$.
\par
But we have $x-x_{1}\in\left(  V_{2}+V_{3}\right)  \cap V_{1}$. In view of
$\left(  V_{2}+V_{3}\right)  \cap V_{1}=0$, this rewrites as $x-x_{1}\in0$.
Hence, $x-x_{1}=0$, so that $x=x_{1}\in U_{1}$.
\par
Now forget that we fixed $x$. We have thus proven that $x\in U_{1}$ for every
$x\in V_{1}$. In other words, $V_{1}\subseteq U_{1}$. Combined with
$U_{1}\subseteq V_{1}$, this yields $U_{1}=V_{1}$. Similarly, $U_{2}=V_{2}$
and $U_{3}=V_{3}$, qed.}.
\end{verlong}

If we apply this fact to $\widetilde{\mathfrak{g}}$, $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  $, $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $, $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, $\widetilde{\mathfrak{g}}\left[
>0\right]  $, $\widetilde{\mathfrak{g}}\left[  <0\right]  $,
$\widetilde{\mathfrak{g}}\left[  0\right]  $ instead of $V$, $U_{1}$, $U_{2}$,
$U_{3}$, $V_{1}$, $V_{2}$, $V_{3}$, then we obtain that $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  =\widetilde{\mathfrak{g}}\left[
>0\right]  $, $\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
=\widetilde{\mathfrak{g}}\left[  <0\right]  $ and $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{g}}\left[  0\right]  $
(because we know that $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
$, $\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $, $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, $\widetilde{\mathfrak{g}}\left[
>0\right]  $, $\widetilde{\mathfrak{g}}\left[  <0\right]  $,
$\widetilde{\mathfrak{g}}\left[  0\right]  $ are six vector subspaces of
$\widetilde{\mathfrak{g}}$ satisfying the four relations $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \subseteq\widetilde{\mathfrak{g}}\left[
>0\right]  $, $\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
\subseteq\widetilde{\mathfrak{g}}\left[  <0\right]  $, $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  \subseteq\widetilde{\mathfrak{g}}\left[
0\right]  $ and $\widetilde{\mathfrak{g}}=\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, and we know that the internal direct sum
$\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}}\left[  0\right]  $ is well-defined).

So we have proven that $\widetilde{\mathfrak{g}}\left[  0\right]  =\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. In other words, the $0$-th
homogeneous component of $\widetilde{\mathfrak{g}}$ (in the $Q$-grading) is
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $.

On the other hand, we have proven that $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  =\widetilde{\mathfrak{g}}\left[
>0\right]  $. Thus,%
\[
\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  =\widetilde{\mathfrak{g}%
}\left[  >0\right]  =\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha
>0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  =\bigoplus
\limits_{\substack{\alpha\text{ is a }\mathbb{Z}\text{-linear combination}%
\\\text{of }\alpha_{1}\text{, }\alpha_{2}\text{, }...\text{, }\alpha_{n}\text{
with nonnegative}\\\text{coefficients; }\alpha\neq0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]
\]
(since an element $\alpha\in Q$ satisfies $\alpha>0$ if and only if $\alpha$
is a $\mathbb{Z}$-linear combination of $\alpha_{1}$, $\alpha_{2}$, $...$,
$\alpha_{n}$ with nonnegative coefficients such that $\alpha\neq0$).

\begin{vershort}
Similarly, $\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
=\bigoplus\limits_{\substack{\alpha\text{ is a }\mathbb{Z}\text{-linear
combination}\\\text{of }\alpha_{1}\text{, }\alpha_{2}\text{, }...\text{,
}\alpha_{n}\text{ with nonpositive}\\\text{coefficients; }\alpha\neq
0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  $.
\end{vershort}

\begin{verlong}
Also, we have proven that $\iota_{-}\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  =\widetilde{\mathfrak{g}}\left[  <0\right]  $. Hence,%
\[
\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  =\widetilde{\mathfrak{g}%
}\left[  <0\right]  =\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha
<0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  =\bigoplus
\limits_{\substack{\alpha\text{ is a }\mathbb{Z}\text{-linear combination}%
\\\text{of }\alpha_{1}\text{, }\alpha_{2}\text{, }...\text{, }\alpha_{n}\text{
with nonpositive}\\\text{coefficients; }\alpha\neq0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]
\]
(since an element $\alpha\in Q$ satisfies $\alpha<0$ if and only if $\alpha$
is a $\mathbb{Z}$-linear combination of $\alpha_{1}$, $\alpha_{2}$, $...$,
$\alpha_{n}$ with nonpositive coefficients such that $\alpha\neq0$).
\end{verlong}

This completes the proof of Theorem \ref{thm.gtilde} \textbf{(e)}.

\bigskip

\textbf{(g)} Define a $\mathbb{Z}$-linear map $\ell:Q\rightarrow\mathbb{Z}$ by%
\[
\left(  \ell\left(  \alpha_{i}\right)  =1\text{ for every }i\in\left\{
1,2,...,n\right\}  \right)  .
\]
(This is well-defined since $Q$ is a free abelian group with generators
$\alpha_{1}$, $\alpha_{2}$, $...$, $\alpha_{n}$.) Then, $\ell$ is a group homomorphism.

We will use the notations we introduced in our proof of Theorem
\ref{thm.gtilde} \textbf{(c)}. As shown in the proof of Theorem
\ref{thm.gtilde} \textbf{(e)}, we have $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  =\widetilde{\mathfrak{g}}\left[
>0\right]  $, $\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
=\widetilde{\mathfrak{g}}\left[  <0\right]  $ and $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{g}}\left[  0\right]  $.

Just as in the proof of Theorem \ref{thm.gtilde} \textbf{(c)}, we will regard
the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ as inclusions. Thus,
$\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
=\widetilde{\mathfrak{n}}_{+}$, $\iota_{-}\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$ and $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{h}}$.

From the proof of Theorem \ref{thm.gtilde} \textbf{(c)}, we know that
$\widetilde{\mathfrak{g}}\left[  0\right]  $, $\widetilde{\mathfrak{g}}\left[
<0\right]  $ and $\widetilde{\mathfrak{g}}\left[  >0\right]  $ are $Q$-graded
Lie subalgebras of $\widetilde{\mathfrak{g}}$. Since $\widetilde{\mathfrak{g}%
}\left[  0\right]  =\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
=\widetilde{\mathfrak{h}}$, $\widetilde{\mathfrak{g}}\left[  <0\right]
=\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)
=\widetilde{\mathfrak{n}}_{-}$ and $\widetilde{\mathfrak{g}}\left[  >0\right]
=\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
=\widetilde{\mathfrak{n}}_{+}$, this rewrites as follows:
$\widetilde{\mathfrak{h}}$, $\widetilde{\mathfrak{n}}_{-}$ and
$\widetilde{\mathfrak{n}}_{+}$ are $Q$-graded Lie subalgebras of
$\widetilde{\mathfrak{g}}$.

Fix $i\in\left\{  1,2,...,n\right\}  $. Since $\alpha_{i}>0$, the space
$\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  $ is an addend in the
direct sum $\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha>0}%
}\widetilde{\mathfrak{g}}\left[  \alpha\right]  $ (namely, the addend for
$\alpha=\alpha_{i}$). Hence, $\widetilde{\mathfrak{g}}\left[  \alpha
_{i}\right]  \subseteq\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha
>0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  =\widetilde{\mathfrak{g}%
}\left[  >0\right]  =\widetilde{\mathfrak{n}}_{+}$. But since
$\widetilde{\mathfrak{n}}_{+}$ is a $Q$-graded vector subspace of
$\widetilde{\mathfrak{g}}$, we have $\widetilde{\mathfrak{n}}_{+}\left[
\alpha_{i}\right]  =\left(  \widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]
\right)  \cap\widetilde{\mathfrak{n}}_{+}=\widetilde{\mathfrak{g}}\left[
\alpha_{i}\right]  $ (since $\widetilde{\mathfrak{g}}\left[  \alpha
_{i}\right]  \subseteq\widetilde{\mathfrak{n}}_{+}$).

\begin{vershort}
Now, $\widetilde{\mathfrak{n}}_{+}$ is a $Q$-graded Lie algebra, and $\ell$ is
a group homomorphism. Hence, we can apply Proposition
\ref{prop.Q-graded.principal} to $\widetilde{\mathfrak{n}}_{+}$ instead of
$\widetilde{\mathfrak{g}}$. Applying Proposition \ref{prop.Q-graded.principal}
\textbf{(a)} to $\widetilde{\mathfrak{n}}_{+}$ instead of $\mathfrak{g}$, we
see that for every $m\in\mathbb{Z}$, the internal direct sum $\bigoplus
\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m}%
}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]  $ is well-defined. Denote
this internal direct sum $\bigoplus\limits_{\substack{\alpha\in Q;\\\ell
\left(  \alpha\right)  =m}}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]
$ by $\widetilde{\mathfrak{n}}_{+\left[  m\right]  }$. Applying Proposition
\ref{prop.Q-graded.principal} \textbf{(b)} to $\widetilde{\mathfrak{n}}_{+}$
instead of $\mathfrak{g}$, we see that the Lie algebra
$\widetilde{\mathfrak{n}}_{+}$ equipped with the grading $\left(
\widetilde{\mathfrak{n}}_{+\left[  m\right]  }\right)  _{m\in\mathbb{Z}}$ is a
$\mathbb{Z}$-graded Lie algebra.

Let $N_{+}$ be the free vector space with basis $e_{1},e_{2},...,e_{n}$. Since
$\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(  e_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $, we then have a canonical
isomorphism $\widetilde{\mathfrak{n}}_{+}\cong\operatorname*{FreeLie}\left(
N_{+}\right)  $ (where $\operatorname*{FreeLie}\left(  N_{+}\right)  $ means
the free Lie algebra over the vector space (not the set) $N_{+}$). We identify
$\widetilde{\mathfrak{n}}_{+}$ with $\operatorname*{FreeLie}\left(
N_{+}\right)  $ along this isomorphism. Due to the construction of the free
Lie algebra, we have a canonical injection $N_{+}\rightarrow
\operatorname*{FreeLie}\left(  N_{+}\right)  =\widetilde{\mathfrak{n}}_{+}$.
We will regard this injection as an inclusion (so that $N_{+}\subseteq
\widetilde{\mathfrak{n}}_{+}$).

Since $\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(
N_{+}\right)  $, it is clear that $\widetilde{\mathfrak{n}}_{+}$ is generated
by $N_{+}$ as a Lie algebra.

Clearly, $e_{j}\in\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{j}\right]
\subseteq\widetilde{\mathfrak{n}}_{+\left[  1\right]  }$ for every
$j\in\left\{  1,2,...,n\right\}  $. Thus, $N_{+}\subseteq
\widetilde{\mathfrak{n}}_{+\left[  1\right]  }$. Combining this with the fact
that $\widetilde{\mathfrak{n}}_{+}$ is generated by $N_{+}$ as a Lie algebra,
we see that we can apply Theorem \ref{thm.FreeLie.grading1} to the Lie algebra
$\widetilde{\mathfrak{n}}_{+}$ (with the $\mathbb{Z}$-grading $\left(
\widetilde{\mathfrak{n}}_{+\left[  m\right]  }\right)  _{m\in\mathbb{Z}}$, not
with the original $Q$-grading) and $N_{+}$ instead of the Lie algebra
$\mathfrak{g}$ and $T$. As a result, we obtain $N_{+}=\widetilde{\mathfrak{n}%
}_{+\left[  1\right]  }$. Since $\widetilde{\mathfrak{g}}\left[  \alpha
_{i}\right]  =\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{i}\right]
\subseteq\widetilde{\mathfrak{n}}_{+\left[  1\right]  }=N_{+}$, we have
$\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  =N_{+}\left[  \alpha
_{i}\right]  $ (since $N_{+}$ is a $Q$-graded subspace of
$\widetilde{\mathfrak{g}}$). But $N_{+}\left[  \alpha_{i}\right]
=\mathbb{C}e_{i}$ (this is clear from the fact that $N_{+}$ has basis
$e_{1},e_{2},...,e_{n}$, and each of the vectors in this basis has a different
degree in the $Q$-grading). Hence, $\widetilde{\mathfrak{g}}\left[  \alpha
_{i}\right]  =N_{+}\left[  \alpha_{i}\right]  =\mathbb{C}e_{i}$. A similar
argument (with $-\ell$ taking the role of $\ell$) shows that
$\widetilde{\mathfrak{g}}\left[  -\alpha_{i}\right]  =\mathbb{C}f_{i}$. This
proves Theorem \ref{thm.gtilde} \textbf{(g)}.
\end{vershort}

\begin{verlong}
Now, $\widetilde{\mathfrak{n}}_{+}$ is a $Q$-graded Lie algebra, and $\ell$ is
a group homomorphism. Hence, we can apply Proposition
\ref{prop.Q-graded.principal} to $\widetilde{\mathfrak{n}}_{+}$ instead of
$\widetilde{\mathfrak{g}}$. Applying Proposition \ref{prop.Q-graded.principal}
\textbf{(a)} to $\widetilde{\mathfrak{n}}_{+}$ instead of $\mathfrak{g}$, we
see that for every $m\in\mathbb{Z}$, the internal direct sum $\bigoplus
\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =m}%
}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]  $ is well-defined. Denote
this internal direct sum $\bigoplus\limits_{\substack{\alpha\in Q;\\\ell
\left(  \alpha\right)  =m}}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]
$ by $\widetilde{\mathfrak{n}}_{+\left[  m\right]  }$. Applying Proposition
\ref{prop.Q-graded.principal} \textbf{(b)} to $\widetilde{\mathfrak{n}}_{+}$
instead of $\mathfrak{g}$, we see that the Lie algebra
$\widetilde{\mathfrak{n}}_{+}$ equipped with the grading $\left(
\widetilde{\mathfrak{n}}_{+\left[  m\right]  }\right)  _{m\in\mathbb{Z}}$ is a
$\mathbb{Z}$-graded Lie algebra. Denote this $\mathbb{Z}$-graded Lie algebra
by $\widetilde{\mathfrak{n}}_{+}^{\operatorname*{principal}}$. Then,
$\widetilde{\mathfrak{n}}_{+}^{\operatorname*{principal}}\left[  m\right]
=\widetilde{\mathfrak{n}}_{+\left[  m\right]  }$ for every $m\in\mathbb{Z}$.
Applied to $m=1$, this yields $\widetilde{\mathfrak{n}}_{+}%
^{\operatorname*{principal}}\left[  1\right]  =\widetilde{\mathfrak{n}%
}_{+\left[  1\right]  }$.

We have $\widetilde{\mathfrak{n}}_{+\left[  1\right]  }=\bigoplus
\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)  =1}%
}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]  $ (by the definition of
$\widetilde{\mathfrak{n}}_{+\left[  1\right]  }$).

For every $j\in\left\{  1,2,...,n\right\}  $, the vector space
$\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{j}\right]  $ is an addend in the
direct sum $\bigoplus\limits_{\substack{\alpha\in Q;\\\ell\left(
\alpha\right)  =1}}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]  $
(namely, the addend for $\alpha=\alpha_{j}$), because $\ell\left(  \alpha
_{j}\right)  =1$. Thus, for every $j\in\left\{  1,2,...,n\right\}  $, we have
$\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{j}\right]  \subseteq
\bigoplus\limits_{\substack{\alpha\in Q;\\\ell\left(  \alpha\right)
=1}}\widetilde{\mathfrak{n}}_{+}\left[  \alpha\right]
=\widetilde{\mathfrak{n}}_{+\left[  1\right]  }$. Applied to $j=i$, this
yields $\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{i}\right]  \subseteq
\widetilde{\mathfrak{n}}_{+\left[  1\right]  }$.

Let $N_{+}$ be the free vector space with basis $e_{1},e_{2},...,e_{n}$. Since
$\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(  e_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $, we then have a canonical
isomorphism $\widetilde{\mathfrak{n}}_{+}\cong\operatorname*{FreeLie}\left(
N_{+}\right)  $ (where $\operatorname*{FreeLie}\left(  N_{+}\right)  $ means
the free Lie algebra over the vector space (not the set) $N_{+}$). We identify
$\widetilde{\mathfrak{n}}_{+}$ with $\operatorname*{FreeLie}\left(
N_{+}\right)  $ along this isomorphism. Due to the construction of the free
Lie algebra, we have a canonical injection $N_{+}\rightarrow
\operatorname*{FreeLie}\left(  N_{+}\right)  =\widetilde{\mathfrak{n}}_{+}$.
We will regard this injection as an inclusion (so that $N_{+}\subseteq
\widetilde{\mathfrak{n}}_{+}$).

Since $\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(
N_{+}\right)  $, it is clear that $\widetilde{\mathfrak{n}}_{+}$ is generated
by $N_{+}$ as a Lie algebra. In other words, $\widetilde{\mathfrak{n}}%
_{+}^{\operatorname*{principal}}$ is generated by $N_{+}$ as a Lie algebra
(since $\widetilde{\mathfrak{n}}_{+}^{\operatorname*{principal}}%
=\widetilde{\mathfrak{n}}_{+}$ as Lie algebra).

Since $N_{+}$ is the free vector space with basis $e_{1},e_{2},...,e_{n}$, we
have $N_{+}=\bigoplus\limits_{j=1}^{n}e_{j}\mathbb{C}$.

For every $j\in\left\{  1,2,...,n\right\}  $, we have $\deg\left(
e_{j}\right)  =\alpha_{j}$ (by the definition of the $Q$-grading on
$\widetilde{\mathfrak{g}}$) and thus $e_{j}\in\widetilde{\mathfrak{n}}%
_{+}\left[  \alpha_{j}\right]  $ (since $e_{j}\in\widetilde{\mathfrak{n}}_{+}%
$). Hence, for every $j\in\left\{  1,2,...,n\right\}  $, we have
$e_{j}\mathbb{C}\subseteq\widetilde{\mathfrak{n}}_{+}\left[  \alpha
_{j}\right]  $. Thus, for every $j\in\left\{  1,2,...,n\right\}  $, the vector
subspace $e_{j}\mathbb{C}$ of $\widetilde{\mathfrak{n}}_{+}$ lies entirely
within one homogeneous component of $\widetilde{\mathfrak{n}}_{+}$. Thus, for
every $j\in\left\{  1,2,...,n\right\}  $, the vector subspace $e_{j}%
\mathbb{C}$ is a $Q$-graded vector subspace of $\widetilde{\mathfrak{n}}_{+}$.
Therefore, the internal direct sum $\bigoplus\limits_{j=1}^{n}e_{j}\mathbb{C}$
also is a $Q$-graded vector subspace of $\widetilde{\mathfrak{n}}_{+}$
(because any internal direct sum of $Q$-graded vector subspaces of a
$Q$-graded vector space must itself be a $Q$-graded vector subspace). Since
$\bigoplus\limits_{j=1}^{n}e_{j}\mathbb{C}=N_{+}$, this means that $N_{+}$ is
a $Q$-graded vector subspace of $\widetilde{\mathfrak{n}}_{+}$.

For every $j\in\left\{  1,2,...,n\right\}  $ satisfying $j\neq i$, we have
$\left(  e_{j}\mathbb{C}\right)  \left[  \alpha_{i}\right]  =0$ (because
\begin{align*}
\left(  e_{j}\mathbb{C}\right)  \left[  \alpha_{i}\right]   &
=\underbrace{\left(  e_{j}\mathbb{C}\right)  }_{\subseteq
\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{j}\right]  }\cap\left(
\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{i}\right]  \right)  \subseteq
\left(  \widetilde{\mathfrak{n}}_{+}\left[  \alpha_{j}\right]  \right)
\cap\left(  \widetilde{\mathfrak{n}}_{+}\left[  \alpha_{i}\right]  \right)
=0\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j\neq i\text{, so that }\alpha_{j}\neq\alpha_{i}\text{, and thus
the }\alpha_{j}\text{-th and the}\\
\alpha_{i}\text{-th homogeneous components of }\widetilde{\mathfrak{n}}%
_{+}\text{ are linearly}\\
\text{disjoint, i. e., they satisfy }\left(  \widetilde{\mathfrak{n}}%
_{+}\left[  \alpha_{j}\right]  \right)  \cap\left(  \widetilde{\mathfrak{n}%
}_{+}\left[  \alpha_{i}\right]  \right)  =0
\end{array}
\right)
\end{align*}
and thus $\left(  e_{j}\mathbb{C}\right)  \left[  \alpha_{i}\right]  =0$).
Moreover, we know that for every $j\in\left\{  1,2,...,n\right\}  $, we have
$e_{j}\mathbb{C}\subseteq\widetilde{\mathfrak{n}}_{+}\left[  \alpha
_{j}\right]  $. Applied to $j=i$, this yields $e_{i}\mathbb{C}\subseteq
\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{i}\right]  $. Now, $\left(
e_{i}\mathbb{C}\right)  \left[  \alpha_{i}\right]  =\left(  e_{i}%
\mathbb{C}\right)  \cap\left(  \widetilde{\mathfrak{n}}_{+}\left[  \alpha
_{i}\right]  \right)  =e_{i}\mathbb{C}$ (since $e_{i}\mathbb{C}\subseteq
\widetilde{\mathfrak{n}}_{+}\left[  \alpha_{i}\right]  $).

Now,
\begin{align*}
N_{+}\left[  \alpha_{i}\right]   &  =\left(  \bigoplus\limits_{j=1}^{n}%
e_{j}\mathbb{C}\right)  \left[  \alpha_{i}\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }N_{+}=\bigoplus\limits_{j=1}^{n}e_{j}\mathbb{C}\right) \\
&  =\bigoplus\limits_{j=1}^{n}\left(  \left(  e_{j}\mathbb{C}\right)  \left[
\alpha_{i}\right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }e_{j}\mathbb{C}\text{ is a
}Q\text{-graded vector subspace of }\widetilde{\mathfrak{n}}_{+}\text{ for
every }j\in\left\{  1,2,...,n\right\}  \right) \\
&  =\bigoplus\limits_{j\in\left\{  1,2,...,n\right\}  }\left(  \left(
e_{j}\mathbb{C}\right)  \left[  \alpha_{i}\right]  \right)  =\left(
\underbrace{\left(  e_{i}\mathbb{C}\right)  \left[  \alpha_{i}\right]
}_{=e_{i}\mathbb{C}}\right)  \oplus\bigoplus\limits_{\substack{j\in\left\{
1,2,...,n\right\}  ;\\j\neq i}}\left(  \underbrace{\left(  e_{j}%
\mathbb{C}\right)  \left[  \alpha_{i}\right]  }_{\substack{=0\\\text{(since
}j\neq i\text{)}}}\right) \\
&  =\left(  e_{i}\mathbb{C}\right)  \oplus\underbrace{\bigoplus
\limits_{\substack{j\in\left\{  1,2,...,n\right\}  ;\\j\neq i}}0}_{=0}%
=e_{i}\mathbb{C}.
\end{align*}


On the other hand,%
\begin{align*}
N_{+}  &  =\bigoplus\limits_{j=1}^{n}e_{j}\mathbb{C}=\sum\limits_{j=1}%
^{n}\underbrace{e_{j}\mathbb{C}}_{\subseteq\widetilde{\mathfrak{n}}_{+}\left[
\alpha_{j}\right]  \subseteq\widetilde{\mathfrak{n}}_{+\left[  1\right]  }%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since direct sums are sums}\right) \\
&  \subseteq\sum\limits_{j=1}^{n}\widetilde{\mathfrak{n}}_{+\left[  1\right]
}\subseteq\widetilde{\mathfrak{n}}_{+\left[  1\right]  }%
=\widetilde{\mathfrak{n}}_{+}^{\operatorname*{principal}}\left[  1\right]  .
\end{align*}
Since $\widetilde{\mathfrak{n}}_{+}^{\operatorname*{principal}}$ is generated
by $N_{+}$ as a Lie algebra, this yields that we can apply Theorem
\ref{thm.FreeLie.grading1} to $\widetilde{\mathfrak{n}}_{+}%
^{\operatorname*{principal}}$ and $N_{+}$ instead of $\mathfrak{g}$ and $T$.
As a result, we obtain that $N_{+}=\widetilde{\mathfrak{n}}_{+}%
^{\operatorname*{principal}}\left[  1\right]  =\widetilde{\mathfrak{n}%
}_{+\left[  1\right]  }$. Now,%
\[
\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  =\widetilde{\mathfrak{n}%
}_{+}\left[  \alpha_{i}\right]  \subseteq\widetilde{\mathfrak{n}}_{+\left[
1\right]  }=N_{+},
\]
so that%
\begin{align*}
\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]   &  =\left(
\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  \right)  \cap N_{+}%
=N_{+}\left[  \alpha_{i}\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}N_{+}\text{ is a }Q\text{-graded vector subspace of }\widetilde{\mathfrak{g}%
}\right) \\
&  =e_{i}\mathbb{C}.
\end{align*}


Now, notice that $Q$ is a free abelian group with generators $-\alpha_{1}$,
$-\alpha_{2}$, $...$, $-\alpha_{n}$. Hence, we can prove
$\widetilde{\mathfrak{g}}\left[  -\alpha_{i}\right]  =f_{i}\mathbb{C}$ by
means of making the following modifications to the above proof of
$\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  =e_{i}\mathbb{C}$:

\begin{itemize}
\item Replace every $>$ sign by a $<$ sign, and vice versa.

\item Replace every $\alpha_{j}$ by $-\alpha_{j}$ (this includes replacing
every $\alpha_{i}$ by $-\alpha_{i}$).

\item Replace every $\widetilde{\mathfrak{n}}_{+}$ by $\widetilde{\mathfrak{n}%
}_{-}$ and vice versa.

\item Replace every $\iota_{+}$ by $\iota_{-}$ and vice versa.

\item Replace every $e_{j}$ by $f_{j}$ (this includes replacing every $e_{i}$
by $f_{i}$).

\item Replace every $N_{+}$ by $N_{-}$.

\item Replace $\widetilde{\mathfrak{n}}_{+}^{\operatorname*{principal}}$ by
$\widetilde{\mathfrak{n}}_{-}^{\operatorname*{principal}}$.
\end{itemize}

Thus we have proven both $\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]
=e_{i}\mathbb{C}$ and $\widetilde{\mathfrak{g}}\left[  -\alpha_{i}\right]
=f_{i}\mathbb{C}$. This proves Theorem \ref{thm.gtilde} \textbf{(g)}.
\end{verlong}

\bigskip

\textbf{(h)} It is clear that $I$ (being a sum of $Q$-graded ideals) is a
$Q$-graded ideal. We only need to prove that $I$ has zero intersection with
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $.

Let $\pi_{0}:\widetilde{\mathfrak{g}}\rightarrow\widetilde{\mathfrak{g}%
}\left[  0\right]  $ be the canonical projection from the $Q$-graded vector
space $\widetilde{\mathfrak{g}}$ on its $0$-th homogeneous component
$\widetilde{\mathfrak{g}}\left[  0\right]  $.

\begin{vershort}
For every $Q$-graded vector subspace $M$ of $\widetilde{\mathfrak{g}}$, we
have $\pi_{0}\left(  M\right)  =M\cap\left(  \widetilde{\mathfrak{g}}\left[
0\right]  \right)  $ (this is just an elementary property of $Q$-graded vector
spaces). Since $\widetilde{\mathfrak{g}}\left[  0\right]  =\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ (by Theorem \ref{thm.gtilde} \textbf{(e)}),
this rewrites as follows: For every $Q$-graded vector subspace $M$ of
$\widetilde{\mathfrak{g}}$, we have $\pi_{0}\left(  M\right)  =M\cap\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. Thus, every $Q$-graded ideal
$\mathfrak{i}$ of $\widetilde{\mathfrak{g}}$ which has zero intersection with
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ satisfies $\pi_{0}\left(
\mathfrak{i}\right)  =\mathfrak{i}\cap\iota_{0}\left(  \widetilde{\mathfrak{h}%
}\right)  =0$. Therefore, the sum $I$ of all such ideals also satisfies
$\pi_{0}\left(  I\right)  =0$ (since $\pi_{0}$ is linear). But since $\pi
_{0}\left(  I\right)  =I\cap\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
$ (because for every $Q$-graded vector subspace $M$ of
$\widetilde{\mathfrak{g}}$, we have $\pi_{0}\left(  M\right)  =M\cap\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $), this rewrites as $I\cap
\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  =0$. In other words, $I$ has
zero intersection with $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $.
Theorem \ref{thm.gtilde} \textbf{(h)} is proven.
\end{vershort}

\begin{verlong}
For every $Q$-graded vector subspace $M$ of $\widetilde{\mathfrak{g}}$, we
have%
\begin{equation}
M\cap\left(  \widetilde{\mathfrak{g}}\left[  0\right]  \right)  =\pi
_{0}\left(  M\right)  \label{pf.gtilde.g.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gtilde.g.1}):} Let $M$ be a $Q$-graded
vector subspace of $\widetilde{\mathfrak{g}}$. Then, we have $M=\bigoplus
\limits_{\alpha\in Q}M\left[  \alpha\right]  $, and every $\alpha\in Q$
satisfies $M\left[  \alpha\right]  =M\cap\left(  \widetilde{\mathfrak{g}%
}\left[  \alpha\right]  \right)  $.
\par
Now, $M=\bigoplus\limits_{\alpha\in Q}M\left[  \alpha\right]  =\sum
\limits_{\alpha\in Q}M\left[  \alpha\right]  $ (since direct sums are sums),
so that%
\[
\pi_{0}\left(  M\right)  =\pi_{0}\left(  \sum\limits_{\alpha\in Q}M\left[
\alpha\right]  \right)  =\sum\limits_{\alpha\in Q}\pi_{0}\left(  M\left[
\alpha\right]  \right)  =\pi_{0}\left(  M\left[  0\right]  \right)
+\sum\limits_{\substack{\alpha\in Q;\\\alpha\neq0}}\pi_{0}\left(  M\left[
\alpha\right]  \right)  .
\]
\par
But now, let $\alpha\in Q$ be such that $\alpha\neq0$. Then, $0$ and $\alpha$
are two distinct elements of $Q$. Whenever $\beta$ and $\gamma$ are two
distinct elements of $Q$, the projection from the $Q$-graded vector space
$\widetilde{\mathfrak{g}}$ on its $\beta$-th homogeneous component
$\widetilde{\mathfrak{g}}\left[  \beta\right]  $ sends
$\widetilde{\mathfrak{g}}\left[  \gamma\right]  $ to zero. Applied to
$\beta=0$ and $\gamma=\alpha$, this yields that the projection from the
$Q$-graded vector space $\widetilde{\mathfrak{g}}$ on its $0$-th homogeneous
component $\widetilde{\mathfrak{g}}\left[  0\right]  $ sends
$\widetilde{\mathfrak{g}}\left[  \alpha\right]  $ to $0$ (since $0$ and
$\alpha$ are two distinct elements of $Q$). Since the projection from the
$Q$-graded vector space $\widetilde{\mathfrak{g}}$ on its $0$-th homogeneous
component $\widetilde{\mathfrak{g}}\left[  0\right]  $ is the map $\pi_{0}$,
this rewrites as follows: The map $\pi_{0}$ sends $\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  $ to $0$. Thus, $\pi_{0}\left(
\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  =0$. Since $M\left[
\alpha\right]  \subseteq\widetilde{\mathfrak{g}}\left[  \alpha\right]  $, we
have $\pi_{0}\left(  M\left[  \alpha\right]  \right)  \subseteq\pi_{0}\left(
\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  =0$, so that $\pi
_{0}\left(  M\left[  \alpha\right]  \right)  =0$.
\par
Now forget that we fixed $\alpha$. We thus have shown that every $\alpha\in Q$
such that $\alpha\neq0$ satisfies $\pi_{0}\left(  M\left[  \alpha\right]
\right)  =0$. Hence, $\sum\limits_{\substack{\alpha\in Q;\\\alpha\neq
0}}\underbrace{\pi_{0}\left(  M\left[  \alpha\right]  \right)  }_{=0}%
=\sum\limits_{\substack{\alpha\in Q;\\\alpha\neq0}}0=0$.
\par
On the other hand, $\pi_{0}$ is a projection on $\widetilde{\mathfrak{g}%
}\left[  0\right]  $, and therefore leaves every subset of
$\widetilde{\mathfrak{g}}\left[  0\right]  $ invariant. Since $M\left[
0\right]  $ is a subset of $\widetilde{\mathfrak{g}}\left[  0\right]  $, this
yields that $\pi_{0}$ leaves $M\left[  0\right]  $ invariant. Hence, $\pi
_{0}\left(  M\left[  0\right]  \right)  =M\left[  0\right]  $.
\par
Now,%
\[
\pi_{0}\left(  M\right)  =\underbrace{\pi_{0}\left(  M\left[  0\right]
\right)  }_{=M\left[  0\right]  }+\underbrace{\sum\limits_{\substack{\alpha\in
Q;\\\alpha\neq0}}\pi_{0}\left(  M\left[  \alpha\right]  \right)  }%
_{=0}=M\left[  0\right]  =M\cap\left(  \widetilde{\mathfrak{g}}\left[
0\right]  \right)
\]
(since every $\alpha\in Q$ satisfies $M\left[  \alpha\right]  =M\cap\left(
\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  $). This proves
(\ref{pf.gtilde.g.1}).}. As a consequence, every $Q$-graded vector subspace
$M$ of $\widetilde{\mathfrak{g}}$ satisfies%
\begin{equation}
M\cap\underbrace{\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
}_{\substack{=\widetilde{\mathfrak{g}}\left[  0\right]  \\\text{(by Theorem
\ref{thm.gtilde} \textbf{(e)})}}}=M\cap\left(  \widetilde{\mathfrak{g}}\left[
0\right]  \right)  =\pi_{0}\left(  M\right)  \label{pf.gtilde.g.2}%
\end{equation}
(by (\ref{pf.gtilde.g.1})).

Now, $I$ is the sum of all $Q$-graded ideals in $\widetilde{\mathfrak{g}}$
which have zero intersection with $\iota_{0}\left(  \widetilde{\mathfrak{h}%
}\right)  $. In other words,%
\begin{align*}
I  &  =\sum\limits_{\substack{\mathfrak{i}\text{ is a }Q\text{-graded ideal in
}\widetilde{\mathfrak{g}}\\\text{such that }\mathfrak{i}\cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =0}}\mathfrak{i}=\sum
\limits_{\substack{\mathfrak{i}\text{ is a }Q\text{-graded ideal in
}\widetilde{\mathfrak{g}}\\\text{such that }\pi_{0}\left(  \mathfrak{i}%
\right)  =0}}\mathfrak{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because every }Q\text{-graded ideal }\mathfrak{i}\text{ in
}\widetilde{\mathfrak{g}}\text{ satisfies }\mathfrak{i}\cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\pi_{0}\left(  \mathfrak{i}\right) \\
\text{(by (\ref{pf.gtilde.g.2}), applied to }M=\mathfrak{i}\text{)}%
\end{array}
\right)  .
\end{align*}
Thus,%
\[
\pi_{0}\left(  I\right)  =\pi_{0}\left(  \sum\limits_{\substack{\mathfrak{i}%
\text{ is a }Q\text{-graded ideal in }\widetilde{\mathfrak{g}}\\\text{such
that }\pi_{0}\left(  \mathfrak{i}\right)  =0}}\mathfrak{i}\right)
=\sum\limits_{\substack{\mathfrak{i}\text{ is a }Q\text{-graded ideal in
}\widetilde{\mathfrak{g}}\\\text{such that }\pi_{0}\left(  \mathfrak{i}%
\right)  =0}}\underbrace{\pi_{0}\left(  \mathfrak{i}\right)  }_{=0}%
=\sum\limits_{\substack{\mathfrak{i}\text{ is a }Q\text{-graded ideal in
}\widetilde{\mathfrak{g}}\\\text{such that }\pi_{0}\left(  \mathfrak{i}%
\right)  =0}}0=0.
\]
But $I$ is $Q$-graded. Thus, (\ref{pf.gtilde.g.2}) (applied to $M=I$) yields
$I\cap\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  =\pi_{0}\left(
I\right)  =0$. In other words, $I$ has zero intersection with $\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. This proves Theorem
\ref{thm.gtilde} \textbf{(h)}.
\end{verlong}

\bigskip

\textbf{(i)} First, we notice that the Lie algebra $\widetilde{\mathfrak{g}}$
is generated by its elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ (since
\[
\widetilde{\mathfrak{g}}=\operatorname*{FreeLie}\left(  h_{i},f_{i}%
,e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)
\]
). Hence, the Lie algebra $\mathfrak{g}$ is generated by its elements $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$ as well (since $\mathfrak{g}=\widetilde{\mathfrak{g}}\diagup I$).

In order to prove that $\mathfrak{g}$ is a contragredient Lie algebra
corresponding to $A$, we must prove that it satisfies the conditions
\textbf{(1)}, \textbf{(2)} and \textbf{(3)} of Definition
\ref{def.contragredient}.

\textit{Proof of condition \textbf{(1)}:} The relations
(\ref{nonserre-relations}) are satisfied in $\widetilde{\mathfrak{g}}$ (by the
definition of $\widetilde{\mathfrak{g}}$ as the quotient Lie algebra
$\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)  $) and thus also in
$\mathfrak{g}$ (since $\mathfrak{g}$ is a quotient Lie algebra of
$\widetilde{\mathfrak{g}}$). This proves condition \textbf{(1)} for our
$Q$-graded Lie algebra $\mathfrak{g}$.

\textit{Proof of condition \textbf{(2)}:} By Theorem \ref{thm.gtilde}
\textbf{(e)}, we have $\widetilde{\mathfrak{g}}\left[  0\right]  =\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. We know that $h_{1}%
,h_{2},...,h_{n}$ is a basis of the vector space $\widetilde{\mathfrak{h}}$
(since $\widetilde{\mathfrak{h}}$ was defined as the free vector space with
basis $h_{1},h_{2},...,h_{n}$). Since $\iota_{0}$ is injective, this yields
that $h_{1},h_{2},...,h_{n}$ is a basis of $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ (because we identify the images of the
vectors $h_{1}$, $h_{2}$, $...$, $h_{n}$ under $\iota_{0}$ with $h_{1}$,
$h_{2}$, $...$, $h_{n}$). Thus, in particular, the vectors $h_{1}%
,h_{2},...,h_{n}$ in $\widetilde{\mathfrak{g}}$ span the vector space
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{g}%
}\left[  0\right]  $. As a consequence, the vectors $h_{1}$, $h_{2}$, $...$,
$h_{n}$ in $\mathfrak{g}$ span the vector space $\mathfrak{g}\left[  0\right]
$ (because $\mathfrak{g}=\widetilde{\mathfrak{g}}\diagup I$).

The vectors $h_{1}$, $h_{2}$, $...$, $h_{n}$ in $\mathfrak{g}$ are linearly
independent\footnote{\textit{Proof.} Let $\left(  \lambda_{1},\lambda
_{2},...,\lambda_{n}\right)  \in\mathbb{C}^{n}$ be such that $\lambda_{1}%
h_{1}+\lambda_{2}h_{2}+...+\lambda_{n}h_{n}=0$ in $\mathfrak{g}$. Then,
$\lambda_{1}h_{1}+\lambda_{2}h_{2}+...+\lambda_{n}h_{n}\in I$ in
$\widetilde{\mathfrak{g}}$ (since $\mathfrak{g}=\widetilde{\mathfrak{g}%
}\diagup I$). Combined with $\lambda_{1}h_{1}+\lambda_{2}h_{2}+...+\lambda
_{n}h_{n}\in\widetilde{\mathfrak{g}}\left[  0\right]  =\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, this yields $\lambda_{1}h_{1}+\lambda
_{2}h_{2}+...+\lambda_{n}h_{n}\in I\cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =0$ (since Theorem \ref{thm.gtilde}
\textbf{(h)} yields that $I$ has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $). Thus, $\lambda_{1}h_{1}+\lambda_{2}%
h_{2}+...+\lambda_{n}h_{n}=0$ in $\iota_{0}\left(  \widetilde{\mathfrak{h}%
}\right)  $. Since $h_{1},h_{2},...,h_{n}$ is a basis of $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, this yields $\lambda_{1}=\lambda
_{2}=...=\lambda_{n}=0$.
\par
Now forget that we fixed $\left(  \lambda_{1},\lambda_{2},...,\lambda
_{n}\right)  $. We have thus shown that every $\left(  \lambda_{1},\lambda
_{2},...,\lambda_{n}\right)  \in\mathbb{C}^{n}$ such that $\lambda_{1}%
h_{1}+\lambda_{2}h_{2}+...+\lambda_{n}h_{n}=0$ in $\mathfrak{g}$ satisfies
$\lambda_{1}=\lambda_{2}=...=\lambda_{n}=0$. In other words, the vectors
$h_{1}$, $h_{2}$, $...$, $h_{n}$ in $\mathfrak{g}$ are linearly independent,
qed.}. Hence, $h_{1},h_{2},...,h_{n}$ is a basis of the vector space
$\mathfrak{g}\left[  0\right]  $ (since the vectors $h_{1}$, $h_{2}$, $...$,
$h_{n}$ in $\mathfrak{g}$ span the vector space $\mathfrak{g}\left[  0\right]
$ and are linearly independent). In other words, the vector space
$\mathfrak{g}\left[  0\right]  $ has $\left(  h_{1},h_{2},...,h_{n}\right)  $
as a $\mathbb{C}$-vector space basis.

Let $i\in\left\{  1,2,...,n\right\}  $. Theorem \ref{thm.gtilde} \textbf{(g)}
yields $\widetilde{\mathfrak{g}}\left[  \alpha_{i}\right]  =\mathbb{C}e_{i}$.
Projecting this onto $\widetilde{\mathfrak{g}}\diagup I=\mathfrak{g}$, we
obtain $\mathfrak{g}\left[  \alpha_{i}\right]  =\mathbb{C}e_{i}$ (since the
projection of $e_{i}$ onto $\mathfrak{g}$ is also called $e_{i}$). Similarly,
$\mathfrak{g}\left[  -\alpha_{i}\right]  =\mathbb{C}f_{i}$.

Condition \textbf{(2)} is thus verified for our $Q$-graded Lie algebra
$\mathfrak{g}$.

\textit{Proof of condition \textbf{(3)}:} Let $J$ be a nonzero $Q$-graded
ideal in $\mathfrak{g}$. Assume that $J\cap\left(  \mathfrak{g}\left[
0\right]  \right)  =0$.

Recall that $I$ has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $. That is, $I\cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =0$.

Let $\operatorname*{proj}:\widetilde{\mathfrak{g}}\rightarrow
\widetilde{\mathfrak{g}}\diagup I=\mathfrak{g}$ be the canonical projection.
Then, $\operatorname*{proj}$ is a $Q$-graded Lie algebra homomorphism, so that
$\operatorname*{proj}\nolimits^{-1}\left(  J\right)  $ is a $Q$-graded ideal
of $\widetilde{\mathfrak{g}}$ (since $J$ is a $Q$-graded ideal of
$\mathfrak{g}$). Also, $\operatorname*{Ker}\operatorname*{proj}=I$ (since
$\operatorname*{proj}$ is the canonical projection $\widetilde{\mathfrak{g}%
}\rightarrow\widetilde{\mathfrak{g}}\diagup I$).

Let $x\in\operatorname*{proj}\nolimits^{-1}\left(  J\right)  \cap\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. Then, $x\in
\operatorname*{proj}\nolimits^{-1}\left(  J\right)  $ and $x\in\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. Since $x\in
\operatorname*{proj}\nolimits^{-1}\left(  J\right)  $, we have
$\operatorname*{proj}\left(  x\right)  \in J$. Since $x\in\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{g}}\left[  0\right]  $
(by Theorem \ref{thm.gtilde} \textbf{(e)}), we have $\operatorname*{proj}%
\left(  x\right)  \in\mathfrak{g}\left[  0\right]  $ (since
$\operatorname*{proj}$ is $Q$-graded). Combined with $\operatorname*{proj}%
\left(  x\right)  \in J$, this yields $\operatorname*{proj}\left(  x\right)
\in J\cap\left(  \mathfrak{g}\left[  0\right]  \right)  =0$, so that
$\operatorname*{proj}\left(  x\right)  =0$, thus $x\in\operatorname*{Ker}%
\operatorname*{proj}=I$. Combined with $x\in\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, this yields $x\in I\cap\left(  \iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  \right)  =0$, so that $x=0$.

Forget that we fixed $x$. We thus have proven that every $x\in
\operatorname*{proj}\nolimits^{-1}\left(  J\right)  \cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ satisfies $x=0$. Hence,
$\operatorname*{proj}\nolimits^{-1}\left(  J\right)  \cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =0$. Thus, $\operatorname*{proj}%
\nolimits^{-1}\left(  J\right)  $ is a $Q$-graded ideal in
$\widetilde{\mathfrak{g}}$ which has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $. Hence,%
\[
\operatorname*{proj}\nolimits^{-1}\left(  J\right)  \subseteq\left(  \text{sum
of all }Q\text{-graded ideals in }\widetilde{\mathfrak{g}}\text{ which have
zero intersection with }\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
\right)  =I.
\]


Now let $y\in J$ be arbitrary. Since $y\in J\subseteq\mathfrak{g}%
=\widetilde{\mathfrak{g}}\diagup I$, there exists a $y^{\prime}\in
\widetilde{\mathfrak{g}}$ such that $y=\operatorname*{proj}\left(  y^{\prime
}\right)  $. Consider this $y$. Since $\operatorname*{proj}\left(  y^{\prime
}\right)  =y\in J$, we have $y^{\prime}\in\operatorname*{proj}\nolimits^{-1}%
\left(  J\right)  \subseteq I=\operatorname*{Ker}\operatorname*{proj}$, so
that $\operatorname*{proj}\left(  y^{\prime}\right)  =0$. Thus,
$y=\operatorname*{proj}\left(  y^{\prime}\right)  =0$. Now, forget that we
fixed $y$. We thus have proven that every $y\in J$ satisfies $y=0$. Thus,
$J=0$, contradicting to the fact that $J$ is nonzero.

This contradiction shows that our assumption (that $J\cap\left(
\mathfrak{g}\left[  0\right]  \right)  =0$) was wrong. In other words,
$J\cap\left(  \mathfrak{g}\left[  0\right]  \right)  \neq0$.

Now forget that we fixed $J$. We thus have proven that every nonzero
$Q$-graded ideal $J$ in $\mathfrak{g}$ satisfies $J\cap\left(  \mathfrak{g}%
\left[  0\right]  \right)  \neq0$. In other words, every nonzero $Q$-graded
ideal in $\mathfrak{g}$ has a nonzero intersection with $\mathfrak{g}\left[
0\right]  $. This proves that Condition \textbf{(3)} holds for our $Q$-graded
Lie algebra $\mathfrak{g}$.

Now that we have checked all three conditions \textbf{(1)}, \textbf{(2)} and
\textbf{(3)} for our $Q$-graded Lie algebra $\mathfrak{g}$, we conclude that
$\mathfrak{g}$ indeed is a contragredient Lie algebra corresponding to $A$.
Theorem \ref{thm.gtilde} \textbf{(i)} is proven.

\bigskip

\textit{Proof of Theorem \ref{thm.g(A).exuni}.} \textbf{(a)} Let the
$Q$-graded Lie algebra $\mathfrak{g}$ be defined as in Theorem
\ref{thm.gtilde}. According to Theorem \ref{thm.gtilde} \textbf{(i)}, this
$\mathfrak{g}$ is a contragredient Lie algebra corresponding to $A$. Thus,
there exists at least one contragredient Lie algebra corresponding to $A$,
namely this $\mathfrak{g}$. Now, it only remains to prove that it is the only
such Lie algebra (up to isomorphism). In other words, it remains to prove that
whenever $\mathfrak{g}^{\prime}$ is a contragredient Lie algebra corresponding
to $A$, then there exists a $Q$-graded Lie algebra isomorphism $\mathfrak{g}%
\rightarrow\mathfrak{g}^{\prime}$ which sends the generators $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ of $\mathfrak{g}$ to the respective generators $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ of $\mathfrak{g}^{\prime}$.

So let $\mathfrak{g}^{\prime}$ be a contragredient Lie algebra. Then,
condition \textbf{(1)} of Definition \ref{def.contragredient} is satisfied for
$\mathfrak{g}^{\prime}$. Thus, the relations (\ref{nonserre-relations}) are
satisfied in $\mathfrak{g}^{\prime}$.

Define a Lie algebra homomorphism $\psi:\widetilde{\mathfrak{g}}%
\rightarrow\mathfrak{g}^{\prime}$ by%
\[
\left\{
\begin{array}
[c]{c}%
\psi\left(  e_{i}\right)  =e_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\psi\left(  f_{i}\right)  =f_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\psi\left(  h_{i}\right)  =h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  .
\]
This $\psi$ is well-defined because the relations (\ref{nonserre-relations})
are satisfied in $\mathfrak{g}^{\prime}$ (and because $\widetilde{\mathfrak{g}%
}=\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  $).

Since the Lie algebra $\mathfrak{g}^{\prime}$ is generated by its elements
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ (by the definition of a contragredient Lie algebra),
the homomorphism $\psi$ is surjective (since all of the elements $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$ clearly lie in the image of $\psi$).

\begin{vershort}
Since $\mathfrak{g}^{\prime}$ is a contragredient Lie algebra, the condition
\textbf{(2)} of Definition \ref{def.contragredient} is satisfied for
$\mathfrak{g}^{\prime}$. In other words, the vector space $\mathfrak{g}%
^{\prime}\left[  0\right]  $ has $\left(  h_{1},h_{2},...,h_{n}\right)  $ as a
$\mathbb{C}$-vector space basis, and we have $\mathfrak{g}^{\prime}\left[
\alpha_{i}\right]  =\mathbb{C}e_{i}$ and $\mathfrak{g}^{\prime}\left[
-\alpha_{i}\right]  =\mathbb{C}f_{i}$ for all $i\in\left\{  1,2,...,n\right\}
$. This yields that the elements $e_{i}$, $f_{i}$ and $h_{i}$ of
$\mathfrak{g}^{\prime}$ satisfy%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]
Of course, the elements $e_{i}$, $f_{i}$ and $h_{i}$ of
$\widetilde{\mathfrak{g}}$ satisfy the same relations (because of the
definition of the $Q$-grading on $\widetilde{\mathfrak{g}}$). As a
consequence, it is easy to see that Lie algebra homomorphism $\psi$ is
$Q$-graded\footnote{\textit{Proof.} Let $T$ be the vector subspace of
$\widetilde{\mathfrak{g}}$ spanned by the elements $e_{1}$, $e_{2}$, $...$,
$e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$.
Then, $\widetilde{\mathfrak{g}}$ is generated by $T$ as a Lie algebra (because
$\widetilde{\mathfrak{g}}$ is generated by the elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ as a Lie algebra). Due to the relations%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}
\]
holding both in $\widetilde{\mathfrak{g}}$ and in $\mathfrak{g}^{\prime}$, it
is clear that the map $\psi\mid_{T}$ is $Q$-graded. Proposition
\ref{prop.generation.Q-gr} (applied to $\widetilde{\mathfrak{g}}$,
$\mathfrak{g}^{\prime}$ and $\psi$ instead of $\mathfrak{g}$, $\mathfrak{h}$
and $f$) now yields that $\psi$ is $Q$-graded, qed.}. As a consequence,
$\operatorname*{Ker}\psi$ is a $Q$-graded Lie ideal of
$\widetilde{\mathfrak{g}}$.
\end{vershort}

\begin{verlong}
Since $\mathfrak{g}^{\prime}$ is a contragredient Lie algebra, the condition
\textbf{(2)} of Definition \ref{def.contragredient} is satisfied for
$\mathfrak{g}^{\prime}$. In other words, the vector space $\mathfrak{g}%
^{\prime}\left[  0\right]  $ has $\left(  h_{1},h_{2},...,h_{n}\right)  $ as a
$\mathbb{C}$-vector space basis, and we have $\mathfrak{g}^{\prime}\left[
\alpha_{i}\right]  =\mathbb{C}e_{i}$ and $\mathfrak{g}^{\prime}\left[
-\alpha_{i}\right]  =\mathbb{C}f_{i}$ for all $i\in\left\{  1,2,...,n\right\}
$. This yields that the elements $e_{i}$, $f_{i}$ and $h_{i}$ of
$\mathfrak{g}^{\prime}$ satisfy%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]
Of course, the elements $e_{i}$, $f_{i}$ and $h_{i}$ of
$\widetilde{\mathfrak{g}}$ satisfy the same relations (because of the
definition of the $Q$-grading on $\widetilde{\mathfrak{g}}$). As a
consequence, it is easy to see that Lie algebra homomorphism $\psi$ is
$Q$-graded\footnote{\textit{Proof.} Let $T$ be the vector subspace of
$\widetilde{\mathfrak{g}}$ spanned by the elements $e_{1}$, $e_{2}$, $...$,
$e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$.
Then, $\widetilde{\mathfrak{g}}$ is generated by $T$ as a Lie algebra (because
$\widetilde{\mathfrak{g}}$ is generated by the elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ as a Lie algebra). Due to the relations%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}
\]
holding in $\widetilde{\mathfrak{g}}$, the subspace $T$ is of
$\widetilde{\mathfrak{g}}$ is $Q$-graded, and its homogeneous components are%
\[
T\left[  \alpha\right]  =\left\{
\begin{array}
[c]{l}%
\mathbb{C}h_{1}+\mathbb{C}h_{2}+...+\mathbb{C}h_{n}%
,\ \ \ \ \ \ \ \ \ \ \text{if }\alpha=0;\\
\mathbb{C}e_{i},\ \ \ \ \ \ \ \ \ \ \text{if }\alpha=\alpha_{i}\text{ for some
}i\in\left\{  1,2,...,n\right\}  ;\\
\mathbb{C}f_{i},\ \ \ \ \ \ \ \ \ \ \text{if }\alpha=-\alpha_{i}\text{ for
some }i\in\left\{  1,2,...,n\right\}  ;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in Q.
\]
Using this fact, it is straightforward to check that $\psi\left(  T\left[
\alpha\right]  \right)  \subseteq\mathfrak{g}^{\prime}\left[  \alpha\right]  $
for every $\alpha\in Q$. In other words, $\left(  \psi\mid_{T}\right)  \left(
T\left[  \alpha\right]  \right)  \subseteq\mathfrak{g}^{\prime}\left[
\alpha\right]  $ for every $\alpha\in Q$. Thus, the map $\psi\mid_{T}$ is
$Q$-graded. Proposition \ref{prop.generation.Q-gr} (applied to
$\widetilde{\mathfrak{g}}$, $\mathfrak{g}^{\prime}$ and $\psi$ instead of
$\mathfrak{g}$, $\mathfrak{h}$ and $f$) now yields that $\psi$ is $Q$-graded,
qed.}. As a consequence, $\operatorname*{Ker}\psi$ is a $Q$-graded Lie ideal
of $\widetilde{\mathfrak{g}}$.
\end{verlong}

Define $\widetilde{\mathfrak{h}}$, $I$ and $\iota_{0}$ as in Theorem
\ref{thm.gtilde}. Then, $\widetilde{\mathfrak{h}}$ is the free vector space
with basis $h_{1},h_{2},...,h_{n}$. Thus, the vector space
$\widetilde{\mathfrak{h}}$ is spanned by $h_{1},h_{2},...,h_{n}$. As a
consequence, the vector space $\iota_{0}\left(  \widetilde{\mathfrak{h}%
}\right)  $ is spanned by $h_{1},h_{2},...,h_{n}$ (since $\iota_{0}$ maps the
elements $h_{1},h_{2},...,h_{n}$ of $\widetilde{\mathfrak{h}}$ to the elements
$h_{1},h_{2},...,h_{n}$ of $\widetilde{\mathfrak{g}}$). Now, it is easy to see
that $\left(  \operatorname*{Ker}\psi\right)  \cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =0$\ \ \ \ \footnote{\textit{Proof.} Let
$x\in\left(  \operatorname*{Ker}\psi\right)  \cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $. Then, $x\in\operatorname*{Ker}\psi$ and
$x\in\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $. Since $x\in\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $, there exist some elements
$\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$ of $\mathbb{C}$ such that
$x=\lambda_{1}h_{1}+\lambda_{2}h_{2}+...+\lambda_{n}h_{n}$ (since the vector
space $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ is spanned by
$h_{1},h_{2},...,h_{n}$). Consider these $\lambda_{1}$, $\lambda_{2}$, $...$,
$\lambda_{n}$. Since $x\in\operatorname*{Ker}\psi$, we have $\psi\left(
x\right)  =0$, so that%
\begin{align*}
0  &  =\psi\left(  x\right)  =\psi\left(  \lambda_{1}h_{1}+\lambda_{2}%
h_{2}+...+\lambda_{n}h_{n}\right)  =\lambda_{1}\psi\left(  h_{1}\right)
+\lambda_{2}\psi\left(  h_{2}\right)  +...+\lambda_{n}\psi\left(  h_{n}\right)
\\
&  =\lambda_{1}h_{1}+\lambda_{2}h_{2}+...+\lambda_{n}h_{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi\left(  h_{i}\right)
=h_{i}\text{ for every }i\in\left\{  1,2,...,n\right\}  \right)
\end{align*}
in $\mathfrak{g}^{\prime}$. But since the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$ of $\mathfrak{g}^{\prime}$ are linearly independent (because the
vector space $\mathfrak{g}^{\prime}\left[  0\right]  $ has $\left(
h_{1},h_{2},...,h_{n}\right)  $ as a $\mathbb{C}$-vector space basis), this
yields that $\lambda_{1}=\lambda_{2}=...=\lambda_{n}=0$. Thus, $x=\lambda
_{1}h_{1}+\lambda_{2}h_{2}+...+\lambda_{n}h_{n}$ becomes $x=0h_{1}%
+0h_{2}+...+0h_{n}=0$.
\par
Now forget that we fixed $x$. We thus have seen that every $x\in\left(
\operatorname*{Ker}\psi\right)  \cap\iota_{0}\left(  \widetilde{\mathfrak{h}%
}\right)  $ satisfies $x=0$. In other words, $\left(  \operatorname*{Ker}%
\psi\right)  \cap\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  =0$, qed.}.
Hence, $\operatorname*{Ker}\psi$ is a $Q$-graded Lie ideal of
$\widetilde{\mathfrak{g}}$ which has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $.

But $I$ is the sum of all $Q$-graded ideals in $\widetilde{\mathfrak{g}}$
which have zero intersection with $\iota_{0}\left(  \widetilde{\mathfrak{h}%
}\right)  $. Thus, every $Q$-graded ideal of $\widetilde{\mathfrak{g}}$ which
has zero intersection with $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
$ must be a subset of $I$. Since $\operatorname*{Ker}\psi$ is a $Q$-graded Lie
ideal of $\widetilde{\mathfrak{g}}$ which has zero intersection with
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $, this yields that
$\operatorname*{Ker}\psi\subseteq I$.

We will now prove the reverse inclusion, i. e., we will show that
$I\subseteq\operatorname*{Ker}\psi$.

We know that $I$ is $Q$-graded (by Theorem \ref{thm.gtilde} \textbf{(h)}).
Since $\psi$ is $Q$-graded, this yields that $\psi\left(  I\right)  $ is a
$Q$-graded vector subspace of $\mathfrak{g}^{\prime}$. On the other hand,
since $I$ is $Q$-graded, we have $I\left[  0\right]  =I\cap\underbrace{\left(
\widetilde{\mathfrak{g}}\left[  0\right]  \right)  }_{\substack{=\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  \\\text{(by Theorem
\ref{thm.gtilde} \textbf{(e)})}}}=I\cap\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =0$ (since Theorem \ref{thm.gtilde}
\textbf{(h)} yields that $I$ has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $).

Since $\mathfrak{g}^{\prime}$ is a contragredient Lie algebra, the condition
\textbf{(3)} of Definition \ref{def.contragredient} is satisfied for
$\mathfrak{g}^{\prime}$. In other words, every nonzero $Q$-graded ideal in
$\mathfrak{g}^{\prime}$ has a nonzero intersection with $\mathfrak{g}^{\prime
}\left[  0\right]  $. Since $I$ is an ideal of $\widetilde{\mathfrak{g}}$, the
image $\psi\left(  I\right)  $ is an ideal of $\mathfrak{g}^{\prime}$ (because
$\psi$ is a surjective homomorphism of Lie algebras, and because the image of
an ideal under a \textbf{surjective} homomorphism of Lie algebras must always
be an ideal of the target Lie algebra). Assume that $\psi\left(  I\right)
\neq0$. Clearly, $\psi\left(  I\right)  $ is $Q$-graded (since $I$ is
$Q$-graded (by Theorem \ref{thm.gtilde} \textbf{(h)}) and since $\psi$ is
$Q$-graded). Thus, $\psi\left(  I\right)  $ is a nonzero $Q$-graded ideal in
$\mathfrak{g}^{\prime}$. Thus, $\psi\left(  I\right)  $ has a nonzero
intersection with $\mathfrak{g}^{\prime}\left[  0\right]  $ (because every
nonzero $Q$-graded ideal in $\mathfrak{g}^{\prime}$ has a nonzero intersection
with $\mathfrak{g}^{\prime}\left[  0\right]  $). In other words, $\psi\left(
I\right)  \cap\left(  \mathfrak{g}^{\prime}\left[  0\right]  \right)  \neq0$.

The following is a known and easy fact from linear algebra: If $A$ and $B$ are
two $Q$-graded vector spaces, and $\Phi:A\rightarrow B$ is a $Q$-graded linear
map, then $\Phi\left(  A\left[  \beta\right]  \right)  =\left(  \Phi\left(
A\right)  \right)  \left[  \beta\right]  $ for every $\beta\in Q$. Applying
this fact to $A=I$, $B=\mathfrak{g}^{\prime}$, $\Phi=\psi$ and $\beta=0$, we
obtain $\psi\left(  I\left[  0\right]  \right)  =\left(  \psi\left(  I\right)
\right)  \left[  0\right]  $. But since $I\left[  0\right]  =0$, this rewrites
as $\psi\left(  0\right)  =\left(  \psi\left(  I\right)  \right)  \left[
0\right]  $. Hence, $\left(  \psi\left(  I\right)  \right)  \left[  0\right]
=\psi\left(  0\right)  =0$.

But since $\psi\left(  I\right)  $ is a $Q$-graded vector subspace of
$\mathfrak{g}^{\prime}$, we have $\psi\left(  I\right)  \cap\left(
\mathfrak{g}^{\prime}\left[  0\right]  \right)  =\left(  \psi\left(  I\right)
\right)  \left[  0\right]  =0$. This contradicts the fact that $\psi\left(
I\right)  \cap\left(  \mathfrak{g}^{\prime}\left[  0\right]  \right)  \neq0$.
Hence, our assumption (that $\psi\left(  I\right)  \neq0$) must have been
wrong. In other words, $\psi\left(  I\right)  =0$, so that $I\subseteq
\operatorname*{Ker}\psi$. Combined with $\operatorname*{Ker}\psi\subseteq I$,
this yields $I=\operatorname*{Ker}\psi$.

Since the $Q$-graded Lie algebra homomorphism $\psi:\widetilde{\mathfrak{g}%
}\rightarrow\mathfrak{g}^{\prime}$ is surjective, it factors (according to the
homomorphism theorem) through a $Q$-graded Lie algebra isomorphism
$\widetilde{\mathfrak{g}}\diagup\left(  \operatorname*{Ker}\psi\right)
\rightarrow\mathfrak{g}^{\prime}$. Since $\widetilde{\mathfrak{g}}%
\diagup\underbrace{\left(  \operatorname*{Ker}\psi\right)  }_{=I}%
=\widetilde{\mathfrak{g}}\diagup I=\mathfrak{g}$, this means that $\psi$
factors through a $Q$-graded Lie algebra isomorphism $\mathfrak{g}%
\rightarrow\mathfrak{g}^{\prime}$. This $Q$-graded Lie algebra isomorphism
$\mathfrak{g}\rightarrow\mathfrak{g}^{\prime}$ clearly sends the generators
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ of $\mathfrak{g}$ to the respective generators $e_{1}%
$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}%
$, $...$, $h_{n}$ of $\mathfrak{g}^{\prime}$.

We have thus proven that there exists a $Q$-graded Lie algebra isomorphism
$\mathfrak{g}\rightarrow\mathfrak{g}^{\prime}$ which sends the generators
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ of $\mathfrak{g}$ to the respective generators $e_{1}%
$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}%
$, $...$, $h_{n}$ of $\mathfrak{g}^{\prime}$. This completes the proof of
Theorem \ref{thm.g(A).exuni} \textbf{(a)}.

\textbf{(b)} Let $A$ be the Cartan matrix of a simple finite-dimensional Lie
algebra. Clearly it is enough to prove that this Lie algebra is a
contragredient Lie algebra corresponding to $A$, that is, is generated by
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ as a Lie algebra and satisfies the conditions
\textbf{(1)}, \textbf{(2)} and \textbf{(3)} of Definition
\ref{def.contragredient}. But this follows from the standard theory of roots
of simple finite-dimensional Lie algebras\footnote{For instance, condition
\textbf{(3)} follows from the fact that the Lie algebra in question is simple
and thus contains no ideals other than $0$ and itself.}. Theorem
\ref{thm.g(A).exuni} \textbf{(b)} is thus proven.

\begin{remark}
Let $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ be a complex $n\times n$
matrix such that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$a_{i,i}=2$. One can show that the Lie algebra $\mathfrak{g}\left(  A\right)
$ is finite-dimensional if and only if $A$ is the Cartan matrix of a
semisimple finite-dimensional Lie algebra. (In this case, $\mathfrak{g}\left(
A\right)  $ is exactly this semisimple Lie algebra, and the ideal $I$ of
Theorem \ref{thm.gtilde} is generated by the left hand sides $\left(
\operatorname*{ad}\left(  e_{i}\right)  \right)  ^{1-a_{i,j}}e_{j}$ and
$\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}$
of the Serre relations.)
\end{remark}

[...]

[Add something about the total degree on $\widetilde{\mathfrak{g}}$, since
this will later be used for the bilinear form. $\widetilde{\mathfrak{g}%
}\left[  \operatorname*{tot}0\right]  =\widetilde{\mathfrak{g}}\left[
0\right]  =\widetilde{\mathfrak{h}}$, $\widetilde{\mathfrak{g}}\left[
\operatorname*{tot}<0\right]  ...$, $\widetilde{\mathfrak{g}}\left[  1\right]
=...$]

\begin{remark}
\label{rmk.g(A+A)}Let $A_{1}$ and $A_{2}$ be two square complex matrices. As
usual, we denote by $A_{1}\oplus A_{2}$ the block-diagonal matrix $\left(
\begin{array}
[c]{cc}%
A_{1} & 0\\
0 & A_{2}%
\end{array}
\right)  $. Then, $\mathfrak{g}\left(  A_{1}\oplus A_{2}\right)
\cong\mathfrak{g}\left(  A_{1}\right)  \oplus\mathfrak{g}\left(  A_{2}\right)
$ as Lie algebras naturally.
\end{remark}

\textit{Proof of Remark \ref{rmk.g(A+A)} (sketched).} Say $A_{1}$ is an
$\ell\times\ell$ matrix, and $A_{2}$ is an $m\times m$ matrix. Let $n=\ell+m$
and $A=A_{1}\oplus A_{2}$. Introduce the notations $\widetilde{\mathfrak{g}}$,
$\widetilde{\mathfrak{h}}$, $\widetilde{\mathfrak{n}}_{+}$,
$\widetilde{\mathfrak{n}}_{-}$, $\iota_{0}$, $\iota_{+}$, $\iota_{-}$ and $I$
as in Theorem \ref{thm.gtilde}. Let $\mathfrak{j}_{+}$ be the ideal of the Lie
algebra $\widetilde{\mathfrak{n}}_{+}$ generated by all elements of the form
$\left[  e_{i},e_{j}\right]  $ with $i\in\left\{  1,2,...,\ell\right\}  $ and
$j\in\left\{  \ell+1,\ell+2,...,n\right\}  $. Let $\mathfrak{j}_{-}$ be the
ideal of the Lie algebra $\widetilde{\mathfrak{n}}_{-}$ generated by all
elements of the form $\left[  f_{i},f_{j}\right]  $ with $i\in\left\{
1,2,...,\ell\right\}  $ and $j\in\left\{  \ell+1,\ell+2,...,n\right\}  $.
Prove that $\iota_{+}\left(  \mathfrak{j}_{+}\right)  $ and $\iota_{-}\left(
\mathfrak{j}_{-}\right)  $ are actually $Q$-graded ideals of
$\widetilde{\mathfrak{g}}$ (and not only of $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  $ and $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $), so that both $\iota_{+}\left(
\mathfrak{j}_{+}\right)  $ and $\iota_{-}\left(  \mathfrak{j}_{-}\right)  $
are subsets of $I$. For every $i\in\left\{  1,2\right\}  $, let
$\widetilde{\mathfrak{g}}_{i}$ be the Lie algebra constructed analogously to
$\widetilde{\mathfrak{g}}$ but for the matrix $A_{i}$ instead of $A$. Notice
that $\widetilde{\mathfrak{g}}\diagup\left(  \iota_{+}\left(  \mathfrak{j}%
_{+}\right)  +\iota_{-}\left(  \mathfrak{j}_{-}\right)  \right)
\cong\widetilde{\mathfrak{g}}_{1}\oplus\widetilde{\mathfrak{g}}_{2}$. Conclude
the proof by noticing that if $J$ is a $Q$-graded ideal in
$\widetilde{\mathfrak{g}}$ which has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, and $K$ is the sum of all $Q$-graded
ideals in $\widetilde{\mathfrak{g}}\diagup J$ which have zero intersection
with the projection of $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ on
$\widetilde{\mathfrak{g}}\diagup J$, then $\left(  \widetilde{\mathfrak{g}%
}\diagup J\right)  \diagup K\cong\widetilde{\mathfrak{g}}\diagup
I=\mathfrak{g}$. The details are left to the reader.

\subsection{\textbf{[unfinished]} Kac-Moody algebras for generalized Cartan
matrices}

For general $A$, we do not know much about $\mathfrak{g}\left(  A\right)  $;
its definition was not even constructive (find that $I$ !). It is not known in
general how to obtain generators for $I$. But for some particular cases -- not
only Cartan matrices of semisimple Lie algebras --, things behave well. Here
is the most important such case:

\begin{definition}
An $n\times n$ matrix $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ of
complex numbers is said to be a \textit{generalized Cartan matrix} if it satisfies:

\textbf{(1)} We have $a_{i,i}=2$ for all $i\in\left\{  1,2,...,n\right\}  $.

\textbf{(2)} For every $i$ and $j$, the number $a_{i,j}$ is a nonpositive
integer. Also, $a_{i,j}=0$ if and only if $a_{j,i}=0$.

\textbf{(3)} The matrix $A$ is symmetrizable, i. e., there exists a diagonal
matrix $D>0$ such that $\left(  DA\right)  ^{T}=DA$.
\end{definition}

Note that a Cartan matrix is the same as a generalized Cartan matrix $A$ with
$DA>0$.

\begin{example}
Let $A=\left(
\begin{array}
[c]{cc}%
2 & -m\\
-1 & 2
\end{array}
\right)  $ for $m\geq1$. This matrix $A$ is a generalized Cartan matrix, since
$\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & m
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
2 & -m\\
-1 & 2
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
2 & -m\\
-m & 2m
\end{array}
\right)  $. Note that $\det\left(
\begin{array}
[c]{cc}%
2 & -m\\
-1 & 2
\end{array}
\right)  =4-m$.

For $m=1$, we have $\mathfrak{g}\left(  A\right)  \cong A_{2}=\mathfrak{sl}%
_{3}$.

For $m=2$, we have $\mathfrak{g}\left(  A\right)  \cong B_{2}\cong C_{2}%
\cong\mathfrak{sp}_{4}\cong\mathfrak{so}_{5}$.

For $m=3$, we have $\mathfrak{g}\left(  A\right)  \cong G_{2}$.

For $m\geq4$, the Lie algebra $\mathfrak{g}\left(  A\right)  $ is infinite-dimensional.

For $m=4$, it is a twisted version of $\widehat{\mathfrak{sl}_{2}}$, called
$A_{2}^{2}$.

For $m\geq5$, the Lie algebra $\mathfrak{g}\left(  A\right)  $ is big (in the
sense of having exponential growth).

This strange behaviour is related to the behaviour of the $m$-subspaces
problem (finite for $m\leq3$, tame for $m=4$, wild for $m\geq5$). More
generally, Kac-Moody algebras are related to representation theory of quivers.
\end{example}

\begin{definition}
A \textit{symmetrizable Kac-Moody algebra} is a Lie algebra of the form
$\mathfrak{g}\left(  A\right)  $ for a generalized Cartan matrix $A$.
\end{definition}

\begin{theorem}
[Gabber-Kac]\label{thm.g(A).gabber-kac}If $A$ is a generalized Cartan matrix,
then the ideal $I\subseteq\widetilde{\mathfrak{g}}\left(  A\right)  $ is
generated by the Serre relations (where the notation $I$ comes from Theorem
\ref{thm.gtilde}).
\end{theorem}

\textit{Partial proof of Theorem \ref{thm.g(A).gabber-kac}.} Proving this
theorem requires showing two assertions: first, that the Serre relations are
contained in $I$; second, that they actually generate $I$. We will only prove
the first of these two assertions.

Set $I_{+}=I\cap\widetilde{\mathfrak{n}}_{+}$ and $I_{-}=I\cap
\widetilde{\mathfrak{n}}_{-}$. Denote $\widetilde{\mathfrak{g}}\left(
A\right)  $ by $\widetilde{\mathfrak{g}}$ as in Theorem \ref{thm.gtilde}.

We know (from Theorem \ref{thm.gtilde} \textbf{(h)}) that $I$ is a $Q$-graded
ideal in $\widetilde{\mathfrak{g}}$ which has zero intersection with
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ (where the notations are
those of Theorem \ref{thm.gtilde}). Since $\widetilde{\mathfrak{g}}\left[
0\right]  =\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ (by Theorem
\ref{thm.gtilde} \textbf{(e)}), this rewrites as follows: $I$ is a $Q$-graded
ideal in $\widetilde{\mathfrak{g}}$ which has zero intersection with
$\widetilde{\mathfrak{g}}\left[  0\right]  $. Thus, $I=I_{+}\oplus I_{-}$.

Let us show that $\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}\in I_{-}$.

To do that, it is sufficient to show that $\left[  e_{k},\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}\right]  =0$
for all $k$. (If we grade $\widetilde{\mathfrak{g}}$ by setting $\deg\left(
f_{i}\right)  =-1$, $\deg\left(  e_{i}\right)  =1$ and $\deg\left(
h_{i}\right)  =0$ (this is called the \textit{principal grading}), then
$f_{k}$ can only lower degree, so that the Lie ideal generated by $\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}$ will lie
entirely in negative degrees, and thus $\left(  \operatorname*{ad}\left(
f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}$ will lie in $I_{-}$.)

\textit{Case 1:} We have $k\neq i,j$. This case is clear since $e_{k}$
commutes with $f_{i}$ and $f_{j}$ (by our relations).

\textit{Case 2:} We have $k=j$. In this case,
\begin{align*}
\left[  e_{k},\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}\right]   &  =\left[  e_{j},\left(  \operatorname*{ad}\left(
f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}\right]  =\left(  \operatorname*{ad}%
\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}\left(  \left[  e_{j},f_{j}\right]
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{ad}\left(
f_{i}\right)  \text{ and }\operatorname*{ad}\left(  e_{j}\right)  \text{
commute, due to }i\neq j\right) \\
&  =\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}%
h_{j}.
\end{align*}
We now distinguish between two cases according to whether $a_{i,j}$ is $=0$ or
$<0$:

\textit{Case 2a:} We have $a_{i,j}=0$. Then, $a_{j,i}=0$ by the definition of
generalized Cartan matrices. Thus, $\left[  f_{i},h_{j}\right]  =-\left[
h_{j},f_{i}\right]  =-a_{j,i}f_{i}=0$, and we are done.

\textit{Case 2b:} We have $a_{i,j}<0$. Then, $1-a_{i,j}\geq2$. Now, $\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{2}h_{j}=\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  \left(  cf_{i}\right)  =0$
for some constant $c$.

\textit{Case 3:} We have $k=i$. Let $\left(  \mathfrak{sl}_{2}\right)
_{i}=\left\langle e_{i},f_{i},h_{i}\right\rangle $. Let $M$ be the $\left(
\mathfrak{sl}_{2}\right)  _{i}$-submodule in $\widetilde{\mathfrak{g}}\left(
A\right)  $ generated by $f_{j}$.

We have $\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j}=mf_{j}$, where
$m=-a_{i,j}\geq0$. Together with $\left[  e_{i},f_{j}\right]  =0$, this shows
that $f_{j}=:v$ is a highest-weight vector of $M$ with weight $m$. Thus,
$f_{i}^{m+1}v=\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}$ is a singular vector for $\left(  \mathfrak{sl}_{2}\right)
_{i}$ (by representation theory of $\mathfrak{sl}_{2}$\ \ \ \ \footnote{What
we are using is the following: Consider the module $M_{\lambda}=\mathbb{C}%
\left[  f\right]  v$ over $\mathfrak{sl}_{2}$. Then, $ef^{n}v=n\left(
\lambda-n+1\right)  f^{n-1}v$. Thus, when $n=m+1$ and $\lambda=m$, we get
$ef^{n}v=0$.}).

So much for our part of the proof of Theorem \ref{thm.g(A).gabber-kac}.

Of course, simple Lie algebras are Kac-Moody algebras. The next class of
Kac-Moody algebras we are interested in is the \textit{affine Lie algebras}:

\begin{remark}
Let $\sigma\in S_{n}$ be a permutation, and $A$ be an $n\times n$ complex
matrix. Then, $\mathfrak{g}\left(  A\right)  \cong\mathfrak{g}\left(  \sigma
A\sigma^{-1}\right)  $.
\end{remark}

\begin{definition}
A generalized Cartan matrix $A$ is said to be \textit{indecomposable} if it
cannot be written in the form $\sigma\left(  A_{1}\oplus A_{2}\right)
\sigma^{-1}$ for some permutation $\sigma$ and nontrivial square matrices
$A_{1}$ and $A_{2}$. Due to the above remark and to Remark \ref{rmk.g(A+A)},
we need to only consider indecomposable generalized Cartan matrices.
\end{definition}

\begin{definition}
A generalized Cartan matrix $A$ is said to be \textit{affine} if $DA\geq0$ but
$DA\not >  0$ (thus, $\det\left(  DA\right)  =0$).
\end{definition}

\begin{definition}
If $A$ is an affine generalized Cartan matrix, then $\mathfrak{g}\left(
A\right)  $ is called an \textit{affine Kac-Moody algebra}.
\end{definition}

Now let $A$ be the (usual) Cartan matrix of a simple Lie algebra, and let
$\mathfrak{g}=\mathfrak{g}\left(  A\right)  $ be this simple Lie algebra. Let
$L\mathfrak{g}=\mathfrak{g}\left[  t,t^{-1}\right]  $, and let
$\widehat{\mathfrak{g}}=L\mathfrak{g}\oplus\mathbb{C}K$ as defined long ago.

\begin{theorem}
This $\widehat{\mathfrak{g}}$ is an affine Kac-Moody algebra with generalized
Cartan matrix $\widetilde{A}$ whose $\left(  1,1\right)  $-entry is $2$ and
whose submatrix obtained by omitting the first row and the first column is
$A$. (We do not yet say what the remaining entries are.)
\end{theorem}

\textit{Proof of Theorem.} Let $\mathfrak{h}$ be the Cartan subalgebra of
$\mathfrak{g}$. Let $r=\dim\mathfrak{h}$; thus, $r$ is the rank of
$\mathfrak{g}$. Let $\left(  h_{1},h_{2},...,h_{r}\right)  $ be a
corresponding basis of $\mathfrak{h}$, and let $e_{i},f_{i}$ be standard
generators for every $i\in\left\{  1,2,...,r\right\}  $.

Let $\theta$ be the maximal root.

Let us now define elements $e_{0}=f_{\theta}\cdot t$, $f_{0}=e_{\theta}\cdot
t^{-1}$ and $h_{0}=\left[  e_{0},f_{0}\right]  =-h_{\theta}%
+\underbrace{\left(  f_{\theta},e_{\theta}\right)  }_{=1\text{ (due to our
normalization)}}K=K-h_{\theta}$ of $\widehat{\mathfrak{g}}$ (the commutator is
computed in $\widehat{\mathfrak{g}}$, not in $L\mathfrak{g}$).

Add these elements to our system of generators.

Why do we then get a system of generators of $\widehat{\mathfrak{g}}$ ?

First, $h_{i}$ for $i\in\left\{  0,1,...,r\right\}  $ are a basis of
$\widehat{\mathfrak{h}}=\mathfrak{h}\oplus\mathbb{C}K$.

Also, $\mathfrak{g}t^{0}$ is generated by $e_{i},f_{i},h_{i}$ for
$i\in\left\{  1,2,...,r\right\}  $. Now, $\mathfrak{g}t^{1}$ is an irreducible
$\mathfrak{g}$-module with lowest-weight vector $f_{\theta}\cdot t$.

$\Longrightarrow$ $U\left(  \mathfrak{g}\right)  \cdot f_{\theta
}t=\mathfrak{g}t$. Now, $\mathfrak{g}t$ generates $\mathfrak{g}t\mathbb{C}%
\left[  t\right]  $ (since $\left[  \mathfrak{g},\mathfrak{g}\right]
=\mathfrak{g}$). Similarly, $U\left(  \mathfrak{g}\right)  \cdot e_{\theta
}t^{-1}=\mathfrak{g}t^{-1}$, and $\mathfrak{g}t^{-1}$ generates $\mathfrak{g}%
t^{-1}\mathbb{C}\left[  t^{-1}\right]  $. $\Longrightarrow$ our $e_{i}$,
$f_{i}$, $h_{i}$ (including $i=0$) generate all of $\widehat{\mathfrak{g}}$.

Now to the relations.

$\left[  h_{i},h_{j}\right]  =0$ is clear for all $\left(  i,j\right)
\in\left\{  0,1,...,r\right\}  ^{2}$.

We have $\left[  h_{0},e_{0}\right]  =\left[  K-h_{\theta},f_{\theta}t\right]
=-\left[  h_{\theta},f_{\theta}\right]  t=2f_{\theta}t=2e_{0}$.

We have $\left[  h_{0},f_{0}\right]  =-2f_{0}$ similarly.

We have $\left[  e_{0},f_{0}\right]  =h_{0}$.

We have $\left[  h_{0},e_{i}\right]  =\left[  K-h_{\theta},e_{i}\right]
=-\alpha_{i}\left(  h_{\theta}\right)  e_{i}=-\left(  \alpha_{i}%
,\theta\right)  e_{i}$ $\Longrightarrow$ $a_{0,i}=-\left(  \alpha_{i}%
,\theta\right)  =\left(  \text{some nonpositive integer}\right)  $.

We have $\left[  h_{0},f_{i}\right]  =\left(  \alpha_{i},\theta\right)  f_{i}%
$, same argument.

We have $\left[  h_{i},e_{0}\right]  =\left[  h_{i},f_{\theta}t\right]
=-\theta\left(  h_{i}\right)  f_{\theta}t=-\theta\left(  h_{i}\right)
e_{0}=-\left(  \alpha_{i}^{\vee},\theta\right)  e_{0}$ (where $\alpha
_{i}^{\vee}=\dfrac{2\alpha_{i}}{\left(  \alpha_{i},\alpha_{i}\right)  }$)
$\Longrightarrow$ $a_{i,0}=-\left(  \alpha_{i}^{\vee},\theta\right)  $.

We have $\left[  h_{i},f_{0}\right]  =\left(  \alpha_{i}^{\vee},\theta\right)
f_{0}$, same argument.

We have $\left[  e_{0},f_{i}\right]  =\left[  f_{\theta}t,f_{i}\right]  =0$.

We have $\left[  e_{i},f_{0}\right]  =\left[  e_{i},e_{\theta}t^{-1}\right]
=0$.

Thus, all basic relations are satisfied.

Now let us define a grading: $\widehat{Q}=Q\oplus\mathbb{Z}\delta$, where $Q$
is the root lattice of $\mathfrak{g}$. Define $\alpha_{0}=\delta-\theta$.
$\delta\mid_{\widehat{\mathfrak{h}}}=0$. So if we think of $\alpha_{0}$ as an
element of $\widehat{\mathfrak{h}}^{\ast}$, then $\alpha_{0},\alpha
_{1},...,\alpha_{r}$ is neither linearly independent nor spanning. So the
direct sum $Q\oplus\mathbb{Z}\delta$ is an external direct sum, not an
internal one!!

$\widehat{Q}$-grading: $\deg\left(  e_{i}\right)  =\alpha_{i}$, $\deg\left(
f_{i}\right)  =-\alpha_{i}$ and $\deg\left(  h_{i}\right)  =0$ for
$i=0,1,...,r$. Also $\deg\left(  at^{k}\right)  =\deg a+k\delta$ (so, so to
speak, ``$\deg t=\delta$'').

So we have $\widehat{\mathfrak{g}}\left[  0\right]  =\widehat{\mathfrak{h}}$
and $\widehat{\mathfrak{g}}\left[  \alpha_{i}\right]  =\left\langle
e_{i}\right\rangle $ and $\widehat{\mathfrak{g}}\left[  -\alpha_{i}\right]
=\left\langle f_{i}\right\rangle $.

Note (which we won't use): $\left[  h,a\right]  =\alpha\left(  h\right)  a$,
$a\in\widehat{\mathfrak{g}}\left[  \alpha\right]  $ ``if you define things
this way''.

The only thing we now have to do is to show that $I=0$ in
$\widehat{\mathfrak{g}}$.

Let $\overline{I}$ be the projection of $I$ to $L\mathfrak{g}%
=\widehat{\mathfrak{g}}\diagup\left(  K\right)  $. Clearly, $\overline{I}%
\cap\mathfrak{h}=0$.

We must prove that $\overline{I}=0$.

But there is a \textbf{claim} that any $\widehat{Q}$-graded ideal in
$L\mathfrak{g}$ is $0$ or $L\mathfrak{g}$. (\textit{Proof:} If $J$ is a
$\widehat{Q}$-graded ideal of $L\mathfrak{g}$ different from $0$, then there
exists a nonzero $a\in\mathfrak{g}$ and an $m\in\mathbb{Z}$ such that
$at^{m}\in J$. But $at^{m}$ generates $L\mathfrak{g}$ under the action of
$L\mathfrak{g}$, since $\left[  bt^{n-m},at^{m}\right]  =\left[  b,a\right]
t^{n}$ and $\mathfrak{g}=\left[  \mathfrak{g},\mathfrak{g}\right]  $.)

Proof of Theorem complete.

Let us show how Dynkin diagrams look like for these affine Kac-Moody algebras.

Consider the case of $A_{n-1}=\mathfrak{sl}_{n}$. Then, $\theta=\left(
1,0,0,...,0,-1\right)  $. Also, $\alpha_{1}=\left(  1,-1,0,0,...,0\right)  $,
$\alpha_{2}=\left(  0,1,-1,0,0,...,0\right)  $, $...$, $\alpha_{n-1}=\left(
0,0,...,0,1,-1\right)  $. Also, $\alpha=\alpha^{\vee}$ for all simple roots
$\alpha$. We thus have $\left(  \theta,\alpha_{i}\right)  =1$ if $\alpha
\in\left\{  1,n-1\right\}  $ and $=0$ otherwise. The Dynkin diagram of
$\widehat{A_{n-1}}=A_{n-1}^{1}=\widehat{\mathfrak{sl}_{n}}$ (these are just
three notations for one and the same thing) is thus $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$ with a cyclically connected dot underneath.

The case $n=2$ is special: double link. $\circ=\circ$ double link.

Now let us consider other types. Suppose that $\theta$ is a fundamental
weight, i. e., satisfies $\left(  \theta,\alpha_{i}^{\vee}\right)  =1$ for
some $i$ and satisfies $\left(  \theta,\alpha_{i}^{\vee}\right)  =0$ for all
other $i$. (This happens for a lot of simple Lie algebras.)

To get $\widehat{D_{n}}=\widehat{\mathfrak{so}_{2n}}$, need to attach a new
vertex to the second vertex from the left.

To get $\widehat{C_{n}}=\widehat{\mathfrak{sp}_{2n}}$, need to attach a new
vertex \textbf{doubly-linked} to the first vertex from the left. (The arrow
points to the right, i. e., to the $C_{n}$ diagram.)

For $\widehat{G_{2}}$, attach a vertex on the left (where the arrow points to
the right).

For $\widehat{F_{4}}$, attach a vertex on the left (where the arrow points to
the right).

For $\widehat{E_{6}}$, attach a vertex to the ``bottom'' (the vertex off the line).

For $\widehat{E_{7}}$, attach a vertex to the short leg (to make the graph symmetric).

For $\widehat{E_{8}}$, attach a vertex to the long leg.

These are untwisted affine Lie algebras ($\widehat{\mathfrak{g}}$).

There are also twisted ones: $A_{2}^{2}$ with Cartan matrix $\left(
\begin{array}
[c]{cc}%
2 & -4\\
-1 & 2
\end{array}
\right)  $ and Dynkin diagram $\circ\left(  4\text{ arrows pointing
rightward}\right)  \circ$. We will not discuss this kind of Lie algebras here.

\subsection{\textbf{[unfinished]} Representation theory of
\texorpdfstring{$\mathfrak{g}\left(  A\right)  $}{g(A)}}

We will now work out the representation theory of $\mathfrak{g}\left(
A\right)  $.

Let us start with the case of $\mathfrak{g}\left(  A\right)  $ being
finite-dimensional. In contrast with usual courses on Lie algebras, we will
not restrict ourselves to finite-dimensional representations. We define a
Category $\mathcal{O}$ which is analogous but (in its details) somewhat
different from the one we defined above. In future, we will use only the new definition.

\begin{definition}
\label{def.O}The objects of \textit{category }$\mathcal{O}$ will be
$\mathfrak{g}$-modules $M$ such that:

\textbf{1)} The module $M$ is $\mathfrak{h}$-diagonalizable. By this we mean
that $M=\bigoplus\limits_{\mu\in\mathfrak{h}^{\ast}}M\left[  \mu\right]  $
(where $M\left[  \mu\right]  $ means the $\mu$-weight space of $M$), and every
$\mu\in\mathfrak{h}^{\ast}$ satisfies $\dim\left(  M\left[  \mu\right]
\right)  <\infty$.

\textbf{2)} Let $\operatorname*{Supp}M$ denote the set of all $\mu
\in\mathfrak{h}^{\ast}$ such that $M\left[  \mu\right]  \neq0$. Then, there
exist finitely many $\lambda_{1},\lambda_{2},...,\lambda_{n}\in\mathfrak{h}%
^{\ast}$ such that $\operatorname*{Supp}M\subseteq D\left(  \lambda
_{1}\right)  \cup D\left(  \lambda_{2}\right)  \cup...\cup D\left(
\lambda_{n}\right)  $, where for every $\lambda\in\mathfrak{h}^{\ast}$, we
denote by $D\left(  \lambda\right)  $ the subset%
\[
\left\{  \lambda-k_{1}\alpha_{1}-k_{2}\alpha_{2}-...-k_{r}\alpha_{r}%
\ \mid\ \left(  k_{1},k_{2},...,k_{r}\right)  \in\mathbb{N}^{r}\right\}
\ \ \ \ \ \ \ \ \ \ \text{of }\mathfrak{h}^{\ast}.
\]


The \textit{morphisms of category }$\mathcal{O}$ will be $\mathfrak{g}$-module homomorphisms.
\end{definition}

Examples of modules in Category $\mathcal{O}$ are Verma modules $M_{\lambda
}=M_{\lambda}^{+}$ and their irreducible quotients $L_{\lambda}$ (and all of
their quotients). Category $\mathcal{O}$ is an abelian category (in our case,
this simply means it is closed under taking subquotients and direct sums).

\begin{definition}
Let $M\in\mathcal{O}$ be a $\mathfrak{g}$-module. Then, the \textit{formal
character} of $M$ denotes the sum $\operatorname*{ch}M=\sum\limits_{\mu
\in\mathfrak{h}^{\ast}}\dim\left(  M\left[  \mu\right]  \right)  e^{\mu}$.
Here $\mathbb{C}\left[  \mathfrak{h}^{\ast}\right]  $ denotes the group
algebra of the additive group $\mathfrak{h}^{\ast}$, where this additive group
$\mathfrak{h}^{\ast}$ is written multiplicatively and every $\mu
\in\mathfrak{h}^{\ast}$ is renamed as $e^{\mu}$.

Where does this sum $\sum\limits_{\mu\in\mathfrak{h}^{\ast}}\dim\left(
M\left[  \mu\right]  \right)  e^{\mu}$ lie?

Let $\Gamma$ be a coset of $Q$ (the root lattice) in $\mathfrak{h}^{\ast}$.
Then, let $R_{\Gamma}$ denote the space $\lim\limits_{\mu\in\Gamma}e^{\mu
}\mathbb{C}\left[  \left[  e^{-\alpha_{1}},e^{-\alpha_{2}},...,e^{-\alpha_{r}%
}\right]  \right]  $ (this is a union, but not a disjoint union, since
$R_{\mu}\subseteq R_{\mu+\alpha_{i}}$ for all $i$ and $\mu$). Let
$R=\bigoplus\limits_{\Gamma\in\mathfrak{h}^{\ast}\diagup Q}R_{\Gamma}$. This
$R$ is a ring. We view $\operatorname*{ch}M$ as an element of $R$.
\end{definition}

Now, for an example, let us compute the formal character $\operatorname*{ch}%
\left(  M_{\lambda}\right)  $ of the Verma module $M_{\lambda}=U\left(
\mathfrak{n}_{-}\right)  v_{\lambda}$.

Recall that $U\left(  \mathfrak{n}_{-}\right)  $ has a
Poincar\'{e}-Birkhoff-Witt basis consisting of all elements of the form
$f_{\alpha^{\left(  1\right)  }}^{m_{1}}f_{\alpha^{\left(  2\right)  }}%
^{m_{2}}...f_{\alpha^{\left(  \ell\right)  }}^{m_{\ell}}$ where $\alpha
^{\left(  1\right)  },\alpha^{\left(  2\right)  },...,\alpha^{\left(
\ell\right)  }$ are all positive roots of $\mathfrak{g}$, and $\ell
=\dim\left(  \mathfrak{n}_{-}\right)  $. The weight of this element
$f_{\alpha^{\left(  1\right)  }}^{m_{1}}f_{\alpha^{\left(  2\right)  }}%
^{m_{2}}...f_{\alpha^{\left(  \ell\right)  }}^{m_{\ell}}$ is $-\left(
m_{1}\alpha^{\left(  1\right)  }+m_{2}\alpha^{\left(  2\right)  }+...+m_{\ell
}\alpha^{\left(  \ell\right)  }\right)  $. Thus, the weight of $f_{\alpha
^{\left(  1\right)  }}^{m_{1}}f_{\alpha^{\left(  2\right)  }}^{m_{2}%
}...f_{\alpha^{\left(  \ell\right)  }}^{m_{\ell}}v_{\lambda}$ is
$\lambda-\left(  m_{1}\alpha^{\left(  1\right)  }+m_{2}\alpha^{\left(
2\right)  }+...+m_{\ell}\alpha^{\left(  \ell\right)  }\right)  $.

Thus, $\dim\left(  M_{\lambda}\left[  \lambda-\beta\right]  \right)  $ is the
number of partitions of $\beta$ into positive roots. We denote this by
$p\left(  \beta\right)  $, and call $p$ the \textit{Kostant partition
function}.

Now, it is very easy (using geometric series) to see that%
\[
\sum\limits_{\beta\in Q_{+}}p\left(  \beta\right)  e^{-\beta}=\prod
\limits_{\substack{\alpha\text{ root;}\\a>0}}\dfrac{1}{1-e^{-\alpha}}.
\]
Thus,%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\sum\limits_{\beta\in Q_{+}%
}p\left(  \beta\right)  e^{\lambda-\beta}=e^{\lambda}\underbrace{\sum
\limits_{\beta\in Q_{+}}p\left(  \beta\right)  e^{-\beta}}_{=\prod
\limits_{\substack{\alpha\text{ root;}\\a>0}}\dfrac{1}{1-e^{-\alpha}}%
}=e^{\lambda}\prod\limits_{\substack{\alpha\text{ root;}\\a>0}}\dfrac
{1}{1-e^{-\alpha}}.
\]


\textbf{Example:} Let $\mathfrak{g}=\mathfrak{sl}_{2}$. Then,%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{e^{\lambda}%
}{1-e^{-\alpha}}=e^{\lambda}+e^{\lambda-\alpha}+e^{\lambda-2\alpha}+....
\]
Classically, one identifies weights of $\mathfrak{sl}_{2}$ with elements of
$\mathbb{C}$ (by $\omega_{1}\mapsto1$ and thus $\alpha\mapsto2$). Write $x$
for $e^{\omega_{1}}$. Then,%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{x^{\lambda}}{1-x^{-2}%
}=x^{\lambda}+x^{\lambda-2}+x^{\lambda-4}+....
\]
The quotient $L_{\lambda}$ has weights $\lambda$, $\lambda-2$, $...$,
$-\lambda$ and thus satisfies%
\[
\operatorname*{ch}\left(  L_{\lambda}\right)  =x^{\lambda}+x^{\lambda
-2}+...+x^{-\lambda}=\dfrac{x^{\lambda+1}-x^{-\lambda-1}}{x-x^{-1}}.
\]


Back to the general case of finite-dimensional $\mathfrak{g}\left(  A\right)
$. First of all, category $\mathcal{O}$ has tensor products, and they make it
into a tensor category.

\begin{proposition}
\textbf{1)} We have $\operatorname*{ch}\left(  M_{1}\otimes M_{2}\right)
=\operatorname*{ch}\left(  M_{1}\right)  \cdot\operatorname*{ch}\left(
M_{2}\right)  $.

\textbf{2)} If $N\subseteq M$ are both in $\mathcal{O}$, then
$\operatorname*{ch}M=\operatorname*{ch}N+\operatorname*{ch}\left(  M\diagup
N\right)  $.
\end{proposition}

\textit{Proof of Proposition.} \textbf{1)}
\[
\left(  M_{1}\otimes M_{2}\right)  \left[  \mu\right]  =\bigoplus
\limits_{\mu_{1}+\mu_{2}=\mu}M_{1}\left[  \mu_{1}\right]  \otimes M_{2}\left[
\mu_{2}\right]  .
\]


\textbf{2)}
\[
\left(  M\diagup N\right)  \left[  \mu\right]  =M\left[  \mu\right]  \diagup
N\left[  \mu\right]  .
\]


Now, let us generalize to the case of Kac-Moody Lie algebras (or
$\mathfrak{g}\left(  A\right)  $ for general $A$). Here we run into troubles:
For example, for $\widehat{\mathfrak{sl}_{2}}$, we have $M_{\lambda}=U\left(
\widetilde{\mathfrak{n}}_{-}\right)  v_{\lambda}$, and the vectors
$ht^{-1}v_{\lambda},ht^{-2}v_{\lambda},...$ all have weight $\lambda$ with
respect to $\widehat{\mathfrak{h}}=\left\langle h_{0},h_{1}\right\rangle $
with $h_{1}=h,$ $h_{0}=K-h$. This yields that weight spaces are
infinite-dimensional, and we cannot define characters.

Let us work around this by adding derivations.

Assume that $A$ is an $r\times r$ complex matrix. Let $\mathfrak{g}%
_{\operatorname*{ext}}\left(  A\right)  =\mathfrak{g}\left(  A\right)
\oplus\bigoplus\limits_{i=1}^{r}\mathbb{C}D_{i}$ with new relations%
\begin{align*}
\left[  D_{i},D_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i,j;\\
\left[  D_{i},e_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\neq
j;\\
\left[  D_{i},f_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\neq
j;\\
\left[  D_{i},h_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\neq
j;\\
\left[  D_{i},e_{i}\right]   &  =e_{i};\\
\left[  D_{i},f_{i}\right]   &  =-f_{i};\\
\left[  D_{i},h_{i}\right]   &  =0.
\end{align*}
Note that this definition is equivalent to making $\mathfrak{g}%
_{\operatorname*{ext}}\left(  A\right)  $ a semidirect product, so there is no
cancellation here.

We have $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  =\mathfrak{n}%
_{+}\oplus\mathfrak{h}_{\operatorname*{ext}}\oplus\mathfrak{n}_{-}$ where
$\mathfrak{h}_{\operatorname*{ext}}=\mathbb{C}^{r}\oplus\mathfrak{h}$ (here
the $\mathbb{C}^{r}$ is spanned by the $\mathbb{C}D_{i}$).

Consider $\alpha_{i}$ as maps $\mathfrak{h}_{\operatorname*{ext}}%
\rightarrow\mathbb{C}$ given by $\alpha_{i}\left(  h_{j}\right)  =a_{j,i}$ and
$\alpha_{i}\left(  D_{j}\right)  =\delta_{i,j}$.

Then, for every $h\in\mathfrak{h}_{\operatorname*{ext}}$, we have $\left[
h,e_{i}\right]  =\alpha_{i}\left(  h\right)  e_{i}$ and $\left[
h,f_{i}\right]  =-\alpha_{i}\left(  h\right)  f_{i}$.

Let $F=Q\otimes_{\mathbb{Z}}\mathbb{C}$ and $P=\mathfrak{h}^{\ast}\oplus F$.

Let $\varphi:P\rightarrow\mathfrak{h}_{\operatorname*{ext}}^{\ast}$ be given
by $\varphi\left(  h_{i}^{\ast}\right)  \left(  D_{j}\right)  =0$,
$\varphi\left(  h_{i}^{\ast}\right)  \left(  h_{j}\right)  =\delta_{i,j}$,
$\varphi\left(  \alpha_{i}\right)  \left(  D_{j}\right)  =\delta_{i,j}$,
$\varphi\left(  \alpha_{i}\right)  \left(  h_{j}\right)  =a_{j,i}$.

Easy to see $\varphi$ is an iso.

Now the trouble disappears. Do the same as for simple Lie algebras. Now
weights lie in $\mathfrak{h}_{\operatorname*{ext}}^{\ast}$.

Annoying fact: Now, even when $A$ is a Cartan matrix and $\mathfrak{g}$ is
simple finite-dimensional, this is not the same as the usual theory [what?].
But it is equivalent. Namely: Suppose $\chi\in\mathfrak{h}%
_{\operatorname*{ext}}^{\ast}$. Let $\mathcal{O}_{\chi}$ be the category of
modules whose weights lie in $\chi+F$. Therefore, $\mathcal{O}=\bigoplus
\limits_{\chi\in\mathfrak{h}^{\ast}}\mathcal{O}_{\chi}$.

\begin{proposition}
If $\chi_{1}-\chi_{2}\in\operatorname*{Im}\left(  F\rightarrow\mathfrak{h}%
^{\ast}\right)  $, then $\mathcal{O}_{\chi_{1}}\cong\mathcal{O}_{\chi_{2}}$.
\end{proposition}

(See Feigin-Zelevinsky paper for proof.)

If $A$ is invertible (in particular, for simple $\mathfrak{g}$), all
$\mathcal{O}_{\chi}$ are the same and we just have a single category
$\mathcal{O}$ (which is the category $\mathcal{O}$ we defined).

Affine case: $\operatorname*{Coker}\left(  F\rightarrow\mathfrak{h}^{\ast
}\right)  $ is $1$-dimensional, so $\chi$ has one essential parameter (namely,
the image $k$ of $\chi$ in this $\operatorname*{Coker}$). So we get a
$1$-parameter category of categories, $\mathcal{O}\left(  k\right)  $,
parametrized by a complex number $k$. In our old approach to
$\widehat{\mathfrak{g}}$, this $k$ is the level of representations (i. e., the
eigenvalue of the action of $K$). So we did not get anything new, but we have
got a uniform way to treat all cases of this kind.

\subsection{\textbf{[unfinished]} Invariant bilinear forms}

Now let us start developing the theory of invariant bilinear forms on
$\mathfrak{g}\left(  A\right)  $ and $\widetilde{\mathfrak{g}}\left(
A\right)  $.

[We denote $\mathfrak{g}\left[  \alpha\right]  $ as $\mathfrak{g}_{\alpha}$.]

Let $A$ be an indecomposable complex matrix. We want to see when we can have
nontrivial nonzero invariant symmetric bilinear forms on
$\widetilde{\mathfrak{g}}\left(  A\right)  $ and $\mathfrak{g}\left(
A\right)  $. Let us only care about forms of degree $0$, which means that they
send $\mathfrak{g}_{\alpha}\times\mathfrak{g}_{\beta}$ to $0$ unless
$\alpha+\beta=0$. It also sounds like a good goal to have the forms
nondegenerate, but this cannot always be reached. Let us impose the weaker
condition that, if $e_{i}$ and $f_{i}$ denote generators of $\mathfrak{g}%
_{\alpha_{i}}$ and $\mathfrak{g}_{-\alpha_{i}}$, respectively, then $\left(
e_{i},f_{i}\right)  =d_{i}$ for some $d_{i}\neq0$.

These conditions already force some properties upon $\mathfrak{g}\left(
A\right)  $: First,
\[
\left(  h_{i},h_{j}\right)  =\left(  h_{i},\left[  e_{j},f_{j}\right]
\right)  =-\left(  \left[  h_{i},f_{j}\right]  ,e_{j}\right)  =a_{i,j}\left(
f_{j},e_{j}\right)  =a_{i,j}d_{j},
\]
so that the symmetry of our form (and the condition $d_{i}\neq0$) enforces
$a_{i,j}d_{j}=a_{j,i}d_{i}$. Thus, if $D$ denotes the matrix
$\operatorname*{diag}\left(  d_{1},d_{2},...,d_{r}\right)  $, then $\left(
AD\right)  ^{T}=AD$. This means that $A$ is symmetrizable. (Our definition of
``symmetrizable'' spoke of $DA$ instead of $AD$, but this is simply a matter
of replacing $D$ by $D^{-1}$.)

\begin{lemma}
Let $A$ be an indecomposable symmetrizable matrix. Then, there is a unique
diagonal matrix $D$ satisfying $\left(  AD\right)  ^{T}=AD$ up to scaling.
\end{lemma}

This lemma is purely combinatorial and more or less trivial.

\begin{proposition}
Let $A$ be an indecomposable symmetrizable matrix. Then, there is at most one
invariant symmetric bilinear form of degree $0$ on $\widetilde{\mathfrak{g}%
}\left(  A\right)  $ up to scaling.
\end{proposition}

Note that the degree in ``degree $0$'' is the degree with respect to
$Q$-grading; this is a tuple.

\textit{Proof of Proposition.} Let $B$ be such a form. Then, we can view $B$
as a $\mathfrak{g}$-module homomorphism $B^{\vee}:\mathfrak{g}\rightarrow
\mathfrak{g}^{\ast}$. If we fix $d_{i}$ (uniquely up to scaling, as we know
from Lemma), then we know $B^{\vee}\left(  h_{i}\right)  $, $B^{\vee}\left(
f_{i}\right)  $ and $B^{\vee}\left(  e_{i}\right)  $ (because the form is of
degree $0$, and thus the linear maps $B^{\vee}\left(  h_{i}\right)  $,
$B^{\vee}\left(  f_{i}\right)  $ and $B^{\vee}\left(  e_{i}\right)  $ are
determined by what they do to the corresponding elements of the corresponding
degree). But $\mathfrak{g}$ is generated as a $\mathfrak{g}$-module by
$e_{i},f_{i},h_{i}$, so $B$ is uniquely determined if it exists. Proposition
is proven.

\begin{theorem}
Let $A$ be a symmetrizable matrix. Then, there is a nonzero invariant bilinear
symmetric form of degree $0$ on $\widetilde{\mathfrak{g}}\left(  A\right)  $.
(We know from the previous proposition that this form is unique up to scaling
if $A$ is indecomposable.)
\end{theorem}

\textit{Proof of Theorem (incomplete, as we will skip some steps).} First, fix
the $d_{i}$. Then, we can calculate the form by%
\begin{align*}
&  \left(  \underbrace{\left[  e_{i_{1}},\left[  e_{i_{2}},...\left[
e_{i_{n-1}},e_{i_{n}}\right]  ...\right]  \right]  }_{\in\mathfrak{g}_{\alpha
}},\underbrace{\left[  f_{j_{1}},\left[  f_{j_{2}},...\left[  f_{j_{n-1}%
},f_{j_{n}}\right]  ...\right]  \right]  }_{\in\mathfrak{g}_{-\alpha}}\right)
\\
&  =-\left(  \left[  e_{i_{1}},...\right]  ,\underbrace{\left[  \left[
e_{i_{2}},\left[  e_{i_{3}},...\left[  e_{i_{n-1}},e_{i_{n}}\right]
...\right]  \right]  ,\left[  f_{j_{1}},\left[  f_{j_{2}},...\left[
f_{j_{n-1}},f_{j_{n}}\right]  ...\right]  \right]  \right]  }_{\in
\mathfrak{g}_{-\alpha}}\right) \\
&  +...
\end{align*}
induction on $\alpha$. For details and well-definedness, see page 51 of the
Feigin-Zelevinsky paper.

Also, $\widetilde{\mathfrak{g}}\left(  A\right)  $ has such a form by pullback.

As usual, denote these forms by $\left(  \cdot,\cdot\right)  $.

\begin{proposition}
The kernel $I$ of the canonical projection $\widetilde{\mathfrak{g}}\left(
A\right)  \rightarrow\mathfrak{g}\left(  A\right)  $ is a subset of
$\operatorname*{Ker}\left(  \left(  \cdot,\cdot\right)  \right)  $.
\end{proposition}

\textit{Proof of Proposition.} We defined the form $\left(  \cdot
,\cdot\right)  $ on $\widetilde{\mathfrak{g}}\left(  A\right)  \times
\widetilde{\mathfrak{g}}\left(  A\right)  $ as the pullback of the form
$\left(  \cdot,\cdot\right)  :\mathfrak{g}\left(  A\right)  \times
\mathfrak{g}\left(  A\right)  \rightarrow\mathbb{C}$ through the canonical
projection $\widetilde{\mathfrak{g}}\left(  A\right)  \times
\widetilde{\mathfrak{g}}\left(  A\right)  \rightarrow\mathfrak{g}\left(
A\right)  \times\mathfrak{g}\left(  A\right)  $. Thus, it is clear that the
kernel of the former form contains the kernel of the canonical projection
$\widetilde{\mathfrak{g}}\left(  A\right)  \rightarrow\mathfrak{g}\left(
A\right)  $. Proposition proven.

\begin{lemma}
\textbf{1)} The center $Z$ of $\mathfrak{g}\left(  A\right)  $ is contained in
$\mathfrak{h}$, and is%
\[
Z=\left\{  \sum\limits_{i}\beta_{i}h_{i}\ \mid\ \beta_{i}\in\mathbb{C}\text{
for all }i\text{, and }\sum\limits_{i}\beta_{i}a_{i,j}=0\text{ for all
}j\right\}  .
\]


\textbf{2)} If $A$ is an indecomposable symmetrizable matrix, and $A\neq0$,
then any graded proper ideal in $\mathfrak{g}\left(  A\right)  $ is contained
in $Z$.

\textbf{3)} If $a_{i,i}\neq0$ for all $i$, then $\left[  \mathfrak{g}\left(
A\right)  ,\mathfrak{g}\left(  A\right)  \right]  =\mathfrak{g}\left(
A\right)  $.
\end{lemma}

\textit{Proof of Lemma.} \textbf{1)} Let $z$ be a nonzero central element of
$\mathfrak{g}\left(  A\right)  $. We can WLOG assume that $z$ is homogeneous.
Then, $\mathbb{C}z$ is a graded nonzero ideal of $\mathfrak{g}\left(
A\right)  $, so that $\deg z$ must be $0$, and thus $z\in\mathfrak{h}$. If
$z=\sum\limits_{i}\beta_{i}h_{i}$, then every $j$ satisfies $0=\left[
z,e_{j}\right]  =\left[  \sum\limits_{i}\beta_{i}h_{i},e_{j}\right]  =\left(
\sum\limits_{i}\beta_{i}a_{i,j}\right)  e_{j}$, so that $\sum\limits_{i}%
\beta_{i}a_{i,j}=0$.

This proves that $Z\subseteq\left\{  \sum\limits_{i}\beta_{i}h_{i}%
\ \mid\ \beta_{i}\in\mathbb{C}\text{ for all }i\text{, and }\sum
\limits_{i}\beta_{i}a_{i,j}=0\text{ for all }j\right\}  $. The reverse
inclusion is easy to see (using $\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j}$).

\textbf{2)} Let $I\neq0$ be a graded ideal. Then, $I\cap\mathfrak{h}\neq0$. So
$I=I_{+}\oplus I_{0}\oplus I_{-}$ with $I_{0}$ being a nonzero subspace of
$\mathfrak{h}$. Assume $I\not \subseteq Z$. Then we claim that $I_{+}\neq0$ or
$I_{-}\neq0$.

(In fact, otherwise, we would have $I_{+}=0$ and $I_{-}=0$, so that
$I\subseteq\mathfrak{h}$, so that there exists some $h\in I\subseteq
\mathfrak{h}$ with $h\notin Z$, so that $\left[  h,e_{j}\right]  =\lambda
e_{j}$ for some $j$ and some $\lambda\neq0$, so that $e_{j}\in I_{+}$,
contradicting $I_{+}=0$ and $I_{-}=0$.)

Let $\mathfrak{G}$ be the subset $\left\{  e_{1},e_{2},...,e_{n},f_{1}%
,f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $ of $\mathfrak{g}\left(
A\right)  $. As we know, this subset $\mathfrak{G}$ generates the Lie algebra
$\mathfrak{g}\left(  A\right)  $.

So let us WLOG assume $I_{+}\neq0$. Then there exists a nonzero $a\in
I_{+}\left[  \alpha\right]  $ for some $\alpha\neq0$. Set $J$ be the ideal
generated by $a$. In other words, $J=U\left(  \mathfrak{g}\left(  A\right)
\right)  \cdot a$. This $J$ is a graded ideal. Thus, $J\cap\mathfrak{h}\neq0$.
Hence, there exists $x\in U\left(  \mathfrak{g}\left(  A\right)  \right)  $
such that $x\rightharpoonup a\in\mathfrak{h}$ and $x\rightharpoonup a\neq0$.
We can WLOG assume that $x$ has degree $-\alpha$ and is a product of some
elements of the set $\mathfrak{G}$ (with repetitions allowed). Of course, this
product is nonempty (otherwise, $a$ itself would be in $I_{0}$, not in $I_{+}%
$), and hence (by splitting off its first factor) can be written as $\xi
\cdot\eta$ with $\xi$ being an element of the set $\mathfrak{G}$ and $\eta$
being a product of elements of $\mathfrak{G}$. Consider these $\xi$ and $\eta
$. We assume WLOG that $\eta$ is a product of elements of $\mathfrak{G}$ with
a minimum possible number of factors. Then, $\xi\notin\left\{  h_{1}%
,h_{2},...,h_{n}\right\}  $ (because otherwise, we could replace $x$ by $\eta
$, and would then, by splitting off the first factor, obtain a new $\eta$ with
an even smaller number of factors). So we have either $\xi=e_{i}$ for some
$i$, or $\xi=f_{i}$ for some $i$. Let us WLOG assume that we are in the first
case, i. e., we have $\xi=e_{i}$ for some $i$.

Let $y=\eta\rightharpoonup a$. Then, $y\in I$ (since $a\in I$ and since $I$ is
an ideal) and
\begin{align*}
\left[  \xi,y\right]   &  =\xi\rightharpoonup\underbrace{y}_{=\eta
\rightharpoonup a}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\xi\in
\mathfrak{G}\subseteq\mathfrak{g}\left(  A\right)  \right) \\
&  =\xi\rightharpoonup\left(  \eta\rightharpoonup a\right)
=\underbrace{\left(  \xi\cdot\eta\right)  }_{=x}\rightharpoonup
a=x\rightharpoonup a\in\mathfrak{h}%
\end{align*}
and $\left[  \xi,y\right]  =x\rightharpoonup a\neq0$. Since $\xi=e_{i}%
\in\mathfrak{g}_{\alpha_{i}}$ and $y$ is homogeneous, this yields that
$y\in\mathfrak{g}_{-\alpha_{i}}$. Thus, $y=\chi\cdot f_{i}$ for some $\chi
\in\mathbb{C}$. This $\chi$ is nonzero, since $y$ is nonzero (since $\left[
\xi,y\right]  \neq0$).

Since $y=\chi\cdot f_{i}$, we have $\left[  e_{i},y\right]  =\chi
\cdot\underbrace{\left[  e_{i},f_{i}\right]  }_{=h_{i}}=\chi h_{i}$. Since
$\left[  e_{i},y\right]  \in I$ (because $I$ is an ideal and $y\in I$), this
becomes $\chi h_{i}\in I$, so that $h_{i}\in I$ (since $\chi$ is nonzero).
Moreover, since $\chi\cdot f_{i}=y\in I$, we have $f_{i}\in I$ (since $\chi$
is nonzero). Altogether, we now know that $h_{i}\in I$ and $f_{i}\in I$.

If $A$ is an $1\times1$ matrix, then $a_{i,i}\neq0$ (since $A\neq0$), so that
$e_{i}=\dfrac{\left[  h_{i},e_{i}\right]  }{a_{i,i}}\in I$ (because $h_{i}\in
I$). Hence, if $A$ is an $1\times1$ matrix, then all of $e_{i}$, $f_{i}$ and
$h_{i}$ lie in $I$, so that $I=\mathfrak{g}\left(  A\right)  $ (because there
exists only one $i$).

If the size of $A$ is $>1$, there exists some $j\neq i$ such that $a_{i,j}%
\neq0$ and $a_{j,i}\neq0$ (since $A$ is indecomposable and symmetrizable), so
that $e_{j}=\dfrac{\left[  h_{i},e_{j}\right]  }{a_{i,j}}\in I$ (since
$h_{i}\in I$), furthermore $f_{j}=-\dfrac{\left[  h_{i},f_{j}\right]
}{a_{i,j}}\in I$, therefore $h_{j}=\left[  e_{j},f_{j}\right]  \in I$, and
finally $e_{i}=\dfrac{\left[  h_{j},e_{i}\right]  }{a_{j,i}}\in I$. And for
every $k\neq i$ with $a_{i,k}\neq0$ and $a_{k,i}\neq0$, we similarly get
$h_{k}$, $f_{k}$, $e_{k}$ $\in I$ etc.. By repeating this argument, we
conclude that $e_{\ell},f_{\ell},h_{\ell}\in I$ for all $\ell$ (since $A$ is
indecomposable). That is, $\mathfrak{G}\subseteq I$. Since $\mathfrak{G}$ is a
generating set of the Lie algebra $\mathfrak{g}\left(  A\right)  $, this
entails $I=\mathfrak{g}\left(  A\right)  $.

\textbf{3)} If $a_{i,i}\neq0$, then the relations (\ref{nonserre-relations})
imply that all generators are in $\left[  \mathfrak{g}\left(  A\right)
,\mathfrak{g}\left(  A\right)  \right]  $.

Qed.

\begin{proposition}
Assume that $A$ is symmetrizable. We have $\operatorname*{Ker}\left(  \left(
\cdot,\cdot\right)  \mid_{\mathfrak{g}\left(  A\right)  }\right)  =Z\left(
\mathfrak{g}\left(  A\right)  \right)  $.
\end{proposition}

\textit{Proof of Proposition.} Assume WLOG that $A$ is indecomposable.

\textbf{1)} $1\times1$ case, $A=0$ trivial: $\left[  e,f\right]  =h$, $\left[
h,e\right]  =\left[  h,f\right]  =0$, $\left(  e,f\right)  =1$. Then the
kernel of this form is a graded ideal and is not $\mathfrak{g}\left(
A\right)  $. Hence, it must be contained in $Z$ by the lemma. But
$Z\subseteq\operatorname*{Ker}\left(  \left(  \cdot,\cdot\right)
\mid_{\mathfrak{g}\left(  A\right)  }\right)  $ is easy (because $\left(
\sum\limits_{i}\beta_{i}h_{i},h_{j}\right)  =\sum\limits_{i}\beta_{i}%
a_{i,j}d_{j}=0$).

Let $F=Q\otimes_{\mathbb{Z}}\mathbb{C}=\bigoplus_{i=1}^{r}\mathbb{C}\alpha
_{i}$.

Define $\gamma:F\rightarrow\mathfrak{h}$ isomorphism by $\gamma\left(
\alpha_{i}\right)  =d_{i}^{-1}h_{i}=:h_{\alpha_{i}}$. Extend by linearity:
$\gamma\left(  \alpha\right)  $ will be called $h_{\alpha}$, $\alpha\in F$.

\textbf{Claim:} $\left(  h_{\alpha},h\right)  =\overline{\alpha}\left(
h\right)  $, where $\overline{\alpha}$ is the image of $\alpha$ in
$\mathfrak{h}^{\ast}$.

Proof: $\left(  h_{\alpha_{i}},h_{j}\right)  =d_{i}^{-1}\left(  h_{i}%
,h_{j}\right)  =d_{i}^{-1}a_{i,j}d_{j}=d_{i}^{-1}a_{j,i}d_{i}=a_{j,i}%
=\overline{\alpha_{i}}\left(  h_{j}\right)  \ \ \ \ $ ($\left[  h_{j}%
,e_{i}\right]  =a_{j,i}e_{i}$).

\begin{proposition}
If $x\in\mathfrak{g}_{\alpha}$ and $y\in\mathfrak{g}_{-\alpha}$, then $\left[
x,y\right]  =\left(  x,y\right)  h_{\alpha}$.
\end{proposition}

\textit{Proof of Proposition.} By induction over $\left\vert \alpha\right\vert
$, where $\left\vert \alpha\right\vert $ means the sum of the coordinates of
$\alpha$.

\textit{Base:} $\left\vert \alpha\right\vert =1$, $\alpha=\alpha_{i}$. Want to
prove $\left[  e_{i},f_{i}\right]  =^{?}\left(  e_{i},f_{i}\right)
h_{\alpha_{i}}$. But $\left[  e_{i},f_{i}\right]  =h_{i}$ and $\left(
e_{i},f_{i}\right)  h_{\alpha_{i}}=d_{i}d_{i}^{-1}h_{i}$, so we are done with
the base.

\textit{Step:} For $x\in\mathfrak{g}_{\alpha-\alpha_{i}}$ and $y\in
\mathfrak{g}_{\alpha-\alpha_{j}}$, we have
\begin{align*}
&  \left[  \left[  e_{i},x\right]  ,\left[  f_{j},y\right]  \right] \\
&  =\left[  \left[  e_{i},\left[  f_{j},y\right]  \right]  ,x\right]  +\left[
e_{i},\left[  x,\left[  f_{j},y\right]  \right]  \right] \\
&  =-\left(  \left[  e_{i},\left[  f_{j},y\right]  \right]  ,x\right)
h_{\alpha-\alpha_{i}}+\left(  e_{i},\left[  x,\left[  f_{j},y\right]  \right]
\right)  h_{\alpha_{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the induction assumption}\right) \\
&  =\left(  \left[  f_{j},y\right]  ,\left[  e_{i},x\right]  \right)  \left(
h_{\alpha-\alpha_{i}}+h_{\alpha_{i}}\right)  =\left(  \left[  e_{i},x\right]
,\left[  f_{j},y\right]  \right)  h_{\alpha}.
\end{align*}
Induction step complete. Proposition proven.

\begin{corollary}
If we give $\mathfrak{g}\left(  A\right)  $ the principal $\mathbb{Z}$-grading
(so that $\mathfrak{g}\left(  A\right)  \left[  n\right]  =\bigoplus
\limits_{\substack{\alpha\in Q;\\\left\vert \alpha\right\vert =n}%
}\mathfrak{g}\left(  A\right)  \left[  \alpha\right]  $), then $\mathfrak{g}%
\left(  A\right)  $ is a nondegenerate Lie algebra.
\end{corollary}

\textit{Proof.} If $\lambda\in\mathfrak{h}^{\ast}$ is such that $\lambda
\left(  h_{\alpha}\right)  \neq0$, then $\lambda\left(  \left[  x,y\right]
\right)  $ is a nondegenerate form $\mathfrak{g}_{\alpha}\times\mathfrak{g}%
_{-\alpha}\rightarrow\mathbb{C}$. Qed.

Recall $P=\mathfrak{h}^{\ast}\oplus F\cong\mathfrak{h}_{\operatorname*{ext}%
}^{\ast}$.

$\left(  \cdot,\cdot\right)  $ on $P$: $\left(  \underbrace{\varphi}%
_{\in\mathfrak{h}^{\ast}}\oplus\underbrace{\alpha}_{\in F},\underbrace{\psi
}_{\in\mathfrak{h}^{\ast}}\oplus\underbrace{\beta}_{\in F}\right)
=\psi\left(  h_{\alpha}\right)  +\varphi\left(  h_{\beta}\right)  +\left(
h_{\alpha},h_{\beta}\right)  $

$\left(  h_{\alpha_{i}},h_{\alpha_{j}}\right)  =d_{i}^{-1}d_{j}^{-1}\left(
h_{i},h_{j}\right)  =d_{i}^{-1}d_{j}^{-1}a_{i,j}d_{j}=d_{i}^{-1}a_{i,j}$.

Basis $h_{\alpha_{i}}^{\ast}\in\mathfrak{h}^{\ast}$, $\alpha_{i}\in F$
$\Longrightarrow$ matrix of the form $\left(
\begin{array}
[c]{cc}%
0 & 1\\
1 & D^{-1}A
\end{array}
\right)  $.

Inverse form on $\mathfrak{h}_{\operatorname*{ext}}$: dual basis:
$h_{\alpha_{i}},D_{i}$.

$\left(  D_{i},D_{j}\right)  =0$, $\left(  D_{i},h_{\alpha_{j}}\right)
=\delta_{i,j}$, $\left(  h_{\alpha_{i}},h_{\alpha_{j}}\right)  =d_{i}%
^{-1}a_{i,j}$.

\begin{proposition}
The form on $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)
=\mathfrak{g}\left(  A\right)  \oplus\mathbb{C}D_{1}\oplus\mathbb{C}%
D_{2}\oplus...\oplus\mathbb{C}D_{r}$ defined by this is a nondegenerate
symmetric invariant form.
\end{proposition}

\subsection{\textbf{[unfinished]} Casimir element}

We now define the Casimir element. The problem with the classical ``sum of
squares of orthonormal basis'' construction which works well in the
finite-dimensional case is that now we are infinite-dimensional and such a sum
needs to be defined.

Note that it will be a generalization of the $L_{0}$ of the Sugawara construction.

Define $\rho\in\mathfrak{h}^{\ast}$ by $\rho\left(  h_{i}\right)
=\dfrac{a_{i,i}}{2}$ (in the Kac-Moody case, this becomes $\rho\left(
h_{i}\right)  =1$).

$\left(  \rho,\rho\right)  =0$.

Case of a finite-dimensional simple Lie algebra: $\Delta=\sum\limits_{a\in
B}a^{2}=\sum\limits_{i=1}^{r}x_{i}^{2}+2h_{\rho}+2\sum\limits_{\alpha
>0}f_{\alpha}e_{\alpha}$ where $\left(  x_{i}\right)  _{i=1,...,r}$ is an
orthonormal basis of $\mathfrak{h}$.

In the infinite-dimensional case, we fix a basis $\left(  e_{\alpha}%
^{i}\right)  _{i}$ of $\mathfrak{g}_{\alpha}$ for every $\alpha$, and a dual
basis $\left(  f_{\alpha}^{i}\right)  _{i}$ of $\mathfrak{g}_{-\alpha}$ under
the inner product. Then define $\Delta_{+}=2\sum\limits_{\alpha>0}%
\sum\limits_{i}f_{\alpha}^{i}e_{\alpha}^{i}$ and $\Delta_{0}=\sum
\limits_{j}x_{j}^{2}+2h_{\rho}$ (where $\left(  x_{j}\right)  $ is an
orthonormal basis of $\mathfrak{h}_{\operatorname*{ext}}$). We set
$\Delta=\Delta_{+}+\Delta_{0}$.

Note that $\Delta_{+}$ is an infinite sum and not in $U\left(  \mathfrak{g}%
\left(  A\right)  \right)  $. But it becomes finite after applying to any
vector in a module in category $\mathcal{O}$.

\begin{theorem}
\textbf{1)} The operator $\Delta$ commutes with $\mathfrak{g}\left(  A\right)
$.

\textbf{2)} We have $\Delta\mid_{M_{\lambda}}=\left(  \lambda,\lambda
+2\rho\right)  \operatorname*{id}$.
\end{theorem}

\textit{Proof of Theorem.} Let us first prove \textbf{2)} using \textbf{1)}:

\textbf{2)} We have $\Delta v_{\lambda}=\Delta_{0}v_{\lambda}=\left(
\sum\limits_{j}\lambda\left(  x_{j}\right)  ^{2}+2\lambda\left(  h_{\rho
}\right)  \right)  v_{\lambda}=\left(  \left(  \lambda,\lambda\right)
+2\left(  \lambda,\rho\right)  \right)  v_{\lambda}=\left(  \lambda
,\lambda+2\rho\right)  v_{\lambda}$.

From \textbf{1)}, we see that every $a\in U\left(  \mathfrak{g}\left(
A\right)  \right)  $ satisfies $\Delta av_{\lambda}=a\Delta v_{\lambda
}=\left(  \lambda,\lambda+2\rho\right)  av_{\lambda}$. This proves \textbf{2)}
since $M_{\lambda}=U\left(  \mathfrak{g}\left(  A\right)  \right)  v_{\lambda
}$.

\textbf{1)} We need to show that $\left[  \Delta,e_{i}\right]  =\left[
\Delta,f_{i}\right]  =0$.

Let us prove $\left[  \Delta,e_{i}\right]  =0$ (the proof of $\left[
\Delta,f_{i}\right]  =0$ is similar).

We have $\left[  \Delta_{0},e_{i}\right]  =\left[  \sum x_{j}^{2}+2h\rho
,e_{i}\right]  =\sum x_{j}\left[  x_{j},e_{i}\right]  +\sum\left[  x_{j}%
,e_{i}\right]  x_{j}+2\left(  \alpha_{i},\rho\right)  e_{i}$

$=\sum x_{j}\underbrace{\alpha_{i}\left(  x_{j}\right)  }_{=\left(
h_{\alpha_{i}},x_{j}\right)  }e_{i}+\sum\alpha_{i}\left(  x_{j}\right)
e_{i}x_{j}+2\left(  \alpha_{i},\rho\right)  e_{i}$

$=2h_{\alpha_{i}}e_{i}-\sum\underbrace{\alpha_{i}\left(  x_{j}\right)
}_{=\left(  \alpha_{i},\alpha_{i}\right)  e_{i}}\alpha_{i}\left(
x_{j}\right)  e_{i}+2\left(  \alpha_{i},\rho\right)  e_{i}=2h_{\alpha}e_{i}$

$\Longrightarrow$ Our job is to show $\left[  \Delta_{+},e_{i}\right]
=-2h_{\alpha_{i}}e_{i}$. But

$\left[  \Delta_{+},e_{i}\right]  =2\sum\limits_{\alpha>0}f_{\alpha}%
^{j}\left[  e_{\alpha}^{j},e_{i}\right]  +2\underbrace{\sum\limits_{\alpha
>0}\left[  f_{\alpha}^{j},e_{i}\right]  e_{\alpha}^{j}}_{\substack{\text{for
}\alpha=\alpha_{i}\text{ the addend is}\\-2h_{\alpha_{i}}e_{i}\\\text{because
}f_{\alpha_{i}}=d_{i}^{-1}f_{i}\text{, }e_{\alpha_{i}}=e_{i}\text{,}\\\left[
d_{i}^{-1}f_{i},e_{i}\right]  e_{i}=-d_{i}^{-1}h_{i}e_{i}=-h_{\alpha_{i}}%
e_{i}}}$.

So we need to show that%
\[
\sum\limits_{\alpha>0}f_{\alpha}^{j}\left[  e_{\alpha}^{j},e_{i}\right]
+2\sum\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}}}\left[  f_{\alpha
}^{j},e_{i}\right]  e_{\alpha}^{j}=0.
\]
For this it is enough to check%
\[
\sum\limits_{\alpha>0}f_{\alpha}^{j}\otimes\left[  e_{\alpha}^{j}%
,e_{i}\right]  +2\sum\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}%
}}\left[  f_{\alpha}^{j},e_{i}\right]  \otimes e_{\alpha}^{j}=0.
\]
For this it is enough to check that $\left[  e_{i},e_{\alpha}^{k}\right]
=\sum\left(  e_{\beta}^{k},\left[  f_{\alpha}^{j},e_{i}\right]  \right)
e_{\alpha}^{j}$. This is somehow obvious. Proof complete.

\textbf{Exercise:} for $\widehat{\mathfrak{g}}$ (affine), $\Delta=\left(
k+h^{\vee}\right)  \left(  L_{0}-d\right)  $ (Sugawara).

\subsection{\textbf{[unfinished]} Preparations for the Weyl-Kac character
formula}

Let $A$ be a symmetrizable generalized Cartan matrix, WLOG indecomposable.

We consider the Kac-Moody algebra $\mathfrak{g}=\mathfrak{g}\left(  A\right)
\subseteq\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  $.

\begin{proposition}
The Serre relations $\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)
^{1-a_{i,j}}e_{j}=\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}=0$ hold in $\mathfrak{g}\left(  A\right)  $.
\end{proposition}

This is a part of Theorem \ref{thm.g(A).gabber-kac} (actually, the part that
we proved above).

\begin{definition}
Let $A$ be an associative algebra (with $1$, as always). Let $V$ be an $A$-module.

\textbf{(a)} Let $v\in V$. Then, the vector $v$ is said to be \textit{of
finite type} if $\dim\left(  Av\right)  <\infty$.

\textbf{(b)} The $A$-module $V$ is said to be \textit{locally finite} if every
$v\in V$ is of finite type.
\end{definition}

It is very easy to check that:

\begin{proposition}
\label{prop.locfin.equiv}Let $A$ be an associative algebra (with $1$, as
always). Let $V$ be an $A$-module. Then, $V$ is locally finite if and only if
$V$ is a sum of finite-dimensional $A$-modules.
\end{proposition}

\textit{Proof of Proposition \ref{prop.locfin.equiv} (sketched).}
$\Longrightarrow:$ Assume that $V$ is locally finite. Then, for every $v\in
V$, we have $\dim\left(  Av\right)  <\infty$ (since $v$ is of finite type), so
that $Av$ is a finite-dimensional $A$-module. Thus, $V=\sum\limits_{v\in V}Av$
is a sum of finite-dimensional $A$-modules.

$\Longleftarrow:$ Assume that $V$ is a sum of finite-dimensional $A$-modules.
Then, for every $v\in V$, the vector $v$ belongs to a sum of \textbf{finitely
many} finite-dimensional $A$-modules. But such a sum is finite-dimensional as
well. As a consequence, for every $v\in V$, the vector $v$ belongs to a
finite-dimensional $A$-module, and thus $\dim\left(  Av\right)  <\infty$, so
that $v$ is of finite type. Thus, $V$ is locally finite.

Proposition \ref{prop.locfin.equiv} is proven.

\begin{Convention}
If $\mathfrak{g}$ is a Lie algebra, then ``locally finite'' and ``of finite
type'' with respect to $\mathfrak{g}$ mean locally finite resp. of finite type
with respect to $U\left(  \mathfrak{g}\right)  $.
\end{Convention}

In the following, let $A=U\left(  \mathfrak{g}\right)  $ for $\mathfrak{g}%
=\mathfrak{g}\left(  A\right)  $.

\begin{definition}
Let $V$ be a $\mathfrak{g}\left(  A\right)  $-module. We say that $V$ is
\textit{integrable} if $V$ is locally finite under the $\mathfrak{sl}_{2}%
$-subalgebra $\left(  \mathfrak{sl}_{2}\right)  _{i}=\left\langle e_{i}%
,f_{i},h_{i}\right\rangle $ for every $i\in\left\{  1,2,...,r\right\}  $.
\end{definition}

To motivate the terminology ``integrable'', let us notice:

\begin{proposition}
If $V$ is a $\mathfrak{sl}_{2}$-module, then $V$ is locally finite if and only
if $V$ is isomorphic to a direct sum $\bigoplus\limits_{n=0}^{\infty}%
W_{n}\otimes V_{n}$, where $W_{n}$ are vector spaces and $V_{n}$ is the
irreducible representation of $\mathfrak{sl}_{2}$ of highest weight $n$ (so
that $\dim\left(  V_{n}\right)  =n+1$) for every $n\in\mathbb{N}$. (In such a
direct sum, we have $W_{n}\cong\operatorname*{Hom}\nolimits_{\mathfrak{sl}%
_{2}}\left(  V_{n},V\right)  $.)

Locally-finite $\mathfrak{sl}_{2}$-modules can be lifted to modules over the
\textbf{algebraic group} $\operatorname*{SL}\nolimits_{2}\left(
\mathbb{C}\right)  $.
\end{proposition}

Since lifting is called ``integrating'' (in analogy to geometry, where an
action of a Lie group gives rise to an action of the corresponding of the Lie
algebra by ``differentiation'', and thus the converse operation, when it makes
sense, is called ``integration''), the last sentence of this proposition
explains the name ``integrable''.

\begin{proposition}
\label{prop.weylkac.gint}The $\mathfrak{g}$-module $\mathfrak{g}%
=\mathfrak{g}\left(  A\right)  $ itself is integrable.
\end{proposition}

The proof of this proposition is based on the following lemma:

\begin{lemma}
\label{lem.weylkac.fintypfintyp}Let $\mathfrak{a}$ be a Lie algebra, and
$\mathfrak{b}$ be another Lie algebra. Assume that we are given a Lie algebra
homomorphism $\mathfrak{b}\rightarrow\operatorname*{Der}\mathfrak{a}$; this
makes $\mathfrak{a}$ into a $\mathfrak{b}$-module. Then, if $x,y\in
\mathfrak{a}$ are of finite type for $\mathfrak{b}$, then so is $\left[
x,y\right]  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.fintypfintyp}.} In $\mathfrak{a}$ (not
in $U\left(  \mathfrak{a}\right)  $), we have%
\[
U\left(  \mathfrak{b}\right)  \cdot\left[  x,y\right]  \subseteq\left[
\underbrace{U\left(  \mathfrak{b}\right)  \cdot x}_{\text{finite dimensional}%
},\underbrace{U\left(  \mathfrak{b}\right)  \cdot y}_{\text{finite
dimensional}}\right]  .
\]
Hence, $U\left(  \mathfrak{b}\right)  \cdot\left[  x,y\right]  $ is
finite-dimensional. Hence, $\left[  x,y\right]  $ is of finite type for
$\mathfrak{b}$. Lemma \ref{lem.weylkac.fintypfintyp} is proven.

\textit{Proof of Proposition \ref{prop.weylkac.gint}.} We know that $e_{i}$ is
of finite type under $\left(  \mathfrak{sl}_{2}\right)  _{i}$ (in fact,
$e_{i}$ generates a $3$-dimensional representation of $\left(  \mathfrak{sl}%
_{2}\right)  _{i}$), and that $e_{j}$ is of finite type under $\left(
\mathfrak{sl}_{2}\right)  _{i}$ for every $j\neq i$ (in fact, $e_{j}$
generates a representation of dimension $1-a_{i,j}$). The same applies to
$f_{j}$, and hence also to $h_{j}$ (by Lemma \ref{lem.weylkac.fintypfintyp}).
Hence (again using Lemma \ref{lem.weylkac.fintypfintyp}), the whole
$\mathfrak{g}\left(  A\right)  $ is locally finite under $\left(
\mathfrak{sl}_{2}\right)  _{i}$. [Fix some stuff here.] Proposition
\ref{prop.weylkac.gint} is proven.

\begin{proposition}
If $V$ is a $\mathfrak{g}\left(  A\right)  $-module, then $V$ is integrable if
and only if there exists a generating family $\left(  v_{\alpha}\right)
_{\alpha\in\mathfrak{A}}$ of the $\mathfrak{g}\left(  A\right)  $-module $V$
such that each $v_{\alpha}$ is of finite type under $\left(  \mathfrak{sl}%
_{2}\right)  _{i}$ for each $i$.
\end{proposition}

Note that this proposition could just as well be formulated for every Lie
algebra $\mathfrak{g}$ instead of $\mathfrak{g}\left(  A\right)  $.

\textit{Proof of Proposition.} $\Longleftarrow:$ Let $v\in V$. We need to show
that $v$ is of finite type under $\left(  \mathfrak{sl}_{2}\right)  _{i}$ for
all $i$.

Pick some $i\in\left\{  1,2,...,r\right\}  $. Let $\mathfrak{g}=\mathfrak{g}%
\left(  A\right)  $.

Fix some $i$. Then, there exist $i_{1},i_{2},...,i_{m}\in\mathfrak{A}$ such
that $v\in U\left(  \mathfrak{g}\right)  \cdot v_{i_{1}}+U\left(
\mathfrak{g}\right)  \cdot v_{i_{2}}+...+U\left(  \mathfrak{g}\right)  \cdot
v_{i_{m}}$. WLOG assume that $i_{1}=1$, $i_{2}=2$, $...$, $i_{m}=m$, and
denote the $\mathfrak{g}$-submodule $U\left(  \mathfrak{g}\right)  \cdot
v_{1}+U\left(  \mathfrak{g}\right)  \cdot v_{2}+...+U\left(  \mathfrak{g}%
\right)  \cdot v_{m}$ of $V$ by $V^{\prime}$. Then, $v\in U\left(
\mathfrak{g}\right)  \cdot v_{i_{1}}+U\left(  \mathfrak{g}\right)  \cdot
v_{i_{2}}+...+U\left(  \mathfrak{g}\right)  \cdot v_{i_{m}}=U\left(
\mathfrak{g}\right)  \cdot v_{1}+U\left(  \mathfrak{g}\right)  \cdot
v_{2}+...+U\left(  \mathfrak{g}\right)  \cdot v_{m}=V^{\prime}\subseteq V$.

Pick a finite-dimensional $\left(  \mathfrak{sl}_{2}\right)  _{i}%
$-subrepresentation $W$ of $V^{\prime}$ such that $v_{1},v_{2},...,v_{m}\in
W$. (This is possible because $v_{1},v_{2},...,v_{m}$ are of finite type under
$\left(  \mathfrak{sl}_{2}\right)  _{i}$.) Then we have a surjective
homomorphism of $\left(  \mathfrak{sl}_{2}\right)  _{i}$-modules $U\left(
\mathfrak{g}\right)  \otimes W\rightarrow V^{\prime}$ (namely, the
homomorphism sending $x\otimes w$ to $xw$), where $\mathfrak{g}$ acts on
$U\left(  \mathfrak{g}\right)  $ by adjoint action, and where $\left(
\mathfrak{sl}_{2}\right)  _{i}$ acts on $U\left(  \mathfrak{g}\right)  $ by
restricting the $\mathfrak{g}$-action on $U\left(  \mathfrak{g}\right)  $ to
$\left(  \mathfrak{sl}_{2}\right)  _{i}$. So it suffices to show that
$U\left(  \mathfrak{g}\right)  $ is integrable for the adjoint action of
$\mathfrak{g}$. But by the symmetrization map (which is an isomorphism by
PBW), we have $U\left(  \mathfrak{g}\right)  \cong S\left(  \mathfrak{g}%
\right)  =\bigoplus\limits_{m\in\mathbb{N}}S^{m}\left(  \mathfrak{g}\right)  $
(as $\mathfrak{g}$-modules) (this is true for every Lie algebra over a field
of characteristic $0$). Since $S^{m}\left(  \mathfrak{g}\right)  $ injects
into $\mathfrak{g}^{\otimes m}$, and since $\mathfrak{g}^{\otimes m}$ is
integrable (because $\mathfrak{g}$ is (in fact, it is easy to see that if $X$
and $Y$ are locally finite $\mathfrak{a}$-modules, then so is $X\otimes Y$)),
this yields that $U\left(  \mathfrak{g}\right)  $ is integrable. Hence,
$U\left(  \mathfrak{g}\right)  \otimes W$ is a locally finite $\left(
\mathfrak{sl}_{2}\right)  _{i}$-module, and thus $V^{\prime}$ (being a
quotient module of $U\left(  \mathfrak{g}\right)  \otimes W$) is a locally
finite $\left(  \mathfrak{sl}_{2}\right)  _{i}$-module also as well. Hence,
$v$ (being an element of $V^{\prime}$) is of finite type under $\left(
\mathfrak{sl}_{2}\right)  _{i}$.

$\Longrightarrow:$ Trivial (take all vectors of $V$ as generators).

Proposition proven.

\begin{corollary}
Let $L_{\lambda}$ be the irreducible highest-weight module for $\mathfrak{g}%
\left(  A\right)  $. Then, $L_{\lambda}$ is integrable if and only if for
every $i\in\left\{  1,2,...,r\right\}  $, the value $\lambda\left(
h_{i}\right)  $ is a nonnegative integer.
\end{corollary}

\textit{Proof of Corollary.} $\Longrightarrow:$ Assume that $L_{\lambda}$ is
integrable. Consider the element $v_{\lambda}$ of $L_{\lambda}$. Since
$L_{\lambda}$ is integrable, we know that $v_{\lambda}$ is of finite type
under $\left(  \mathfrak{sl}_{2}\right)  _{i}$. In other words, $U\left(
\left(  \mathfrak{sl}_{2}\right)  _{i}\right)  v_{\lambda}$ is a
finite-dimensional $\left(  \mathfrak{sl}_{2}\right)  _{i}$-module. Also, we
know that $v_{\lambda}\neq0$, $e_{i}v_{\lambda}=0$ and $h_{i}v_{\lambda
}=\lambda\left(  h_{i}\right)  v_{\lambda}$. Hence, Lemma
\ref{lem.serre-gen.sl2} \textbf{(c)} (applied to $\left(  \mathfrak{sl}%
_{2}\right)  _{i}$, $e_{i}$, $h_{i}$, $f_{i}$, $U\left(  \left(
\mathfrak{sl}_{2}\right)  _{i}\right)  v_{\lambda}$, $v_{\lambda}$ and
$\lambda\left(  h_{i}\right)  $ instead of $\mathfrak{sl}_{2}$, $e$, $h$, $f$,
$V$, $x$ and $\lambda$) yields that $\lambda\left(  h_{i}\right)
\in\mathbb{N}$ and $f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}=0$. In
particular, $\lambda\left(  h_{i}\right)  $ is a nonnegative integer.

$\Longleftarrow:$ We have%
\begin{align*}
e_{i}f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}  &  =\left(
\lambda\left(  h_{i}\right)  +1\right)  \underbrace{\left(  \lambda\left(
h_{i}\right)  -\left(  \lambda\left(  h_{i}\right)  +1\right)  +1\right)
}_{=0}f_{i}^{\lambda\left(  h_{i}\right)  }v_{\lambda}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the formula }e_{i}f_{i}^{m}v_{\lambda
}=m\left(  \lambda\left(  h_{i}\right)  -m+1\right)  f_{i}^{m-1}v_{\lambda
}\right) \\
&  =0.
\end{align*}
Hence, $f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}$ must also be zero
(since otherwise, this vector would generate a proper graded submodule). This
implies that $v_{\lambda}$ generates a finite-dimensional $\left(
\mathfrak{sl}_{2}\right)  _{i}$-module of dimension $\lambda\left(
h_{i}\right)  +1$ with basis $\left(  v_{\lambda},f_{i}v_{\lambda}%
,...,f_{i}^{\lambda\left(  h_{i}\right)  }v_{\lambda}\right)  $. Hence,
$v_{\lambda}$ is of finite type with respect to $\left(  \mathfrak{sl}%
_{2}\right)  _{i}$.

By the previous proposition, this yields that $L_{\lambda}$ is integrable.
Proof of Corollary complete.

\begin{remark}
Assume that for every $i\in\left\{  1,2,...,r\right\}  $, the value
$\lambda\left(  h_{i}\right)  $ is a nonnegative integer. Then, the relations
$f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}=0$ are defining for
$L_{\lambda}$.
\end{remark}

We will not prove this now, but this will follow from things we do later (from
the main theorem for the character formula).

\begin{definition}
A weight $\lambda$ for which all $\lambda\left(  h_{i}\right)  $ are
nonnegative integers is called \textit{integral} (for $\mathfrak{g}\left(
A\right)  $ or for $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  $).
\end{definition}

Now, our next goal is to compute the character of $L_{\lambda}$ for any
dominant integral weight $\lambda$.

For finite-dimensional simple Lie algebras, these $L_{\lambda}$ are exactly
the finite-dimensional irreducible representations, and their characters can
be computed by the well-known Weyl character formula. So our goal is to
generalize this formula.

The Weyl character formula involves a summation over the Weyl group. So, first
of all, we need to define a ``Weyl group'' for Kac-Moody Lie algebras.

\subsection{\textbf{[unfinished]} Weyl group}

\begin{definition}
Consider $P=\mathfrak{h}^{\ast}\oplus F$. We know that there is a
nondegenerate form $\left(  \cdot,\cdot\right)  $ on $P$, and we have $\dim
P=2r$. Let $i\in\left\{  1,2,...,r\right\}  $. Let $r_{i}:P\rightarrow P$ be
the map given by $r_{i}\left(  \chi\right)  =\chi-\chi\left(  h_{i}\right)
\alpha_{i}$.
\end{definition}

Note that $r_{i}$ is an involution, since%
\[
r_{i}^{2}\left(  \chi\right)  =\chi-\chi\left(  h_{i}\right)  \alpha_{i}%
-\chi\left(  h_{i}\right)  \alpha_{i}+\chi\left(  h_{i}\right)
\underbrace{\alpha_{i}\left(  h_{i}\right)  }_{=2}\alpha_{i}=\chi
\]
for every $\chi\in P$. Since $r_{i}\left(  \alpha_{i}\right)  =-\alpha_{i}$,
this yields $\det\left(  r_{i}\right)  =-1$.

Easy to check that $\left(  r_{i}x,r_{i}y\right)  =\left(  x,y\right)  $ for
all $x,y\in P$.

\begin{proposition}
Let $V$ be an integrable $\mathfrak{g}\left(  A\right)  $-module. Then, for
each $i\in\left\{  1,2,...,r\right\}  $ and any $\mu\in P$, we have an
isomorphism $V\left[  \mu\right]  \rightarrow V\left[  r_{i}\mu\right]  $. In
particular, $\dim\left(  V\left[  \mu\right]  \right)  =\dim\left(  V\left[
r_{i}\mu\right]  \right)  $.
\end{proposition}

\textit{Proof of Proposition.} We have $r_{i}\mu=\mu-\mu\left(  h_{i}\right)
\alpha_{i}$. Since $V$ is integrable for $\left(  \mathfrak{sl}_{2}\right)
_{i}$, we know that $\mu\left(  h_{i}\right)  $ is an integer. We have
$\left(  r_{i}\mu\right)  \left(  h_{i}\right)  =-\mu\left(  h_{i}\right)  $.
Hence, we can assume WLOG that $\mu\left(  h_{i}\right)  $ is nonnegative
(because otherwise, we can switch $\mu$ with $r_{i}\mu$, and it will change
sign). Then we have $f_{i}^{\mu\left(  h_{i}\right)  }:V\left[  \mu\right]
\rightarrow V\left[  r_{i}\mu\right]  $.

I claim that $f_{i}^{\mu\left(  h_{i}\right)  }$ is an isomorphism.

This follows from:

\begin{lemma}
If $V$ is a locally finite $\mathfrak{sl}_{2}$-module, then $f^{m}:V\left[
m\right]  \rightarrow V\left[  -m\right]  $ is an isomorphism.
\end{lemma}

\begin{definition}
The \textit{Weyl group of} $\mathfrak{g}\left(  A\right)  $ is defined as the
subgroup of $\operatorname*{GL}\left(  P\right)  $ generated by the $r_{i}$.
This Weyl group is denoted by $W$. The elements $r_{i}$ are called
\textit{simple reflections}.
\end{definition}

We will not prove:

\begin{remark}
The Weyl group $W$ is finite if and only if $A$ is a Cartan matrix (of a
finite-dimensional Lie algebra).
\end{remark}

\begin{proposition}
\label{prop.weylkac.prop0}\textbf{1)} The form $\left(  \cdot,\cdot\right)  $
on $P$ is $W$-invariant.

\textbf{2)} There exists an isomorphism $V\left[  \mu\right]  \rightarrow
V\left[  w\mu\right]  $ for every $\mu\in P$, $w\in W$ and any integrable $V$.

\textbf{3)} The set of roots $R$ is $W$-invariant. (We recall that a
\textit{root} means a nonzero element $\alpha\in F=Q\otimes_{\mathbb{Z}%
}\mathbb{C}$ such that $\mathfrak{g}_{\alpha}\neq0$. We consider $F$ as a
subspace of $P$.)

\textbf{4)} We have $r_{i}\left(  \alpha_{i}\right)  =-\alpha_{i}$. Moreover,
$r_{i}$ induces a permutation of all positive roots except for $\alpha_{i}$.
\end{proposition}

\textit{Proof of Proposition.} \textbf{1)} and \textbf{2)} follow easily from
the corresponding statement for generators proven above.

\textbf{3)} By part \textbf{2)}, the set of weights $P\left(  V\right)  $ of
an integrable $\mathfrak{g}$-module $V$ is $W$-invariant. (Here, ``weight''
means a weight whose weight subspace is nonzero.) Applied to $V=\mathfrak{g}$,
this implies \textbf{3)} (since $P\left(  \mathfrak{g}\right)  =0\cup R$).

\textbf{4)} Proving $r_{i}\left(  \alpha_{i}\right)  =-\alpha_{i}$ is
straightforward. Now for the other part:

Any positive root can be written as $\alpha=\sum_{i}k_{i}\alpha_{i}$ where all
$k_{i}$ are $\geq0$ and $\sum_{i}k_{i}>0$.

Thus, for such a root, $r_{i}\left(  \alpha\right)  =\alpha-\alpha\left(
h_{i}\right)  \alpha_{i}=\sum_{j\neq i}k_{j}\alpha_{j}+\left(  k_{i}%
-\alpha\left(  h_{i}\right)  \right)  \alpha_{i}$.

If there exists a $j\neq i$ such that $k_{j}>0$, then $r_{i}\left(
\alpha\right)  $ must be a positive root (since there is no such thing as a
partly-negative-partly-positive root).

Alternative: $k_{j}=0$ for all $j\neq i$. But then $\alpha=k_{i}\alpha_{i}$,
so that $k_{i}=1$ (because a positive multiple of a simple root is not a root,
unless we are multiplying with $1$), but this is the case we excluded
(``except for $\alpha_{i}$''). Proposition proven.

\subsection{\textbf{[unfinished]} The Weyl-Kac character formula}

\begin{theorem}
[Kac]\label{thm.weylkac.weylkac}Denote by $P_{+}$ the set $\left\{  \chi\in
P\ \mid\ \chi\left(  h_{i}\right)  \in\mathbb{N}\text{ for all }i\in\left\{
1,2,...,r\right\}  \right\}  $.

Let $\chi$ be a dominant integral weight of $\mathfrak{g}\left(  A\right)  $.
(This means that $\chi\left(  h_{i}\right)  $ is a nonnegative integer for
every $i\in\left\{  1,2,...,r\right\}  $.) Let $V$ be an integrable
highest-weight $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  $-module
with highest weight $\chi$. Then:

\textbf{(1)} The $\mathfrak{g}$-module $V$ is isomorphic to $L_{\chi}$. (In
other words, the $\mathfrak{g}$-module $V$ is irreducible.)

\textbf{(2)} The character of $V$ is%
\[
\operatorname*{ch}\left(  V\right)  =\dfrac{\sum\limits_{w\in W}\det\left(
w\right)  \cdot e^{w\left(  \chi+\rho\right)  -\rho}}{\prod\limits_{\alpha
>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)
}}\ \ \ \ \ \ \ \ \ \ \text{in }R.
\]
Here, we recall that $R$ is the ring $\lim\limits_{\lambda\in P_{+}}%
e^{\lambda}\mathbb{C}\left[  \left[  e^{-\alpha_{1}},e^{-\alpha_{2}%
},...,e^{-\alpha_{r}}\right]  \right]  $ (note that this term increases when
$\lambda$ is changed to $\lambda+\alpha_{i}$) in which the characters are defined.

Here, $\rho$ is the element of $\mathfrak{h}^{\ast}$ satisfying $\rho\left(
h_{i}\right)  =1$ (as defined above). Since $\mathfrak{h}^{\ast}\subseteq P$,
this $\rho$ becomes an element of $P$.

Note that $\det\left(  w\right)  $ is always $1$ or $-1$ (and, in fact, equals
$\left(  -1\right)  ^{k}$, where $w$ is written in the form $w=r_{i_{1}%
}r_{i_{2}}...r_{i_{k}}$).
\end{theorem}

Part \textbf{(2)} of this theorem is called the \textit{Weyl-Kac character
formula}.

We want to prove this theorem.

Since $\chi$ is a dominant integral weight, we have $\chi\in P_{+}$.

Some comments on the theorem:

First of all, part \textbf{(2)} implies part \textbf{(1)}, since both $V$ and
$L_{\chi}$ satisfy the conditions of the Theorem and thus (according to part
\textbf{(2)}) share the same character, but we also have a surjective
homomorphism $\varphi:V\rightarrow L_{\chi}$, so (because of the characters
being the same) it is an isomorphism. Thus, we only need to bother about
proving part \textbf{(2)}.

Secondly, let us remark that the theorem yields $L_{\lambda}=M_{\lambda
}\diagup\left\langle f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}%
\ \mid\ i\in\left\{  1,2,...,r\right\}  \right\rangle $ for all dominant
integral weights $\lambda$. Indeed, denote $M_{\lambda}\diagup\left\langle
f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}\ \mid\ i\in\left\{
1,2,...,r\right\}  \right\rangle $ by $L_{\lambda}^{\prime}$. Then,
$L_{\lambda}^{\prime}$ is integrable (as we showed above more or less; more
precisely, we showed that $L_{\lambda}$ was integrable, but this proof went
exactly through proving that $L_{\lambda}^{\prime}$ is integrable), so that
the theorem is still applicable to $L_{\lambda}^{\prime}$ and we obtain
$L_{\lambda}^{\prime}\cong L_{\lambda}$.

Our third remark: In the case of a simple finite-dimensional Lie algebra
$\mathfrak{g}$, we have%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{e^{\lambda}}%
{\prod\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)  }.
\]
The denominator can be rewritten $\prod\limits_{\alpha>0}\left(  1-e^{-\alpha
}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }$, since $\dim\left(
\mathfrak{g}_{\alpha}\right)  =1$ for all roots $\alpha$.

In the case of Kac-Moody Lie algebras $\mathfrak{g}=\mathfrak{g}\left(
A\right)  $, we can use similar arguments to show that%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{e^{\lambda}}%
{\prod\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(
\mathfrak{g}_{\alpha}\right)  }}.
\]


So the Weyl-Kac character formula can be written as%
\[
\operatorname*{ch}\left(  V\right)  =\sum\limits_{w\in W}\det\left(  w\right)
\cdot\operatorname*{ch}\left(  M_{w\left(  \chi+\rho\right)  -\rho}\right)  .
\]


This formula can be proven using the BGG\footnote{Bernstein-Gelfand-Gelfand}
resolution (in fact, it is obtained as the Euler character of that
resolution), but we will take a different route here.

Another remark before we prove the formula. The Weyl-Kac character formula has
the following corollary:

\begin{corollary}
[Weyl-Kac denominator formula]We have $\prod\limits_{\alpha>0}\left(
1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }%
=\sum\limits_{w\in W}\det\left(  w\right)  \cdot e^{w\rho-\rho}$.
\end{corollary}

\textit{Proof of Corollary (using Weyl-Kac character formula).} Set $\chi=0$.
Then $L_{\chi}=\mathbb{C}$, so that $\operatorname*{ch}\left(  L_{\chi
}\right)  =1$ but on the other hand $\operatorname*{ch}\left(  L_{\chi
}\right)  =\dfrac{\sum\limits_{w\in W}\det\left(  w\right)  \cdot
e^{w\rho-\rho}}{\prod\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)
^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }}$. Thus, $\prod\limits_{\alpha
>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)
}=\sum\limits_{w\in W}\det\left(  w\right)  \cdot e^{w\rho-\rho}$.

To prove the Weyl-Kac character formula, we will have to show several lemmas.

\begin{lemma}
\label{lem.weylkac.1}Let $\chi\in P_{+}$.

\textbf{(1)} Then, $W\chi\subseteq D\left(  \chi\right)  $ (where, as we
recall, $D\left(  \chi\right)  $ denotes the set $\left\{  \chi-\sum_{i}%
k_{i}\alpha_{i}\ \mid\ k_{i}\in\mathbb{N}\text{ for all }i\right\}  $.

\textbf{(2)} If $D\subseteq D\left(  \chi\right)  $ is a $W$-invariant subset,
then $D\cap P_{+}\neq\varnothing$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.1}.} \textbf{(1)} Consider $L_{\chi}$.
Since $L_{\chi}$ is integrable, the set $P\left(  L_{\chi}\right)  $ is
$W$-invariant, so that $W\chi\subseteq P\left(  L_{\chi}\right)  $. But
$P\left(  L_{\chi}\right)  \subseteq D\left(  \chi\right)  $, since any weight
of $L_{\chi}$ is $\chi$ minus a sum of positive roots. Part \textbf{(1)} is proven.

\textbf{(2)} Let $\psi\in D$. Pick $w\in W$ such that $x-w\psi=\sum_{i}%
k_{i}\alpha_{i}$ with nonnegative integers $k_{i}$ and minimal $\sum_{i}k_{i}%
$. We claim that this $w$ satisfies $w\psi\in P_{+}$. This, of course, will
prove part \textbf{(2)}.

To prove $w\psi\in P_{+}$, assume that $w\psi\notin P_{+}$. Then, there exists
an $i$ such that $\left(  w\psi,\alpha_{i}\right)  =d_{i}^{-1}\left(
w\psi\right)  \left(  h_{i}\right)  <0$. (Note that all the $d_{i}$ are $>0$.)
Then, $r_{i}w\psi=w\psi-\left(  w\psi\right)  \left(  h_{i}\right)  \alpha
_{i}$, so that $\chi-r_{i}w\psi=\chi-w\psi+\left(  w\psi\right)  \left(
h_{i}\right)  \alpha_{i}=\sum_{j}k_{j}\alpha_{j}+\left(  w\psi\right)  \left(
h_{i}\right)  \alpha_{i}=\sum_{j}k_{j}^{\prime}\alpha_{j}$ and $\sum_{j}%
k_{j}^{\prime}=\sum_{j}k_{j}+\left(  w\psi\right)  \left(  h_{i}\right)
<\sum_{j}k_{j}$. This contradicts the minimality in our choice of $w$. Part
\textbf{(2)} is thus proven.

\begin{corollary}
\label{cor.weylkac.2}Let $w\in W$ satisfy $w\neq1$. Then, there exists $i$
such that $w\alpha_{i}<0$. (By $w\alpha_{i}<0$ we mean that $w\alpha_{i}$ is a
negative root.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.weylkac.2}.} Choose $\chi\in P_{+}$ such
that $w\chi\neq\chi$. (Such a $\chi$ always exists, due to the definition of
$P_{+}$). Then, $w^{-1}\chi=\chi-\sum k_{i}\alpha_{i}$ for some $k_{i}%
\in\mathbb{N}$ (by Lemma \ref{lem.weylkac.1} \textbf{(1)}). Hence,%
\[
\chi=ww^{-1}\chi=w\chi-\sum k_{i}w\alpha_{i}=\left(  \chi-\sum k_{i}^{\prime
}\alpha_{i}\right)  -\sum k_{i}w\alpha_{i}.
\]
Thus, $\sum k_{i}^{\prime}\alpha_{i}+\sum k_{i}w\alpha_{i}=0$. But $\sum
k_{i}^{\prime}>0$, so there must exist an $i$ such that $w\alpha_{i}<0$.
Corollary \ref{cor.weylkac.2} is proven.

\begin{proposition}
\label{prop.weylkac.3}Let $\varphi,\psi\in P$ be such that $\varphi\left(
h_{i}\right)  >0$ and $\psi\left(  h_{i}\right)  \geq0$ for each $i$. Let
$w\in W$.

Then, $w\varphi=\psi$ if and only if $\varphi=\psi$ and $w=1$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.3}.} For every $i$, we have
$\varphi\left(  h_{i}\right)  >0$ if and only if $\left(  \varphi,\alpha
_{i}\right)  >0$. Now suppose that there exists a $w\neq1$ such that
$w\varphi=\psi$. Then, by Corollary \ref{cor.weylkac.2}, there exists an $i$
such that $w\alpha_{i}<0$. Then, $\left(  \varphi,\alpha_{i}\right)  >0$ but
$\left(  \varphi,\alpha_{i}\right)  =\left(  w^{-1}\psi,\alpha_{i}\right)
=\left(  \psi,w\alpha_{i}\right)  \leq0$. This is a contradiction. Proposition
\ref{prop.weylkac.3} is proven.

Next, notice that $W$ acts on $R$.

\begin{proposition}
\label{prop.weylkac.4}Let $K$ denote the Weyl-Kac denominator $\prod
\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}%
_{\alpha}\right)  }$. Then, $w\cdot K=\det\left(  w\right)  \cdot K$ for every
$w\in W$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.4}.} We can WLOG take $w=r_{i}$
(since $\det$ is multiplicative). Then,%
\begin{align*}
r_{i}K  &  =e^{r_{i}\rho}\prod\limits_{\alpha>0}\left(  1-e^{-r_{i}\alpha
}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }=e^{r_{i}\rho}\left(
1-e^{+\alpha_{i}}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha_{i}}\right)
}\prod\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}}}\left(
1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.weylkac.prop0}%
}\right) \\
&  =e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}}\right)  \prod
\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}}}\left(  1-e^{-\alpha
}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\dim\left(  \mathfrak{g}_{\alpha_{i}%
}\right)  =1\right) \\
&  =\dfrac{e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}}\right)  }{e^{\rho}\left(
1-e^{-\alpha_{i}}\right)  }\cdot K.
\end{align*}
Thus, we must only prove that $\dfrac{e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}%
}\right)  }{e^{\rho}\left(  1-e^{-\alpha_{i}}\right)  }=-1$.

But this is very easy: We have $r_{i}\rho=\rho-\underbrace{\rho\left(
h_{i}\right)  }_{=1}\alpha_{i}=\rho-\alpha_{i}$, so that
\[
\dfrac{e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}}\right)  }{e^{\rho}\left(
1-e^{-\alpha_{i}}\right)  }=\dfrac{e^{\rho-\alpha_{i}}\left(  1-e^{+\alpha
_{i}}\right)  }{e^{\rho}\left(  1-e^{-\alpha_{i}}\right)  }=\dfrac
{e^{-\alpha_{i}}\left(  1-e^{+\alpha_{i}}\right)  }{1-e^{-\alpha_{i}}}%
=\dfrac{e^{-\alpha_{i}}-1}{1-e^{-\alpha_{i}}}=-1.
\]
Proposition \ref{prop.weylkac.4} is proven.

\begin{proposition}
\label{prop.weylkac.5}Let $\mu,\nu\in P_{+}$ be such that $\mu\in D\left(
\nu\right)  $ and $\mu\neq\nu$. Then, $\left(  \nu+\rho\right)  ^{2}-\left(
\mu+\rho\right)  ^{2}>0$. Here, $\lambda^{2}$ is defined to mean the inner
product $\left(  \lambda,\lambda\right)  $.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.5}.} We have $\nu-\mu
=\sum\limits_{i}k_{i}\alpha_{i}$ for some $k_{i}\geq0$ (since $\mu\in D\left(
\nu\right)  $). There exists an $i$ such that $k_{i}>0$ (because $\mu\neq\nu
$). Now,%
\[
\left(  \nu+\rho\right)  ^{2}-\left(  \mu+\rho\right)  ^{2}=\left(  \nu
-\mu,\mu+\nu+2\rho\right)  =\sum\limits_{i}k_{i}\left(  \alpha_{i},\mu
+\nu+2\rho\right)  .
\]
But now use $\left(  \alpha_{i},\mu\right)  \geq0$ (since $\mu\in P_{+}$),
also $\left(  \alpha_{i},\nu\right)  \geq0$ (since $\nu\in P_{+}$) and
$\left(  \alpha_{i},\rho\right)  =d_{i}^{-1}>0$ to conclude that this is $>0$
(since there exists an $i$ such that $k_{i}>0$). Proposition
\ref{prop.weylkac.5} is proven.

\begin{proposition}
\label{prop.weylkac.6}Suppose that $V$ is a $\mathfrak{g}_{\operatorname*{ext}%
}\left(  A\right)  $-module from Category $\mathcal{O}$ such that the Casimir
$C$ satisfies $\Delta\mid_{V}=\gamma\cdot\operatorname*{id}$. Then,
$\operatorname*{ch}\left(  V\right)  =\sum c_{\lambda}\operatorname*{ch}%
\left(  M_{\lambda}\right)  $, where the sum is over all $\lambda$ satisfying
$\left(  \lambda,\lambda+2\rho\right)  =\gamma$, and $c_{\lambda}\in
\mathbb{Z}$ are some integers.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.6}.} The expansion is built
inductively as follows:

Suppose $P\left(  V\right)  \subseteq D\left(  \lambda_{1}\right)  \cup
D\left(  \lambda_{2}\right)  \cup...\cup D\left(  \lambda_{m}\right)  $ for
some weights $\lambda_{1},\lambda_{2},...,\lambda_{m}$. Assume that this is a
minimal such union. Then, $\lambda_{i}+\alpha_{j}\notin P\left(  V\right)  $
for any $i,j$.

Let $d_{i}=\dim\left(  V\left[  \lambda_{i}\right]  \right)  $. Then, we have
a homomorphism $\varphi:\bigoplus_{i}d_{i}M_{\lambda_{i}}\rightarrow V$ which
is an isomorphism in weight $\lambda_{i}$. Let $K=\operatorname*{Ker}\varphi$.
Let $C=\operatorname*{Coker}\varphi$. Clearly, both $K$ and $C$ lie in
Category $\mathcal{O}$. We have an exact sequence $0\rightarrow K\rightarrow
\bigoplus_{i}d_{i}M_{\lambda_{i}}\rightarrow V\rightarrow C\rightarrow0$.
Since the alternating sum of characters in an exact sequence is $0$, this
yields $\operatorname*{ch}V=\sum_{i}d_{i}\operatorname*{ch}\left(
M_{\lambda_{i}}\right)  -\operatorname*{ch}K+\operatorname*{ch}C$.

Now we claim that $\Delta\mid_{M_{\lambda_{i}}}=\left(  \lambda_{i}%
,\lambda_{i}+2\rho\right)  =\gamma$ if $d_{i}\neq0$. (Otherwise, a
homomorphism $\varphi$ could not exist.)

Also, $\Delta\mid_{K}=\Delta\mid_{C}=\gamma$.

But if $\mu\in P\left(  K\right)  \cup P\left(  C\right)  $, then for some
$i$, we have $\lambda_{i}-\mu=\sum k_{j}\alpha_{j}$ with $\sum k_{j}\geq1$.

Next step: $\sum k_{i}\geq2$.

Etc.

If we run this procedure indefinitely, eventually every weight in this cone
will be exhausted. Then we apply the procedure to $K$ and $C$, and then to
their $K$ and $C$ etc..

\textit{Proof of Weyl-Kac character formula.} According to Proposition
\ref{prop.weylkac.6}, we have%
\[
\operatorname*{ch}\left(  V\right)  =\sum_{\psi\in D\left(  \chi\right)
}c_{\psi}\operatorname*{ch}\left(  M_{\psi}\right)
\ \ \ \ \ \ \ \ \ \ \text{with }c_{\chi}=1.
\]


We will now need:

\begin{corollary}
\label{cor.weylkac.7}If $c_{\psi}\neq0$, then $\left(  \psi+\rho\right)
^{2}=\left(  \chi+\rho\right)  ^{2}$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.weylkac.7}.} This follows from Proposition
\ref{prop.weylkac.6}.

\begin{lemma}
\label{lem.weylkac.8}If $\psi+\rho=w\left(  \chi+\rho\right)  $, then
$c_{\psi}=\det\left(  w\right)  \cdot c_{\chi}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.8}.} We have $wK=\left(  \det
w\right)  \cdot K$ and $w\cdot\operatorname*{ch}V=\operatorname*{ch}V$. Hence,
$w\left(  K\cdot\operatorname*{ch}V\right)  =\left(  \det w\right)
\cdot\left(  K\operatorname*{ch}V\right)  $. But since $\operatorname*{ch}%
\left(  M_{\psi}\right)  =\dfrac{\sum c_{\psi}e^{\psi+\rho}}{K}$, we have
$K\operatorname*{ch}V=\sum\limits_{\psi\in D\left(  \chi\right)  }c_{\psi
}e^{\psi+\rho}=\left(  \det w\right)  \cdot\sum\limits_{\psi\in D\left(
\chi\right)  }c_{\psi}e^{\psi+\rho}$. (If $\psi+\rho=w\left(  \chi
+\rho\right)  $.) Thus, $c_{\psi}=\left(  \det w\right)  \cdot c_{\chi}$.

\begin{lemma}
\label{lem.weylkac.9}Let $D=\left\{  \psi\ \mid\ c_{\psi-\rho}\neq0\right\}
$. Then, $D=W\left(  \chi+\rho\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.9}.} We have $W\left(  \chi
+\rho\right)  \subseteq D$ by Lemma \ref{lem.weylkac.8}. Also, $D$ is
$W$-invariant since $V$ is integrable.

Suppose $D\neq W\left(  \chi+\rho\right)  $. Then, $\left(  D\diagdown
W\left(  \chi+\rho\right)  \right)  \cap P_{+}\neq\varnothing$ by Lemma
\ref{lem.weylkac.1} \textbf{(2)}. Take some $\beta\in\left(  D\diagdown
W\left(  \chi+\rho\right)  \right)  \cap P_{+}$. Then, $\beta-\rho\in D\left(
\chi\right)  $, so that $\left(  \chi+\rho,\chi+\rho\right)  -\left(
\beta,\beta\right)  >0$ (by Proposition \ref{prop.weylkac.5}). Thus, $\beta$
cannot occur in the sum (by Corollary \ref{cor.weylkac.7}).

Punchline: $\operatorname*{ch}V=\sum_{w\in W}\dfrac{\left(  \det w\right)
\cdot e^{w\left(  \chi+\rho\right)  }}{K}$. This is exactly the Weyl-Kac
character formula.

\subsection{\textbf{[unfinished]} ...}

[...]

\section{\textbf{[unfinished]} ...}

[...] [747l22.pdf]

KZ equations, consistent (define a flat connection)

$\mathfrak{g}$ simple Lie algebra

$V_{1},V_{2},...,V_{N}$ representations of $\mathfrak{g}$ from Category
$\mathcal{O}$.

$\mathbb{C}_{0}^{N}=\mathbb{C}^{N}\diagdown\left\{  z_{i}=z_{j}\right\}  $

$U\subseteq\mathbb{C}_{0}^{N}$ simply connected open set

$F\left(  z_{1},...,z_{N}\right)  \in\left(  V_{1}\otimes V_{2}\otimes
...\otimes V_{N}\right)  \left[  \nu\right]  $ holomorphic function in
$z_{1},...,z_{N}$ for a fixed weight $\nu$.

$x\in\mathbb{C}$ [or was it $\kappa\in\mathbb{C}$ ?]

$\dfrac{\partial F}{\partial z_{i}}-\overline{h}\sum\limits_{i\neq j}%
\dfrac{\Omega_{i,j}}{z_{i}-z_{j}}F$ where $\Omega_{i,j}:V_{1}\otimes
V_{2}\otimes...\otimes V_{N}\rightarrow V_{1}\otimes V_{2}\otimes...\otimes
V_{N}$

$\Omega\in\left(  S^{2}\mathfrak{g}\right)  ^{\mathfrak{g}}$

Consistent means: setting $\nabla_{i}=\dfrac{\partial}{\partial z_{i}%
}-\overline{h}\sum\limits_{i\neq j}\dfrac{\Omega_{i,j}}{z_{i}-z_{j}}$, we have
$\left[  \nabla_{i},\nabla_{j}\right]  =0$. Consistent systems are known to
have locally unique-and-existent solutions.

Why is this in our course?

The reason is that these equations arise in the representation theory of
affine Lie algebras.

Interpretation of KZ equations in terms of $\widehat{\mathfrak{g}}$:

Consider $L\mathfrak{g}$, $\widehat{\mathfrak{g}}$, $\widetilde{\mathfrak{g}%
}=\widehat{\mathfrak{g}}\rtimes\mathbb{C}d$.

Define Weyl modules:

\begin{definition}
Let $\lambda\in P_{+}$ be a dominant integral weight for a simple
finite-dimensional Lie algebra $\mathfrak{g}$. Let $L_{\lambda}$ be an
irreducible finite-dimensional representation of $\mathfrak{g}$ with highest
weight $\lambda$. Let us extend $L_{\lambda}$ to a $\mathfrak{g}\left[
t\right]  \oplus\mathbb{C}K$-module by making $t\mathfrak{g}\left[  t\right]
$ act by $0$ and $K$ act by some scalar $k$ (that is, $K\mid_{L_{\lambda}%
}=k\cdot\operatorname*{id}$ for some $k\in\mathbb{C}$).

Denote this $\mathfrak{g}\left[  t\right]  \oplus\mathbb{C}K$-module by
$L_{\lambda}^{\left(  k\right)  }$. Then, we define a $\widehat{\mathfrak{g}}%
$-module $V_{\lambda,k}=U\left(  \widehat{\mathfrak{g}}\right)  \otimes
_{U\left(  \mathfrak{g}\left[  t\right]  \oplus\mathbb{C}K\right)  }%
L_{\lambda}^{\left(  k\right)  }$. This module is called a \textit{Weyl
module} for $\widehat{\mathfrak{g}}$ at level $k$.
\end{definition}

By the PBW theorem, we immediately see that $U\left(  \widehat{\mathfrak{g}%
}\right)  \cong U\left(  t^{-1}\mathfrak{g}\left[  t^{-1}\right]  \right)
\otimes U\left(  \mathfrak{g}\left[  t\right]  \oplus\mathbb{C}K\right)  $ and
thus $V_{\lambda,k}\cong U\left(  t^{-1}\mathfrak{g}\left[  t^{-1}\right]
\right)  \otimes L_{\lambda}$ (canonically, but only as vector spaces).

Assuming that $k\neq-h^{\vee}$, we can extend $V_{\lambda,k}$ to
$\widetilde{\mathfrak{g}}$ by letting $d$ act as $-L_{0}$ (from Sugawara construction).

\begin{definition}
If $V$ is a $\mathfrak{g}$-module, then $V\left[  z,z^{-1}\right]  $ is an
$L\mathfrak{g}$-module, and in fact a $\widehat{\mathfrak{g}}$-module where
$K$ acts by $0$. It extends to $\widetilde{\mathfrak{g}}$ by setting
$d=z\dfrac{\partial}{\partial z}$.

More generally: Can set $d\left(  vz^{n}\right)  =\left(  n-\Delta\right)
vz^{n}$ for any fixed $\Delta\in\mathbb{C}$.

Call this module $z^{-\Delta}V\left[  z,z^{-1}\right]  $.
\end{definition}

\begin{lemma}
If $k\notin\mathbb{Q}$, then $V_{\lambda,k}$ is irreducible.
\end{lemma}

\textit{Proof of Lemma.} Assume $V_{\lambda,k}$ is reducible. This
$V_{\lambda,k}$ is a highest-weight module. So, it must have a singular vector
in degree $\ell>0$. Let $C$ be the Casimir for $\widetilde{\mathfrak{g}}$. We
know $C=L_{0}-\deg$ (where $\deg$ returns the positive degree).

Assume that $w$ (our singular vector) lives in an irr. repr. of $\mathfrak{g}%
$. Singular vector means $a\left(  m\right)  w=0$ for all $m>0$. Here
$a\left(  m\right)  $ means $at^{m}$.

$C\mid_{V_{\lambda,k}}=\dfrac{\left(  \lambda,\lambda+2\rho\right)  }{2\left(
k+h^{\vee}\right)  }$

$Cw=\left(  \dfrac{\left(  \mu,\mu+2\rho\right)  }{2\left(  k+h^{\vee}\right)
}-\ell\right)  w$

$L_{0}=\dfrac{1}{2\left(  k+h^{\vee}\right)  }\sum_{i\in\mathbb{Z}}\sum_{a\in
B}:a\left(  i\right)  a\left(  -i\right)  :\ =\dfrac{1}{2\left(  k+h^{\vee
}\right)  }\left(  \sum_{a\in B}a\left(  0\right)  ^{2}+2\sum_{a\in B}%
\sum_{m\geq1}a\left(  -m\right)  a\left(  m\right)  \right)  $ where $a\left(
m\right)  =at^{m}$.

$\Longrightarrow$ $\underbrace{\left(  \lambda,\lambda+2\rho\right)  =\left(
\mu,\mu+2\rho\right)  }_{\in\mathbb{Z}}-2\ell\left(  k+h^{\vee}\right)  $
$\Longrightarrow$ $k=-h^{\vee}+\dfrac{\left(  \lambda,\lambda+2\rho\right)
-\left(  \mu,\mu+2\rho\right)  }{2\ell}\in\mathbb{Q}$. $\Longrightarrow$ contradiction.

\begin{corollary}
If $k\notin\mathbb{Q}$, then $V_{\lambda,k}^{\ast}$ (restricted dual) is
$U\left(  \widehat{\mathfrak{g}}\right)  \otimes_{U\left(  \mathfrak{g}\left[
t^{-1}\right]  \oplus\mathbb{C}K\right)  }L_{\lambda}^{\ast\left(  -k\right)
}$. (Here, $L_{\lambda}^{\ast\left(  -k\right)  }$ means $L_{\lambda}^{\ast}$
with $K$ acting as $-k$.)
\end{corollary}

\textit{Proof of Corollary.} From Frobenius reciprocity, we have a
homomorphism $\varphi:U\left(  \widehat{\mathfrak{g}}\right)  \otimes
_{U\left(  \mathfrak{g}\left[  t^{-1}\right]  \oplus\mathbb{C}K\right)
}L_{\lambda}^{\ast\left(  -k\right)  }\rightarrow V_{\lambda,k}^{\ast}$ which
is $\operatorname*{id}$ in degree $0$. In fact, Frobenius reciprocity tells us
that%
\[
\operatorname*{Hom}\nolimits_{\widehat{\mathfrak{g}}}\left(  U\left(
\widehat{\mathfrak{g}}\right)  \otimes_{U\left(  \mathfrak{g}\left[
t^{-1}\right]  \oplus\mathbb{C}K\right)  }L_{\lambda}^{\ast\left(  -k\right)
},M\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{g}\left[
t^{-1}\right]  \oplus\mathbb{C}K}\left(  L_{\lambda}^{\ast\left(  -k\right)
},M\right)  ,
\]
which, in the case $M=V_{\lambda,k}^{\ast}$, becomes [...].

Because $V_{\lambda,k}$ is irreducible (here we are using $k\notin\mathbb{Q}%
$), $V_{\lambda,k}^{\ast}$ is irreducible as well, this homomorphism $\varphi$
is surjective. This $\varphi$ also preserves grading, and the characters are
equal. $\Longrightarrow$ $\varphi$ is an isomorphism.

\begin{corollary}
$\operatorname*{Hom}\nolimits_{\widetilde{\mathfrak{g}}}\left(  V_{\lambda
,k}\otimes V_{\nu,k}^{\ast},z^{-\Delta}V\left[  z,z^{-1}\right]  \right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  L_{\lambda}\otimes
L_{\nu}^{\ast},V\right)  $ if $\Delta=\Delta\left(  \lambda\right)
-\Delta\left(  \nu\right)  $.
\end{corollary}

\textit{Proof of Corollary.} Frobenius reciprocity as for the previous
corollary. (Skip.)

[...]

We now cite a classical theorem on ODEs.

\begin{theorem}
Let $N\in\mathbb{N}$. Let $A\left(  z\right)  =A_{0}+A_{1}z+A_{2}z^{2}+...$ be
a holomorphic function on $\left\{  z\in\mathbb{C}\ \mid\ \left\vert
z\right\vert <1\right\}  $ with values in $\operatorname*{M}\nolimits_{N}%
\left(  \mathbb{C}\right)  $. Assume that for any eigenvalues $\lambda$ and
$\mu$ of $A_{0}$ such that $\lambda\neq\mu$, one has $\lambda-\mu
\notin\mathbb{Z}$. Then, the ODE $z\dfrac{dF}{dz}=A\left(  z\right)  F$
(which, of course, is equivalent to $\dfrac{dF}{dz}=\dfrac{A\left(  z\right)
}{z}F$) has a matrix solution of the form $F\left(  z\right)  =\left(
1+B_{1}z+B_{2}z^{2}+...\right)  z^{A_{0}}$ such that the power series
$1+B_{1}z+B_{2}z^{2}+...$ converges for $\left\vert z\right\vert <1$. Here,
$z^{A_{0}}$ means $\exp\left(  A_{0}\log z\right)  $ (on $\mathbb{C}%
\diagdown\mathbb{R}_{\leq0}$).
\end{theorem}

\begin{remark}
This is a development of the following basic theorem: If we are given an ODE
$\dfrac{dF}{dz}=C\left(  z\right)  F$ with $C\left(  z\right)  $ holomorphic,
then there exists a holomorphic $F$ satisfying this equation and having the
form $F=1+O\left(  z\right)  $ (the so-called fundamental equation).
\end{remark}

\textit{Proof of Theorem.} Plug in the solution $F\left(  z\right)  $ in the
above formula:%
\[
\left(  \sum\limits_{n\geq1}nB_{n}z^{n}\right)  z^{A_{0}}+\left(
1+\sum\limits_{n\geq1}B_{n}z^{n}\right)  A_{0}z^{A_{0}}=\left(  A_{0}%
+A_{1}z+A_{2}z^{2}+...\right)  \left(  1+B_{1}z+B_{2}z^{2}+...\right)
z^{A_{0}}.
\]
Cancel $z^{A_{0}}$ from this to obtain%
\[
\sum\limits_{n\geq1}nB_{n}z^{n}+\left(  1+\sum\limits_{n\geq1}B_{n}%
z^{n}\right)  A_{0}=\left(  A_{0}+A_{1}z+A_{2}z^{2}+...\right)  \left(
1+B_{1}z+B_{2}z^{2}+...\right)  .
\]
This is the system of recursive equations%
\[
nB_{n}-A_{0}B_{n}+B_{n}A_{0}=A_{1}B_{n-1}+A_{2}B_{n-2}+...+A_{n-1}B_{1}%
+A_{n}.
\]
This rewrites as%
\[
\left(  n-\operatorname*{ad}A_{0}\right)  \left(  B_{n}\right)  =A_{1}%
B_{n-1}+A_{2}B_{n-2}+...+A_{n-1}B_{1}+A_{n}.
\]
The operator $n-\operatorname*{ad}A_{0}:\operatorname*{M}\nolimits_{N}\left(
\mathbb{C}\right)  \rightarrow\operatorname*{M}\nolimits_{N}\left(
\mathbb{C}\right)  $ is invertible (because eigenvalues of this operator are
$n-\left(  \lambda-\mu\right)  $ for $\lambda$ and $\mu$ being eigenvalues of
$A_{0}$, and because of the condition that for any eigenvalues $\lambda$ and
$\mu$ of $A_{0}$ such that $\lambda\neq\mu$, one has $\lambda-\mu
\notin\mathbb{Z}$). Hence, we can use the above equation to recursively
compute $B_{n}$ for all $n$.

This implies that a solution in the formal sense exists.

We also need to estimate radius of convergence. [...]

The following generalizes our theorem to several variables:

\begin{theorem}
Let $m\in\mathbb{N}$ and $N\in\mathbb{N}$. For every $i\in\left\{
1,2,...,m\right\}  $, let $A_{i}\left(  \xi_{1},\xi_{2},...,\xi_{m}\right)  $
be a holomorphic on $\left\{  \left(  \xi_{1},\xi_{2},...,\xi_{m}\right)
\ \mid\ \left\vert \xi_{j}\right\vert <1\text{ for all }j\right\}  $ with
values in $\operatorname*{M}\nolimits_{N}\left(  \mathbb{C}\right)  $.
Consider the system of differential equations $\xi_{i}\dfrac{dF}{d\xi_{i}%
}=A_{i}\left(  \xi\right)  F$ for all $i\in\left\{  1,2,...,m\right\}  $ on a
single function $F:\mathbb{C}^{m}\rightarrow\operatorname*{M}\nolimits_{N}%
\left(  \mathbb{C}\right)  $. Assume
\[
\left[  \xi_{i}\dfrac{d}{d\xi_{i}}-A_{i},\xi_{j}\dfrac{d}{d\xi_{j}}%
-A_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{
1,2,...,m\right\}
\]
(this is called a \textit{consistency condition}, aka a zero curvature
equation). Then, $\left[  A_{i}\left(  0\right)  ,A_{j}\left(  0\right)
\right]  =0$ for all $i,j\in\left\{  1,2,...,m\right\}  $, and thus the
matrices $A_{i}\left(  0\right)  $ for all $i$ can be simultaneously
trigonalized. Under this trigonalization, let $\lambda_{i,1}$, $\lambda_{i,2}%
$, $...$, $\lambda_{i,N}$ be the diagonal entries of $A_{i}\left(  0\right)  $.

Assume that the condition
\[
\left(  \lambda_{1,k}-\lambda_{1,\ell},\lambda_{2,k}-\lambda_{2,\ell
},...,\lambda_{m,k}-\lambda_{m,\ell}\right)  \notin\mathbb{Z}^{m}\diagdown0
\]
holds for all $k$ and $\ell$. [...]
\end{theorem}


\end{document}
