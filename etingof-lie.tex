%\usepackage{bigfoot}
%\usepackage{fixltx2e}
%\renewcommand{\footnote}[1]{\footnotemark \footnotetext{#1}}
% This paper comes in two versions:
% The long version:
% \includecomment{verlong}
% \excludecomment{vershort}
% The short version:
% \excludecomment{verlong}
% \includecomment{vershort}

\documentclass
[numbers=enddot,12pt,final,onecolumn,german,notitlepage]{scrartcl}%
\usepackage[all,cmtip]{xy}
\usepackage{lscape}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{amsthm}
\usepackage{graphicx}%
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Thursday, April 12, 2012 16:00:28}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newcommand{\Ker}{\operatorname*{Ker}}
\newcommand{\id}{\operatorname*{id}}
\newcommand{\inc}{\operatorname*{inc}}
\newcommand{\gr}{\operatorname*{gr}}
\newcommand{\Hom}{\operatorname*{Hom}}
\newcommand{\calA}{\mathcal A}
\newcommand{\arinj}{\ar@{_{(}->}}
\newcommand{\arsurj}{\ar@{->>}}
\newcommand{\arelem}{\ar@{|->}}
\newcommand{\fraka}{\mathfrak{a}}
\newcommand{\frakb}{\mathfrak{b}}
\newcommand{\frakc}{\mathfrak{c}}
\newcommand{\PBW}{\operatorname*{PBW}}
\newcommand{\xycs}{\xymatrixcolsep}
\newcommand{\xyrs}{\xymatrixrowsep}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{impnt}[theo]{Important Notice}
\newenvironment{impnot}[1][]
{\begin{impnt}[#1]\begin{leftbar}\color{blue}}
{\end{leftbar}\end{impnt}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{exe}[theo]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exe}[#1]\begin{leftbar}}
{\end{leftbar}\end{exe}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{Convention}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{Warning}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{example}[theo]{Example}
\voffset=-0.5cm
\hoffset=-0.7cm
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\setlength\textheight{24cm}
\setlength\textwidth{15.5cm}
\begin{document}

\title{18.747: Infinite-dimensional Lie algebras (Spring term 2012 at MIT)}
\author{Pavel Etingof\\Scribed by Darij Grinberg}
\date{Version 0.22 (\today) (not proofread!)}
\maketitle
\tableofcontents

\begin{noncompile}
\textbf{TO-DO\ LIST:}

[footnotes correct order]
\end{noncompile}

\subsection{Version notes}

These notes are mostly based on what is being said and written on the
blackboard in the lectures, and less so on Pavel Etingof's handwritten notes
posted on \newline\texttt{http://www-math.mit.edu/\symbol{126}etingof/} . At
the moment, they lag behind Etingof's handwritten notes, but are more
detailed. But Etingof's notes contain some remarks that are not in (and might
never be added to) these notes!

Thanks to Pavel Etingof for his patience in explaining me things until I
actually understand them. Thanks to Dorin Boger for finding mistakes.

\subsection{Introduction}

The standard text on infinite-dimensional Lie algebras (although we will not
really follow it) is:

\begin{itemize}
\item V. G. Kac, A. K. Raina, \textit{(Bombay Lectures on) Highest Weight
Representations of Infinite Dimensional Lie Algebras}, World Scientific 1987.
\end{itemize}

Further recommended sources are:

\begin{itemize}
\item Victor G. Kac, \textit{Infinite dimensional Lie algebras}, Third
Edition, CUP 1995.

\item B. L. Feigin, A. Zelevinsky, \textit{Representations of contragredient
Lie algebras and the Kac-Macdonald identities}, a paper in: Representations of
Lie groups and Lie algebras (Budapest, 1971), pp. 25-77, Akad. Kiad\'{o},
Budapest, 1985.
\end{itemize}

The goal of this lecture is to discuss the structure and the representation
theory (mainly the latter) of the most important infinite-dimensional Lie
algebras. And we are also going to show some connections of this subject to
other fields of mathematics (such as conformal field theory, the theory of
integrable systems, and the theory of quantum groups).

Basic algebra will be used, but also occasionally some results from the
representation theory of finite-dimensional Lie algebras.

The biggest difference between the theory of finite-dimensional Lie algebras
and that of infinite-dimensional ones is that in the finite-dimensional case,
we have a complete picture -- we can classify simple Lie algebras, etc. --,
whereas in the infinite-dimensional one there are lots and lots of simple Lie
algebras and we have no real hope to classify them; what we can do is study
some very specific classes and families. The main classes of Lie algebras that
we will study in this course are:

\textbf{1.} The Heisenberg algebra (aka oscillator algebra).

\textbf{2.} The Virasoro algebra.

\textbf{3.} Kac-Moody algebras (this class contains semisimple Lie algebras
and also affine Lie algebras, which are central extensions of $\mathfrak{g}%
\left[  t,t^{-1}\right]  $ where $\mathfrak{g}$ is simple finite-dimensional).

\textbf{4.} The Lie algebra $\mathfrak{gl}_{\infty}$.

We will almost always work over $\mathbb{C}$ in this course. All algebras are
over $\mathbb{C}$ unless specified otherwise. Characteristic $p$ is too
complicated for us, although very interesting. Sometimes we will work over
$\mathbb{R}$, and occasionally even over rings (as auxiliary constructions
require this).

\section{The main examples}

\subsection{The Heisenberg algebra}

We start with the definition of the Heisenberg algebra. Before we formulate
it, let us introduce polynomial differential forms on $\mathbb{C}^{\times}$
(in the algebraic sense):

\begin{definition}
\label{def.diffform}Consider the free $\mathbb{C}\left[  t,t^{-1}\right]
$-module on the basis $\left(  dt\right)  $ (where $dt$ is just a symbol). The
elements of this module are called \textit{polynomial differential forms on
}$\mathbb{C}^{\times}$. Thus, polynomial differential forms on $\mathbb{C}%
^{\times}$ are just formal expressions of the form $fdt$ where $f\in
\mathbb{C}\left[  t,t^{-1}\right]  $.

Whenever $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ is a Laurent polynomial, we
define a polynomial differential form $dg$ by $dg=g^{\prime}dt$. This notation
$dg$ does not conflict with the previously defined notation $dt$ (which was a
symbol), because the polynomial $t$ satisfies $t^{\prime}=1$.
\end{definition}

\begin{definition}
\label{def.res}For every polynomial differential form $fdt$ on $\mathbb{C}%
^{\times}$ (with $f\in\mathbb{C}\left[  t,t^{-1}\right]  $), we define a
complex number $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ to be
the coefficient of the Laurent polynomial $f$ before $t^{-1}$. In other words,
we define $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ to be
$a_{-1}$, where $f$ is written as $\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$
(with $a_{i}\in\mathbb{C}$ for all $i\in\mathbb{Z}$).

This number $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ is called
the \textit{residue} of the form $fdt$ at $0$.
\end{definition}

(The same definition could have been done for Laurent series instead of
Laurent polynomials, but this would require us to consider a slightly
different notion of differential forms, and we do not want to do this here.)

\begin{remark}
\label{rmk.res}\textbf{(a)} Every Laurent polynomial $f\in\mathbb{C}\left[
t,t^{-1}\right]  $ satisfies $\operatorname*{Res}\nolimits_{t=0}\left(
df\right)  =0$.

\textbf{(b)} Every Laurent polynomial $f\in\mathbb{C}\left[  t,t^{-1}\right]
$ satisfies $\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)  =0$.
\end{remark}

\textit{Proof of Remark \ref{rmk.res}.} \textbf{(a)} Write $f$ in the form
$\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ (with $b_{i}\in\mathbb{C}$ for all
$i\in\mathbb{Z}$). Then, $f^{\prime}=\sum\limits_{i\in\mathbb{Z}}ib_{i}%
t^{i-1}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}$. Now,
$df=f^{\prime}dt$, so that
\begin{align*}
\operatorname*{Res}\nolimits_{t=0}\left(  df\right)   &  =\operatorname*{Res}%
\nolimits_{t=0}\left(  f^{\prime}dt\right)  =\left(  \text{the coefficient of
the Laurent polynomial }f^{\prime}\text{ before }t^{-1}\right) \\
&  =\underbrace{\left(  -1+1\right)  }_{=0}b_{-1+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }f^{\prime}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)
b_{i+1}t^{i}\right) \\
&  =0,
\end{align*}
proving Remark \ref{rmk.res} \textbf{(a)}.

\textbf{(b)} \textit{First proof of Remark \ref{rmk.res} \textbf{(b)}:} By the
Leibniz identity, $\left(  f^{2}\right)  ^{\prime}=ff^{\prime}+f^{\prime
}f=2ff^{\prime}$, so that $ff^{\prime}=\dfrac{1}{2}\left(  f^{2}\right)
^{\prime}$ and thus $f\underbrace{df}_{=f^{\prime}dt}=\underbrace{ff^{\prime}%
}_{=\dfrac{1}{2}\left(  f^{2}\right)  ^{\prime}}dt=\dfrac{1}{2}%
\underbrace{\left(  f^{2}\right)  ^{\prime}dt}_{=d\left(  f^{2}\right)
}=\dfrac{1}{2}d\left(  f^{2}\right)  $. Thus,
\[
\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)  =\operatorname*{Res}%
\nolimits_{t=0}\left(  \dfrac{1}{2}d\left(  f^{2}\right)  \right)  =\dfrac
{1}{2}\underbrace{\operatorname*{Res}\nolimits_{t=0}\left(  d\left(
f^{2}\right)  \right)  }_{\substack{=0\text{ (by Remark \ref{rmk.res}
\textbf{(a)},}\\\text{applied to }f^{2}\text{ instead of }f\text{)}}}=0,
\]
and Remark \ref{rmk.res} \textbf{(b)} is proven.

\textit{Second proof of Remark \ref{rmk.res} \textbf{(b)}:} Write $f$ in the
form $\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ (with $b_{i}\in\mathbb{C}$ for
all $i\in\mathbb{Z}$). Then, $f^{\prime}=\sum\limits_{i\in\mathbb{Z}}%
ib_{i}t^{i-1}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}$.
Now,%
\[
ff^{\prime}=\left(  \sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}\right)  \left(
\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}\right)
=\sum\limits_{n\in\mathbb{Z}}\left(  \sum\limits_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\i+j=n}}b_{i}\cdot\left(  j+1\right)
b_{j+1}\right)  t^{n}%
\]
(by the definition of the product of Laurent polynomials). Also,
$df=f^{\prime}dt$, so that%
\begin{align*}
\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)   &  =\operatorname*{Res}%
\nolimits_{t=0}\left(  ff^{\prime}dt\right)  =\left(  \text{the coefficient of
the Laurent polynomial }ff^{\prime}\text{ before }t^{-1}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2}%
;\\i+j=-1}}b_{i}\cdot\left(  j+1\right)  b_{j+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }ff^{\prime}=\sum\limits_{n\in\mathbb{Z}}\sum
\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\i+j=n}}\left(
b_{i}\cdot\left(  j+1\right)  b_{j+1}\right)  t^{n}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2}%
;\\i+j=0}}b_{i}\cdot jb_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
substituted }\left(  i,j\right)  \text{ for }\left(  i,j+1\right)  \text{ in
the sum}\right) \\
&  =\sum\limits_{j\in\mathbb{Z}}b_{-j}\cdot jb_{j}=\underbrace{\sum
\limits_{\substack{j\in\mathbb{Z};\\j<0}}b_{-j}\cdot jb_{j}}_{\substack{=\sum
\limits_{\substack{j\in\mathbb{Z};\\j>0}}b_{-\left(  -j\right)  }\cdot\left(
-j\right)  b_{-j}\\\text{(here, we substituted }j\text{ for }-j\text{ in the
sum)}}}+\underbrace{b_{-0}\cdot0b_{0}}_{=0}+\sum\limits_{\substack{j\in
\mathbb{Z};\\j>0}}b_{-j}\cdot jb_{j}\\
&  =\sum\limits_{\substack{j\in\mathbb{Z};\\j>0}}\underbrace{b_{-\left(
-j\right)  }\cdot\left(  -j\right)  b_{-j}}_{=b_{j}\left(  -j\right)
b_{-j}=-b_{-j}\cdot jb_{j}}+\sum\limits_{\substack{j\in\mathbb{Z}%
;\\j>0}}b_{-j}\cdot jb_{j}=\sum\limits_{\substack{j\in\mathbb{Z}%
;\\j>0}}\left(  -b_{-j}\cdot jb_{j}\right)  +\sum\limits_{\substack{j\in
\mathbb{Z};\\j>0}}b_{-j}\cdot jb_{j}=0.
\end{align*}
This proves Remark \ref{rmk.res} \textbf{(b)}.

Note that the first proof of Remark \ref{rmk.res} \textbf{(b)} made use of the
fact that $2$ is invertible in $\mathbb{C}$, whereas the second proof works
over any commutative ring instead of $\mathbb{C}$.

Now, finally, we define the Heisenberg algebra:

\begin{definition}
\label{def.osc}The \textit{oscillator algebra} $\mathcal{A}$ is the vector
space $\mathbb{C}\left[  t,t^{-1}\right]  \oplus\mathbb{C}$ endowed with the
Lie bracket%
\[
\left[  \left(  f,\alpha\right)  ,\left(  g,\beta\right)  \right]  =\left(
0,\operatorname*{Res}\nolimits_{t=0}\left(  gdf\right)  \right)  .
\]
Since this Lie bracket satisfies the Jacobi identity (because the definition
quickly yields that $\left[  \left[  x,y\right]  ,z\right]  =0$ for all
$x,y,z\in\mathcal{A}$) and is skew-symmetric (due to Remark \ref{rmk.res}
\textbf{(b)}), this $\mathcal{A}$ is a Lie algebra.

This oscillator algebra $\mathcal{A}$ is also known as the \textit{Heisenberg
algebra}.
\end{definition}

Thus, $\mathcal{A}$ has a basis
\[
\left\{  a_{n}\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  K\right\}  ,
\]
where $a_{n}=\left(  t^{n},0\right)  $ and $K=\left(  0,1\right)  $. The
bracket is given by%
\begin{align*}
\left[  a_{n},K\right]   &  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{thus, }K\text{
is central}\right)  ;\\
\left[  a_{n},a_{m}\right]   &  =n\delta_{n,-m}K
\end{align*}
(in fact, $\left[  a_{n},a_{-n}\right]  =\operatorname*{Res}\nolimits_{t=0}%
\left(  t^{-n}dt^{n}\right)  K=\operatorname*{Res}\nolimits_{t=0}\left(
nt^{-1}dt\right)  K=nK$). Thus, $\mathcal{A}$ is a $1$-dimensional central
extension of the abelian Lie algebra $\mathbb{C}\left[  t,t^{-1}\right]  $;
this means that we have a short exact sequence%
\[
\xymatrix{
0 \ar[r] & \mathbb C K \ar[r] & \mathcal A \ar[r] & \mathbb C\left[t,t^{-1}\right] \ar[r] & 0
},
\]
where $\mathbb{C}K$ is contained in the center of $\mathcal{A}$ and where
$\mathbb{C}\left[  t,t^{-1}\right]  $ is an abelian Lie algebra.

Note that $\mathcal{A}$ is a $2$-nilpotent Lie algebra. Also note that the
center of $\mathcal{A}$ is spanned by $a_{0}$ and $K$.

\subsection{The Witt algebra}

The next introductory example will be the Lie algebra of vector fields:

\begin{definition}
Consider the free $\mathbb{C}\left[  t,t^{-1}\right]  $-module on the basis
$\left(  \partial\right)  $ (where $\partial$ is just a symbol). This module,
regarded as a $\mathbb{C}$-vector space, will be denoted by $W$. Thus, the
elements of $W$ are formal expressions of the form $f\partial$ where
$f\in\mathbb{C}\left[  t,t^{-1}\right]  $. (Thus, $W\cong\mathbb{C}\left[
t,t^{-1}\right]  $.)

Define a Lie bracket on the $\mathbb{C}$-vector space $W$ by%
\[
\left[  f\partial,g\partial\right]  =\left(  fg^{\prime}-gf^{\prime}\right)
\partial\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }g\in\mathbb{C}\left[  t,t^{-1}\right]  .
\]
This Lie bracket is easily seen to be skew-symmetric and satisfy the Jacobi
identity. Thus, it makes $W$ into a Lie algebra. This Lie algebra is called
the \textit{Witt algebra}.

The elements of $W$ are called \textit{polynomial vector fields on
}$\mathbb{C}^{\times}$.

The symbol $\partial$ is often denoted by $\dfrac{d}{dt}$.
\end{definition}

\begin{remark}
It is not by chance that $\partial$ is also known as $\dfrac{d}{dt}$. In fact,
this notation allows us to view the elements of $W$ as actual polynomial
vector fields on $\mathbb{C}^{\times}$ in the sense of algebraic geometry over
$\mathbb{C}$. The Lie bracket of the Witt algebra $W$ is then exactly the
usual Lie bracket of vector fields (because if $f\in\mathbb{C}\left[
t,t^{-1}\right]  $ and $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ are two
Laurent polynomials, then a simple application of the Leibniz rule shows that
the commutator of the differential operators $f\dfrac{d}{dt}$ and $g\dfrac
{d}{dt}$ is indeed the differential operator $\left(  fg^{\prime}-gf^{\prime
}\right)  \dfrac{d}{dt}$).
\end{remark}

A basis of the Witt algebra $W$ is $\left\{  L_{n}\ \mid\ n\in\mathbb{Z}%
\right\}  $, where $L_{n}$ means $-t^{n+1}\dfrac{d}{dt}=-t^{n+1}\partial$.
(Note that some other references like to define $L_{n}$ as $t^{n+1}\partial$
instead, thus getting a different sign in many formulas.) It is easy to see
that the Lie bracket of the Witt algebra is given on this basis by
\[
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ and }m\in
\mathbb{Z}.
\]


\subsection{A digression: Lie groups (and the absence thereof)}

Let us make some remarks about the relationship between Lie algebras and Lie
groups. In analysis and geometry, linearizations (tangent spaces etc.) usually
only give a crude approximation of non-linear things (manifolds etc.). This is
what makes the theory of Lie groups special: The linearization of a
finite-dimensional Lie group (i. e., its corresponding Lie algebra) carries
very much information about the Lie group. The relation between
finite-dimensional Lie groups and finite-dimensional Lie algebras is almost a
one-to-one correspondence (at least if we restrict ourselves to simply
connected Lie groups). This correspondence breaks down in the
infinite-dimensional case. There are lots of important infinite-dimensional
Lie groups, but their relation to Lie algebras is not as close as in the
finite-dimensional case anymore. One example for this is that there is no Lie
group corresponding to the Witt algebra $W$. There are a few things that come
close to such a Lie group:

We can consider the real subalgebra $W_{\mathbb{R}}$ of $W$, consisting of the
vector fields in $W$ which are tangent to $S^{1}$ (the unit circle in
$\mathbb{C}$). This is a real Lie algebra satisfying $W_{\mathbb{R}}%
\otimes_{\mathbb{R}}\mathbb{C}\cong W$ (thus, $W_{\mathbb{R}}$ is what is
called a \textit{real form} of $W$). And we can say that
$\widehat{W_{\mathbb{R}}}=\operatorname*{Lie}\left(  \operatorname*{Diff}%
S^{1}\right)  $ for some kind of completion $\widehat{W_{\mathbb{R}}}$ of
$W_{\mathbb{R}}$ (although $W_{\mathbb{R}}$ itself is not the Lie algebra of
any Lie group).\footnote{Here is how this completion $\widehat{W_{\mathbb{R}}%
}$ is defined exactly: Notice that%
\[
W_{\mathbb{R}}=\left\{  \varphi\left(  \theta\right)  \dfrac{d}{dt}\ \mid\
\begin{array}
[c]{c}%
\varphi\text{ is a trigonometric polynomial, i. e.,}\\
\varphi\left(  \theta\right)  =a_{0}+\sum\limits_{n}a_{n}\cos n\theta
+\sum\limits_{n}b_{n}\sin n\theta\\
\text{where both sums are finite}%
\end{array}
\right\}  .
\]
Now, define the completion $\widehat{W_{\mathbb{R}}}$ by%
\[
\widehat{W_{\mathbb{R}}}=\left\{  \varphi\left(  \theta\right)  \dfrac{d}%
{dt}\ \mid\
\begin{array}
[c]{c}%
\varphi\left(  \theta\right)  =a_{0}+\sum\limits_{n}a_{n}\cos n\theta
+\sum\limits_{n}b_{n}\sin n\theta\\
\text{where both sums are infinite sums with rapidly}\\
\text{decreasing coefficients}%
\end{array}
\right\}  .
\]
} Now if we take two one-parameter families%
\begin{align*}
g_{s}  &  \in\operatorname*{Diff}S^{1},\ \ \ \ \ \ \ \ \ \ g_{s}\mid
_{s=0}=\operatorname*{id},\ \ \ \ \ \ \ \ \ \ g_{s}^{\prime}\mid_{s=0}%
=\varphi;\\
h_{u}  &  \in\operatorname*{Diff}S^{1},\ \ \ \ \ \ \ \ \ \ h_{u}\mid
_{u=0}=\operatorname*{id},\ \ \ \ \ \ \ \ \ \ h_{u}^{\prime}\mid_{u=0}=\psi,
\end{align*}
then%
\begin{align*}
g_{s}\left(  \theta\right)   &  =\theta+s\varphi\left(  \theta\right)
+O\left(  s^{2}\right)  ;\\
h_{u}\left(  \theta\right)   &  =\theta+u\psi\left(  \theta\right)  +O\left(
t^{2}\right)  ;\\
g_{s}\circ h_{u}\circ g_{s}^{-1}\circ h_{u}^{-1}  &  =\theta+su\left(
\varphi\psi^{\prime}-\psi\varphi^{\prime}\right)  \left(  \theta\right)
+\left(  \text{cubic terms in }s\text{ and }u\text{ and higher}\right)  .
\end{align*}
So we get something like the standard Lie-group-Lie-algebra correspondence,
but only for the completion of the real part. For the complex one, some people
have done some work yielding something like Lie semigroups (the so-called
"semigroup of annuli" of G. Segal), but no Lie groups.

Anyway, this was a digression, just to show that we don't have Lie groups
corresponding to our Lie algebras. Still, this should not keep us from
heuristically thinking of Lie algebras as linearizations of Lie groups. We can
even formalize this heuristic, by using the purely algebraic notion of formal groups.

\subsection{The Witt algebra acts on the Heisenberg algebra by derivations}

Let's return to topic.

From now on, if $\mathfrak{g}$ is a Lie algebra, we will denote by
$\operatorname*{Der}\mathfrak{g}$ the Lie algebra of all derivations of
$\mathfrak{g}$.

\begin{lemma}
\label{lem.WtoDerA}There is a natural homomorphism $\eta:W\rightarrow
\operatorname*{Der}\mathcal{A}$ of Lie algebras given by
\[
\left(  \eta\left(  f\partial\right)  \right)  \left(  g,\alpha\right)
=\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }\alpha\in\mathbb{C}.
\]

\end{lemma}

\textit{First proof of Lemma \ref{lem.WtoDerA}.} Lemma \ref{lem.WtoDerA} can
be proven by direct calculation:

For every $f\partial\in W$, the map%
\[
\mathcal{A}\rightarrow\mathcal{A},\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)
\mapsto\left(  fg^{\prime},0\right)
\]
is a derivation of $\mathcal{A}$\ \ \ \ \footnote{\textit{Proof.} Let
$f\partial$ be an element of $W$. (In other words, let $f$ be an element of
$\mathbb{C}\left[  t,t^{-1}\right]  $.) Let $\tau$ denote the map%
\[
\mathcal{A}\rightarrow\mathcal{A},\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)
\mapsto\left(  fg^{\prime},0\right)  .
\]
Then, we must prove that $\tau$ is a derivation of $\mathcal{A}$.
\par
In fact, first it is clear that $\tau$ is $\mathbb{C}$-linear. Moreover, any
$\left(  u,\beta\right)  \in\mathcal{A}$ and $\left(  v,\gamma\right)
\in\mathcal{A}$ satisfy%
\begin{align*}
\tau\left(  \underbrace{\left[  \left(  u,\beta\right)  ,\left(
v,\gamma\right)  \right]  }_{=\left(  0,\operatorname*{Res}\nolimits_{t=0}%
\left(  vdu\right)  \right)  }\right)   &  =\tau\left(  0,\operatorname*{Res}%
\nolimits_{t=0}\left(  vdu\right)  \right)  =\left(  f0,0\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\right) \\
&  =\left(  0,0\right)
\end{align*}
and%
\begin{align*}
&  \left[  \underbrace{\tau\left(  u,\beta\right)  }_{=\left(  fu^{\prime
},0\right)  },\left(  v,\gamma\right)  \right]  +\left[  \left(
u,\beta\right)  ,\underbrace{\tau\left(  v,\gamma\right)  }_{=\left(
fv^{\prime},0\right)  }\right] \\
&  =\underbrace{\left[  \left(  fu^{\prime},0\right)  ,\left(  v,\gamma
\right)  \right]  }_{=\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(
vd\left(  fu^{\prime}\right)  \right)  \right)  }+\underbrace{\left[  \left(
u,\beta\right)  ,\left(  fv^{\prime},0\right)  \right]  }_{=\left(
0,\operatorname*{Res}\nolimits_{t=0}\left(  fv^{\prime}du\right)  \right)  }\\
&  =\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(  vd\left(  fu^{\prime
}\right)  \right)  \right)  +\left(  0,\operatorname*{Res}\nolimits_{t=0}%
\left(  fv^{\prime}du\right)  \right) \\
&  =\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(  vd\left(  fu^{\prime
}\right)  +fv^{\prime}du\right)  \right)  =\left(  0,\operatorname*{Res}%
\nolimits_{t=0}\left(  d\left(  vfu^{\prime}\right)  \right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }v\underbrace{d\left(  fu^{\prime}\right)  }_{=\left(  fu^{\prime
}\right)  ^{\prime}dt}+fv^{\prime}\underbrace{du}_{=u^{\prime}dt}=v\left(
fu^{\prime}\right)  ^{\prime}dt+fv^{\prime}u^{\prime}dt\\
=\left(  v\left(  fu^{\prime}\right)  ^{\prime}+fv^{\prime}u^{\prime}\right)
dt=\underbrace{\left(  v\left(  fu^{\prime}\right)  ^{\prime}+v^{\prime
}\left(  fu^{\prime}\right)  \right)  }_{=\left(  vfu^{\prime}\right)
^{\prime}}dt=\left(  vfu^{\prime}\right)  ^{\prime}dt=d\left(  vfu^{\prime
}\right)
\end{array}
\right) \\
&  =\left(  0,0\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since Remark
\ref{rmk.res} \textbf{(a)} (applied to }vfu^{\prime}\text{ instead of
}f\text{) yields }\operatorname*{Res}\nolimits_{t=0}\left(  d\left(
vfu^{\prime}\right)  \right)  =0\right)  ,
\end{align*}
so that $\tau\left(  \left[  \left(  u,\beta\right)  ,\left(  v,\gamma\right)
\right]  \right)  =\left[  \tau\left(  u,\beta\right)  ,\left(  v,\gamma
\right)  \right]  +\left[  \left(  u,\beta\right)  ,\tau\left(  v,\gamma
\right)  \right]  $. Thus, $\tau$ is a derivation of $\mathcal{A}$, qed.},
thus lies in $\operatorname*{Der}\mathcal{A}$. Hence, we can define a map
$\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ by%
\[
\eta\left(  f\partial\right)  =\left(  \mathcal{A}\rightarrow\mathcal{A}%
,\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)  \mapsto\left(  fg^{\prime
},0\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[
t,t^{-1}\right]  .
\]
In other words, we can define a map $\eta:W\rightarrow\operatorname*{Der}%
\mathcal{A}$ by%
\[
\left(  \eta\left(  f\partial\right)  \right)  \left(  g,\alpha\right)
=\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }\alpha\in\mathbb{C}.
\]
Now, it remains to show that this map $\eta$ is a homomorphism of Lie algebras.

In fact, any $f_{1}\in\mathbb{C}\left[  t,t^{-1}\right]  $ and $f_{2}%
\in\mathbb{C}\left[  t,t^{-1}\right]  $ and any $g\in\mathbb{C}\left[
t,t^{-1}\right]  $ and $\alpha\in\mathbb{C}$ satisfy%
\[
\left(  \eta\left(  \underbrace{\left[  f_{1}\partial,f_{2}\partial\right]
}_{=\left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  \partial}\right)
\right)  \left(  g,\alpha\right)  =\left(  \eta\left(  \left(  f_{1}%
f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  \partial\right)  \right)  \left(
g,\alpha\right)  =\left(  \left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime
}\right)  g^{\prime},0\right)
\]
and%
\begin{align*}
&  \left[  \eta\left(  f_{1}\partial\right)  ,\eta\left(  f_{2}\partial
\right)  \right]  \left(  g,\alpha\right) \\
&  =\left(  \eta\left(  f_{1}\partial\right)  \right)  \underbrace{\left(
\left(  \eta\left(  f_{2}\partial\right)  \right)  \left(  g,\alpha\right)
\right)  }_{=\left(  f_{2}g^{\prime},0\right)  }-\left(  \eta\left(
f_{2}\partial\right)  \right)  \underbrace{\left(  \left(  \eta\left(
f_{1}\partial\right)  \right)  \left(  g,\alpha\right)  \right)  }_{=\left(
f_{1}g^{\prime},0\right)  }\\
&  =\underbrace{\left(  \eta\left(  f_{1}\partial\right)  \right)  \left(
f_{2}g^{\prime},0\right)  }_{=\left(  f_{1}\left(  f_{2}g^{\prime}\right)
^{\prime},0\right)  }-\underbrace{\left(  \eta\left(  f_{2}\partial\right)
\right)  \left(  f_{1}g^{\prime},0\right)  }_{=\left(  f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right)  }=\left(  f_{1}\left(
f_{2}g^{\prime}\right)  ^{\prime},0\right)  -\left(  f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right) \\
&  =\left(  f_{1}\left(  f_{2}g^{\prime}\right)  ^{\prime}-f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right)  =\left(  \left(  f_{1}%
f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  g^{\prime},0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }f_{1}\underbrace{\left(  f_{2}g^{\prime}\right)  ^{\prime}%
}_{=f_{2}^{\prime}g^{\prime}+f_{2}g^{\prime\prime}}-f_{2}\underbrace{\left(
f_{1}g^{\prime}\right)  ^{\prime}}_{=f_{1}^{\prime}g^{\prime}+f_{1}%
g^{\prime\prime}}=f_{1}\left(  f_{2}^{\prime}g^{\prime}+f_{2}g\right)
-f_{2}\left(  f_{1}^{\prime}g^{\prime}+f_{1}g^{\prime\prime}\right) \\
=f_{1}f_{2}^{\prime}g^{\prime}-f_{2}f_{1}^{\prime}g^{\prime}=\left(
f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  g^{\prime}%
\end{array}
\right)  ,
\end{align*}
so that $\left(  \eta\left(  \left[  f_{1}\partial,f_{2}\partial\right]
\right)  \right)  \left(  g,\alpha\right)  =\left[  \eta\left(  f_{1}%
\partial\right)  ,\eta\left(  f_{2}\partial\right)  \right]  \left(
g,\alpha\right)  $. Thus, any $f_{1}\in\mathbb{C}\left[  t,t^{-1}\right]  $
and $f_{2}\in\mathbb{C}\left[  t,t^{-1}\right]  $ satisfy $\eta\left(  \left[
f_{1}\partial,f_{2}\partial\right]  \right)  )=\left[  \eta\left(
f_{1}\partial\right)  ,\eta\left(  f_{2}\partial\right)  \right]  $. This
proves that $\eta$ is a Lie algebra homomorphism, and thus Lemma
\ref{lem.WtoDerA} is proven.

\textit{Second proof of Lemma \ref{lem.WtoDerA}.} Let us really interpret the
elements of $W$ as vector fields on $\mathbb{C}^{\times}$. The bracket
$\left[  \cdot,\cdot\right]  $ in $\mathcal{A}$ was defined in an invariant
way:%
\[
\left[  f,g\right]  =\operatorname*{Res}\nolimits_{t=0}\left(  gdf\right)
=\dfrac{1}{2\pi i}\oint\limits_{\left\vert z\right\vert =1}%
gdf\ \ \ \ \ \ \ \ \ \ \left(  \text{by Cauchy's residue theorem}\right)
\]
is an integral of a $1$-form, thus invariant under diffeomorphisms, thus
invariant under "infinitesimal diffeomorphisms" such as the ones given by
elements of $W$. Thus, Lemma \ref{lem.WtoDerA} becomes obvious.

The first of these two proofs is obviously the more straightforward one (and
generalizes better to fields other than $\mathbb{C}$), but it does not offer
any explanation why Lemma \ref{lem.WtoDerA} is more than a mere coincidence.
Meanwhile, the second proof gives Lemma \ref{lem.WtoDerA} a philosophical
reason to be true.

\subsection{The Virasoro algebra}

In representation theory, one often doesn't encounter representations of $W$
directly, but instead one finds representations of a $1$-dimensional central
extension of $W$ called the Virasoro algebra. I will now construct this
extension and show that it is the only one (up to isomorphism of extensions).

Let us recollect the theory of central extensions of Lie algebras (more
precisely, the $1$-dimensional ones):

\begin{definition}
\label{def.centex}If $L$ is a Lie algebra, then a $1$-dimensional central
extension of $L$ is a Lie algebra $\widehat{L}$ along with an exact sequence%
\begin{equation}
0\rightarrow\mathbb{C}\rightarrow\widehat{L}\rightarrow L\rightarrow0,
\label{def.2-cocyc.es}%
\end{equation}
where $\mathbb{C}$ is central in $\widehat{L}$. Since all exact sequences of
vector spaces split, we can pick a splitting of this exact sequence on the
level of vector spaces, and thus identify $\widehat{L}$ with $L\oplus
\mathbb{C}$ as a vector space (not as a Lie algebra). Upon this
identification, the Lie bracket of $\widehat{L}$ can be written as%
\begin{equation}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]  =\left(
\left[  a,b\right]  ,\omega\left(  a,b\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for }a\in L\text{, }\alpha\in\mathbb{C}\text{, }b\in
L\text{, }\beta\in\mathbb{C}, \label{def.2-cocyc.form}%
\end{equation}
for some skew-symmetric bilinear form $\omega:L\times L\rightarrow\mathbb{C}$.
(We can also write this skew-symmetric bilinear form $\omega:L\times
L\rightarrow\mathbb{C}$ as a linear form $\wedge^{2}L\rightarrow\mathbb{C}$.)
But $\omega$ cannot be a completely arbitrary skew-symmetric bilinear form. It
needs to satisfy the so-called $2$\textit{-cocycle condition}%
\begin{equation}
\omega\left(  \left[  a,b\right]  ,c\right)  +\omega\left(  \left[
b,c\right]  ,a\right)  +\omega\left(  \left[  c,a\right]  ,b\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }a,b,c\in L. \label{def.2-cocyc.eq}%
\end{equation}
This condition comes from the requirement that the bracket in $\widehat{L}$
have to satisfy the Jacobi identity.

In the following, a $2$\textit{-cocycle on }$L$ will mean a skew-symmetric
bilinear form $\omega:L\times L\rightarrow\mathbb{C}$ (not necessarily
obtained from a central extension!) which satisfies the equation
(\ref{def.2-cocyc.eq}). (The name "$2$-cocycle" comes from Lie algebra
cohomology, where $2$-cocycles are indeed the cocycles in the $2$nd degree.)
Thus, we have assigned a $2$-cocycle on $L$ to every $1$-dimensional central
extension of $L$ (although the assignment depended on the splitting).

Conversely, if $\omega$ is any $2$-cocycle on $L$, then we can define a
central extension $\widehat{L}_{\omega}=L\oplus\mathbb{C}$ (whose Lie bracket
is defined by (\ref{def.2-cocyc.form})). Thus, every $2$-cocycle on $L$
canonically determines a $1$-dimensional central extension of $L$.

However, our assignment of the $2$-cocycle $\omega$ to the central extension
$\widehat{L}$ was not canonical, but depended on the splitting of the exact
sequence (\ref{def.2-cocyc.es}). If we change the splitting by some $\xi\in
L^{\ast}$, then $\omega$ is changed by $d\xi$ (this means that $\omega$ is
being replaced by $\omega+d\xi$), where $d\xi$ is the $2$-cocycle on $L$
defined by
\[
d\xi\left(  a,b\right)  =\xi\left(  \left[  a,b\right]  \right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in L.
\]
The $2$-cocycle $d\xi$ is called a $2$\textit{-coboundary}. As a conclusion,
$1$-dimensional central extensions of $L$ are parametrized up to isomorphism
by%
\[
\left(  2\text{-cocycles}\right)  \diagup\left(  2\text{-coboundaries}\right)
=H^{2}\left(  L\right)  .
\]
(Note that "up to isomorphism" means "up to isomorphism of extensions" here,
not "up to isomorphism of Lie algebras".) The vector space $H^{2}\left(
L\right)  $ is called the $2$\textit{-nd cohomology space} (or just the $2$-nd
cohomology) of the Lie algebra $L$.
\end{definition}

\begin{theorem}
\label{thm.H^2(W)}The space $H^{2}\left(  W\right)  $ is $1$-dimensional and
is spanned by the residue class of the $2$-cocycle $\omega$ given by%
\[
\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z}.
\]

\end{theorem}

Note that in this theorem, we could have replaced the factor $\dfrac{n^{3}%
-n}{6}$ by $n^{3}-n$ (since the vector space spanned by a vector obviously
doesn't change if we rescale the vector by a scalar factor), or even by
$n^{3}$ (since the $2$-cocycle $\left(  L_{n},L_{m}\right)  \mapsto
n\delta_{n,-m}$ is a coboundary, and two $2$-cocycles which differ by a
coboundary give the same residue class in $H^{2}\left(  W\right)  $). But we
prefer $\dfrac{n^{3}-n}{6}$ since this is closer to how this class appears in
representation theory (and, also, comes up in the proof below).

\textit{Proof of Theorem \ref{thm.H^2(W)}.} First of all, it is easy to prove
by computation that the bilinear form $\omega:W\times W\rightarrow\mathbb{C}$
given by%
\[
\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z}%
\]
is indeed a $2$-cocycle. Now, let us prove that every $2$-cocycle on $W$ is
congruent to a multiple of $\omega$ modulo the $2$-coboundaries.

Let $\beta$ be a $2$-cocycle on $W$. We must prove that $\beta$ is congruent
to a multiple of $\omega$ modulo the $2$-coboundaries.

Pick $\xi\in W^{\ast}$ such that $\xi\left(  L_{n}\right)  =\dfrac{1}{n}%
\beta\left(  L_{n},L_{0}\right)  $ for all $n\neq0$ (such a $\xi$ clearly
exists, but is not unique since we have complete freedom in choosing
$\xi\left(  L_{0}\right)  $). Let $\widetilde{\beta}$ be the $2$-cocycle
$\beta-d\xi$. Then,
\[
\widetilde{\beta}\left(  L_{n},L_{0}\right)  =\underbrace{\beta\left(
L_{n},L_{0}\right)  }_{\substack{=n\xi\left(  L_{n}\right)  \\\text{(since
}\xi\left(  L_{n}\right)  =\dfrac{1}{n}\beta\left(  L_{n},L_{0}\right)
\text{)}}}-\xi\left(  \underbrace{\left[  L_{n},L_{0}\right]  }_{=nL_{n}%
}\right)  =n\xi\left(  L_{n}\right)  -\xi\left(  nL_{n}\right)  =0
\]
for every $n\neq0$. Thus, by replacing $\beta$ by $\widetilde{\beta}$, we can
WLOG assume that $\beta\left(  L_{n},L_{0}\right)  =0$ for every $n\neq0$.
This clearly also holds for $n=0$ since $\beta$ is skew-symmetric. Hence,
$\beta\left(  X,L_{0}\right)  =0$ for every $X\in W$. Now, by the $2$-cocycle
condition, we have%
\[
\beta\left(  \left[  L_{0},L_{m}\right]  ,L_{n}\right)  +\beta\left(  \left[
L_{n},L_{0}\right]  ,L_{m}\right)  +\beta\left(  \left[  L_{m},L_{n}\right]
,L_{0}\right)  =0
\]
for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Thus,%
\begin{align*}
0  &  =\beta\left(  \underbrace{\left[  L_{0},L_{m}\right]  }_{=-mL_{m}}%
,L_{n}\right)  +\beta\left(  \underbrace{\left[  L_{n},L_{0}\right]
}_{=nL_{n}},L_{m}\right)  +\underbrace{\beta\left(  \left[  L_{m}%
,L_{n}\right]  ,L_{0}\right)  }_{=0\text{ (since }\beta\left(  X,L_{0}\right)
=0\text{ for every }X\in W\text{)}}\\
&  =-m\beta\left(  L_{m},L_{n}\right)  +n\underbrace{\beta\left(  L_{n}%
,L_{m}\right)  }_{=-\beta\left(  L_{m},L_{n}\right)  }=-\left(  m+n\right)
\beta\left(  L_{m},L_{n}\right)
\end{align*}
for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Hence, for all $n\in\mathbb{Z}$
and $m\in\mathbb{Z}$ with $n+m\neq0$, we have $\beta\left(  L_{m}%
,L_{n}\right)  =0$. In other words, there exists some sequence $\left(
b_{n}\right)  _{n\in\mathbb{Z}}\in\mathbb{C}^{\mathbb{Z}}$ such that
\begin{equation}
\beta\left(  L_{n},L_{m}\right)  =b_{n}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\label{thm.H^2(W).pf.2}%
\end{equation}
This sequence satisfies
\begin{equation}
b_{-n}=-b_{n}\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}
\label{thm.H^2(W).pf.1}%
\end{equation}
(since $\beta$ is skew-symmetric and thus $\beta\left(  L_{n},L_{-n}\right)
=-\beta\left(  L_{-n},L_{n}\right)  $) and thus, in particular, $b_{0}=0$. We
will now try to get a recursive equation for this sequence.

Let $m$, $n$ and $p$ be three integers satisfying $m+n+p=0$. Then, the
$2$-cocycle condition yields%
\[
\beta\left(  \left[  L_{p},L_{n}\right]  ,L_{m}\right)  +\beta\left(  \left[
L_{m},L_{p}\right]  ,L_{n}\right)  +\beta\left(  \left[  L_{n},L_{m}\right]
,L_{p}\right)  =0.
\]
Due to%
\begin{align*}
\beta\left(  \underbrace{\left[  L_{p},L_{n}\right]  }_{=\left(  p-n\right)
L_{p+n}},L_{m}\right)   &  =\left(  p-n\right)  \underbrace{\beta\left(
L_{p+n},L_{m}\right)  }_{\substack{=-\beta\left(  L_{m},L_{p+n}\right)
\\\text{(since }\beta\text{ is skew-symmetric)}}}=-\left(  p-n\right)
\underbrace{\beta\left(  L_{m},L_{p+n}\right)  }_{\substack{=b_{m}%
\delta_{m,-\left(  p+n\right)  }\\\text{(by (\ref{thm.H^2(W).pf.2}))}}}\\
&  =-\left(  p-n\right)  b_{m}\underbrace{\delta_{m,-\left(  p+n\right)  }%
}_{\substack{=1\\\text{(since }m+n+p=0\text{)}}}=-\left(  p-n\right)  b_{m}%
\end{align*}
and the two cyclic permutations of this equality, this rewrites as%
\[
\left(  -\left(  p-n\right)  b_{m}\right)  +\left(  -\left(  m-p\right)
b_{n}\right)  +\left(  -\left(  n-m\right)  b_{p}\right)  =0.
\]
In other words,%
\begin{equation}
\left(  n-m\right)  b_{p}+\left(  m-p\right)  b_{n}+\left(  p-n\right)
b_{m}=0. \label{thm.H^2(W).pf.3}%
\end{equation}


Now define a form $\xi_{0}\in W^{\ast}$ by $\xi_{0}\left(  L_{0}\right)  =1$
and $\xi_{0}\left(  L_{i}\right)  =0$ for all $i\neq0$.

By replacing $\beta$ with $\beta-\dfrac{b_{1}}{2}d\xi_{0}$, we can assume WLOG
that $b_{1}=0$.

Now let $n\in\mathbb{Z}$ be arbitrary. Setting $m=1$ and $p=-\left(
n+1\right)  $ in (\ref{thm.H^2(W).pf.3}) (this is allowed since $1+n+\left(
-\left(  n+1\right)  \right)  =0$), we get%
\[
\left(  n-1\right)  b_{-\left(  n+1\right)  }+\left(  1-\left(  -\left(
n+1\right)  \right)  \right)  b_{n}+\left(  n-1\right)  b_{1}=0.
\]
Thus,%
\begin{align*}
0  &  =\left(  n-1\right)  \underbrace{b_{-\left(  n+1\right)  }}%
_{=-b_{n+1}\text{ (by (\ref{thm.H^2(W).pf.1}))}}+\underbrace{\left(  1-\left(
-\left(  n+1\right)  \right)  \right)  }_{=n+2}b_{n}+\left(  n-1\right)
\underbrace{b_{1}}_{=0}\\
&  =-\left(  n-1\right)  b_{n+1}+\left(  n+2\right)  b_{n},
\end{align*}
so that $\left(  n-1\right)  b_{n+1}=\left(  n+2\right)  b_{n}$. This
recurrence equation rewrites as $b_{n+1}=\dfrac{n+2}{n-1}b_{n}$ for $n\geq2$.
Thus, by induction we see that every $n\geq2$ satisfies%
\[
b_{n}=\dfrac{n+1}{n-2}\cdot\dfrac{n}{n-3}\cdot\dfrac{n-1}{n-4}\cdot
...\cdot\dfrac{4}{1}b_{2}=\dfrac{\left(  n+1\right)  \cdot n\cdot...\cdot
4}{\left(  n-2\right)  \cdot\left(  n-3\right)  \cdot...\cdot1}b_{2}%
=\dfrac{\left(  n+1\right)  \left(  n-1\right)  n}{6}b_{2}=\dfrac{n^{3}-n}%
{6}b_{2}.
\]
But $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$ also holds for $n=1$ (since $b_{1}=0$ and
$\dfrac{1^{3}-1}{6}=0$) and for $n=0$ (since $b_{0}=0$ and $\dfrac{0^{3}-0}%
{6}=0$). Hence, $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$ holds for every $n\geq0$. By
(\ref{thm.H^2(W).pf.1}), we conclude that $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$
holds also for every $n\leq0$. Thus, every $n\in\mathbb{Z}$ satisfies
$b_{n}=\dfrac{n^{3}-n}{6}b_{2}$. From (\ref{thm.H^2(W).pf.2}), we thus see
that $\beta$ is a scalar multiple of $\omega$.

We thus have proven that every $2$-cocycle $\beta$ on $W$ is congruent to a
multiple of $\omega$ modulo the $2$-coboundaries. This yields that the space
$H^{2}\left(  W\right)  $ is \textit{at most }$1$-dimensional and is spanned
by the residue class of the $2$-cocycle $\omega$. In order to complete the
proof of Theorem \ref{thm.H^2(W)}, we have yet to prove that $H^{2}\left(
W\right)  $ is indeed $1$-dimensional (and not $0$-dimensional), i. e., that
the $2$-cocycle $\omega$ is \textit{not} a $2$-coboundary. But this is
easy\footnote{\textit{Proof.} Assume the contrary. Then, the $2$-cocycle
$\omega$ is a $2$-coboundary. This means that there exists a linear map
$\xi:W\rightarrow\mathbb{C}$ such that $\omega=d\xi$. Pick such a $\xi$. Then,%
\[
\omega\left(  L_{2},L_{-2}\right)  =\left(  d\xi\right)  \left(  L_{2}%
,L_{-2}\right)  =\xi\left(  \underbrace{\left[  L_{2},L_{-2}\right]
}_{=4L_{0}}\right)  =4\xi\left(  L_{0}\right)
\]
and%
\[
\omega\left(  L_{1},L_{-1}\right)  =\left(  d\xi\right)  \left(  L_{1}%
,L_{-1}\right)  =\xi\left(  \underbrace{\left[  L_{1},L_{-1}\right]
}_{=2L_{0}}\right)  =2\xi\left(  L_{0}\right)  .
\]
Hence,%
\[
2\underbrace{\omega\left(  L_{1},L_{-1}\right)  }_{=2\xi\left(  L_{0}\right)
}=4\xi\left(  L_{0}\right)  =\omega\left(  L_{2},L_{-2}\right)  .
\]
But this contradicts with the equalities $\omega\left(  L_{1},L_{-1}\right)
=0$ and $\omega\left(  L_{2},L_{-2}\right)  =1$ (which easily follow from the
definition of $\omega$). This contradiction shows that our assumption was
wrong, and thus the $2$-cocycle $\omega$ is not a $2$-coboundary, qed.}. The
proof of Theorem \ref{thm.H^2(W)} is thus complete.

The $2$-cocycle $\dfrac{1}{2}\omega$ (where $\omega$ is the $2$-cocycle
introduced in Theorem \ref{thm.H^2(W)}) gives a central extension of the Witt
algebra $W$: the so-called Virasoro algebra. Let us recast the definition of
this algebra in elementary terms:

\begin{definition}
The \textit{Virasoro algebra} $\operatorname*{Vir}$ is defined as the vector
space $W\oplus\mathbb{C}$ with Lie bracket defined by%
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}%
-n}{12}\delta_{n,-m}C;\\
\left[  L_{n},C\right]   &  =0,
\end{align*}
where $L_{n}$ denotes $\left(  L_{n},0\right)  $ for every $n\in\mathbb{Z}$,
and where $C$ denotes $\left(  0,1\right)  $. Note that $\left\{  L_{n}%
\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  C\right\}  $ is a basis of
$\operatorname*{Vir}$.
\end{definition}

If we change the denominator $12$ to any other nonzero complex number, we get
a Lie algebra isomorphic to $\operatorname*{Vir}$ (it is just a rescaling of
$C$). It is easy to show that the Virasoro algebra is not isomorphic to the
Lie-algebraic direct sum $W\oplus\mathbb{C}$. Thus, $\operatorname*{Vir}$ is
the unique (up to Lie algebra isomorphism) nontrivial $1$-dimensional central
extension of $W$.

\subsection{Affine Lie algebras}

Now let us talk about affine Lie algebras:

\begin{definition}
\label{def.loop}Let $\mathfrak{g}$ be a Lie algebra with a symmetric bilinear
form $\left(  \cdot,\cdot\right)  $ invariant under the Lie bracket (this
means that $\left(  \left[  a,b\right]  ,c\right)  +\left(  b,\left[
a,c\right]  \right)  =0$ for all $a,b,c\in\mathfrak{g}$). The $\mathbb{C}$-Lie
algebra $\mathfrak{g}$ induces (by extension of scalars) a $\mathbb{C}\left[
t,t^{-1}\right]  $-Lie algebra $\mathbb{C}\left[  t,t^{-1}\right]
\otimes\mathfrak{g}=\left\{  \sum\limits_{i}a_{i}t^{i}\ \mid\ a_{i}%
\in\mathfrak{g}\right\}  $. This Lie algebra $\mathbb{C}\left[  t,t^{-1}%
\right]  \otimes\mathfrak{g}$, considered as a $\mathbb{C}$-Lie algebra, will
be called the \textit{loop algebra} of $\mathfrak{g}$, and denoted by
$\mathfrak{g}\left[  t,t^{-1}\right]  $.

Then, we can define a $2$-cocycle $\omega$ on the loop algebra $\mathfrak{g}%
\left[  t,t^{-1}\right]  $ by%
\begin{equation}
\omega\left(  f,g\right)  =\operatorname*{Res}\nolimits_{t=0}%
\underbrace{\left(  df,g\right)  }_{\text{scalar-valued }1\text{-form}}%
=\sum\limits_{i\in\mathbb{Z}}i\left(  f_{i},g_{-i}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }f,g\in\mathfrak{g}\left[  t,t^{-1}%
\right]  \label{loop.w}%
\end{equation}
(where we write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$
with $f_{i}\in\mathfrak{g}$, and where we write $g$ in the form $g=\sum
\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in\mathfrak{g}$).

Proving that $\omega$ is a $2$-cocycle is an exercise. So we can define a
$1$-dimensional central extension $\mathfrak{g}\left[  t,t^{-1}\right]
_{\omega}=\mathfrak{g}\left[  t,t^{-1}\right]  \oplus\mathbb{C}$ with bracket
defined by $\omega$.

We are going to abbreviate $\mathfrak{g}\left[  t,t^{-1}\right]  _{\omega}$ by
$\widehat{\mathfrak{g}}_{\omega}$, or, more radically, by
$\widehat{\mathfrak{g}}$.
\end{definition}

We already know one example of this construction:

\begin{remark}
If $\mathfrak{g}$ is the abelian Lie algebra $\mathbb{C}$, and $\left(
\cdot,\cdot\right)  $ is the bilinear form $\mathbb{C}\times\mathbb{C}%
\rightarrow\mathbb{C},\ \left(  x,y\right)  \mapsto xy$, then the $2$-cocycle
$\omega$ on the loop algebra $\mathbb{C}\left[  t,t^{-1}\right]  $ is given by%
\[
\omega\left(  x,y\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
gdf\right)  =\sum\limits_{i\in\mathbb{Z}}if_{i}g_{-i}%
\ \ \ \ \ \ \ \ \ \ \text{for every }f,g\in\mathbb{C}\left[  t,t^{-1}\right]
\]
(where we write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$
with $f_{i}\in\mathbb{C}$, and where we write $g$ in the form $g=\sum
\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in\mathbb{C}$). Hence, in this
case, the central extension $\mathfrak{g}\left[  t,t^{-1}\right]  _{\omega
}=\widehat{\mathfrak{g}}_{\omega}$ is precisely the Heisenberg algebra
$\mathcal{A}$ as introduced in Definition \ref{def.osc}.
\end{remark}

The main example that we will care about is when $\mathfrak{g}$ is a simple
finite-dimensional Lie algebra and $\left(  \cdot,\cdot\right)  $ is the
unique (up to scalar) invariant symmetric bilinear form (i. e., a multiple of
the Killing form).

\begin{theorem}
\label{thm.H^2(gtt)}If $\mathfrak{g}$ is a simple finite-dimensional Lie
algebra, then $H^{2}\left(  \mathfrak{g}\left[  t,t^{-1}\right]  \right)  $ is
$1$-dimensional and spanned by the cocycle $\omega$ corresponding to $\left(
\cdot,\cdot\right)  $.
\end{theorem}

\begin{corollary}
\label{cor.g_w^hat}If $\mathfrak{g}$ is a simple finite-dimensional Lie
algebra, then the Lie algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $ has a
unique (up to isomorphism of Lie algebras, not up to isomorphism of
extensions) nontrivial $1$-dimensional central extension
$\widehat{\mathfrak{g}}_{\omega}$.
\end{corollary}

\begin{definition}
\label{def.kac}The Lie algebra $\widehat{\mathfrak{g}}_{\omega}$ defined in
Corollary \ref{cor.g_w^hat} is called the \textit{affine Kac-Moody algebra}.
(Or, more precisely, the \textit{untwisted affine Kac-Moody algebra}.)
\end{definition}

In order to prepare for the proof of Theorem \ref{thm.H^2(gtt)}, we recollect
some facts from the cohomology of Lie algebras:

\begin{definition}
Let $\mathfrak{g}$ be a Lie algebra. Let $M$ be a $\mathfrak{g}$-module. We
define the \textit{semidirect product} $\mathfrak{g}\ltimes M$ to be the Lie
algebra which, as a vector space, is $\mathfrak{g}\oplus M$, but whose Lie
bracket is defined by%
\[
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]  =\left(
\left[  a,b\right]  ,a\circ\beta-b\circ\alpha\right)  .
\]
(The symbol $\circ$ means action here; i. e., a term like $c\circ m$ (with
$c\in\mathfrak{g}$ and $m\in M$) means the action of $c$ on $m$.) Thus, the
canonical injection $\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,$
$a\mapsto\left(  a,0\right)  $ is a Lie algebra homomorphism, and so is the
canonical projection $\mathfrak{g}\ltimes M\rightarrow\mathfrak{g},$ $\left(
a,\alpha\right)  \mapsto a$. Also, $M$ is embedded into $\mathfrak{g}\ltimes
M$ by the injection $M\rightarrow\mathfrak{g}\ltimes M,$ $\alpha\mapsto\left(
0,\alpha\right)  $; this makes $M$ an abelian Lie subalgebra of $\mathfrak{g}%
\ltimes M$.
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a Lie algebra. Let $M$ be a $\mathfrak{g}$-module. A
$1$\textit{-cocycle} \textit{of }$\mathfrak{g}$\textit{ with coefficients in
}$M$ is a linear map $\eta:\mathfrak{g}\rightarrow M$ such that%
\[
\eta\left(  \left[  a,b\right]  \right)  =a\circ\eta\left(  b\right)
-b\circ\eta\left(  a\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}.
\]
(The symbol $\circ$ means action here; i. e., a term like $c\circ m$ (with
$c\in\mathfrak{g}$ and $m\in M$) means the action of $c$ on $m$.)

It is easy to see (and known) that $1$-cocycles of $\mathfrak{g}$ with
coefficients in $M$ are in bijection with Lie algebra homomorphisms
$\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M$. This bijection sends every
$1$-cocycle $\eta$ to the map $\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,$
$a\mapsto\left(  a,\eta\left(  a\right)  \right)  $.

A $1$\textit{-coboundary of }$\mathfrak{g}$ \textit{with coefficients in }$M$
means a linear map $\eta:\mathfrak{g}\rightarrow M$ which has the form
$a\mapsto a\circ m$ for some $m\in M$. Every $1$-coboundary of $\mathfrak{g}$
with coefficients in $M$ is a $1$-cocycle.

The space of $1$-cocycles of $\mathfrak{g}$ with coefficients in $M$ is
denoted by $Z^{1}\left(  \mathfrak{g},M\right)  $. The space of $1$%
-coboundaries of $\mathfrak{g}$ with coefficients in $M$ is denoted by
$B^{1}\left(  \mathfrak{g},M\right)  $. We have $B^{1}\left(  \mathfrak{g}%
,M\right)  \subseteq Z^{1}\left(  \mathfrak{g},M\right)  $. The quotient space
$Z^{1}\left(  \mathfrak{g},M\right)  \diagup B^{1}\left(  \mathfrak{g}%
,M\right)  $ is denoted by $H^{1}\left(  \mathfrak{g},M\right)  $ is called
the $1$\textit{-st cohomology space} of $\mathfrak{g}$\textit{ with
coefficients in }$M$.

Of course, these spaces $Z^{1}\left(  \mathfrak{g},M\right)  $, $B^{1}\left(
\mathfrak{g},M\right)  $ and $H^{1}\left(  \mathfrak{g},M\right)  $ are but
particular cases of more general constructions $Z^{i}\left(  \mathfrak{g}%
,M\right)  $, $B^{i}\left(  \mathfrak{g},M\right)  $ and $H^{i}\left(
\mathfrak{g},M\right)  $ which are defined for every $i\in\mathbb{N}$. (In
particular, $H^{0}\left(  \mathfrak{g},M\right)  $ is the subspace $\left\{
m\in M\ \mid\ a\circ m=0\text{ for all }a\in\mathfrak{g}\right\}  $ of $M$,
and often denoted by $M^{\mathfrak{g}}$.) The spaces $H^{i}\left(
\mathfrak{g},M\right)  $ (or, more precisely, the functors assigning these
spaces to every $\mathfrak{g}$-module $M$) can be understood as the so-called
derived functors of the functor $M\mapsto M^{\mathfrak{g}}$. However, we won't
use $H^{i}\left(  \mathfrak{g},M\right)  $ for any $i$ other than $1$ here.

We record a relation between $H^{1}\left(  \mathfrak{g},M\right)  $ and the
$\operatorname*{Ext}$ bifunctor:%
\[
H^{1}\left(  \mathfrak{g},M\right)  =\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)  .
\]
More generally, $\operatorname*{Ext}\nolimits_{\mathfrak{g}}^{1}\left(
N,M\right)  =H^{1}\left(  \mathfrak{g},\operatorname*{Hom}%
\nolimits_{\mathbb{C}}\left(  N,M\right)  \right)  $ for any two
$\mathfrak{g}$-modules $N$ and $M$.
\end{definition}

\begin{theorem}
[Whitehead]\label{thm.white}If $\mathfrak{g}$ is a simple finite-dimensional
Lie algebra, and $M$ is a finite-dimensional $\mathfrak{g}$-module, then
$H^{1}\left(  \mathfrak{g},M\right)  =0$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.white}.} Since $\mathfrak{g}$ is a simple
Lie algebra, Weyl's theorem says that finite-dimensional $\mathfrak{g}%
$-modules are completely reducible. Hence, if $N$ and $M$ are
finite-dimensional $\mathfrak{g}$-modules, we have $\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  N,M\right)  =0$. In particular,
$\operatorname*{Ext}\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)
=0$. Since $H^{1}\left(  \mathfrak{g},M\right)  =\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)  $, this yields
$H^{1}\left(  \mathfrak{g},M\right)  =0$. Theorem \ref{thm.white} is thus proven.

\begin{lemma}
\label{lem.Z^1}Let $\omega$ be a $2$-cocycle on a Lie algebra $\mathfrak{g}$.
Let $\mathfrak{g}_{0}\subseteq\mathfrak{g}$ be a Lie subalgebra, and
$M\subseteq\mathfrak{g}$ be a $\mathfrak{g}_{0}$-submodule. Then, $\omega
\mid_{\mathfrak{g}_{0}\times M}$, when considered as a map $\mathfrak{g}%
_{0}\rightarrow M^{\ast}$, belongs to $Z^{1}\left(  \mathfrak{g}_{0},M^{\ast
}\right)  $.
\end{lemma}

The proof of Lemma \ref{lem.Z^1} is a straightforward manipulation of formulas:

\textit{Proof of Lemma \ref{lem.Z^1}.} Let $\eta$ denote the $2$-cocycle
$\omega\mid_{\mathfrak{g}_{0}\times M}$, considered as a map $\mathfrak{g}%
_{0}\rightarrow M^{\ast}$. Thus, $\eta$ is defined by%
\[
\eta\left(  x\right)  =\left(  M\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ y\mapsto\omega\left(  x,y\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}_{0}.
\]
Hence, $\left(  \eta\left(  x\right)  \right)  \left(  y\right)
=\omega\left(  x,y\right)  $ for all $x\in\mathfrak{g}_{0}$ and $y\in M$.

Now, any $a\in\mathfrak{g}_{0}$, $b\in\mathfrak{g}_{0}$ and $c\in M$ satisfy
$\left(  \eta\left(  \left[  a,b\right]  \right)  \right)  \left(  c\right)
=\omega\left(  \left[  a,b\right]  ,c\right)  $ and%
\begin{align*}
&  \left(  a\circ\eta\left(  b\right)  -b\circ\eta\left(  a\right)  \right)
\left(  c\right) \\
&  =\underbrace{\left(  a\circ\eta\left(  b\right)  \right)  \left(  c\right)
}_{\substack{=-\left(  \eta\left(  b\right)  \right)  \left(  \left[
a,c\right]  \right)  \\\text{(by the definition of the dual of a }%
\mathfrak{g}_{0}\text{-module)}}}-\underbrace{\left(  b\circ\eta\left(
a\right)  \right)  \left(  c\right)  }_{\substack{=-\left(  \eta\left(
a\right)  \right)  \left(  \left[  b,c\right]  \right)  \\\text{(by the
definition of the dual of a }\mathfrak{g}_{0}\text{-module)}}}\\
&  =\left(  -\underbrace{\left(  \eta\left(  b\right)  \right)  \left(
\left[  a,c\right]  \right)  }_{=\omega\left(  b,\left[  a,c\right]  \right)
}\right)  -\left(  -\underbrace{\left(  \eta\left(  a\right)  \right)  \left(
\left[  b,c\right]  \right)  }_{=\omega\left(  a,\left[  b,c\right]  \right)
}\right) \\
&  =\left(  -\omega\left(  b,\left[  a,c\right]  \right)  \right)  -\left(
-\omega\left(  a,\left[  b,c\right]  \right)  \right)  =-\omega\left(
b,\underbrace{\left[  a,c\right]  }_{=-\left[  c,a\right]  }\right)
+\omega\left(  a,\left[  b,c\right]  \right) \\
&  =\underbrace{\omega\left(  b,\left[  c,a\right]  \right)  }%
_{\substack{=-\omega\left(  \left[  c,a\right]  ,b\right)  \\\text{(since
}\omega\text{ is antisymmetric)}}}+\underbrace{\omega\left(  a,\left[
b,c\right]  \right)  }_{\substack{=-\omega\left(  \left[  b,c\right]
,a\right)  \\\text{(since }\omega\text{ is antisymmetric)}}}\\
&  =-\omega\left(  \left[  c,a\right]  ,b\right)  -\omega\left(  \left[
b,c\right]  ,a\right)  =\omega\left(  \left[  a,b\right]  ,c\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.2-cocyc.eq})}\right)  ,
\end{align*}
so that $\left(  \eta\left(  \left[  a,b\right]  \right)  \right)  \left(
c\right)  =\left(  a\circ\eta\left(  b\right)  -b\circ\eta\left(  a\right)
\right)  \left(  c\right)  $. Thus, any $a\in\mathfrak{g}_{0}$ and
$b\in\mathfrak{g}_{0}$ satisfy $\eta\left(  \left[  a,b\right]  \right)
=a\circ\eta\left(  b\right)  -b\circ\eta\left(  a\right)  $. This shows that
$\eta$ is a $1$-cocycle, i. e., belongs to $Z^{1}\left(  \mathfrak{g}%
_{0},M^{\ast}\right)  $. Lemma \ref{lem.Z^1} is proven.

\textit{Proof of Theorem \ref{thm.H^2(gtt)}.} First notice that any
$a,b,c\in\mathfrak{g}$ satisfy%
\begin{equation}
\left(  \left[  a,b\right]  ,c\right)  =\left(  \left[  b,c\right]  ,a\right)
=\left(  \left[  c,a\right]  ,b\right)  \label{thm.H^2(gtt).pf.0}%
\end{equation}
\footnote{\textit{Proof.} First of all, any $a,b,c\in\mathfrak{g}$ satisfy%
\begin{align*}
\left(  \left[  a,b\right]  ,c\right)   &  =\left(  a,\left[  b,c\right]
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the form }\left(  \cdot
,\cdot\right)  \text{ is invariant}\right) \\
&  =\left(  \left[  b,c\right]  ,a\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the form }\left(  \cdot,\cdot\right)  \text{ is symmetric}\right)
.
\end{align*}
Applying this to $b,c,a$ instead of $a,b,c$, we obtain $\left(  \left[
b,c\right]  ,a\right)  =\left(  \left[  c,a\right]  ,b\right)  $. Hence,
$\left(  \left[  a,b\right]  ,c\right)  =\left(  \left[  b,c\right]
,a\right)  =\left(  \left[  c,a\right]  ,b\right)  $, so that
(\ref{thm.H^2(gtt).pf.0}) is proven.}. Moreover,%
\begin{equation}
\text{there exist }a,b,c\in\mathfrak{g}\text{ such that }\left(  \left[
a,b\right]  ,c\right)  =\left(  \left[  b,c\right]  ,a\right)  =\left(
\left[  c,a\right]  ,b\right)  \neq0. \label{thm.H^2(gtt).pf.00}%
\end{equation}
\footnote{\textit{Proof.} Since $\mathfrak{g}$ is simple, we have $\left[
\mathfrak{g},\mathfrak{g}\right]  =\mathfrak{g}$ and thus $\left(  \left[
\mathfrak{g},\mathfrak{g}\right]  ,\mathfrak{g}\right)  =\left(
\mathfrak{g},\mathfrak{g}\right)  \neq0$ (since the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate). Hence, there exist $a,b,c\in\mathfrak{g}$
such that $\left(  \left[  a,b\right]  ,c\right)  \neq0$. The rest is handled
by (\ref{thm.H^2(gtt).pf.0}).} This will be used later in our proof; but as
for now, forget about these $a,b,c$.

It is easy to see that the $2$-cocycle $\omega$ on $\mathfrak{g}\left[
t,t^{-1}\right]  $ defined by (\ref{loop.w}) is not a $2$%
-coboundary.\footnote{\textit{Proof.} Assume the contrary. Then, this
$2$-cocycle $\omega$ is a coboundary, i. e., there exists a linear map
$\xi:\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$ such that
$\omega=d\xi$.
\par
Now, pick some $a\in\mathfrak{g}$ and $b\in\mathfrak{g}$ such that $\left(
a,b\right)  \neq0$ (this is possible since the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate). Then,%
\[
\underbrace{\omega}_{=d\xi}\left(  at,bt^{-1}\right)  =\left(  d\xi\right)
\left(  at,bt^{-1}\right)  =\xi\left(  \underbrace{\left[  at,bt^{-1}\right]
}_{=\left[  a,b\right]  }\right)  =\xi\left(  \left[  a,b\right]  \right)
\]
and%
\[
\underbrace{\omega}_{=d\xi}\left(  a,b\right)  =\left(  d\xi\right)  \left(
a,b\right)  =\xi\left(  \left[  a,b\right]  \right)  ,
\]
so that $\omega\left(  at,bt^{-1}\right)  =\omega\left(  a,b\right)  $. But by
the definition of $\omega$, we easily see that $\omega\left(  at,bt^{-1}%
\right)  =1\underbrace{\left(  a,b\right)  }_{\neq0}\neq0$ and $\omega\left(
a,b\right)  =0\left(  a,b\right)  =0$, which yields a contradiction.}

Now let us consider the structure of $\mathfrak{g}\left[  t,t^{-1}\right]  $.
We have $\mathfrak{g}\left[  t,t^{-1}\right]  =\bigoplus\limits_{n\in
\mathbb{Z}}\mathfrak{g}t^{n}\supseteq\mathfrak{g}t^{0}=\mathfrak{g}$. This is,
actually, an inclusion of Lie algebras. So $\mathfrak{g}$ is a Lie subalgebra
of $\mathfrak{g}\left[  t,t^{-1}\right]  $, and $\mathfrak{g}t^{n}$ is a
$\mathfrak{g}$-submodule of $\mathfrak{g}\left[  t,t^{-1}\right]  $ isomorphic
to $\mathfrak{g}$ for every $n\in\mathbb{Z}$.

Let $\omega$ be an arbitrary $2$-cocycle on $\mathfrak{g}\left[
t,t^{-1}\right]  $ (not necessarily the one defined by (\ref{loop.w})).

Let $n\in\mathbb{Z}$. Then, $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}%
$, when considered as a map $\mathfrak{g}\rightarrow\left(  \mathfrak{g}%
t^{n}\right)  ^{\ast}$, belongs to $Z^{1}\left(  \mathfrak{g},\left(
\mathfrak{g}t^{n}\right)  ^{\ast}\right)  $ (by Lemma \ref{lem.Z^1}, applied
to $\mathfrak{g}$, $\mathfrak{g}t^{n}$ and $\mathfrak{g}\left[  t,t^{-1}%
\right]  $ instead of $\mathfrak{g}_{0}$, $M$ and $\mathfrak{g}$), i. e., is a
$1$-cocycle. But by Theorem \ref{thm.white}, we have $H^{1}\left(
\mathfrak{g},\left(  \mathfrak{g}t^{n}\right)  ^{\ast}\right)  =0$, so this
rewrites as $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}\in B^{1}\left(
\mathfrak{g},\left(  \mathfrak{g}t^{n}\right)  ^{\ast}\right)  $. In other
words, there exists some $\xi_{n}\in\left(  \mathfrak{g}t^{n}\right)  ^{\ast}$
such that $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}=d\xi_{n}$. Pick
such a $\xi_{n}$. Thus,%
\[
\omega\left(  a,bt^{n}\right)  =\underbrace{\left(  \omega\mid_{\mathfrak{g}%
\times\mathfrak{g}t^{n}}\right)  }_{=d\xi_{n}}\left(  a,bt^{n}\right)
=\left(  d\xi_{n}\right)  \left(  a,bt^{n}\right)  =\xi_{n}\left(  \left[
a,bt^{n}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in
\mathfrak{g}.
\]


Define a map $\xi:\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$
by requiring that $\xi\mid_{\mathfrak{g}t^{n}}=\xi_{n}$ for every
$n\in\mathbb{Z}$.

Now, let $\widetilde{\omega}=\omega-d\xi$. Then,%
\[
\widetilde{\omega}\left(  x,y\right)  =\omega\left(  x,y\right)  -\xi\left(
\left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
x,y\in\mathfrak{g}\left[  t,t^{-1}\right]  .
\]
Replace $\omega$ by $\widetilde{\omega}$ (this doesn't change the residue
class of $\omega$ in $H^{2}\left(  \mathfrak{g}\left[  t,t^{-1}\right]
\right)  $, since $\widetilde{\omega}$ differs from $\omega$ by a
$2$-coboundary). By doing this, we have reduced to a situation when
\[
\omega\left(  a,bt^{n}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
a,b\in\mathfrak{g}\text{ and }n\in\mathbb{Z}.
\]
\footnote{But all the $\xi$-freedom has been used up in this reduction - i.
e., if the new $\omega$ is nonzero, then the original $\omega$ was not a
$2$-coboundary. This gives us an alternative way of proving that the
$2$-cocycle $\omega$ on $\mathfrak{g}\left[  t,t^{-1}\right]  $ defined by
(\ref{loop.w}) is not a $2$-coboundary.} Since $\omega$ is antisymmetric, this
yields%
\begin{equation}
\omega\left(  bt^{n},a\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
a,b\in\mathfrak{g}\text{ and }n\in\mathbb{Z}. \label{thm.H^2(gtt).pf.1}%
\end{equation}


Now, fix some $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Since $\omega$ is a
$2$-cocycle, the $2$-cocycle condition yields%
\begin{align*}
0  &  =\omega\left(  \underbrace{\left[  a,bt^{n}\right]  }_{=\left[
a,b\right]  t^{n}},ct^{m}\right)  +\omega\left(  \underbrace{\left[
ct^{m},a\right]  }_{\substack{=\left[  c,a\right]  t^{m}\\=-\left[
a,c\right]  t^{m}}},bt^{n}\right)  +\omega\left(  \underbrace{\left[
bt^{n},ct^{m}\right]  }_{=\left[  b,c\right]  t^{n+m}},a\right) \\
&  =\omega\left(  \left[  a,b\right]  t^{n},ct^{m}\right)  +\underbrace{\omega
\left(  -\left[  a,c\right]  t^{m},bt^{n}\right)  }_{=\omega\left(
bt^{n},\left[  a,c\right]  t^{m}\right)  }+\underbrace{\omega\left(  \left[
b,c\right]  t^{n+m},a\right)  }_{\substack{=0\\\text{(by
(\ref{thm.H^2(gtt).pf.1}))}}}\\
&  =\omega\left(  \left[  a,b\right]  t^{n},ct^{m}\right)  +\omega\left(
bt^{n},\left[  a,c\right]  t^{m}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a,b,c\in\mathfrak{g}\text{.}%
\end{align*}
In other words, the bilinear form on $\mathfrak{g}$ given by $\left(
b,c\right)  \mapsto\omega\left(  bt^{n},ct^{m}\right)  $ is $\mathfrak{g}%
$-invariant. But every $\mathfrak{g}$-invariant bilinear form on
$\mathfrak{g}$ must be a multiple of our bilinear form $\left(  \cdot
,\cdot\right)  $ (since $\mathfrak{g}$ is simple, and thus the space of all
$\mathfrak{g}$-invariant bilinear forms on $\mathfrak{g}$ is $1$%
-dimensional\footnote{and spanned by the Killing form}). Hence, there exists
some constant $\gamma_{n,m}\in\mathbb{C}$ (depending on $n$ and $m$) such
that
\begin{equation}
\omega\left(  bt^{n},ct^{m}\right)  =\gamma_{n,m}\cdot\left(  b,c\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }b,c\in\mathfrak{g}.
\label{thm.H^2(gtt).pf.2}%
\end{equation}
It is easy to see that%
\begin{equation}
\gamma_{n,m}=-\gamma_{m,n}\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z},
\label{thm.H^2(gtt).pf.3}%
\end{equation}
since the bilinear form $\omega$ is skew-symmetric whereas the bilinear form
$\left(  \cdot,\cdot\right)  $ is symmetric.

Now, for any $m\in\mathbb{Z}$, $n\in\mathbb{Z}$ and $p\in\mathbb{Z}$, the
$2$-cocycle condition yields%
\[
\omega\left(  \left[  at^{n},bt^{m}\right]  ,ct^{p}\right)  +\omega\left(
\left[  bt^{m},ct^{p}\right]  ,at^{n}\right)  +\omega\left(  \left[
ct^{p},at^{n}\right]  ,bt^{m}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all
}a,b,c\in\mathfrak{g}.
\]
Due to%
\[
\omega\left(  \underbrace{\left[  at^{n},bt^{m}\right]  }_{=\left[
a,b\right]  t^{n+m}},ct^{p}\right)  =\omega\left(  \left[  a,b\right]
t^{n+m},ct^{p}\right)  =\gamma_{n+m,p}\cdot\left(  \left[  a,b\right]
,c\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.H^2(gtt).pf.2}%
)}\right)
\]
and the two cyclic permutations of this identity, this rewrites as%
\[
\gamma_{n+m,p}\cdot\left(  \left[  a,b\right]  ,c\right)  +\gamma_{m+p,n}%
\cdot\left(  \left[  b,c\right]  ,a\right)  +\gamma_{p+n,m}\cdot\left(
\left[  c,a\right]  ,b\right)  =0.
\]
Since this holds for all $a,b,c\in\mathfrak{g}$, we can use
(\ref{thm.H^2(gtt).pf.00}) to transform this into%
\[
\gamma_{n+m,p}+\gamma_{m+p,n}+\gamma_{p+n,m}=0.
\]
Due to (\ref{thm.H^2(gtt).pf.3}), this rewrites as%
\[
\gamma_{n,m+p}+\gamma_{m,p+n}+\gamma_{p,m+n}=0.
\]
Denoting by $s$ the sum $m+n+p$, we can rewrite this as%
\[
\gamma_{n,s-n}+\gamma_{m,s-m}-\gamma_{m+n,s-m-n}=0.
\]
In other words, for fixed $s\in\mathbb{Z}$, the function $\mathbb{Z}%
\rightarrow\mathbb{C},$ $n\mapsto\gamma_{n,s-n}$ is additive. Hence,
$\gamma_{n,s-n}=n\gamma_{1,s-1}$ and $\gamma_{s-n,n}=\left(  s-n\right)
\gamma_{1,s-1}$ for every $n\in\mathbb{Z}$. Thus,
\begin{align*}
\left(  s-n\right)  \gamma_{1,s-1}  &  =\gamma_{s-n,n}=-\gamma_{n,s-n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.H^2(gtt).pf.3})}\right) \\
&  =-n\gamma_{1,s-1}\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}%
\end{align*}
Hence, $s\gamma_{1,s-1}=0$. Thus, for every $s\neq0$, we conclude that
$\gamma_{1,s-1}=0$ and hence $\gamma_{n,s-n}=n\underbrace{\gamma_{1,s-1}}%
_{=0}=0$ for every $n\in\mathbb{Z}$. In other words, $\gamma_{n,m}=0$ for
every $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfying $n+m\neq0$.

What happens for $s=0$ ? For $s=0$, the equation $\gamma_{n,s-n}%
=n\gamma_{1,s-1}$ becomes $\gamma_{n,-n}=n\gamma_{1,-1}$.

Thus we have proven that $\gamma_{n,m}=0$ for every $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$ satisfying $n+m\neq0$, and that every $n\in\mathbb{Z}$
satisfies $\gamma_{n,-n}=n\gamma_{1,-1}$.

Hence, the form $\omega$ must be a scalar multiple of the form which sends
every $\left(  f,g\right)  $ to $\operatorname*{Res}\nolimits_{t=0}%
\underbrace{\left(  df,g\right)  }_{\text{scalar-valued }1\text{-form}}%
=\sum\limits_{i\in\mathbb{Z}}i\left(  f_{i},g_{-i}\right)  $. We have thus
proven that every $2$-cocycle $\omega$ is a scalar multiple of the $2$-cocycle
$\omega$ defined by (\ref{loop.w}) modulo the $2$-coboundaries. Since we also
know that the $2$-cocycle $\omega$ defined by (\ref{loop.w}) is not a
$2$-coboundary, this yields that the space $H^{2}\left(  \mathfrak{g}\left[
t,t^{-1}\right]  \right)  $ is $1$-dimensional and spanned by the residue
class of the $2$-cocycle $\omega$ defined by (\ref{loop.w}). This proves
Theorem \ref{thm.H^2(gtt)}.

\section{Representation theory: generalities}

\subsection{Representation theory: general facts}

The first step in the representation theory of any objects (groups, algebras,
etc.) is usually proving some kind of Schur's lemma. There is one form of
Schur's lemma that holds almost tautologically: This is the form that claims
that every morphism between irreducible representations is either $0$ or an
isomorphism.\footnote{There are also variations on this assertion:
\par
\textbf{1)} Every morphism from an irreducible representation to a
representation is either $0$ or injective.
\par
\textbf{2)} Every morphism from a representation to an irreducible
representation is either $0$ or surjective.
\par
Both of these variations follow very easily from the definition of
"irreducible".} However, the more often used form of Schur's lemma is a bit
different: It claims that, over an algebraically closed field, every
endomorphism of a finite-dimensional irreducible representation is a scalar
multiple of the identity map. This is usually proven using eigenvalues, and
this proof depends on the fact that eigenvalues exist; this (in general)
requires the irreducible representation to be \textit{finite-dimensional}.
Hence, it should not come as a surprise that this latter form of Schur's lemma
does not generally hold for infinite-dimensional representations. This makes
this lemma not particularly useful in the case of infinite-dimensional Lie
algebras. But we still can show the following version of Schur's lemma over
$\mathbb{C}$:

\begin{lemma}
[Dixmier's Lemma]\label{lem.dix}Let $A$ be an algebra over $\mathbb{C}$, and
let $V$ be an irreducible $A$-module of countable dimension. Then, any
$A$-module homomorphism $\phi:V\rightarrow V$ is a scalar multiple of the identity.
\end{lemma}

\textit{Proof of Lemma \ref{lem.dix}.} Let $D=\operatorname*{End}%
\nolimits_{A}V$. Then, $D$ is a division algebra (in fact, the endomorphism
ring of an irreducible representation always is a division algebra).

For any nonzero $v\in V$, we have $Av=V$ (otherwise, $Av$ would be a nonzero
proper $A$-submodule of $V$, contradicting the fact that $V$ is irreducible
and thus does not have any such submodules). In other words, for any nonzero
$v\in V$, every element of $V$ can be written as $av$ for some $a\in A$. Thus,
for any nonzero $v\in V$, any element $\phi\in D$ is completely determined by
$\phi\left(  v\right)  $ (because $\phi\left(  av\right)  =a\phi\left(
v\right)  $ for every $a\in A$, so that the value $\phi\left(  v\right)  $
uniquely determines the value of $\phi\left(  av\right)  $ for every $a\in A$,
and thus (since we know that every element of $V$ can be written as $av$ for
some $a\in A$) every value of $\phi$ is uniquely determined). Thus, we have an
embedding of $D$ into $V$. Hence, $D$ is countably-dimensional (since $V$ is
countably-dimensional). But a countably-dimensional division algebra $D$ over
$\mathbb{C}$ must be $\mathbb{C}$ itself\footnote{\textit{Proof.} Indeed,
assume the contrary. So there exists some $\phi\in D$ not belonging to
$\mathbb{C}$. Then, $\phi$ is transcendental over $\mathbb{C}$, so that
$\mathbb{C}\left(  \phi\right)  \subseteq D$ is the field of rational
functions in one variable $\phi$ over $\mathbb{C}$. Now, $\mathbb{C}\left(
\phi\right)  $ contains the rational function $\dfrac{1}{\phi-\lambda}$ for
every $\lambda\in\mathbb{C}$, and these rational functions for varying
$\lambda$ are linearly independent. Since $\mathbb{C}$ is uncountable, we thus
have an uncountable linearly independent set of elements of $\mathbb{C}\left(
\phi\right)  $, contradicting the fact that $\mathbb{C}\left(  \phi\right)  $
is a subspace of the countably-dimensional space $D$, qed.}, so that
$D=\mathbb{C}$, and this is exactly what we wanted to show. Lemma
\ref{lem.dix} is proven.

Note that Lemma \ref{lem.dix} is a general fact, not particular to Lie
algebras; however, it is not as general as it seems: It really makes use of
the uncountability of $\mathbb{C}$, not just of the fact that $\mathbb{C}$ is
an algebraically closed field of characteristic $0$. It would be wrong if we
would replace $\mathbb{C}$ by (for instance) the algebraic closure of
$\mathbb{Q}$.

\begin{remark}
\label{rem.dix}Let $A$ be a countably-dimensional algebra over $\mathbb{C}$,
and let $V$ be an irreducible $A$-module. Then, $V$ itself is countably dimensional.
\end{remark}

\textit{Proof of Remark \ref{rem.dix}.} For any nonzero $v\in V$, we have
$Av=V$ (by the same argument as in the proof of Lemma \ref{lem.dix}), and thus
$\dim\left(  Av\right)  =\dim V$. Since $\dim\left(  Av\right)  \leq\dim A$,
we thus have $\dim V=\dim\left(  Av\right)  \leq\dim A$, so that $V$ has
countable dimension (since $A$ has countable dimension). This proves Remark
\ref{rem.dix}.

\begin{corollary}
\label{cor.dix2}Let $A$ be an algebra over $\mathbb{C}$, and let $V$ be an
irreducible $A$-module of countable dimension. Let $C$ be a central element of
$A$. Then, $C\mid_{V}$ is a scalar (i. e., a scalar multiple of the identity map).
\end{corollary}

\textit{Proof of Corollary \ref{cor.dix2}.} Since $C$ is central, the element
$C$ commutes with any element of $A$. Thus, $C\mid_{V}$ is an $A$-module
homomorphism, and hence (by Lemma \ref{lem.dix}, applied to $\phi=C\mid_{V}$)
a scalar multiple of the identity. This proves Corollary \ref{cor.dix2}.

\subsection{Representations of the Heisenberg algebra $\mathcal{A}$}

Consider the oscillator algebra (aka Heisenberg algebra) $\mathcal{A}%
=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle +\left\langle
K\right\rangle $. Recall that%
\begin{align*}
\left[  a_{i},a_{j}\right]   &  =i\delta_{i,-j}K\ \ \ \ \ \ \ \ \ \ \text{for
any }i,j\in\mathbb{Z};\\
\left[  K,a_{i}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for any }%
i\in\mathbb{Z}.
\end{align*}


Let us try to classify the irreducible $\mathcal{A}$-modules.

Let $V$ be an irreducible $\mathcal{A}$-module. Then, $V$ is
countably-dimensional (by Remark \ref{rem.dix}, since $U\left(  \mathcal{A}%
\right)  $ is countably-dimensional), so that by Corollary \ref{cor.dix2}, the
endomorphism $K\mid_{V}$ is a scalar (because $K$ is a central element of
$\mathcal{A}$ and thus also a central element of $U\left(  \mathcal{A}\right)
$).

If $K\mid_{V}=0$, then $V$ is a module over the Lie algebra $\mathcal{A}%
\diagup\mathbb{C}K=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle $.
But since $\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle $ is an
abelian Lie algebra, irreducible modules over $\left\langle a_{i}\ \mid
\ i\in\mathbb{Z}\right\rangle $ are $1$-dimensional (again by Corollary
\ref{cor.dix2}), so that $V$ must be $1$-dimensional in this case. Thus, the
case when $K\mid_{V}=0$ is not an interesting case.

Now consider the case when $K\mid_{V}=k\neq0$. Then, we can WLOG assume that
$k=1$, because the Lie algebra $\mathcal{A}$ has an automorphism sending $K$
to $\lambda K$ for any arbitrary $\lambda\neq0$ (this automorphism is given by
$a_{i}\mapsto\lambda a_{i}$ for $i>0$, and $a_{i}\mapsto a_{i}$ for $i\leq0$).

We are thus interested in irreducible representations $V$ of $\mathcal{A}$
satisfying $K\mid_{V}=1$. These are in an obvious 1-to-1 correspondence with
irreducible representations of $U\left(  \mathcal{A}\right)  \diagup\left(
K-1\right)  $.

\begin{proposition}
\label{prop.K-1}We have an algebra isomorphism%
\[
\xi:U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  \rightarrow
D\left(  x_{1},x_{2},x_{3},...\right)  \otimes\mathbb{C}\left[  x_{0}\right]
,
\]
where $D\left(  x_{1},x_{2},x_{3},...\right)  $ is the algebra of differential
operators in the variables $x_{1}$, $x_{2}$, $x_{3}$, $...$ with polynomial
coefficients. This isomorphism is given by%
\begin{align*}
\xi\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{0}\right)   &  =x_{0}.
\end{align*}

\end{proposition}

Note that we are sloppy with notation here: Since $\xi$ is a homomorphism from
$U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  $ (rather than
$U\left(  \mathcal{A}\right)  $), we should write $\xi\left(  \overline
{a_{-i}}\right)  $ instead of $\xi\left(  a_{-i}\right)  $, etc.. We are using
the same letters to denote elements of $U\left(  \mathcal{A}\right)  $ and
their residue classes in $U\left(  \mathcal{A}\right)  \diagup\left(
K-1\right)  $, and are relying on context to keep them apart. We hope that the
reader will forgive us this abuse of notation.

\textit{Proof of Proposition \ref{prop.K-1}.} It is clear\footnote{from the
universal property of the universal enveloping algebra, and the universal
property of the quotient algebra} that there exists a unique algebra
homomorphism $\xi:U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)
\rightarrow D\left(  x_{1},x_{2},x_{3},...\right)  $ satisfying%
\begin{align*}
\xi\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{0}\right)   &  =x_{0}.
\end{align*}
It is also clear that this $\xi$ is surjective (since all the generators
$x_{i}$, $\dfrac{\partial}{\partial x_{i}}$ and $x_{0}$ of $D\left(
x_{1},x_{2},x_{3},...\right)  \otimes\mathbb{C}\left[  x_{0}\right]  $ are in
its image).

In the following, a map $\varphi:A\rightarrow\mathbb{N}$ (where $A$ is some
set) is said to be \textit{finitely supported} if all but finitely many $a\in
A$ satisfy $\varphi\left(  a\right)  =0$. Sequences (finite, infinite, or
two-sided infinite) are considered as maps (from finite sets, $\mathbb{N}$ or
$\mathbb{Z}$, or occasionally other sets). Thus, a sequence is finitely
supported if and only if all but finitely many of its elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

By the easy part of the Poincar\'{e}-Birkhoff-Witt theorem (this is the part
which states that the increasing monomials \textit{span} the universal
enveloping algebra\footnote{The hard part says that these increasing monomials
are linearly independent.}), the family\footnote{Here, $\overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}$ denotes the product
$...a_{-2}^{n_{-2}}a_{-1}^{n_{-1}}a_{0}^{n_{0}}a_{1}^{n_{1}}a_{2}^{n_{2}}...$.
(This product is infinite, but still has a value since only finitely many
$n_{i}$ are nonzero.)}%
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}\cdot
K^{m}\right)  _{\left(  ...,n_{-2},n_{-1},n_{0},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}},\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}\right)  $. Hence,
the family
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}%
}\right)  _{\left(  ...,n_{-2},n_{-1},n_{0},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}}}%
\]
is a spanning set of $U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)
$, and since this family maps to a linearly independent set under $\xi$ (this
is very easy to see), it follows that $\xi$ is injective. Thus, $\xi$ is an
isomorphism, so that Proposition \ref{prop.K-1} is proven.

\begin{definition}
\label{def.A0}Define a vector subspace $\mathcal{A}_{0}$ of $\mathcal{A}$ by
$\mathcal{A}_{0}=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\diagdown\left\{
0\right\}  \right\rangle +\left\langle K\right\rangle $.
\end{definition}

\begin{proposition}
\label{prop.A0}This subspace $\mathcal{A}_{0}$ is a Lie subalgebra of
$\mathcal{A}$, and $\mathbb{C}a_{0}$ is also a Lie subalgebra of $\mathcal{A}%
$. We have $\mathcal{A}=\mathcal{A}_{0}\oplus\mathbb{C}a_{0}$ as Lie algebras.
Hence,%
\[
U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  =U\left(
\mathcal{A}_{0}\oplus\mathbb{C}a_{0}\right)  \diagup\left(  K-1\right)
\cong\underbrace{\left(  U\left(  \mathcal{A}_{0}\right)  \diagup\left(
K-1\right)  \right)  }_{\cong D\left(  x_{1},x_{2},x_{3},...\right)  }%
\otimes\underbrace{\mathbb{C}\left[  a_{0}\right]  }_{\cong\mathbb{C}\left[
x_{0}\right]  }%
\]
(since $K\in\mathcal{A}_{0}$). Here, the isomorphism $U\left(  \mathcal{A}%
_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(  x_{1},x_{2}%
,x_{3},...\right)  $ is defined as follows: In analogy to Proposition
\ref{prop.K-1}, we have an algebra isomorphism%
\[
\widetilde{\xi}:U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)
\rightarrow D\left(  x_{1},x_{2},x_{3},...\right)
\]
given by%
\begin{align*}
\widetilde{\xi}\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for
}i\geq1;\\
\widetilde{\xi}\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}%
}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1.
\end{align*}

\end{proposition}

The proof of Proposition \ref{prop.A0} is analogous to that of Proposition
\ref{prop.K-1} (where it is not completely straightforward).

\begin{corollary}
\label{cor.fock}The Lie algebra $\mathcal{A}_{0}$ has a representation
$F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ which is given by
\begin{align*}
a_{-i}  &  \mapsto x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where "$a_{i}\mapsto x_{-i}$" is just shorthand for "$a_{i}\mapsto\left(
\text{multiplication by }x_{i}\right)  $"). This is called the \textit{Fock
representation}. For every $\mu\in\mathbb{C}$, we can upgrade $F$ to a
representation $F_{\mu}$ of $\mathcal{A}$ by adding the condition that
$a_{0}\mid_{F_{\mu}}=\mu\cdot\operatorname*{id}$.
\end{corollary}

\begin{definition}
\label{def.fock}The representation $F$ of $\mathcal{A}_{0}$ introduced in
Corollary \ref{cor.fock} is called the \textit{Fock module} or the
\textit{Fock representation}. For every $\mu\in\mathbb{C}$, the representation
$F_{\mu}$ of $\mathcal{A}$ introduced in Corollary \ref{cor.fock} will be
called the $\mu$\textit{-Fock representation} of $\mathcal{A}$. The vector
space $F$ itself is called the \textit{Fock space}.
\end{definition}

Let us now define some gradings to make these infinite-dimensional spaces more manageable:

First of all, let us grade $\mathcal{A}$ by $\mathcal{A}=\bigoplus
\limits_{n\in\mathbb{Z}}\mathcal{A}\left[  n\right]  $, where $\mathcal{A}%
\left[  n\right]  =\left\langle a_{n}\right\rangle $ for $n\neq0$, and where
$\mathcal{A}\left[  0\right]  =\left\langle a_{0},K\right\rangle $.
\ \ \ \footnote{Note that we are denoting the $n$-th graded component of
$\mathcal{A}$ by $\mathcal{A}\left[  n\right]  $ rather than $\mathcal{A}_{n}%
$, since otherwise the notation $\mathcal{A}_{0}$ would have two different
meanings.}\ This is a grading of a Lie algebra, i. e., we have $\left[
\mathcal{A}\left[  n\right]  ,\mathcal{A}\left[  m\right]  \right]
\subseteq\mathcal{A}\left[  n+m\right]  $ for all $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$.

We also grade the polynomial algebra $F$ by setting $\deg\left(  x_{i}\right)
=-i$ for each $i$. Thus, $F=\bigoplus\limits_{n\geq0}F\left[  -n\right]  $,
where $F\left[  -n\right]  $ is the space of polynomials of degree $-n$, where
the degree is our degree defined by $\deg\left(  x_{i}\right)  =-i$ (so that,
for instance, $x_{1}^{2}+x_{2}$ is homogeneous of degree $-2$). With this
grading, $\dim\left(  F\left[  -n\right]  \right)  $ is the number $p\left(
n\right)  $ of all partitions of $n$. Hence,%
\[
\sum\limits_{n\geq0}\dim\left(  F\left[  -n\right]  \right)  q^{n}%
=\sum\limits_{n\geq0}p\left(  n\right)  q^{n}=\dfrac{1}{\left(  1-q\right)
\left(  1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}=\dfrac{1}%
{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }%
\]
in the ring of power series $\mathbb{Z}\left[  \left[  q\right]  \right]  $.

The action of $\mathcal{A}$ on $F_{\mu}$ is now graded, i. e., it maps
$\mathcal{A}\left[  n\right]  \otimes F_{\mu}\left[  m\right]  $ to $F_{\mu
}\left[  n+m\right]  $. This makes $F_{\mu}$ into a $\mathbb{Z}$-graded
$\mathcal{A}$-module.

\begin{Convention}
\label{conv.fockgrad}It is usual (for reasons to be explained later) to grade
$F_{\mu}$ somewhat differently from $F$: namely, it is usual to shift the
grading for $F_{\mu}$ by $\dfrac{\mu^{2}}{2}$, so that $\deg1=-\dfrac{\mu^{2}%
}{2}$ in $F_{\mu}$, and generally $F_{\mu}\left[  z\right]  =F\left[
\dfrac{\mu^{2}}{2}+z\right]  $ (as vector spaces) for every $z\in\mathbb{C}$.
Of course, with this new grading, $F_{\mu}$ still is a graded $\mathcal{A}%
$-module, although this is a grading by complex numbers rather than integers
(in general). (The advantage of this grading is that we will eventually find
an operator whose eigenspace to the eigenvalue $n$ is $F_{\mu}\left[
n\right]  =F\left[  \dfrac{\mu^{2}}{2}+n\right]  $ for every $n\in\mathbb{C}$.)

With this grading, the equality $\sum\limits_{n\geq0}\dim\left(  F\left[
-n\right]  \right)  q^{n}=\dfrac{1}{\prod\limits_{i\geq1}\left(
1-q^{i}\right)  }$ rewrites as $\sum\limits_{n\in\mathbb{C}}\dim\left(
F_{\mu}\left[  -n\right]  \right)  q^{n+\dfrac{\mu^{2}}{2}}=\dfrac{q^{\mu^{2}%
}}{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }$, if we allow power series
with complex exponents. We define a "power series" $\operatorname*{ch}\left(
F_{\mu}\right)  $ by%
\[
\operatorname*{ch}\left(  F_{\mu}\right)  =\sum\limits_{n\in\mathbb{C}}%
\dim\left(  F_{\mu}\left[  -n\right]  \right)  q^{n+\dfrac{\mu^{2}}{2}}%
=\dfrac{q^{\mu^{2}}}{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }.
\]

\end{Convention}

\begin{proposition}
\label{prop.F.irrep}The representation $F$ is an irreducible representation of
$\mathcal{A}_{0}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.F.irrep}.} \textbf{1)} The
representation $F$ is generated by $1$ as a $U\left(  \mathcal{A}_{0}\right)
$-module.\footnote{This is clear due to $\xi\left(  a_{-i}\right)  =x_{i}$ for
every $i\geq1$.} In other words, $F=U\left(  \mathcal{A}_{0}\right)  \cdot1$.

\textbf{2)} If $P\in F$ and $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}%
...x_{k}^{m_{k}}$ is a monomial in $P$ of degree $\deg P$, with $\alpha\neq0$,
then $\dfrac{1}{\alpha}\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac
{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}%
!}P=1$\ \ \ \ \footnote{\textit{Proof.} When we apply the differential
operator $\dfrac{1}{\alpha}\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}%
\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}%
}{m_{k}!}$ to $P$, all monomials $\beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}%
}...x_{k}^{n_{k}}$ with at least one $n_{\ell}$ being smaller than the
corresponding $m_{\ell}$ are annihilated (because if $n_{\ell}<m_{\ell}$ for
some $\ell$, then $\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac
{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}%
!}\left(  \beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}\right)  =0$).
Hence, the only monomials in $P$ which survive under this operator are
monomials of the form $\beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}$
with each $n_{\ell}$ being $\geq$ to the corresponding $m_{\ell}$. But since
$m_{1}+m_{2}+...+m_{k}=\deg P$ (because $\alpha\cdot x_{1}^{m_{1}}x_{2}%
^{m_{2}}...x_{k}^{m_{k}}$ is a monomial of degree $\deg P$), the only such
monomial in $P$ is $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$
(because for every other monomial of the form $\beta\cdot x_{1}^{n_{1}}%
x_{2}^{n_{2}}...x_{k}^{n_{k}}$ with each $n_{\ell}$ being $\geq$ to the
corresponding $m_{\ell}$, the sum $n_{1}+n_{2}+...+n_{k}$ must be greater than
$m_{1}+m_{2}+...+m_{k}=\deg P$, and thus such a monomial cannot occur in $P$).
Hence, the only monomial in $P$ which survives is the monomial $\alpha\cdot
x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$. This monomial clearly gets mapped
to $1$ by the differential operator $\dfrac{1}{\alpha}\dfrac{\partial_{x_{1}%
}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac
{\partial_{x_{k}}^{m_{k}}}{m_{k}!}$. Thus, $\dfrac{1}{\alpha}\dfrac
{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}%
!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}!}P=1$, qed.} and thus $1\in
U\left(  \mathcal{A}_{0}\right)  \cdot P$. Combined with \textbf{1)}, this
yields that for every nonzero $P\in F$, the representation $F$ is generated by
$P$ as a $U\left(  \mathcal{A}_{0}\right)  $-module (since $F=U\left(
\mathcal{A}_{0}\right)  \cdot\underbrace{1}_{\in U\left(  \mathcal{A}%
_{0}\right)  \cdot P}\subseteq U\left(  \mathcal{A}_{0}\right)  \cdot U\left(
\mathcal{A}_{0}\right)  \cdot P=U\left(  \mathcal{A}_{0}\right)  \cdot P$).
Consequently, $F$ is irreducible. Proposition \ref{prop.F.irrep} is proven.

\begin{proposition}
\label{prop.V=F}Let $V$ be an irreducible $\mathcal{A}_{0}$-module on which
$K$ acts as $1$. Assume that for any $v\in V$, the space $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v$ is finite-dimensional, and the $a_{i}$
with $i>0$ act on it by nilpotent operators. Then, $V\cong F$ as
$\mathcal{A}_{0}$-modules.
\end{proposition}

Before we prove this, a simple lemma:

\begin{lemma}
\label{lem.V=F}Let $V$ be an $\mathcal{A}_{0}$-module. Let $u\in V$ be such
that $a_{i}u=0$ for all $i>0$, and such that $Ku=u$. Then, there exists a
homomorphism $\eta:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =u$. (This homomorphism $\eta$ is unique, although we
won't need this.)
\end{lemma}

We give two proofs of this lemma. The first one is conceptual and gives us a
glimpse into the more general theory (it proceeds by constructing an
$\mathcal{A}_{0}$-module $\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$, which is an example
of what we will later call a Verma highest-weight module in Definition
\ref{def.verma}). The second one is down-to-earth and proceeds by direct
construction and computation.

\textit{First proof of Lemma \ref{lem.V=F}.} Define a vector subspace
$\mathcal{A}_{0}^{+}$ of $\mathcal{A}_{0}$ by $\mathcal{A}_{0}^{+}%
=\left\langle a_{i}\ \mid\ i\text{ positive integer}\right\rangle $. It is
clear that the direct sum $\mathbb{C}K\oplus\mathcal{A}_{0}^{+}$ is
well-defined and an abelian Lie subalgebra of $\mathcal{A}_{0}$. We can make
$\mathbb{C}$ into an $\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
$-module by setting%
\begin{align*}
K\lambda &  =\lambda\ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in
\mathbb{C};\\
a_{i}\lambda &  =0\ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in
\mathbb{C}\text{ and every positive integer }i.
\end{align*}
Now, consider the $\mathcal{A}_{0}$-module $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}=U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }\mathbb{C}$. Denote the element
$1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\in
U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }\mathbb{C}$ of this module by $1$.

We will now show the following important property of this module:
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{For any }\mathcal{A}_{0}\text{-module }T\text{, and any }t\in T\text{
satisfying }\left(  a_{i}t=0\text{ for all }i>0\right)  \text{ and
}Kt=t\text{,}\\
\text{there exists a homomorphism }\overline{\eta}_{T,t}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow T\text{ of }\mathcal{A}_{0}\text{-modules such that
}\overline{\eta}_{T,t}\left(  1\right)  =t
\end{array}
\right)  . \label{lem.V=F.pf.A1}%
\end{equation}
Once this is proven, we will (by considering $\overline{\eta}_{F,1}$) show
that $\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}}\mathbb{C}\cong F$, so this property will translate into the
assertion of Lemma \ref{lem.V=F}.

\textit{Proof of (\ref{lem.V=F.pf.A1}).} Let $\tau:\mathbb{C}\rightarrow T$ be
the map which sends every $\lambda\in\mathbb{C}$ to $\lambda t\in T$. Then,
$\tau$ is $\mathbb{C}$-linear and satisfies%
\[
\tau\underbrace{\left(  K\lambda\right)  }_{=\lambda}=\tau\left(
\lambda\right)  =\lambda\underbrace{t}_{=Kt}=\lambda\cdot Kt=K\cdot
\underbrace{\lambda t}_{=\tau\left(  \lambda\right)  }=K\cdot\tau\left(
\lambda\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in\mathbb{C}%
\]
and%
\begin{align*}
\tau\underbrace{\left(  a_{i}\lambda\right)  }_{=0}  &  =\tau\left(  0\right)
=0=\lambda\cdot\underbrace{0}_{=a_{i}t}=\lambda\cdot a_{i}t=a_{i}%
\cdot\underbrace{\lambda t}_{=\tau\left(  \lambda\right)  }=a_{i}\tau\left(
\lambda\right) \\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in\mathbb{C}\text{ and every
positive integer }i\text{.}%
\end{align*}
Thus, $\tau$ is a $\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
$-module map. In other words, $\tau\in\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  $.

By Frobenius reciprocity, we have%
\[
\operatorname*{Hom}\nolimits_{\mathcal{A}_{0}}\left(  \operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C},T\right)  \cong\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  .
\]
The preimage of $\tau\in\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  $
under this isomorphism is an $\mathcal{A}_{0}$-module map $\overline{\eta
}_{T,t}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow T$ such that
\begin{align*}
\overline{\eta}_{T,t}\underbrace{\left(  1\right)  }_{=1\otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1}  &  =\overline{\eta}%
_{T,t}\left(  1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
}1\right)  =1\underbrace{\tau\left(  1\right)  }_{=1t=t}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the proof of Frobenius reciprocity}%
\right) \\
&  =1t=t.
\end{align*}
Hence, there exists a homomorphism $\overline{\eta}_{T,t}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow T$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{T,t}\left(  1\right)  =t$. This proves (\ref{lem.V=F.pf.A1}).

It is easy to see that the element $1\in F$ satisfies $\left(  a_{i}1=0\text{
for all }i>0\right)  $ and $K1=1$. Thus, (\ref{lem.V=F.pf.A1}) (applied to
$T=F$ and $t=1$) yields that there exists a homomorphism $\overline{\eta
}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}$-modules such
that $\overline{\eta}_{F,1}\left(  1\right)  =1$. This homomorphism
$\overline{\eta}_{F,1}$ is clearly surjective, since
\begin{align*}
F  &  =U\left(  \mathcal{A}_{0}\right)  \cdot\underbrace{1}_{=\overline{\eta
}_{F,1}\left(  1\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{as proven in the
proof of Proposition \ref{prop.F.irrep}}\right) \\
&  =U\left(  \mathcal{A}_{0}\right)  \cdot\overline{\eta}_{F,1}\left(
1\right)  =\overline{\eta}_{F,1}\left(  U\left(  \mathcal{A}_{0}\right)
\cdot1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\overline{\eta}%
_{F,1}\text{ is an }\mathcal{A}_{0}\text{-module map}\right) \\
&  \subseteq\operatorname{Im}\overline{\eta}_{F,1}.
\end{align*}


Now we will prove that this homomorphism $\overline{\eta}_{F,1}$ is injective.

In the following, a map $\varphi:A\rightarrow\mathbb{N}$ (where $A$ is any
set) is said to be \textit{finitely supported} if all but finitely many $a\in
A$ satisfy $\varphi\left(  a\right)  =0$. Sequences (finite, infinite, or
two-sided infinite) are considered as maps (from finite sets, $\mathbb{N}$ or
$\mathbb{Z}$, or occasionally other sets). Thus, a sequence is finitely
supported if and only if all but finitely many of its elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

By the easy part of the Poincar\'{e}-Birkhoff-Witt theorem (this is the part
which states that the increasing monomials \textit{span} the universal
enveloping algebra), the family\footnote{Here, $\overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}$
denotes the product $...a_{-2}^{n_{-2}}a_{-1}^{n_{-1}}a_{1}^{n_{1}}%
a_{2}^{n_{2}}...$. (This product is infinite, but still has a value since only
finitely many $n_{i}$ are nonzero.)}%
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  _{\left(  ...,n_{-2}%
,n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  },\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}_{0}\right)  $.

Hence, the family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  \otimes
_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  },\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}_{0}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }%
\mathbb{C}=\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}$.

Let us first notice that this family is redundant: Each of its elements is
contained in the smaller family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }}.
\]
\footnote{This is because any sequence $\left(  ...,n_{-2},n_{-1},n_{1}%
,n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}%
\diagdown\left\{  0\right\}  }$ and any $m\in\mathbb{N}$ satisfy%
\begin{align*}
&  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown
\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }\underbrace{\left(  K^{m}1\right)
}_{\substack{=1\\\text{(by repeated application of }K1=1\text{)}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K^{m}\in U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  \right) \\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown
\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1.
\end{align*}
} Hence, this smaller family is also a spanning set of the vector space
$\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}$.

This smaller family is still redundant: Every of its elements corresponding to
a sequence $\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$
satisfying $n_{1}+n_{2}+n_{3}+...>0$ is zero\footnote{\textit{Proof.} Let
$\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$ be a
sequence satisfying $n_{1}+n_{2}+n_{3}+...>0$. Then, the sequence $\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  $ is finitely supported (as it is an
element of $\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{
0\right\}  }$), so that only finitely many $n_{i}$ are nonzero.
\par
There exists some positive integer $\ell$ satisfying $n_{\ell}>0$ (since
$n_{1}+n_{2}+n_{3}+...>0$). Let $j$ be the greatest such $\ell$ (this is
well-defined, since only finitely many $n_{i}$ are nonzero).
\par
Since $j$ is the greatest positive integer $\ell$ satisfying $n_{\ell}>0$, it
is clear that $j$ is the greatest integer $\ell$ satisfying $n_{\ell}>0$. In
other words, $a_{j}^{n_{j}}$ is the rightmost factor in the product
$\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}$ which is
not equal to $1$. Thus,%
\[
\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}
}}a_{i}^{n_{i}}=\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}%
\cdot\underbrace{a_{j}^{n_{j}}}_{\substack{=a_{j}^{n_{j}-1}a_{j}\\\text{(since
}n_{j}>0\text{)}}}=\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}\cdot
a_{j}^{n_{j}-1}a_{j},
\]
so that%
\begin{align*}
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1  &  =\left(  \overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}  \diagdown\left\{
j\right\}  }}a_{i}^{n_{i}}\cdot a_{j}^{n_{j}-1}a_{j}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\\
&  =\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}\cdot a_{j}^{n_{j}%
-1}\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
}\underbrace{a_{j}1}_{\substack{=0\\\text{(since }j>0\text{, so that}%
\\a_{j}1=j\dfrac{\partial}{\partial x_{j}}1=0\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\in U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  \right) \\
&  =0.
\end{align*}
We have thus proven that every sequence $\left(  ...,n_{-2},n_{-1},n_{1}%
,n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}%
\diagdown\left\{  0\right\}  }$ satisfying $n_{1}+n_{2}+n_{3}+...>0$ satisfies
$\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1=0$, qed.}, and zero elements in a spanning set
are automatically redundant. Hence, we can replace this smaller family by the
even smaller family%
\begin{align*}
&  \left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }\text{; we do \textit{not} have
}n_{1}+n_{2}+n_{3}+...>0}\\
&  =\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }\text{; }n_{1}=n_{2}=n_{3}=...=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the condition }\left(  \text{we do \textit{not} have }n_{1}%
+n_{2}+n_{3}+...>0\right) \\
\text{is equivalent to the condition }\left(  n_{1}=n_{2}=n_{3}=...=0\right)
\\
\text{(because }n_{i}\in\mathbb{N}\text{ for all }i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \text{)}%
\end{array}
\right)  ,
\end{align*}
and we still have a spanning set of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$.

Clearly, sequences $\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$
satisfying $n_{1}=n_{2}=n_{3}=...=0$ are in 1-to-1 correspondence with
sequences $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$. Hence, we can
reindex the above family as follows:
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}.
\]
So we have proven that the family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}%
\]
is a spanning set of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$.
But the map $\overline{\eta}_{F,1}$ sends this family to%
\begin{align*}
&  \left(  \overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right)  _{\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }}\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}%
\end{align*}
\footnote{\textit{Proof.} Let $\left(  ...,n_{-2},n_{-1}\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$ be
arbitrary. Then,%
\begin{align*}
&  \overline{\eta}_{F,1}\left(  \underbrace{\left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1}_{=\left(
\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}%
a_{i}^{n_{i}}\right)  \left(  1\otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1\right)  }\right) \\
&  =\overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \left(
1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right) \\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \overline{\eta}_{F,1}%
\underbrace{\left(  1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}%
^{+}\right)  }1\right)  }_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\overline{\eta}_{F,1}\text{ is an }\mathcal{A}_{0}\text{-module map}\right)
\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \underbrace{\overline{\eta
}_{F,1}\left(  1\right)  }_{=1}=\left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  1=\left(
\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
}x_{-i}^{n_{i}}\right)  1\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because each }a_{i}\text{ with negative
}i\text{ acts on }F\text{ by multiplication with }x_{-i}\right) \\
&  =\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
}x_{-i}^{n_{i}}=\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
x_{-i}^{n_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }F\text{ is
commutative}\right)  .
\end{align*}
Now forget that we fixed $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$. We thus have shown
that every $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$ satisfies
$\overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
=\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }x_{-i}^{n_{i}}$. Thus,%
\begin{align*}
&  \left(  \overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right)  _{\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }}\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }},
\end{align*}
qed.}. Since the family $\left(  \overset{\rightarrow}{\prod\limits_{i\in
\left\{  ...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(
...,n_{-2},n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}$ is a basis of the vector space $F$ (in fact, this
family consists of all monomials of the polynomial ring $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  =F$), we thus conclude that $\overline{\eta
}_{F,1}$ sends a spanning family of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$
to a basis of the vector space $F$. Thus, $\overline{\eta}_{F,1}$ must be
injective\footnote{Here we are using the following trivial fact from linear
algebra: If a linear map $\varphi:V\rightarrow W$ sends a spanning family of
the vector space $V$ to a basis of the vector space $W$ (as families, not just
as sets), then this map $\varphi$ must be injective.}.

Altogether, we now know that $\overline{\eta}_{F,1}$ is a surjective and
injective $\mathcal{A}_{0}$-module map. Thus, $\overline{\eta}_{F,1}$ is an
isomorphism of $\mathcal{A}_{0}$-modules.

Now, apply (\ref{lem.V=F.pf.A1}) to $T=V$ and $t=u$. This yields that there
exists a homomorphism $\overline{\eta}_{V,u}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{V,u}\left(  1\right)  =u$.

Now, the composition $\overline{\eta}_{V,u}\circ\overline{\eta}_{F,1}^{-1}$ is
a homomorphism $F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that
\[
\left(  \overline{\eta}_{V,u}\circ\overline{\eta}_{F,1}^{-1}\right)  \left(
1\right)  =\overline{\eta}_{V,u}\underbrace{\left(  \overline{\eta}_{F,1}%
^{-1}\left(  1\right)  \right)  }_{\substack{=1\\\text{(since }\overline{\eta
}_{F,1}\left(  1\right)  =1\text{)}}}=\overline{\eta}_{V,u}\left(  1\right)
=u.
\]
Thus, there exists a homomorphism $\eta:F\rightarrow V$ of $\mathcal{A}_{0}%
$-modules such that $\eta\left(  1\right)  =u$ (namely, $\eta=\overline{\eta
}_{V,u}\circ\overline{\eta}_{F,1}^{-1}$). This proves Lemma \ref{lem.V=F}.

\textit{Second proof of Lemma \ref{lem.V=F}.} Let $\eta$ be the map
$F\rightarrow V$ which sends every polynomial $P\in F=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ to $P\left(  a_{-1},a_{-2},a_{-3},...\right)
\cdot u\in V$.\ \ \ \ \footnote{Note that the term $P\left(  a_{-1}%
,a_{-2},a_{-3},...\right)  $ denotes the evaluation of the polynomial $P$ at
$\left(  x_{1},x_{2},x_{3},...\right)  =\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  $. This evaluation is a well-defined element of $U\left(
\mathcal{A}_{0}\right)  $, since the elements $a_{-1}$, $a_{-2}$, $a_{-3}$,
$...$ of $U\left(  \mathcal{A}_{0}\right)  $ commute.} This map $\eta$ is
clearly $\mathbb{C}$-linear, and satisfies $\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u$. In order to prove that $\eta$ is an
$\mathcal{A}_{0}$-module homomorphism, we must prove that
\begin{equation}
\eta\left(  a_{i}P\right)  =a_{i}\eta\left(  P\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{Z}\diagdown\left\{
0\right\}  \text{ and }P\in F \label{lem.V=F.pf.1}%
\end{equation}
and that%
\begin{equation}
\eta\left(  KP\right)  =K\eta\left(  P\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }P\in F. \label{lem.V=F.pf.2}%
\end{equation}


First we show that%
\begin{equation}
Kv=v\ \ \ \ \ \ \ \ \ \ \text{for every }v\in U\left(  \mathcal{A}_{0}\right)
\cdot u. \label{lem.V=F.pf.3}%
\end{equation}


\textit{Proof of (\ref{lem.V=F.pf.3}).} Since $K$ lies in the center of the
Lie algebra $\mathcal{A}_{0}$, it is clear that $K$ lies in the center of the
universal enveloping algebra $U\left(  \mathcal{A}_{0}\right)  $. Thus,
$Kx=xK$ for every $x\in U\left(  \mathcal{A}_{0}\right)  $.

Now let $v\in U\left(  \mathcal{A}_{0}\right)  \cdot u$. Then, there exists
some $x\in U\left(  \mathcal{A}_{0}\right)  $ such that $v=xu$. Thus,
$Kv=Kxu=x\underbrace{Ku}_{=u}=xu=v$. This proves (\ref{lem.V=F.pf.3}).

\textit{Proof of (\ref{lem.V=F.pf.2}).} Since $K$ acts as the identity on $F$,
we have $KP=P$ for every $P\in F$. Thus, for every $P\in F$, we have%
\[
\eta\left(  KP\right)  =\eta\left(  P\right)  =K\eta\left(  P\right)
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{lem.V=F.pf.3}) (applied to }v=\eta\left(  P\right)  \text{)
yields }K\eta\left(  P\right)  =\eta\left(  P\right) \\
\text{(because }\eta\left(  P\right)  \in\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u\text{)}%
\end{array}
\right)  .
\]
This proves (\ref{lem.V=F.pf.2}).

\textit{Proof of (\ref{lem.V=F.pf.1}).} Let $i\in\mathbb{Z}\diagdown\left\{
0\right\}  $. If $i<0$, then (\ref{lem.V=F.pf.1}) is pretty much obvious
(because in this case, $a_{i}$ acts as $x_{-i}$ on $F$, so that $a_{i}%
P=x_{-i}P$ and thus%
\[
\eta\left(  a_{i}P\right)  =\eta\left(  x_{-i}P\right)  =\left(
x_{-i}P\right)  \left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u=a_{i}%
\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}_{=\eta\left(
P\right)  }=a_{i}\eta\left(  P\right)
\]
for every $P\in F$). Hence, from now on, we can WLOG assume that $i$ is not
$<0$. Assume this. Then, $i\geq0$, so that $i>0$ (since $i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  $).

In order to prove the equality (\ref{lem.V=F.pf.1}) for all $P\in F$, it is
enough to prove it for the case when $P$ is a monomial of the form
$x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}$ for some $m\in\mathbb{N}$ and some
$\left(  \ell_{1},\ell_{2},...,\ell_{m}\right)  \in\left\{  1,2,3,...\right\}
^{m}$.\ \ \ \ \footnote{This is because such monomials generate $F$ as a
$\mathbb{C}$-vector space, and because the equality (\ref{lem.V=F.pf.1}) is
linear in $P$.} In other words, in order to prove the equality
(\ref{lem.V=F.pf.1}), it is enough to prove that%
\begin{equation}
\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}\right)
\right)  =a_{i}\eta\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}\text{ and every }\left(
\ell_{1},\ell_{2},...,\ell_{m}\right)  \in\left\{  1,2,3,...\right\}  ^{m}.
\label{lem.V=F.pf.4}%
\end{equation}


Thus, let us now prove (\ref{lem.V=F.pf.4}). In fact, we are going to prove
(\ref{lem.V=F.pf.4}) by induction over $m$. The induction base is very easy
(using $a_{i}1=i\dfrac{\partial}{\partial x_{i}}1=0$ and $a_{i}u=0$) and thus
left to the reader. For the induction step, fix some positive $M\in\mathbb{N}%
$, and assume that (\ref{lem.V=F.pf.4}) is already proven for $m=M-1$. Our
task is now to prove (\ref{lem.V=F.pf.4}) for $m=M$.

So let $\left(  \ell_{1},\ell_{2},...,\ell_{M}\right)  \in\left\{
1,2,3,...\right\}  ^{M}$ be arbitrary. Denote by $Q$ the polynomial
$x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}}$. Then, $x_{\ell_{1}}Q=x_{\ell_{1}%
}x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}}=x_{\ell_{1}}x_{\ell_{2}}%
...x_{\ell_{M}}$.

Since (\ref{lem.V=F.pf.4}) is already proven for $m=M-1$, we can apply
(\ref{lem.V=F.pf.4}) to $M-1$ and $\left(  \ell_{2},\ell_{3},...,\ell
_{M}\right)  $ instead of $m$ and $\left(  \ell_{1},\ell_{2},...,\ell
_{m}\right)  $. We obtain $\eta\left(  a_{i}\left(  x_{\ell_{2}}x_{\ell_{3}%
}...x_{\ell_{M}}\right)  \right)  =a_{i}\eta$ $\left(  x_{\ell_{2}}x_{\ell
_{3}}...x_{\ell_{M}}\right)  $. Since $x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}%
}=Q$, this rewrites as $\eta\left(  a_{i}Q\right)  =a_{i}\eta\left(  Q\right)
$.

Since any $x,y\in U\left(  \mathcal{A}_{0}\right)  $ satisfy $xy=yx+\left[
x,y\right]  $ (by the definition of $U\left(  \mathcal{A}_{0}\right)  $), we
have%
\[
a_{i}a_{-\ell_{1}}=a_{-\ell_{1}}a_{i}+\underbrace{\left[  a_{i},a_{-\ell_{1}%
}\right]  }_{=i\delta_{i,-\left(  -\ell_{1}\right)  }K}=a_{-\ell_{1}}%
a_{i}+i\underbrace{\delta_{i,-\left(  -\ell_{1}\right)  }}_{=\delta
_{i,\ell_{1}}}K=a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K.
\]


On the other hand, by the definition of $\eta$, every $P\in F$ satisfies the
two equalities $\eta\left(  P\right)  =P\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  \cdot u$ and%
\begin{align}
\eta\left(  x_{\ell_{1}}P\right)   &  =\underbrace{\left(  x_{\ell_{1}%
}P\right)  \left(  a_{-1},a_{-2},a_{-3},...\right)  }_{=a_{-\ell_{1}}\cdot
P\left(  a_{-1},a_{-2},a_{-3},...\right)  }\cdot u=a_{-\ell_{1}}%
\cdot\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}%
_{=\eta\left(  P\right)  }\nonumber\\
&  =a_{-\ell_{1}}\cdot\eta\left(  P\right)  . \label{lem.V=F.pf.5}%
\end{align}


Since $a_{i}$ acts on $F$ as $i\dfrac{\partial}{\partial x_{i}}$, we have
$a_{i}\left(  x_{\ell_{1}}Q\right)  =i\dfrac{\partial}{\partial x_{i}}\left(
x_{\ell_{1}}Q\right)  $ and $a_{i}Q=i\dfrac{\partial}{\partial x_{i}}Q$. Now,%
\begin{align*}
a_{i}\left(  \underbrace{x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}}%
_{=x_{\ell_{1}}Q}\right)   &  =a_{i}\left(  x_{\ell_{1}}Q\right)
=i\dfrac{\partial}{\partial x_{i}}\left(  x_{\ell_{1}}Q\right)  =i\left(
\left(  \dfrac{\partial}{\partial x_{i}}x_{\ell_{1}}\right)  Q+x_{\ell_{1}%
}\left(  \dfrac{\partial}{\partial x_{i}}Q\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the Leibniz rule}\right) \\
&  =i\underbrace{\left(  \dfrac{\partial}{\partial x_{i}}x_{\ell_{1}}\right)
}_{=\delta_{i,\ell_{1}}}Q+x_{\ell_{1}}\cdot\underbrace{i\dfrac{\partial
}{\partial x_{i}}Q}_{=a_{i}Q}=i\delta_{i,\ell_{1}}Q+x_{\ell_{1}}\cdot
a_{i}Q=x_{\ell_{1}}\cdot a_{i}Q+i\delta_{i,\ell_{1}}Q,
\end{align*}
so that%
\begin{align*}
\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}\right)
\right)   &  =\eta\left(  x_{\ell_{1}}\cdot a_{i}Q+i\delta_{i,\ell_{1}%
}Q\right)  =\underbrace{\eta\left(  x_{\ell_{1}}\cdot a_{i}Q\right)
}_{\substack{=a_{-\ell_{1}}\cdot\eta\left(  a_{i}Q\right)  \\\text{(by
(\ref{lem.V=F.pf.5}), applied to }P=a_{i}Q\text{)}}}+i\delta_{i,\ell_{1}}%
\eta\left(  Q\right) \\
&  =a_{-\ell_{1}}\cdot\underbrace{\eta\left(  a_{i}Q\right)  }_{=a_{i}%
\eta\left(  Q\right)  }+i\delta_{i,\ell_{1}}\eta\left(  Q\right)
=a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}}%
\eta\left(  Q\right)  .
\end{align*}
Compared to%
\begin{align*}
a_{i}\eta\left(  \underbrace{x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}%
}_{=x_{\ell_{1}}Q}\right)   &  =a_{i}\underbrace{\eta\left(  x_{\ell_{1}%
}Q\right)  }_{\substack{=a_{-\ell_{1}}\cdot\eta\left(  Q\right)  \\\text{(by
(\ref{lem.V=F.pf.5}), applied to }P=Q\text{)}}}=\underbrace{a_{i}a_{-\ell_{1}%
}}_{=a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K}\cdot\eta\left(  Q\right)
=\left(  a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K\right)  \cdot\eta\left(
Q\right) \\
&  =a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}%
}\underbrace{K\eta\left(  Q\right)  }_{\substack{=\eta\left(  Q\right)
\\\text{(by (\ref{lem.V=F.pf.3}), applied to }v=\eta\left(  Q\right)
\\\text{(since }\eta\left(  Q\right)  \in\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u\text{))}}}\\
&  =a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}}%
\eta\left(  Q\right)  ,
\end{align*}
this yields $\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}%
}\right)  \right)  =a_{i}\eta\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}%
}\right)  $. Since we have proven this for every $\left(  \ell_{1},\ell
_{2},...,\ell_{M}\right)  \in\left\{  1,2,3,...\right\}  ^{M}$, we have thus
proven (\ref{lem.V=F.pf.4}) for $m=M$. This completes the induction step, and
thus the induction proof of (\ref{lem.V=F.pf.4}) is complete. As we have seen
above, this proves (\ref{lem.V=F.pf.1}).

From (\ref{lem.V=F.pf.1}) and (\ref{lem.V=F.pf.2}), it is clear that $\eta$ is
$\mathcal{A}_{0}$-linear (since $\mathcal{A}_{0}$ is spanned by the $a_{i}$
for $i\in\mathbb{Z}\diagdown\left\{  0\right\}  $ and $K$). Since $\eta\left(
1\right)  =u$ is obvious, this proves Lemma \ref{lem.V=F}.

\textit{Proof of Proposition \ref{prop.V=F}.} Pick some nonzero vector $v\in
V$. Let $W=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \cdot v$. Then, by
the condition, we have $\dim W<\infty$, and $a_{i}:W\rightarrow W$ are
commuting nilpotent operators\footnote{Of course, when we write $a_{i}%
:W\rightarrow W$, we don't mean the elements $a_{i}$ of $\mathcal{A}_{0}$
themselves, but their actions on $W$.}. Hence, $\bigcap\limits_{i\geq
1}\operatorname*{Ker}a_{i}\neq0\ \ \ \ $\footnote{Here, we are using the
following linear-algebraic fact:
\par
If $T$ is a nonzero finite-dimensional vector space over an algebraically
closed field, and if $b_{1}$, $b_{2}$, $b_{3}$, $...$ are commuting linear
maps $T\rightarrow T$, then there exists a nonzero common eigenvector of
$b_{1}$, $b_{2}$, $b_{3}$, $...$. If $b_{1}$, $b_{2}$, $b_{3}$, $...$ are
nilpotent, this yields $\bigcap\limits_{i\geq1}\operatorname*{Ker}b_{i}\neq0$
(since any eigenvector of a nilpotent map must lie in its kernel).}. Hence,
there exists some nonzero $u\in\bigcap\limits_{i\geq1}\operatorname*{Ker}%
a_{i}$. Pick such a $u$. Then, $a_{i}u=0$ for all $i>0$, and $Ku=u$ (since $K$
acts as $1$ on $V$). Thus, there exists a homomorphism $\eta:F\rightarrow V$
of $\mathcal{A}_{0}$-modules such that $\eta\left(  1\right)  =u$ (by Lemma
\ref{lem.V=F}). Since both $F$ and $V$ are irreducible and $\eta\neq0$, this
yields that $\eta$ is an isomorphism. This proves Proposition \ref{prop.V=F}.

\begin{proposition}
\label{prop.V=F(X)U}Let $V$ be any $\mathcal{A}_{0}$-module having a locally
nilpotent action of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $. (Here,
we say that the $\mathcal{A}_{0}$-module $V$ has a \textit{locally nilpotent
action of$\mathbb{\ }$}$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ if
for any $v\in V$, the space $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]
\cdot v$ is finite-dimensional, and the $a_{i}$ with $i>0$ act on it by
nilpotent operators.) Assume that $K$ acts as $1$ on $V$. Assume that for
every $v\in V$, there exists some $N\in\mathbb{N}$ such that for every $n\geq
N$, we have $a_{n}v=0$. Then, $V\cong F\otimes U$ as $\mathcal{A}_{0}$-modules
for some vector space $U$. (The vector space $U$ is not supposed to carry any
$\mathcal{A}_{0}$-module structure.)
\end{proposition}

\begin{remark}
From Proposition \ref{prop.V=F(X)U}, we cannot remove the condition that for
every $v\in V$, there exists some $N\in\mathbb{N}$ such that for every $n\geq
N$, we have $a_{n}v=0$. In fact, here is a counterexample of how Proposition
\ref{prop.V=F(X)U} can fail without this condition:

Let $V$ be the representation $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]
\left[  y\right]  \diagup\left(  y^{2}\right)  $ of $\mathcal{A}_{0}$ given
by
\begin{align*}
a_{-i}  &  \mapsto x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto i\dfrac{\partial}{\partial x_{i}}%
+y\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where we are being sloppy and abbreviating the residue class $\overline{y}%
\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \left[  y\right]
\diagup\left(  y^{2}\right)  $ by $y$, and similarly all other residue
classes). We have an exact sequence%
\[%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%0 \ar[r] & F \ar[r]^i & V \ar[r]^{\pi} & F \ar[r] & 0
%}}}%
%BeginExpansion
\xymatrix{
0 \ar[r] & F \ar[r]^i & V \ar[r]^{\pi} & F \ar[r] & 0
}%
%EndExpansion
\]
of $\mathcal{A}_{0}$-modules, where the map $i:F\rightarrow V$ is given by%
\[
i\left(  P\right)  =yP\ \ \ \ \ \ \ \ \ \ \text{for every }p\in F=\mathbb{C}%
\left[  x_{1},x_{2},x_{3},...\right]  ,
\]
and the map $\pi:V\rightarrow F$ is the canonical projection $V\rightarrow
V\diagup\left(  y\right)  \cong F$. Thus, $V$ is an extension of $F$ by $F$.
It is easily seen that $V$ has a locally nilpotent action of$\ \mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  $. But $V$ is not isomorphic to
$F\otimes U$ as $\mathcal{A}_{0}$-modules for any vector space $U$, since
there is a vector $v\in V$ satisfying $V=U\left(  \mathcal{A}_{0}\right)
\cdot v$ (for example, $v=1$), whereas there is no vector $v\in F\otimes U$
satisfying $F\otimes U=U\left(  \mathcal{A}_{0}\right)  \cdot v$ if $\dim
U>1$, and the case $\dim U\leq1$ is easily ruled out (in this case, $\dim U$
would have to be $1$, so that $U$ would be $\cong F$ and thus irreducible, and
thus the homomorphisms $i$ and $\pi$ would have to be isomorphisms, which is absurd).
\end{remark}

\textit{Proof of Proposition \ref{prop.V=F(X)U}.} Let $v\in V$ be arbitrary.
Let $I_{v}\subseteq\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ be the
annihilator of $v$. Then, the canonical $\mathbb{C}$-algebra map
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \rightarrow
\operatorname*{End}\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]
\cdot v\right)  $ (this map comes from the action of the $\mathbb{C}$-algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ on $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v$) gives rise to an \textit{injective}
map $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}%
\rightarrow\operatorname*{End}\left(  \mathbb{C}\left[  a_{1},a_{2}%
,a_{3},...\right]  \cdot v\right)  $. Since this map is injective, we have
$\dim\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
I_{v}\right)  \leq\dim\left(  \operatorname*{End}\left(  \mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v\right)  \right)  <\infty$ (since
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \cdot v$ is
finite-dimensional). In other words, the vector space $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$ is finite-dimensional.

Let $W$ be the $\mathcal{A}_{0}$-submodule of $V$ generated by $v$. In other
words, let $W=U\left(  \mathcal{A}_{0}\right)  \cdot v$. Then, $W$ is a
quotient of $U\left(  \mathcal{A}_{0}\right)  $ (as an $\mathcal{A}_{0}%
$-module). Since $K$ acts as $1$ on $W$, it follows that $W$ is a quotient of
$U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(
x_{1},x_{2},x_{3},...\right)  $. Since $I_{v}$ annihilates $v$, it follows
that $W$ is a quotient of $D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  $. Let us denote the
$\mathcal{A}_{0}$-module $D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  $ by $\widetilde{W}$.

We now will prove that $\widetilde{W}$ is a finite-length $\mathcal{A}_{0}%
$-module with all composition factors isomorphic to $F$.\ \ \ \ \footnote{We
can even prove that there are exactly $\dim\left(  \mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \diagup I_{v}\right)  $ composition factors.}

Let $\mathfrak{i}$ be the ideal $\left(  a_{1},a_{2},a_{3},...\right)  $ of
the commutative algebra $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $.

Since $I_{v}$ is an ideal of the commutative algebra $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $, the quotient $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  \diagup I_{v}$ is an algebra. For every
$q\in\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $, let $\overline{q}$ be
the projection of $q$ onto the quotient algebra $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  \diagup I_{v}$. Let also $\overline{\mathfrak{i}}$ be
the projection of the ideal $\mathfrak{i}$ onto the quotient algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$. Clearly,
$\overline{\mathfrak{i}}=\left(  \overline{a_{1}},\overline{a_{2}}%
,\overline{a_{3}},...\right)  $.

For every $j>0$, there exists some $i\in\mathbb{N}$ such that $a_{j}^{i}v=0$
(since $V$ has a locally nilpotent action of $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  $). Hence, for every $j>0$, the element
$\overline{a_{j}}$ of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
I_{v}$ is nilpotent (because there exists some $i\in\mathbb{N}$ such that
$a_{j}^{i}v=0$, and thus this $i$ satisfies $a_{j}^{i}\in I_{v}$, so that
$\overline{a_{j}}^{i}=0$). Hence, the ideal $\overline{\mathfrak{i}}$ is
generated by nilpotent generators (since $\overline{\mathfrak{i}}=\left(
\overline{a_{1}},\overline{a_{2}},\overline{a_{3}},...\right)  $). Since we
also know that $\overline{\mathfrak{i}}$ is finitely generated (since
$\overline{\mathfrak{i}}$ is an ideal of the finite-dimensional algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$), it follows
that $\overline{\mathfrak{i}}$ is generated by \textit{finitely many}
nilpotent generators. But if an ideal of a commutative ring is generated by
finitely many nilpotent generators, it must be nilpotent. Thus, $\overline
{\mathfrak{i}}$ is nilpotent. In other words, there exists some $M\in
\mathbb{N}$ such that $\overline{\mathfrak{i}}^{M}=0$. Consider this $M$.
Since $\overline{\mathfrak{i}}^{M}=0$, we have $\mathfrak{i}^{M}\subseteq
I_{v}$. Now, consider the decreasing sequence $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  =\mathfrak{i}^{0}\supseteq\mathfrak{i}^{1}%
\supseteq...\supseteq\mathfrak{i}^{M}$ of subspaces of $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $. By adding $I_{v}$ to each subspace, we can
make it into a sequence
\begin{equation}
\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  =\mathfrak{i}^{0}%
+I_{v}\supseteq\mathfrak{i}^{1}+I_{v}\supseteq...\supseteq\mathfrak{i}%
^{M}+I_{v}=I_{v} \label{prop.V=F(X)U.pf.1}%
\end{equation}
(where $\mathfrak{i}^{M}+I_{v}=I_{v}$ is because $\mathfrak{i}^{M}\subseteq
I_{v}$). We can now refine this sequence to a complete flag of vector
subspaces of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ lying between
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ and $I_{v}$ (since
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$ is
finite-dimensional). Let this flag be%
\[
\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  =J_{0}\supseteq J_{1}%
\supseteq...\supseteq J_{N}=I_{v},
\]
with $\dim\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
J_{i}\right)  =i$ for every $i\in\left\{  0,1,...,N\right\}  $. Since this
flag is a refinement of the sequence (\ref{prop.V=F(X)U.pf.1}), there exists a
strictly increasing subsequence $\left(  j_{0},j_{1},...,j_{M}\right)  $ of
$\left(  0,1,...,N\right)  $ such that $J_{j_{k}}=\mathfrak{i}^{k}+I_{v}$ for
every $k\in\left\{  0,1,...,M\right\}  $. This quickly yields that%
\begin{equation}
\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  0,1,...,N-1\right\}  \label{prop.V=F(X)U.pf.2}%
\end{equation}
(since $\mathfrak{i}\cdot\left(  \mathfrak{i}^{\ell}+I_{v}\right)
\subseteq\mathfrak{i}^{\ell+1}+I_{v}$ for every $\ell\in\mathbb{N}$).

For every $i\in\left\{  0,1,...,N\right\}  $, let $D_{i}=D\left(  x_{1}%
,x_{2},...\right)  \cdot J_{i}$. Then,%
\[
D_{0}=D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{0}}_{=\mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  }=D\left(  x_{1},x_{2},...\right)
\]
and%
\[
D_{N}=D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{N}}_{=I_{v}%
}=D\left(  x_{1},x_{2},...\right)  \cdot I_{v}.
\]
Hence, $D_{0}\diagup D_{N}=D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  =\widetilde{W}$.

Now, we are going to prove that%
\begin{equation}
D_{i}\diagup D_{i+1}\cong F\text{ or }D_{i}\diagup D_{i+1}%
=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  0,1,...,N-1\right\}
\label{prop.V=F(X)U.pf.3}%
\end{equation}
(where $\cong$ means isomorphism of $\mathcal{A}_{0}$-modules).

\textit{Proof of (\ref{prop.V=F(X)U.pf.3}).} Let $i\in\left\{
0,1,...,N-1\right\}  $. Since $\dim\left(  \mathbb{C}\left[  a_{1},a_{2}%
,a_{3},...\right]  \diagup J_{i}\right)  =i$ and $\dim\left(  \mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  \diagup J_{i+1}\right)  =i+1$, there
exists some $u\in J_{i}$ such that $J_{i}=u+J_{i+1}$. Consider this $u$. By
abuse of notation, we also use the letter $u$ to denote the element $1\cdot
u\in D\left(  x_{1},x_{2},...\right)  \cdot J_{i}=D_{i}$. Then,
\begin{align*}
D_{i}  &  =D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{i}%
}_{=u+J_{i+1}}=D\left(  x_{1},x_{2},...\right)  \cdot\left(  u+J_{i+1}\right)
\\
&  =D\left(  x_{1},x_{2},...\right)  \cdot u+\underbrace{D\left(  x_{1}%
,x_{2},...\right)  \cdot J_{i+1}}_{=D_{i+1}}=D\left(  x_{1},x_{2},...\right)
\cdot u+D_{i+1}.
\end{align*}
Thus,
\[
D_{i}\diagup D_{i+1}=D\left(  x_{1},x_{2},...\right)  \cdot u^{\prime},
\]
where $u^{\prime}$ denotes the residue class of $u\in D_{i}$ modulo $D_{i+1}$.
For every $j>0$, we have $\underbrace{a_{j}}_{\in\mathfrak{i}}\underbrace{u}%
_{\in J_{i}}\in\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}$ (by
(\ref{prop.V=F(X)U.pf.2})) and thus $a_{j}u\in D\left(  x_{1},x_{2}%
,...\right)  \cdot J_{i+1}=D_{i+1}$. In other words, for every $j>0$, we have
$a_{j}u^{\prime}=0$. Also, it is pretty clear that $Ku^{\prime}=u^{\prime}$.
Thus, Lemma \ref{lem.V=F} (applied to $D_{i}\diagup D_{i+1}$ and $u^{\prime}$
instead of $V$ and $u$) yields that there exists a homomorphism $\eta
:F\rightarrow D_{i}\diagup D_{i+1}$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =u^{\prime}$. This homomorphism $\eta$ must be
surjective\footnote{since its image is $\eta\left(  \underbrace{F}_{=D\left(
x_{1},x_{2},...\right)  \cdot1}\right)  =D\left(  x_{1},x_{2},...\right)
\cdot\underbrace{\eta\left(  1\right)  }_{=u^{\prime}}=D\left(  x_{1}%
,x_{2},...\right)  \cdot u^{\prime}=D_{i}\diagup D_{i+1}$}, and thus
$D_{i}\diagup D_{i+1}$ is a factor module of $F$. Since $F$ is irreducible,
this yields that $D_{i}\diagup D_{i+1}\cong F$ or $D_{i}\diagup D_{i+1}=0$.
This proves (\ref{prop.V=F(X)U.pf.3}).

Now, clearly, the $\mathcal{A}_{0}$-module $\widetilde{W}=D_{0}\diagup D_{N}$
is filtered by the $\mathcal{A}_{0}$-modules $D_{i}\diagup D_{N}$ for
$i\in\left\{  0,1,...,N\right\}  $. Due to (\ref{prop.V=F(X)U.pf.3}), the
subquotients of this filtration are all $\cong F$ or $=0$, so that
$\widetilde{W}$ is a finite-length $\mathcal{A}_{0}$-module with all
composition factors isomorphic to $F$ (since $F$ is irreducible).

Since $W$ is a quotient module of $\widetilde{W}$, this yields that $W$ must
also be a finite-length $\mathcal{A}_{0}$-module with all composition factors
isomorphic to $F$.

Now forget that we fixed $v$. We have thus shown that for every $v\in V$, the
$\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of $V$
(this submodule is what we called $W$) is a finite-length module with
composition factors isomorphic to $F$.

By the assumption (that for every $v\in V$, there exists some $N\in\mathbb{N}$
such that for every $n\geq N$, we have $a_{n}v=0$), we can define an action of
$E=\sum\limits_{i>0}a_{-i}a_{i}\in\widehat{\mathcal{A}}$ (the so-called
\textit{Euler field}) on $V$. Note that $E$ acts on $V$ in a locally finite
way (this means that for any $v\in V$, the space $\mathbb{C}\left[  E\right]
\cdot v$ is finite-dimensional)\footnote{\textit{Proof.} Notice that $E$ acts
on $F$ as $\sum\limits_{i>0}ix_{i}\dfrac{\partial}{\partial x_{i}}$, and thus
$E$ acts on $F$ in a locally finite way (since the differential operator
$\sum\limits_{i>0}ix_{i}\dfrac{\partial}{\partial x_{i}}$ preserves the
degrees of polynomials), and thus also on $V$ (because for every $v\in V$, the
$\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of $V$
is a finite-length module with composition factors isomorphic to $F$).}. Now,
let us notice that the eigenvalues of the map $E\mid_{V}:V\rightarrow V$ (this
is the action of $E$ on $V$) are nonnegative
integers.\footnote{\textit{Proof.} Let $\rho$ be an eigenvalue of $E\mid_{V}$.
Then, there exists some nonzero eigenvector $v\in V$ to the eigenvalue $\rho$.
Consider this $v$. Clearly, $\rho$ must thus also be an eigenvalue of
$E\mid_{U\left(  \mathcal{A}_{0}\right)  \cdot v}$ (because $v$ is a nonzero
eigenvector of $E\mid_{V}$ to the eigenvalue $\rho$ and lies in $U\left(
\mathcal{A}_{0}\right)  \cdot v$). But the eigenvalues of $E\mid_{U\left(
\mathcal{A}_{0}\right)  \cdot v}$ are nonnegative integers (since we know that
the $\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of
$V$ is a finite-length module with composition factors isomorphic to $F$, and
we can easily check that the eigenvalues of $E\mid_{F}$ are nonnegative
integers). Hence, $\rho$ is a nonnegative integer. We have thus shown that
every eigenvalue of $E\mid_{V}$ is a nonnegative integer, qed.} Hence, we can
write $V$ as $V=\bigoplus\limits_{j\geq0}V\left[  j\right]  $, where $V\left[
j\right]  $ is the generalized eigenspace of $E\mid_{V}$ with eigenvalue $j$
for every $j\in\mathbb{N}$.

If some $v\in V$ satisfies $a_{i}v=0$ for all $i>0$, then $Ev=0$ and thus
$v\in V\left[  0\right]  $.

Conversely, if $v\in V\left[  0\right]  $, then $a_{i}v=0$ for all
$i>0$.\ \ \ \ \footnote{\textit{Proof.} Let $v\in V\left[  0\right]  $.
Suppose that $a_{i}v=0$ holds for not all $i>0$. Let $k$ be the largest
integer such that there exist $k_{1},...,k_{m}$ with $\sum\limits_{j\geq
1}jk_{j}=k$ and $\prod\limits_{j\geq1}a_{j}^{k_{j}}v\neq0$. (Such a largest
$k$ exists since $V$ has a locally nilpotent action of $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $.) Note that $k>0$, since otherwise $a_{i}v=0$
for all $i>0$.
\par
Then, $E\cdot\left(  \prod\limits_{j\geq1}a_{j}^{k_{j}}v\right)
=-k\prod\limits_{j\geq1}a_{j}^{k_{j}}v$ (this can be shown by a
straightforward induction using the fact that $a_{-i}a_{i}a_{j}=a_{j}%
a_{-i}a_{i}-i\delta_{i,j}a_{i}$ for any positive $i$ and $j$), and thus
$\prod\limits_{j\geq1}a_{j}^{k_{j}}v$ is a nonzero eigenvector of $E\mid_{V}$
with eigenvalue $-k$. This is a contradiction since the eigenvalues of
$E\mid_{V}$ are nonnegative integers. This contradiction shows that our
supposition that $a_{i}v=0$ holds for not all $i>0$ was wrong. Hence,
$a_{i}v=0$ for all $i>0$, qed.}

So we conclude that $V\left[  0\right]  =\operatorname*{Ker}E=\bigcap
\limits_{i\geq1}\operatorname*{Ker}a_{i}$.

Now, $F\otimes V\left[  0\right]  $ is an $\mathcal{A}_{0}$-module (where
$\mathcal{A}_{0}$ acts only on the $F$ tensorand, where $V\left[  0\right]  $
is considered just as a vector space). We will now construct an isomorphism
$F\otimes V\left[  0\right]  \rightarrow V$ of $\mathcal{A}_{0}$-modules. This
will prove Proposition \ref{prop.V=F(X)U}.

For every $v\in V\left[  0\right]  $, there exists a homomorphism $\eta
_{v}:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\eta_{v}\left(
1\right)  =v$ (according to Lemma \ref{lem.V=F}, applied to $v$ instead of $u$
(since $a_{i}v=0$ for all $i>0$ and $Kv=v$)). Consider these homomorphisms
$\eta_{v}$ for various $v$. Clearly, every $v\in V\left[  0\right]  $ and
$P\in F$ satisfy%
\begin{align*}
\eta_{v}\left(  P\right)   &  =\eta_{v}\left(  P\left(  a_{-1},a_{-2}%
,a_{-3},...\right)  \cdot1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}P=P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  \underbrace{\eta_{v}\left(
1\right)  }_{=v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\eta_{v}\text{ is an
}\mathcal{A}_{0}\text{-module map}\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  v.
\end{align*}
Hence, we can define a $\mathbb{C}$-linear map $\rho:F\otimes V\left[
0\right]  \rightarrow V$ by%
\[
\rho\left(  P\otimes v\right)  =\eta_{v}\left(  P\right)  =P\left(
a_{-1},a_{-2},a_{-3},...\right)  v\ \ \ \ \ \ \ \ \ \ \text{for any }P\in
F\text{ and }v\in V\left[  0\right]  .
\]
This map $\rho$ is an $\mathcal{A}_{0}$-module map (because $\eta_{v}$ is an
$\mathcal{A}_{0}$-module map for every $v\in V\left[  0\right]  $).

The restriction of the map $\rho$ to the subspace $\mathbb{C}\cdot1\otimes
V\left[  0\right]  $ of $F\otimes V\left[  0\right]  $ is injective (since it
maps every $1\otimes v$ to $v$). Hence, the map $\rho$ is
injective\footnote{This follows from the following general
representation-theoretical fact (applied to $A=U\left(  \mathcal{A}%
_{0}\right)  $, $I=F$, $R=V\left[  0\right]  $, $S=V$, $i=1$ and $\phi=\rho$):
\par
Let $A$ be a $\mathbb{C}$-algebra. Let $I$ be an irreducible $A$-module, and
let $S$ be an $A$-module. Let $R$ be a vector space. Let $i\in I$ be nonzero.
Let $\phi:I\otimes R\rightarrow S$ be an $A$-module homomorphism such that the
restriction of $\phi$ to $\mathbb{C}i\otimes R$ is injective. Then, $\phi$ is
injective.}. Also, considering the quotient $\mathcal{A}_{0}$-module
$V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  $, we notice that
$E\mid_{V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  }$ has only
strictly positive eigenvalues (since $\rho\left(  F\otimes V\left[  0\right]
\right)  \supseteq V\left[  0\right]  $, so that all eigenvectors of
$E\mid_{V}$ to eigenvalue $0$ have been killed when factoring modulo
$\rho\left(  F\otimes V\left[  0\right]  \right)  $), and thus $V\diagup
\rho\left(  F\otimes V\left[  0\right]  \right)  =0$%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, $V\diagup
\rho\left(  F\otimes V\left[  0\right]  \right)  \neq0$. Thus, there exists
some nonzero $w\in V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  $.
Write $w$ as $\overline{v}$, where $v$ is an element of $V$ and $\overline{v}$
denotes the residue class of $v$ modulo $\rho\left(  F\otimes V\left[
0\right]  \right)  $. As we know, the $\mathcal{A}_{0}$-submodule $U\left(
\mathcal{A}_{0}\right)  \cdot v$ of $V$ is a finite-length module with
composition factors isomorphic to $F$. Thus, the $\mathcal{A}_{0}$-module
$U\left(  \mathcal{A}_{0}\right)  \cdot w$ (being a quotient module of
$U\left(  \mathcal{A}_{0}\right)  \cdot v$) must also be a finite-length
module with composition factors isomorphic to $F$. Hence, there exists a
submodule of $U\left(  \mathcal{A}_{0}\right)  \cdot w$ isomorphic to $F$
(since $w\neq0$ and thus $U\left(  \mathcal{A}_{0}\right)  \cdot w\neq0$).
This submodule contains a nonzero eigenvector of $E$ to eigenvalue $0$
(because $F$ contains a nonzero eigenvector of $E$ to eigenvalue $0$, namely
$1$). This is a contradiction to the fact that $E\mid_{V\diagup\rho\left(
F\otimes V\left[  0\right]  \right)  }$ has only strictly positive
eigenvalues. This contradiction shows that our assumption was wrong, so we do
have $V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  =0$, qed.}. In
other words, $V=\rho\left(  F\otimes V\left[  0\right]  \right)  $, so that
$\rho$ is surjective. Since $\rho$ is an injective and surjective
$\mathcal{A}_{0}$-module map, we conclude that $\rho$ is an $\mathcal{A}_{0}%
$-module isomorphism. Thus, $V\cong F\otimes V\left[  0\right]  $ as
$\mathcal{A}_{0}$-modules. This proves Proposition \ref{prop.V=F(X)U}.

\subsection{Some consequences of Poincar\'{e}-Birkhoff-Witt}

We will now spend some time with generalities on Lie algebras and their
universal enveloping algebras. These generalities will be applied later, and
while these applications could be substituted by concrete computations, it
appears to me that it is better for the sake of clarity to do them generally
in here.

\begin{proposition}
\label{prop.U(X)U}Let $k$ be a field. Let $\mathfrak{c}$ be a $k$-Lie algebra.
Let $\mathfrak{a}$ and $\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$
such that $\mathfrak{a}+\mathfrak{b}=\mathfrak{c}$. Notice that $\mathfrak{a}%
\cap\mathfrak{b}$ is also a Lie subalgebra of $\mathfrak{c}$.

Let $\rho:U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \rightarrow U\left(
\mathfrak{c}\right)  $ be the $k$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
U\left(  \mathfrak{a}\right)  \text{ and }\beta\in U\left(  \mathfrak{b}%
\right)
\]
(this is clearly well-defined). Then, $\rho$ is an isomorphism of vector
spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and of right
$U\left(  \mathfrak{b}\right)  $-modules.
\end{proposition}

\begin{corollary}
\label{cor.U(X)U}Let $k$ be a field. Let $\mathfrak{c}$ be a $k$-Lie algebra.
Let $\mathfrak{a}$ and $\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$
such that $\mathfrak{a}\oplus\mathfrak{b}=\mathfrak{c}$ (as vector spaces, not
necessarily as Lie algebras). Let $\rho:U\left(  \mathfrak{a}\right)
\otimes_{k}U\left(  \mathfrak{b}\right)  \rightarrow U\left(  \mathfrak{c}%
\right)  $ be the $k$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \mathfrak{a}\right)
\text{ and }\beta\in U\left(  \mathfrak{b}\right)
\]
(this is clearly well-defined). Then, $\rho$ is an isomorphism of filtered
vector spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and of right
$U\left(  \mathfrak{b}\right)  $-modules.
\end{corollary}

We give two proofs of Proposition \ref{prop.U(X)U}. They are very similar
(both use the Poincar\'{e}-Birkhoff-Witt theorem, albeit different versions
thereof). The first is more conceptual (and more general), while the second is
more down-to-earth.

\textit{First proof of Proposition \ref{prop.U(X)U}.} For any Lie algebra
$\mathfrak{u}$, we have a $k$-algebra homomorphism $\operatorname*{PBW}%
\nolimits_{\mathfrak{u}}:S\left(  \mathfrak{u}\right)  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{u}\right)  \right)  $ which sends
$u_{1}u_{2}...u_{\ell}$ to $\overline{u_{1}u_{2}...u_{\ell}}\in
\operatorname*{gr}\nolimits_{\ell}\left(  U\left(  \mathfrak{u}\right)
\right)  $ for every $\ell\in\mathbb{N}$ and every $u_{1},u_{2},...,u_{\ell
}\in\mathfrak{u}$. This homomorphism $\operatorname*{PBW}%
\nolimits_{\mathfrak{u}}$ is an isomorphism due to the
Poincar\'{e}-Birkhoff-Witt theorem.

\begin{verlong}
It is rather clear that $\operatorname*{gr}\left(  U\left(  \mathfrak{a}%
\right)  \right)  $ and $\operatorname*{gr}\left(  U\left(  \mathfrak{b}%
\right)  \right)  $ are $\operatorname*{gr}\left(  U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  \right)  $-modules (since $U\left(  \mathfrak{a}%
\right)  $ and $U\left(  \mathfrak{b}\right)  $ are filtered $U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  $-modules)
\end{verlong}

We can define a $k$-algebra homomorphism $f:\operatorname*{gr}\left(  U\left(
\mathfrak{a}\right)  \right)  \otimes_{\operatorname*{gr}\left(  U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \right)  }\operatorname*{gr}\left(
U\left(  \mathfrak{b}\right)  \right)  \rightarrow\operatorname*{gr}\left(
U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \right)  $ by
\[
f\left(  \overline{u}\otimes_{\operatorname*{gr}\left(  U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \right)  }\overline{v}\right)
=\overline{u\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }v}%
\in\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)
\]
for any $k\in\mathbb{N}$, any $\ell\in\mathbb{N}$, any $u\in U_{\leq k}\left(
\mathfrak{a}\right)  $ and $v\in U_{\leq\ell}\left(  \mathfrak{b}\right)  $.
This $f$ is easily seen to be well-defined. Moreover, $f$ is
surjective\footnote{To show this, either notice that the image of $f$ contains
a generating set of $\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)  $ (because the definition of $f$ easily rewrites
as
\par%
\[
f\left(  \overline{\alpha_{1}\alpha_{2}...\alpha_{k}}\otimes
_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
\right)  }\overline{\beta_{1}\beta_{2}...\beta_{\ell}}\right)  =\overline
{\alpha_{1}\alpha_{2}...\alpha_{k}\otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\beta_{1}\beta_{2}...\beta_{\ell}}\in
\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)
\]
for any $k\in\mathbb{N}$, any $\ell\in\mathbb{N}$, any $\alpha_{1},\alpha
_{2},...,\alpha_{k}\in\mathfrak{a}$ and $\beta_{1},\beta_{2},...,\beta_{\ell
}\in\mathfrak{b}$), or prove the more general fact that for any $\mathbb{Z}%
_{+}$-filtered algebra $A$, any filtered right $A$-module $M$ and any filtered
left $A$-module $N$, the canonical map%
\begin{align*}
\operatorname*{gr}\left(  M\right)  \otimes_{\operatorname*{gr}\left(
A\right)  }\operatorname*{gr}\left(  N\right)   &  \rightarrow
\operatorname*{gr}\left(  M\otimes_{A}N\right)  ,\\
\overline{\mu}\otimes_{\operatorname*{gr}\left(  A\right)  }\overline{\nu}  &
\mapsto\overline{\mu\otimes_{A}\nu}\in\operatorname*{gr}\nolimits_{m+n}\left(
M\otimes_{A}N\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{for all }\mu\in
M_{m}\text{ and }\nu\in N_{n}\text{, for all }m,n\in\mathbb{N}\right)
\end{align*}
is well-defined and surjective (this is easy to prove).}.

It is easy to see that the isomorphisms $\operatorname*{PBW}%
\nolimits_{\mathfrak{a}}:S\left(  \mathfrak{a}\right)  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)  \right)  $,
$\operatorname*{PBW}\nolimits_{\mathfrak{b}}:S\left(  \mathfrak{b}\right)
\rightarrow\operatorname*{gr}\left(  U\left(  \mathfrak{b}\right)  \right)  $
and $\operatorname*{PBW}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}:S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \rightarrow\operatorname*{gr}\left(
U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \right)  $ are "compatible" with
each other in the sense that the diagrams%
\[%
%TCIMACRO{\TeXButton{X}{\xycs{12pc}
%\xymatrix{
%S\left(\fraka\right) \otimes S\left(\fraka\cap\frakb\right) \ar[r]^-{\text
%{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\fraka\right)}
%\ar[d]_{\PBW_{\fraka}\otimes\PBW_{\fraka\cap\frakb}}^{\cong} & S\left
%(\fraka\right) \ar[d]_{\PBW_{\fraka}}^{\cong} \\
%\gr\left(U\left(\fraka\right)\right) \otimes\gr\left(U\left(\fraka\cap
%\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
%\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\fraka\right)\right)}
%& \gr\left(U\left(\fraka\right)\right)
%}}}%
%BeginExpansion
\xycs{12pc}
\xymatrix{
S\left(\fraka\right) \otimes S\left(\fraka\cap\frakb\right) \ar[r]^-{\text
{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\fraka\right)}
\ar[d]_{\PBW_{\fraka}\otimes\PBW_{\fraka\cap\frakb}}^{\cong} & S\left
(\fraka\right) \ar[d]_{\PBW_{\fraka}}^{\cong} \\
\gr\left(U\left(\fraka\right)\right) \otimes\gr\left(U\left(\fraka\cap
\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\fraka\right)\right)}
& \gr\left(U\left(\fraka\right)\right)
}%
%EndExpansion
\]
and%
\[%
%TCIMACRO{\TeXButton{X}{\xycs{12pc}
%\xymatrix{
%S\left(\fraka\cap\frakb\right) \otimes S\left(\frakb\right) \ar[r]^-{\text
%{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\frakb\right)}
%\ar[d]_{\PBW_{\fraka\cap\frakb}\otimes\PBW_{\frakb}}^{\cong} & S\left
%(\frakb\right) \ar[d]_{\PBW_{\frakb}}^{\cong} \\
%\gr\left(U\left(\fraka\cap\frakb\right)\right) \otimes\gr\left(U\left
%(\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
%\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\frakb\right)\right)}
%& \gr\left(U\left(\frakb\right)\right)
%}}}%
%BeginExpansion
\xycs{12pc}
\xymatrix{
S\left(\fraka\cap\frakb\right) \otimes S\left(\frakb\right) \ar[r]^-{\text
{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\frakb\right)}
\ar[d]_{\PBW_{\fraka\cap\frakb}\otimes\PBW_{\frakb}}^{\cong} & S\left
(\frakb\right) \ar[d]_{\PBW_{\frakb}}^{\cong} \\
\gr\left(U\left(\fraka\cap\frakb\right)\right) \otimes\gr\left(U\left
(\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\frakb\right)\right)}
& \gr\left(U\left(\frakb\right)\right)
}%
%EndExpansion
\]
commute\footnote{This is pretty easy to see from the definition of
$\operatorname*{PBW}\nolimits_{\mathfrak{u}}$.}. Hence, they give rise to an
isomorphism%
\begin{align*}
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)   &  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)  \right)
\otimes_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  \right)  }\operatorname*{gr}\left(  U\left(  \mathfrak{b}\right)
\right)  ,\\
\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }\beta &
\mapsto\left(  \operatorname*{PBW}\nolimits_{\mathfrak{a}}\alpha\right)
\otimes_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  \right)  }\left(  \operatorname*{PBW}\nolimits_{\mathfrak{b}}%
\beta\right)  .
\end{align*}
Denote this isomorphism by $\left(  \operatorname*{PBW}\nolimits_{\mathfrak{a}%
}\right)  \otimes_{\operatorname*{PBW}\nolimits_{\mathfrak{a}\cap\mathfrak{b}%
}}\left(  \operatorname*{PBW}\nolimits_{\mathfrak{b}}\right)  $.

Finally, let $\sigma:S\left(  \mathfrak{a}\right)  \otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)
\rightarrow S\left(  \mathfrak{c}\right)  $ be the vector space homomorphism
defined by%
\[
\sigma\left(  \alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
S\left(  \mathfrak{a}\right)  \text{ and }\beta\in S\left(  \mathfrak{b}%
\right)  .
\]
This $\sigma$ is rather obviously an algebra homomorphism. Now, it is easy to
see that $\sigma$ is an algebra isomorphism\footnote{\textit{Proof.} Since
every subspace of a vector space has a complementary subspace, we can find a
subspace $\mathfrak{d}$ of $\mathfrak{a}$ such that $\mathfrak{a}%
=\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $. Consider
such a $\mathfrak{d}$.
\par
Since $\mathfrak{a}=\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  =\mathfrak{d}+\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $, the
fact that $\mathfrak{c}=\mathfrak{a}+\mathfrak{b}$ rewrites as $\mathfrak{c}%
=\mathfrak{d}+\underbrace{\left(  \mathfrak{a}\cap\mathfrak{b}\right)
+\mathfrak{b}}_{\substack{=\mathfrak{b}\\\text{(since }\mathfrak{a}%
\cap\mathfrak{b}\subseteq\mathfrak{b}\text{)}}}=\mathfrak{d}+\mathfrak{b}$.
Combined with $\underbrace{\mathfrak{d}}_{\substack{=\mathfrak{d}%
\cap\mathfrak{a}\\\text{(since }\mathfrak{d}\subseteq\mathfrak{a}\text{)}%
}}\cap\mathfrak{b}\subseteq\mathfrak{d}\cap\mathfrak{a}\cap\mathfrak{b}=0$
(since $\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $ is a
direct sum), this yields $\mathfrak{c}=\mathfrak{d}\oplus\mathfrak{b}$.
\par
Recall a known fact from multilinear algebra: Any two vector spaces $U$ and
$V$ satisfy $S\left(  U\oplus V\right)  \cong S\left(  U\right)  \otimes
_{k}S\left(  V\right)  $ by the canonical algebra isomorphism. Hence,
$S\left(  \mathfrak{d}\oplus\mathfrak{b}\right)  \cong S\left(  \mathfrak{d}%
\right)  \otimes_{k}S\left(  \mathfrak{b}\right)  $.
\par
But $\mathfrak{a}= \mathfrak{d} \oplus\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  $ yields $S\left(  \mathfrak{a}\right)  =S\left(  \mathfrak{d}%
\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \right)  \cong S\left(
\mathfrak{d}\right)  \otimes_{k}S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
$ (by the above-quoted known fact). Hence,%
\begin{align*}
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)   &  \cong\left(  S\left(
\mathfrak{d}\right)  \otimes_{k}S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
\right)  \otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right) \\
&  \cong S\left(  \mathfrak{d}\right)  \otimes_{k}\underbrace{\left(  S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  \right)  }_{=S\left(
\mathfrak{b}\right)  }=S\left(  \mathfrak{d}\right)  \otimes_{k}S\left(
\mathfrak{b}\right)  \cong S\left(  \underbrace{\mathfrak{d}\oplus
\mathfrak{b}}_{=\mathfrak{c}}\right)  =S\left(  \mathfrak{c}\right)  .
\end{align*}
Thus we have constructed an algebra isomorphism $S\left(  \mathfrak{a}\right)
\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right)  \rightarrow S\left(  \mathfrak{c}\right)  $. If we track
down what happens to elements of $\mathfrak{d}$, $\mathfrak{a}\cap
\mathfrak{b}$ and $\mathfrak{b}$ under this isomorphism, we notice that they
just get sent to themselves, so this isomorphism must coincide with $\sigma$
(since two algebra homomorphisms coinciding on a set generators of algebra
must be equal). Thus, $\sigma$ is an algebra isomorphism, qed.}.

Now, it is easy to see (by elementwise checking) that the diagram%
\[%
%TCIMACRO{\TeXButton{X}{\xymatrixcolsep{11pc}
%\xymatrix{
%\gr\left(U\left(\fraka\right)\right) \otimes_{\gr\left(U\left(\fraka\cap
%\frakb\right)\right)} \gr\left(U\left(\frakb\right)\right) \ar[d]^{f}
%& S\left(\fraka\right) \otimes_{S\left(\fraka\cap\frakb\right)} S\left
%(\frakb\right) \ar[l]_-{\left(\PBW_{\fraka}\right) \otimes_{\PBW_{\fraka
%\cap\frakb}} \left(\PBW_{\frakb}\right)}^{\cong} \ar[d]_{\cong}^{\sigma} \\
%\gr\left(U\left(\fraka\right) \otimes_{U\left(\fraka\cap\frakb\right)}
%U\left(\frakb\right)\right) \ar[dr]_{\gr\rho} & S\left(\frakc\right
%) \ar[d]^{\PBW_{\frakc}}_{\cong} \\
%& \gr\left(U\left(\frakc\right)\right)
%}}}%
%BeginExpansion
\xymatrixcolsep{11pc}
\xymatrix{
\gr\left(U\left(\fraka\right)\right) \otimes_{\gr\left(U\left(\fraka\cap
\frakb\right)\right)} \gr\left(U\left(\frakb\right)\right) \ar[d]^{f}
& S\left(\fraka\right) \otimes_{S\left(\fraka\cap\frakb\right)} S\left
(\frakb\right) \ar[l]_-{\left(\PBW_{\fraka}\right) \otimes_{\PBW_{\fraka
\cap\frakb}} \left(\PBW_{\frakb}\right)}^{\cong} \ar[d]_{\cong}^{\sigma} \\
\gr\left(U\left(\fraka\right) \otimes_{U\left(\fraka\cap\frakb\right)}
U\left(\frakb\right)\right) \ar[dr]_{\gr\rho} & S\left(\frakc\right
) \ar[d]^{\PBW_{\frakc}}_{\cong} \\
& \gr\left(U\left(\frakc\right)\right)
}%
%EndExpansion
\]
is commutative.\footnote{In fact, if we follow the pure tensor $\alpha
_{1}\alpha_{2}...\alpha_{k}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\beta_{1}\beta_{2}...\beta_{\ell}$ (with $k\in\mathbb{N}$, $\ell
\in\mathbb{N}$, $\alpha_{1},\alpha_{2},...,\alpha_{k}\in\mathfrak{a}$ and
$\beta_{1},\beta_{2},...,\beta_{\ell}\in\mathfrak{b}$) through this diagram,
we get $\overline{\alpha_{1}\alpha_{2}...\alpha_{k}\beta_{1}\beta_{2}%
...\beta_{\ell}}\in\operatorname*{gr}\nolimits_{k+\ell}\left(  \mathfrak{c}%
\right)  $ both ways.} Hence, $\left(  \operatorname*{gr}\rho\right)  \circ f$
is an isomorphism, so that $f$ is injective. Since $f$ is also surjective,
this yields that $f$ is an isomorphism. Thus, $\operatorname*{gr}\rho$ is an
isomorphism (since $\left(  \operatorname*{gr}\rho\right)  \circ f$ is an
isomorphism). Since $\rho$ is a filtered map and $\operatorname*{gr}\rho$ is
an isomorphism, it follows that $\rho$ is an isomorphism of filtered vector
spaces. Hence, $\rho$ is an isomorphism of filtered vector spaces, of left
$U\left(  \mathfrak{a}\right)  $-modules and of right $U\left(  \mathfrak{b}%
\right)  $-modules (since it is clear that $\rho$ is a homomorphism of
$U\left(  \mathfrak{a}\right)  $-left modules and of $U\left(  \mathfrak{b}%
\right)  $-right modules). This proves Proposition \ref{prop.U(X)U}.

\textit{Second proof of Proposition \ref{prop.U(X)U}.} Let $\left(
z_{i}\right)  _{i\in I}$ be a basis of the $k$-vector space $\mathfrak{a}%
\cap\mathfrak{b}$. We extend this basis to a basis $\left(  z_{i}\right)
_{i\in I}\cup\left(  x_{j}\right)  _{j\in J}$ of the $k$-vector space
$\mathfrak{a}$ and to a basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
y_{k}\right)  _{k\in K}$ of the $k$-vector space\footnote{Sorry for using the
letter $k$ in two different meanings. I hope they are easy enough to tell
apart.} $\mathfrak{b}$. Then, $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}\cup\left(  y_{k}\right)  _{k\in K}$ is a basis of the
$k$-vector space $\mathfrak{c}$. We endow this basis with a total ordering in
such a way that every $x_{j}$ is smaller than every $z_{i}$, and that every
$z_{i}$ is smaller than every $y_{k}$. By the Poincar\'{e}-Birkhoff-Witt
theorem, we have a basis of $U\left(  \mathfrak{c}\right)  $ consisting of
increasing products of elements of the basis $\left(  z_{i}\right)  _{i\in
I}\cup\left(  x_{j}\right)  _{j\in J}\cup\left(  y_{k}\right)  _{k\in K}$. On
the other hand, again by the Poincar\'{e}-Birkhoff-Witt theorem, we have a
basis of $U\left(  \mathfrak{a}\right)  $ consisting of increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}$. Note that the $z_{i}$ accumulate at the right end of
these products, while the $x_{j}$ accumulate at the left end (because we
defined the total ordering in such a way that every $x_{j}$ is smaller than
every $z_{i}$). Hence, $U\left(  \mathfrak{a}\right)  $ is a free right
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $-module, with a basis (over
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $, not over $k$) consisting of
increasing products of elements of the basis $\left(  x_{j}\right)  _{j\in J}%
$. Combined with the fact that $U\left(  \mathfrak{b}\right)  $ is a free
$k$-vector space with a basis consisting of increasing products of elements of
the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(  y_{k}\right)  _{k\in K}$
(again by Poincar\'{e}-Birkhoff-Witt), this yields that $U\left(
\mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}U\left(  \mathfrak{b}\right)  $ is a free $k$-vector space with a basis
consisting of tensors of the form%
\begin{align*}
&  \left(  \text{increasing product of elements of the basis }\left(
x_{j}\right)  _{j\in J}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\left(  \text{increasing product of elements of the basis }\left(
z_{i}\right)  _{i\in I}\cup\left(  y_{k}\right)  _{k\in K}\right)  .
\end{align*}
The map $\rho$ clearly maps such terms bijectively into increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}\cup\left(  y_{k}\right)  _{k\in K}$. Hence, $\rho$
maps a basis of $U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  $
bijectively to a basis of $U\left(  \mathfrak{c}\right)  $. Thus, $\rho$ is an
isomorphism of vector spaces. Moreover, since both of our bases were
filtered\footnote{A basis $\mathcal{B}$ of a filtered vector space $V$ is said
to be \textit{filtered} if for every $n\in\mathbb{N}$, the subfamily of
$\mathcal{B}$ consisting of those elements of $\mathcal{B}$ lying in the
$n$-th filtration of $V$ is a basis of the $n$-th filtration of $V$.}, and
$\rho$ respects this filtration on the bases, we can even conclude that $\rho$
is an isomorphism of filtered vector spaces. Since it is clear that $\rho$ is
a homomorphism of $U\left(  \mathfrak{a}\right)  $-left modules and of
$U\left(  \mathfrak{b}\right)  $-right modules, it follows that $\rho$ is an
isomorphism of filtered vector spaces, of left $U\left(  \mathfrak{a}\right)
$-modules and of right $U\left(  \mathfrak{b}\right)  $-modules. This proves
Proposition \ref{prop.U(X)U}.

\textit{Proof of Corollary \ref{cor.U(X)U}.} Corollary \ref{cor.U(X)U}
immediately follows from Proposition \ref{prop.U(X)U} (since $\mathfrak{a}%
\oplus\mathfrak{b}=\mathfrak{c}$ yields $\mathfrak{a}\cap\mathfrak{b}=0$, thus
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  =U\left(  0\right)  =k$).

\begin{remark}
\label{rmk.U(X)U}While we have required $k$ to be a field in Proposition
\ref{prop.U(X)U} and Corollary \ref{cor.U(X)U}, these two results hold in more
general situations as well. For instance, Proposition \ref{prop.U(X)U} holds
whenever $k$ is a commutative ring, as long as $\mathfrak{a}$, $\mathfrak{b}$
and $\mathfrak{a}\cap\mathfrak{b}$ are free $k$-modules, and $\mathfrak{a}%
\cap\mathfrak{b}$ is a direct summand of $\mathfrak{a}$ as a $k$-module. In
fact, the first proof of Proposition \ref{prop.U(X)U} works in this situation
(because the Poincar\'{e}-Birkhoff-Witt theorem holds for free modules). In a
more restrictive situation (namely, when $\mathfrak{a}\cap\mathfrak{b}$ is a
free $k$-module, and a direct summand of each of $\mathfrak{a}$ and
$\mathfrak{b}$, with the other two summands also being free), the second proof
of Proposition \ref{prop.U(X)U} works as well. As for Corollary
\ref{cor.U(X)U}, it holds whenever $k$ is a commutative ring, as long as
$\mathfrak{a}$ and $\mathfrak{b}$ are free $k$-modules.

This generality is more than enough for most applications of Proposition
\ref{prop.U(X)U} and Corollary \ref{cor.U(X)U}. Yet we can go even further
using the appropriate generalizations of the Poincar\'{e}-Birkhoff-Witt
theorem (for these, see, e. g., P. J. Higgins, \textit{Baer Invariants and the
Birkhoff-Witt theorem}, J. of Alg. 11, pp. 469-482, (1969)).
\end{remark}

\subsection{$\mathbb{Z}$-graded Lie algebras and Verma modules}

Let us show some general results about representations of $\mathbb{Z}$-graded
Lie algebras -- particularly of \textit{nondegenerate} $\mathbb{Z}$-graded Lie
algebras. This is a notion that encompasses many of the concrete Lie algebras
that we want to study (among others, $\mathcal{A}$, $\mathcal{A}_{0}$, $W$ and
$\operatorname*{Vir}$), and thus by proving the properties of nondegenerate
$\mathbb{Z}$-graded Lie algebras now we can avoid proving them separately in
many different cases.

\begin{definition}
\label{def.gradLie}A $\mathbb{Z}$\textit{-graded Lie algebra} is a Lie algebra
$\mathfrak{g}$ with a decomposition $\mathfrak{g}=\bigoplus\limits_{n\in
\mathbb{Z}}\mathfrak{g}_{n}$ (as a vector space) such that $\left[
\mathfrak{g}_{n},\mathfrak{g}_{m}\right]  \subseteq\mathfrak{g}_{n+m}$ for all
$n,m\in\mathbb{Z}$.
\end{definition}

Note that if $\mathfrak{g}=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}$
is a $\mathbb{Z}$-graded Lie algebra, then $\bigoplus\limits_{n<0}%
\mathfrak{g}_{n}$, $\mathfrak{g}_{0}$ and $\bigoplus\limits_{n>0}%
\mathfrak{g}_{n}$ are Lie subalgebras of $\mathfrak{g}$.

\begin{definition}
\label{def.gradLienondeg}A $\mathbb{Z}$-graded Lie algebra $\mathfrak{g}%
=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}$ is said to be
\textit{nondegenerate} if

\textbf{(1)} the vector space $\mathfrak{g}_{n}$ is finite-dimensional for
every $n\in\mathbb{Z}$;

\textbf{(2)} the Lie algebra $\mathfrak{g}_{0}$ is abelian;

\textbf{(3)} for every positive integer $n$, for generic $\lambda
\in\mathfrak{g}_{0}^{\ast}$, the bilinear form $\mathfrak{g}_{n}%
\times\mathfrak{g}_{-n}\rightarrow\mathbb{C},\ \left(  a,b\right)
\mapsto\lambda\left(  \left[  a,b\right]  \right)  $ is nondegenerate.
("Generic $\lambda$" means "$\lambda$ lying in some dense open subset of
$\mathfrak{g}_{0}^{\ast}$ with respect to the Zariski topology".)
\end{definition}

Note that condition \textbf{(3)} in Definition \ref{def.gradLienondeg} implies
that $\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}%
_{-n}\right)  $ for all $n\in\mathbb{Z}$.

Here are some examples:

\begin{proposition}
The Lie algebras $\mathcal{A}$, $\mathcal{A}_{0}$, $W$ and
$\operatorname*{Vir}$ are nondegenerate (with the usual gradings).
\end{proposition}

\begin{proposition}
\label{prop.grad.g}Let $\mathfrak{g}$ be a finite-dimensional simple Lie
algebra. The following is a reasonable (although non-canonical) way to define
a grading on $\mathfrak{g}$:

Using a Cartan subalgebra and the roots of $\mathfrak{g}$, we can present the
Lie algebra $\mathfrak{g}$ as a Lie algebra with generators $e_{1}$, $e_{2}$,
$...$, $e_{m}$, $f_{1}$, $f_{2}$, $...$, $f_{m}$, $h_{1}$, $h_{2}$, $...$,
$h_{m}$ (the so-called Chevalley generators) and some relations (the Serre
relations). Then, we can define a grading on $\mathfrak{g}$ by setting
$\deg\left(  e_{i}\right)  =1$, $\deg\left(  f_{i}\right)  =-1$ and
$\deg\left(  h_{i}\right)  =0$ and continuing this grading in such a way that
$\mathfrak{g}$ becomes a graded Lie algebra. This grading is non-canonical,
but it makes $\mathfrak{g}$ into a nondegenerate graded Lie algebra.
\end{proposition}

\begin{proposition}
If $\mathfrak{g}$ is a simple Lie algebra, then the loop algebra
$\mathfrak{g}\left[  t,t^{-1}\right]  $ and the affine Kac-Moody algebra
$\widehat{\mathfrak{g}}=\mathfrak{g}\left[  t,t^{-1}\right]  \oplus
\mathbb{C}K$ can be graded as follows:

Fix Chevalley generators for $\mathfrak{g}$ and grade $\mathfrak{g}$ as in
Proposition \ref{prop.grad.g}. Now let $\theta$ be the maximal root of
$\mathfrak{g}$, i. e., the highest weight of the adjoint representation of
$\mathfrak{g}$. Let $e_{\theta}$ and $f_{\theta}$ be the root elements
corresponding to $\theta$. The \textit{Coxeter number} of $\mathfrak{g}$ is
defined as $\deg\left(  e_{\theta}\right)  +1$, and denoted by $h$. Now let us
grade $\widehat{\mathfrak{g}}$ by setting $\deg K=0$ and $\deg\left(
at^{m}\right)  =\deg a+mh$ for every homogeneous $a\in\mathfrak{g}$ and every
$m\in\mathbb{Z}$. This grading satisfies $\deg t=h$, $\deg\left(  f_{\theta
}t\right)  =1$ and $\deg\left(  e_{\theta}t^{-1}\right)  =-1$. It is easy to
see that the elements of $\widehat{\mathfrak{g}}$ of positive degree span
$\mathfrak{n}_{+}\oplus t\mathfrak{g}\left[  t\right]  $. The graded Lie
algebra $\widehat{\mathfrak{g}}$ is nondegenerate. The loop algebra
$\mathfrak{g}\left[  t,t^{-1}\right]  $, however, is not (with the grading
defined in the same way).
\end{proposition}

If $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie algebra, we can write%
\[
\mathfrak{g}=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}=\bigoplus
\limits_{n<0}\mathfrak{g}_{n}\oplus\mathfrak{g}_{0}\oplus\bigoplus
\limits_{n>0}\mathfrak{g}_{n}.
\]
We denote $\bigoplus\limits_{n<0}\mathfrak{g}_{n}$ by $\mathfrak{n}_{-}$ and
we denote $\bigoplus\limits_{n>0}\mathfrak{g}_{n}$ by $\mathfrak{n}_{+}$. We
also denote $\mathfrak{g}_{0}$ by $\mathfrak{h}$. Then, $\mathfrak{n}_{-}$,
$\mathfrak{n}_{+}$ and $\mathfrak{h}$ are Lie subalgebras of $\mathfrak{g}$,
and the above decomposition rewrites as $\mathfrak{g}=\mathfrak{n}_{-}%
\oplus\mathfrak{h}\oplus\mathfrak{n}_{+}$ (but this is, of course, not a
decomposition of Lie algebras). This is called the \textit{triangular
decomposition} of $\mathfrak{g}$.

In the following, $\mathfrak{g}$ will be a $\mathbb{Z}$-graded Lie algebra
(not necessarily nondegenerate), and we will work with the notations
introduced above.

\begin{definition}
\label{def.verma}Let $\lambda\in\mathfrak{h}^{\ast}$.

Let $\mathbb{C}_{\lambda}$ denote the $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{+}\right)  $-module which, as a $\mathbb{C}$-vector space, is
the free vector space with basis $\left(  v_{\lambda}^{+}\right)  $ (thus, a
$1$-dimensional vector space), and whose $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{+}\right)  $-action is given by%
\begin{align*}
hv_{\lambda}^{+}  &  =\lambda\left(  h\right)  v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h};\\
\mathfrak{n}_{+}v_{\lambda}^{+}  &  =0.
\end{align*}


The \textit{Verma highest-weight module }$M_{\lambda}^{+}$ \textit{of
}$\left(  \mathfrak{g},\lambda\right)  $ is defined by%
\[
M_{\lambda}^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{\lambda}.
\]
The element $1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
}v_{\lambda}^{+}$ of $M_{\lambda}^{+}$ will still be denoted by $v_{\lambda
}^{+}$ by abuse of notation, and will be called the \textit{defining vector}
of $M_{\lambda}^{+}$. Since $U\left(  \mathfrak{g}\right)  $ and
$\mathbb{C}_{\lambda}$ are graded $U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  $-modules, their tensor product $U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{\lambda}=M_{\lambda}^{+}$ becomes graded as well.

Let $\mathbb{C}_{\lambda}$ denote the $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  $-module which, as a $\mathbb{C}$-vector space, is
the free vector space with basis $\left(  v_{\lambda}^{-}\right)  $ (thus, a
$1$-dimensional vector space), and whose $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  $-action is given by%
\begin{align*}
hv_{\lambda}^{-}  &  =\lambda\left(  h\right)  v_{\lambda}^{-}%
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h};\\
\mathfrak{n}_{-}v_{\lambda}^{-}  &  =0.
\end{align*}
(Note that we denote this $\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
$-module by $\mathbb{C}_{\lambda}$, although we already have denoted an
$\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-module by $\mathbb{C}%
_{\lambda}$. This is ambiguous, but misunderstandings are unlikely to occur
since these modules are modules over different Lie algebras, and their
restrictions to $\mathfrak{h}$ are identical.)

The \textit{Verma lowest-weight module }$M_{\lambda}^{-}$ \textit{of }$\left(
\mathfrak{g},\lambda\right)  $ is defined by%
\[
M_{\lambda}^{-}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{\lambda}.
\]
The element $1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
}v_{\lambda}^{-}$ of $M_{\lambda}^{-}$ will still be denoted by $v_{\lambda
}^{-}$ by abuse of notation, and will be called the \textit{defining vector}
of $M_{\lambda}^{-}$. Since $U\left(  \mathfrak{g}\right)  $ and
$\mathbb{C}_{\lambda}$ are graded $U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{-}\right)  $-modules, their tensor product $U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}%
_{\lambda}=M_{\lambda}^{-}$ becomes graded as well.
\end{definition}

We notice some easy facts about these modules:

\begin{proposition}
\label{prop.verma1}Let $\lambda\in\mathfrak{h}^{\ast}$.

\textbf{(a)} As a graded $\mathfrak{n}_{-}$-module, $M_{\lambda}^{+}=U\left(
\mathfrak{n}_{-}\right)  v_{\lambda}^{+}$; more precisely, there exists a
graded $\mathfrak{n}_{-}$-module isomorphism $U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{+}$ which
sends every $x\otimes t\in U\left(  \mathfrak{n}_{-}\right)  \otimes
\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{+}$. The Verma module $M_{\lambda
}^{+}$ is concentrated in nonpositive degrees:%
\[
M_{\lambda}^{+}=\bigoplus\limits_{n\geq0}M_{\lambda}^{+}\left[  -n\right]
;\ \ \ \ \ \ \ \ \ \ M_{\lambda}^{+}\left[  -n\right]  =U\left(
\mathfrak{n}_{-}\right)  \left[  -n\right]  v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\geq0.
\]
Also, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\leq-1$, we have%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]


\textbf{(b)} As a graded $\mathfrak{n}_{+}$-module, $M_{\lambda}^{-}=U\left(
\mathfrak{n}_{+}\right)  v_{\lambda}^{-}$; more precisely, there exists a
graded $\mathfrak{n}_{+}$-module isomorphism $U\left(  \mathfrak{n}%
_{+}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{-}$ which
sends every $x\otimes t\in U\left(  \mathfrak{n}_{+}\right)  \otimes
\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{-}$. The Verma module $M_{\lambda
}^{-}$ is concentrated in nonnegative degrees:%
\[
M_{\lambda}^{-}=\bigoplus\limits_{n\geq0}M_{\lambda}^{-}\left[  n\right]
;\ \ \ \ \ \ \ \ \ \ M_{\lambda}^{-}\left[  n\right]  =U\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  v_{\lambda}^{-}\ \ \ \ \ \ \ \ \ \ \text{for
every }n\geq0.
\]
Also, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\geq1$, we have%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\geq1}\left(  1-q^{j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.verma1}.} \textbf{(a)} Let
$\rho:U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}}U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \rightarrow U\left(  \mathfrak{g}%
\right)  $ be the $\mathbb{C}$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \mathfrak{n}_{-}\right)
\text{ and }\beta\in U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
\]
(this is clearly well-defined). By Corollary \ref{cor.U(X)U} (applied to
$\mathfrak{a}=\mathfrak{n}_{-}$, $\mathfrak{b}=\mathfrak{h}\oplus
\mathfrak{n}_{+}$ and $\mathfrak{c}=\mathfrak{g}$), this $\rho$ is an
isomorphism of filtered\footnote{Filtered by the usual filtration on the
universal enveloping algebra of a Lie algebra. This filtration does not take
into account the grading on $\mathfrak{n}_{-}$, $\mathfrak{h}\oplus
\mathfrak{n}_{+}$ and $\mathfrak{g}$.} vector spaces, of left $U\left(
\mathfrak{n}_{-}\right)  $-modules and of right $U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  $-modules. Also, it is a graded linear
map\footnote{Here we \textit{do} take into account the grading on
$\mathfrak{n}_{-}$, $\mathfrak{h}\oplus\mathfrak{n}_{+}$ and $\mathfrak{g}$.}
(this is clear from its definition), and thus an isomorphism of graded vector
spaces (because if a vector space isomorphism of graded vector spaces is a
graded linear map, then it must be an isomorphism of graded vector
spaces\footnote{If you are wondering why this statement is more than a
blatantly obvious tautology, let me add some clarifications:
\par
A \textit{graded linear map} is a morphism in the category of graded vector
spaces. What I am stating here is that if a vector space isomorphism between
graded vector spaces is at the same time a morphism in the category of graded
vector spaces, then it must be an \textit{isomorphism} in the category of
graded vector spaces. This is very easy to show, but not a self-evident
tautology. In fact, the analogous assertion about filtered vector spaces (i.
e., the assertion that if a vector space isomorphism between filtered vector
spaces is at the same time a morphism in the category of filtered vector
spaces, then it must be an \textit{isomorphism} in the category of filtered
vector spaces) is wrong.}. Altogether, $\rho$ is an isomorphism of graded
filtered vector spaces, of left $U\left(  \mathfrak{n}_{-}\right)  $-modules
and of right $U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-modules.
Hence,%
\begin{align*}
M_{\lambda}^{+}  &  =\underbrace{U\left(  \mathfrak{g}\right)  }%
_{\substack{\cong U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}%
}U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \\\text{(by the
isomorphism }\rho\text{)}}}\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}\cong\left(  U\left(  \mathfrak{n}%
_{-}\right)  \otimes_{\mathbb{C}}U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  \right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}\\
&  \cong U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}}%
\underbrace{\left(  U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{\lambda}\right)  }_{\cong\mathbb{C}_{\lambda}}\cong U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\ \ \ \ \ \ \ \ \ \ \text{as graded
}U\left(  \mathfrak{n}_{-}\right)  \text{-modules.}%
\end{align*}
This gives us a graded $\mathfrak{n}_{-}$-module isomorphism $U\left(
\mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda
}^{+}$ which is easily seen to send every $x\otimes t\in U\left(
\mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{+}$.
Hence, $M_{\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)  v_{\lambda}^{+}$.
Since $\mathfrak{n}_{-}$ is concentrated in negative degrees, it is clear that
$U\left(  \mathfrak{n}_{-}\right)  $ is concentrated in nonpositive degrees.
Hence, $U\left(  \mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ is
concentrated in nonpositive degrees, and thus the same holds for $M_{\lambda
}^{+}$ (since $M_{\lambda}^{+}\cong U\left(  \mathfrak{n}_{-}\right)
\otimes\mathbb{C}_{\lambda}$ as graded $U\left(  \mathfrak{n}_{-}\right)
$-modules). In other words, $M_{\lambda}^{+}=\bigoplus\limits_{n\geq
0}M_{\lambda}^{+}\left[  -n\right]  $.

Since the isomorphism $U\left(  \mathfrak{n}_{-}\right)  \otimes
\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{+}$ which sends every $x\otimes
t\in U\left(  \mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ to
$xtv_{\lambda}^{+}$ is graded, it sends $U\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  \otimes\mathbb{C}_{\lambda}=\left(  U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\right)  \left[  -n\right]  $ to
$M_{\lambda}^{+}\left[  -n\right]  $ for every $n\geq0$. Thus, $M_{\lambda
}^{+}\left[  -n\right]  =U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]
v_{\lambda}^{+}$ for every $n\geq0$. Hence,
\begin{align*}
\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)   &  =\dim\left(
U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  v_{\lambda}^{+}\right)
=\dim\left(  U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \right)
=\dim\left(  S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }U\left(  \mathfrak{n}_{-}\right)  \cong S\left(  \mathfrak{n}%
_{-}\right)  \text{ as graded vector spaces}\\
\text{(by the Poincar\'{e}-Birkhoff-Witt theorem)}%
\end{array}
\right)
\end{align*}
for every $n\geq0$. This easily yields that if $\dim\mathfrak{g}_{j}<\infty$
for all $j\leq-1$, then%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim\left(
\left(  \mathfrak{n}_{-}\right)  _{j}\right)  }}=\dfrac{1}{\prod
\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim\mathfrak{g}_{j}}}.
\]
This proves Proposition \ref{prop.verma1} \textbf{(a)}.

\textbf{(b)} The proof of part \textbf{(b)} is analogous to that of
\textbf{(a)}.

This proves Proposition \ref{prop.verma1}.

We have already encountered an example of a Verma highest-weight module:

\begin{proposition}
\label{prop.fockverma}Let $\mathfrak{g}$ be the Lie algebra $\mathcal{A}_{0}$.
Consider the Fock module $F$ over the Lie algebra $\mathcal{A}_{0}$. Then,
there is a canonical isomorphism $M_{0}^{+}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules (where $0$ is the zero element of $\mathfrak{h}^{\ast}$) which sends
$v_{0}^{+}\in M_{0}^{+}$ to $1\in F$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.fockverma}.} As we showed in the First
proof of Lemma \ref{lem.V=F}, there exists a homomorphism $\overline{\eta
}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}$-modules such
that $\overline{\eta}_{F,1}\left(  1\right)  =1$. In the same proof, we also
showed that this $\overline{\eta}_{F,1}$ is an isomorphism. We thus have an
isomorphism $\overline{\eta}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of
$\mathcal{A}_{0}$-modules such that $\overline{\eta}_{F,1}\left(  1\right)
=1$. Since%
\begin{align*}
\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}  &  =U\left(  \mathcal{A}_{0}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }%
\mathbb{C}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathcal{A}_{0}=\mathfrak{g}%
\text{, }\mathbb{C}K=\mathfrak{h}\text{, }\mathcal{A}_{0}^{+}=\mathfrak{n}%
_{+}\text{ and }\mathbb{C}=\mathbb{C}_{0}\right) \\
&  =M_{0}^{+},
\end{align*}
and since the element $1$ of $\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$ is exactly the
element $v_{0}^{+}$ of $M_{0}^{+}$, this rewrites as follows: We have an
isomorphism $\overline{\eta}_{F,1}:M_{0}^{+}\rightarrow F$ of $\mathcal{A}%
_{0}$-modules such that $\overline{\eta}_{F,1}\left(  v_{0}^{+}\right)  =1$.
This proves Proposition \ref{prop.fockverma}.

\subsection{\label{subsect.invform}The invariant bilinear form on Verma
modules}

\subsubsection{The invariant bilinear form}

The study of the Verma modules rests on a $\mathfrak{g}$-bilinear form which
connects a highest-weight Verma module with a lowest-weight Verma module for
the opposite weight. First, let us prove its existence and basic properties:

\begin{proposition}
\label{prop.invform}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra,
and $\lambda\in\mathfrak{h}^{\ast}$.

\textbf{(a)} There exists a unique $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =1$ (where we denote this
bilinear form by $\left(  \cdot,\cdot\right)  $).

\textbf{(b)} This form has degree $0$. (This means that if we consider this
bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ as
a linear map $M_{\lambda}^{+}\otimes M_{-\lambda}^{-}\rightarrow\mathbb{C}$,
then it is a graded map, where $M_{\lambda}^{+}\otimes M_{-\lambda}^{-}$ is
graded as a tensor product of graded vector spaces, and $\mathbb{C}$ is
concentrated in degree $0$.)

\textbf{(c)} Every $\mathfrak{g}$-invariant bilinear form $M_{\lambda}%
^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ is a scalar multiple of this
form $\left(  \cdot,\cdot\right)  $.
\end{proposition}

\begin{remark}
\label{rmk.invform.1}Proposition \ref{prop.invform} still holds when the
ground field $\mathbb{C}$ is replaced by a commutative ring $k$, as long as
some rather weak conditions hold (for instance, it is enough that
$\mathfrak{n}_{-}$, $\mathfrak{n}_{+}$ and $\mathfrak{h}$ are free $k$-modules).
\end{remark}

\begin{definition}
\label{def.invform}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra,
and $\lambda\in\mathfrak{h}^{\ast}$. According to Proposition
\ref{prop.invform} \textbf{(a)}, there exists a unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$ (where we denote this bilinear form by $\left(  \cdot
,\cdot\right)  $). This form is going to be denoted by $\left(  \cdot
,\cdot\right)  _{\lambda}$ (to stress its dependency on $\lambda$). (Later we
will also denote this form by $\left(  \cdot,\cdot\right)  _{\lambda
}^{\mathfrak{g}}$ to point out its dependency on both $\lambda$ and
$\mathfrak{g}$.)
\end{definition}

To prove Proposition \ref{prop.invform}, we recall two facts about modules
over Lie algebras:

\begin{lemma}
\label{lem.pushpull}Let $\mathfrak{a}$ be a Lie algebra, and let
$\mathfrak{b}$ be a Lie subalgebra of $\mathfrak{a}$. Let $V$ be a
$\mathfrak{b}$-module, and $W$ be an $\mathfrak{a}$-module. Then, $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\cong\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ as $\mathfrak{a}$-modules (where the $W$ on the right
hand side is to be understood as $\operatorname*{Res}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}W$). More precisely, there exists a canonical $\mathfrak{a}%
$-module isomorphism $\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}V\right)  \otimes W\rightarrow\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  $ which maps
$\left(  1\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes w$ to
$1\otimes_{U\left(  \mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all
$v\in V$ and $w\in W$.
\end{lemma}

\begin{lemma}
\label{lem.IndRes}Let $\mathfrak{c}$ be a Lie algebra. Let $\mathfrak{a}$ and
$\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$ such that
$\mathfrak{a}+\mathfrak{b}=\mathfrak{c}$. Notice that $\mathfrak{a}%
\cap\mathfrak{b}$ is also a Lie subalgebra of $\mathfrak{c}$. Let $N$ be a
$\mathfrak{b}$-module. Then, $\operatorname*{Ind}\nolimits_{\mathfrak{a}%
\cap\mathfrak{b}}^{\mathfrak{a}}\left(  \operatorname*{Res}%
\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{b}}N\right)
\cong\operatorname*{Res}\nolimits_{\mathfrak{a}}^{\mathfrak{c}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\right)  $ as
$\mathfrak{a}$-modules.
\end{lemma}

We will give two proofs of Lemma \ref{lem.pushpull}: one which is direct and
uses Hopf algebras; the other which is more elementary but less direct.

\textit{First proof of Lemma \ref{lem.pushpull}.} Remember that $U\left(
\mathfrak{a}\right)  $ is a Hopf algebra (a cocommutative one, actually; but
we won't use this). Let us denote its antipode by $S$ and use sumfree Sweedler notation.

Recalling that $\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V=U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{b}\right)  }V$
and $\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  =U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{b}\right)  }\left(  V\otimes W\right)  $, we define a $\mathbb{C}%
$-linear map $\phi:\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}V\right)  \otimes W\rightarrow\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  $ by
$\left(  \alpha\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes
w\mapsto\alpha_{\left(  1\right)  }\otimes_{U\left(  \mathfrak{b}\right)
}\left(  v\otimes S\left(  \alpha_{\left(  2\right)  }\right)  w\right)  $.
This map is easily checked to be well-defined and $\mathfrak{a}$-linear. Also,
we define a $\mathbb{C}$-linear map $\psi:\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)
\rightarrow\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V\right)  \otimes W$ by $\alpha\otimes_{U\left(  \mathfrak{b}\right)
}\left(  v\otimes w\right)  \mapsto\left(  \alpha_{\left(  1\right)  }%
\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes\alpha_{\left(
2\right)  }w$. This map is easily checked to be well-defined. It is also easy
to see that $\phi\circ\psi=\operatorname*{id}$ and $\psi\circ\phi
=\operatorname*{id}$. Hence, $\phi$ and $\psi$ are mutually inverse
isomorphisms between the $\mathfrak{a}$-modules $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W$ and
$\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $. This proves that $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W\cong%
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $ as $\mathfrak{a}$-modules. Moreover, the isomorphism $\phi:\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\rightarrow\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ is canonical and maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$. In other words, Lemma \ref{lem.pushpull} is proven.

\textit{Second proof of Lemma \ref{lem.pushpull}.} For every $\mathfrak{a}%
$-module $Y$, we have%
\begin{align*}
&  \operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W,Y\right) \\
&  =\left(  \underbrace{\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)
\otimes W,Y\right)  }_{\cong\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
}\right)  ^{\mathfrak{a}}\\
&  \cong\left(  \operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
\right)  ^{\mathfrak{a}}=\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{b}}\left(
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right) \\
&  =\left(  \underbrace{\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
}_{\cong\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  V\otimes W,Y\right)
}\right)  ^{\mathfrak{b}}\cong\left(  \operatorname*{Hom}\nolimits_{\mathbb{C}%
}\left(  V\otimes W,Y\right)  \right)  ^{\mathfrak{b}}\\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{b}}\left(  V\otimes W,Y\right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  ,Y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right)  .
\end{align*}
Since this isomorphism is canonical, it gives us a natural isomorphism between
the functors $\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W,-\right)  $ and $\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  ,-\right)  $. By Yoneda's lemma, this yields that $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\cong\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ as $\mathfrak{a}$-modules. It is also rather clear that
the $\mathfrak{a}$-module isomorphism $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W\rightarrow
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $ we have just obtained is canonical.

In order to check that this isomorphism maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$, we must retrace the proof of Yoneda's lemma. This proof proceeds by
evaluating the natural isomorphism $\operatorname*{Hom}\nolimits_{\mathfrak{a}%
}\left(  \left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V\right)  \otimes W,-\right)  \rightarrow\operatorname*{Hom}%
\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}\left(  V\otimes W\right)  ,-\right)  $ at the object
$\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $, thus obtaining an isomorphism%
\[
\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W,\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  \right)
\rightarrow\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  ,\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  \right)  ,
\]
and taking the preimage of $\operatorname*{id}\in\operatorname*{Hom}%
\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}\left(  V\otimes W\right)  ,\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  \right)  $
under this isomorphism. This preimage is our isomorphism $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\rightarrow\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $. Checking that this maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$ is a matter of routine now, and left to the reader. Lemma
\ref{lem.pushpull} is thus proven.

\textit{Proof of Lemma \ref{lem.IndRes}.} Let $\rho:U\left(  \mathfrak{a}%
\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \rightarrow U\left(  \mathfrak{c}\right)  $ be the
$\mathbb{C}$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
U\left(  \mathfrak{a}\right)  \text{ and }\beta\in U\left(  \mathfrak{b}%
\right)
\]
(this is clearly well-defined). By Proposition \ref{prop.U(X)U}, this map
$\rho$ is an isomorphism of left $U\left(  \mathfrak{a}\right)  $-modules and
of right $U\left(  \mathfrak{b}\right)  $-modules. Hence, $U\left(
\mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}U\left(  \mathfrak{b}\right)  \cong U\left(  \mathfrak{c}\right)  $ as left
$U\left(  \mathfrak{a}\right)  $-modules and simultaneously right $U\left(
\mathfrak{b}\right)  $-modules. Now,%
\begin{align*}
\operatorname*{Ind}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}%
}\underbrace{\left(  \operatorname*{Res}\nolimits_{\mathfrak{a}\cap
\mathfrak{b}}^{\mathfrak{b}}N\right)  }_{=N\text{ (as a left }U\left(
\mathfrak{b}\right)  \text{-module)}}  &  =\operatorname*{Ind}%
\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}}\underbrace{N}_{\cong
U\left(  \mathfrak{b}\right)  \otimes_{U\left(  \mathfrak{b}\right)  }N}\cong
U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }\left(  U\left(  \mathfrak{b}\right)  \otimes_{U\left(
\mathfrak{b}\right)  }N\right) \\
&  \cong\underbrace{\left(  U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \right)
}_{\cong U\left(  \mathfrak{c}\right)  }\otimes_{U\left(  \mathfrak{b}\right)
}N\cong U\left(  \mathfrak{c}\right)  \otimes_{U\left(  \mathfrak{b}\right)
}N=\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\\
&  \cong\operatorname*{Res}\nolimits_{\mathfrak{a}}^{\mathfrak{c}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\right)
\ \ \ \ \ \ \ \ \ \ \text{as }\mathfrak{a}\text{-modules.}%
\end{align*}
This proves Lemma \ref{lem.IndRes}.

\textit{Proof of Proposition \ref{prop.invform}.} We have $M_{\lambda}%
^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{\lambda}=\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}%
_{\lambda}$. Thus,%
\begin{align*}
&  \operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+}\otimes
M_{-\lambda}^{-},\mathbb{C}\right) \\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  \underbrace{\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}^{\mathfrak{g}}\mathbb{C}_{\lambda}\right)  \otimes M_{-\lambda}^{-}%
}_{\substack{\cong\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus
\mathfrak{n}_{+}}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes
M_{-\lambda}^{-}\right)  \\\text{(by Lemma \ref{lem.pushpull})}}%
},\mathbb{C}\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes M_{-\lambda}^{-}\right)
,\mathbb{C}\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}\left(  \mathbb{C}_{\lambda}\otimes\underbrace{M_{-\lambda}^{-}%
}_{\substack{=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{-\lambda}\\=\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\mathbb{C}%
_{-\lambda}}},\mathbb{C}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
Frobenius reciprocity}\right) \\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\underbrace{\mathbb{C}_{\lambda}\otimes\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\mathbb{C}%
_{-\lambda}\right)  }_{\substack{\cong\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right)  \\\text{(by Lemma
\ref{lem.pushpull})}}},\mathbb{C}\right)  \cong\operatorname*{Hom}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right)  ,\mathbb{C}\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{h}}^{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}\otimes\mathbb{C}%
_{-\lambda}\right)  ,\mathbb{C}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Lemma \ref{lem.IndRes} (applied to }\mathfrak{c}=\mathfrak{g}%
\text{, }\mathfrak{a}=\mathfrak{h}\oplus\mathfrak{n}_{+}\text{, }%
\mathfrak{b}=\mathfrak{h}\oplus\mathfrak{n}_{-}\text{ and }N=\mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda}\text{)}\\
\text{yields }\operatorname*{Ind}\nolimits_{\mathfrak{h}}^{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \operatorname*{Res}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{-}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \right)  \cong\operatorname*{Res}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}%
}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda
}\right)  \right)  \text{,}\\
\text{which rewrites as }\operatorname*{Ind}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \cong\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right) \\
\text{ (since we are suppressing the }\operatorname*{Res}\text{ functors),}\\
\text{ so that }\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus
\mathfrak{n}_{-}}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes
\mathbb{C}_{-\lambda}\right)  \cong\operatorname*{Ind}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \text{ (as }\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  \text{-modules)}%
\end{array}
\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}}\left(  \mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda},\mathbb{C}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right) \\
&  \cong\mathbb{C}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda}\cong\mathbb{C}\text{ as }%
\mathfrak{h}\text{-modules (this is easy to see)}\right)  .
\end{align*}
This isomorphism $\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(
M_{\lambda}^{+}\otimes M_{-\lambda}^{-},\mathbb{C}\right)  \rightarrow
\mathbb{C}$ is easily seen to map every $\mathfrak{g}$-invariant bilinear form
$\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ (seen as a linear map $M_{\lambda}^{+}\otimes
M_{-\lambda}^{-}\rightarrow\mathbb{C}$) to the value $\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  $. Hence, there exists a unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$ (where we denote this bilinear form by $\left(  \cdot
,\cdot\right)  $), and every other $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ must be a scalar
multiple of this one. This proves Proposition \ref{prop.invform} \textbf{(a)}
and \textbf{(c)}.

Now, for the proof of \textbf{(b)}: Denote by $\left(  \cdot,\cdot\right)  $
the unique $\mathfrak{g}$-invariant bilinear form $M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  =1$. Let us now prove that this bilinear form is
of degree $0$:

Consider the antipode $S:U\left(  \mathfrak{g}\right)  \rightarrow U\left(
\mathfrak{g}\right)  $ of the Hopf algebra $U\left(  \mathfrak{g}\right)  $.
This $S$ is a graded algebra antiautomorphism satisfying $S\left(  x\right)
=-x$ for every $x\in\mathfrak{g}$. It can be explicitly described by
\[
S\left(  x_{1}x_{2}...x_{m}\right)  =\left(  -1\right)  ^{m}x_{m}%
x_{m-1}...x_{1}\ \ \ \ \ \ \ \ \ \ \text{for all }m\in\mathbb{N}\text{ and
}x_{1},x_{2},...,x_{m}\in\mathfrak{g}.
\]


We can easily see by induction (using the $\mathfrak{g}$-invariance of the
bilinear form $\left(  \cdot,\cdot\right)  $) that $\left(  v,aw\right)
=\left(  S\left(  a\right)  v,w\right)  $ for all $v\in M_{\lambda}^{+}$ and
$w\in M_{-\lambda}^{-}$ and $a\in U\left(  \mathfrak{g}\right)  $. In
particular,%
\[
\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(  b\right)
av_{\lambda}^{+},v_{-\lambda}^{-}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a\in U\left(  \mathfrak{g}\right)  \text{ and }b\in U\left(  \mathfrak{g}%
\right)  .
\]
Thus, $\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(
b\right)  av_{\lambda}^{+},v_{-\lambda}^{-}\right)  =0$ whenever $a$ and $b$
are homogeneous elements of $U\left(  \mathfrak{g}\right)  $ satisfying $\deg
b>-\deg a$ (this is because any two homogeneous elements $a$ and $b$ of
$U\left(  \mathfrak{g}\right)  $ satisfying $\deg b>-\deg a$ satisfy $S\left(
b\right)  av_{\lambda}^{+}=0$\ \ \ \ \footnote{\textit{Proof.} Let $a$ and $b$
be homogeneous elements of $U\left(  \mathfrak{g}\right)  $ satisfying $\deg
b>-\deg a$. Then, $\deg b+\deg a>0$, and thus the element $S\left(  b\right)
av_{\lambda}^{+}$ of $M_{\lambda}^{+}$ is a homogeneous element of positive
degree (since $\deg v_{\lambda}^{+}=0$), but the only homogeneous element of
$M_{\lambda}^{+}$ of positive degree is $0$ (since $M_{\lambda}^{+}$ is
concentrated in nonpositive degrees), so that $S\left(  b\right)  av_{\lambda
}^{+}=0$.}). In other words, whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
are integers satisfying $m>-n$, we have $\left(  av_{\lambda}^{+}%
,bv_{-\lambda}^{-}\right)  =0$ for every $a\in U\left(  \mathfrak{g}\right)
\left[  n\right]  $ and $b\in U\left(  \mathfrak{g}\right)  \left[  m\right]
$. Since $M_{\lambda}^{+}\left[  n\right]  =\left\{  av_{\lambda}^{+}%
\ \mid\ a\in U\left(  \mathfrak{g}\right)  \left[  n\right]  \right\}  $ and
$M_{-\lambda}^{-}\left[  m\right]  =\left\{  bv_{-\lambda}^{-}\ \mid\ b\in
U\left(  \mathfrak{g}\right)  \left[  m\right]  \right\}  $, this rewrites as
follows: Whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ are integers
satisfying $m>-n$, we have $\left(  M_{\lambda}^{+}\left[  n\right]
,M_{-\lambda}^{-}\left[  m\right]  \right)  =0$.

Similarly, using the formula $\left(  av,w\right)  =\left(  v,S\left(
a\right)  w\right)  $ (which holds for all $v\in M_{\lambda}^{+}$ and $w\in
M_{-\lambda}^{-}$ and $a\in U\left(  \mathfrak{g}\right)  $), we can show that
whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ are integers satisfying $m<-n$,
we have $\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}^{-}\left[
m\right]  \right)  =0$.

Thus we have $\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}%
^{-}\left[  m\right]  \right)  =0$ whenever $m>-n$ and whenever $m<-n$. Hence,
$\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}^{-}\left[  m\right]
\right)  $ can only be nonzero when $m=-n$. In other words, the form $\left(
\cdot,\cdot\right)  $ has degree $0$. This proves Proposition
\ref{prop.invform}. In this proof, we have not used any properties of
$\mathbb{C}$ other than being a commutative ring over which $\mathfrak{n}_{-}%
$, $\mathfrak{n}_{+}$ and $\mathfrak{h}$ are free modules (the latter was only
used for applying consequences of Poincar\'{e}-Birkhoff-Witt); we thus have
also verified Remark \ref{rmk.invform.1}.

\subsubsection{Generic nondegeneration: Statement of the fact}

We will later (Theorem \ref{thm.verma}) see that the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ is nondegenerate if and only if the $\mathfrak{g}%
$-module $M_{\lambda}^{+}$ is irreducible. This makes the question of when the
form $\left(  \cdot,\cdot\right)  _{\lambda}$ is nondegenerate an important
question to study. It can, in many concrete cases, be answered by
combinatorial computations. But let us first give a general result about how
it is nondegenerate "if $\lambda$ is in sufficiently general position":

\begin{theorem}
\label{thm.invformnondeg}Assume that $\mathfrak{g}$ is a nondegenerate
$\mathbb{Z}$-graded Lie algebra.

Let $\left(  \cdot,\cdot\right)  $ be the form $\left(  \cdot,\cdot\right)
_{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$. (In
other words, let $\left(  \cdot,\cdot\right)  $ be the unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$. Such a form exists and is unique by Proposition
\ref{prop.invform} \textbf{(a)}.)

In every degree, the form $\left(  \cdot,\cdot\right)  $ is nondegenerate for
generic $\lambda$. More precisely: For every $n\in\mathbb{N}$, the restriction
of the form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda
}^{-}\rightarrow\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times
M_{-\lambda}^{-}\left[  n\right]  $ is nondegenerate for generic $\lambda$.

(What "generic $\lambda$" means here may depend on the degree. Thus, we cannot
claim that "for generic $\lambda$, the form $\left(  \cdot,\cdot\right)  $ is
nondegenerate in every degree"!)
\end{theorem}

The proof of this theorem will occupy the rest of Section
\ref{subsect.invform}. While the statement of Theorem \ref{thm.invformnondeg}
itself will never be used in this text, the proof involves several useful
ideas and provides good examples of how to work with Verma modules
computationally; moreover, the main auxiliary result (Proposition
\ref{prop.det.US}) will be used later in the text.

\textbf{[Note: The below proof has been written at nighttime and not been
checked for mistakes. It also has not been checked for redundancies and
readability.]}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Casting bilinear
forms on coinvariant spaces}

Before we start with the proof, a general fact from representation theory:

\begin{lemma}
\label{lem.bilform}Let $k$ be a field, and let $G$ be a finite group. Let
$\Lambda\in k\left[  G\right]  $ be the element $\sum\limits_{g\in G}g$.

Let $V$ and $W$ be representations of $G$ over $k$. Let $B:V\times
W\rightarrow k$ be a $G$-invariant bilinear form.

\textbf{(a)} Then, there exists one and only one bilinear form $B^{\prime
}:V_{G}\times W_{G}\rightarrow k$ satisfying%
\[
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W\text{.}%
\]
(Here, $\overline{v}$ denotes the projection of $v$ onto $V_{G}$, and
$\overline{w}$ denotes the projection of $w$ onto $W_{G}$.)

\textbf{(b)} Assume that $\left\vert G\right\vert $ is invertible in $k$ (in
other words, assume that $\operatorname*{char}k$ is either $0$ or coprime to
$\left\vert G\right\vert $). If the form $B$ is nondegenerate, then this form
$B^{\prime}$ is nondegenerate, too.
\end{lemma}

\textit{Proof of Lemma \ref{lem.bilform}.} Every $h\in G$ satisfies%
\begin{align*}
h\Lambda &  =h\sum\limits_{g\in G}g\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\Lambda=\sum\limits_{g\in G}g\right) \\
&  =\sum\limits_{g\in G}hg=\sum\limits_{i\in G}i\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }i\text{ for }hg\text{ in the sum, since the map}\\
G\rightarrow G,\ g\mapsto hg\text{ is a bijection}%
\end{array}
\right) \\
&  =\sum\limits_{g\in G}g=\Lambda
\end{align*}
and similarly $\Lambda h=\Lambda$.

Also,%
\begin{align*}
\sum\limits_{g\in G}g^{-1}  &  =\sum\limits_{g\in G}%
g\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }g\text{ for }g^{-1}\text{ in the sum, since the
map}\\
G\rightarrow G,\ g\mapsto g^{-1}\text{ is a bijection}%
\end{array}
\right) \\
&  =\Lambda.
\end{align*}


We further notice that the group $G$ acts trivially on the $G$-modules $k$ and
$W_{G}$ (this follows from the definitions of these modules), and thus $G$
acts trivially on $\operatorname*{Hom}\left(  W_{G},k\right)  $ as well.

For every $v\in V$, the map%
\[
W\rightarrow k,\ \ \ \ \ \ \ \ \ \ w\mapsto B\left(  \Lambda v,w\right)
\]
is clearly $G$-equivariant (since it maps $hw$ to%
\begin{align*}
B\left(  \underbrace{\Lambda}_{=h\Lambda}v,hw\right)   &  =B\left(  h\Lambda
v,hw\right)  =B\left(  \Lambda v,w\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }B\text{ is }G\text{-invariant}\right) \\
&  =hB\left(  \Lambda v,w\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}G\text{ acts trivially on }k\right)
\end{align*}
for every $h\in G$ and $w\in W$), and thus descends to a map%
\[
W_{G}\rightarrow k_{G},\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto\overline
{B\left(  \Lambda v,w\right)  }.
\]
Hence, we have obtained a map%
\[
V\rightarrow\operatorname*{Hom}\left(  W_{G},k_{G}\right)
,\ \ \ \ \ \ \ \ \ \ v\mapsto\left(  \overline{w}\mapsto\overline{B\left(
\Lambda v,w\right)  }\right)  .
\]
Since $k_{G}=k$ (because $G$ acts trivially on $k$), this rewrites as a map%
\[
V\rightarrow\operatorname*{Hom}\left(  W_{G},k\right)
,\ \ \ \ \ \ \ \ \ \ v\mapsto\left(  \overline{w}\mapsto B\left(  \Lambda
v,w\right)  \right)  .
\]


This map, too, is $G$-equivariant (since it maps $hv$ to the map%
\begin{align*}
&  \left(  W_{G}\rightarrow k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto B\left(
\underbrace{\Lambda h}_{=\Lambda}v,w\right)  \right) \\
&  =\left(  W_{G}\rightarrow k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto
B\left(  \Lambda v,w\right)  \right)  =h\left(  W_{G}\rightarrow
k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto B\left(  \Lambda v,w\right)  \right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }G\text{ acts trivially on
}\operatorname*{Hom}\left(  W_{G},k\right)  \right)
\end{align*}
for every $h\in G$ and $v\in V$). Thus, it descends to a map%
\[
V_{G}\rightarrow\left(  \operatorname*{Hom}\left(  W_{G},k\right)  \right)
_{G},\ \ \ \ \ \ \ \ \ \ \overline{v}\mapsto\overline{\left(  \overline
{w}\mapsto B\left(  \Lambda v,w\right)  \right)  }.
\]
Since $\left(  \operatorname*{Hom}\left(  W_{G},k\right)  \right)
_{G}=\operatorname*{Hom}\left(  W_{G},k\right)  $ (because $G$ acts trivially
on $\operatorname*{Hom}\left(  W_{G},k\right)  $), this rewrites as a map%
\[
V_{G}\rightarrow\operatorname*{Hom}\left(  W_{G},k\right)
,\ \ \ \ \ \ \ \ \ \ \overline{v}\mapsto\left(  \overline{w}\mapsto B\left(
\Lambda v,w\right)  \right)  .
\]


This map can be rewritten as a bilinear form $V_{G}\times W_{G}\rightarrow k$
which maps $\left(  \overline{v},\overline{w}\right)  $ to $B\left(  \Lambda
v,w\right)  $ for all $v\in V$ and $w\in W$. Since
\begin{align*}
B\left(  \Lambda v,w\right)   &  =B\left(  \sum\limits_{g\in G}gv,w\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Lambda=\sum\limits_{g\in G}g\right)
\\
&  =\sum\limits_{g\in G}B\left(  gv,\underbrace{w}_{=gg^{-1}w}\right)
=\sum\limits_{g\in G}\underbrace{B\left(  gv,gg^{-1}w\right)  }%
_{\substack{=B\left(  v,g^{-1}w\right)  \\\text{(since }B\text{ is
}G\text{-invariant)}}}=\sum\limits_{g\in G}B\left(  v,g^{-1}w\right) \\
&  =B\left(  v,\underbrace{\sum\limits_{g\in G}g^{-1}}_{=\Lambda}w\right)
=B\left(  v,\Lambda w\right)
\end{align*}
for all $v\in V$ and $w\in W$, we have thus proven that there exists a
bilinear form $B^{\prime}:V_{G}\times W_{G}\rightarrow k$ satisfying%
\[
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W\text{.}%
\]
The uniqueness of such a form is self-evident. This proves Lemma
\ref{lem.bilform} \textbf{(a)}.

\textbf{(b)} Assume that $\left\vert G\right\vert $ is invertible in $k$.
Assume that the form $B$ is nondegenerate.

Let $p\in V_{G}$ be such that $B^{\prime}\left(  p,W_{G}\right)  =0$. Since
$p\in V_{G}$, there exists some $v\in V$ such that $p=\overline{v}$. Consider
this $v$. Then, every $w\in W$ satisfies $B\left(  \Lambda v,w\right)  =0$
(since $B\left(  \Lambda v,w\right)  =B\left(  \underbrace{\overline{v}}%
_{=p},\underbrace{\overline{w}}_{\in W_{G}}\right)  \in B^{\prime}\left(
p,W_{G}\right)  =0$). Hence, $\Lambda v=0$ (since $B$ is nondegenerate).

But since the projection of $V$ to $V_{G}$ is a $G$-module map, we have
\begin{align*}
\overline{\Lambda v}  &  =\Lambda\overline{v}=\sum\limits_{g\in G}%
\underbrace{g\overline{v}}_{\substack{=\overline{v}\\\text{(since }G\text{
acts}\\\text{trivially on }V_{G}\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Lambda=\sum\limits_{g\in G}g\right) \\
&  =\sum\limits_{g\in G}\overline{v}=\left\vert G\right\vert \overline{v}.
\end{align*}
Since $\left\vert G\right\vert $ is invertible in $k$, this yields
$\overline{v}=\dfrac{1}{\left\vert G\right\vert }\overline{\Lambda v}=0$
(since $\Lambda v=0$), so that $p=\overline{v}=0$.

We have thus shown that every $p\in V_{G}$ such that $B^{\prime}\left(
p,W_{G}\right)  =0$ must satisfy $p=0$. In other words, the form $B^{\prime}$
is nondegenerate. Lemma \ref{lem.bilform} \textbf{(b)} is proven.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The form $\left(
\cdot,\cdot\right)  _{\lambda}^{\circ}$}

Let us formulate some standing assumptions:

\begin{Convention}
From now on until the end of Section \ref{subsect.invform}, we let
$\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra, and let $\lambda
\in\mathfrak{h}^{\ast}$. We also require that $\mathfrak{g}_{0}$ is abelian
(this is condition \textbf{(2)} of Definition \ref{def.gradLienondeg}), but we
do \textit{not} require $\mathfrak{g}$ to be nondegenerate (unless we
explicitly state this).
\end{Convention}

As vector spaces, $M_{\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\lambda}^{+}\cong U\left(  \mathfrak{n}_{-}\right)  $ (where the
isomorphism maps $v_{\lambda}^{+}$ to $1$) and $M_{-\lambda}^{-}=U\left(
\mathfrak{n}_{+}\right)  v_{-\lambda}^{-}\cong U\left(  \mathfrak{n}%
_{+}\right)  $ (where the isomorphism maps $v_{-\lambda}^{-}$ to $1$). Thus,
the bilinear form $\left(  \cdot,\cdot\right)  =\left(  \cdot,\cdot\right)
_{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$
corresponds to a bilinear form $U\left(  \mathfrak{n}_{-}\right)  \times
U\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$.

For every $n\in\mathbb{N}$, let $\left(  \cdot,\cdot\right)  _{\lambda,n}$
denote the restriction of our form $\left(  \cdot,\cdot\right)  =\left(
\cdot,\cdot\right)  _{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times
M_{-\lambda}^{-}\left[  n\right]  $. In order to prove Theorem
\ref{thm.invformnondeg}, it is enough to prove that for every $n\in\mathbb{N}%
$, when $\mathfrak{g}$ is nondegenerate, this form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ is nondegenerate for generic $\lambda$.

We now introduce a $\mathbb{C}$-bilinear form, which will turn out to be, in
some sense, the "highest term" of the form $\left(  \cdot,\cdot\right)  $ with
respect to $\lambda$ (what this exactly means will be explained in Proposition
\ref{prop.det.US}).

\begin{proposition}
\label{prop.lambda_k}For every $k\in\mathbb{N}$, there exists one and only one
$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ by
\begin{align}
\lambda_{k}\left(  \alpha_{1}\alpha_{2}...\alpha_{k},\beta_{1}\beta
_{2}...\beta_{k}\right)   &  =\sum\limits_{\sigma\in S_{k}}\lambda\left(
\left[  \alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)
\lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  \alpha_{k},\beta_{\sigma\left(  k\right)
}\right]  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }\alpha_{1},\alpha_{2},...,\alpha_{k}%
\in\mathfrak{n}_{-}\text{ and }\beta_{1},\beta_{2},...,\beta_{k}%
\in\mathfrak{n}_{+}. \label{thm.invformnondeg.pf.lambda}%
\end{align}

\end{proposition}

Here, we are using the following convention:

\begin{Convention}
From now on until the end of Section \ref{subsect.invform}, the map
$\lambda:\mathfrak{g}_{0}\rightarrow\mathbb{C}$ is extended to a linear map
$\lambda:\mathfrak{g}\rightarrow\mathbb{C}$ by composing it with the canonical
projection $\mathfrak{g}\rightarrow\mathfrak{g}_{0}$.
\end{Convention}

\textit{First proof of Proposition \ref{prop.lambda_k} (sketched).} Let
$k\in\mathbb{N}$. The value of
\[
\sum\limits_{\sigma\in S_{k}}\lambda\left(  \left[  \alpha_{1},\beta
_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[  \alpha
_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)
\]
depends linearly on each of the $\alpha_{1},\alpha_{2},...,\alpha_{k}$ and
$\beta_{1},\beta_{2},...,\beta_{k}$, and is invariant under any permutation of
the $\alpha_{1},\alpha_{2},...,\alpha_{k}$ and under any permutation of the
$\beta_{1},\beta_{2},...,\beta_{k}$ (as is easily checked). This readily shows
that we can indeed define a $\mathbb{C}$-bilinear form $\lambda_{k}%
:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$ by (\ref{thm.invformnondeg.pf.lambda}).
This proves Proposition \ref{prop.lambda_k}.

\textit{Second proof of Proposition \ref{prop.lambda_k}.} Let $G=S_{k}$. Let
$\Lambda\in\mathbb{C}\left[  G\right]  $ be the element $\sum\limits_{g\in
S_{k}}g=\sum\limits_{\sigma\in S_{k}}\sigma=\sum\limits_{\sigma\in S_{k}%
}\sigma^{-1}$. Let $V$ and $W$ be the canonical representations $\mathfrak{n}%
_{-}^{\otimes k}$ and $\mathfrak{n}_{+}^{\otimes k}$ of $S_{k}$ (where $S_{k}$
acts by permuting the tensorands). Let $B:V\times W\rightarrow\mathbb{C}$ be
the $\mathbb{C}$-bilinear form defined as the $k$-th tensor power of the
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}_{+}%
\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  $. It is easy to see that this form is
$S_{k}$-invariant (in fact, more generally, the $k$-th tensor power of any
bilinear form is $S_{k}$-invariant). Thus, Lemma \ref{lem.bilform}
\textbf{(a)} (applied to $\mathbb{C}$ instead of $k$) yields that there exists
one and only one bilinear form $B^{\prime}:V_{G}\times W_{G}\rightarrow
\mathbb{C}$ satisfying%
\begin{equation}
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W \label{thm.invformnondeg.pf.B'}%
\end{equation}
(where $\overline{v}$ denotes the projection of $v$ onto $V_{G}=V_{S_{k}%
}=S^{k}\left(  \mathfrak{n}_{-}\right)  $, and $\overline{w}$ denotes the
projection of $w$ onto $W_{G}=W_{S_{k}}=S^{k}\left(  \mathfrak{n}_{+}\right)
$). Consider this form $B^{\prime}$. All $\alpha_{1},\alpha_{2},...,\alpha
_{k}\in\mathfrak{n}_{-}$ and $\beta_{1},\beta_{2},...,\beta_{k}\in
\mathfrak{n}_{+}$ satisfy%
\begin{align*}
&  B^{\prime}\left(  \alpha_{1}\alpha_{2}...\alpha_{k},\beta_{1}\beta
_{2}...\beta_{k}\right) \\
&  =B^{\prime}\left(  \overline{\alpha_{1}\otimes\alpha_{2}\otimes
...\otimes\alpha_{k}},\overline{\beta_{1}\otimes\beta_{2}\otimes
...\otimes\beta_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha_{1}\alpha_{2}...\alpha
_{k}=\overline{\alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}}\text{
and }\beta_{1}\beta_{2}...\beta_{k}=\overline{\beta_{1}\otimes\beta_{2}%
\otimes...\otimes\beta_{k}}\right) \\
&  =B\left(  \alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}%
,\Lambda\left(  \beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.invformnondeg.pf.B'}),
applied to }v=\alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}\text{ and
}w=\beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right) \\
&  =B\left(  \alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}%
,\sum\limits_{\sigma\in S_{k}}\beta_{\sigma\left(  1\right)  }\otimes
\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\Lambda=\sum\limits_{\sigma\in S_{k}}\sigma^{-1}\text{ yields
}\Lambda\left(  \beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)
=\sum\limits_{\sigma\in S_{k}}\underbrace{\sigma^{-1}\left(  \beta_{1}%
\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)  }_{=\beta_{\sigma\left(
1\right)  }\otimes\beta_{\sigma\left(  2\right)  }\otimes...\otimes
\beta_{\sigma\left(  k\right)  }}\\
=\sum\limits_{\sigma\in S_{k}}\beta_{\sigma\left(  1\right)  }\otimes
\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }%
\end{array}
\right) \\
&  =\sum\limits_{\sigma\in S_{k}}\underbrace{B\left(  \alpha_{1}\otimes
\alpha_{2}\otimes...\otimes\alpha_{k},\beta_{\sigma\left(  1\right)  }%
\otimes\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }\right)  }_{\substack{=\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)
\\\text{(since }B\text{ is the }k\text{-th tensor power of the }%
\mathbb{C}\text{-bilinear form }\mathfrak{n}_{-}\times\mathfrak{n}%
_{+}\rightarrow\mathbb{C},\ \left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  \text{)}}}\\
&  =\sum\limits_{\sigma\in S_{k}}\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)  .
\end{align*}
Thus, there exists a $\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(
\mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}_{+}\right)
\rightarrow\mathbb{C}$ satisfying (\ref{thm.invformnondeg.pf.lambda}). But
such a form is also seen to be unique. Hence, we can indeed define a
$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ by
(\ref{thm.invformnondeg.pf.lambda}). And, moreover,%
\begin{equation}
\text{this form }\lambda_{k}\text{ is the form }B^{\prime}\text{ satisfying
(\ref{thm.invformnondeg.pf.B'}).} \label{thm.invformnondeg.pf.B'=l_k}%
\end{equation}
Proposition \ref{prop.lambda_k} is thus proven.

\begin{definition}
\label{def.lambda_k}For every $k\in\mathbb{N}$, let $\lambda_{k}:S^{k}\left(
\mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}_{+}\right)
\rightarrow\mathbb{C}$ be the $\mathbb{C}$-bilinear form whose existence and
uniqueness is guaranteed by Proposition \ref{prop.lambda_k}. These forms can
be added together, resulting in a bilinear form $\bigoplus\limits_{k\geq
0}\lambda_{k}:S\left(  \mathfrak{n}_{-}\right)  \times S\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$. It is very easy to see that this form is
of degree $0$ (where the grading on $S\left(  \mathfrak{n}_{-}\right)  $ and
$S\left(  \mathfrak{n}_{+}\right)  $ is not the one that gives the $k$-th
symmetric power the degree $k$ for every $k\in\mathbb{N}$, but is the one
induced by the grading on $\mathfrak{n}_{-}$ and $\mathfrak{n}_{+}$). Denote
this form by $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$.
\end{definition}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Generic nondegeneracy
of $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$}

\begin{lemma}
\label{lem.lambda_k}Let $\lambda\in\mathfrak{h}^{\ast}$ be such that the
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}_{+}%
\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  $ is nondegenerate. Then, the form
$\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$ is nondegenerate.
\end{lemma}

\textit{Proof of Lemma \ref{lem.lambda_k}.} Let $k\in\mathbb{N}$. Introduce
the same notations as in the Second proof of Proposition \ref{prop.lambda_k}.

The $\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}%
_{+}\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto
\lambda\left(  \left[  \alpha,\beta\right]  \right)  $ is nondegenerate. Thus,
the $k$-th tensor power of this form is also nondegenerate (since all tensor
powers of a nondegenerate form are always nondegenerate). But the $k$-th
tensor power of this form is $B$. Thus, $B$ is nondegenerate. Hence, Lemma
\ref{lem.bilform} \textbf{(b)} yields that the form $B^{\prime}$ is
nondegenerate. Due to (\ref{thm.invformnondeg.pf.B'=l_k}), this yields that
the form $\lambda_{k}$ is nondegenerate.

Forget that we fixed $k$. We thus have shown that for every $k\in\mathbb{N}$,
the form $\lambda_{k}$ is nondegenerate. Thus, the direct sum $\bigoplus
\limits_{k\geq0}\lambda_{k}$ of these forms is also nondegenerate. Since
$\bigoplus\limits_{k\geq0}\lambda_{k}=\left(  \cdot,\cdot\right)  _{\lambda
}^{\circ}$, this yields that $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$
is nondegenerate. This proves Lemma \ref{lem.lambda_k}.

For every $n\in\mathbb{N}$, define $\left(  \cdot,\cdot\right)  _{\lambda
,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \times
S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  \rightarrow\mathbb{C}$ to
be the restriction of this form $\left(  \cdot,\cdot\right)  _{\lambda}%
^{\circ}=\bigoplus\limits_{k\geq0}\lambda_{k}:S\left(  \mathfrak{n}%
_{-}\right)  \times S\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$
to $S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \times S\left(
\mathfrak{n}_{+}\right)  \left[  n\right]  $. We now need the following
strengthening of Lemma \ref{lem.lambda_k}:

\begin{lemma}
\label{lem.lambda_k.2}Let $n\in\mathbb{N}$ and $\lambda\in\mathfrak{h}^{\ast}$
be such that the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $. Then, the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must also be nondegenerate.
\end{lemma}

\textit{Proof of Lemma \ref{lem.lambda_k.2}.} For Lemma \ref{lem.lambda_k} to
hold, we did not need $\mathfrak{g}$ to be a graded Lie algebra; we only
needed that $\mathfrak{g}$ is a graded vector space with a well-defined
bilinear map $\left[  \cdot,\cdot\right]  :\mathfrak{g}_{-k}\times
\mathfrak{g}_{k}\rightarrow\mathbb{C}$ for every positive integer $k$. This is
a rather weak condition, and holds not only for $\mathfrak{g}$, but also for
the graded subspace $\mathfrak{g}_{-n}\oplus\mathfrak{g}_{-n+1}\oplus
...\oplus\mathfrak{g}_{n}$ of $\mathfrak{g}$. Denote this graded subspace
$\mathfrak{g}_{-n}\oplus\mathfrak{g}_{-n+1}\oplus...\oplus\mathfrak{g}_{n}$ by
$\mathfrak{g}^{\prime}$, and let $\mathfrak{n}_{-}^{\prime}\oplus
\mathfrak{h}^{\prime}\oplus\mathfrak{n}_{+}^{\prime}$ be its triangular
decomposition (thus, $\mathfrak{n}_{-}^{\prime}=\mathfrak{g}_{-n}%
\oplus\mathfrak{g}_{-n+1}\oplus...\oplus\mathfrak{g}_{-1}$, $\mathfrak{h}%
^{\prime}=\mathfrak{g}_{0}=\mathfrak{h}$ and $\mathfrak{n}_{+}^{\prime
}=\mathfrak{g}_{1}\oplus\mathfrak{g}_{2}\oplus...\oplus\mathfrak{g}_{n}$). The
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}^{\prime}\times\mathfrak{n}%
_{+}^{\prime}\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)
\mapsto\lambda\left(  \left[  \alpha,\beta\right]  \right)  $ is nondegenerate
(because the bilinear form $\mathfrak{g}_{-k}\times\mathfrak{g}_{k}%
\rightarrow\mathbb{C},\ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)  $ is nondegenerate for every $k\in\left\{
1,2,...,n\right\}  $). Hence, by Lemma \ref{lem.lambda_k}, the form $\left(
\cdot,\cdot\right)  _{\lambda}^{\circ}$ \textit{defined for }$\mathfrak{g}%
^{\prime}$ \textit{instead of }$\mathfrak{g}$ is nondegenerate. Since this
form is of degree $0$, the restriction $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}$ of this form to $S\left(  \mathfrak{n}_{-}^{\prime
}\right)  \left[  -n\right]  \times S\left(  \mathfrak{n}_{+}^{\prime}\right)
\left[  n\right]  $ must also be nondegenerate\footnote{This is because if $V$
and $W$ are two graded vector spaces, and $\phi:V\times W\rightarrow
\mathbb{C}$ is a nondegenerate bilinear form of degree $0$, then for every
$n\in\mathbb{Z}$, the restriction of $\phi$ to $V\left[  -n\right]  \times
W\left[  n\right]  $ must also be nondegenerate.}. But since $S\left(
\mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]  =S\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  $\ \ \ \ \footnote{\textit{Proof.} Since
$\mathfrak{n}_{+}=\sum\limits_{i\geq1}\mathfrak{g}_{i}$, we have $S\left(
\mathfrak{n}_{+}\right)  =\sum\limits_{k\in\mathbb{N}}\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{k}\right)  \in\mathbb{N}%
^{k};\\\text{each }i_{j}\geq1}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}%
}...\mathfrak{g}_{i_{k}}$ and thus%
\[
S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  =\sum\limits_{k\in
\mathbb{N}}\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{k}\right)
\in\mathbb{N}^{k};\\\text{each }i_{j}\geq1;\\i_{1}+i_{2}+...+i_{k}%
=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
\]
(since $\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}%
}\subseteq S\left(  \mathfrak{n}_{+}\right)  \left[  i_{1}+i_{2}%
+...+i_{k}\right]  $ for all $\left(  i_{1},i_{2},...,i_{k}\right)
\in\mathbb{N}^{k}$). Similarly,%
\[
S\left(  \mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]  =\sum
\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq1;\\\text{each
}\left\vert i_{j}\right\vert \leq n;\\i_{1}+i_{2}+...+i_{k}=n}}\mathfrak{g}%
_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
\]
(because $\mathfrak{g}^{\prime}$ is obtained from $\mathfrak{g}$ by removing
all $\mathfrak{g}_{i}$ with $\left\vert i\right\vert >n$). Thus,%
\begin{align*}
S\left(  \mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]   &
=\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq
1;\\\text{each }\left\vert i_{j}\right\vert \leq n;\\i_{1}+i_{2}+...+i_{k}%
=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
=\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq
1;\\i_{1}+i_{2}+...+i_{k}=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}%
}...\mathfrak{g}_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we removed the condition }\left(  \text{each }\left\vert
i_{j}\right\vert \leq n\right)  \text{, because it was redundant}\\
\text{(since every }\left(  i_{1},i_{2},...,i_{k}\right)  \in\mathbb{N}%
^{k}\text{ satisfying }i_{1}+i_{2}+...+i_{k}=n\text{ automatically}\\
\text{satisfies }\left(  \text{each }\left\vert i_{j}\right\vert \leq
n\right)  \text{)}%
\end{array}
\right) \\
&  =S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  ,
\end{align*}
qed.} and $S\left(  \mathfrak{n}_{-}^{\prime}\right)  \left[  -n\right]
=S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  $\ \ \ \ \footnote{for
analogous reasons}, this restriction is exactly our form $\left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  \times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
\rightarrow\mathbb{C}$ (in fact, the form is clearly given by the same
formula). Thus we have shown that our form $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]
\times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  \rightarrow
\mathbb{C}$ is nondegenerate. Lemma \ref{lem.lambda_k.2} is proven.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: $\left(  \cdot
,\cdot\right)  _{\lambda}^{\circ}$ is the "highest term" of $\left(
\cdot,\cdot\right)  _{\lambda}$}

Before we go on, let us sketch the direction in which we want to go. We want
to study how, for a fixed $n\in\mathbb{N}$, the form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ changes with $\lambda$. If $V$ and $W$ are two
finite-dimensional vector spaces \textbf{of the same dimension}, and if we
have chosen bases for these two vector spaces $V$ and $W$, then we can
represent every bilinear form $V\times W\rightarrow\mathbb{C}$ as a square
matrix with respect to these two bases, and the bilinear form is nondegenerate
if and only if this matrix has nonzero determinant. This suggests that we
study how the determinant $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ of the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$
with respect to some bases of $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ changes with $\lambda$ (and, in
particular, show that this determinant is nonzero for generic $\lambda$ when
$\mathfrak{g}$ is nondegenerate). Of course, speaking of this determinant
$\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $ only makes
sense when the bases of $M_{\lambda}^{+}\left[  -n\right]  $ and $M_{-\lambda
}^{-}\left[  n\right]  $ have the same size (since only square matrices have
determinants), but this is automatically satisfied if we have\textit{ }%
$\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}_{-n}\right)  $
for every integer $n>0$ (this condition is automatically satisfied when
$\mathfrak{g}$ is a nondegenerate $\mathbb{Z}$-graded Lie algebra, but of
course not only then).

Unfortunately, the spaces $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ themselves change with $\lambda$. Thus,
if we want to pick some bases of $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ for all $\lambda\in\mathfrak{h}^{\ast}$,
we have to pick new bases \textbf{for every }$\lambda$. If we just pick these
bases randomly, then the determinant $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ can change very unpredictably (because the determinant
depends on the choice of bases). Thus, if we want to say something interesting
about how $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $
changes with $\lambda$, then we should specify a reasonable choice of bases
for all $\lambda$. Fortunately, this is not difficult: It is enough to choose
Poincar\'{e}-Birkhoff-Witt bases for $U\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  $ and $U\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
$, and thus obtain bases $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ due to the isomorphisms $M_{\lambda}%
^{+}\left[  -n\right]  \cong U\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  \cong U\left(
\mathfrak{n}_{+}\right)  \left[  n\right]  $. (See Convention
\ref{conv.invformnondeg.bases} for details.) With bases chosen this way, the
determinant $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $
will depend on $\lambda$ polynomially, and we will be able to conclude some
useful properties of this polynomial.

So much for our roadmap. Let us first make a convention:

\begin{Convention}
If $V$ and $W$ are two finite-dimensional vector spaces \textbf{of the same
dimension}, and if we have chosen bases for these two vector spaces $V$ and
$W$, then we can represent every bilinear form $B:V\times W\rightarrow
\mathbb{C}$ as a square matrix with respect to these two bases. The
determinant of this matrix will be denoted by $\det B$ and called the
\textit{determinant of the form }$B$. Of course, this determinant $\det B$
depends on the bases chosen. A change of either basis induces a scaling of
$\det B$ by a \textit{nonzero} scalar. Thus, while the determinant $\det B$
itself depends on the choice of bases, the property of $\det B$ to be zero or
nonzero does \textbf{not} depend on the choice of basis.
\end{Convention}

Let us now look at how the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$ and
its determinant $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
$ depend on $\lambda$. We want to show that this dependence is polynomial. In
order to make sense of this, let us define what we mean by "polynomial" here:

\begin{definition}
\label{def.det.US.poly}Let $V$ be a finite-dimensional vector space. A
function $\phi:V\rightarrow\mathbb{C}$ is said to be a \textit{polynomial
function} (or just to be \textit{polynomial} -- but this is not the same as
being \textit{a polynomial}) if one of the following equivalent conditions holds:

\textbf{(1)} There exist a basis $\left(  \beta_{1},\beta_{2},...,\beta
_{m}\right)  $ of the dual space $V^{\ast}$ and a polynomial $P\in
\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]


\textbf{(2)} For every basis $\left(  \beta_{1},\beta_{2},...,\beta
_{m}\right)  $ of the dual space $V^{\ast}$, there exists a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]


\textbf{(3)} There exist finitely many elements $\beta_{1}$, $\beta_{2}$,
$...$, $\beta_{m}$ of the dual space $V^{\ast}$ and a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]

\end{definition}

Note that this is exactly the meaning of the word "polynomial function" that
is used in Classical Invariant Theory. In our case (where the field is
$\mathbb{C}$), polynomial functions $V\rightarrow\mathbb{C}$ can be identified
with elements of the symmetric algebra $\operatorname*{S}\left(  V^{\ast
}\right)  $, and in some sense are an "obsoleted version" of the
latter.\footnote{The identification of polynomial functions $V\rightarrow
\mathbb{C}$ with elements of the symmetric algebra $\operatorname*{S}\left(
V^{\ast}\right)  $ works similarly over any \textit{infinite} field instead of
$\mathbb{C}$. It breaks down over finite fields, however (because different
elements of $\operatorname*{S}\left(  V^{\ast}\right)  $ may correspond to the
same polynomial function over a finite field).} For our goals, however,
polynomial functions are enough. Let us define the notion of
\textit{homogeneous polynomial functions}:

\begin{definition}
\label{def.det.US.poly.hom}Let $V$ be a finite-dimensional vector space.

\textbf{(a)} Let $n\in\mathbb{N}$. A polynomial function $\phi:V\rightarrow
\mathbb{C}$ is said to be \textit{homogeneous of degree }$n$ if and only if%
\[
\text{every }v\in V\text{ and every }\lambda\in\mathbb{C}\text{ satisfy }%
\phi\left(  \lambda v\right)  =\lambda^{n}\phi\left(  v\right)  .
\]


\textbf{(b)} A polynomial function $\phi:V\rightarrow\mathbb{C}$ is said to be
\textit{homogeneous} if and only if there exists some $n\in\mathbb{N}$ such
that $\phi$ is homogeneous of degree $n$.

\textbf{(c)} It is easy to see that for every polynomial function
$\phi:V\rightarrow\mathbb{C}$, there exists a unique sequence $\left(
\phi_{n}\right)  _{n\in\mathbb{N}}$ of polynomial functions $\phi
_{n}:V\rightarrow\mathbb{C}$ such that all but finitely $n\in\mathbb{N}$
satisfy $\phi_{n}=0$, such that $\phi_{n}$ is homogeneous of degree $n$ for
every $n\in\mathbb{N}$, and such that $\phi=\sum\limits_{n\in\mathbb{N}}%
\phi_{n}$. This sequence is said to be the \textit{graded decomposition} of
$\phi$. For every $n\in\mathbb{N}$, its member $\phi_{n}$ is called the
$n$\textit{-th homogeneous component} of $\phi$. If $N$ is the highest
$n\in\mathbb{N}$ such that $\phi_{n}\neq0$, then $\phi_{N}$ is said to be the
\textit{leading term} of $\phi$.
\end{definition}

Note that Definition \ref{def.det.US.poly.hom} \textbf{(c)} defines the
"leading term" of a polynomial as its highest-degree nonzero homogeneous
component. This "leading term" may (and usually will) contain more than one
monomial, so this notion of a "leading term" is not the same as the notion of
a "leading term" commonly used, e. g., in Gr\"{o}bner basis theory.

We now state the following crucial fact:

\begin{proposition}
\label{prop.det.US}Let $n\in\mathbb{N}$. Assume that $\mathfrak{g}$ is a
nondegenerate $\mathbb{Z}$-graded Lie algebra. As a consequence,
$\dim\mathfrak{h}=\dim\left(  \mathfrak{g}_{0}\right)  \neq\infty$, so that
$\dim\left(  \mathfrak{h}^{\ast}\right)  \neq\infty$, and thus the notion of a
polynomial function $\mathfrak{h}^{\ast}\rightarrow\mathbb{C}$ is well-defined.

There is an appropriate way of choosing bases of the vector spaces $S\left(
\mathfrak{n}_{-}\right)  \left[  -n\right]  $ and $S\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  $ and bases of the vector spaces $M_{\lambda
}^{+}\left[  -n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  $ for all
$\lambda\in\mathfrak{h}^{\ast}$ such that the following holds:

\textbf{(a)} The determinants $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ and $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ (these determinants are defined with respect to
the chosen bases of $S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  $,
$S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  $, $M_{\lambda}%
^{+}\left[  -n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  $) depend
polynomially on $\lambda$. By this, we mean that the functions%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
and%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
are polynomial functions).

\textbf{(b)} The leading term of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]

\end{proposition}

\begin{remark}
We can extend Proposition \ref{prop.det.US} to the case when $\mathfrak{g}$ is
no longer nondegenerate. However, this requires the following changes to
Proposition \ref{prop.det.US}:

Replace the requirement that $\mathfrak{g}$ be nondegenerate by the
requirement that $\mathfrak{g}$ satisfy the conditions \textbf{(1)} and
\textbf{(2)} in Definition \ref{def.gradLienondeg} as well as the condition
that $\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}%
_{-n}\right)  $ for every integer $n>0$ (this condition is a weakening of
condition \textbf{(3)} in Definition \ref{def.gradLienondeg}). Replace the
claim that "The leading term of the polynomial function $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}\right)  $ is $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  $, up to multiplication by a
nonzero scalar" by the claim that "There exists some $k\in\mathbb{N}$ such
that the polynomial function $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ is the $k$-th homogeneous component of the
polynomial function $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda
,n}\right)  $, and such that the $\ell$-th homogeneous component of the
polynomial function $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda
,n}\right)  $ is $0$ for all $\ell>k$". Note that this does not imply that
$\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  $ is not
identically zero, and indeed $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ can be identically zero.
\end{remark}

Before we prove Proposition \ref{prop.det.US}, let us show how it completes
the proof of Theorem \ref{thm.invformnondeg}:

\textit{Proof of Theorem \ref{thm.invformnondeg}.} Fix a positive
$n\in\mathbb{N}$. For generic $\lambda$, the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $ (because
$\mathfrak{g}$ is nondegenerate). Thus, for generic $\lambda$, the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must also be nondegenerate
(by Lemma \ref{lem.lambda_k.2}), so that $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. Since the leading term of
the polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
(by Proposition \ref{prop.det.US}), this yields that $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}\right)  \neq0$ for generic $\lambda$. In
other words, the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$ is
nondegenerate for generic $\lambda$. But this form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ is exactly the restriction of the form $\left(
\cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow
\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times M_{-\lambda}%
^{-}\left[  n\right]  $. Hence, the restriction of the form $\left(
\cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow
\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times M_{-\lambda}%
^{-}\left[  n\right]  $ is nondegenerate for generic $\lambda$. This proves
Theorem \ref{thm.invformnondeg}.

So all that remains to finish the proof of Theorem \ref{thm.invformnondeg} is
verifying Proposition \ref{prop.det.US}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Polynomial maps}

We already defined the notion of a polynomial function in Definition
\ref{def.det.US.poly}. Let us give a definition of a notion of a "polynomial
map" which is tailored for our proof of Theorem \ref{thm.invformnondeg}. I
cannot guarantee that it is the same as what other people call "polynomial
map", but it should be very close.

\begin{definition}
\label{def.det.US.polymap}Let $V$ be a finite-dimensional vector space. Let
$W$ be a vector space. A map $\phi:V\rightarrow W$ is said to be a
\textit{polynomial map} if and only if there exist:

- some $n\in\mathbb{N}$;

- $n$ vectors $w_{1}$, $w_{2}$, $...$, $w_{n}$ in $W$;

- $n$ polynomial functions $P_{1}$, $P_{2}$, $...$, $P_{n}$ from $V$ to
$\mathbb{C}$

such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =\sum
\limits_{i=1}^{n}P_{i}\left(  v\right)  w_{i}.
\]

\end{definition}

Note that it is clear that:

\begin{itemize}
\item If $V$ is a finite-dimensional vector space and $W$ is a vector space,
then any $\mathbb{C}$-linear combination of polynomial maps $V\rightarrow W$
is a polynomial map.

\item If $V$ is a finite-dimensional vector space and $W$ is a $\mathbb{C}%
$-algebra, then any product of polynomial maps $V\rightarrow W$ is a
polynomial map.

\item If $V$ is a finite-dimensional vector space, then polynomial maps
$V\rightarrow\mathbb{C}$ are exactly the same as polynomial functions
$V\rightarrow\mathbb{C}$ (since $\mathbb{C}$-linear combinations of polynomial
functions are polynomial functions).
\end{itemize}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The deformed Lie
algebra $\mathfrak{g}^{\varepsilon}$}

Before we go on, here is a rough plan of how we will attack Proposition
\ref{prop.det.US}:

In order to gain a foothold on $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $, we are going to consider not just one Lie algebra
$\mathfrak{g}$ but a whole family $\left(  \mathfrak{g}^{\varepsilon}\right)
_{\varepsilon\in\mathbb{C}}$ of its "deformations" at the same time. Despite
all of these deformations being isomorphic as Lie algebras with one exception,
they will give us useful information: we will show that the bilinear forms
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ they
induce, in some sense, depend "polynomially" on $\lambda$ and $\varepsilon$.
We will have to restrain from speaking directly of the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ as depending
polynomially on $\lambda$, since this makes no sense (the domain of the
bilinear form $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}$ changes with $\lambda$), but instead we will sample this form
on particular elements of the Verma modules coming from appropriately chosen
Poincar\'{e}-Birkhoff-Witt bases of $U\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $ and $U\left(  \mathfrak{n}_{+}^{\varepsilon}\right)  $. These
sampled values of the form will turn out to depend polynomially on $\lambda$
and $\varepsilon$, and thus the determinant $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\varepsilon}\right)  $ will be a polynomial
function in $\lambda$ and $\varepsilon$. This polynomial function will turn
out to be "homogeneity with respect to $\lambda$ and $\varepsilon^{2}$" (this
is not a standard notion, but see Corollary
\ref{cor.invformnondeg.polynomiality} for what exactly this means in our
context), so that the leading term of $\lambda$ will be the term with smallest
power of $\varepsilon$ (and, as it will turn out, this will be the power
$\varepsilon^{0}$, so this term will be obtainable by setting $\varepsilon$ to
$0$). Once this all is formalized is proven, we will explicitly show that
(more or less) $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}%
}=\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ (again this does not
literally hold but must be correctly interpreted), and we know the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ to be nondegenerate (by
Lemma \ref{lem.lambda_k.2}), so that the form $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{0}}$ will be nondegenerate, and this will quickly
yield the nondegeneracy of $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\varepsilon}\right)  $ for generic $\lambda$ and $\varepsilon$,
and thus the nondegeneracy of $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ for generic $\lambda$.

Now, to the details. Consider the situation of Proposition \ref{prop.det.US}.
In particular, this means that (from now on until the end of Section
\ref{subsect.invform}) the Lie algebra $\mathfrak{g}$ will be assumed nondegenerate.

First, let us define $\mathfrak{g}^{\varepsilon}$.

For every $\varepsilon\in\mathbb{C}$, let us define a new Lie bracket $\left[
\cdot,\cdot\right]  ^{\varepsilon}$ on the vector space $\mathfrak{g}$ by the
formula%
\begin{align}
\left[  x,y\right]  ^{\varepsilon}  &  =\varepsilon\left[  x,y\right]
+\left(  1-\varepsilon\right)  \pi\left(  \left[  x,y\right]  \right)
-\varepsilon\left(  1-\varepsilon\right)  \left[  x,\pi\left(  y\right)
\right]  -\varepsilon\left(  1-\varepsilon\right)  \left[  \pi\left(
x\right)  ,y\right] \label{pf.invformnondeg.g^epsi.1}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}\text{ and }%
y\in\mathfrak{g},\nonumber
\end{align}
where $\pi$ is the canonical projection $\mathfrak{g}\rightarrow
\mathfrak{g}_{0}$. In other words, let us define a new Lie bracket $\left[
\cdot,\cdot\right]  ^{\varepsilon}$ on the vector space $\mathfrak{g}$ by%
\begin{align}
\left[  x,y\right]  ^{\varepsilon}  &  =\varepsilon^{\delta_{n,0}+\delta
_{m,0}+1-\delta_{n+m,0}}\left[  x,y\right]  \label{pf.invformnondeg.g^epsi.2}%
\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z}\text{, }m\in
\mathbb{Z}\text{, }x\in\mathfrak{g}_{n}\text{ and }y\in\mathfrak{g}%
_{m}\nonumber
\end{align}
(note that the right hand side of this equation makes sense since
$1-\delta_{n+m,0}\geq0$ for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}%
$)\ \ \ \ \footnote{Proving that these two definitions of $\left[  \cdot
,\cdot\right]  ^{\varepsilon}$ are equivalent is completely straightforward:
just assume WLOG that $x$ and $y$ are homogeneous, so that $x\in
\mathfrak{g}_{n}$ and $y\in\mathfrak{g}_{m}$ for $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, and distinguish between the following four cases:
\par
\textit{Case 1:} We have $n=0$ and $m=0$.
\par
\textit{Case 2:} We have $n\neq0$ and $m\neq0$ but $n+m=0$.
\par
\textit{Case 3:} We have $n\neq0$, $m\neq0$ and $n+m\neq0$.
\par
\textit{Case 4:} Exactly one of $n$ and $m$ is $0$.
\par
In Case 1, the assumption that $\mathfrak{g}_{0}$ is abelian must be used.}.
It is easy to prove that this Lie bracket $\left[  \cdot,\cdot\right]
^{\varepsilon}$ is antisymmetric and satisfies the Jacobi
identity\footnote{\textit{Proof.} Antisymmetry is obvious. As for the Jacobi
identity, it can be proven in a straightforward way:
\par
We must show the equality $\left[  x,\left[  y,z\right]  ^{\varepsilon
}\right]  ^{\varepsilon}+\left[  y,\left[  z,x\right]  ^{\varepsilon}\right]
^{\varepsilon}+\left[  z,\left[  x,y\right]  ^{\varepsilon}\right]
^{\varepsilon}=0$ for all $x,y,z\in\mathfrak{g}$. Since this equality is
linear in each of $x$, $y$ and $z$, it is enough to prove it for homogeneous
$x,y,z\in\mathfrak{g}$. So let $x,y,z\in\mathfrak{g}$ be homogeneous. Then,
there exist $n,m,p\in\mathbb{Z}$ such that $x\in\mathfrak{g}_{n}$,
$y\in\mathfrak{g}_{m}$ and $z\in\mathfrak{g}_{p}$. Consider these $n$, $m$ and
$p$. Then, by (\ref{pf.invformnondeg.g^epsi.2}) (applied to $y$, $z$, $m$ and
$p$ instead of $x$, $y$, $n$ and $m$), we have $\left[  y,z\right]
^{\varepsilon}=\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\left[
y,z\right]  $. Thus,%
\begin{align*}
&  \left[  x,\left[  y,z\right]  ^{\varepsilon}\right]  ^{\varepsilon}\\
&  =\left[  x,\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\left[
y,z\right]  \right]  ^{\varepsilon}=\varepsilon^{\delta_{m,0}+\delta
_{p,0}+1-\delta_{m+p,0}}\left[  x,\left[  y,z\right]  \right]  ^{\varepsilon
}\\
&  =\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\varepsilon
^{\delta_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because (\ref{pf.invformnondeg.g^epsi.2}) (applied to }\left[
y,z\right]  \text{ and }m+p\text{ instead of }y\text{ and }m\text{) yields}\\
\left[  x,\left[  y,z\right]  \right]  ^{\varepsilon}=\varepsilon
^{\delta_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right]  \text{ (since }\left[  y,z\right]  \in\mathfrak{g}_{m+p}\text{ (since
}y\in\mathfrak{g}_{m}\text{ and }z\in\mathfrak{g}_{p}\text{))}%
\end{array}
\right) \\
&  =\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}+\delta
_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right]  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta
_{n+m+p,0}}\left[  x,\left[  y,z\right]  \right]  .
\end{align*}
Similarly,
\begin{align*}
\left[  y,\left[  z,x\right]  ^{\varepsilon}\right]  ^{\varepsilon}  &
=\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  y,\left[  z,x\right]  \right]  \ \ \ \ \ \ \ \ \ \ \text{and}\\
\left[  z,\left[  x,y\right]  ^{\varepsilon}\right]  ^{\varepsilon}  &
=\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  z,\left[  x,y\right]  \right]  .
\end{align*}
Adding up these three equations yields%
\begin{align*}
&  \left[  x,\left[  y,z\right]  ^{\varepsilon}\right]  ^{\varepsilon}+\left[
y,\left[  z,x\right]  ^{\varepsilon}\right]  ^{\varepsilon}+\left[  z,\left[
x,y\right]  ^{\varepsilon}\right]  ^{\varepsilon}\\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  x,\left[  y,z\right]  \right]  +\varepsilon^{\delta_{n,0}%
+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}}\left[  y,\left[  z,x\right]
\right]  +\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta
_{n+m+p,0}}\left[  z,\left[  x,y\right]  \right] \\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\underbrace{\left(  \left[  x,\left[  y,z\right]  \right]  +\left[  y,\left[
z,x\right]  \right]  +\left[  z,\left[  x,y\right]  \right]  \right)
}_{=0\text{ (since }\mathfrak{g}\text{ is a Lie algebra)}}=0.
\end{align*}
This proves the Jacobi identity for the Lie bracket $\left[  \cdot
,\cdot\right]  ^{\varepsilon}$, qed.} and is graded. Thus, this Lie bracket
$\left[  \cdot,\cdot\right]  ^{\varepsilon}$ defines a graded Lie algebra
structure on $\mathfrak{g}$. Let us denote this Lie algebra by $\mathfrak{g}%
^{\varepsilon}$. Thus, $\mathfrak{g}^{\varepsilon}$ is identical with
$\mathfrak{g}$ as a vector space, but the Lie bracket on $\mathfrak{g}%
^{\varepsilon}$ is $\left[  \cdot,\cdot\right]  ^{\varepsilon}$ rather than
$\left[  \cdot,\cdot\right]  $.

Trivially, $\mathfrak{g}^{1}=\mathfrak{g}$ (this is an actual equality, not
only an isomorphism) and $\left[  \cdot,\cdot\right]  ^{1}=\left[  \cdot
,\cdot\right]  $.

For every $\varepsilon\in\mathbb{C}$, define a $\mathbb{C}$-linear map
$J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow\mathfrak{g}$ by%
\[
J_{\varepsilon}\left(  x\right)  =\varepsilon^{1+\delta_{n,0}}%
x\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ and }%
x\in\mathfrak{g}_{n}.
\]
Then, $J_{\varepsilon}$ is a Lie algebra homomorphism\footnote{\textit{Proof.}
We must show that $J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon
}\right)  =\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(
y\right)  \right]  $ for all $x,y\in\mathfrak{g}$. In order to show this, it
is enough to prove that $J_{\varepsilon}\left(  \left[  x,y\right]
^{\varepsilon}\right)  =\left[  J_{\varepsilon}\left(  x\right)
,J_{\varepsilon}\left(  y\right)  \right]  $ for all homogeneous
$x,y\in\mathfrak{g}$ (because of linearity). So let $x,y\in\mathfrak{g}$ be
homogeneous. Thus, there exist $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ such that
$x\in\mathfrak{g}_{n}$ and $y\in\mathfrak{g}_{m}$. Consider these $n$ and $m$.
Then, $\left[  x,y\right]  \in\mathfrak{g}_{n+m}$. Now, $J_{\varepsilon
}\left(  x\right)  =\varepsilon^{1+\delta_{n,0}}x$ and $J_{\varepsilon}\left(
y\right)  =\varepsilon^{1+\delta_{m,0}}y$ by the definition of $J_{\varepsilon
}$. Thus,%
\[
\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(  y\right)
\right]  =\left[  \varepsilon^{1+\delta_{n,0}}x,\varepsilon^{1+\delta_{m,0}%
}y\right]  =\varepsilon^{1+\delta_{n,0}}\varepsilon^{1+\delta_{m,0}}\left[
x,y\right]  =\varepsilon^{2+\delta_{n,0}+\delta_{m,0}}\left[  x,y\right]  .
\]
Compared with%
\begin{align*}
J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon}\right)   &
=J_{\varepsilon}\left(  \varepsilon^{\delta_{n,0}+\delta_{m,0}+1-\delta
_{n+m,0}}\left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.invformnondeg.g^epsi.2})}\right) \\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+1-\delta_{n+m,0}}%
\underbrace{J_{\varepsilon}\left(  \left[  x,y\right]  \right)  }%
_{\substack{=\varepsilon^{1+\delta_{n+m,0}}\left[  x,y\right]  \\\text{(by the
definition of }J_{\varepsilon}\text{,}\\\text{since }\left[  x,y\right]
\in\mathfrak{g}_{n+m}\text{)}}}=\varepsilon^{\delta_{n,0}+\delta
_{m,0}+1-\delta_{n+m,0}}\varepsilon^{1+\delta_{n+m,0}}\left[  x,y\right] \\
&  =\varepsilon^{2+\delta_{n,0}+\delta_{m,0}}\left[  x,y\right]  ,
\end{align*}
this yields $J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon}\right)
=\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(  y\right)
\right]  $, qed.}. Also, $J_{\varepsilon}$ is a vector space isomorphism when
$\varepsilon\neq0$. Hence, $J_{\varepsilon}$ is a Lie algebra isomorphism when
$\varepsilon\neq0$. Moreover, $J_{1}=\operatorname*{id}$.

For every $\varepsilon\in\mathbb{C}$, we are going to denote by $\mathfrak{n}%
_{-}^{\varepsilon}$, $\mathfrak{n}_{+}^{\varepsilon}$ and $\mathfrak{h}%
^{\varepsilon}$ the vector spaces $\mathfrak{n}_{-}$, $\mathfrak{n}_{+}$ and
$\mathfrak{h}$ \textbf{as Lie subalgebras of }$\mathfrak{g}^{\varepsilon}$.
Note that $\mathfrak{h}^{\varepsilon}=\mathfrak{h}$ as Lie algebras (because
$\mathfrak{h}$ and $\mathfrak{h}^{\varepsilon}$ are abelian Lie algebras), but
the equalities $\mathfrak{n}_{-}^{\varepsilon}=\mathfrak{n}_{-}$ and
$\mathfrak{n}_{+}^{\varepsilon}=\mathfrak{n}_{+}$ hold only as equalities of
vector spaces (unless we are in some rather special situation). Since the
grading of $\mathfrak{g}^{\varepsilon}$ is the same as the grading of
$\mathfrak{g}$, the triangular decomposition of $\mathfrak{g}^{\varepsilon}$
is $\mathfrak{n}_{-}^{\varepsilon}\oplus\mathfrak{h}^{\varepsilon}%
\oplus\mathfrak{n}_{+}^{\varepsilon}$ for every $\varepsilon\in\mathbb{C}$.

Now, we are dealing with several Lie algebras on the same vector space, and we
are going to be dealing with their Verma modules. In order not to confuse
them, let us introduce a notation:

\begin{Convention}
In the following, whenever $\mathfrak{e}$ is a $\mathbb{Z}$-graded Lie
algebra, and $\lambda\in\mathfrak{e}_{0}^{\ast}$, we are going to denote by
$M_{\lambda}^{+\mathfrak{e}}$ the Verma highest-weight module of $\left(
\mathfrak{e},\lambda\right)  $, and we are going to denote by $M_{\lambda
}^{-\mathfrak{e}}$ the Verma lowest-weight module of $\left(  \mathfrak{e}%
,\lambda\right)  $. We will furthermore denote by $v_{\lambda}^{+\mathfrak{e}%
}$ the defining vector of $M_{\lambda}^{+\mathfrak{e}}$, and we will denote by
$v_{\lambda}^{-\mathfrak{e}}$ the defining vector of $M_{\lambda
}^{-\mathfrak{e}}$.

Further, we denote by $\left(  \cdot,\cdot\right)  _{\lambda}^{\mathfrak{e}}$
and $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{e}}$ the forms
$\left(  \cdot,\cdot\right)  _{\lambda}$ and $\left(  \cdot,\cdot\right)
_{\lambda,n}$ defined for the Lie algebra $\mathfrak{e}$ instead of
$\mathfrak{g}$.
\end{Convention}

Thus, for instance, the Verma highest-weight module of $\left(  \mathfrak{g}%
,\lambda\right)  $ (which we have always denoted by $M_{\lambda}^{+}$) can now
be called $M_{\lambda}^{+\mathfrak{g}}$, and thus can be discerned from the
Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$ of
$\left(  \mathfrak{g}^{\varepsilon},\lambda\right)  $.

\begin{Convention}
\label{conv.invformnondeg.bases}For every $n\in\mathbb{N}$, let $\left(
e_{n,i}\right)  _{i\in\left\{  1,2,...,m_{n}\right\}  }$ be a basis of the
vector space $\mathfrak{g}_{n}$ (such a basis exists since $\dim\left(
\mathfrak{g}_{n}\right)  <\infty$). Then, $\left(  e_{n,i}\right)  _{\left(
n,i\right)  \in E}$ is a basis of the vector space $\mathfrak{g}$, where
$E=\left\{  \left(  n,i\right)  \ \mid\ n\in\mathbb{Z};\ i\in\left\{
1,2,...,m_{n}\right\}  \right\}  $.

We totally order the set $E$ lexicographically. Let $\operatorname*{Seq}E$ be
the set of all finite sequences of elements of $E$. For every $\mathbf{i}%
\in\operatorname*{Seq}E$ and every $\varepsilon\in\mathbb{C}$, we define an
element $e_{\mathbf{i}}^{\varepsilon}$ of $U\left(  \mathfrak{g}^{\varepsilon
}\right)  $ by%
\[
e_{\mathbf{i}}^{\varepsilon}=e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell
},i_{\ell}},\ \ \ \ \ \ \ \ \ \ \text{where we write }\mathbf{i}\text{ in the
form }\left(  \left(  n_{1},i_{1}\right)  ,\left(  n_{2},i_{2}\right)
,...,\left(  n_{\ell},i_{\ell}\right)  \right)  .
\]
For every $\mathbf{i}\in\operatorname*{Seq}E$, we define the \textit{length}
$\operatorname*{len}\mathbf{i}$ of $\mathbf{i}$ to be the number of elements
of $\mathbf{i}$ (in other words, we set $\operatorname*{len}\mathbf{i}=\ell$,
where we write $\mathbf{i}$ in the form $\left(  \left(  n_{1},i_{1}\right)
,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
$), and we define the \textit{degree} $\deg\mathbf{i}$ of $\mathbf{i}$ to be
the sum $n_{1}+n_{2}+...+n_{\ell}$, where we write $\mathbf{i}$ in the form
$\left(  \left(  n_{1},i_{1}\right)  ,\left(  n_{2},i_{2}\right)  ,...,\left(
n_{\ell},i_{\ell}\right)  \right)  $. It is clear that $e_{\mathbf{i}%
}^{\varepsilon}\in U\left(  \mathfrak{g}^{\varepsilon}\right)  \left[
\deg\mathbf{i}\right]  $.

Let $\operatorname*{Seq}\nolimits_{+}E$ be the set of all
\textbf{nondecreasing} sequences $\left(  \left(  n_{1},i_{1}\right)  ,\left(
n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
\in\operatorname*{Seq}E$ such that all of $n_{1}$, $n_{2}$, $...$, $n_{\ell}$
are \textbf{positive}. By the Poincar\'{e}-Birkhoff-Witt theorem (applied to
the Lie algebra $\mathfrak{n}_{+}^{\varepsilon}$), the family $\left(
e_{\mathbf{j}}^{\varepsilon}\right)  _{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E}$ is a basis of the vector space $U\left(  \mathfrak{n}%
_{+}^{\varepsilon}\right)  $. Moreover, it is a graded basis, i. e., the
family $\left(  e_{\mathbf{j}}^{\varepsilon}\right)  _{\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ is a basis of the
vector space $U\left(  \mathfrak{n}_{+}^{\varepsilon}\right)  \left[
n\right]  $ for every $n\in\mathbb{Z}$. Hence, $\left(  e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ is a
basis of the vector space $M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[
n\right]  $ for every $n\in\mathbb{Z}$ and $\lambda\in\mathfrak{h}^{\ast}$.

Let $\operatorname*{Seq}\nolimits_{-}E$ be the set of all
\textbf{nonincreasing} sequences $\left(  \left(  n_{1},i_{1}\right)  ,\left(
n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
\in\operatorname*{Seq}E$ such that all of $n_{1}$, $n_{2}$, $...$, $n_{\ell}$
are \textbf{negative}. By the Poincar\'{e}-Birkhoff-Witt theorem (applied to
the Lie algebra $\mathfrak{n}_{-}^{\varepsilon}$), the family $\left(
e_{\mathbf{i}}^{\varepsilon}\right)  _{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E}$ is a basis of the vector space $U\left(  \mathfrak{n}%
_{-}^{\varepsilon}\right)  $. Moreover, it is a graded basis, i. e., the
family $\left(  e_{\mathbf{i}}^{\varepsilon}\right)  _{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ is a basis of the
vector space $U\left(  \mathfrak{n}_{-}^{\varepsilon}\right)  \left[
-n\right]  $ for every $n\in\mathbb{Z}$. Hence, $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ is a
basis of the vector space $M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[
-n\right]  $ for every $n\in\mathbb{Z}$ and $\lambda\in\mathfrak{h}^{\ast}$.

We can define a bijection%
\begin{align*}
E  &  \rightarrow E,\\
\left(  n,i\right)   &  \mapsto\left(  -n,i\right)
\end{align*}
(because $\dim\left(  \mathfrak{g}_{m}\right)  =\dim\left(  \mathfrak{g}%
_{-m}\right)  $ for every $m\in\mathbb{Z}$). This bijection canonically
induces a bijection $\operatorname*{Seq}E\rightarrow\operatorname*{Seq}E$,
which maps $\operatorname*{Seq}\nolimits_{+}E$ to $\operatorname*{Seq}%
\nolimits_{-}E$ and vice versa, and reverses the degree of every sequence
while keeping the length of every sequence invariant. One consequence of this
bijection is that for every $n\in\mathbb{Z}$, the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$. Another consequence is that $\sum
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}\sum
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$.

For every $n\in\mathbb{N}$, we represent the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}:M_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right]  \times M_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]  \rightarrow\mathbb{C}$ by its
matrix with respect to the bases $\left(  e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\right)  _{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ and $\left(
e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ of
$M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right]  $ and
$M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]  $, respectively.
This is the matrix%
\[
\left(  \left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}.
\]
This matrix is a square matrix (since the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$), and its determinant is what we are going to
denote by $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  $.
\end{Convention}

A few words about tensor algebras:

\begin{Convention}
In the following, we let $T$ denote the tensor algebra functor. Hence, for
every vector space $V$, we denote by $T\left(  V\right)  $ the tensor algebra
of $V$.

We notice that $T\left(  V\right)  $ is canonically graded even if $V$ is not.
In fact, $T\left(  V\right)  =\bigoplus\limits_{i\in\mathbb{N}}V^{\otimes i}$,
so that we get a grading on $T\left(  V\right)  $ if we set $V^{\otimes i}$ to
be the $i$-th graded component of $T\left(  V\right)  $. This grading is
called the \textit{tensor length grading} on $T\left(  V\right)  $. It makes
$T\left(  V\right)  $ concentrated in nonnegative degrees.

If $V$ itself is a graded vector space, then we can also grade $T\left(
V\right)  $ by canonically extending the grading on $V$ to $T\left(  V\right)
$ (this means that the degree of a pure tensor is the sum of the degrees of
its tensorands). This grading is called the \textit{internal grading} on
$T\left(  V\right)  $. It is different from the tensor length grading (unless
$V$ is concentrated in degree $1$).

Hence, if $V$ is a graded vector space, then $T\left(  V\right)  $ becomes a
bigraded vector space (i. e., a vector space with two gradings). Let us agree
to denote by $T\left(  V\right)  \left[  n,m\right]  $ the intersection of the
$n$-th graded component in the internal grading with the $m$-th graded
component in the tensor length grading (i. e., with $V^{\otimes m}$).
\end{Convention}

Let us notice that \textbf{as vector spaces}, we have $\mathfrak{g}%
=\mathfrak{g}^{\varepsilon}$, $\mathfrak{n}_{-}=\mathfrak{n}_{-}^{\varepsilon
}$, $\mathfrak{n}_{+}=\mathfrak{n}_{+}^{\varepsilon}$ and $\mathfrak{h}%
=\mathfrak{h}^{\varepsilon}$ for every $\varepsilon\in\mathbb{C}$. Hence,
$T\left(  \mathfrak{g}\right)  =T\left(  \mathfrak{g}^{\varepsilon}\right)  $,
$T\left(  \mathfrak{n}_{-}\right)  =T\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $, $T\left(  \mathfrak{n}_{+}\right)  =T\left(  \mathfrak{n}%
_{+}^{\varepsilon}\right)  $ and $T\left(  \mathfrak{h}\right)  =T\left(
\mathfrak{h}^{\varepsilon}\right)  $.

\begin{definition}
In the following, for every Lie algebra $\mathfrak{a}$ and every element $x\in
T\left(  \mathfrak{a}\right)  $, we denote by $\operatorname*{env}%
\nolimits_{\mathfrak{a}}x$ the projection of $x$ onto the factor algebra
$U\left(  \mathfrak{a}\right)  $ of $T\left(  \mathfrak{a}\right)  $.
\end{definition}

Let us again stress that $T\left(  \mathfrak{g}\right)  =T\left(
\mathfrak{g}^{\varepsilon}\right)  $, so that $T\left(  \mathfrak{g}%
^{\varepsilon}\right)  $ does not depend on $\varepsilon$, whereas $U\left(
\mathfrak{g}^{\varepsilon}\right)  $ does. Hence, if we want to study the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ as it
changes with $\varepsilon$, the easiest thing to do is to study the values of
$\left(  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}a\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}b\right)
v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}$ for fixed $a\in T\left(  \mathfrak{g}\right)  =T\left(
\mathfrak{g}^{\varepsilon}\right)  $ and $b\in T\left(  \mathfrak{g}\right)
=T\left(  \mathfrak{g}^{\varepsilon}\right)  $. Here is the polynomiality
lemma that we want to have:

\begin{lemma}
\label{lem.invformnondeg.polynomiality}Let $\mathbf{i}\in\operatorname*{Seq}E$
and $\mathbf{j}\in\operatorname*{Seq}E$. Then, there exists a polynomial
function $Q_{\mathbf{i},\mathbf{j}}:\mathfrak{h}^{\ast}\times\mathbb{C}%
\rightarrow\mathbb{C}$ such that every $\lambda\in\mathfrak{h}^{\ast}$ and
every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  .
\]

\end{lemma}

To prove this lemma, we show something more general:

\begin{lemma}
\label{lem.invformnondeg.polynomiality2}For every $n\in\mathbb{Z}$ and $c\in
T\left(  \mathfrak{g}\right)  \left[  n\right]  $, there exists a polynomial
map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ such that every $\lambda\in\mathfrak{h}%
^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]

\end{lemma}

To get some intuition about Lemma \ref{lem.invformnondeg.polynomiality2},
recall that the Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ was defined as $U\left(  \mathfrak{g}^{\varepsilon}\right)
\otimes_{U\left(  \mathfrak{h}^{\varepsilon}\oplus\mathfrak{n}_{+}%
^{\varepsilon}\right)  }\mathbb{C}_{\lambda}$, but turned out to be $U\left(
\mathfrak{n}_{-}^{\varepsilon}\right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ (as a vector space), so that every term of the form
$xv_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$ with $x\in U\left(
\mathfrak{g}^{\varepsilon}\right)  $ can be reduced to the form $yv_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}$ with $y\in U\left(  \mathfrak{n}%
_{-}^{\varepsilon}\right)  $. Lemma \ref{lem.invformnondeg.polynomiality2}
says that, if $x$ is given as the projection $\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}c$ of some tensor $c\in T\left(
\mathfrak{g}\right)  \left[  n\right]  $ onto $U\left(  \mathfrak{g}%
^{\varepsilon}\right)  $, then $y$ can be found as the projection of some
tensor $d\left(  \lambda,\varepsilon\right)  \in T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ onto $U\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $ which depends polynomially on $\lambda$ and $\varepsilon$. This is
not particularly surprising, since $y$ is found from $x$ by picking a
tensorial representation\footnote{By a "tensorial representation"\ of $x$, I
mean a tensor $c\in T\left(  \mathfrak{g}\right)  $ such that
$\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c=x$.} of $x$ and
"gradually" stratifying it\footnote{By "stratifying" a tensorial
representation of $x$, I mean writing it as a linear combination of pure
tensors, and whenever such a pure tensor has a negative tensorand (i. e., a
tensorand in $\mathfrak{n}_{-}$) standing directly before a positive tensorand
(i. e., a tensorand in $\mathfrak{n}_{+}$), applying the $xy-yx=\left[
x,y\right]  ^{\varepsilon}$ relations in $U\left(  \mathfrak{g}^{\varepsilon
}\right)  $ to move the negative tensorand past the positive one. As soon as a
positive tensorand hits the right end of the tensor, the tensor can be thrown
away since $\mathfrak{n}_{+}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$. For
instance, in Example \ref{exa.Vir} further below, we compute $L_{1}%
L_{-1}v_{\lambda}^{+}$ by stratifying the tensorial representation
$L_{1}\otimes L_{-1}$ of $L_{1}L_{-1}$, and we compute $L_{1}^{2}L_{-1}%
^{2}v_{\lambda}^{+}$ by stratifying the tensorial representation $L_{1}\otimes
L_{1}\otimes L_{-1}\otimes L_{-1}$ of $L_{1}^{2}L_{-1}^{2}$.}, and the
$\lambda$'s and $\varepsilon$'s which appear during this stratification
process don't appear "randomly", but rather appear at foreseeable places. The
following proof of Lemma \ref{lem.invformnondeg.polynomiality2} will formalize
this idea.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality2}.} First some notations:

If $n\in\mathbb{Z}$, then a tensor $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is said to be $n$\textit{-stratifiable} if there exists a
polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ such that every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]


Lemma \ref{lem.invformnondeg.polynomiality2} states that for every
$n\in\mathbb{Z}$, every tensor $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable.

We will now prove that
\begin{equation}
\text{for every }n\in\mathbb{Z}\text{ and every }m\in\mathbb{N}\text{, every
tensor }c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  \text{ is
}n\text{-stratifiable.} \label{lem.invformnondeg.polynomiality2.ind}%
\end{equation}


Before we start proving this, let us formulate two easy observations about
stratifiable tensors:

\textit{Observation 1:} For any fixed $n$, any $\mathbb{C}$-linear combination
of $n$-stratifiable tensors is $n$-stratifiable. (In fact, we can just take
the corresponding $\mathbb{C}$-linear combination of the corresponding
polynomial maps $d$.)

\textit{Observation 2:} If an integer $n$, a negative integer $\nu$, a vector
$x\in\mathfrak{g}_{\nu}$ and a tensor $y\in T\left(  \mathfrak{g}\right)
\left[  n-\nu\right]  $ are such that $y$ is $\left(  n-\nu\right)
$-stratifiable, then $x\otimes y\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable.\footnote{\textit{Proof of Observation 2.} Let
an integer $n$, a negative integer $\nu$, a vector $x\in\mathfrak{g}_{\nu}$
and a tensor $y\in T\left(  \mathfrak{g}\right)  \left[  n-\nu\right]  $ be
such that $y$ is $\left(  n-\nu\right)  $-stratifiable. Then, there exists a
polynomial map $\widetilde{d}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow
T\left(  \mathfrak{n}_{-}\right)  \left[  n-\nu\right]  $ such that every
$\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}y\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}%
\]
(by the definition of "$\left(  n-\nu\right)  $-stratifiable"). Now, define a
map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ by
\[
d\left(  \lambda,\varepsilon\right)  =x\otimes\widetilde{d}\left(
\lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(
\lambda,\varepsilon\right)  \in\mathfrak{h}^{\ast}\times\mathbb{C}.
\]
(This is well-defined, since $x\in\mathfrak{g}_{\nu}\subseteq\mathfrak{n}_{-}$
(since $\nu$ is negative).) This map $d$ is clearly polynomial (since
$\widetilde{d}$ is a polynomial map), and every $\lambda\in\mathfrak{h}^{\ast
}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\begin{align*}
\underbrace{\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x\otimes y\right)  \right)  }_{=x\cdot\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}y}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}  &  =x\cdot\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}y\right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}_{=\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}%
=\underbrace{x\cdot\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  }_{=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon
}}\left(  x\otimes\widetilde{d}\left(  \lambda,\varepsilon\right)  \right)
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  x\otimes\widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  }_{=d\left(  \lambda,\varepsilon\right)  }\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\end{align*}
Hence, $x\otimes y$ is $n$-stratifiable (by the definition of "$n$%
-stratifiable"). This proves Observation 2.}

We are now going to prove (\ref{lem.invformnondeg.polynomiality2.ind}) by
induction on $m$:

\textit{Induction base:} We have $T\left(  \mathfrak{g}\right)  \left[
n,0\right]  =\mathbb{C}\left[  n\right]  $. Hence, every tensor $c\in T\left(
\mathfrak{g}\right)  \left[  n,0\right]  $ is $\left(  n,0\right)
$-stratifiable (because we can define the polynomial map $d:\mathfrak{h}%
^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}_{-}\right)  \left[
n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =c\ \ \ \ \ \ \ \ \ \ \text{for all
}\left(  \lambda,\varepsilon\right)  \in\mathfrak{h}^{\ast}\times\mathbb{C}%
\]
). In other words, (\ref{lem.invformnondeg.polynomiality2.ind}) is proven for
$m=0$. In other words, the induction base is complete.

\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. We must show that
(\ref{lem.invformnondeg.polynomiality2.ind}) holds for this $m$, using the
assumption that (\ref{lem.invformnondeg.polynomiality2.ind}) holds for $m-1$
instead of $m$.

Let $n\in\mathbb{Z}$. Let $\pi_{n}:T\left(  \mathfrak{g}\right)  \rightarrow
T\left(  \mathfrak{g}\right)  \left[  n\right]  $ denote the canonical
projection of $T\left(  \mathfrak{g}\right)  $ to the $n$-th graded component
with respect to the internal grading.

Let $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $. We must prove
that $c$ is $n$-stratifiable.

We have $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  \subseteq
\mathfrak{g}^{\otimes m}$, and since the $m$-th tensor power is generated by
pure tensors, this yields that $c$ is a $\mathbb{C}$-linear combination of
pure tensors. In other words, $c$ is a $\mathbb{C}$-linear combination of
finitely many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes
x_{m}$ with $x_{1},x_{2},...,x_{m}\in\mathfrak{g}$. We can WLOG assume that,
in each of these pure tensors, the elements $x_{1},x_{2},...,x_{m}$ are
homogeneous (since otherwise we can break each of $x_{1},x_{2},...,x_{m}$ into
homogeneous components, and thus the pure tensors $x_{1}\otimes x_{2}%
\otimes...\otimes x_{m}$ break into smaller pieces which are still pure
tensors). So we can write $c$ as a $\mathbb{C}$-linear combination of finitely
many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
\textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$. If we apply the
projection $\pi_{n}$ to this, then $c$ remains invariant (since $c\in T\left(
\mathfrak{g}\right)  \left[  n,m\right]  \subseteq T\left(  \mathfrak{g}%
\right)  \left[  n\right]  $), and the terms of the form $x_{1}\otimes
x_{2}\otimes...\otimes x_{m}$ with \textbf{homogeneous }$x_{1},x_{2}%
,...,x_{m}\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$ remain invariant as well
(since they also lie in $T\left(  \mathfrak{g}\right)  \left[  n\right]  $),
whereas the terms of the form $x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
\textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ satisfying
$\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)  +...+\deg\left(
x_{m}\right)  \neq n$ are mapped to $0$ (since they lie in graded components
of $T\left(  \mathfrak{g}\right)  $ other than $T\left(  \mathfrak{g}\right)
\left[  n\right]  $). Hence, we write $c$ as a $\mathbb{C}$-linear combination
of finitely many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes
x_{m}$ with \textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$
\textbf{satisfying} $\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)
+...+\deg\left(  x_{m}\right)  =n$.

Therefore, in proving (\ref{lem.invformnondeg.polynomiality2.ind}), we can
WLOG assume that $c$ \textbf{is} a pure tensor of the form $x_{1}\otimes
x_{2}\otimes...\otimes x_{m}$ with homogeneous\textbf{ }$x_{1},x_{2}%
,...,x_{m}\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$ (because, clearly, once Lemma
\ref{lem.invformnondeg.polynomiality2} is proven for certain values of $c\in
T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $, it must clearly also hold
for all their $\mathbb{C}$-linear combinations\footnote{due to Observation
1}). Let us now assume this.

So we have $c=x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
homogeneous\textbf{ }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ satisfying
$\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)  +...+\deg\left(
x_{m}\right)  =n$. We must now prove that $c$ is $n$-stratifiable.

For every $i\in\left\{  1,2,...,m\right\}  $, let $n_{i}$ be the degree of
$x_{i}$ (this is well-defined since $x_{i}$ is homogeneous). Thus, $x_{i}%
\in\mathfrak{g}_{n_{i}}$.

We have%
\[
\deg\left(  x_{2}\right)  +\deg\left(  x_{3}\right)  +...+\deg\left(
x_{m}\right)  =\underbrace{\left(  \deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  \right)  }_{=n}-\underbrace{\deg
\left(  x_{1}\right)  }_{=n_{1}}=n-n_{1},
\]
so that $x_{2}\otimes x_{3}\otimes...\otimes x_{m}\in T\left(  \mathfrak{g}%
\right)  \left[  n-n_{1}\right]  $ and thus $x_{2}\otimes x_{3}\otimes
...\otimes x_{m}\in T\left(  \mathfrak{g}\right)  \left[  n-n_{1},m-1\right]
$. Since we have assumed that (\ref{lem.invformnondeg.polynomiality2.ind})
holds for $m-1$ instead of $m$, we can thus apply
(\ref{lem.invformnondeg.polynomiality2.ind}) to $n-n_{1}$, $m-1$ and
$x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ instead of $n$, $m$ and $c$. We
conclude that $x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ is $\left(
n-n_{1}\right)  $-stratifiable. In other words, there exists a polynomial map
$\widetilde{d}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n-n_{1}\right]  $ such that every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\]


We notice that $c=x_{1}\otimes x_{2}\otimes...\otimes x_{m}$, so that%
\begin{align}
&  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\nonumber\\
&  =x_{1}x_{2}...x_{m}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\underbrace{\left(  x_{2}x_{3}...x_{i-1}x_{i}\cdot
x_{1}\cdot x_{i+1}x_{i+2}...x_{m}-x_{2}x_{3}...x_{i}x_{i+1}\cdot x_{1}\cdot
x_{i+2}x_{i+3}...x_{m}\right)  }_{=x_{2}x_{3}...x_{i-1}x_{i}\left(
x_{1}x_{i+1}-x_{i+1}x_{1}\right)  x_{i+2}x_{i+3}...x_{m}}+x_{2}x_{3}%
...x_{m}\cdot x_{1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the sum }\sum\limits_{i=1}^{m-1}\left(  x_{2}x_{3}...x_{i-1}%
x_{i}\cdot x_{1}\cdot x_{i+1}x_{i+2}...x_{m}-x_{2}x_{3}...x_{i}x_{i+1}\cdot
x_{1}\cdot x_{i+2}x_{i+3}...x_{m}\right) \\
\text{telescopes to }x_{1}x_{2}...x_{m}-x_{2}x_{3}...x_{m}\cdot x_{1}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{i=1}^{m-1}x_{2}x_{3}...x_{i-1}x_{i}\underbrace{\left(
x_{1}x_{i+1}-x_{i+1}x_{1}\right)  }_{\substack{=\left[  x_{1},x_{i+1}\right]
^{\varepsilon}\\\text{(since we are in }U\left(  \mathfrak{g}^{\varepsilon
}\right)  \text{)}}}x_{i+2}x_{i+3}...x_{m}+x_{2}x_{3}...x_{m}\cdot
x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}x_{2}x_{3}...x_{i-1}x_{i}\underbrace{\left[
x_{1},x_{i+1}\right]  ^{\varepsilon}}_{\substack{=\varepsilon^{\delta
_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\left[  x_{1}%
,x_{i+1}\right]  \\\text{(by (\ref{pf.invformnondeg.g^epsi.2}) (applied to
}x_{1}\text{ and }x_{i+1}\text{ instead of }x\text{ and }y\text{),}%
\\\text{since }x_{1}\in\mathfrak{g}_{n_{1}}\text{ and }x_{i+1}\in
\mathfrak{g}_{n_{i+1}}\text{)}}}x_{i+2}x_{i+3}...x_{m}+x_{2}x_{3}...x_{m}\cdot
x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{x_{2}x_{3}...x_{i-1}x_{i}\left[
x_{1},x_{i+1}\right]  x_{i+2}x_{i+3}...x_{m}}_{=\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[  x_{1},x_{i+1}\right]
\otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{x_{2}x_{3}...x_{m}}_{=\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{m}\right)  }\cdot x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{i-1}\otimes
x_{i}\otimes\left[  x_{1},x_{i+1}\right]  \otimes x_{i+2}\otimes
x_{i+3}\otimes...\otimes x_{m}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
x_{1}. \label{pf.invformnondeg.polynomiality2.big}%
\end{align}


Now, for every $i\in\left\{  1,2,...,m-1\right\}  $, denote the element
$x_{2}\otimes x_{3}\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[
x_{1},x_{i+1}\right]  \otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}$
by $c_{i}$. It is easily seen that $c_{i}\in T\left(  \mathfrak{g}\right)
\left[  n,m-1\right]  $. Since \newline$c_{i}=x_{2}\otimes x_{3}%
\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[  x_{1},x_{i+1}\right]
\otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}$, the equality
(\ref{pf.invformnondeg.polynomiality2.big}) rewrites as%
\begin{align}
&  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  c_{i}\right)  +\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{m}\right)  \cdot x_{1}.
\label{pf.invformnondeg.polynomiality2.small}%
\end{align}


For every $i\in\left\{  1,2,...,m-1\right\}  $, we can apply
(\ref{lem.invformnondeg.polynomiality2.ind}) to $n$, $m-1$ and $c_{i}$ instead
of $n$, $m$ and $c$ (since $c_{i}\in T\left(  \mathfrak{g}\right)  \left[
n,m-1\right]  $, and since we have assumed that
(\ref{lem.invformnondeg.polynomiality2.ind}) holds for $m-1$ instead of $m$).
We conclude that $c_{i}$ is $n$-stratifiable for every $i\in\left\{
1,2,...,m-1\right\}  $. In other words, for every $i\in\left\{
1,2,...,m-1\right\}  $, there exists a polynomial map $\widetilde{d_{i}%
}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ such that every $\lambda\in\mathfrak{h}%
^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
c_{i}\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)  \right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]


We now distinguish between three cases:

\textit{Case 1:} We have $n_{1}>0$.

\textit{Case 2:} We have $n_{1}=0$.

\textit{Case 3:} We have $n_{1}<0$.

First, let us consider Case 1. In this case, $n_{1}>0$. Thus, $x_{1}%
\in\mathfrak{n}_{+}$ (since $x_{1}\in\mathfrak{g}_{n_{1}}$), so that
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in\mathfrak{n}_{+}%
^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$ and thus
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$. Now,
(\ref{pf.invformnondeg.polynomiality2.small}) yields%
\begin{align}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}%
+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)
+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \cdot x_{1}\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
\underbrace{x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=0}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality2.small1}%
\end{align}
If we define a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =\sum\limits_{i=1}^{m-1}\varepsilon
^{\delta_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}%
}\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \lambda,\varepsilon\right)
\in\mathfrak{h}^{\ast}\times\mathbb{C},
\]
then this map $d$ is polynomial (since $\widetilde{d_{i}}$ are polynomial maps
for all $i$), and (\ref{pf.invformnondeg.polynomiality2.small1}) becomes%
\begin{align*}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1}%
,0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  }_{=d\left(  \lambda,\varepsilon\right)
}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\end{align*}
Hence, $c$ is $n$-stratifiable (by the definition of "$n$-stratifiable").

Next, let us consider Case 2. In this case, $n_{1}=0$. Thus, $x_{1}%
\in\mathfrak{h}$ (since $x_{1}\in\mathfrak{g}_{n_{1}}$), so that
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\lambda\left(  x_{1}\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$. Now,
(\ref{pf.invformnondeg.polynomiality2.small}) yields%
\begin{align}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}%
+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)
+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \cdot x_{1}\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
\underbrace{x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\lambda\left(
x_{1}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\lambda\left(  x_{1}\right)  \underbrace{\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}_{=\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}+\lambda\left(  x_{1}\right)  \left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(  \lambda,\varepsilon
\right)  +\lambda\left(  x_{1}\right)  \widetilde{d}\left(  \lambda
,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}. \label{pf.invformnondeg.polynomiality2.small2}%
\end{align}
If we define a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =\sum\limits_{i=1}^{m-1}\varepsilon
^{\delta_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}%
}\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)  +\lambda\left(
x_{1}\right)  \widetilde{d}\left(  \lambda,\varepsilon\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \lambda,\varepsilon\right)
\in\mathfrak{h}^{\ast}\times\mathbb{C}%
\]
(this map is well-defined, since $\widetilde{d}\left(  \lambda,\varepsilon
\right)  \in T\left(  \mathfrak{n}_{-}\right)  \left[  n-n_{1}\right]
=T\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $ (due to $n_{1}=0$)),
then this map $d$ is polynomial (since $\widetilde{d_{i}}$ are polynomial maps
for all $i$, and since $\widetilde{d}$ is polynomial), and
(\ref{pf.invformnondeg.polynomiality2.small2}) becomes%
\begin{align*}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1}%
,0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  +\lambda\left(  x_{1}\right)  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  }_{=d\left(  \lambda,\varepsilon\right)
}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\end{align*}
Hence, $c$ is $n$-stratifiable (by the definition of "$n$-stratifiable").

Now, let us consider Case 3. In this case, $n_{1}<0$. Thus, we can apply
Observation 2 to $x_{1}$, $x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ and
$n_{1}$ instead of $x$, $y$ and $\nu$, and conclude that $x_{1}\otimes\left(
x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  $ is $n$-stratifiable (since
$x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ is $\left(  n-n_{1}\right)
$-stratifiable). Since $x_{1}\otimes\left(  x_{2}\otimes x_{3}\otimes
...\otimes x_{m}\right)  =x_{1}\otimes x_{2}\otimes...\otimes x_{m}=c$, this
shows that $c$ is $n$-stratifiable.

Hence, in each of the cases 1, 2 and 3, we have shown that $c$ is
$n$-stratifiable. Thus, $c$ is always $n$-stratifiable.

Forget that we fixed $c$. We thus have shown that $c$ is $n$-stratifiable for
every tensor $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $. In
other words, we have proven (\ref{lem.invformnondeg.polynomiality2.ind}) for
our $m$. This completes the induction step.

Thus, (\ref{lem.invformnondeg.polynomiality2.ind}) is proven by induction.

Now, let $n\in\mathbb{Z}$. Then, every $c\in T\left(  \mathfrak{g}\right)
\left[  n\right]  $ is a $\mathbb{C}$-linear combination of elements of
$T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $ for varying
$m\in\mathbb{N}$ (since $T\left(  \mathfrak{g}\right)  \left[  n\right]
=\bigoplus\limits_{m\in\mathbb{N}}T\left(  \mathfrak{g}\right)  \left[
n,m\right]  $), and thus every $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable (since
(\ref{lem.invformnondeg.polynomiality2.ind}) shows that every element of
$T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $ is $n$-stratifiable, and
due to Observation 1).

Now forget that we fixed $n$. We have thus proven that for every
$n\in\mathbb{Z}$, every $c\in T\left(  \mathfrak{g}\right)  \left[  n\right]
$ is $n$-stratifiable. In other words, we have proved Lemma
\ref{lem.invformnondeg.polynomiality2}.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality}.} We have
$e_{\mathbf{i}}^{\varepsilon}\in U\left(  \mathfrak{g}^{\varepsilon}\right)
\left[  \deg\mathbf{i}\right]  $ and thus $e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{i}\right]  $. Similarly, $e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\in M_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\left[  \deg\mathbf{j}\right]  $. Hence, if
$\deg\mathbf{i}+\deg\mathbf{j}\neq0$, then $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}\in\left(  M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{i}\right]  ,M_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{j}\right]  \right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}=0$ (because the form $\left(  \cdot
,\cdot\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}$ is of degree $0$,
while $\deg\mathbf{i}+\deg\mathbf{j}\neq0$) and thus $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}=0$. Thus, if $\deg\mathbf{i}+\deg\mathbf{j}%
\neq0$, then Lemma \ref{lem.invformnondeg.polynomiality} trivially holds
(because we can then just take $Q_{\mathbf{i},\mathbf{j}}=0$). Thus, for the
rest of the proof of Lemma \ref{lem.invformnondeg.polynomiality}, we can WLOG
assume that we \textit{don't} have $\deg\mathbf{i}+\deg\mathbf{j}\neq0$.
Hence, we have $\deg\mathbf{i}+\deg\mathbf{j}=0$.

Write the sequence $\mathbf{j}$ in the form $\left(  \left(  m_{1}%
,j_{1}\right)  ,\left(  m_{2},j_{2}\right)  ,...,\left(  m_{k},j_{k}\right)
\right)  $. Then, $e_{\mathbf{j}}^{\varepsilon}=e_{m_{1},j_{1}}e_{m_{2},j_{2}%
}...e_{m_{k},j_{k}}$ and $\deg\mathbf{j}=m_{1}+m_{2}+...+m_{k}=m_{k}%
+m_{k-1}+...+m_{1}$.

Since $e_{\mathbf{j}}^{\varepsilon}=e_{m_{1},j_{1}}e_{m_{2},j_{2}}%
...e_{m_{k},j_{k}}$, we have%
\begin{align}
&  \left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{m_{1},j_{1}}e_{m_{2},j_{2}}...e_{m_{k},j_{k}}v_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon
}}\nonumber\\
&  =\left(  -1\right)  ^{k}\left(  e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}v_{\lambda}%
^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we applied the }\mathfrak{g}%
^{\varepsilon}\text{-invariance of the form }\left(  \cdot,\cdot\right)
_{\lambda}^{\mathfrak{g}^{\varepsilon}}\text{ for a total of }k\text{
times}\right) \nonumber\\
&  =\left(  \left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}v_{\lambda}%
^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.1}%
\end{align}


Write the sequence $\mathbf{i}$ in the form $\left(  \left(  n_{1}%
,i_{1}\right)  ,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell
}\right)  \right)  $. Then, $e_{\mathbf{i}}^{\varepsilon}=e_{n_{1},i_{1}%
}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ and $\deg\mathbf{i}=n_{1}%
+n_{2}+...+n_{\ell}$. Now,%
\begin{align}
&  \left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1},j_{1}%
}\cdot\underbrace{e_{\mathbf{i}}^{\varepsilon}}_{=e_{n_{1},i_{1}}%
e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}}\nonumber\\
&  =\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1}%
,j_{1}}\cdot e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\nonumber\\
&  =\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \left(
-1\right)  ^{k}e_{m_{k},j_{k}}\otimes e_{m_{k-1},j_{k-1}}\otimes...\otimes
e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}}\otimes e_{n_{2},i_{2}}\otimes...\otimes
e_{n_{\ell},i_{\ell}}\right)  . \label{pf.invformnondeg.polynomiality.2}%
\end{align}
Denote the tensor $\left(  -1\right)  ^{k}e_{m_{k},j_{k}}\otimes
e_{m_{k-1},j_{k-1}}\otimes...\otimes e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}%
}\otimes e_{n_{2},i_{2}}\otimes...\otimes e_{n_{\ell},i_{\ell}}$ by $c$. Then,
(\ref{pf.invformnondeg.polynomiality.2}) rewrites as%
\begin{equation}
\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1},j_{1}%
}\cdot e_{\mathbf{i}}^{\varepsilon}=\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}c. \label{pf.invformnondeg.polynomiality.3}%
\end{equation}


Since
\begin{align*}
c  &  =\left(  -1\right)  ^{k}e_{m_{k},j_{k}}\otimes e_{m_{k-1},j_{k-1}%
}\otimes...\otimes e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}}\otimes
e_{n_{2},i_{2}}\otimes...\otimes e_{n_{\ell},i_{\ell}}\\
&  \in T\left(  \mathfrak{g}\right)  \left[  m_{k}+m_{k-1}+...+m_{1}%
+n_{1}+n_{2}+...+n_{\ell}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }e_{m_{k},j_{k}}\in\mathfrak{g}_{m_{k}}\text{, }e_{m_{k-1}%
,j_{k-1}}\in\mathfrak{g}_{m_{k-1}}\text{, }...\text{, }e_{m_{1},j_{1}}%
\in\mathfrak{g}_{m_{1}}\\
\text{and }e_{n_{1},i_{1}}\in\mathfrak{g}_{n_{1}}\text{, }e_{n_{2},i_{2}}%
\in\mathfrak{g}_{n_{2}}\text{, }...\text{, }e_{n_{\ell},i_{\ell}}%
\in\mathfrak{g}_{n_{\ell}}%
\end{array}
\right) \\
&  =T\left(  \mathfrak{g}\right)  \left[  0\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{m_{k}+m_{k-1}%
+...+m_{1}}_{=\deg\mathbf{j}}+\underbrace{n_{1}+n_{2}+...+n_{\ell}}%
_{=\deg\mathbf{i}}=\deg\mathbf{j}+\deg\mathbf{i}=\deg\mathbf{i}+\deg
\mathbf{j}=0\right)  ,
\end{align*}
we can apply Lemma \ref{lem.invformnondeg.polynomiality2} to $n=0$. We
conclude that there exists a polynomial map $d:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}_{-}\right)  \left[
0\right]  $ such that every $\lambda\in\mathfrak{h}^{\ast}$ and every
$\varepsilon\in\mathbb{C}$ satisfy%
\begin{equation}
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.4}%
\end{equation}
Since $T\left(  \mathfrak{n}_{-}\right)  \left[  0\right]  =\mathbb{C}$
(because $\mathfrak{n}_{-}$ is concentrated in negative degrees), this
polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  0\right]  $ is a polynomial function
$d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$. Denote this
function $d$ by $Q_{\mathbf{i},\mathbf{j}}$. Then, every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy $d\left(
\lambda,\varepsilon\right)  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda
,\varepsilon\right)  $ and thus $\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon\right)  \right)
=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  $ (since
$Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \in\mathbb{C}$).
Thus, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon
\in\mathbb{C}$ satisfy%
\begin{align}
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}  &  =\underbrace{\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  }_{=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  }v_{\lambda}^{+\mathfrak{g}^{\varepsilon
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.invformnondeg.polynomiality.4}%
)}\right) \nonumber\\
&  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \cdot
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.5}%
\end{align}


Now, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in
\mathbb{C}$ satisfy%
\begin{align*}
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}  &  =\left(
\underbrace{\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}}%
_{\substack{=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}c\\\text{(by (\ref{pf.invformnondeg.polynomiality.3}))}}}v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.invformnondeg.polynomiality.1})}\right) \\
&  =\left(  \underbrace{\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}c\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}%
}_{\substack{=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)
\cdot v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\\text{(by
(\ref{pf.invformnondeg.polynomiality.5}))}}},v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=\left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \cdot v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\\
&  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)
\cdot\underbrace{\left(  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}%
,v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}%
^{\varepsilon}}}_{=1}=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon
\right)  .
\end{align*}
This proves Lemma \ref{lem.invformnondeg.polynomiality}.

We shall now take a closer look at the polynomial function $Q_{\mathbf{i}%
,\mathbf{j}}$ of Lemma \ref{lem.invformnondeg.polynomiality}:

\begin{lemma}
\label{lem.invformnondeg.polynomiality3}Let $\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E$ and $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$. Consider
the polynomial function $Q_{\mathbf{i},\mathbf{j}}:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow\mathbb{C}$ of Lemma
\ref{lem.invformnondeg.polynomiality}. Then, every $\lambda\in\mathfrak{h}%
^{\ast}$ and every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  .
\]

\end{lemma}

Note that Lemma \ref{lem.invformnondeg.polynomiality3} does not really need
the conditions $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ and
$\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$. It is sufficient that
$\mathbf{i}\in\operatorname*{Seq}E$ is such that no element $\left(
n,i\right)  $ of the sequence $\mathbf{i}$ satisfies $n=0$, and that a similar
condition holds for $\mathbf{j}$. But since we will only use Lemma
\ref{lem.invformnondeg.polynomiality3} in the case when $\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E$ and $\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E$, we would not gain much from thus generalizing it.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality3}.} We recall that
the definition of $Q_{\mathbf{i},\mathbf{j}}$ said that%
\begin{equation}
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}\lambda\in\mathfrak{h}^{\ast}\text{ and }\varepsilon\in\mathbb{C}.
\label{pf.invformnondeg.polynomiality3.1}%
\end{equation}


Let $\lambda\in\mathfrak{h}^{\ast}$ be arbitrary, and let $\varepsilon
\in\mathbb{C}$ be nonzero. Since $\varepsilon\neq0$, the Lie algebra
isomorphism $J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow
\mathfrak{g}$ exists and satisfies $\left(  \lambda/\varepsilon^{2}\right)
\circ J_{\varepsilon}=\lambda$. Hence, we have an isomorphism $J_{\varepsilon
}:\left(  \mathfrak{g}^{\varepsilon},\lambda\right)  \rightarrow\left(
\mathfrak{g},\lambda/\varepsilon^{2}\right)  $ in the category of pairs of a
$\mathbb{Z}$-graded Lie algebra and a linear form on its $0$-th graded
component (where the morphisms in this category are defined in the obvious
way). This isomorphism induces a corresponding isomorphism $M_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\rightarrow M_{\lambda/\varepsilon^{2}%
}^{+\mathfrak{g}}$ of Verma modules which sends $xv_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ to $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
x\right)  v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}}$ for every $x\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  $ (where $U\left(  J_{\varepsilon
}\right)  $ is the isomorphism $U\left(  \mathfrak{g}^{\varepsilon}\right)
\rightarrow U\left(  \mathfrak{g}\right)  $ canonically induced by the Lie
algebra isomorphism $\mathfrak{g}^{\varepsilon}\rightarrow\mathfrak{g}$).
Similarly, we get an isomorphism $M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\rightarrow M_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}$ of Verma modules
which sends $yv_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}$ to $\left(  U\left(
J_{\varepsilon}\right)  \right)  \left(  y\right)  v_{-\lambda/\varepsilon
^{2}}^{-\mathfrak{g}}$ for every $y\in U\left(  \mathfrak{g}^{\varepsilon
}\right)  $. Since the bilinear form $\left(  \cdot,\cdot\right)  _{\mu
}^{\mathfrak{e}}$ depends functorially on a $\mathbb{Z}$-graded Lie algebra
$\mathfrak{e}$ and a linear form $\mu:\mathfrak{e}_{0}^{\ast}\rightarrow
\mathbb{C}$, these isomorphisms leave the bilinear form invariant, i. e., we
have%
\[
\left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  x\right)
v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon
}\right)  \right)  \left(  y\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}=\left(
xv_{\lambda}^{+\mathfrak{g}^{\varepsilon}},yv_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}%
\]
for every $x\in U\left(  \mathfrak{g}^{\varepsilon}\right)  $ and $y\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  $. Applied to $x=e_{\mathbf{i}%
}^{\varepsilon}$ and $y=e_{\mathbf{j}}^{\varepsilon}$, this yields%
\begin{equation}
\left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  v_{\lambda/\varepsilon^{2}}%
^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}=\left(
e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  \label{pf.invformnondeg.polynomiality3.2}%
\end{equation}
(by the definition of $Q_{\mathbf{i},\mathbf{j}}$).

But we have $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  =\varepsilon^{\operatorname*{len}%
\mathbf{i}}e_{\mathbf{i}}^{1}$\ \ \ \ \footnote{\textit{Proof.} Write the
sequence $\mathbf{i}$ in the form $\left(  \left(  n_{1},i_{1}\right)
,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
$. Since $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$, all of the numbers
$n_{1}$, $n_{2}$, $...$, $n_{\ell}$ are negative, so that none of them is $0$.
As a consequence, $\delta_{n_{u},0}=0$ for every $u\in\left\{  1,2,...,\ell
\right\}  $. By the definition of $J_{\varepsilon}$, we have%
\begin{align*}
J_{\varepsilon}\left(  e_{n_{u},i_{u}}\right)   &  =\underbrace{\varepsilon
^{1+\delta_{n_{u},0}}}_{\substack{=\varepsilon\\\text{(since }\delta_{n_{u}%
,0}=0\text{)}}}e_{n_{u},i_{u}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}e_{n_{u},i_{u}}\in\mathfrak{g}_{n_{u}}\right) \\
&  =\varepsilon e_{n_{u},i_{u}}%
\end{align*}
for every $u\in\left\{  1,2,...,\ell\right\}  $.
\par
Now, $e_{\mathbf{i}}^{\varepsilon}$ is defined as the product $e_{n_{1},i_{1}%
}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ in $U\left(  \mathfrak{g}%
^{\varepsilon}\right)  $, and $e_{\mathbf{i}}^{1}$ is defined as the product
$e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ in $U\left(
\mathfrak{g}^{1}\right)  $. Hence,%
\begin{align*}
\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  e_{\mathbf{i}%
}^{\varepsilon}\right)   &  =\left(  U\left(  J_{\varepsilon}\right)  \right)
\left(  e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }e_{\mathbf{i}}^{\varepsilon}%
=e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\right) \\
&  =J_{\varepsilon}\left(  e_{n_{1},i_{1}}\right)  J_{\varepsilon}\left(
e_{n_{2},i_{2}}\right)  ...J_{\varepsilon}\left(  e_{n_{\ell},i_{\ell}}\right)
\\
&  =\varepsilon e_{n_{1},i_{1}}\cdot\varepsilon e_{n_{2},i_{2}}\cdot
...\cdot\varepsilon e_{n_{\ell},i_{\ell}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }J_{\varepsilon}\left(  e_{n_{u},i_{u}}\right)  =\varepsilon
e_{n_{u},i_{u}}\text{ for every }u\in\left\{  1,2,...,\ell\right\}  \right) \\
&  =\varepsilon^{\ell}\underbrace{e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell
},i_{\ell}}}_{=e_{\mathbf{i}}^{1}}=\varepsilon^{\ell}e_{\mathbf{i}}%
^{1}=\varepsilon^{\operatorname*{len}\mathbf{i}}e_{\mathbf{i}}^{1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell=\operatorname*{len}%
\mathbf{i}\text{ by the definition of }\operatorname*{len}\mathbf{i}\right)  ,
\end{align*}
qed.} and similarly $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  =\varepsilon^{\operatorname*{len}%
\mathbf{j}}e_{\mathbf{j}}^{1}$. Hence,%
\begin{align*}
&  \left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  v_{\lambda/\varepsilon^{2}}%
^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}\\
&  =\left(  \varepsilon^{\operatorname*{len}\mathbf{i}}e_{\mathbf{i}}%
^{1}v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}},\varepsilon
^{\operatorname*{len}\mathbf{j}}e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}%
}=\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}\left(  e_{\mathbf{i}}^{1}v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}%
},e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}\right)
_{\lambda/\varepsilon^{2}}^{\mathfrak{g}}\\
&  =\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}\underbrace{\left(  e_{\mathbf{i}}^{1}v_{\lambda/\varepsilon^{2}%
}^{+\mathfrak{g}^{1}},e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}^{1}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}^{1}}%
}_{\substack{=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon
^{2},1\right)  \\\text{(by (\ref{pf.invformnondeg.polynomiality3.1}), applied
to }\lambda/\varepsilon^{1}\text{ and }1\text{ instead of }\lambda\text{ and
}\varepsilon\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}%
=\mathfrak{g}^{1}\right) \\
&  =\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  .
\end{align*}
Compared to (\ref{pf.invformnondeg.polynomiality3.2}), this yields
$Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  $. This proves Lemma
\ref{lem.invformnondeg.polynomiality3}.

Here is the consequence of Lemmas \ref{lem.invformnondeg.polynomiality} and
\ref{lem.invformnondeg.polynomiality3} that we will actually use:

\begin{corollary}
\label{cor.invformnondeg.polynomiality}Let $n\in\mathbb{N}$. Let
$\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}%
\sum\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$ (we are using the fact
that $\sum\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}\sum
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$, which we proved above).

Then, there exists a polynomial function $Q_{n}:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow\mathbb{C}$ such that every $\lambda\in
\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\begin{equation}
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  =Q_{n}\left(  \lambda,\varepsilon\right)  .
\label{cor.invformnondeg.polynomiality.1}%
\end{equation}
This function $Q_{n}$ satisfies%
\[
Q_{n}\left(  \lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}%
n}Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }\lambda\in\mathfrak{h}^{\ast}\text{ and every nonzero }\varepsilon
\in\mathbb{C}.
\]

\end{corollary}

\textit{Proof of Corollary \ref{cor.invformnondeg.polynomiality}.} For any
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ satisfying $\deg
\mathbf{i}=-n$, and any $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$
satisfying $\deg\mathbf{j}=n$, consider the polynomial function $Q_{\mathbf{i}%
,\mathbf{j}}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$ of
Lemma \ref{lem.invformnondeg.polynomiality}. Define a polynomial function
$Q_{n}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$ by%
\[
Q_{n}=\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  .
\]
Then, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon
\in\mathbb{C}$ satisfy%
\begin{align*}
Q_{n}\left(  \lambda,\varepsilon\right)   &  =\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  =\det\left(  \left(  \left(  e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon
}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda,n}%
^{\mathfrak{g}^{\varepsilon}}\right)  _{\substack{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Lemma \ref{lem.invformnondeg.polynomiality} yields}\\
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\left(
e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
,n}^{\mathfrak{g}^{\varepsilon}}\\
\text{(since }\deg\mathbf{i}=-n\text{ yields }e_{\mathbf{i}}^{\varepsilon}\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  \left[  -n\right]  \text{ and thus
}e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in
M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right] \\
\text{and similarly }e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\in M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]
\text{)}%
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  .
\end{align*}
We have thus proven that every $\lambda\in\mathfrak{h}^{\ast}$ and every
$\varepsilon\in\mathbb{C}$ satisfy $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{\varepsilon}}\right)  =Q_{n}\left(  \lambda
,\varepsilon\right)  $.

Now, it remains to show that this function $Q_{n}$ satisfies $Q_{n}\left(
\lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}n}Q_{n}\left(
\lambda/\varepsilon^{2},1\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$
and every nonzero $\varepsilon\in\mathbb{C}$. In order to do this, we let
$\lambda\in\mathfrak{h}^{\ast}$ be arbitrary and $\varepsilon\in\mathbb{C}$ be
nonzero. Then,
\begin{equation}
Q_{n}\left(  \lambda,\varepsilon\right)  =\det\left(  \left(  Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  =\det\left(  \left(  \varepsilon^{\operatorname*{len}\mathbf{i}%
}\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)
\label{pf.invformnondeg.polynomiality5.1}%
\end{equation}
(since Lemma \ref{lem.invformnondeg.polynomiality3} yields $Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  $ for all
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ and$\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$).

Now, recall that if we multiply a row of a square matrix by some scalar, then
the determinant of the matrix is also multiplied by the same scalar. A similar
fact holds for the columns. Thus,
\begin{align*}
&  \det\left(  \left(  \varepsilon^{\operatorname*{len}\mathbf{i}}%
\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  =\left(  \prod\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}%
\mathbf{i}}\right)  \cdot\left(  \prod\limits_{\substack{\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon
^{\operatorname*{len}\mathbf{j}}\right)  \cdot\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)
\end{align*}
(because the matrix $\left(  \varepsilon^{\operatorname*{len}\mathbf{i}%
}\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}$ is obtained from the
matrix $\left(  Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon
^{2},1\right)  \right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}$ by multiplying every row $\mathbf{i}$ by
the scalar $\varepsilon^{\operatorname*{len}\mathbf{i}}$ and multiplying every
column $\mathbf{j}$ by the scalar $\varepsilon^{\operatorname*{len}\mathbf{j}%
}$). Hence, (\ref{pf.invformnondeg.polynomiality5.1}) becomes%
\begin{equation}
Q_{n}\left(  \lambda,\varepsilon\right)  =\left(  \prod
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}\mathbf{i}}\right)
\cdot\left(  \prod\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}\right)  \cdot\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  .
\label{pf.invformnondeg.polynomiality5.3}%
\end{equation}


Now, since $\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}%
\mathbf{i}$, we have $\varepsilon^{\operatorname*{LEN}n}=\prod
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}\mathbf{i}}$. Also,
since $\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{j}\in
\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{j}=-n}}\operatorname*{len}%
\mathbf{j}$, we have $\varepsilon^{\operatorname*{LEN}n}=\prod
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}}$. Thus,%
\begin{equation}
\underbrace{\left(  \prod\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}%
\mathbf{i}}\right)  }_{=\varepsilon^{\operatorname*{LEN}n}}\cdot
\underbrace{\left(  \prod\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}\right)  }_{=\varepsilon^{\operatorname*{LEN}n}}=\varepsilon
^{\operatorname*{LEN}n}\varepsilon^{\operatorname*{LEN}n}=\varepsilon
^{2\operatorname*{LEN}n}. \label{pf.invformnondeg.polynomiality5.6}%
\end{equation}


On the other hand,%
\begin{equation}
Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  =\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }Q_{n}=\det\left(
\left(  Q_{\mathbf{i},\mathbf{j}}\right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  \right)  .
\label{pf.invformnondeg.polynomiality5.5}%
\end{equation}
Hence, (\ref{pf.invformnondeg.polynomiality5.3}) becomes%
\begin{align*}
&  Q_{n}\left(  \lambda,\varepsilon\right) \\
&  =\underbrace{\left(  \prod\limits_{\substack{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon
^{\operatorname*{len}\mathbf{i}}\right)  \cdot\left(  \prod
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}}\right)
}_{\substack{=\varepsilon^{2\operatorname*{LEN}n}\\\text{(by
(\ref{pf.invformnondeg.polynomiality5.5}))}}}\cdot\underbrace{\det\left(
\left(  Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)
\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}%
=-n;\ \deg\mathbf{j}=n}}\right)  }_{\substack{=Q_{n}\left(  \lambda
/\varepsilon^{2},1\right)  \\\text{(by
(\ref{pf.invformnondeg.polynomiality5.6}))}}}\\
&  =\varepsilon^{2\operatorname*{LEN}n}\cdot Q_{n}\left(  \lambda
/\varepsilon^{2},1\right)  .
\end{align*}
We have thus proven that $Q_{n}\left(  \lambda,\varepsilon\right)
=\varepsilon^{2\operatorname*{LEN}n}Q_{n}\left(  \lambda/\varepsilon
^{2},1\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$ and every nonzero
$\varepsilon\in\mathbb{C}$. This concludes the proof of Corollary
\ref{cor.invformnondeg.polynomiality}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: On leading terms of
pseudo-homogeneous polynomial maps}

The following lemma about polynomial maps could be an easy exercise in any
algebra text. Unfortunately I do not see a quick way to prove it, so the proof
is going to take a few pages. Reading it will probably waste more of the
reader's time than proving it on her own.

\begin{lemma}
\label{lem.invformnondeg.elemen}Let $V$ be a finite-dimensional $\mathbb{C}%
$-vector space. Let $k\in\mathbb{N}$. Let $\phi:V\times\mathbb{C}%
\rightarrow\mathbb{C}$ be a polynomial function such that every $\lambda\in V$
and every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
\phi\left(  \lambda,\varepsilon\right)  =\varepsilon^{2k}\phi\left(
\lambda/\varepsilon^{2},1\right)  .
\]
Then:

\textbf{(a)} The polynomial function
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,0\right)
\]
is homogeneous of degree $k$.

\textbf{(b)} For every integer $N>k$, the $N$-th homogeneous component of the
polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,1\right)
\]
is zero.

\textbf{(c)} The $k$-th homogeneous component of the polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,1\right)
\]
is the polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,0\right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.invformnondeg.elemen}.} \textbf{(a)} Let
$\left(  v_{1},v_{2},...,v_{n}\right)  $ be a basis of the vector space
$V^{\ast}$. Let $\pi_{V}:V\times\mathbb{C}\rightarrow V$ and $\pi_{\mathbb{C}%
}:V\times\mathbb{C}\rightarrow\mathbb{C}$ be the canonical projections. Then,
$\left(  v_{1}\circ\pi_{V},v_{2}\circ\pi_{V},...,v_{n}\circ\pi_{V}%
,\pi_{\mathbb{C}}\right)  $ is a basis of the vector space $\left(
V\times\mathbb{C}\right)  ^{\ast}$.

Since $\phi$ is a polynomial function, there exists a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{n},X_{n+1}\right]  $ such that every
$w\in V\times\mathbb{C}$ satisfies%
\[
\phi\left(  w\right)  =P\left(  \left(  v_{1}\circ\pi_{V}\right)  \left(
w\right)  ,\left(  v_{2}\circ\pi_{V}\right)  \left(  w\right)  ,...,\left(
v_{n}\circ\pi_{V}\right)  \left(  w\right)  ,\pi_{\mathbb{C}}\left(  w\right)
\right)  .
\]
In other words, every $\left(  \lambda,\varepsilon\right)  \in V\times
\mathbb{C}$ satisfies%
\begin{equation}
\phi\left(  \lambda,\varepsilon\right)  =P\left(  v_{1}\left(  \lambda\right)
,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda\right)  ,\varepsilon
\right)  . \label{pf.invformnondeg.elemen.1}%
\end{equation}


Now, it is easy to see that for every $\left(  x_{1},x_{2},...,x_{n}\right)
\in\mathbb{C}^{n}$ and nonzero $\varepsilon\in\mathbb{C}$, we have
\begin{equation}
P\left(  x_{1},x_{2},...,x_{n},\varepsilon\right)  =\varepsilon^{2k}P\left(
x_{1}/\varepsilon^{2},x_{2}/\varepsilon^{2},...,x_{n}/\varepsilon
^{2},1\right)  . \label{pf.invformnondeg.elemen.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.invformnondeg.elemen.2}).} Let $\left(
x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ be arbitrary, and let
$\varepsilon\in\mathbb{C}$ be nonzero.
\par
Let $\lambda\in V$ be a vector satisfying%
\[
v_{i}\left(  \lambda\right)  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\]
(such a vector $\lambda$ exists since $\left(  v_{1},v_{2},...,v_{n}\right)  $
is a basis of $V^{\ast}$). Then,%
\begin{align*}
P\left(  x_{1},x_{2},...,x_{n},\varepsilon\right)   &  =P\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  ,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
x_{i}=v_{i}\left(  \lambda\right)  \text{ for every }i\in\left\{
1,2,...,n\right\}  \right) \\
&  =\phi\left(  \lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.invformnondeg.elemen.1})}\right) \\
&  =\varepsilon^{2k}\underbrace{\phi\left(  \lambda/\varepsilon^{2},1\right)
}_{\substack{=P\left(  v_{1}\left(  \lambda/\varepsilon^{2}\right)
,v_{2}\left(  \lambda/\varepsilon^{2}\right)  ,...,v_{n}\left(  \lambda
/\varepsilon^{2}\right)  ,1\right)  \\\text{(by
(\ref{pf.invformnondeg.elemen.1}), applied to }\left(  \lambda/\varepsilon
^{2},1\right)  \text{ instead of }\left(  \lambda,\varepsilon\right)
\text{)}}}\\
&  =\varepsilon^{2k}P\left(  v_{1}\left(  \lambda/\varepsilon^{2}\right)
,v_{2}\left(  \lambda/\varepsilon^{2}\right)  ,...,v_{n}\left(  \lambda
/\varepsilon^{2}\right)  ,1\right)  =\varepsilon^{2k}P\left(  x_{1}%
/\varepsilon^{2},x_{2}/\varepsilon^{2},...,x_{n}/\varepsilon^{2},1\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v_{i}\left(  \lambda
/\varepsilon^{2}\right)  =\underbrace{v_{i}\left(  \lambda\right)  }_{=x_{i}%
}/\varepsilon^{2}=x_{i}/\varepsilon^{2}\text{ for every }i\in\left\{
1,2,...,n\right\}  \right)  .
\end{align*}
This proves (\ref{pf.invformnondeg.elemen.2}).}

Now, since $P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{n},X_{n+1}\right]
\cong\left(  \mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  \right)  \left[
X_{n+1}\right]  $, we can write the polynomial $P$ as a polynomial in the
variable $X_{n+1}$ over the ring $\mathbb{C}\left[  X_{1},X_{2},...,X_{n}%
\right]  $. In other words, we can write the polynomial $P$ in the form
$P=\sum\limits_{i\in\mathbb{N}}P_{i}\cdot X_{n+1}^{i}$ for some polynomials
$P_{0}$, $P_{1}$, $P_{2}$, $...$ in $\mathbb{C}\left[  X_{1},X_{2}%
,...,X_{n}\right]  $ such that all but finitely $i\in\mathbb{N}$ satisfy
$P_{i}=0$. Consider these $P_{0}$, $P_{1}$, $P_{2}$, $...$.

Since all but finitely $i\in\mathbb{N}$ satisfy $P_{i}=0$, there exists a
$d\in\mathbb{N}$ such that every integer $i>d$ satisfies $P_{i}=0$. Consider
this $d$. Then, $P=\sum\limits_{i\in\mathbb{N}}P_{i}\cdot X_{n+1}^{i}%
=\sum\limits_{i=0}^{d}P_{i}\cdot X_{n+1}^{i}$ (here, we have removed all the
terms with $i>d$ from the sum, because every integer $i>d$ satisfies $P_{i}=0$
and thus $P_{i}\cdot X_{n+1}^{i}=0$).

For every $i\in\mathbb{N}$ and every $j\in\mathbb{N}$, let $Q_{i,j}$ be the
$j$-th homogeneous component of the polynomial $P_{i}$. Then, $P_{i}%
=\sum\limits_{j\in\mathbb{N}}Q_{i,j}$ for every $i\in\mathbb{N}$, and each
$Q_{i,j}$ is homogeneous of degree $j$.

Hence,%
\begin{equation}
P=\sum\limits_{i\in\mathbb{N}}\underbrace{P_{i}}_{=\sum\limits_{j\in
\mathbb{N}}Q_{i,j}}\cdot X_{n+1}^{i}=\sum\limits_{i\in\mathbb{N}}%
\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}.
\label{pf.invformnondeg.elemen.3a}%
\end{equation}


Now, we are going to show the following fact: We have%
\begin{equation}
Q_{u,v}=0\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  u,v\right)  \in
\mathbb{N}\times\mathbb{N}\text{ which don't satisfy }u+2v=2k.
\label{pf.invformnondeg.elemen.4}%
\end{equation}


\textit{Proof of (\ref{pf.invformnondeg.elemen.4}).} Let $\left(  u,v\right)
\in\mathbb{N}\times\mathbb{N}$ be such that $u+2v\neq2k$. We must prove that
$Q_{u,v}=0$.

If $u>d$, then $Q_{u,v}=0$ is clear (because $Q_{u,v}$ is the $v$-th
homogeneous component of $P_{u}$, but we have $P_{u}=0$ since $u>d$). Hence,
for the rest of the proof of $Q_{u,v}=0$, we can WLOG assume that $u\leq d$.

We have%
\[
P=\sum\limits_{i=0}^{d}\underbrace{P_{i}}_{=\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}}\cdot X_{n+1}^{i}=\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}X_{n+1}^{i}.
\]


Let $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ and
$\varepsilon\in\mathbb{C}\diagdown\left\{  0\right\}  $. Then, $\varepsilon$
is nonzero, and we have%
\begin{align*}
P\left(  x_{1},x_{2},...,x_{n},1/\varepsilon\right)   &  =\sum\limits_{i=0}%
^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)
\underbrace{\left(  1/\varepsilon\right)  ^{i}}_{=\varepsilon^{d-i}%
/\varepsilon^{d}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum
\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}\right) \\
&  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}\left(
x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}/\varepsilon^{d}=\dfrac
{1}{\varepsilon^{d}}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}%
\end{align*}
and%
\begin{align*}
P\left(  \varepsilon^{2}x_{1},\varepsilon^{2}x_{2},...,\varepsilon^{2}%
x_{n},1\right)   &  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\underbrace{Q_{i,j}\left(  \varepsilon^{2}x_{1},\varepsilon^{2}%
x_{2},...,\varepsilon^{2}x_{n}\right)  }_{\substack{=\left(  \varepsilon
^{2}\right)  ^{j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \\\text{(since
}Q_{i,j}\text{ is homogeneous of degree }j\text{)}}}\underbrace{1^{i}}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{i=0}^{d}%
\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}\right) \\
&  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}\underbrace{\left(
\varepsilon^{2}\right)  ^{j}}_{=\varepsilon^{2j}}Q_{i,j}\left(  x_{1}%
,x_{2},...,x_{n}\right)  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  .
\end{align*}
Now,%
\begin{align*}
&  \dfrac{1}{\varepsilon^{d}}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}\\
&  =P\left(  x_{1},x_{2},...,x_{n},1/\varepsilon\right)  =\left(
1/\varepsilon\right)  ^{2k}\underbrace{P\left(  x_{1}/\left(  \dfrac
{1}{\varepsilon}\right)  ^{2},x_{2}/\left(  \dfrac{1}{\varepsilon}\right)
^{2},...,x_{n}/\left(  \dfrac{1}{\varepsilon}\right)  ^{2},1\right)
}_{=P\left(  \varepsilon^{2}x_{1},\varepsilon^{2}x_{2},...,\varepsilon
^{2}x_{n},1\right)  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.invformnondeg.elemen.2}),
applied to }1/\varepsilon\text{ instead of }\varepsilon\right) \\
&  =\left(  1/\varepsilon\right)  ^{2k}\sum\limits_{i=0}^{d}\sum
\limits_{j\in\mathbb{N}}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2}%
,...,x_{n}\right)  ,
\end{align*}
so that%
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}=\varepsilon
^{d}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}\varepsilon^{2j}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  .
\]
For fixed $\varepsilon$, this is a polynomial identity in $\left(  x_{1}%
,x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$. Since it holds for all $\left(
x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ (as we just have shown), it
thus must hold as a formal identity, i. e., we must have%
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\varepsilon^{d-i}=\varepsilon^{d}\sum\limits_{i=0}^{d}\sum
\limits_{j\in\mathbb{N}}\varepsilon^{2j}Q_{i,j}\ \ \ \ \ \ \ \ \ \ \text{in
}\mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  .
\]
Let us take the $v$-th homogeneous components of both sides of this equation.
Since each $Q_{i,j}$ is homogeneous of degree $j$, this amounts to removing
all $Q_{i,j}$ with $j\neq v$, and leaving the $Q_{i,j}$ with $j=v$ unchanged.
Thus, we obtain%
\begin{equation}
\varepsilon^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\varepsilon^{d-i}=\varepsilon
^{d}\sum\limits_{i=0}^{d}\varepsilon^{2v}Q_{i,v}\ \ \ \ \ \ \ \ \ \ \text{in
}\mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  .
\label{pf.invformnondeg.elemen.6}%
\end{equation}


Now, let $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ be
arbitrary again. Then, evaluating the identity
(\ref{pf.invformnondeg.elemen.6}) at $\left(  X_{1},X_{2},...,X_{n}\right)
=\left(  x_{1},x_{2},...,x_{n}\right)  $, we obtain
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}%
\right)  \varepsilon^{d-i}=\varepsilon^{d}\sum\limits_{i=0}^{d}\varepsilon
^{2v}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}\right)  .
\]
For fixed $\left(  x_{1},x_{2},...,x_{n}\right)  $, this is a polynomial
identity in $\varepsilon$ (since $d-i\geq0$ for all $i\in\left\{
0,1,...,d\right\}  $). Since it holds for all nonzero $\varepsilon
\in\mathbb{C}$ (as we just have shown), it thus must hold as a formal identity
(since any polynomial in one variable which evaluates to zero at all nonzero
complex numbers must be the zero polynomial). In other words, we must have%
\[
E^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}\right)
E^{d-i}=E^{d}\sum\limits_{i=0}^{d}E^{2v}Q_{i,v}\left(  x_{1},x_{2}%
,...,x_{n}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }\mathbb{C}\left[  E\right]
\]
(where $\mathbb{C}\left[  E\right]  $ denotes the polynomial ring over
$\mathbb{C}$ in one variable $E$). Let us compare the coefficients of
$E^{2k+d-u}$ on both sides of this equation: The coefficient of $E^{2k+d-u}$
on the left hand side of this equation is clearly $Q_{u,v}\left(  x_{1}%
,x_{2},...,x_{n}\right)  $, while the coefficient of $E^{2k+d-u}$ on the right
hand side is $0$ (in fact, the only coefficient on the right hand side of the
equation which is not trivially zero is the coefficient of $E^{d+2v}$, but
$d+2v\neq2k+d-u$ (since $u+2v\neq2k$ and thus $2v\neq2k-u$)). Hence,
comparison yields $Q_{u,v}\left(  x_{1},x_{2},...,x_{n}\right)  =0$. Since
this holds for all $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$,
we thus obtain $Q_{u,v}=0$ (because any polynomial which vanishes on the whole
$\mathbb{C}^{n}$ must be the zero polynomial). This proves
(\ref{pf.invformnondeg.elemen.4}).

Now, (\ref{pf.invformnondeg.elemen.3a}) rewrites as%
\begin{align*}
P  &  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}Q_{i,j}%
X_{n+1}^{i}=\sum\limits_{u\in\mathbb{N}}\sum\limits_{v\in\mathbb{N}}%
Q_{u,v}X_{n+1}^{u}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the
indices }i\text{ and }j\text{ as }u\text{ and }v\right) \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{N}\times\mathbb{N}}%
Q_{u,v}X_{n+1}^{u}=\sum\limits_{\substack{\left(  u,v\right)  \in
\mathbb{N}\times\mathbb{N};\\u+2v=2k}}Q_{u,v}X_{n+1}^{u}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we removed from our sum all terms for }\left(  u,v\right)
\in\mathbb{N}\times\mathbb{N}\text{ which}\\
\text{don't satisfy }u+2v=2k\text{ (because (\ref{pf.invformnondeg.elemen.4})
shows that these terms}\\
\text{don't contribute anything to the sum)}%
\end{array}
\right) \\
&  =\sum\limits_{v=0}^{k}Q_{2k-2v,v}X_{n+1}^{2k-2v}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we substituted }\left(  2k-2v,v\right)  \text{ for }\left(
u,v\right)  \text{ in the sum}\right)  .
\end{align*}


Now, for every $v\in\left\{  0,1,...,k\right\}  $, let $\psi_{v}%
:V\rightarrow\mathbb{C}$ be the polynomial map defined by%
\[
\psi_{v}\left(  \lambda\right)  =Q_{2k-2v,v}\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in V.
\]
Then, $\psi_{v}$ is homogeneous of degree $v$ (since $Q_{2k-2v,v}$ is
homogeneous of degree $v$). In particular, this yields that $\psi_{k}$ is
homogeneous of degree $k$.

Every $\left(  \lambda,\varepsilon\right)  \in V\times\mathbb{C}$ satisfies%
\begin{align}
\phi\left(  \lambda,\varepsilon\right)   &  =P\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  ,\varepsilon\right) \nonumber\\
&  =\sum\limits_{v=0}^{k}\underbrace{Q_{2k-2v,v}\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  \right)  }_{=\psi_{v}\left(  \lambda\right)  }\varepsilon
^{2k-2v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{v=0}%
^{k}Q_{2k-2v,v}X_{n+1}^{2k-2v}\right) \nonumber\\
&  =\sum\limits_{v=0}^{k}\psi_{v}\left(  \lambda\right)  \varepsilon^{2k-2v}.
\label{pf.invformnondeg.elemen.10}%
\end{align}
Applied to $\varepsilon=0$, this yields%
\[
\phi\left(  \lambda,0\right)  =\sum\limits_{v=0}^{k}\psi_{v}\left(
\lambda\right)  0^{2k-2v}=\psi_{k}\left(  \lambda\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0^{2k-2v}=0\text{ for all
}v<k\right)
\]
for every $\lambda\in V$. Hence, the polynomial function $V\rightarrow
\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,0\right)  $ equals the
polynomial function $\psi_{k}$, and thus is homogeneous of degree $k$ (since
$\psi_{k}$ is homogeneous of degree $k$). This proves Lemma
\ref{lem.invformnondeg.elemen} \textbf{(a)}.

Applying (\ref{pf.invformnondeg.elemen.10}) to $\varepsilon=1$, we obtain%
\[
\phi\left(  \lambda,1\right)  =\sum\limits_{v=0}^{k}\psi_{v}\left(
\lambda\right)  \underbrace{1^{2k-2v}}_{=1}=\sum\limits_{v=0}^{k}\psi
_{v}\left(  \lambda\right)  .
\]
Hence, the polynomial function $V\rightarrow\mathbb{C},\ \lambda\mapsto
\phi\left(  \lambda,1\right)  $ equals the sum $\sum\limits_{v=0}^{k}\psi_{v}%
$. Since we know that the polynomial function $\psi_{v}$ is homogeneous of
degree $v$ for every $v\in\left\{  0,1,...,k\right\}  $, this yields that, for
every integer $N>k$, the $N$-th homogeneous component of the polynomial
function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,1\right)
$ is zero. This proves Lemma \ref{lem.invformnondeg.elemen} \textbf{(b)}.

Finally, recall that the polynomial function $V\rightarrow\mathbb{C}%
,\ \lambda\mapsto\phi\left(  \lambda,1\right)  $ equals the sum $\sum
\limits_{v=0}^{k}\psi_{v}$, and the polynomial function $\psi_{v}$ is
homogeneous of degree $v$ for every $v\in\left\{  0,1,...,k\right\}  $. Hence,
for every $v\in\left\{  0,1,...,k\right\}  $, the $v$-th homogeneous component
of the polynomial function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(
\lambda,1\right)  $ is $\psi_{v}$. In particular, the $k$-th homogeneous
component of the polynomial function $V\rightarrow\mathbb{C},\ \lambda
\mapsto\phi\left(  \lambda,1\right)  $ is $\psi_{k}$. Since $\psi_{k}$ equals
the function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(
\lambda,0\right)  $, this rewrites as follows: The $k$-th homogeneous
component of the polynomial function $V\rightarrow\mathbb{C},\ \lambda
\mapsto\phi\left(  \lambda,1\right)  $ is the function $V\rightarrow
\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,0\right)  $. This proves Lemma
\ref{lem.invformnondeg.elemen} \textbf{(c)}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The Lie algebra
$\mathfrak{g}^{0}$}

Consider the polynomial function $Q_{n}$ of Corollary
\ref{cor.invformnondeg.polynomiality}. Due to Corollary
\ref{cor.invformnondeg.polynomiality}, it satisfies the condition of Lemma
\ref{lem.invformnondeg.elemen} for $k=\operatorname*{LEN}n$. Hence, Lemma
\ref{lem.invformnondeg.elemen} suggests that we study the Lie algebra
$\mathfrak{g}^{0}$, since this will show us what the function $\mathfrak{h}%
^{\ast}\rightarrow\mathbb{C},$ $\lambda\mapsto Q_{n}\left(  \lambda,0\right)
$ looks like.

First, let us reformulate the definition of $\mathfrak{g}^{0}$ as follows: As
a vector space, $\mathfrak{g}^{0}=\mathfrak{g}$, but the bracket on
$\mathfrak{g}^{0}$ is given by%
\begin{equation}
\left[  \cdot,\cdot\right]  ^{0}:\mathfrak{g}_{i}\otimes\mathfrak{g}%
_{j}\rightarrow\mathfrak{g}_{i+j}\text{ }\text{is }\left\{
\begin{array}
[c]{c}%
\text{zero if }i+j\neq0\text{;}\\
\text{the Lie bracket }\left[  \cdot,\cdot\right]  \text{ of }\mathfrak{g}%
\text{ if }i+j=0
\end{array}
\right.  . \label{prop.det.US.pf.-1}%
\end{equation}


It is very easy to see (from this) that $\left[  \mathfrak{n}_{-}%
,\mathfrak{n}_{-}\right]  ^{0}=0$, $\left[  \mathfrak{n}_{+},\mathfrak{n}%
_{+}\right]  ^{0}=0$, $\left[  \mathfrak{n}_{-},\mathfrak{n}_{+}\right]
^{0}=\left[  \mathfrak{n}_{+},\mathfrak{n}_{-}\right]  ^{0}\subseteq
\mathfrak{h}$ and that $\mathfrak{h}\subseteq Z\left(  \mathfrak{g}%
^{0}\right)  $.

We notice that $\mathfrak{n}_{-}^{0}=\mathfrak{n}_{-}$, $\mathfrak{n}_{+}%
^{0}=\mathfrak{n}_{+}$ and $\mathfrak{h}^{0}=\mathfrak{h}$ as vector spaces.

Since $\left[  \mathfrak{n}_{-}^{0},\mathfrak{n}_{-}^{0}\right]  ^{0}=\left[
\mathfrak{n}_{-},\mathfrak{n}_{-}\right]  ^{0}=0$, the Lie algebra
$\mathfrak{n}_{-}^{0}$ is abelian, so that $U\left(  \mathfrak{n}_{-}%
^{0}\right)  =S\left(  \mathfrak{n}_{-}^{0}\right)  =S\left(  \mathfrak{n}%
_{-}\right)  $. Similarly, $U\left(  \mathfrak{n}_{+}^{0}\right)  =S\left(
\mathfrak{n}_{+}^{0}\right)  =S\left(  \mathfrak{n}_{+}\right)  $.

We notice that%
\begin{equation}
\lambda\left(  \left[  x,y\right]  ^{0}\right)  =\lambda\left(  \left[
x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for any }x\in\mathfrak{g}\text{
and }y\in\mathfrak{g}. \label{prop.det.US.pf.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{prop.det.US.pf.0}).} Let $x\in\mathfrak{g}$
and $y\in\mathfrak{g}$. Since the equation (\ref{prop.det.US.pf.0}) is linear
in each of $x$ and $y$, we can WLOG assume that $x$ and $y$ are homogeneous
(since every element of $\mathfrak{g}$ is a sum of homogeneous elements). So
we can assume that $x\in\mathfrak{g}_{i}$ and $y\in\mathfrak{g}_{j}$ for some
$i\in\mathbb{N}$ and $j\in\mathbb{N}$. Consider these $i$ and $j$. If
$i+j\neq0$, then $\left[  x,y\right]  ^{0}=0$ (by (\ref{prop.det.US.pf.-1}))
and $\lambda\left(  \left[  x,y\right]  \right)  =0$ (since $x\in
\mathfrak{g}_{i}$ and $y\in\mathfrak{g}_{j}$ yield $\left[  x,y\right]
\in\mathfrak{g}_{i+j}$, and due to $i+j\neq0$ the form $\lambda$ annihilates
$\mathfrak{g}_{i+j}$), so that (\ref{prop.det.US.pf.0}) trivially holds in
this case. If $i+j=0$, then $\left[  x,y\right]  ^{0}=\left[  x,y\right]  $
(again by (\ref{prop.det.US.pf.-1})), and thus (\ref{prop.det.US.pf.0}) holds
in this case as well. We have thus proven (\ref{prop.det.US.pf.0}) both in the
case $i+j\neq0$ and in the case $i+j=0$. These cases cover all possibilites,
and thus (\ref{prop.det.US.pf.0}) is proven.}

In the following, we will use the form $\left(  \cdot,\cdot\right)  _{\lambda
}^{\circ}$ defined in Definition \ref{def.lambda_k}. We will only consider
this form for the Lie algebra $\mathfrak{g}$, not for the Lie algebras
$\mathfrak{g}^{\varepsilon}$ and $\mathfrak{g}^{0}$; thus we don't have any
reason to rename it as $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ
\mathfrak{g}}$.

\begin{lemma}
\label{lem.invform.g^0.1}We have%
\begin{equation}
\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  a,b\right)  _{\lambda}^{\circ
}\ \ \ \ \ \ \ \ \ \ \text{for all }a\in S\left(  \mathfrak{n}_{-}\right)
\text{ and }b\in S\left(  \mathfrak{n}_{+}\right)  . \label{prop.det.US.pf.1}%
\end{equation}
Here, $av_{\lambda}^{+\mathfrak{g}^{0}}$ and $bv_{-\lambda}^{-\mathfrak{g}%
^{0}}$ are elements of $M_{\lambda}^{+\mathfrak{g}^{0}}$ and $M_{-\lambda
}^{-\mathfrak{g}^{0}}$, respectively (because $a\in S\left(  \mathfrak{n}%
_{-}\right)  =U\left(  \mathfrak{n}_{-}^{0}\right)  $ and $b\in S\left(
\mathfrak{n}_{+}\right)  =U\left(  \mathfrak{n}_{+}^{0}\right)  $).
\end{lemma}

\textit{Proof of Lemma \ref{lem.invform.g^0.1}.} Let $a\in S\left(
\mathfrak{n}_{-}\right)  $ and $b\in S\left(  \mathfrak{n}_{+}\right)  $ be
arbitrary. Since the claim that $\left(  av_{\lambda}^{+\mathfrak{g}^{0}%
},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  a,b\right)  _{\lambda}^{\circ}$ is linear in each of $a$ and $b$, we
can WLOG assume that $a=a_{1}a_{2}...a_{u}$ for some homogeneous $a_{1}%
,a_{2},...,a_{u}\in\mathfrak{n}_{-}$ and that $b=b_{1}b_{2}...b_{v}$ for some
homogeneous $b_{1},b_{2},...,b_{v}\in\mathfrak{n}_{+}$ (because every element
of $S\left(  \mathfrak{n}_{-}\right)  $ is a $\mathbb{C}$-linear combination
of products of the form $a_{1}a_{2}...a_{u}$ with homogeneous $a_{1}%
,a_{2},...,a_{u}\in\mathfrak{n}_{-}$, and because every element of $S\left(
\mathfrak{n}_{+}\right)  $ is a $\mathbb{C}$-linear combination of products of
the form $b_{1}b_{2}...b_{v}$ with homogeneous $b_{1},b_{2},...,b_{v}%
\in\mathfrak{n}_{+}$).

WLOG assume that $v\geq u$. (Else, the proof is analogous.)

From the proof of Proposition \ref{prop.invform}, we recall that $\left(
av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(  b\right)
av_{\lambda}^{+},v_{-\lambda}^{-}\right)  $. Applied to $\mathfrak{g}^{0}$
instead of $\mathfrak{g}$, this yields $\left(  av_{\lambda}^{+\mathfrak{g}%
^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  S\left(  b\right)  av_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}$.

Since $\mathfrak{h}\subseteq Z\left(  \mathfrak{g}^{0}\right)  $, we have
$\mathfrak{h}\subseteq Z\left(  U\left(  \mathfrak{g}^{0}\right)  \right)  $
(because the center of a Lie algebra always lies in the center of its
universal enveloping algebra).

Since $b=b_{1}b_{2}...b_{v}$, we have $S\left(  b\right)  =\left(  -1\right)
^{v}b_{v}b_{v-1}...b_{1}$. Combined with $a=a_{1}a_{2}...a_{u}$, this yields%
\[
S\left(  b\right)  a=\left(  -1\right)  ^{v}b_{v}b_{v-1}...b_{1}a_{1}%
a_{2}...a_{u},
\]
so that%
\begin{equation}
\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  \underbrace{S\left(  b\right)
a}_{=\left(  -1\right)  ^{v}b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}}v_{\lambda
}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
}^{\mathfrak{g}^{0}}=\left(  -1\right)  ^{v}\left(  b_{v}b_{v-1}...b_{1}%
a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}. \label{prop.det.US.pf.4}%
\end{equation}


We will now prove some identities in order to simplify the $b_{v}%
b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}$ term here.

First: In the Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}^{0}}$ of
$\left(  \mathfrak{g}^{0},\lambda\right)  $, we have%
\begin{align}
\beta\alpha_{1}\alpha_{2}...\alpha_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\sum\limits_{p=1}^{\ell}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}\label{prop.det.US.pf.3}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{N}\text{, }\alpha
_{1},\alpha_{2},...,\alpha_{\ell}\in\mathfrak{n}_{-}\text{ and }\beta
\in\mathfrak{n}_{+}\text{.}\nonumber
\end{align}
\footnote{\textit{Proof of (\ref{prop.det.US.pf.3}).} We will prove
(\ref{prop.det.US.pf.3}) by induction over $\ell$:
\par
\textit{Induction base:} For $\ell=0$, the left hand side of
(\ref{prop.det.US.pf.3}) is $\beta v_{\lambda}^{+\mathfrak{g}^{0}}=0$ (since
$\beta\in\mathfrak{n}_{+}=\mathfrak{n}_{+}^{0}$), and the right hand side of
(\ref{prop.det.US.pf.3}) is $\left(  \text{empty sum}\right)  =0$. Thus, for
$\ell=0$, the equality (\ref{prop.det.US.pf.3}) holds. This completes the
induction base.
\par
\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. Assume that
(\ref{prop.det.US.pf.3}) holds for $\ell=m-1$. We now must show that
(\ref{prop.det.US.pf.3}) holds for $\ell=m$.
\par
Let $\alpha_{1},\alpha_{2},...,\alpha_{m}\in\mathfrak{n}_{-}$ and $\beta
\in\mathfrak{n}_{+}$.
\par
Since (\ref{prop.det.US.pf.3}) holds for $\ell=m-1$, we can apply
(\ref{prop.det.US.pf.3}) to $m-1$ and $\left(  \alpha_{2},\alpha
_{3},...,\alpha_{m}\right)  $ instead of $\ell$ and $\left(  \alpha_{1}%
,\alpha_{2},...,\alpha_{\ell}\right)  $, and thus obtain%
\begin{align*}
\beta\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\sum\limits_{p=1}^{m-1}\lambda\left(  \left[  \beta,\alpha_{p+1}\right]
\right)  \alpha_{2}\alpha_{3}...\alpha_{p-1+1}\alpha_{p+1+1}\alpha
_{p+2+1}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }p\text{ for
}p+1\text{ in the sum}\right)  .
\end{align*}
\par
Now, we notice that $\beta\in\mathfrak{n}_{+}$ and $\alpha_{1}\in
\mathfrak{n}_{-}$, so that $\left[  \beta,\alpha_{1}\right]  ^{0}\in\left[
\mathfrak{n}_{+},\mathfrak{n}_{-}\right]  ^{0}\subseteq\mathfrak{h}\subseteq
Z\left(  U\left(  \mathfrak{g}^{0}\right)  \right)  $. Thus, $\left[
\beta,\alpha_{1}\right]  ^{0}\alpha_{2}\alpha_{3}...\alpha_{m}=\alpha
_{2}\alpha_{3}...\alpha_{m}\left[  \beta,\alpha_{1}\right]  ^{0}$. But since
$\left[  \beta,\alpha_{1}\right]  ^{0}\in\mathfrak{h}=\mathfrak{h}^{0}$, we
also have $\left[  \beta,\alpha_{1}\right]  ^{0}v_{\lambda}^{+\mathfrak{g}%
^{0}}=\lambda\left(  \left[  \beta,\alpha_{1}\right]  ^{0}\right)  v_{\lambda
}^{+\mathfrak{g}^{0}}=\lambda\left(  \left[  \beta,\alpha_{1}\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}$ (since $\lambda\left(  \left[  \beta
,\alpha_{1}\right]  ^{0}\right)  =\lambda\left(  \left[  \beta,\alpha
_{1}\right]  \right)  $ by (\ref{prop.det.US.pf.0})).
\par
We now compute:
\begin{align*}
\beta\alpha_{1}\alpha_{2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\underbrace{\beta\alpha_{1}}_{\substack{=\alpha_{1}\beta+\left[  \beta
,\alpha_{1}\right]  ^{0}\\\text{(since we are in }U\left(  \mathfrak{g}%
^{0}\right)  \text{)}}}\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}=\left(  \alpha_{1}\beta+\left[  \beta,\alpha_{1}\right]
^{0}\right)  \alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}%
}\\
&  =\alpha_{1}\underbrace{\beta\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}}_{\substack{=\sum\limits_{p=2}^{m}\lambda\left(  \left[
\beta,\alpha_{p}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}%
\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}%
}}+\underbrace{\left[  \beta,\alpha_{1}\right]  ^{0}\alpha_{2}\alpha
_{3}...\alpha_{m}}_{\substack{=\alpha_{2}\alpha_{3}...\alpha_{m}\left[
\beta,\alpha_{1}\right]  ^{0}}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\underbrace{\alpha_{1}\sum\limits_{p=2}^{m}\lambda\left(  \left[
\beta,\alpha_{p}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}%
\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}%
_{=\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}+\alpha_{2}\alpha
_{3}...\alpha_{m}\underbrace{\left[  \beta,\alpha_{1}\right]  ^{0}v_{\lambda
}^{+\mathfrak{g}^{0}}}_{\substack{=\lambda\left(  \left[  \beta,\alpha
_{1}\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}}}\\
&  =\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}+\lambda\left(  \left[
\beta,\alpha_{1}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=1}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=1}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}.
\end{align*}
Thus, (\ref{prop.det.US.pf.3}) holds for $\ell=m$. This completes the
induction step. Thus, (\ref{prop.det.US.pf.3}) is proven.}

Next we will show that in the Verma highest-weight module $M_{\lambda
}^{+\mathfrak{g}^{0}}$ of $\left(  \mathfrak{g}^{0},\lambda\right)  $, we have%
\begin{align}
\beta_{\ell}\beta_{\ell-1}...\beta_{1}\alpha_{1}\alpha_{2}...\alpha_{\ell
}v_{\lambda}^{+\mathfrak{g}^{0}}  &  =\left(  -1\right)  ^{\ell}%
\sum\limits_{\sigma\in S_{\ell}}\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{\ell},\beta_{\sigma\left(  \ell\right)  }\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}\label{prop.det.US.pf.2}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{N}\text{, }\alpha
_{1},\alpha_{2},...,\alpha_{\ell}\in\mathfrak{n}_{-}\text{ and }\beta
_{1},\beta_{2},...,\beta_{\ell}\in\mathfrak{n}_{+}\text{.}\nonumber
\end{align}


\textit{Proof of (\ref{prop.det.US.pf.2}).} We will prove
(\ref{prop.det.US.pf.2}) by induction over $\ell$:

\textit{Induction base:} For $\ell=0$, we have $\underbrace{\beta_{\ell}%
\beta_{\ell-1}...\beta_{1}}_{\text{empty product}}\underbrace{\alpha_{1}%
\alpha_{2}...\alpha_{\ell}}_{\text{empty product}}v_{\lambda}^{+\mathfrak{g}%
^{0}}=v_{\lambda}^{+\mathfrak{g}^{0}}$ and \newline$\underbrace{\left(
-1\right)  ^{\ell}}_{=1}\underbrace{\sum\limits_{\sigma\in S_{\ell}}%
}_{\text{sum over }1\text{ element}}\underbrace{\lambda\left(  \left[
\alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(
\left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  \alpha_{\ell},\beta_{\sigma\left(  \ell\right)
}\right]  \right)  }_{\text{empty product}}v_{\lambda}^{+\mathfrak{g}^{0}%
}=v_{\lambda}^{+\mathfrak{g}^{0}}$. Thus, for $\ell=0$, the equality
(\ref{prop.det.US.pf.2}) holds. This completes the induction base.

\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. Assume that
(\ref{prop.det.US.pf.2}) holds for $\ell=m-1$. We now must show that
(\ref{prop.det.US.pf.2}) holds for $\ell=m$.

Let $\alpha_{1},\alpha_{2},...,\alpha_{m}\in\mathfrak{n}_{-}$ and $\beta
_{1},\beta_{2},...,\beta_{m}\in\mathfrak{n}_{+}$.

For every $p\in\left\{  1,2,...,m\right\}  $, let $c_{p}$ denote the
permutation in $S_{m}$ which is written in row form as $\left(
1,2,...,p-1,p+1,p+2,...,m,p\right)  $. (This is the permutation with cycle
decomposition $\left(  1\right)  \left(  2\right)  ...\left(  p-1\right)
\left(  p,p+1,...,m\right)  $.) Since (\ref{prop.det.US.pf.2}) holds for
$\ell=m-1$, we can apply (\ref{prop.det.US.pf.2}) to $m-1$ and $\left(
\alpha_{c_{p}\left(  1\right)  },\alpha_{c_{p}\left(  2\right)  }%
,...,\alpha_{c_{p}\left(  m-1\right)  }\right)  $ instead of $\ell$ and
$\left(  \alpha_{1},\alpha_{2},...,\alpha_{\ell}\right)  $. This results in%
\begin{align*}
&  \beta_{m-1}\beta_{m-2}...\beta_{1}\alpha_{c_{p}\left(  1\right)  }%
\alpha_{c_{p}\left(  2\right)  }...\alpha_{c_{p}\left(  m-1\right)
}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}%
\underbrace{\lambda\left(  \left[  \alpha_{c_{p}\left(  1\right)  }%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{c_{p}\left(  2\right)  },\beta_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  \alpha_{c_{p}\left(  m-1\right)  }%
,\beta_{\sigma\left(  m-1\right)  }\right]  \right)  }_{\substack{=\prod
\limits_{i\in\left\{  1,2,...,m-1\right\}  }\lambda\left(  \left[
\alpha_{c_{p}\left(  i\right)  },\beta_{\sigma\left(  i\right)  }\right]
\right)  =\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  c_{p}%
^{-1}\left(  i\right)  \right)  }\right]  \right)  \\\text{(here, we
substituted }i\text{ for }c_{p}\left(  i\right)  \text{ in the product)}%
}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}\prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\underbrace{\beta_{\sigma\left(  c_{p}%
^{-1}\left(  i\right)  \right)  }}_{=\beta_{\left(  \sigma\circ c_{p}%
^{-1}\right)  \left(  i\right)  }}\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}\prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\left(  \sigma\circ c_{p}%
^{-1}\right)  \left(  i\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m};\ \sigma\left(
m\right)  =m}\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\left(  \sigma\circ
c_{p}^{-1}\right)  \left(  i\right)  }\right]  \right)  v_{\lambda
}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we identified the permutations in }S_{m-1}\text{ with the
permutations}\\
\sigma\in S_{m}\text{ satisfying }\sigma\left(  m\right)  =m
\end{array}
\right) \\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m};\ \sigma\left(
p\right)  =m}\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)
}\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\sigma\text{ for
}\sigma\circ c_{p}^{-1}\text{ in the sum}\right)  .
\end{align*}


The elements $\beta_{m}$, $\beta_{m-1}$, $...$, $\beta_{1}$ all lie in
$\mathfrak{n}_{+}$ and thus commute in $U\left(  \mathfrak{g}^{0}\right)  $
(since $\left[  \mathfrak{n}_{+},\mathfrak{n}_{+}\right]  ^{0}=0$). Thus,
$\beta_{m}\beta_{m-1}...\beta_{1}=\beta_{m-1}\beta_{m-2}...\beta_{1}\beta_{m}$
in $U\left(  \mathfrak{g}^{0}\right)  $, so that%
\begin{align*}
&  \beta_{m}\beta_{m-1}...\beta_{1}\alpha_{1}\alpha_{2}...\alpha_{m}%
v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\beta_{m-1}\beta_{m-2}...\beta_{1}\underbrace{\beta_{m}\alpha_{1}%
\alpha_{2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}_{\substack{=\sum
\limits_{p=1}^{m}\lambda\left(  \left[  \beta_{m},\alpha_{p}\right]  \right)
\alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha
_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\\text{(by (\ref{prop.det.US.pf.3}),
applied to }\beta=\beta_{m}\text{ and }\ell=m\text{)}}}\\
&  =\beta_{m-1}\beta_{m-2}...\beta_{1}\sum\limits_{p=1}^{m}\lambda\left(
\left[  \beta_{m},\alpha_{p}\right]  \right)  \alpha_{1}\alpha_{2}%
...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\sum\limits_{p=1}^{m}\underbrace{\lambda\left(  \left[  \beta_{m}%
,\alpha_{p}\right]  \right)  }_{=\lambda\left(  -\left[  \alpha_{p},\beta
_{m}\right]  \right)  =-\lambda\left(  \left[  \alpha_{p},\beta_{m}\right]
\right)  }\beta_{m-1}\beta_{m-2}...\beta_{1}\underbrace{\alpha_{1}\alpha
_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha_{m}}_{\substack{=\alpha
_{c_{p}\left(  1\right)  }\alpha_{c_{p}\left(  2\right)  }...\alpha
_{c_{p}\left(  m-1\right)  }\\\text{(by the definition of }c_{p}\text{)}%
}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =-\sum\limits_{p=1}^{m}\lambda\left(  \left[  \alpha_{p},\beta_{m}\right]
\right)  \underbrace{\beta_{m-1}\beta_{m-2}...\beta_{1}\alpha_{c_{p}\left(
1\right)  }\alpha_{c_{p}\left(  2\right)  }...\alpha_{c_{p}\left(  m-1\right)
}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)  ^{m-1}\sum
\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)  =m}\prod\limits_{i\in
\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }\lambda\left(
\left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}}\\
&  =\underbrace{-\left(  -1\right)  ^{m-1}}_{=\left(  -1\right)  ^{m}}%
\sum\limits_{p=1}^{m}\sum\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)
=m}\lambda\left(  \left[  \alpha_{p},\underbrace{\beta_{m}}_{\substack{=\beta
_{\sigma\left(  p\right)  }\\\text{(since }\sigma\left(  p\right)  =m\text{)}%
}}\right]  \right)  \prod\limits_{i\in\left\{  1,2,...,m\right\}
\diagdown\left\{  p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta
_{\sigma\left(  i\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m}\sum\limits_{p=1}^{m}\sum\limits_{\sigma\in
S_{m};\ \sigma\left(  p\right)  =m}\underbrace{\lambda\left(  \left[
\alpha_{p},\beta_{\sigma\left(  p\right)  }\right]  \right)  \prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]
\right)  }_{\substack{=\prod\limits_{i\in\left\{  1,2,...,m\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]
\right)  \\=\lambda\left(  \left[  \alpha_{1},\beta_{\sigma\left(  1\right)
}\right]  \right)  \lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  \alpha_{m}%
,\beta_{\sigma\left(  m\right)  }\right]  \right)  }}v_{\lambda}%
^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m}\underbrace{\sum\limits_{p=1}^{m}\sum
\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)  =m}}_{=\sum\limits_{\sigma
\in S_{m}}}\lambda\left(  \left[  \alpha_{1},\beta_{\sigma\left(  1\right)
}\right]  \right)  \lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  \alpha_{m}%
,\beta_{\sigma\left(  m\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m}\sum\limits_{\sigma\in S_{m}}\lambda\left(  \left[
\alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(
\left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  \alpha_{m},\beta_{\sigma\left(  m\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}}.
\end{align*}
In other words, (\ref{prop.det.US.pf.2}) is proven for $\ell=m$. This
completes the induction step. Thus, the induction proof of
(\ref{prop.det.US.pf.2}) is done.

Now, back to proving $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$. Applying (\ref{prop.det.US.pf.2}) to $\ell
=u$, $\alpha_{i}=a_{i}$ and $\beta_{i}=b_{i}$, we obtain%
\[
b_{u}b_{u-1}...b_{1}a_{1}a_{2}...a_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}%
=\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}.
\]
Hence, if $v>u$, then%
\begin{align*}
&  b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =b_{v}b_{v-1}...b_{u+2}b_{u+1}\underbrace{b_{u}b_{u-1}...b_{1}a_{1}%
a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)  ^{u}%
\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}}\\
&  =b_{v}b_{v-1}...b_{u+2}b_{u+1}\left(  -1\right)  ^{u}\sum\limits_{\sigma\in
S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]
\right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[
a_{1},b_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
a_{2},b_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(  \left[
a_{u},b_{\sigma\left(  u\right)  }\right]  \right)  b_{v}b_{v-1}%
...b_{u+2}\underbrace{b_{u+1}v_{\lambda}^{+\mathfrak{g}^{0}}}%
_{\substack{=0\\\text{(since }b_{u+1}\in\mathfrak{n}_{+}=\mathfrak{n}_{+}%
^{0}\text{)}}}\\
&  =0,
\end{align*}
and thus%
\begin{align*}
&  \left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{v}\left(  \underbrace{b_{v}b_{v-1}...b_{1}a_{1}%
a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=0},v_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{prop.det.US.pf.4})}\right) \\
&  =0=\left(  a,b\right)  _{\lambda}^{\circ}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the form }\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}\text{
was defined as a restriction of a sum}\\
\bigoplus\limits_{k\geq0}\lambda_{k}:S\left(  \mathfrak{n}_{-}\right)  \times
S\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}\text{ of bilinear
forms }\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(
\mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}\text{,}\\
\text{and thus }\left(  S^{u}\left(  \mathfrak{n}_{-}\right)  ,S^{v}\left(
\mathfrak{n}_{+}\right)  \right)  _{\lambda}^{\circ}=0\text{ for }u\neq
v\text{, so that }\left(  a,b\right)  _{\lambda}^{\circ}=0\\
\text{(since }a\in S^{u}\left(  \mathfrak{n}_{-}\right)  \text{ and }b\in
S^{v}\left(  \mathfrak{n}_{+}\right)  \text{ and }u\neq v\text{)}%
\end{array}
\right)  .
\end{align*}
We thus have proven $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$ in the case when $v>u$. It remains to prove
that $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  a,b\right)  _{\lambda
}^{\circ}$ in the case when $v=u$. So let us assume that $v=u$. In this case,%
\begin{align*}
b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=b_{u}b_{u-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[
a_{1},b_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
a_{2},b_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(  \left[
a_{u},b_{\sigma\left(  u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}},
\end{align*}
so that%
\begin{align*}
&  \left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\underbrace{\left(  -1\right)  ^{v}}_{\substack{=\left(  -1\right)
^{u}\\\text{(since }v=u\text{)}}}\left(  \underbrace{b_{v}b_{v-1}...b_{1}%
a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)
^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}},v_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\left(  \left(  -1\right)  ^{u}\sum\limits_{\sigma
\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]
\right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)  ^{u}}%
_{\substack{=\left(  -1\right)  ^{u+u}=\left(  -1\right)  ^{2u}%
=1\\\text{(since }2u\text{ is even)}}}\sum\limits_{\sigma\in S_{u}}%
\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]  \right)
\lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]  \right)
\underbrace{\left(  v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}%
^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}}_{=1}\\
&  =\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  .
\end{align*}
Compared to%
\begin{align*}
\left(  \underbrace{a}_{=a_{1}a_{2}...a_{u}},\underbrace{b}_{\substack{=b_{1}%
b_{2}...b_{v}=b_{1}b_{2}...b_{u}\\\text{(since }v=u\text{)}}}\right)
_{\lambda}^{\circ}  &  =\left(  a_{1}a_{2}...a_{u},b_{1}b_{2}...b_{u}\right)
_{\lambda}^{\circ}=\lambda_{u}\left(  a_{1}a_{2}...a_{u},b_{1}b_{2}%
...b_{u}\right) \\
&  =\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  ,
\end{align*}
this yields $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$. Now that $\left(  av_{\lambda}^{+\mathfrak{g}%
^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  a,b\right)  _{\lambda}^{\circ}$ is proven in each of the cases $v>u$
and $v=u$ (and the case $v<u$ is analogous), we are done with proving
(\ref{prop.det.US.pf.1}).

This proves Proposition \ref{lem.invform.g^0.1}.

\begin{corollary}
\label{cor.invform.g^0.1}Let $n\in\mathbb{N}$. Recall that the family $\left(
e_{\mathbf{i}}^{0}\right)  _{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \deg\mathbf{i}=-n}$ is a basis of the vector space $U\left(
\mathfrak{n}_{-}^{0}\right)  \left[  -n\right]  =S\left(  \mathfrak{n}%
_{-}\right)  \left[  -n\right]  $, and that the family $\left(  e_{\mathbf{j}%
}^{0}\right)  _{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg
\mathbf{j}=n}$ is a basis of the vector space $U\left(  \mathfrak{n}_{+}%
^{0}\right)  \left[  n\right]  =S\left(  \mathfrak{n}_{+}\right)  \left[
n\right]  $. Thus, let us represent the bilinear form $\left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  \times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  $ by its
matrix with respect to the bases $\left(  e_{\mathbf{i}}^{0}\right)
_{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ and
$\left(  e_{\mathbf{j}}^{0}\right)  _{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\ \deg\mathbf{j}=n}$ of $S\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  $ and $S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
$, respectively. This is the matrix%
\[
\left(  \left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda
,n}^{\circ}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}.
\]
This matrix is a square matrix (since the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$), and its determinant is what we are going to
denote by $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ
}\right)  $.

Then,%
\[
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}%
}\right)  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ
}\right)  .
\]

\end{corollary}

\textit{Proof of Corollary \ref{cor.invform.g^0.1}.} For every $\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E$ satisfying $\deg\mathbf{i}=-n$, and
every $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$ satisfying
$\deg\mathbf{j}=n$, we have%
\begin{align*}
\left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}},e_{\mathbf{j}}%
^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda,n}^{\mathfrak{g}^{0}}
&  =\left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}},e_{\mathbf{j}%
}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda}^{\circ}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.invform.g^0.1}, applied
to }a=e_{\mathbf{i}}^{0}\text{ and }b=e_{\mathbf{j}}^{0}\right) \\
&  =\left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda,n}^{\circ
}.
\end{align*}
Thus,%
\[
\det\left(  \left(  \left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}%
},e_{\mathbf{j}}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
,n}^{\mathfrak{g}^{0}}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  =\det\left(  \left(  \left(
e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda,n}^{\circ}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  .
\]


Now,%
\begin{align*}
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)   &
=\det\left(  \left(  \left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)
_{\lambda,n}^{\circ}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  =\det\left(  \left(  \left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}%
^{0}},e_{\mathbf{j}}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
,n}^{\mathfrak{g}^{0}}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  =\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}}\right)  .
\end{align*}
This proves Corollary \ref{cor.invform.g^0.1}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Joining the threads}

\textit{Proof of Proposition \ref{prop.det.US}.} Consider the polynomial
function $Q_{n}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$
introduced in Corollary \ref{cor.invformnondeg.polynomiality}. Due to
Corollary \ref{cor.invformnondeg.polynomiality}, every $\lambda\in V$ and
every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
Q_{n}\left(  \lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}%
n}Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  .
\]
Hence, we can apply Lemma \ref{lem.invformnondeg.elemen} to $V=\mathfrak{h}%
^{\ast}$, $\phi=Q_{n}$ and $k=\operatorname*{LEN}n$. Thus, we obtain the
following three observations:

\textit{Observation 1:} The polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)
\]
is homogeneous of degree $k$. (This follows from Lemma
\ref{lem.invformnondeg.elemen} \textbf{(a)}.)

\textit{Observation 2:} For every integer $N>k$, the $N$-th homogeneous
component of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is zero. (This follows from Lemma \ref{lem.invformnondeg.elemen} \textbf{(b)}.)

\textit{Observation 3:} The $k$-th homogeneous component of the polynomial
function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)  .
\]
(This follows from Lemma \ref{lem.invformnondeg.elemen} \textbf{(c)}.)

Since every $\lambda\in\mathfrak{h}^{\ast}$ satisfies%
\begin{align*}
Q_{n}\left(  \lambda,1\right)   &  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{1}}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{cor.invformnondeg.polynomiality.1}) (applied to }%
\varepsilon=1\text{)}\\
\text{yields }\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}%
^{\mathfrak{g}^{1}}\right)  =Q_{n}\left(  \lambda,1\right)
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}^{1}=\mathfrak{g}\text{
and thus }\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{1}}=\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}}=\left(  \cdot,\cdot\right)
_{\lambda,n}\right)  ,
\end{align*}
the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  .
\]
This yields that%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is a polynomial function.

Since every $\lambda\in\mathfrak{h}^{\ast}$ satisfies%
\begin{align*}
Q_{n}\left(  \lambda,0\right)   &  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{0}}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{cor.invformnondeg.polynomiality.1}) (applied to }%
\varepsilon=0\text{)}\\
\text{yields }\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}%
^{\mathfrak{g}^{0}}\right)  =Q_{n}\left(  \lambda,0\right)
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Corollary \ref{cor.invform.g^0.1}%
}\right)  ,
\end{align*}
the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]
This yields that%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is a polynomial function. This polynomial function is not identically
zero\footnote{\textit{Proof.} Since $\mathfrak{g}$ is nondegenerate, there
exists $\lambda\in\mathfrak{h}^{\ast}$ such that the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $. For such
$\lambda$, the form $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must be
nondegenerate (by Lemma \ref{lem.lambda_k.2}), so that $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. Hence, there exists
$\lambda\in\mathfrak{h}^{\ast}$ such that $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. In other words, the
polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is not identically zero, qed.}.

Since $Q_{n}\left(  \lambda,1\right)  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$, Observation
2 rewrites as follows:

\textit{Observation 2':} For every integer $n>k$, the $n$-th homogeneous
component of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is zero.

Since $Q_{n}\left(  \lambda,1\right)  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ and $Q_{n}\left(  \lambda,0\right)  =\det\left(
\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  $ for every
$\lambda\in\mathfrak{h}^{\ast}$, Observation 3 rewrites as follows:

\textit{Observation 3':} The $k$-th homogeneous component of the polynomial
function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]


Combining Observations 2' and 3' and the fact that the polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is not identically zero, we conclude that the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is the leading term of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  .
\]


This proves Proposition \ref{prop.det.US}.

Now that Proposition \ref{prop.det.US} is proven, the proof of Theorem
\ref{thm.invformnondeg} is also complete (because we have already proven
Theorem \ref{thm.invformnondeg} using Proposition \ref{prop.det.US}).

\subsection{The irreducible quotients of the Verma modules}

We will now use the form $\left(  \cdot,\cdot\right)  _{\lambda}$ to develop
the representation theory of $\mathfrak{g}$. In the following, we assume that
$\mathfrak{g}$ is nondegenerate.

\begin{definition}
Let $\left(  \cdot,\cdot\right)  $ denote the form $\left(  \cdot
,\cdot\right)  _{\lambda}$. Let $J_{\lambda}^{\pm}$ be the kernel of $\left(
\cdot,\cdot\right)  $ on $M_{\lambda}^{\pm}$. This is a graded $\mathfrak{g}%
$-submodule of $M_{\lambda}^{\pm}$ (since the form $\left(  \cdot
,\cdot\right)  $ is $\mathfrak{g}$-invariant). Let $L_{\lambda}^{\pm}$ be the
quotient module $M_{\lambda}^{\pm}\diagup J_{\lambda}^{\pm}$. Then, $\left(
\cdot,\cdot\right)  $ descends to a nondegenerate pairing $L_{\lambda}%
^{+}\times L_{-\lambda}^{-}\rightarrow\mathbb{C}$.
\end{definition}

\begin{remark}
For Weil-generic $\lambda$ (away from a countable union of hypersurfaces), we
have $J_{\lambda}^{\pm}=0$ (by Theorem \ref{thm.invformnondeg}) and thus
$L_{\lambda}^{\pm}=M_{\lambda}^{\pm}$.
\end{remark}

\begin{theorem}
\label{thm.verma}\textbf{(i)} The $\mathfrak{g}$-module $L_{\lambda}^{\pm}$ is irreducible.

\textbf{(ii)} The $\mathfrak{g}$-module $J_{\lambda}^{\pm}$ is the maximal
proper graded submodule of $M_{\lambda}^{\pm}$. (This means that $J_{\lambda
}^{\pm}$ contains all proper graded submodules in $M_{\lambda}^{\pm}$.)

\textbf{(iii)} Assume that there exists some $L\in\mathfrak{g}_{0}$ such that
every $n\in\mathbb{Z}$ satisfies
\[
\left(  \operatorname*{ad}L\right)  \mid_{\mathfrak{g}_{n}}=n\cdot
\operatorname*{id}\mid_{\mathfrak{g}_{n}}.
\]
(In this case it is said that \textit{the grading on }$\mathfrak{g}$
\textit{is internal}, i. e., comes from bracketing with some $L\in
\mathfrak{g}_{0}$.) Then $J_{\lambda}^{\pm}$ is the maximal proper submodule
of $M_{\lambda}^{\pm}$.
\end{theorem}

\begin{remark}
The grading is internal if $\mathfrak{g}$ is a simple Lie algebra (in this
case, we can take $L=\rho^{\vee}$) and if $\mathfrak{g}=\operatorname*{Vir}$
(in this case, $L=-L_{0}$), but not for $\mathfrak{g}$ being the affine
Kac-Moody algebra $\widehat{\mathfrak{g}}_{\omega}$ of Definition
\ref{def.kac}.
\end{remark}

\textit{Proof of Theorem \ref{thm.verma}.} \textbf{(i)} Let us show that
$L_{\lambda}^{-}$ is irreducible (the proof for $L_{\lambda}^{+}$ will be similar).

In fact, assume the contrary. Then, there exists a nonzero $w\in L_{\lambda
}^{-}$ such that $U\left(  \mathfrak{g}\right)  \cdot w\neq L_{\lambda}^{-}$.
Since $L_{\lambda}^{-}$ is graded by \textit{nonnegative} integers, we can
choose $w$ to have the smallest possible degree $m$ (without necessarily being
homogeneous). Clearly, $m>0$. Thus we can write $w=w_{0}+w_{1}+...+w_{m}$,
where each $w_{i}$ is homogeneous of degree $\deg w_{i}=i$ and $w_{m}\neq0$.

Let $a\in\mathfrak{g}_{j}$ for some $j<0$. Then $aw=0$ (since $\deg\left(
aw\right)  <\deg w$, but still $U\left(  \mathfrak{g}\right)  \cdot aw\neq
L_{\lambda}^{-}$ (since $U\left(  \mathfrak{g}\right)  \cdot aw\subseteq
U\left(  \mathfrak{g}\right)  \cdot w$ and $U\left(  \mathfrak{g}\right)
\cdot w\neq L_{\lambda}^{-}$), and we have chosen $w$ to have the smallest
possible degree). By homogeneity, this yields $aw_{m}=0$.

For every $u\in L_{-\lambda}^{+}\left[  -m-j\right]  $, the term $\left(
au,w_{m}\right)  $ is well-defined (since $au\in L_{-\lambda}^{+}$ and
$w_{m}\in L_{\lambda}^{-}$). Since the form $\left(  \cdot,\cdot\right)  $ is
$\mathfrak{g}$-invariant, it satisfies $\left(  au,w_{m}\right)  =-\left(
u,\underbrace{aw_{m}}_{=0}\right)  =0$. But since $m>0$, we have $L_{-\lambda
}^{+}\left[  -m\right]  =\sum\limits_{j<0}\mathfrak{g}_{j}\cdot L_{-\lambda
}^{+}\left[  -m-j\right]  $ (because Proposition \ref{prop.verma1}
\textbf{(a)} yields $M_{-\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\lambda}^{+}$, so that $L_{-\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
\overline{v_{\lambda}^{+}}$, thus%
\[
L_{-\lambda}^{+}\left[  -m\right]  =\underbrace{U\left(  \mathfrak{n}%
_{-}\right)  \left[  -m\right]  }_{=\sum\limits_{j<0}\left(  \mathfrak{n}%
_{-}\right)  \left[  j\right]  \cdot U\left(  \mathfrak{n}_{-}\right)  \left[
-m-j\right]  }\overline{v_{\lambda}^{+}}=\sum\limits_{j<0}\underbrace{\left(
\mathfrak{n}_{-}\right)  \left[  j\right]  }_{=\mathfrak{g}\left[  j\right]
=\mathfrak{g}_{j}}\cdot\underbrace{U\left(  \mathfrak{n}_{-}\right)  \left[
-m-j\right]  \overline{v_{\lambda}^{+}}}_{\substack{=L_{-\lambda}^{+}\left[
-m-j\right]  \\\text{(since }U\left(  \mathfrak{n}_{-}\right)  \overline
{v_{\lambda}^{+}}=L_{-\lambda}^{+}\text{)}}}=\sum\limits_{j<0}\mathfrak{g}%
_{j}\cdot L_{-\lambda}^{+}\left[  -m-j\right]
\]
). Hence, any element of $L_{-\lambda}^{+}\left[  -m\right]  $ is a linear
combination of elements of the form $au$ with $a\in\mathfrak{g}_{j}$ (for
$j<0$) and $u\in L_{-\lambda}^{+}\left[  -m-j\right]  $. Thus, since we know
that $\left(  au,w_{m}\right)  =0$ for every $a\in\mathfrak{g}_{j}$ and $u\in
L_{-\lambda}^{+}\left[  -m-j\right]  $, we conclude that $\left(  L_{-\lambda
}^{+}\left[  -m\right]  ,w_{m}\right)  =0$. As a consequence, $\left(
L_{-\lambda}^{+},w_{m}\right)  =0$ (because the form $\left(  \cdot
,\cdot\right)  :L_{-\lambda}^{+}\times L_{\lambda}^{-}\rightarrow\mathbb{C}$
is of degree $0$, and thus $\left(  L_{-\lambda}^{+}\left[  j\right]
,w_{m}\right)  =0$ for all $j\neq-m$). Since the form $\left(  \cdot
,\cdot\right)  :L_{-\lambda}^{+}\times L_{\lambda}^{-}\rightarrow\mathbb{C}$
is nondegenerate, this yields $w_{m}=0$. This is a contradiction to $w_{m}%
\neq0$. This contradiction shows that our assumption was wrong. Thus,
$L_{\lambda}^{-}$ is irreducible. Similarly, $L_{\lambda}^{+}$ is irreducible.

\textbf{(ii)} First let us prove that the $\mathfrak{g}$-module $J_{\lambda
}^{+}$ is the maximal proper graded submodule of $M_{\lambda}^{+}$.

Let $K\subseteq M_{\lambda}^{+}$ be a proper graded submodule, and let
$\overline{K}$ be its image in $L_{\lambda}^{+}$. Then, $K$ lives in strictly
negative degrees (because it is graded, so if it would have a component in
degrees $\geq0$, it would contain $v_{\lambda}^{+}$ and thus contain
everything, and thus not be proper). Hence, $\overline{K}$ also lives in
strictly negative degrees, and thus is proper. Hence, by \textbf{(i)}, we have
$\overline{K}=0$, thus $K\subseteq J_{\lambda}^{+}$. This shows that
$J_{\lambda}^{+}$ is the maximal proper graded submodule of $M_{\lambda}^{+}$.
The proof of the corresponding statement for $J_{\lambda}^{-}$ and
$M_{\lambda}^{-}$ is similar.

\textbf{(iii)} Assume that there exists some $L\in\mathfrak{g}_{0}$ such that
every $n\in\mathbb{Z}$ satisfies
\[
\left(  \operatorname*{ad}L\right)  \mid_{\mathfrak{g}_{n}}=n\cdot
\operatorname*{id}\mid_{\mathfrak{g}_{n}}.
\]
Consider this $L$. It is easy to prove (by induction) that $\left[
L,a\right]  =na$ for every $a\in U\left(  \mathfrak{g}\right)  \left[
n\right]  $.

We are now going to show that all $\mathfrak{g}$-submodules of $M_{\lambda
}^{+}$ are automatically graded.

In fact, it is easy to see that $M_{\lambda}^{+}\left[  n\right]
\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}}-\left(
\lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $ for every
$n\in\mathbb{Z}$.\ \ \ \ \footnote{\textit{Proof.} Let $n\in\mathbb{Z}$. Let
$a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $. Then, $a\in
U\left(  \mathfrak{g}\right)  \left[  n\right]  $, so that $\left[
L,a\right]  =na$ and thus $La=aL+\underbrace{\left[  L,a\right]  }%
_{=na}=aL+na$. Thus,%
\begin{align*}
\left(  L\mid_{M_{\lambda}^{+}}\right)  \left(  av_{\lambda}^{+}\right)   &
=\underbrace{La}_{=aL+na}v_{\lambda}^{+}=\left(  aL+na\right)  v_{\lambda}%
^{+}=a\underbrace{Lv_{\lambda}^{+}}_{=\lambda\left(  L\right)  v_{\lambda}%
^{+}}+nav_{\lambda}^{+}=\lambda\left(  L\right)  av_{\lambda}^{+}%
+nav_{\lambda}^{+}\\
&  =\left(  \lambda\left(  L\right)  +n\right)  av_{\lambda}^{+},
\end{align*}
so that $av_{\lambda}^{+}\in\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}%
}-\left(  \lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $.
Forget that we fixed $a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]
$. Thus we have showed that every $a\in U\left(  \mathfrak{n}_{-}\right)
\left[  n\right]  $ satisfies $av_{\lambda}^{+}\in\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right)  $. In other words, $\left\{  av_{\lambda}^{+}%
\ \mid\ a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \right\}
\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}}-\left(
\lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $. Since
$\left\{  av_{\lambda}^{+}\ \mid\ a\in U\left(  \mathfrak{n}_{-}\right)
\left[  n\right]  \right\}  =U\left(  \mathfrak{n}_{-}\right)  \left[
n\right]  \cdot v_{\lambda}^{+}=M_{\lambda}^{+}\left[  n\right]  $, this
becomes $M_{\lambda}^{+}\left[  n\right]  \subseteq\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right)  $, qed.} In other words, for every $n\in\mathbb{Z}%
$, the $n$-th graded component $M_{\lambda}^{+}\left[  n\right]  $ of
$M_{\lambda}^{+}$ is contained in the eigenspace of the operator
$L\mid_{M_{\lambda}^{+}}$ for the eigenvalue $\lambda\left(  L\right)  +n$.
Now,%
\begin{align*}
M_{\lambda}^{+}  &  =\bigoplus\limits_{n\in\mathbb{Z}}M_{\lambda}^{+}\left[
n\right]  =\sum\limits_{n\in\mathbb{Z}}\underbrace{M_{\lambda}^{+}\left[
n\right]  }_{\substack{\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda
}^{+}}-\left(  \lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)
\\=\left(  \text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for
the eigenvalue }\lambda\left(  L\right)  +n\right)  }}\\
&  \subseteq\sum\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  .
\end{align*}
Since all eigenspaces of $L\mid_{M_{\lambda}^{+}}$ are clearly contained in
$M_{\lambda}^{+}$, this rewrites as%
\[
M_{\lambda}^{+}=\sum\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  .
\]
Since eigenspaces of an operator corresponding to distinct eigenvalues are
linearly disjoint, the sum $\sum\limits_{n\in\mathbb{Z}}\left(
\text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for the
eigenvalue }\lambda\left(  L\right)  +n\right)  $ must be a direct sum, so
this becomes%
\begin{equation}
M_{\lambda}^{+}=\bigoplus\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of
the operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  . \label{thm.verma.pf.5}%
\end{equation}
As a consequence of this, the map $L\mid_{M_{\lambda}^{+}}$ is diagonalizable,
and all of its eigenvalues belong to the set $\left\{  \lambda\left(
L\right)  +n\ \mid\ n\in\mathbb{Z}\right\}  $.

So for every $n\in\mathbb{Z}$, we have the inclusion%
\begin{align*}
M_{\lambda}^{+}\left[  n\right]   &  \subseteq\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right) \\
&  =\left(  \text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{
for the eigenvalue }\lambda\left(  L\right)  +n\right)  ,
\end{align*}
but the direct sum of these inclusions over all $n\in\mathbb{Z}$ is an
equality (since%
\[
\bigoplus\limits_{n\in\mathbb{Z}}M_{\lambda}^{+}\left[  n\right]  =M_{\lambda
}^{+}=\bigoplus\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)
\]
by (\ref{thm.verma.pf.5})). Hence, each of these inclusions must be an
equality. In other words,
\begin{equation}
M_{\lambda}^{+}\left[  n\right]  =\left(  \text{eigenspace of the operator
}L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}.
\label{thm.verma.pf.6}%
\end{equation}


Now, let $K$ be a $\mathfrak{g}$-submodule of $M_{\lambda}^{+}$. Then,
$L\mid_{K}$ is a restriction of $L\mid_{M_{\lambda}^{+}}$ to $K$. Hence, map
$L\mid_{K}$ is diagonalizable, and all of its eigenvalues belong to the set
$\left\{  \lambda\left(  L\right)  +n\ \mid\ n\in\mathbb{Z}\right\}  $
(because we know that the map $L\mid_{M_{\lambda}^{+}}$ is diagonalizable, and
all of its eigenvalues belong to the set $\left\{  \lambda\left(  L\right)
+n\ \mid\ n\in\mathbb{Z}\right\}  $). In other words,%
\begin{align*}
K  &  =\bigoplus\limits_{n\in\mathbb{Z}}\underbrace{\left(  \text{eigenspace
of the operator }L\mid_{K}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  }_{=K\cap\left(  \text{eigenspace of the operator }L\mid
_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  }\\
&  =\bigoplus\limits_{n\in\mathbb{Z}}\left(  K\cap\underbrace{\left(
\text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for the
eigenvalue }\lambda\left(  L\right)  +n\right)  }_{=M_{\lambda}^{+}\left[
n\right]  }\right) \\
&  =\bigoplus\limits_{n\in\mathbb{Z}}\left(  K\cap M_{\lambda}^{+}\left[
n\right]  \right)  .
\end{align*}
Hence, $K$ is graded. We thus have shown that every $\mathfrak{g}$-submodule
of $M_{\lambda}^{+}$ is graded. Similarly, every $\mathfrak{g}$-submodule of
$M_{\lambda}^{-}$ is graded. Thus, Theorem \ref{thm.verma} \textbf{(iii)}
follows from Theorem \ref{thm.verma} \textbf{(ii)}.

\begin{remark}
Theorem \ref{thm.verma} \textbf{(ii)} does not hold if the word "graded" is
removed. In fact, here is a counterexample: Let $\mathfrak{g}$ be the
3-dimensional Heisenberg algebra. (This is the Lie algebra with vector-space
basis $\left(  x,K,y\right)  $ and with Lie bracket given by $\left[
y,x\right]  =K$, $\left[  x,K\right]  =0$ and $\left[  y,K\right]  =0$. It can
be considered as a Lie subalgebra of the oscillator algebra $\mathcal{A}$
defined in Definition \ref{def.osc}.) It is easy to see that $\mathfrak{g}$
becomes a nondegenerate $\mathbb{Z}$-graded Lie algebra by setting
$\mathfrak{g}_{-1}=\left\langle x\right\rangle $, $\mathfrak{g}_{0}%
=\left\langle K\right\rangle $, $\mathfrak{g}_{1}=\left\langle y\right\rangle
$ and $\mathfrak{g}_{i}=0$ for every $i\in\mathbb{Z}\diagdown\left\{
-1,0,1\right\}  $. Then, on the Verma highest-weight module $M_{0}%
^{+}=\mathbb{C}\left[  x\right]  v_{0}^{+}$, both $K$ and $y$ act as $0$ (and
$x$ acts as multiplication with $x$), so that $Iv_{0}^{+}$ is a $\mathfrak{g}%
$-submodule of $M_{0}^{+}$ for every ideal $I\subseteq\mathbb{C}\left[
x\right]  $, but not all of these ideals are graded, and not all of them are
contained in $J_{0}^{+}$ (as can be easily checked).
\end{remark}

\begin{corollary}
\label{cor.verma.irred}For Weil-generic $\lambda$ (this means a $\lambda$
outside of countably many hypersurfaces in $\mathfrak{h}^{\ast}$), the
$\mathfrak{g}$-modules $M_{\lambda}^{+}$ and $M_{\lambda}^{-}$ are irreducible.
\end{corollary}

\begin{remark}
Let $Y$ be a $\mathfrak{g}$-module. A vector $w\in Y$ is called a
\textit{singular vector of weight }$\mu\in\mathfrak{h}^{\ast}$ (here, recall
that $\mathfrak{h}=\mathfrak{g}_{0}$) if it satisfies%
\[
hw=\mu\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h}%
\]
and%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0\text{.}%
\]
We denote by $\operatorname*{Sing}\nolimits_{\mu}\left(  Y\right)  $ the space
of singular vectors of $Y$ of weight $\mu$.
\end{remark}

When people talk about "singular vectors", they usually mean nonzero singular
vectors in negative degrees. We are not going to adhere to this convention, though.

\begin{lemma}
\label{lem.singvec}Let $Y$ be a $\mathfrak{g}$-module. Then there is a
canonical isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
&  \rightarrow\operatorname*{Sing}\nolimits_{\lambda}Y,\\
\phi &  \mapsto\phi\left(  v_{\lambda}^{+}\right)  .
\end{align*}

\end{lemma}

\textit{Proof of Lemma \ref{lem.singvec}.} We have $M_{\lambda}^{+}=U\left(
\mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}=\operatorname*{Ind}\nolimits_{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}_{\lambda}$, so that
\[
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
=\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}%
_{\lambda},Y\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda},Y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right)  .
\]
But $\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\mathbb{C}_{\lambda},Y\right)  \cong\operatorname*{Sing}\nolimits_{\lambda}Y$
(because every $\mathbb{C}$-linear map $\mathbb{C}_{\lambda}\rightarrow Y$ is
uniquely determined by the image of $v_{\lambda}^{+}$, and this map is a
$\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-module map if and only
if this image is a singular vector of $Y$ of weight $\lambda$). Thus,
$\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\mathbb{C}_{\lambda},Y\right)  \cong\operatorname*{Sing}\nolimits_{\lambda}Y$.
If we make this isomorphism explicit, we notice that it sends every $\phi$ to
$\phi\left(  v_{\lambda}^{+}\right)  $, so that Lemma \ref{lem.singvec} is proven.

\begin{corollary}
\label{cor.singvec}The representation $M_{\lambda}^{+}$ is irreducible if and
only if it does not have nonzero singular vectors in negative degrees. Here, a
vector in $M_{\lambda}^{+}$ is said to be "in negative degrees" if its
projection on the $0$-th graded component $M_{\lambda}^{+}\left[  0\right]  $
is zero.
\end{corollary}

\textit{Proof of Corollary \ref{cor.singvec}.} $\Longleftarrow:$ Assume that
$M_{\lambda}^{+}$ does not have nonzero singular vectors in negative degrees.

We must then show that $M_{\lambda}^{+}$ is irreducible.

In fact, assume the contrary. Then, $M_{\lambda}^{+}$ is not irreducible.
Hence, there exists a nonzero \textit{homogeneous} $v\in M_{\lambda}^{+}$ such
that $U\left(  \mathfrak{g}\right)  \cdot v\neq M_{\lambda}^{+}$%
.\ \ \ \ \footnote{\textit{Proof.} Since $M_{\lambda}^{+}$ is not irreducible,
there exists a nonzero $w\in M_{\lambda}^{+}$ such that $U\left(
\mathfrak{g}\right)  \cdot w\neq M_{\lambda}^{+}$. Since $M_{\lambda}^{+}$ is
graded by \textit{nonpositive} integers, we can write $w$ in the form
$w=\sum\limits_{j=0}^{m}w_{j}$, where each $w_{i}$ is homogeneous of degree
$\deg w_{i}=-i$ and $m\in\mathbb{Z}$. Now,
\begin{align*}
\underbrace{U\left(  \mathfrak{g}\right)  }_{=\sum\limits_{i\in\mathbb{Z}%
}U\left(  \mathfrak{g}\right)  \left[  i\right]  }\cdot\underbrace{w}%
_{=\sum\limits_{j=0}^{m}w_{j}}  &  =\left(  \sum\limits_{i\in\mathbb{Z}%
}U\left(  \mathfrak{g}\right)  \left[  i\right]  \right)  \cdot\left(
\sum\limits_{j=0}^{m}w_{j}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}%
\right)  \left[  i\right]  \cdot w_{j}.
\end{align*}
Hence, for every $n\in\mathbb{Z}$, we have
\[
\left(  U\left(  \mathfrak{g}\right)  \cdot w\right)  \left[  n\right]
=\left(  \sum\limits_{i\in\mathbb{Z}}\sum\limits_{j=0}^{m}U\left(
\mathfrak{g}\right)  \left[  i\right]  \cdot w_{j}\right)  \left[  n\right]
=\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}\right)  \left[  n+j\right]  \cdot
w_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M_{\lambda}^{+}\text{ is a
graded }\mathfrak{g}\text{-module}\right)  .
\]
Now, since $U\left(  \mathfrak{g}\right)  \cdot w\neq M_{\lambda}^{+}$, there
exists at least one $n\in\mathbb{Z}$ such that $\left(  U\left(
\mathfrak{g}\right)  \cdot w\right)  \left[  n\right]  \neq M_{\lambda}%
^{+}\left[  n\right]  $. Consider such an $n$. Then, $M_{\lambda}^{+}\left[
n\right]  \neq\left(  U\left(  \mathfrak{g}\right)  \cdot w\right)  \left[
n\right]  =\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}\right)  \left[
n+j\right]  \cdot w_{j}$. Thus, $U\left(  \mathfrak{g}\right)  \left[
n+j\right]  \cdot w_{j}\neq M_{\lambda}^{+}\left[  n\right]  $ for all
$j\in\left\{  0,1,...,m\right\}  $. But some $j\in\left\{  0,1,...,m\right\}
$ satisfies $w_{j}\neq0$ (since $\sum\limits_{j=0}^{m}w_{j}=w\neq0$). Consider
this $j$. Then, $w_{j}$ is a nonzero homogeneous element of $M_{\lambda}^{+}$
satisfying $U\left(  \mathfrak{g}\right)  \cdot w_{j}\neq M_{\lambda}^{+}$
(because $\left(  U\left(  \mathfrak{g}\right)  \cdot w_{j}\right)  \left[
n\right]  =U\left(  \mathfrak{g}\right)  \left[  n+j\right]  \cdot w_{j}\neq
M_{\lambda}^{+}\left[  n\right]  $). This proves that there exists a nonzero
\textit{homogeneous} $v\in M_{\lambda}^{+}$ such that $U\left(  \mathfrak{g}%
\right)  \cdot v\neq M_{\lambda}^{+}$. Qed.} Consider this $v$. Then,
$U\left(  \mathfrak{g}\right)  \cdot v$ is a proper graded submodule of
$M_{\lambda}^{+}$, and thus is contained in $J_{\lambda}^{+}$. Hence,
$J_{\lambda}^{+}\neq0$.

There exist some $d\in\mathbb{Z}$ such that $J_{\lambda}^{+}\left[  d\right]
\neq0$ (since $J_{\lambda}^{+}\neq0$ and since $J_{\lambda}^{+}$ is graded).
All such $d$ are nonpositive (since $J_{\lambda}^{+}$ is nonpositively
graded). Thus, there exists a highest integer $d$ such that $J_{\lambda}%
^{+}\left[  d\right]  \neq0$. Consider this $d$. Clearly, $d<0$ (since the
bilinear form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda
}^{-}$ is obviously nondegenerate on $M_{\lambda}^{+}\left[  0\right]  \times
M_{-\lambda}^{-}\left[  0\right]  $, so that $J_{\lambda}^{+}\left[  0\right]
=0$).

Every $i>0$ satisfies
\begin{align*}
\mathfrak{g}_{i}\cdot\left(  J_{\lambda}^{+}\left[  d\right]  \right)   &
\subseteq J_{\lambda}^{+}\left[  i+d\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }J_{\lambda}^{+}\text{ is a graded }\mathfrak{g}\text{-module}%
\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i+d>d\text{, but }d\text{ was
the highest integer such that }J_{\lambda}^{+}\left[  d\right]  \neq0\right)
.
\end{align*}


By Conditions \textbf{(1)} and \textbf{(2)} of Definition
\ref{def.gradLienondeg}, the Lie algebra $\mathfrak{g}_{0}$ is abelian and
finite-dimensional. Hence, every nonzero $\mathfrak{g}_{0}$-module has a
one-dimensional submodule\footnote{\textit{Proof.} This is because of the
following fact:
\par
Every nonzero finite-dimensional module over an abelian finite-dimensional Lie
algebra has a one-dimensional submodule. (This is just a restatement of the
fact that a finite set of pairwise commuting matrices on a finite-dimensional
nonzero $\mathbb{C}$-vector space has a common nonzero eigenvector.)}. Thus,
the nonzero $\mathfrak{g}_{0}$-module $J_{\lambda}^{+}\left[  d\right]  $ has
a one-dimensional submodule. Let $w$ be the generator of this submodule. Then,
this submodule is $\left\langle w\right\rangle $.

For every $h\in\mathfrak{h}$, the vector $hw$ is a scalar multiple of $w$
(since $h\in\mathfrak{h}=\mathfrak{g}_{0}$, so that $hw$ lies in the
$\mathfrak{g}_{0}$-submodule of $J_{\lambda}^{+}\left[  d\right]  $ generated
by $w$, but this submodule is $\left\langle w\right\rangle $). Thus, we can
write $hw=\lambda_{h}w$ for some $\lambda_{h}\in\mathbb{C}$. This $\lambda
_{h}$ is uniquely determined (since $w\neq0$), so we can define a map
$\mu:\mathfrak{h}\rightarrow\mathbb{C}$ such that $\mu\left(  h\right)
=\lambda_{h}$ for every $h\in\mathfrak{h}$. This map $\mu$ is easily seen to
be $\mathbb{C}$-linear, so that we have found a $\mu\in\mathfrak{h}^{\ast}$
such that%
\[
hw=\mu\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }h\in
\mathfrak{h}.
\]
Also,%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0
\]
(since $\underbrace{a}_{\in\mathfrak{g}_{i}}\underbrace{w}_{\in J_{\lambda
}^{+}\left[  d\right]  }\in\mathfrak{g}_{i}\cdot\left(  J_{\lambda}^{+}\left[
d\right]  \right)  \subseteq0$). Thus, $w$ is a nonzero singular vector. Since
$w\in J_{\lambda}^{+}\left[  d\right]  $ and $d<0$, this vector $w$ is in
negative degrees. This contradicts to the assumption that $M_{\lambda}^{+}$
does not have nonzero singular vectors in negative degrees. This contradiction
shows that our assumption was wrong, so that $M_{\lambda}^{+}$ is irreducible.
This proves the $\Longleftarrow$ direction of Corollary \ref{cor.singvec}.

$\Longrightarrow:$ Assume that $M_{\lambda}^{+}$ is irreducible.

We must then show that $M_{\lambda}^{+}$ does not have nonzero singular
vectors in negative degrees.

Let $v$ be a singular vector of $M_{\lambda}^{+}$ in negative degrees. Let it
be a singular vector of weight $\mu$ for some $\mu\in\mathfrak{h}^{\ast}$.

By Lemma \ref{lem.singvec} (applied to $\mu$ and $M_{\lambda}^{+}$ instead of
$\lambda$ and $Y$), we have an isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\mu}^{+},M_{\lambda}%
^{+}\right)   &  \rightarrow\operatorname*{Sing}\nolimits_{\mu}\left(
M_{\lambda}^{+}\right)  ,\\
\phi &  \mapsto\phi\left(  v_{\mu}^{+}\right)  .
\end{align*}
Let $\phi$ be the preimage of $v$ under this isomorphism. Then, $v=\phi\left(
v_{\mu}^{+}\right)  $.

Since $v$ is in negative degrees, we have $v\in\sum\limits_{n<0}M_{\lambda
}^{+}\left[  n\right]  $. Now, $M_{\mu}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\mu}^{+}=\sum\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[
m\right]  v_{\mu}^{+}$ (since $M_{\mu}^{+}$ is nonpositively graded), so that%
\begin{align*}
\phi\left(  M_{\mu}^{+}\right)   &  =\phi\left(  \sum\limits_{m\leq0}U\left(
\mathfrak{n}_{-}\right)  \left[  m\right]  v_{\mu}^{+}\right)  =\sum
\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[  m\right]
\underbrace{\phi\left(  v_{\mu}^{+}\right)  }_{=v\in\sum\limits_{n<0}%
M_{\lambda}^{+}\left[  n\right]  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\phi\in\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\mu}%
^{+},M_{\lambda}^{+}\right)  \right) \\
&  \in\sum\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[  m\right]
\sum\limits_{n<0}M_{\lambda}^{+}\left[  n\right]  =\sum\limits_{m\leq0}%
\sum\limits_{n<0}\underbrace{U\left(  \mathfrak{n}_{-}\right)  \left[
m\right]  \cdot M_{\lambda}^{+}\left[  n\right]  }_{\substack{\subseteq
M_{\lambda}^{+}\left[  m+n\right]  \\\text{(since }M_{\lambda}^{+}\text{ is a
graded }\mathfrak{g}\text{-module)}}}\\
&  \subseteq\sum\limits_{m\leq0}\sum\limits_{n<0}M_{\lambda}^{+}\left[
m+n\right]  \subseteq\sum\limits_{r<0}M_{\lambda}^{+}\left[  r\right]  .
\end{align*}
Thus, the projection of $\phi\left(  M_{\mu}^{+}\right)  $ onto the $0$-th
degree of $M_{\lambda}^{+}$ is $0$. Hence, $\phi\left(  M_{\mu}^{+}\right)  $
is a proper $\mathfrak{g}$-submodule of $M_{\lambda}^{+}$. Therefore,
$\phi\left(  M_{\mu}^{+}\right)  =0$ (since $M_{\lambda}^{+}$ is irreducible).
Thus, $v=\phi\left(  v_{\mu}^{+}\right)  \in\phi\left(  M_{\mu}^{+}\right)
=0$, so that $v=0$.

We have thus proven: Whenever $v$ is a singular vector of $M_{\lambda}^{+}$ in
negative degrees, we have $v=0$. In other words, $M_{\lambda}^{+}$ does not
have nonzero singular vectors in negative degrees. This proves the
$\Longrightarrow$ direction of Corollary \ref{cor.singvec}.

Here is a variation on Corollary \ref{cor.singvec}:

\begin{corollary}
\label{cor.singvec.2}The representation $M_{\lambda}^{+}$ is irreducible if
and only if it does not have nonzero homogeneous singular vectors in negative degrees.
\end{corollary}

\textit{Proof of Corollary \ref{cor.singvec.2}.} $\Longrightarrow:$ This
follows from the $\Longrightarrow$ direction of Corollary \ref{cor.singvec}.

$\Longleftarrow:$ Repeat the proof of the $\Longleftarrow$ direction of
Corollary \ref{cor.singvec}, noticing that $w$ is homogeneous (since $w\in
J_{\lambda}^{+}\left[  d\right]  $).

Corollary \ref{cor.singvec.2} is thus proven.

\subsection{Highest/lowest-weight modules}

\begin{definition}
A \textit{highest-weight module} with highest weight $\lambda\in
\mathfrak{h}^{\ast}$ means a quotient $V$ of the graded $\mathfrak{g}$-module
$M_{\lambda}^{+}$ by a proper graded submodule. The projection of $v_{\lambda
}^{+}\in M_{\lambda}^{+}$ onto this quotient will be called a
\textit{highest-weight vector} of $V$. (Note that a highest-weight module may
have several highest-weight vectors: in fact, every nonzero vector in its
$0$-th graded component is a highest-weight vector.) The notion
"highest-weight representation" is also used as a synonym for "highest-weight module".

A \textit{lowest-weight module} with lowest weight $\lambda\in\mathfrak{h}%
^{\ast}$ means a quotient $V$ of the graded $\mathfrak{g}$-module $M_{\lambda
}^{-}$ by a proper graded submodule. The projection of $v_{\lambda}^{-}\in
M_{\lambda}^{-}$ onto this quotient will be called a \textit{lowest-weight
vector} of $V$. (Note that a lowest-weight module may have several
lowest-weight vectors: in fact, every nonzero vector in its $0$-th graded
component is a lowest-weight vector.) The notion "lowest-weight
representation" is also used as a synonym for "lowest-weight module".

If $Y$ is a highest-weight module with highest-weight $\lambda$, then we have
an exact sequence $%
%TCIMACRO{\TeXButton{M surj Y surj L}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{+}_{\lambda}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{+}_{\lambda}
}%
%EndExpansion
$ (by Theorem \ref{thm.verma} \textbf{(ii)}).

If $Y$ is a lowest-weight module with lowest weight $\lambda$, then we have an
exact sequence $%
%TCIMACRO{\TeXButton{M surj Y surj L}{\xymatrix{
%M^{-}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{-}_{\lambda}
%}}}%
%BeginExpansion
\xymatrix{
M^{-}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{-}_{\lambda}
}%
%EndExpansion
$ (by Theorem \ref{thm.verma} \textbf{(ii)}).
\end{definition}

\subsection{Categories $\mathcal{O}^{+}$ and $\mathcal{O}^{-}$}

The category of all $\mathfrak{g}$-modules for a graded Lie algebra is normally

We now define the so-called Category $\mathcal{O}$; actually, there are two of
these categories, $\mathcal{O}^{+}$ and $\mathcal{O}^{-}$, which are
antiequivalent to each other (in general) and equivalent to each other (in
some more restrictive cases). There are several definitions for each of these
categories, and some of them are not even equivalent to each other, although
they mostly differ in minor technicalities. Here is our definition:

\begin{definition}
\label{def.O+}The objects of \textit{category }$\mathcal{O}^{+}$ will be
$\mathbb{C}$-graded $\mathfrak{g}$-modules $M$ such that:

\textbf{(1)} all degrees lie in a halfplane $\operatorname{Re}z<a$ and fall
into finitely many arithmetic progressions with step $1$;

\textbf{(2)} for every $d\in\mathbb{C}$, the space $M\left[  d\right]  $ is finite-dimensional.

The \textit{morphisms of category }$\mathcal{O}^{+}$ will be graded
$\mathfrak{g}$-module homomorphisms.
\end{definition}

\begin{definition}
\label{def.O-}The objects of \textit{category }$\mathcal{O}^{-}$ will be
$\mathbb{C}$-graded $\mathfrak{g}$-modules $M$ such that:

\textbf{(1)} all degrees lie in a halfplane $\operatorname{Re}z>a$ and fall
into finitely many arithmetic progressions with step $1$;

\textbf{(2)} for every $d\in\mathbb{C}$, the space $M\left[  d\right]  $ is finite-dimensional.

The \textit{morphisms of category }$\mathcal{O}^{-}$ will be graded
$\mathfrak{g}$-module homomorphisms.
\end{definition}

It is rather clear that for a nondegenerate $\mathbb{Z}$-graded Lie algebra
(or, more generally, for a $\mathbb{Z}$-graded Lie algebra satisfying
conditions \textbf{(1)} and \textbf{(2)} of Definition \ref{def.gradLienondeg}%
), the Verma highest-weight module $M_{\lambda}^{+}$ lies in category
$\mathcal{O}^{+}$ for every $\lambda\in\mathfrak{h}^{\ast}$, and the Verma
lowest-weight module $M_{\lambda}^{-}$ lies in category $\mathcal{O}^{-}$ for
every $\lambda\in\mathfrak{h}^{\ast}$.

\begin{definition}
Let $V$ and $W$ be two $\mathbb{C}$-graded vector spaces, and $x\in\mathbb{C}%
$. A map $f:V\rightarrow W$ is said to be \textit{homogeneous of degree }$x$
if and only if every $z\in\mathbb{C}$ satisfies $f\left(  V\left[  z\right]
\right)  \subseteq W\left[  z+x\right]  $. (For example, this yields that a
map is homogeneous of degree $0$ if and only if it is graded.)
\end{definition}

\begin{proposition}
\label{prop.O.irred}The irreducible modules in category $\mathcal{O}^{\pm}$
(up to homogeneous isomorphism) are $L_{\lambda}^{\pm}$ for varying
$\lambda\in\mathbb{C}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.O.irred}.} First of all, for every
$\lambda\in\mathfrak{h}^{\ast}$, the $\mathfrak{g}$-module $L_{\lambda}^{+}$
has a unique singular vector (up to scaling), and this vector is a singular
vector of weight $\lambda$.\ \ \ \ \footnote{\textit{Proof.} It is clear that
$\overline{v_{\lambda}^{+}}\in L_{\lambda}^{+}$ is a singular vector of weight
$\lambda$. Now we must prove that it is the only singular vector (up to
scaling).
\par
In fact, assume the opposite. Then, there exists a singular vector in
$L_{\lambda}^{+}$ which is not a scalar multiple of $\overline{v_{\lambda}%
^{+}}$. This singular vector must have a nonzero $d$-th homogeneous component
for some $d<0$ (because it is not a scalar multiple of $\overline{v_{\lambda
}^{+}}$), and this component itself must be a singular vector (since any
homogeneous component of a singular vector must itself be a singular vector).
So the module $L_{\lambda}^{+}$ has a nonzero homogeneous singular vector $w$
of degree $d$.
\par
Now, repeat the proof of the $\Longrightarrow$ part of Corollary
\ref{cor.singvec}, with $M_{\lambda}^{+}$ replaced by $L_{\lambda}^{+}$ (using
the fact that $L_{\lambda}^{+}$ is irreducible). As a consequence, it follows
that $L_{\lambda}^{+}$ does not have nonzero singular vectors in negative
degrees. This contradicts the fact that the module $L_{\lambda}^{+}$ has a
nonzero homogeneous singular vector $w$ of degree $d<0$. This contradiction
shows that our assumption was wrong, so that indeed, $\overline{v_{\lambda
}^{+}}$ is the only singular vector of $L_{\lambda}^{+}$ (up to scaling),
qed.} Thus, the $\mathfrak{g}$-modules $L_{\lambda}^{+}$ are pairwise
nonisomorphic for varying $\lambda$. Similarly, the $\mathfrak{g}$-modules
$L_{\lambda}^{-}$ are pairwise nonisomorphic for varying $\lambda$.

Let $Y$ be any irreducible module in category $\mathcal{O}^{+}$. We are now
going to prove that $Y\cong L_{\lambda}^{+}$ for some $\lambda\in
\mathfrak{h}^{\ast}$.

Let $d$ be a complex number such that $Y\left[  d\right]  \neq0$ and $Y\left[
d+j\right]  =0$ for all $j\geq1$. (Such a complex number exists due to
condition \textbf{(1)} in Definition \ref{def.O+}.) For every $v\in Y\left[
d\right]  $, we have $av=0$ for every $a\in\mathfrak{g}_{i}$ for every
$i>0$\ \ \ \ \footnote{\textit{Proof.} Let $i>0$ and $a\in\mathfrak{g}_{i}$.
Then, $i\geq1$. Now, $a\in\mathfrak{g}_{i}$ and $v\in Y\left[  d\right]  $
yield $av\in\mathfrak{g}_{i}\cdot Y\left[  d\right]  \subseteq Y\left[
d+i\right]  =0$ (since $Y\left[  d+j\right]  =0$ for all $j\geq1$), so that
$av=0$, qed.}.

By Conditions \textbf{(1)} and \textbf{(2)} of Definition
\ref{def.gradLienondeg}, the Lie algebra $\mathfrak{g}_{0}$ is abelian and
finite-dimensional. Hence, every nonzero $\mathfrak{g}_{0}$-module has a
one-dimensional submodule\footnote{\textit{Proof.} This is because of the
following fact:
\par
Every nonzero finite-dimensional module over an abelian finite-dimensional Lie
algebra has a one-dimensional submodule. (This is just a restatement of the
fact that a finite set of pairwise commuting matrices on a finite-dimensional
nonzero $\mathbb{C}$-vector space has a common nonzero eigenvector.)}. Thus,
the nonzero $\mathfrak{g}_{0}$-module $Y\left[  d\right]  $ has a
one-dimensional submodule. Let $w$ be the generator of this submodule. Then,
this submodule is $\left\langle w\right\rangle $.

For every $h\in\mathfrak{h}$, the vector $hw$ is a scalar multiple of $w$
(since $h\in\mathfrak{h}=\mathfrak{g}_{0}$, so that $hw$ lies in the
$\mathfrak{g}_{0}$-submodule of $Y\left[  d\right]  $ generated by $w$, but
this submodule is $\left\langle w\right\rangle $). Thus, we can write
$hw=\lambda_{h}w$ for some $\lambda_{h}\in\mathbb{C}$. This $\lambda_{h}$ is
uniquely determined (since $h\neq0$), so we can define a map $\lambda
:\mathfrak{h}\rightarrow\mathbb{C}$ such that $\lambda\left(  h\right)
=\lambda_{h}$ for every $h\in\mathfrak{h}$. This map $\lambda$ is easily seen
to be $\mathbb{C}$-linear, so that we have found a $\lambda\in\mathfrak{h}%
^{\ast}$ such that%
\[
hw=\lambda\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }%
h\in\mathfrak{h}.
\]
Also,%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0
\]
(since $av=0$ for every $v\in Y\left[  d\right]  $ and every $a\in
\mathfrak{g}_{i}$ for every $i>0$). Thus, $w$ is a nonzero singular vector of
weight $\lambda$.

By Lemma \ref{lem.singvec}, we have an isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
&  \rightarrow\operatorname*{Sing}\nolimits_{\lambda}Y,\\
\phi &  \mapsto\phi\left(  v_{\lambda}^{+}\right)  .
\end{align*}
Let $\phi$ be the preimage of $w$ under this isomorphism. Then, $w=\phi\left(
v_{\lambda}^{+}\right)  $. Since $w\in Y\left[  d\right]  $, it is easy to see
that $\phi$ is a homogeneous homomorphism of degree $d$ (in fact, every
$n\in\mathbb{Z}$ satisfies $M_{\lambda}^{+}\left[  n\right]  =U\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  \cdot v_{\lambda}^{+}$, so that%
\begin{align*}
\phi\left(  M_{\lambda}^{+}\left[  n\right]  \right)   &  =\phi\left(
U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \cdot v_{\lambda}%
^{+}\right)  =U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]
\cdot\underbrace{\phi\left(  v_{\lambda}^{+}\right)  }_{=w\in Y\left[
d\right]  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\phi\text{ is
}\mathfrak{g}\text{-linear}\right) \\
&  \subseteq U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \cdot
Y\left[  d\right]  \subseteq Y\left[  n+d\right]
\end{align*}
). This homomorphism $\phi$ must be surjective, since $Y$ is irreducible.
Thus, we have a homogeneous isomorphism $M_{\lambda}^{+}\diagup\left(
\operatorname*{Ker}\phi\right)  \cong Y$. Also, $\operatorname*{Ker}\phi$ is a
proper graded submodule of $M_{\lambda}^{+}$, thus a submodule of $J_{\lambda
}^{+}$ (by Theorem \ref{thm.verma} \textbf{(ii)}). Hence, we have a projection
$M_{\lambda}^{+}\diagup\left(  \operatorname*{Ker}\phi\right)  \rightarrow
M_{\lambda}^{+}\diagup J_{\lambda}^{+}$. Since $M_{\lambda}^{+}\diagup\left(
\operatorname*{Ker}\phi\right)  \cong Y$ is irreducible, this projection must
either be an isomorphism or the zero map. It cannot be the zero map (since it
is a projection onto the nonzero module $M_{\lambda}^{+}\diagup J_{\lambda
}^{+}$), so it therefore is an isomorphism. Thus, $M_{\lambda}^{+}\diagup
J_{\lambda}^{+}\cong M_{\lambda}^{+}\diagup\left(  \operatorname*{Ker}%
\phi\right)  \cong Y$, so we have a homogeneous isomorphism $Y\cong
M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$.

We thus have showed that any irreducible module in category $\mathcal{O}^{+}$
is isomorphic to $L_{\lambda}^{+}$ for some $\lambda\in\mathfrak{h}^{\ast}$.
Similarly, the analogous assertion holds for $\mathcal{O}^{-}$. Proposition
\ref{prop.O.irred} is thus proven.

\begin{definition}
Let $M$ be a module in category $\mathcal{O}^{+}$. We define the
\textit{character} $\operatorname*{ch}M$ of $M$ as follows:

Write $M=\bigoplus\limits_{d}M\left[  d\right]  $. Then, define
$\operatorname*{ch}M$ by%
\[
\operatorname*{ch}M=\sum\limits_{d}q^{-d}\operatorname*{tr}\nolimits_{M\left[
d\right]  }\left(  e^{x}\right)  \ \ \ \ \ \ \ \ \ \ \text{as a power series
in }q
\]
for every $x\in\mathfrak{h}$. We also write $\left(  \operatorname*{ch}%
M\right)  \left(  q,x\right)  $ for this, so it becomes a formal power series
in both $q$ and $x$. (Note that this power series can contain noninteger
powers of $q$, but due to $M\in\mathcal{O}^{+}$, the exponents in these powers
are bounded from above in their real part, and fall into infinitely many
arithmetic progressions with step $1$.)
\end{definition}

\begin{proposition}
\label{prop.chVerma}Here is an example:%
\[
\left(  \operatorname*{ch}M_{\lambda}^{+}\right)  \left(  x\right)  =\dfrac
{1}{\prod\limits_{j>0}\det\nolimits_{\mathfrak{g}\left[  -j\right]  }\left(
1-q^{j}e^{\operatorname*{ad}\left(  x\right)  }\right)  }.
\]
(To prove this, use Molien's identity which states that, for every linear map
$A:V\rightarrow V$, we have%
\[
\sum\limits_{n\in\mathbb{N}}q^{n}\operatorname*{Tr}\nolimits_{S^{n}V}\left(
S^{n}A\right)  =\dfrac{1}{\det\left(  1-qA\right)  },
\]
where $S^{n}A$ denotes the $n$-th symmetric power of the operator $A$.)
\end{proposition}

Let us consider some examples:

\begin{example}
\label{exa.sl2}Let $\mathfrak{g}=\mathfrak{sl}_{2}$. We can write this Lie
algebra in terms of Chevalley generators and Serre relations (this is a
particular case of what we did in Proposition \ref{prop.grad.g}). The most
traditional way to do this is by setting $e=\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $, $f=\left(
\begin{array}
[c]{cc}%
0 & 0\\
1 & 0
\end{array}
\right)  $ and $h=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & -1
\end{array}
\right)  $; then, $\mathfrak{g}$ is generated by $e$, $f$ and $h$ as a Lie
algebra, and these generators satisfy $\left[  h,e\right]  =2e$, $\left[
h,f\right]  =-2f$ and $\left[  e,f\right]  =h$. Also, $\left(  e,f,h\right)  $
is a basis of the vector space $\mathfrak{g}$. In accordance with Proposition
\ref{prop.grad.g}, we grade $\mathfrak{g}$ by setting $\deg e=1$, $\deg f=-1$
and $\deg h=0$. Then, $\mathfrak{n}_{+}=\left\langle e\right\rangle $,
$\mathfrak{n}_{-}=\left\langle f\right\rangle $ and $\mathfrak{h}=\left\langle
h\right\rangle $. Hence, linear maps $\lambda:\mathfrak{h}\rightarrow
\mathbb{C}$ are in 1-to-1 correspondence with complex numbers (namely, the
images $\lambda\left(  h\right)  $ of $h$ under these maps). Thus, we can
identify any linear map $\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ with the
image $\lambda\left(  h\right)  \in\mathbb{C}$.

Consider any $\lambda\in\mathfrak{h}^{\ast}$. Since $\mathfrak{n}%
_{-}=\left\langle f\right\rangle $, the universal enveloping algebra $U\left(
\mathfrak{n}_{-}\right)  $ is the polynomial algebra $\mathbb{C}\left[
f\right]  $, and Proposition \ref{prop.verma1} \textbf{(a)} yields
$M_{\lambda}^{+}=\underbrace{U\left(  \mathfrak{n}_{-}\right)  }%
_{=\mathbb{C}\left[  f\right]  }v_{\lambda}^{+}=\mathbb{C}\left[  f\right]
v_{\lambda}^{+}$. Similarly, $M_{-\lambda}^{-}=\mathbb{C}\left[  e\right]
v_{-\lambda}^{-}$. In order to compute the bilinear form $\left(  \cdot
,\cdot\right)  $ on $M_{\lambda}^{+}\times M_{-\lambda}^{-}$, it is thus
enough to compute $\left(  f^{n}v_{\lambda}^{+},e^{n}v_{-\lambda}^{-}\right)
$ for all $n\in\mathbb{N}$. (The values $\left(  f^{n}v_{\lambda}^{+}%
,e^{m}v_{-\lambda}^{-}\right)  $ for $n\neq m$ are zero since the form has
degree $0$.) In order to do this, we notice that $e^{n}f^{n}v_{\lambda}%
^{+}=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
v_{\lambda}^{+}$\ \ \ \ \footnote{\textit{Proof (sketched).} First show that
$hf^{m}v_{\lambda}^{+}=\left(  \lambda-2m\right)  f^{m}v_{\lambda}^{+}$ for
every $m\in\mathbb{N}$. (This follows easily by induction over $m$, using
$hf-fh=\left[  h,f\right]  =-2f$.)
\par
Next show that $ef^{n}v_{\lambda}^{+}=n\left(  \lambda-n+1\right)
f^{n-1}v_{\lambda}^{+}$ for every positive $n\in\mathbb{N}$. (This is again an
easy induction proof using the equalities $ef-fe=\left[  e,f\right]  =h$,
$hv_{\lambda}^{+}=\underbrace{\lambda\left(  h\right)  }_{=\lambda}v_{\lambda
}^{+}=\lambda v_{\lambda}^{+}$ and $ev_{\lambda}^{+}=0$, and using the
equality $hf^{m}v_{\lambda}^{+}=\left(  \lambda-2m\right)  f^{m}v_{\lambda
}^{+}$ applied to $m=n-1$.)
\par
Now show that $e^{n}f^{n}v_{\lambda}^{+}=n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  v_{\lambda}^{+}$ for every $n\in\mathbb{N}$.
(For this, again use induction.)} and thus%
\begin{align*}
\left(  f^{n}v_{\lambda}^{+},e^{n}v_{-\lambda}^{-}\right)   &  =\left(
\underbrace{S\left(  e^{n}\right)  }_{=\left(  -1\right)  ^{n}e^{n}}%
f^{n}v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =\left(  \left(  -1\right)
^{n}\underbrace{e^{n}f^{n}}_{\substack{=n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  }}v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =\left(  \left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =\left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  \underbrace{\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  }_{=1}\\
&  =\left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  .
\end{align*}
So $M_{\lambda}^{+}$ is irreducible if $\lambda\notin\mathbb{Z}_{+}$. If
$\lambda\in\mathbb{Z}_{+}$, then $J_{\lambda}^{+}=\left\langle f^{n}%
v_{\lambda}^{+}\ \mid\ n\geq\lambda+1\right\rangle =\mathbb{C}\left[
f\right]  \cdot\left(  f^{\lambda+1}v_{\lambda}^{+}\right)  $, and the
irreducible $\mathfrak{g}$-module $L_{\lambda}^{+}=\left\langle \overline
{v_{\lambda}^{+}},f\overline{v_{\lambda}^{+}},...,f^{\lambda}\overline
{v_{\lambda}^{+}}\right\rangle $ has dimension $\dim\lambda+1$%
.\ \ \ \ \footnote{If you know the representation theory of $\mathfrak{sl}%
_{2}$, you probably recognize this module $L_{\lambda}^{+}$ as the $\left(
\dim\lambda\right)  $-th symmetric power of the vector module $\mathbb{C}^{2}$
(as there is only one irreducible $\mathfrak{sl}_{2}$-module of every
dimension).}
\end{example}

\begin{example}
\label{exa.Vir}Let $\mathfrak{g}=\operatorname*{Vir}$. With the grading that
we have defined on $\operatorname*{Vir}$, we have $\mathfrak{h}=\mathfrak{g}%
_{0}=\left\langle L_{0},C\right\rangle $. Thus, linear maps $\lambda
:\mathfrak{h}\rightarrow\mathbb{C}$ can be uniquely described by the images of
$L_{0}$ and $C$ under these maps. We thus identify every linear map
$\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ with the pair $\left(
\lambda\left(  L_{0}\right)  ,\lambda\left(  C\right)  \right)  $.

For every $\lambda=\left(  \lambda\left(  L_{0}\right)  ,\lambda\left(
C\right)  \right)  $, the number $\lambda\left(  L_{0}\right)  $ is denoted by
$h$ and called the \textit{conformal weight} of $\lambda$, and the number
$\lambda\left(  C\right)  $ is denoted by $c$ and called the \textit{central
charge} of $\lambda$. Thus, $\lambda$ is identified with the pair $\left(
h,c\right)  $. As a consequence, the Verma modules $M_{\lambda}^{+}$ and
$M_{\lambda}^{-}$ are often denoted by $M_{h,c}^{+}$ and $M_{h,c}^{-}$,
respectively, and the modules $L_{\lambda}^{+}$ and $L_{\lambda}^{-}$ are
often denoted by $L_{h,c}^{+}$ and $L_{h,c}^{-}$, respectively.

Consider any $\lambda\in\mathfrak{h}^{\ast}$. Let us compute the bilinear form
$\left(  \cdot,\cdot\right)  $ on $M_{\lambda}^{+}\times M_{-\lambda}^{-}$. In
order to compute $\left(  L_{-1}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)
$, we notice that
\[
\underbrace{L_{1}L_{-1}}_{=L_{-1}L_{1}+\left[  L_{1},L_{-1}\right]
}v_{\lambda}^{+}=L_{-1}\underbrace{L_{1}v_{\lambda}^{+}}_{=0}%
+\underbrace{\left[  L_{1},L_{-1}\right]  }_{=2L_{0}}v_{\lambda}%
^{+}=2\underbrace{L_{0}v_{\lambda}^{+}}_{=\lambda\left(  L_{0}\right)
v_{\lambda}^{+}}=2\underbrace{\lambda\left(  L_{0}\right)  }_{=h}v_{\lambda
}^{+}=2hv_{\lambda}^{+},
\]
so that%
\[
\left(  L_{-1}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)  =\left(
-\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}},v_{-\lambda}%
^{-}\right)  =\left(  -2hv_{\lambda}^{+},v_{-\lambda}^{-}\right)
=-2h\underbrace{\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  }_{=1}=-2h.
\]
Since $\left(  L_{-1}v_{\lambda}^{+}\right)  $ is a basis of $M_{\lambda}%
^{+}\left[  -1\right]  $ and $\left(  L_{1}v_{-\lambda}^{-}\right)  $ is a
basis of $M_{-\lambda}^{-}\left[  1\right]  $, this yields $\det\left(
\left(  \cdot,\cdot\right)  _{1}\right)  =2h$ (where $\left(  \cdot
,\cdot\right)  _{1}$ denotes the restriction of the form $\left(  \cdot
,\cdot\right)  $ to $M_{\lambda}^{+}\left[  -1\right]  \times M_{-\lambda}%
^{-}\left[  1\right]  $). This vanishes for $h=0$.

In degree $2$, the form is somewhat more complicated: With respect to the
basis $\left(  L_{-1}^{2}v_{\lambda}^{+},L_{-2}v_{\lambda}^{+}\right)  $ of
$M_{\lambda}^{+}\left[  -2\right]  $, and the basis $\left(  L_{1}%
^{2}v_{-\lambda}^{-},L_{2}v_{-\lambda}^{-}\right)  $ of $M_{-\lambda}%
^{-}\left[  2\right]  $, the restriction $\left(  \cdot,\cdot\right)  _{2}$ of
the form $\left(  \cdot,\cdot\right)  $ to $M_{\lambda}^{+}\left[  -2\right]
\times M_{-\lambda}^{-}\left[  2\right]  $ is given by the matrix%
\[
\left(
\begin{array}
[c]{cc}%
\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)  & \left(
L_{-1}^{2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right) \\
\left(  L_{-2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)  & \left(
L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)
\end{array}
\right)  .
\]


Let us compute, as an example, the lower right entry of this matrix, that is,
the entry $\left(  L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)  $. We
have%
\begin{align*}
\underbrace{L_{2}L_{-2}}_{=L_{-2}L_{2}+\left[  L_{2},L_{-2}\right]
}v_{\lambda}^{+}  &  =L_{-2}\underbrace{L_{2}v_{\lambda}^{+}}_{=0}%
+\underbrace{\left[  L_{2},L_{-2}\right]  }_{=4L_{0}+\dfrac{1}{2}C}v_{\lambda
}^{+}=\left(  4L_{0}+\dfrac{1}{2}C\right)  v_{\lambda}^{+}\\
&  =4\underbrace{L_{0}v_{\lambda}^{+}}_{=\lambda\left(  L_{0}\right)
v_{\lambda}^{+}}+\dfrac{1}{2}\underbrace{Cv_{\lambda}^{+}}_{=\lambda\left(
C\right)  v_{\lambda}^{+}}=4\underbrace{\lambda\left(  L_{0}\right)  }%
_{=h}v_{\lambda}^{+}+\dfrac{1}{2}\underbrace{\lambda\left(  C\right)  }%
_{=c}v_{\lambda}^{+}\\
&  =\left(  4h+\dfrac{1}{2}c\right)  v_{\lambda}^{+},
\end{align*}
so that%
\begin{align*}
\left(  L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)   &  =\left(
-\underbrace{L_{2}L_{-2}v_{\lambda}^{+}}_{=\left(  4h+\dfrac{1}{2}c\right)
v_{\lambda}^{+}},v_{-\lambda}^{-}\right)  =\left(  -\left(  4h+\dfrac{1}%
{2}c\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =-\left(  4h+\dfrac{1}{2}c\right)  \underbrace{\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  }_{=1}=-\left(  4h+\dfrac{1}{2}c\right)  .
\end{align*}
As a further (more complicated) example, let us compute the upper left entry
of the matrix, namely $\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda
}^{-}\right)  $. We have%
\begin{align*}
L_{1}^{2}L_{-1}^{2}v_{\lambda}^{+}  &  =L_{1}\underbrace{L_{1}L_{-1}}%
_{=L_{-1}L_{1}+\left[  L_{1},L_{-1}\right]  }L_{-1}v_{\lambda}^{+}=L_{1}%
L_{-1}\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}%
+L_{1}\underbrace{\left[  L_{1},L_{-1}\right]  }_{=2L_{0}}L_{-1}v_{\lambda
}^{+}\\
&  =2h\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}%
+2L_{1}\underbrace{L_{0}L_{-1}}_{\substack{=L_{-1}L_{0}+\left[  L_{0}%
,L_{-1}\right]  \\=L_{-1}L_{0}+L_{-1}\\\text{(since }\left[  L_{0}%
,L_{-1}\right]  =L_{-1}\text{)}}}v_{\lambda}^{+}=4h^{2}v_{\lambda}^{+}%
+2L_{1}L_{-1}\underbrace{L_{0}v_{\lambda}^{+}}_{\substack{=\lambda\left(
L_{0}\right)  v_{\lambda}^{+}=hv_{\lambda}^{+}\\\text{(since }\lambda\left(
L_{0}\right)  =h\text{)}}}+2\underbrace{L_{1}L_{-1}v_{\lambda}^{+}%
}_{=2hv_{\lambda}^{+}}\\
&  =4h^{2}v_{\lambda}^{+}+2h\underbrace{L_{1}L_{-1}v_{\lambda}^{+}%
}_{=2hv_{\lambda}^{+}}+4hv_{\lambda}^{+}=4h^{2}v_{\lambda}^{+}+4h^{2}%
v_{\lambda}^{+}+4hv_{\lambda}^{+}=\left(  8h^{2}+4h\right)  v_{\lambda}^{+}%
\end{align*}
and thus%
\begin{align*}
\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)   &
=\left(  -L_{1}L_{-1}^{2}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)
=\left(  \underbrace{L_{1}^{2}L_{-1}^{2}v_{\lambda}^{+}}_{=\left(
8h^{2}+4h\right)  v_{\lambda}^{+}},v_{-\lambda}^{-}\right)  =\left(  \left(
8h^{2}+4h\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =\left(  8h^{2}+4h\right)  \underbrace{\left(  v_{\lambda}^{+},v_{-\lambda
}^{-}\right)  }_{=1}=8h^{2}+4h.
\end{align*}


Similarly, we compute the other two entries of the matrix. The matrix thus
becomes%
\[
\left(
\begin{array}
[c]{cc}%
8h^{2}+4h & 6h\\
-6h & -\left(  4h+\dfrac{1}{2}c\right)
\end{array}
\right)  .
\]
The determinant of this matrix is%
\[
\det\left(  \left(  \cdot,\cdot\right)  _{2}\right)  =\left(  8h^{2}%
+4h\right)  \left(  -\left(  4h+\dfrac{1}{2}c\right)  \right)  -6h\left(
-6h\right)  =-4h\left(  \left(  2h+1\right)  \left(  4h+\dfrac{1}{2}c\right)
-9h\right)  .
\]
Notice the term $\left(  2h+1\right)  \left(  4h+\dfrac{1}{2}c\right)  -9h$:
The set of zeroes of this term is a hyperbola (an affine conic). The
determinant of $\left(  \cdot,\cdot\right)  _{2}$ thus vanishes on the union
of a line and a hyperbola. For every point $\left(  h,c\right)  $ lying on
this hyperbola, the highest-weight module $M_{h,c}^{+}$ has a nonzero singular
vector in degree $-2$ (this means a nonzero singular vector of the form
$\alpha L_{-2}v_{\lambda}^{+}+\beta L_{-1}^{2}v_{\lambda}^{+}$ for some
$\alpha,\beta\in\mathbb{C}$).

We will later discuss $\det\left(  \left(  \cdot,\cdot\right)  _{n}\right)  $
for generic $n$. In fact, there is an explicit formula for this determinant,
namely the so-called Kac determinant formula.
\end{example}

\subsubsection{Restricted dual modules}

\begin{definition}
Let $V=\bigoplus\limits_{i\in I}V\left[  i\right]  $ be a graded vector space
(where $I$ might be $\mathbb{Z}$, $\mathbb{N}$, $\mathbb{C}$ or any other
set). The \textit{restricted dual} $V^{\vee}$ of $V$ is defined to be the
direct sum $\bigoplus\limits_{i\in I}V\left[  i\right]  ^{\ast}$. This is a
vector subspace of the dual $V^{\ast}$ of $V$, but (in general) not the same
as $V^{\ast}$ unless the direct sum is finite.

If $V\left[  i\right]  $ is finite-dimensional for every $i\in I$, then
$V^{\vee\vee}\cong V$ canonically.

If $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie algebra, and $V$ is a
$\mathbb{C}$-graded $\mathfrak{g}$-module, then $V^{\vee}$ canonically becomes
a $\mathbb{C}$-graded $\mathfrak{g}$-module. (The grading on $V^{\vee}$ is
such that $V^{\vee}\left[  -i\right]  =V\left[  i\right]  ^{\ast}$ for every
$i\in\mathbb{C}$.)
\end{definition}

It is clear that:

\begin{proposition}
We have two mutually inverse antiequivalences of categories $\mathcal{O}%
^{+}\overset{\vee}{\rightarrow}\mathcal{O}^{-}$ and $\mathcal{O}%
^{-}\overset{\vee}{\rightarrow}\mathcal{O}^{+}$, each defined by mapping every
$\mathfrak{g}$-module in one category to its restricted dual.
\end{proposition}

We can view the form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ as a linear map $M_{\lambda}%
^{+}\rightarrow\left(  M_{-\lambda}^{-}\right)  ^{\vee}$. The kernel of this
map is $J_{\lambda}^{+}$, and therefore, when $\mathfrak{g}$ is nondegenerate,
this map is an isomorphism for generic $\lambda$. In general, this map factors
as $%
%TCIMACRO{\TeXButton{diag}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong} &
%\left(L^{-}_{-\lambda}\right)^{\vee} \arinj[r] & \left(M^{-}_{-\lambda}%
%\right)^{\vee}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong} &
\left(L^{-}_{-\lambda}\right)^{\vee} \arinj[r] & \left(M^{-}_{-\lambda}%
\right)^{\vee}
}%
%EndExpansion
$.

\subsubsection{Involutions}

In many applications, we are not just working with a graded Lie algebra
$\mathfrak{g}$. Very often we additionally have a degree-reversing involution:

\begin{definition}
\label{def.invol}Let $\mathfrak{g}$ be a graded Lie algebra. Let
$\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be an involutive automorphism of
the Lie algebra $\mathfrak{g}$ ("involutive" means $\omega^{2}%
=\operatorname*{id}$) such that $\omega\left(  \mathfrak{g}_{i}\right)
=\mathfrak{g}_{-i}$ for all $i\in\mathbb{Z}$ and such that $\omega
\mid_{\mathfrak{g}_{0}}=-1$. Then, for every graded $\mathfrak{g}$-module $M$,
we can define a graded $\mathfrak{g}$-module $M^{c}$ as being the
$\mathfrak{g}$-module $M^{\omega}$ with opposite grading (i. e., the grading
on $M^{c}$ is defined by $M^{c}\left[  i\right]  =M^{\omega}\left[  -i\right]
$ for every $i$). Then, we have an equivalence of categories $\mathcal{O}%
^{+}\overset{\omega}{\rightarrow}\mathcal{O}^{-}$ which sends every
$\mathfrak{g}$-module $M\in\mathcal{O}^{+}$ to the $\mathfrak{g}$-module
$M^{c}\in\mathcal{O}^{-}$, and the quasiinverse equivalence of categories
$\mathcal{O}^{-}\overset{\omega}{\rightarrow}\mathcal{O}^{+}$ which does the
same thing.

So the functor $\mathcal{O}^{+}\overset{\vee}{\rightarrow}\mathcal{O}%
^{-}\overset{\omega}{\rightarrow}\mathcal{O}^{+}$ is an antiequivalence,
called the \textit{functor of contragredient module}. This functor allows us
to identify $\left(  M_{-\lambda}^{-}\right)  ^{\omega}$ with $M_{\lambda}%
^{+}$ (via the isomorphism $M_{\lambda}^{+}\rightarrow\left(  M_{-\lambda}%
^{-}\right)  ^{\omega}$ which sends $x\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }v_{\lambda}^{+}$ to $\left(  U\left(
\omega\right)  \right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }v_{-\lambda}^{-}$ for every $x\in U\left(
\mathfrak{g}\right)  $), and thus to view the form $\left(  \cdot
,\cdot\right)  $ as a form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$. But this form is not symmetric; it is
contravariant: $\left(  av,w\right)  =-\left(  v,\omega\left(  a\right)
w\right)  $ and $\left(  v,aw\right)  =-\left(  \omega\left(  a\right)
v,w\right)  $.

This form can be viewed as a linear map $M_{\lambda}^{+}\rightarrow\left(
M_{\lambda}^{+}\right)  ^{c}$, which factors into $%
%TCIMACRO{\TeXButton{diag}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong}
%& \left(L^{+}_{\lambda}\right)^{c} \arinj[r] & \left(M^{+}_{\lambda}%
%\right)^{c}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong}
& \left(L^{+}_{\lambda}\right)^{c} \arinj[r] & \left(M^{+}_{\lambda}%
\right)^{c}
}%
%EndExpansion
$.
\end{definition}

Involutive automorphisms of $\mathfrak{g}$ satisfying the conditions of
Definition \ref{def.invol} are not uncommon; here are four examples:

\begin{proposition}
The $\mathbb{C}$-linear map $\omega:\mathcal{A}\rightarrow\mathcal{A}$ defined
by $\omega\left(  K\right)  =-K$ and $\omega\left(  a_{i}\right)  =-a_{-i}$
for every $i\in\mathbb{Z}$ is an involutive automorphism of the Lie algebra
$\mathcal{A}$. This automorphism $\omega$ satisfies the conditions of
Definition \ref{def.invol} (for $\mathfrak{g}=\mathcal{A}$).
\end{proposition}

\begin{proposition}
The $\mathbb{C}$-linear map $\omega:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ defined by $\omega\left(  C\right)  =-C$ and
$\omega\left(  L_{i}\right)  =-L_{-i}$ for every $i\in\mathbb{Z}$ is an
involutive automorphism of the Lie algebra $\operatorname*{Vir}$. This
automorphism $\omega$ satisfies the conditions of Definition \ref{def.invol}
(for $\mathfrak{g}=\operatorname*{Vir}$).
\end{proposition}

\begin{proposition}
\label{prop.simple.omega}Let $\mathfrak{g}$ be a simple Lie algebra, graded
and presented as in Proposition \ref{prop.grad.g}. Then, there exists a unique
Lie algebra homomorphism $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$
satisfying $\omega\left(  e_{i}\right)  =-f_{i}$, $\omega\left(  h_{i}\right)
=-h_{i}$ and $\omega\left(  f_{i}\right)  =-e_{i}$ for every $i\in\left\{
1,2,...,m\right\}  $. This automorphism $\omega$ satisfies the conditions of
Definition \ref{def.invol}.
\end{proposition}

\begin{proposition}
Let $\mathfrak{g}$ be a simple finite-dimensional Lie algebra, graded and
presented as in Proposition \ref{prop.grad.g}. Let $\widehat{\mathfrak{g}}$ be
the Kac-Moody Lie algebra defined in Definition \ref{def.kac}. Let $K$ denote
the element $\left(  0,1\right)  $ of $\mathfrak{g}\left[  t,t^{-1}\right]
\otimes\mathbb{C}=\widehat{\mathfrak{g}}$.

Let $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be defined as in Proposition
\ref{prop.simple.omega}. Then, the $\mathbb{C}$-linear map $\widehat{\omega
}:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$ defined by
$\widehat{\omega}\left(  a\cdot t^{j}\right)  =\omega\left(  a\right)  t^{-j}$
for every $a\in\mathfrak{g}$ and $j\in\mathbb{N}$, and $\widehat{\omega
}\left(  K\right)  =-K$, is an involutive automorphism of the Lie algebra
$\widehat{\mathfrak{g}}$. This automorphism $\widehat{\omega}$ satisfies the
conditions of Definition \ref{def.invol} (for $\widehat{\mathfrak{g}}$ and
$\widehat{\omega}$ instead of $\mathfrak{g}$ and $\omega$).
\end{proposition}

More generally:

\begin{proposition}
Let $\mathfrak{g}$ be a graded Lie algebra with a symmetric bilinear form
$\left(  \cdot,\cdot\right)  $ of degree $0$ invariant under the Lie bracket.
Let $\widehat{\mathfrak{g}}$ be the Lie algebra defined in Definition
\ref{def.loop}. Let $K$ denote the element $\left(  0,1\right)  $ of
$\mathfrak{g}\left[  t,t^{-1}\right]  \otimes\mathbb{C}=\widehat{\mathfrak{g}%
}$. Give $\widehat{\mathfrak{g}}$ a grading which makes $K$ homogeneous of
degree $0$ and the multiplications by $t$ and $t^{-1}$ into graded maps.

Let $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be an involutive automorphism
satisfying the conditions of Definition \ref{def.invol} (not to be confused
with the $2$-cocycle $\omega$ of Definition \ref{def.loop}). Then, the
$\mathbb{C}$-linear map $\widehat{\omega}:\widehat{\mathfrak{g}}%
\rightarrow\widehat{\mathfrak{g}}$ defined by $\widehat{\omega}\left(  a\cdot
t^{j}\right)  =\omega\left(  a\right)  t^{-j}$ for every $a\in\mathfrak{g}$
and $j\in\mathbb{N}$, and $\widehat{\omega}\left(  K\right)  =-K$, is an
involutive automorphism of the Lie algebra $\widehat{\mathfrak{g}}$. This
automorphism $\widehat{\omega}$ satisfies the conditions of Definition
\ref{def.invol} (for $\widehat{\mathfrak{g}}$ and $\widehat{\omega}$ instead
of $\mathfrak{g}$ and $\omega$).
\end{proposition}

\subsubsection{Unitary structures}

\begin{impnot}
\textbf{The parts of these notes concerned with unitary/Hermitian/real
structures are in an unfinished state and contain mistakes which I don't know
how to fix.}

For instance, if we define $\mathfrak{g}_{\mathbb{R}}$ by $\mathfrak{g}%
_{\mathbb{R}}=\left\{  a\in\mathfrak{g}\ \mid\ a^{\dag}=-a\right\}  $, and
define $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ by $\mathfrak{g}_{0\mathbb{R}%
}^{\ast}=\left\{  f\in\mathfrak{g}_{0}^{\ast}\ \mid\ f\left(  \mathfrak{g}%
_{0\mathbb{R}}\right)  \subseteq\mathbb{R}\right\}  $ (as I do below), and
define the antiinvolution $\dag:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ on $\operatorname*{Vir}$ by $L_{i}^{\dag}=L_{-i}$ for all
$i\in\mathbb{Z}$, and $C^{\dag}=C$, then $\operatorname*{Vir}%
\nolimits_{0\mathbb{R}}^{\ast}$ is \textbf{not} the set of all weights
$\left(  h,c\right)  $ satisfying $h,c\in\mathbb{R}$, but it is the set of all
weights $\left(  h,c\right)  $ satisfying $ih,ic\in\mathbb{R}$ (because the
definition of $\dag$ that we gave leads to $\operatorname*{Vir}%
\nolimits_{0\mathbb{R}}=\left\langle iC,iL_{0}\right\rangle _{\mathbb{R}}$).
This is not what we want later. Probably it is possible to fix these issues by
correcting some signs, but I do not know how. If you know a consistent way to
correct these definitions and results, please drop me a mail
(AB\texttt{@gmail.com} where A=\texttt{darij} and B=\texttt{grinberg}).
\end{impnot}

Over $\mathbb{C}$, it makes sense to study not only linear but also antilinear
maps. Sometimes, the latter actually enjoy even better properties of the
former (e. g., Hermitian forms are better behaved than complex-symmetric forms).

\begin{definition}
If $\mathfrak{g}$ and $\mathfrak{h}$ are two Lie algebras over a field $k$,
then a $k$-\textit{antihomomorphism} from $\mathfrak{g}$ to $\mathfrak{h}$
means a $k$-linear map $f:\mathfrak{g}\rightarrow\mathfrak{h}$ such that
$f\left(  \left[  x,y\right]  \right)  =-\left[  f\left(  x\right)  ,f\left(
y\right)  \right]  $ for all $x,y\in\mathfrak{g}$.
\end{definition}

\begin{definition}
In the following, an \textit{antiinvolution} of a complex Lie algebra
$\mathfrak{g}$ means an $\mathbb{R}$-antihomomorphism from $\mathfrak{g}$ to
$\mathfrak{g}$ which is simultaneously an involution.
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra. Let $\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ be an antilinear antiinvolution. This means that
$\dag$ is an $\mathbb{R}$-linear map and satisfies the relations%
\begin{align*}
\dag^{2}  &  =\operatorname*{id};\\
\left(  za\right)  ^{\dag}  &  =\overline{z}a^{\dag}%
\ \ \ \ \ \ \ \ \ \ \text{for all }z\in\mathbb{C}\text{ and }a\in
\mathfrak{g};\\
\left[  a,b\right]  ^{\dag}  &  =-\left[  a^{\dag},b^{\dag}\right]
\ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in\mathfrak{g}.
\end{align*}
(Here and in the following, we write $c^{\dag}$ for the image of an element
$c\in\mathfrak{g}$ under $\dag$.) Such a map $\dag$ is called a \textit{real
structure}, for the following reason: If $\dag$ is such a map, then we can
define a subspace $\mathfrak{g}_{\mathbb{R}}=\left\{  a\in\mathfrak{g}%
\ \mid\ a^{\dag}=-a\right\}  $ of $\mathfrak{g}$, and this $\mathfrak{g}%
_{\mathbb{R}}$ is a real Lie algebra such that $\mathfrak{g}\cong%
\mathfrak{g}_{\mathbb{R}}\otimes_{\mathbb{R}}\mathbb{C}$ as complex Lie
algebras. (It is said that $\mathfrak{g}_{\mathbb{R}}$ is a \textit{real form}
of $\mathfrak{g}$.)
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. If
$V$ is a $\mathfrak{g}$-module, we say that $V$ is \textit{Hermitian} if $V$
is equipped with a nondegenerate Hermitian form $\left(  \cdot,\cdot\right)  $
satisfying%
\[
\left(  av,w\right)  =\left(  v,a^{\dag}w\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }v\in V\text{ and
}w\in V.
\]
The $\mathfrak{g}$-module $V$ is said to be \textit{unitary} if this form is
positive definite.
\end{definition}

The real Lie algebra $\mathfrak{g}_{\mathbb{R}}$ acts on a Hermitian module by
skew-Hermitian operators.

\begin{remark}
While we will not be studying Lie groups in this course, here are some facts
about them that explain why unitary $\mathfrak{g}$-modules are called "unitary":

If $\mathfrak{g}$ is a finite-dimensional Lie algebra, and $V$ is a unitary
$\mathfrak{g}$-module, then the Hilbert space completion of $V$ is a unitary
representation of the Lie group $G_{\mathbb{R}}=\exp\left(  \mathfrak{g}%
_{\mathbb{R}}\right)  $ corresponding to $\mathfrak{g}_{\mathbb{R}}$ by Lie's
Third Theorem. (Note that this Hilbert space completion of $V$ is $V$ itself
if $\dim V<\infty$.) This even holds for some infinite-dimensional
$\mathfrak{g}$ under sufficiently restrictive conditions.
\end{remark}

\begin{Convention}
In the following, when we will talk about real structures on graded Lie
algebras, we will always consider the situation when $\mathfrak{g}$ is a
graded Lie algebra, and the map $\dag$ reverses the degree (i. e., every
$j\in\mathbb{Z}$ satisfies $\dag\left(  \mathfrak{g}_{j}\right)
\subseteq\mathfrak{g}_{-j}$). In particular, $\dag\left(  \mathfrak{g}%
_{0}\right)  \subseteq\mathfrak{g}_{0}$. We also assume that $\mathfrak{g}%
_{0}$ is an abelian Lie algebra (but we don't need to require $\mathfrak{g}$
to be nondegenerate).
\end{Convention}

So let us consider this situation. Two definitions:

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. Let
$V$ be a $\mathfrak{g}$-module. A Hermitian form $\left(  \cdot,\cdot\right)
$ on $V$ is said to be $\dag$\textit{-invariant} if and only if%
\[
\left(  av,w\right)  =\left(  v,a^{\dag}w\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }v\in V\text{ and
}w\in V.
\]

\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. For
every $f\in\mathfrak{g}_{0}^{\ast}$, we denote by $f^{\dag}$ the map
$\mathfrak{g}_{0}\rightarrow\mathbb{C},$ $x\mapsto\overline{f\left(  x^{\dag
}\right)  }$ (this map $f^{\dag}$ is easily seen to be $\mathbb{C}$-linear).
Let $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ be the subset $\left\{  f\in
\mathfrak{g}_{0}^{\ast}\ \mid\ f^{\dag}=-f\right\}  $ of $\mathfrak{g}%
_{0}^{\ast}$. Then, it is easily seen that%
\[
\mathfrak{g}_{0\mathbb{R}}^{\ast}=\left\{  f\in\mathfrak{g}_{0}^{\ast}%
\ \mid\ f\left(  \mathfrak{g}_{0\mathbb{R}}\right)  \subseteq\mathbb{R}%
\right\}  .
\]
Hence, we get an $\mathbb{R}$-bilinear form $\mathfrak{g}_{0\mathbb{R}}^{\ast
}\times\mathfrak{g}_{0\mathbb{R}}\rightarrow\mathbb{R},$ $\left(  f,a\right)
\mapsto f\left(  a\right)  $, which enables us to identify $\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ with the dual space of the $\mathbb{R}$-vector space
$\mathfrak{g}_{0\mathbb{R}}$. (More precisely, we have an isomorphism from
$\mathfrak{g}_{0\mathbb{R}}^{\ast}$ to the dual space of the $\mathbb{R}%
$-vector space $\mathfrak{g}_{0\mathbb{R}}$. This isomorphism sends every
$f\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$ to the map $f\mid_{\mathfrak{g}%
_{0\mathbb{R}}}$ (with target restricted to $\mathbb{R}$), and conversely, the
preimage of any $\mathbb{R}$-linear map $F:\mathfrak{g}_{0\mathbb{R}%
}\rightarrow\mathbb{R}$ is the $\mathbb{C}$-linear map $f\in\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ given by%
\[
f\left(  a\right)  =F\left(  \dfrac{a-a^{\dag}}{2}\right)  +iF\left(
\dfrac{a+a^{\dag}}{2i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathfrak{g}_{0}.
\]
)

The elements of $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ are said to be
\textit{real}.
\end{definition}

\begin{proposition}
\label{prop.M+l.unitary}If $\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$, then
the $\mathfrak{g}$-module $M_{\lambda}^{+}$ carries a $\dag$-invariant
Hermitian form satisfying $\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.M+l.unitary}.} In the following,
whenever $U$ is a $\mathbb{C}$-vector space, we will denote by $\overline{U}$
the $\mathbb{C}$-vector space which is identical to $U$ as a set, but with the
$\mathbb{C}$-vector structure twisted by complex conjugation.

The antilinear $\mathbb{R}$-Lie algebra homomorphism $-\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ can be viewed as a $\mathbb{C}$-Lie algebra
homomorphism $-\dag:\mathfrak{g}\rightarrow\overline{\mathfrak{g}}$, and thus
induces a $\mathbb{C}$-algebra homomorphism $U\left(  -\dag\right)  :U\left(
\mathfrak{g}\right)  \rightarrow U\left(  \overline{\mathfrak{g}}\right)  $.
Since $U\left(  \overline{\mathfrak{g}}\right)  \cong\overline{U\left(
\mathfrak{g}\right)  }$ canonically as $\mathbb{C}$-algebras (because taking
the universal enveloping algebra commutes with base change)\footnote{Warning:
This isomorphism $U\left(  \overline{\mathfrak{g}}\right)  \rightarrow
\overline{U\left(  \mathfrak{g}\right)  }$ sends $i\cdot1_{U\left(
\overline{\mathfrak{g}}\right)  }$ to $-i\cdot1_{U\left(  \mathfrak{g}\right)
}$.}, we can thus consider this $U\left(  -\dag\right)  $ as a $\mathbb{C}%
$-algebra homomorphism $U\left(  \mathfrak{g}\right)  \rightarrow
\overline{U\left(  \mathfrak{g}\right)  }$. This, in turn, can be viewed as an
antilinear $\mathbb{R}$-algebra homomorphism $U\left(  -\dag\right)  :U\left(
\mathfrak{g}\right)  \rightarrow U\left(  \mathfrak{g}\right)  $.

Let $\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$. Let $\left(  M_{-\lambda
}^{-}\right)  ^{-\dag}$ be the $\mathfrak{g}$-module $M_{-\lambda}^{-}$
twisted by the isomorphism $-\dag:\mathfrak{g}\rightarrow\mathfrak{g}$ of
$\mathbb{R}$-Lie algebras. Then, $\left(  M_{-\lambda}^{-}\right)  ^{-\dag}$
is a module over the $\mathbb{R}$-Lie algebra $\mathfrak{g}$, but not a module
over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$, since it satisfies $\left(
za\right)  \rightharpoonup v=\overline{z}\left(  a\rightharpoonup v\right)  $
(rather than $\left(  za\right)  \rightharpoonup v=z\left(  a\rightharpoonup
v\right)  $) for all $z\in\mathbb{C}$, $a\in\mathfrak{g}$ and $v\in
M_{-\lambda}^{-}$ (where $\rightharpoonup$ denotes the action of
$\mathfrak{g}$). However, this can be easily transformed into a $\mathbb{C}%
$-Lie algebra action: Namely, $\overline{\left(  M_{-\lambda}^{-}\right)
^{-\dag}}$ is a module over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$.

We have an isomorphism%
\begin{align*}
\overline{\left(  M_{-\lambda}^{-}\right)  ^{-\dag}}  &  \rightarrow
M_{\lambda}^{+},\\
x\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda
}^{-}  &  \mapsto U\left(  -\dag\right)  \left(  x\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}%
\end{align*}
of modules over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$%
.\ \ \ \ \footnote{Here are some details on the definition of this
isomorphism:
\par
As $\mathbb{R}$-vector spaces, $\overline{\left(  M_{-\lambda}^{-}\right)
^{-\dag}}=M_{-\lambda}^{-}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{-\lambda}$ and
$M_{\lambda}^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{\lambda}$. Hence, we
can define an $\mathbb{R}$-linear map $\overline{\left(  M_{-\lambda}%
^{-}\right)  ^{-\dag}}\rightarrow M_{\lambda}^{+}$ that sends $x\otimes
_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda}^{-}$ to
$U\left(  -\dag\right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}$ for every $x\in
U\left(  \mathfrak{g}\right)  $ and $z\in\mathbb{C}$ if we are able to show
that
\[
U\left(  -\dag\right)  \left(  xw\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}=U\left(
-\dag\right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  }\overline{wz}v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for all }x\in U\left(  \mathfrak{g}\right)  \text{,
}w\in U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \text{ and }%
z\in\mathbb{C}.
\]
But showing this is rather easy (left to the reader), and thus we get an
$\mathbb{R}$-linear map $\overline{\left(  M_{-\lambda}^{-}\right)  ^{-\dag}%
}\rightarrow M_{\lambda}^{+}$ that sends $x\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda}^{-}$ to $U\left(  -\dag\right)
\left(  x\right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
}\overline{z}v_{\lambda}^{+}$ for every $x\in U\left(  \mathfrak{g}\right)  $
and $z\in\mathbb{C}$. This map is easily seen to be $\mathfrak{g}$-linear and
$\mathbb{C}$-linear, so it is a homomorphism of modules over $\mathbb{C}$-Lie
algebra $\mathfrak{g}$. Showing that it is an isomorphism is easy as well (one
just has to construct its inverse).} Hence, $M_{-\lambda}^{-}\cong%
\overline{\left(  M_{\lambda}^{+}\right)  ^{-\dag}}$.

Hence, our bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ can be viewed as a bilinear form $M_{\lambda}^{+}%
\times\overline{M_{\lambda}^{+}}\rightarrow\mathbb{C}$, id est, as a
sesquilinear form $M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}%
$. This sesquilinear form is the unique sesquilinear Hermitian form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$\ \ \ \ \footnote{This can
be easily derived from Proposition \ref{prop.invform} \textbf{(a)}, which
claims that our form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ is the unique $\mathfrak{g}$-invariant
bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$
satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =1$.}. As a
consequence, this sesquilinear form can be easily seen to be Hermitian
symmetric, i. e., to satisfy%
\[
\left(  v,w\right)  =\overline{\left(  w,v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for all }v\in M_{\lambda}^{+}\text{ and }w\in
M_{\lambda}^{+}.
\]
\footnote{In fact, the form which sends $v\times w$ to $\overline{\left(
w,v\right)  }$ is also a sesquilinear Hermitian form $M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{\lambda}^{+}\right)  =1$, so that by uniqueness, it must be identical
with the form which sends $v\times w$ to $\left(  v,w\right)  $.}

However, this form can be degenerate. Its kernel is $J_{\lambda}^{+}$, so it
descends to a nondegenerate Hermitian form on $L_{\lambda}^{+}$. Thus, we get:

\begin{proposition}
\label{prop.hermitian.lambdareal}If $\lambda$ is real (this means that
$\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$), then $L_{\lambda}^{+}$ carries
a $\dag$-invariant nondegenerate Hermitian form. Different degrees in
$L_{\lambda}^{+}$ are orthogonal with respect to this form.
\end{proposition}

A reasonable (and, in most cases, difficult and interesting) question to ask
is the following: For which $\lambda$ is $L_{\lambda}^{+}$ unitary?

We are going to address this question in some cases and give hints in some
others, leaving many more unanswered.

First, let us give several examples of complex Lie algebras $\mathfrak{g}$
with antilinear antiinvolutions $\dag:\mathfrak{g}\rightarrow\mathfrak{g}$:

\begin{proposition}
We can define an antilinear map $\dag:\mathcal{A}\rightarrow\mathcal{A}$
by\ $a_{i}^{\dag}=a_{-i}$ for all $i\in\mathbb{Z}$. This map is an antilinear
antiinvolution of the Heisenberg algebra $\mathcal{A}$.
\end{proposition}

\begin{proposition}
One can define an antilinear map $\dag:\mathfrak{sl}_{2}\rightarrow
\mathfrak{sl}_{2}$ by$\ e^{\dag}=f,\ f^{\dag}=e,\ h^{\dag}=h$. This map is an
antilinear antiinvolution of the Lie algebra $\mathfrak{sl}_{2}$.
\end{proposition}

More generally:

\begin{proposition}
Let $\mathfrak{g}$ be a simple finite-dimensional Lie algebra. Using the
Chevalley generators $e_{1}$, $e_{2}$, $...$, $e_{m}$, $f_{1}$, $f_{2}$,
$...$, $f_{m}$, $h_{1}$, $h_{2}$, $...$, $h_{m}$ of Proposition
\ref{prop.grad.g}, we can define an antilinear map $\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ by $e_{i}^{\dag}=f_{i},$ $f_{i}^{\dag}=e_{i}%
,\ h_{i}^{\dag}=h_{i}$ for all $i\in\left\{  1,2,...,m\right\}  $. This map is
an antilinear antiinvolution of the Lie algebra $\mathfrak{g}$.
\end{proposition}

\begin{proposition}
We can define an antilinear map $\dag:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ by $L_{i}^{\dag}=L_{-i}$ for all $i\in\mathbb{Z}$, and
$C^{\dag}=C$. This map is an antilinear antiinvolution of the Virasoro algebra
$\operatorname*{Vir}$.
\end{proposition}

\begin{proposition}
If $\mathfrak{g}$ is a Lie algebra with an antilinear antiinvolution
$\dag:\mathfrak{g}\rightarrow\mathfrak{g}$ and with a symmetric bilinear form
$\left(  \cdot,\cdot\right)  $ of degree $0$ invariant under the Lie bracket,
then we can define an antilinear map $\dag:\widehat{\mathfrak{g}}%
\rightarrow\widehat{\mathfrak{g}}$ (where $\widehat{\mathfrak{g}}$ is the Lie
algebra defined in Definition \ref{def.loop}) by $\left(  at^{n}\right)
^{\dag}=a^{\dag}\cdot t^{-n}$ for every $a\in\mathfrak{g}$ and $t\in
\mathbb{Z}$, and by $K^{\dag}=K$ (where $K$ denotes the element $\left(
0,1\right)  $ of $\mathfrak{g}\left[  t,t^{-1}\right]  \otimes\mathbb{C}%
=\widehat{\mathfrak{g}}$). This map $\dag$ is an antilinear involution of the
Lie algebra $\widehat{\mathfrak{g}}$.
\end{proposition}

As for examples of Hermitian modules: The $\operatorname*{Vir}$-module
$L_{h,c}^{+}$ (see Example \ref{exa.Vir} for the definition of this module)
for $h,c\in\mathbb{R}$ has a $\dag$-invariant nondegenerate Hermitian form.
(This is because the requirement $h,c\in\mathbb{R}$ forces the form
$\lambda\in\mathfrak{g}_{0}^{\ast}$ which corresponds to the pair $\left(
h,c\right)  $ to lie in $\mathfrak{g}_{0\mathbb{R}}^{\ast}$, and thus we can
apply Proposition \ref{prop.hermitian.lambdareal}.)

But now, back to the general case:

\begin{proposition}
\label{prop.unitrick}Let $V$ be a unitary representation in Category
$\mathcal{O}^{+}$. Then, $V$ is completely reducible (i. e., the
representation $V$ is a direct sum of irreducible representations).
\end{proposition}

To prove this, we will use a lemma:

\begin{lemma}
\label{lem.unitrick}If $V$ is a highest-weight representation, and $V$ has a
nondegenerate $\dag$-invariant Hermitian form, then $V$ is irreducible. (We
recall that a "highest-weight representation" means a quotient of $M_{\lambda
}^{+}$ by a proper graded submodule for some $\lambda$.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.unitrick}.} Let $V$ be a highest-weight
representation having a nondegenerate $\dag$-invariant Hermitian form. Since
$V$ is a highest-weight representation, $V$ is a quotient of $M_{\lambda}^{+}$
by a proper graded submodule $P$ for some $\lambda$. The nondegenerate $\dag
$-invariant Hermitian form on $V$ thus induces a $\dag$-invariant Hermitian
form on $M_{\lambda}^{+}$ whose kernel is $P$. It is easy to see that
$\lambda$ is real. Thus, this $\dag$-invariant Hermitian form on $M_{\lambda
}^{+}$ can be rewritten as a $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$, which still has
kernel $P$. Such a form is unique up to scaling (by Proposition
\ref{prop.invform} \textbf{(c)}), and thus must be the form defined in
Proposition \ref{prop.invform} \textbf{(a)}. But the kernel of this form is
$J_{\lambda}^{+}$. Thus, the kernel of this form is, at the same time, $P$ and
$J_{\lambda}^{+}$. Hence, $P=J_{\lambda}^{+}$, so that $V=L_{\lambda}^{+}$
(since $V$ is the quotient of $M_{\lambda}^{+}$ by $P$), and thus $V$ is
irreducible. Lemma \ref{lem.unitrick} is proven.

\textit{Proof of Proposition \ref{prop.unitrick}.} Take a nonzero homogeneous
vector $v\in V$ of maximal degree. ("Maximal" means "maximal in real part".
Such a maximal degree exists by the definition of Category $\mathcal{O}^{+}$.)
Let $v$ be an eigenvector of $\mathfrak{g}_{0}$ with eigenvalue $\lambda$.
Consider the submodule of $V$ generated by $v$. This submodule is
highest-weight (since $\mathfrak{g}_{j}v=0$ for $j>0$). Hence, by Lemma
\ref{lem.unitrick}, this submodule is irreducible and therefore $\cong
L_{\lambda_{1}}^{+}$ for some $\lambda_{1}\in\mathfrak{h}^{\ast}$. Let $V_{1}$
be the orthogonal complement of $L_{\lambda_{1}}^{+}$. Then, $V=L_{\lambda
_{1}}^{+}\oplus V_{1}$. Now take a vector in $V_{1}$ etc. etf.. Since the
degrees of $V$ lie in finitely many arithmetic progressions, and homogeneous
subspaces have finite dimension, this process is exhaustive, so we obtain
$V=L_{\lambda_{1}}^{+}\oplus L_{\lambda_{2}}^{+}\oplus...$.

\begin{remark}
In this decomposition, every irreducible object of Category $\mathcal{O}^{+}$
occurs finitely many times.
\end{remark}

\section{Representation theory: concrete examples}

\subsection{\label{subsect.fockvir}Representations of $\operatorname*{Vir}$ on
$F_{\mu}$}

From Lemma \ref{lem.WtoDerA}, we know a Lie algebra action of $W$ on
$\mathcal{A}$. Thus, we get a semidirect product $W\ltimes\mathcal{A}$. On the
other hand, recall (from Definition \ref{def.fock}) that, for every $\mu
\in\mathbb{C}$, we have a representation $F_{\mu}$ of the Lie algebra
$\mathcal{A}$ on the Fock space $F$.

Can we extend this representation $F_{\mu}$ of $\mathcal{A}$ to a
representation of the semidirect product $W\ltimes\mathcal{A}$ ?

This question splits into two questions:

\textbf{Question 1:} Can we find linear operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ for all $n\in\mathbb{Z}$ such that $\left[  L_{n},a_{m}\right]
=-ma_{n+m}$ ? (Note that there are several abuses of notation in this
question. First, we denote the sought operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ by the same letters as the elements $L_{n}$ of $W$ because our
intuition for the $L_{n}$ is as if they would form a representation of $W$,
although we do not actually require them to form a representation of $W$ in
Question 1. Second, in the equation $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$,
we use $a_{m}$ and $a_{n+m}$ as abbreviations for $a_{m}\mid_{F_{\mu}}$ and
$a_{n+m}\mid_{F_{\mu}}$, respectively (so that this equation actually means
$\left[  L_{n},a_{m}\mid_{F_{\mu}}\right]  =-ma_{n+m}\mid_{F_{\mu}}$).)

\textbf{Question 2:} Do the operators $L_{n}:F_{\mu}\rightarrow F_{\mu}$ that
answer Question 1 also satisfy $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$? (In other words, do they really form a representation of
$W$ ?)

The answers to these questions are the following:

\textbf{1)} Yes, and moreover, these operators are unique up to adding a
constant (a new constant for each operator). (The uniqueness is rather easy to
prove: If we have two families $\left(  L_{n}^{\prime}\right)  _{n\in
\mathbb{Z}}$ and $\left(  L_{n}^{\prime\prime}\right)  _{n\in\mathbb{Z}}$ of
linear maps $F_{\mu}\rightarrow F_{\mu}$ satisfying $\left[  L_{n}^{\prime
},a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n}^{\prime\prime},a_{m}\right]
=-ma_{n+m}$, then every $L_{n}^{\prime}-L_{n}^{\prime\prime}$ commutes with
all $a_{m}$, and thus is constant by Dixmier's lemma.)

\textbf{2)} No, but almost. Our operators $L_{n}$ satisfy $\left[  L_{n}%
,L_{m}\right]  =\left(  n-m\right)  L_{n+m}$ whenever $n+m\neq0$, but the
$n+m=0$ case requires a correction term. This correction terms happens to be
the $2$-cocycle $\omega$ of Theorem \ref{thm.H^2(W)}. So the $\mathcal{A}%
$-module $F_{\mu}$ does not extend to a $W\ltimes\mathcal{A}$-module, but
extends to a $\operatorname*{Vir}\ltimes\mathcal{A}$-module.

Now we are going to prove the answers. First, we must define our operators
$L_{n}$. "Formally" (in the sense of "not caring about divergence of sums"),
one could define $L_{n}$ by
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z} \label{def.fockvir.wrong}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$), and this would "formally" work (in the sense that if the
sums were not divergent, one could manipulate them to prove $\left[
L_{n},a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$ for all $n$ and $m$). But the problem with this "formal"
approach is that the sum $\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}$ does not
make sense for $n=0$: it is an infinite sum, and infinitely many of its terms
yield nonzero values when applied to a given vector.\footnote{In fact, assume
that this sum would make sense for $n=0$. Thus we would have $L_{0}=\dfrac
{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{m}$. Applied to the vector $1\in
F_{0}$, this would give $L_{0}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}%
a_{-m}a_{m}1$. The terms for $m>0$ will get killed (since $a_{m}1=0$ for
$m>0$), but the terms for $m\leq0$ will survive. The sum would become
\begin{align*}
L_{0}1  &  =\dfrac{1}{2}\left(  a_{0}a_{-0}1+a_{1}a_{-1}1+a_{2}a_{-2}%
1+a_{3}a_{-3}1+...\right) \\
&  =\dfrac{1}{2}\left(  \mu^{2}1+1\dfrac{\partial}{\partial x_{1}}%
x_{1}+2\dfrac{\partial}{\partial x_{2}}x_{2}+3\dfrac{\partial}{\partial x_{3}%
}x_{3}+...\right)  =\dfrac{1}{2}\left(  \mu^{2}+1+2+3+...\right)  .
\end{align*}
Unless we interpret $1+2+3+...$ as $-\dfrac{1}{12}$ (which we are going to do
in some sense: the modified formulae further below include $-\dfrac{1}{12}$
factors), this makes no sense.} We thus must modify this form slightly.

In order to modify it, we define the so-called \textit{normal ordering}:

\begin{definition}
\label{def.fockvir.normal}For any two integers $m$ and $n$, define the
\textit{normal ordered product }$\left.  :a_{m}a_{n}:\right.  $ in the
universal enveloping algebra $U\left(  \mathcal{A}\right)  $ by\textit{ }%
\[
\left.  :a_{m}a_{n}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]


More generally, for any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$, define the
\textit{normal ordered product }$\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}%
}:\right.  $ in the universal enveloping algebra $U\left(  \mathcal{A}\right)
$ by%
\[
\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.  \ =\left(
\begin{array}
[c]{c}%
\text{the product of the elements }a_{n_{1}}\text{, }a_{n_{2}}\text{,
}...\text{, }a_{n_{k}}\text{ of }U\left(  \mathcal{A}\right)  \text{,}\\
\text{rearranged in such a way that the subscripts are in increasing order}%
\end{array}
\right)  .
\]
(More formally, this normal ordered product $\left.  :a_{n_{1}}a_{n_{2}%
}...a_{n_{k}}:\right.  $ is defined as the product $a_{m_{1}}a_{m_{2}%
}...a_{m_{k}}$, where $\left(  m_{1},m_{2},...,m_{k}\right)  $ is the
permutation of the list $\left(  n_{1},n_{2},...,n_{k}\right)  $ satisfying
$m_{1}\leq m_{2}\leq...\leq m_{k}$.)
\end{definition}

\begin{remark}
\label{rmk.fockvir.normal.mn}If $m$ and $n$ are integers such that $m\neq-n$,
then $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}$. (This is because $\left[
a_{m},a_{n}\right]  =0$ in $\mathcal{A}$ when $m\neq-n$.)
\end{remark}

Normal ordered products have the property of being commutative:

\begin{remark}
\label{rmk.fockvir.normal.comm}\textbf{(a)} Any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$ satisfy $\left.  :a_{m}a_{n}:\right.  =\left.  :a_{n}%
a_{m}:\right.  $.

\textbf{(b)} Any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$ and any permutation
$\pi\in S_{k}$ satisfy $\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.
=\left.  :a_{n_{\pi\left(  1\right)  }}a_{n_{\pi\left(  2\right)  }%
}...a_{n_{\pi\left(  k\right)  }}:\right.  $.
\end{remark}

The proof of this is trivial.

By Remark \ref{rmk.fockvir.normal.mn} (and by the rather straightforward
generalization of this fact to many integers), normal ordered products are
rarely different from the usual products. But even when they are different,
they don't differ much:

\begin{remark}
\label{rmk.fockvir.normal.K}Let $m$ and $n$ be integers.

\textbf{(a)} Then, $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K$. Here, when $\mathfrak{A}$ is an assertion, we
denote by $\left[  \mathfrak{A}\right]  $ the truth value of $\mathfrak{A}$
(that is, the number $\left\{
\begin{array}
[c]{c}%
1\text{, if }\mathfrak{A}\text{ is true;}\\
0\text{, if }\mathfrak{A}\text{ is false }%
\end{array}
\right.  $).

\textbf{(b)} For any $x\in U\left(  \mathcal{A}\right)  $, we have $\left[
x,\left.  :a_{m}a_{n}:\right.  \right]  =\left[  x,a_{m}a_{n}\right]  $ (where
$\left[  \cdot,\cdot\right]  $ denotes the commutator in $U\left(
\mathcal{A}\right)  $).
\end{remark}

Note that when we denote by $\left[  \cdot,\cdot\right]  $ the commutator in
$U\left(  \mathcal{A}\right)  $, we are seemingly risking a confusion with the
notation $\left[  \cdot,\cdot\right]  $ for the Lie bracket of $\mathcal{A}$
(because we embed $\mathcal{A}$ in $U\left(  \mathcal{A}\right)  $). However,
this confusion is harmless, because the very definition of $U\left(
\mathcal{A}\right)  $ ensures that the commutator of two elements of
$\mathcal{A}$, taken in $U\left(  \mathcal{A}\right)  $, equals to their Lie
bracket in $\mathcal{A}$.

\textit{Proof of Remark \ref{rmk.fockvir.normal.K}.} \textbf{(a)} We
distinguish between three cases:

\textit{Case 1:} We have $m\neq-n$.

\textit{Case 2:} We have $m=-n$ and $m>0$.

\textit{Case 3:} We have $m=-n$ and $m\leq0$.

In Case 1, we have $m\neq-n$, so that $\delta_{m,-n}=0$ and thus%
\[
a_{m}a_{n}+n\left[  m>0\right]  \underbrace{\delta_{m,-n}}_{=0}K=a_{m}%
a_{n}=\left.  :a_{m}a_{n}:\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark
\ref{rmk.fockvir.normal.mn})}\right)  .
\]
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 1.

In Case 2, we have $m=-n$ and $m>0$, so that $m>n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{n}a_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m>n\right) \\
&  =a_{m}a_{n}+\underbrace{\left[  a_{n},a_{m}\right]  }_{=n\delta
_{n,-m}K=n1\delta_{m,-n}K}=a_{m}a_{n}+n\underbrace{1}_{\substack{=\left[
m>0\right]  \\\text{(since }m>0\text{)}}}\delta_{m,-n}K=a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K.
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 2.

In Case 3, we have $m=-n$ and $m\leq0$, so that $m\leq n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{m}a_{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\leq n\right) \\
&  =a_{m}a_{n}+n\left[  m>0\right]  \delta_{m,-n}K\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{we have just added a zero term}\\
\text{(since }m\leq0\text{, so that }\left[  m>0\right]  =0\text{ and thus}\\
n\left[  m>0\right]  \delta_{m,-n}K=0\text{)}%
\end{array}
\right)  .
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 3.

Thus, we have proven Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} in all
three possible cases. This completes the proof of Remark
\ref{rmk.fockvir.normal.K} \textbf{(a)}.

\textbf{(b)} We have $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(
U\left(  \mathcal{A}\right)  \right)  $ (since the center of a Lie algebra is
contained in the center of its universal enveloping algebra). Hence, $\left[
x,K\right]  =0$ for any $x\in U\left(  \mathcal{A}\right)  $. Thus,%
\begin{align*}
\left[  x,\left.  :a_{m}a_{n}:\right.  \right]   &  =\left[  x,a_{m}%
a_{n}+n\left[  m>0\right]  \delta_{m,-n}K\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[  m>0\right]
\delta_{m,-n}K\right) \\
&  =\left[  x,a_{m}a_{n}\right]  +n\left[  m>0\right]  \delta_{m,-n}%
\underbrace{\left[  x,K\right]  }_{=0}=\left[  x,a_{m}a_{n}\right]  .
\end{align*}
This proves Remark \ref{rmk.fockvir.normal.K} \textbf{(b)}.

Now, the true definition of our maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ will
be the following:

\begin{definition}
\label{def.fockvir}For every $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$, define a
linear map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ by%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
\]
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$). This sum $\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  $ is an infinite sum, but it is well-defined in the
following sense: For any vector $v\in F_{\mu}$, applying $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ to the vector $v$ gives the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  v$, which has
only finitely many nonzero addends (because of Lemma \ref{lem.fockvir.welldef} below).
\end{definition}

Note that we have not defined the meaning of the sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ in the universal enveloping
algebra $U\left(  \mathcal{A}\right)  $ itself, but only its meaning as an
endomorphism of $F_{\mu}$. However, if we wanted, we could also define the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ as an element
of a suitable completion of the universal enveloping algebra $U\left(
\mathcal{A}\right)  $ (although not in $U\left(  \mathcal{A}\right)  $
itself). We don't really have a reason to do so here, however.

\begin{Convention}
\label{conv.fockvir.L}During the rest of Section \ref{subsect.fockvir}, we are
going to use the labels $L_{n}$ for the maps $L_{n}:F_{\mu}\rightarrow F_{\mu
}$ introduced in Definition \ref{def.fockvir}, and \textbf{not} for the
eponymous elements of the Virasoro algebra $\operatorname*{Vir}$ or of the
Witt algebra $W$, unless we explicitly refer to "the element $L_{n}$ of
$\operatorname*{Vir}$" or "the element $L_{n}$ of $W$" or something similarly unambiguous.

(While it is correct that the maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ satisfy
the same relations as the eponymous elements $L_{n}$ of $\operatorname*{Vir}$
(but not the eponymous elements $L_{n}$ of $W$), this is a nontrivial fact
that needs to be proven, and until it is proven we must avoid any confusion
between these different meanings of $L_{n}$.)
\end{Convention}

Let us first show that Definition \ref{def.fockvir} makes sense:

\begin{lemma}
\label{lem.fockvir.welldef}Let $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$. Let
$v\in F_{\mu}$. Then, the sum $\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  v$ has only finitely many nonzero addends. More precisely:

\textbf{(a)} If $m\in\mathbb{Z}$ is sufficiently high, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(b)} If $m\in\mathbb{Z}$ is sufficiently low, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.fockvir.welldef}.} \textbf{(a)} Since $v\in
F_{\mu}\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $, the vector $v$ is
a polynomial in infinitely many variables. Since every polynomial contains
only finitely many variables, there exists an integer $N\in\mathbb{N}$ such
that no variable $x_{r}$ with $r>N$ occurs in $v$. Consider this $N$. Then,
\begin{equation}
\dfrac{\partial}{\partial x_{r}}v=0\ \ \ \ \ \ \ \ \ \ \text{for every integer
}r>N. \label{pf.fockvir.welldef.1}%
\end{equation}


Now, let $m\geq\max\left\{  0,-n\right\}  +N+1$. Then, $m\geq N+1$ and
$m\geq-n+N+1$.

Adding $m\geq N+1$ and $m\geq-n+N+1$, we get $2m\geq\left(  N+1\right)
+\left(  -n+N+1\right)  =-n+\underbrace{\left(  2N+1\right)  }_{\geq0}\geq-n$,
so that $-m\leq n+m$.

From $m\geq-n+N+1$, we get $n+m\geq N+1$, so that $n+m>0$. Hence, $a_{n+m}%
\mid_{F_{\mu}}=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}$, so
that $a_{n+m}v=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}v$. Since
$\dfrac{\partial}{\partial x_{n+m}}v=0$ (by (\ref{pf.fockvir.welldef.1}),
applied to $r=n+m$ (since $n+m\geq N+1>N$)), we thus have $a_{n+m}v=0$.

By Definition \ref{def.fockvir.normal}, we have
\[
\left.  :a_{-m}a_{n+m}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{-m}a_{n+m},\ \ \ \ \ \ \ \ \ \ \text{if }-m\leq n+m;\\
a_{n+m}a_{-m},\ \ \ \ \ \ \ \ \ \ \text{if }-m>n+m
\end{array}
\right.  .
\]
Since $-m\leq n+m$, this rewrites as $\left.  :a_{-m}a_{n+m}:\right.
=a_{-m}a_{n+m}$. Thus, $\left.  :a_{-m}a_{n+m}:\right.  v=a_{-m}%
\underbrace{a_{n+m}v}_{=0}=0$, and Lemma \ref{lem.fockvir.welldef}
\textbf{(a)} is proven.

\textbf{(b)} Applying Lemma \ref{lem.fockvir.welldef} \textbf{(a)} to $-n-m$
instead of $m$, we see that, if $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.  v=0$.
Since%
\[
\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.
=\left.  :a_{n+m}a_{-m}:\right.  =\left.  :a_{-m}a_{n+m}:\right.
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.fockvir.normal.comm}
\textbf{(a)}}\right)  ,
\]
this rewrites as follows: If $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-m}a_{n+m}:\right.  v=0$. This proves Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\begin{remark}
\label{rmk.fockvir.explicit}\textbf{(a)} If $n\neq0$, then the operator
$L_{n}$ defined in Definition \ref{def.fockvir} can be rewritten as%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}.
\]
In other words, for $n\neq0$, our old definition (\ref{def.fockvir.wrong}) of
$L_{n}$ makes sense and is equivalent to the new definition (Definition
\ref{def.fockvir}).

\textbf{(b)} But when $n=0$, the formula (\ref{def.fockvir.wrong}) is devoid
of sense, whereas Definition \ref{def.fockvir} is legit. However, we can
rewrite the definition of $L_{0}$ without using normal ordered products:
Namely, we have%
\[
L_{0}=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}}{2}=\sum\limits_{m>0}%
a_{-m}a_{m}+\dfrac{\mu^{2}}{2}.
\]


\textbf{(c)} Let us grade the space $F_{\mu}$ by the grading which gives every
variable $x_{i}$ the degree $i$ and makes $F_{\mu}$ into a graded $\mathbb{C}%
$-algebra (recall that $F_{\mu}=\mathbb{C}\left[  x_{1},x_{2},x_{3}%
,...\right]  $). (This is the way how we always have been grading the space
$F$; but this is \textbf{not} the modified grading that we gave to the space
$F_{\mu}$ in Convention \ref{conv.fockgrad}.) Let $d\in\mathbb{N}$. Then,
every homogeneous polynomial $f\in F_{\mu}$ of degree $d$ satisfies
$L_{0}f=\left(  d+\dfrac{\mu^{2}}{2}\right)  f$.

\textbf{(d)} Consider the grading on $F_{\mu}$ defined in part \textbf{(c)}.
For every $n\in\mathbb{Z}$, the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ is
homogeneous of degree $n$.
\end{remark}

\textit{Proof of Remark \ref{rmk.fockvir.explicit}.} \textbf{(a)} Let $n\neq
0$. Then, every $m\in\mathbb{Z}$ satisfies $-m\neq-\left(  n+m\right)  $ and
thus $\left.  :a_{-m}a_{n+m}:\right.  =a_{-m}a_{n+m}$ (by Remark
\ref{rmk.fockvir.normal.mn}, applied to $-m$ and $n+m$ instead of $m$ and
$n$)). Hence, the formula $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  $ (which is how we defined $L_{n}$) rewrites
as $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}$. This proves
Remark \ref{rmk.fockvir.explicit} \textbf{(a)}.

\textbf{(b)} By the definition of $L_{0}$ (in Definition \ref{def.fockvir}),
we have%
\begin{align*}
L_{0}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{0+m}:\right.  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{m}:\right. \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m<0}\underbrace{\left.  :a_{-m}%
a_{m}:\right.  }_{\substack{=a_{m}a_{-m}\\\text{(by the definition of }\left.
:a_{-m}a_{m}:\right.  \\\text{(since }m<0\text{ and thus }m\leq-m\text{))}%
}}+\underbrace{\left.  :a_{-0}a_{0}:\right.  }_{\substack{=\left.  :a_{0}%
a_{0}:\right.  =a_{0}a_{0}\\\text{(by the definition of }\left.  :a_{0}%
a_{0}:\right.  \\\text{(since }0\leq0\text{))}}}+\sum\limits_{m>0}%
\underbrace{\left.  :a_{-m}a_{m}:\right.  }_{\substack{=a_{-m}a_{m}\\\text{(by
the definition of }\left.  :a_{-m}a_{m}:\right.  \\\text{(since }m>0\text{ and
thus }m>-m\text{))}}}\right) \\
&  =\dfrac{1}{2}\left(  \underbrace{\sum\limits_{m<0}a_{m}a_{-m}%
}_{\substack{=\sum\limits_{m>0}a_{-m}a_{m}\\\text{(here, we substituted
}m\text{ for }-m\text{ in the sum)}}}+\underbrace{a_{0}a_{0}}_{=a_{0}^{2}%
}+\sum\limits_{m>0}a_{-m}a_{m}\right)  =\dfrac{1}{2}\left(  \sum
\limits_{m>0}a_{-m}a_{m}+a_{0}^{2}+\sum\limits_{m>0}a_{-m}a_{m}\right) \\
&  =\dfrac{1}{2}\left(  2\sum\limits_{m>0}a_{-m}a_{m}+a_{0}^{2}\right)
=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}}{2}=\sum\limits_{m>0}%
a_{-m}a_{m}+\dfrac{\mu^{2}}{2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{0}\text{ acts as multiplication
with }\mu\text{ on }F_{\mu}\right)
\end{align*}
on $F_{\mu}$. This proves Remark \ref{rmk.fockvir.explicit} \textbf{(b)}.

\textbf{(c)} We must prove the equation $L_{0}f=\left(  d+\dfrac{\mu^{2}}%
{2}\right)  f$ for every homogeneous polynomial $f\in F_{\mu}$ of degree $d$.
Since this equation is linear in $f$, it is clearly enough to prove this for
the case of $f$ being a monomial\footnote{Here, "monomial" means "monomial
without coefficient".} of degree $d$. So let $f$ be a monomial of degree $d$.
Then, $f$ can be written in the form $f=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}%
}x_{3}^{\alpha_{3}}...$ for a sequence $\left(  \alpha_{1},\alpha_{2}%
,\alpha_{3},...\right)  $ of nonnegative integers such that $\sum
\limits_{m>0}m\alpha_{m}=d$ and such that all but finitely many $i\in\left\{
1,2,3,...\right\}  $ satisfy $\alpha_{i}=0$. Consider this sequence. By Remark
\ref{rmk.fockvir.explicit} \textbf{(b)}, we have $L_{0}=\sum\limits_{m>0}%
a_{-m}a_{m}+\dfrac{\mu^{2}}{2}$. Since $a_{m}=m\dfrac{\partial}{\partial
x_{m}}$ and $a_{-m}=x_{m}$ for every integer $m>0$ (by the definition of the
action of $a_{m}$ on $F_{\mu}$), this rewrites as $L_{0}=\sum\limits_{m>0}%
x_{m}m\dfrac{\partial}{\partial x_{m}}+\dfrac{\mu^{2}}{2}$. Now, since
$f=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}^{\alpha_{3}}...$, every $m>0$
satisfies%
\begin{align*}
x_{m}m\dfrac{\partial}{\partial x_{m}}f  &  =x_{m}m\underbrace{\dfrac
{\partial}{\partial x_{m}}\left(  x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
x_{3}^{\alpha_{3}}...\right)  }_{\substack{=\alpha_{m}x_{1}^{\alpha_{1}}%
x_{2}^{\alpha_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}%
^{\alpha_{m+1}}x_{m+2}^{\alpha_{m+2}}...\\\text{(this term should be
understood as }0\text{ if }\alpha_{m}=0\text{)}}}\\
&  =x_{m}m\alpha_{m}x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}...x_{m-1}%
^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}}x_{m+2}^{\alpha
_{m+2}}...\\
&  =m\alpha_{m}\cdot\underbrace{x_{m}\cdot x_{1}^{\alpha_{1}}x_{2}^{\alpha
_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}%
}x_{m+2}^{\alpha_{m+2}}...}_{=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}}x_{m+1}^{\alpha_{m+1}}%
x_{m+2}^{\alpha_{m+2}}...=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}%
^{\alpha_{3}}...=f}\\
&  =m\alpha_{m}f.
\end{align*}
Hence,
\begin{align*}
L_{0}f  &  =\sum\limits_{m>0}\underbrace{x_{m}m\dfrac{\partial}{\partial
x_{m}}f}_{=m\alpha_{m}f}+\dfrac{\mu^{2}}{2}f\ \ \ \ \ \ \ \ \ \ \left(
\text{since }L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}%
}+\dfrac{\mu^{2}}{2}\right) \\
&  =\underbrace{\sum\limits_{m>0}m\alpha_{m}}_{=d}f+\dfrac{\mu^{2}}%
{2}f=df+\dfrac{\mu^{2}}{2}f=\left(  d+\dfrac{\mu^{2}}{2}\right)  f.
\end{align*}
We thus have proven the equation $L_{0}f=\left(  d+\dfrac{\mu^{2}}{2}\right)
f$ for every monomial $f$ of degree $d$. As we said above, this completes the
proof of Remark \ref{rmk.fockvir.explicit} \textbf{(c)}.

\textbf{(d)} For every $m\in\mathbb{Z}$,%
\begin{equation}
\text{the map }a_{m}:F_{\mu}\rightarrow F_{\mu}\text{ is homogeneous of degree
}m\text{.} \label{pf.fockvir.explicit.5}%
\end{equation}
(In fact, this is easily seen from the definition of how $a_{m}$ acts on
$F_{\mu}$.)

Thus, for every $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$, the map $\left.
:a_{u}a_{v}:\right.  $ is homogeneous of degree $u+v$%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$.
By (\ref{pf.fockvir.explicit.5}) (applied to $m=u$), the map $a_{u}$ is
homogeneous of degree $u$. Similarly, the map $a_{v}$ is homogeneous of degree
$v$. Thus, the map $a_{u}a_{v}$ is homogeneous of degree $u+v$. Similarly, the
map $a_{v}a_{u}$ is homogeneous of degree $v+u=u+v$.
\par
Since $\left.  :a_{u}a_{v}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{u}a_{v},\ \ \ \ \ \ \ \ \ \ \text{if }u\leq v;\\
a_{v}a_{u},\ \ \ \ \ \ \ \ \ \ \text{if }u>v
\end{array}
\right.  $ (by the definition of normal ordered products), the map $\left.
:a_{u}a_{v}:\right.  $ equals one of the maps $a_{u}a_{v}$ and $a_{v}a_{u}$.
Since both of these maps $a_{u}a_{v}$ and $a_{v}a_{u}$ are homogeneous of
degree $u+v$, this yields that $\left.  :a_{u}a_{v}:\right.  $ is homogeneous
of degree $u+v$, qed.}. Applied to $u=-m$ and $v=n+m$, this yields: For every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the map $\left.  :a_{-m}a_{n+m}%
:\right.  $ is homogeneous of degree $\left(  -m\right)  +\left(  n+m\right)
=n$. Now, the map%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\underbrace{\left.
:a_{-m}a_{n+m}:\right.  }_{\text{this map is homogeneous of degree }n}%
\]
must be homogeneous of degree $n$. This proves Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}.

Now it turns out that the operators $L_{n}$ that we have defined give a
positive answer to question \textbf{1)}:

\begin{proposition}
\label{prop.fockvir.answer1}Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and $\mu
\in\mathbb{C}$. Then, $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$ (where $L_{n}$
is defined as in Definition \ref{def.fockvir}, and $a_{\ell}$ is shorthand
notation for $a_{\ell}\mid_{F_{\mu}}$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.fockvir.answer1}.} Since%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
=\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.
\]
we have%
\begin{align}
\left[  L_{n},a_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{j\in
\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.  ,a_{m}\right]  =\dfrac{1}{2}%
\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-j}a_{n+j}%
:\right.  ,a_{m}\right]  }_{=-\left[  a_{m},\left.  :a_{-j}a_{n+j}:\right.
\right]  }=-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[
a_{m},\left.  :a_{-j}a_{n+j}:\right.  \right]  }_{\substack{=\left[
a_{m},a_{-j}a_{n+j}\right]  \\\text{(by Remark \ref{rmk.fockvir.normal.K},
applied}\\\text{to }-j\text{ and }n+j\text{ instead of }m\text{ and }%
n\text{)}}}\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  a_{m}%
,a_{-j}a_{n+j}\right]  }_{=\left[  a_{m},a_{-j}\right]  a_{n+j}+a_{-j}\left[
a_{m},a_{n+j}\right]  }=-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(
\underbrace{\left[  a_{m},a_{-j}\right]  }_{=m\delta_{m,-\left(  -j\right)
}K}a_{n+j}+a_{-j}\underbrace{\left[  a_{m},a_{n+j}\right]  }_{=m\delta
_{m,-\left(  n+j\right)  }K}\right) \nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\underbrace{\delta
_{m,-\left(  -j\right)  }}_{=\delta_{m,j}}Ka_{n+j}+a_{-j}m\underbrace{\delta
_{m,-\left(  n+j\right)  }}_{=\delta_{-m,n+j}=\delta_{-m-n,j}}K\right)
\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\delta_{m,j}%
Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  . \label{pf.fockvir.answer1.2}%
\end{align}


But each of the two sums $\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}$
and $\sum\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K$ is
convergent\footnote{In fact, due to the factors $\delta_{m,j}$ and
$\delta_{-m-n,j}$ in the addends, it is clear that in each of these two sums,
only at most one addend can be nonzero. Concretely:%
\[
\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}=mKa_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K=a_{-\left(  -m-n\right)  }mK.
\]
}. Hence, we can split the sum $\sum\limits_{j\in\mathbb{Z}}\left(
m\delta_{m,j}Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  $ into $\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}+\sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K$. Thus, (\ref{pf.fockvir.answer1.2}) becomes%
\begin{align*}
\left[  L_{n},a_{m}\right]   &  =-\dfrac{1}{2}\left(  \underbrace{\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}}_{=mKa_{n+m}}+\underbrace{\sum
\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K}_{=a_{-\left(  -m-n\right)
}mK}\right)  =-\dfrac{1}{2}\left(  mKa_{n+m}+a_{-\left(  -m-n\right)
}mK\right) \\
&  =-\dfrac{1}{2}\left(  ma_{n+m}+a_{-\left(  -m-n\right)  }m\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }\operatorname*{id}%
\text{ on }F_{\mu}\right) \\
&  =-\dfrac{1}{2}m\left(  a_{n+m}+\underbrace{a_{-\left(  -m-n\right)  }%
}_{=a_{m+n}}\right)  =-\dfrac{1}{2}m\left(  a_{n+m}+a_{n+m}\right)  =ma_{n+m}.
\end{align*}
This proves Proposition \ref{prop.fockvir.answer1}.

Now let us check whether our operators $L_{n}$ answer Question \textbf{2)}, or
at least try to do so. We are going to make some "dirty" arguments; cleaner
ones can be found in the proof of Proposition \ref{prop.fockvir.answer2} that
we give below.

First, it is easy to see that any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
satisfy%
\[
\left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m},a_{k}\right]
=0\ \ \ \ \ \ \ \ \ \ \text{for any }k\in\mathbb{Z}%
\]
(this follows using the Jacobi identity for commutators and Proposition
\ref{prop.fockvir.answer1}). Hence, for any $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, the endomorphism $\left[  L_{n},L_{m}\right]  -\left(
n-m\right)  L_{n+m}$ of $F_{\mu}$ is an $\mathcal{A}$-module homomorphism
(since $\left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)
L_{n+m},K\right]  =0$ also holds, for obvious reasons). Since $F_{\mu}$ is an
irreducible $\mathcal{A}$-module of countable dimension, this yields (by Lemma
\ref{lem.dix}) that, for any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the map
$\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}:F_{\mu}\rightarrow
F_{\mu}$ is a scalar multiple of the identity. But since this map $\left[
L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ must also be homogeneous of
degree $n+m$ (by an application of Remark \ref{rmk.fockvir.explicit}
\textbf{(d)}), this yields that $\left[  L_{n},L_{m}\right]  -\left(
n-m\right)  L_{n+m}=0$ whenever $n+m\neq0$ (because any homogeneous map of
degree $\neq0$ which is, at the same time, a scalar multiple of the identity,
must be the $0$ map). Thus, for every $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$,
we can write%
\begin{equation}
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\gamma_{n}%
\delta_{n,-m}\ \ \ \ \ \ \ \ \ \ \text{for some }\gamma_{n}\in\mathbb{C}\text{
depending on }n\text{.} \label{pf.fockvir.answer2.1}%
\end{equation}
We can get some more information about these $\gamma_{n}$ if we consider the
Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}\cup\left(
\operatorname*{id}\right)  $. (Note that, according to Convention
\ref{conv.fockvir.L}, these $L_{n}$ still denote maps from $F_{\mu}$ to
$F_{\mu}$, rather than elements of $\operatorname*{Vir}$ or $W$. Of course,
this Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}%
\cup\left(  \operatorname*{id}\right)  $ \textbf{will} turn out to be
isomorphic to $\operatorname*{Vir}$, but we have not yet proven this.) This
Lie algebra, due to the formula (\ref{pf.fockvir.answer2.1}) and to the fact
that $\operatorname*{id}$ commutes with everything, must be a $1$-dimensional
central extension of the Witt algebra. Hence, the map
\[
W\times W\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \left(  L_{n},L_{m}\right)
\mapsto\gamma_{n}\delta_{n,-m}%
\]
(where $L_{n}$ and $L_{m}$ really mean the elements $L_{n}$ and $L_{m}$ of $W$
this time) must be a $2$-cocycle on $W$. But since we know (from Theorem
\ref{thm.H^2(W)}) that every $2$-cocycle on $W$ is a scalar multiple of the
$2$-cocycle $\omega$ defined in Theorem \ref{thm.H^2(W)}, this yields that
this $2$-cocycle is a scalar multiple of $\omega$ modulo the $2$-coboundaries.
In other words, there exist $c\in\mathbb{C}$ and $\xi\in W^{\ast}$ such that%
\[
\gamma_{n}\delta_{n,-m}=c\omega\left(  L_{n},L_{m}\right)  +\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Since $\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}$,
this rewrites as%
\[
\gamma_{n}\delta_{n,-m}=c\dfrac{n^{3}-n}{6}\delta_{n,-m}+\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Applied to $m=-n$, this yields%
\begin{equation}
\gamma_{n}=c\dfrac{n^{3}-n}{6}+\xi\left(  \underbrace{\left[  L_{n}%
,L_{-n}\right]  }_{=2nL_{0}}\right)  =c\dfrac{n^{3}-n}{6}+2n\xi\left(
L_{0}\right)  . \label{pf.fockvir.answer2.2}%
\end{equation}


All that remains now, in order to get the values of $\left[  L_{n}%
,L_{m}\right]  -\left(  n-m\right)  L_{n+m}$, is to compute the scalars $c$
and $\xi\left(  L_{0}\right)  $. For this, we only need to compute $\gamma
_{1}$ and $\gamma_{2}$ (because this will give $2$ linear equations for $c$
and $L_{0}$). In order to do this, we will evaluate the endomorphisms $\left[
L_{1},L_{-1}\right]  -2L_{0}$ and $\left[  L_{2},L_{-2}\right]  -4L_{0}$ at
the element $1$ of $F_{\mu}$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(c)} (applied to $d=0$ and
$f=1$), we get $L_{0}1=\left(  0+\dfrac{\mu^{2}}{2}\right)  1=\dfrac{\mu^{2}%
}{2}$.

Since $L_{1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{1+m}:\right.  $, we have $L_{1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{1+m}:\right.  1=0$ (because, as it is easily seen, $\left.
:a_{-m}a_{1+m}:\right.  1=0$ for every $m\in\mathbb{Z}$). Similarly,
$L_{2}1=0$.

Since $L_{-1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{-1+m}:\right.  $, we have $L_{-1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{-1+m}:\right.  1$. It is easy to see that the only
$m\in\mathbb{Z}$ for which $\left.  :a_{-m}a_{-1+m}:\right.  1$ is nonzero are
$m=0$ and $m=1$. Hence,
\[
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-1+m}:\right.
1=\underbrace{\left.  :a_{-0}a_{-1+0}:\right.  1}_{=\left.  :a_{0}%
a_{-1}:\right.  1=a_{-1}a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}+\underbrace{\left.
:a_{-1}a_{-1+1}:\right.  1}_{=\left.  :a_{-1}a_{0}:\right.  1=a_{-1}%
a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}=\mu x_{1}+\mu x_{1}=2\mu x_{1},
\]
so that $L_{-1}1=\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-1+m}:\right.  1}_{=2\mu x_{1}}=\mu x_{1}$. Thus,%
\begin{align*}
L_{1}L_{-1}1  &  =L_{1}\mu x_{1}=\mu\underbrace{L_{1}}_{=\dfrac{1}{2}%
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{1+m}:\right.  }x_{1}=\mu
\cdot\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{1+m}:\right.  x_{1}}_{\substack{=\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}+\left.  :a_{-0}a_{1+0}:\right.
x_{1}\\\text{(in fact, it is easy to see that the only}\\m\in\mathbb{Z}\text{
for which }\left.  :a_{-m}a_{1+m}:\right.  x_{1}\neq0\text{ are }m=-1\text{
and }m=0\text{)}}}\\
&  =\mu\cdot\dfrac{1}{2}\left(  \underbrace{\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}}_{=\left.  :a_{1}a_{0}:\right.
x_{1}=a_{0}a_{1}x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}+\underbrace{\left.  :a_{-0}a_{1+0}:\right.  x_{1}}_{=\left.  :a_{0}%
a_{1}:\right.  x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}\right) \\
&  =\mu\cdot\dfrac{1}{2}\left(  \mu+\mu\right)  =\mu^{2}.
\end{align*}


A similar (but messier) computation works for $L_{2}L_{-2}1$: Since
$L_{-2}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-2+m}%
:\right.  $, we have $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-2+m}:\right.  1$. It is easy to see that the only $m\in\mathbb{Z}$
for which $\left.  :a_{-m}a_{-2+m}:\right.  1$ is nonzero are $m=0$, $m=1$ and
$m=2$. This allows us to simplify $L_{-2}=\dfrac{1}{2}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{-2+m}:\right.  $ to $L_{-2}=\mu x_{2}+\dfrac
{1}{2}x_{1}^{2}$ (the details are left to the reader). Thus,%
\[
L_{2}L_{-2}1=L_{2}\left(  \mu x_{2}+\dfrac{1}{2}x_{1}^{2}\right)  =\mu
L_{2}x_{2}+\dfrac{1}{2}L_{2}x_{1}^{2}.
\]
Straightforward computations, which I omit, show that $L_{2}x_{2}=2\mu$ and
$L_{2}x_{1}^{2}=1$. Hence,%
\[
L_{2}L_{-2}1=\mu\underbrace{L_{2}x_{2}}_{=2\mu}+\dfrac{1}{2}\underbrace{L_{2}%
x_{1}^{2}}_{=1}=2\mu^{2}+\dfrac{1}{2}.
\]


Now,%
\[
\left(  \left[  L_{1},L_{-1}\right]  -2L_{0}\right)  1=\underbrace{L_{1}%
L_{-1}1}_{=\mu^{2}}-L_{-1}\underbrace{L_{1}1}_{=0}-2\underbrace{L_{0}%
1}_{=\dfrac{\mu^{2}}{2}}=\mu^{2}-0-2\cdot\dfrac{\mu^{2}}{2}=0.
\]
Since%
\begin{align*}
\left[  L_{1},L_{-1}\right]  -2L_{0}  &  =\gamma_{1}\underbrace{\delta
_{1,-\left(  -1\right)  }}_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.fockvir.answer2.1}), applied to }n=1\text{ and }m=-1\right) \\
&  =\gamma_{1}=c\underbrace{\dfrac{1^{3}-1}{6}}_{=0}+2\cdot1\cdot\xi\left(
L_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.2}%
), applied to }n=1\right) \\
&  =0+2\cdot1\cdot\xi\left(  L_{0}\right)  =2\xi\left(  L_{0}\right)  ,
\end{align*}
this rewrites as $2\xi\left(  L_{0}\right)  \cdot1=0$, so that $\xi\left(
L_{0}\right)  =0$.

On the other hand,%
\[
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1=\underbrace{L_{2}%
L_{-2}1}_{=2\mu^{2}+\dfrac{1}{2}}-L_{-2}\underbrace{L_{2}1}_{=0}%
-4\underbrace{L_{0}1}_{=\dfrac{\mu^{2}}{2}}=\left(  2\mu^{2}+\dfrac{1}%
{2}\right)  -0-4\cdot\dfrac{\mu^{2}}{2}=\dfrac{1}{2}.
\]
Since%
\begin{align*}
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1  &  =\gamma
_{2}\underbrace{\delta_{2,-\left(  -2\right)  }}_{=1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.1}), applied to
}n=2\text{ and }m=-2\right) \\
&  =\gamma_{2}=c\underbrace{\dfrac{2^{3}-2}{6}}_{=1}+2\cdot2\cdot
\underbrace{\xi\left(  L_{0}\right)  }_{=0}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.fockvir.answer2.2}), applied to }n=2\right) \\
&  =c+0=c,
\end{align*}
this rewrites as $c\cdot1=\dfrac{1}{2}$, so that $c=\dfrac{1}{2}$.

Due to $\xi\left(  L_{0}\right)  =0$ and $c=\dfrac{1}{2}$, we can rewrite
(\ref{pf.fockvir.answer2.2}) as
\[
\gamma_{n}=\dfrac{1}{2}\cdot\dfrac{n^{3}-n}{6}+2n0=\dfrac{n^{3}-n}{12}.
\]
Hence, (\ref{pf.fockvir.answer2.1}) becomes%
\[
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\dfrac{n^{3}-n}%
{12}\delta_{n,-m}.
\]
We have thus proven:

\begin{proposition}
\label{prop.fockvir.answer2}For any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we
have%
\begin{equation}
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}-n}%
{12}\delta_{n,-m} \label{prop.fockvir.answer2.form}%
\end{equation}
(where $L_{n}$ and $L_{m}$ are maps $F_{\mu}\rightarrow F_{\mu}$ as explained
in Convention \ref{conv.fockvir.L}). Thus, we can make $F_{\mu}$ a
representation of $\operatorname*{Vir}$ by letting the element $L_{n}$ of
$\operatorname*{Vir}$ act as the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$, and letting the element $C$ of $\operatorname*{Vir}$
act as $\operatorname*{id}$.
\end{proposition}

Due to Proposition \ref{prop.fockvir.answer1}, this $\operatorname*{Vir}%
$-action harmonizes with the $\mathcal{A}$-action on $F_{\mu}$:

\begin{proposition}
The $\mathcal{A}$-action on $F_{\mu}$ extends (essentially uniquely) to an
action of $\operatorname*{Vir}\ltimes\mathcal{A}$ on $F_{\mu}$ with $C$ acting
as $1$.
\end{proposition}

This is the reason why the construction of the Virasoro algebra involved the
$2$-cocycle $\dfrac{1}{2}\omega$ rather than $\omega$ (or, actually, rather
than simpler-looking $2$-cocycles like $\left(  L_{n},L_{m}\right)  \mapsto
n^{3}\delta_{n,-m}$).

Our proof of Proposition \ref{prop.fockvir.answer2} above was rather insidious
and nonconstructive: We used the Dixmier theorem to prove (what boils down to)
an algebraic identity, and later we used Theorem \ref{thm.H^2(W)} (which is
constructive but its application is rather unexpected) to reduce our
computations to two concrete cases. One might wonder whether it is possible to
prove Proposition \ref{prop.fockvir.answer2} more directly. Indeed, it is
possible, and one such proof is given in the Kac-Raina book. Let us give
another such proof, which is similar to our above one but cleaner:

\textit{Second proof of Proposition \ref{prop.fockvir.answer2}.} \textit{1st
Step:} For every $n\in\mathbb{Z}$, $m\in\mathbb{Z}$, we have%
\begin{equation}
\left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m},a_{k}\right]
=0\ \ \ \ \ \ \ \ \ \ \text{for any }k\in\mathbb{Z}.
\label{pf.fockvir.answer2.pf.1}%
\end{equation}
(Let us recall that the $\left[  \cdot,\cdot\right]  $ brackets mean
commutators of endomorphisms of $F_{\mu}$ here, not Lie brackets of
$\mathcal{A}$.)

\textit{Proof of (\ref{pf.fockvir.answer2.pf.1}).} Let $k\in\mathbb{Z}$. Then,%
\begin{align*}
&  \left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}%
,a_{k}\right] \\
&  =\underbrace{\left[  \left[  L_{n},L_{m}\right]  ,a_{k}\right]
}_{\substack{=\left[  \left[  L_{n},a_{k}\right]  ,L_{m}\right]  +\left[
L_{n},\left[  L_{m},a_{k}\right]  \right]  \\\text{(by the Leibniz identity
for commutators)}}}-\left(  n-m\right)  \left[  L_{n+m},a_{k}\right] \\
&  =\left[  \underbrace{\left[  L_{n},a_{k}\right]  }_{\substack{=-ka_{n+k}%
\\\text{(by Proposition \ref{prop.fockvir.answer1},}\\\text{applied to
}k\text{ instead of }m\text{)}}},L_{m}\right]  +\left[  L_{n}%
,\underbrace{\left[  L_{m},a_{k}\right]  }_{\substack{=-ka_{m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }m\text{ and
}k\text{ instead of }n\text{ and }m\text{)}}}\right]  -\left(  n-m\right)
\underbrace{\left[  L_{n+m},a_{k}\right]  }_{\substack{=-ka_{n+m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }n+m\text{ and
}k\text{ instead of }n\text{ and }m\text{)}}}\\
&  =-k\underbrace{\left[  a_{n+k},L_{m}\right]  }_{=-\left[  L_{m}%
,a_{n+k}\right]  }-k\left[  L_{n},a_{m+k}\right]  +\left(  n-m\right)
ka_{n+m+k}\\
&  =k\underbrace{\left[  L_{m},a_{n+k}\right]  }_{\substack{=-\left(
n+k\right)  a_{m+n+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m\text{ and }n+k\text{ instead of }n\text{ and
}m\text{)}}}-k\underbrace{\left[  L_{n},a_{m+k}\right]  }_{\substack{=-\left(
m+k\right)  a_{n+m+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m+k\text{ instead of }m\text{)}}}+\left(  n-m\right)
ka_{n+m+k}\\
&  =-k\left(  n+k\right)  \underbrace{a_{m+n+k}}_{=a_{n+m+k}}+k\left(
m+k\right)  a_{n+m+k}+\left(  n-m\right)  ka_{n+m+k}\\
&  =-k\left(  n+k\right)  a_{n+m+k}+k\left(  m+k\right)  a_{n+m+k}+\left(
n-m\right)  ka_{n+m+k}\\
&  =\underbrace{\left(  -k\left(  n+k\right)  +k\left(  m+k\right)  +\left(
n-m\right)  k\right)  }_{=0}a_{n+m+k}=0.
\end{align*}
This proves (\ref{pf.fockvir.answer2.pf.1}).

\textit{2nd Step:} For every positive integer $n$, we have%
\[
L_{n}1=
\]


[...]

Remark \ref{rmk.fockvir.explicit} \textbf{(d)}.

[...]

We can generalize our family $\left(  L_{n}\right)  _{n\in\mathbb{Z}}$ of
operators on $F_{\mu}$ as follows (the so-called \textit{Fairlie construction}):

\begin{theorem}
\label{thm.fockvir.hw2ex1}Let $\mu\in\mathbb{C}$ and $\lambda\in\mathbb{C}$.
We can define a linear map $\widetilde{L}_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$ as follows: For $n\neq0$, define the map $\widetilde{L}%
_{n}$ by%
\[
\widetilde{L}_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{m+n}:\right.  +i\lambda na_{n}%
\]
(where $i$ stands for the complex number$\sqrt{-1}$). Define the map
$\widetilde{L}_{0}$ by%
\[
\widetilde{L}_{0}=\dfrac{\mu^{2}}{2}+\dfrac{\lambda^{2}}{2}+\sum
\limits_{j>0}a_{-j}a_{j}.
\]
Then, this defines an action of $\operatorname*{Vir}$ on $F_{\mu}$ with
$c=1+12\lambda^{2}$ (by letting $L_{n}\in\operatorname*{Vir}$ act as the
operator $\widetilde{L}_{n}$, and by letting $C\in\operatorname*{Vir}$ acting
as $1+12\lambda^{2}$).
\end{theorem}

Proving this proposition was exercise 1 in homework problem set 2. It is
rather easy now that we have proven Propositions \ref{prop.fockvir.answer1}
and \ref{prop.fockvir.answer2} and thus left to the reader.

\begin{proposition}
If $\mu\in\mathbb{R}$, then $F_{\mu}$ is a unitary representation of
$\mathcal{A}$.
\end{proposition}

\textit{Proof.} Let $n_{1}+...+n_{k}=m_{1}+...+m_{k}$. Then,%
\[
\left\langle x_{1}^{n_{1}}...x_{k}^{n_{k}},x_{1}^{m_{1}}...x_{k}^{m_{k}%
}\right\rangle =\left(  k\dfrac{\partial}{\partial x_{k}}\right)  ^{m_{k}%
}...\left(  1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}x_{1}^{n_{1}%
}...x_{k}^{n_{k}}=\delta_{\overrightarrow{n},-\overrightarrow{m}}\cdot
\prod\limits_{j=1}^{k}j^{m_{j}}\prod\limits_{j=1}^{k}m_{j}!>0.
\]
Thus, the matrix of the form is diagonal with positive diagonal entries.

\begin{corollary}
If $\mu,\lambda\in\mathbb{R}$, then the $\operatorname*{Vir}$-representation
on $F_{\mu}$ given by $\widetilde{L}_{n}$ is unitary.
\end{corollary}

\textit{Proof.}
\begin{align*}
\widetilde{L}_{n}^{\dag}  &  =\dfrac{1}{2}\sum\limits_{m}\left.
:a_{-m}a_{n+m}:\right.  ^{\dag}+\left(  i\lambda na_{n}\right)  ^{\dag}\\
&  =\dfrac{1}{2}\sum\limits_{m}\left.  :a_{-m}a_{-n-m}:\right.  -i\lambda
na_{-n}=\widetilde{L}_{-n}.
\end{align*}


\begin{corollary}
The $\operatorname*{Vir}$-representation $F_{\mu}$ is completely reducible for
$\mu\in\mathbb{R}$.
\end{corollary}

Now, $L_{0}1=\dfrac{\mu^{2}+\lambda^{2}}{2}1$ and $C1=\left(  1+12\lambda
^{2}\right)  1$. Thus, the Verma module $M_{h,c}:=M_{h,c}^{+}$ of the Virasoro
algebra $\operatorname*{Vir}$ for $h=\dfrac{\mu^{2}+\lambda^{2}}{2}$ and
$c=1+12\lambda^{2}$ maps to $F_{\mu}$ with $v_{h,c}\mapsto1$.

\begin{proposition}
For Weil generic $\mu$ and $\lambda$, this is an isomorphism.
\end{proposition}

\textit{Proof.} The dimension of the degree-$n$ part of both modules is
$p\left(  n\right)  $. The map has degree $0$. Hence, if it is injective, it
is surjective. But for Weil generic $\mu$ and $\lambda$, the
$\operatorname*{Vir}$-module $M_{h,c}$ is irreducible, so the map is injective.

\begin{corollary}
For Weil generic $\mu$ and $\lambda$ in $\mathbb{R}$, the representation
$M_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

For any $\mu$ and $\lambda$ in $\mathbb{R}$, the representation $L_{\dfrac
{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

In other words, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq\dfrac{c-1}{24}$.
\end{corollary}

\subsection{Quantum fields}

For us, the $a_{n}$ etc. (vectors in Lie algebras) were the primary objects of
concern. For physicists, instead, certain generating functions built of these
objects are primary objects, since they are closer to what they observe. They
are called \textit{quantum fields}.

For example, in $\mathcal{A}$, let us set $a\left(  z\right)  =\sum
\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$, where $z$ is a formal variable. This
sum $\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is a formal sum which is
infinite in both directions, so it is not an element of any of the rings
$\mathcal{A}\otimes\mathbb{C}\left[  \left[  z\right]  \right]  $ or
$\mathcal{A}\otimes\mathbb{C}\left[  \left[  z,z^{-1}\right.  \right]  $.
Instead, we can consider such sums as elements of the \textbf{vector space}
$\mathcal{A}^{\mathbb{Z}}$ of all maps $\mathbb{Z}\rightarrow\mathcal{A}$ (id
est, of all sequences of elements of $\mathcal{A}$ indexed by integers). In
this interpretation, a sum of the form $\sum\limits_{n\in\mathbb{Z}}b_{n}%
z^{n}$ (with $b_{n}\in\mathcal{A}$) is just a fancy way to write the sequence
$\left(  b_{n}\right)  _{n\in\mathbb{Z}}$, and thus our sum $a\left(
z\right)  =\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is the sequence $\left(
a_{-n-1}\right)  _{n\in\mathbb{Z}}\in\mathcal{A}^{\mathbb{Z}}$.

As we said, "sums" like $a\left(  z\right)  $ lie in the vector space
$\mathcal{A}^{\mathbb{Z}}$. This vector space is not a ring, and therefore we
cannot multiply two such "sums" in general. \textbf{However}, in the
following, we are going to learn about several things that we \textbf{can} do
with such "sums". One first thing that we notice about our concrete "sum"
$a\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is that if we
apply $a\left(  z\right)  $ to some vector $v$ in $F_{\mu}$
(componentwise\footnote{By "evaluating" a term like $\left(  a\left(
z\right)  \right)  v$ at a vector $v$ "componentwise", we mean evaluating
$\sum\limits_{n\in\mathbb{Z}}\left(  a_{n}z^{-n-1}\right)  \left(  v\right)
$. Here, the variable $z$ is decreed to commute with everything else, so that
$\left(  a_{n}z^{-n-1}\right)  \left(  v\right)  $ means $z^{-n-1}a_{n}v$.}),
then we get a sum $\sum\limits_{n\in\mathbb{Z}}z^{-n-1}a_{n}v$ which is
infinite in one direction only (namely, in the direction $n\rightarrow-\infty
$). So this sum $\sum\limits_{n\in\mathbb{Z}}z^{-n-1}a_{n}v$ evaluates to an
element of $F_{\mu}\otimes\mathbb{C}\left[  \left[  z,z^{-1}\right.  \right]
=F_{\mu}\left[  \left[  z,z^{-1}\right.  \right]  $.

Physicists call $a\left(  z\right)  $ a \textit{quantum field} (more
precisely, a free bosonic field).

While we cannot take the square $\left(  a\left(  z\right)  \right)  ^{2}$ of
our "sum" $a\left(  z\right)  $ (since $\mathcal{A}^{\mathbb{Z}}$ is not a
ring), we can multiply two sums "with different variables"; e. g., we can
multiply $a\left(  z\right)  $ and $a\left(  w\right)  $, where $z$ and $w$
are two distinct formal variables. The product $a\left(  z\right)  a\left(
w\right)  $ is defined as the formal sum $\sum\limits_{\left(  n,m\right)
\in\mathbb{Z}^{2}}a_{n}a_{m}z^{-n-1}z^{-m-1}$; such formal sums can be
interpreted as maps $\mathbb{Z}^{2}\rightarrow U\left(  \mathcal{A}\right)  $.

It is easy to see that $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$. This identity, in the
first place, holds on the level of formal sums\footnote{with the "sum"
$\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$ being shorthand notation for
the map
\[
\mathbb{Z}^{2}\rightarrow U\left(  \mathcal{A}\right)
,\ \ \ \ \ \ \ \ \ \ \left(  n,m\right)  \mapsto\delta_{n+m+2,0}\left(
m+1\right)
\]
}, but if we evaluate it on an element $v$ of $F_{\mu}$, then we get an
identity $\left[  \left(  a\left(  z\right)  \right)  v,\left(  a\left(
w\right)  \right)  v\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}v$
which holds in the actual ring $F_{\mu}\otimes\mathbb{C}\left[  \left[
z,z^{-1}\right.  \right]  \otimes\mathbb{C}\left[  \left[  w,w^{-1}\right.
\right]  $.

We can obtain the "series" $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$ by differentiating a
more basic "series":%
\[
\delta\left(  w-z\right)  :=\sum_{n\in\mathbb{Z}}z^{-n-1}w^{n}.
\]
This, again, is a formal series infinite in both directions. Why do we call it
$\delta\left(  w-z\right)  $ ? Because in analysis, we have the formula
$\int\delta\left(  x-y\right)  f\left(  y\right)  dy=f\left(  x\right)  $;
this series satisfies a similar property (namely, $\dfrac{1}{2\pi i}%
\oint\limits_{\left\vert z\right\vert =1}\delta\left(  w-z\right)  f\left(
z\right)  dz=f\left(  y\right)  $ formally by using the rule $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}z^{-n-1}f\left(  z\right)
dz=f_{n}$ where $f=\sum\limits_{n\in\mathbb{Z}}f_{n}z^{n}$). And now, $\left[
a\left(  z\right)  ,a\left(  w\right)  \right]  =\sum\limits_{n\in\mathbb{Z}%
}nz^{-n-1}w^{n-1}$ becomes $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\partial_{w}\delta\left(  w-z\right)  =:\delta^{\prime}\left(
w-z\right)  $.

Something more interesting comes out for the Virasoro algebra: Set $T\left(
z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$. Then, \textit{in the
Witt algebra}, we have%
\begin{align*}
\left[  T\left(  z\right)  ,T\left(  w\right)  \right]   &  =\sum
\limits_{n,m}\left(  n-m\right)  L_{n+m}z^{-n-2}w^{-m-2}\\
&  =\sum_{k,m}L_{k}\underbrace{\left(  k-2m\right)  }_{=\left(  k+2\right)
-\left(  2m+2\right)  }z^{m-k-2}w^{-m-2}\\
&  =\left(  \sum_{k}L_{k}\left(  k+2\right)  z^{-k-3}\right)  \left(  \sum
_{m}z^{m+1}w^{-m-2}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  \sum_{k}L^{k}z^{-k-2}\right)  \left(  \sum
_{m}\left(  -2m-2\right)  z^{m}w^{-m-2}\right) \\
&  =2T\left(  z\right)  \delta^{\prime}\left(  w-z\right)  -T^{\prime}\left(
z\right)  \delta\left(  w-z\right)  .
\end{align*}
Note that this formula defines the Lie bracket of the Witt algebra. This is
how physicists would define the Witt algebra.

For the Virasoro algebra: add%
\[
\sum_{n}\dfrac{n^{3}-n}{12}Cz^{-n-2}w^{n-2}=\dfrac{c}{12}\delta^{\prime
\prime\prime}\left(  w-z\right)  .
\]
Together:%
\[
\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  =-T^{\prime}\left(
z\right)  \delta\left(  w-z\right)  +2T\left(  z\right)  \delta^{\prime
}\left(  w-z\right)  +\dfrac{c}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  .
\]


\textbf{Exercise:} For $\operatorname*{Vir}\ltimes\mathcal{A}$, we have in
$F_{\mu}$:%
\[
\left[  T\left(  z\right)  ,a\left(  w\right)  \right]  =a\left(  z\right)
\delta^{\prime}\left(  w-z\right)  .
\]


Recall%
\[
:a_{m}a_{n}:\ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
So we can write $:a\left(  z\right)  a\left(  w\right)  :$. This is equivalent
to the homework sheet.

Also, more importantly, we can write $:a\left(  z\right)  ^{2}:$ (defined as
$:a\left(  z\right)  a\left(  z\right)  :$), but not $a\left(  z\right)  ^{2}$.

[...] [homework sheet definition switches if $m<0$

Now we can write $T\left(  z\right)  =\dfrac{1}{2}:a\left(  z\right)  ^{2}:$.

[...] [IMPORTANT!\ THE\ DEFINITION\ IN\ THIS\ TEXT\ PERTAINS\ ONLY\ TO\ THE\ HEISENBERG\ ALGEBRA.\ THE\ DEFINITION\ ON\ THE\ EXERCISE\ SHEET\ IS\ GENERAL.]

\textbf{Exercise 1.} For any $\beta\in\mathbb{C}$, the formula $T\left(
v\right)  =\dfrac{1}{2}:a\left(  z\right)  ^{2}:+\beta a^{\prime}\left(
z\right)  $ defines a representation of $\operatorname*{Vir}$ on $F_{\mu}$
with $c=1-12\beta^{2}$.

\textbf{Exercise 2.} For any $\beta\in\mathbb{C}$, there is a homomorphism
$\varphi_{\beta}:\operatorname*{Vir}\rightarrow\operatorname*{Vir}%
\ltimes\mathcal{A}$ (a splitting of the projection $\operatorname*{Vir}%
\ltimes\mathcal{A}\rightarrow\operatorname*{Vir}$) given by%
\begin{align*}
\varphi_{\beta}\left(  L_{n}\right)   &  =L_{n}+\beta a_{n}%
,\ \ \ \ \ \ \ \ \ \ n\neq0;\\
\varphi_{\beta}\left(  L_{0}\right)   &  =L_{0}+\beta a_{0}+\dfrac{\beta^{2}%
}{2}K.
\end{align*}


\textbf{Exercise 3.} If we twist the action of Exercise 1 by this map, we
recover the action of problem 1 of Homework 2 for $\beta=i\lambda$.

\subsection{More on unitary representations}

\textbf{Last time:} $L_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is
unitary (for $\lambda,\mu\in\mathbb{R}$), so $L_{h,c}$ is unitary if $c\geq1$
and $h\geq\dfrac{c-1}{24}$.

We can extend this as follows: $L_{0,1}^{\otimes m-1}\otimes L_{h,c}$ is
unitary and has a highest-weight vector $v_{0,1}^{\otimes m-1}\otimes v_{h,c}$
which has weight $\left(  h,c+m-1\right)  $. Hence, the representation
$L_{h,c+m-1}$ is unitary [why? use irreducibility of unitary modules and stuff].

Hence, $L_{h,c}$ is unitary if $c\geq m$ and $h\geq\dfrac{c-m}{24}$.

\begin{theorem}
In fact, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq0$.
\end{theorem}

But this is harder to show.

This is still not an only-if. For example, $L_{0,0}$ is unitary (and $1$-dimensional).

\begin{proposition}
\label{prop.Lhc.unitary.triv}If $L_{h,c}$ is unitary, then $h\geq0$ and
$c\geq0$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Lhc.unitary.triv}.} We have $\left(
L_{-n}v_{h,c},L_{-n}v_{h,c}\right)  \geq0$. But $\left(  L_{-n}v_{h,c}%
,L_{-n}v_{h,c}\right)  =\left(  \underbrace{L_{n}L_{-n}}_{=2nL_{0}%
+\dfrac{n^{3}-n}{12}C}v_{h,c},v_{h,c}\right)  =2nh+\dfrac{n^{3}-n}{12}c$. By
taking $n\rightarrow\infty$, we show $c\geq0$. By taking $n=1$, we get
$h\geq0$.

Let $\delta\in\left\{  0,\dfrac{1}{2}\right\}  $. Let $C_{\delta}$ be the
algebra of free fermions (for $\delta=0$ it is called Ramond sector; for
$\delta=\dfrac{1}{2}$ it is called Neveu-Schwarz sector). It is defined as%
\[
C_{\delta}=\left\langle
\begin{array}
[c]{c}%
\psi_{j}\ \mid\ j\in\delta+\mathbb{Z}\\
\mid\ \psi_{j}\psi_{k}+\psi_{k}\psi_{j}=\delta_{k,-j}%
\end{array}
\right\rangle .
\]
This is an infinite-dimensional Clifford algebra.

We have a representation $V_{\delta}$ of $C_{\delta}$: Namely, $V_{\delta
}=\wedge\left(  \xi_{n}\ \mid\ n\in\left(  \delta+\mathbb{Z}\right)  _{\geq
0}\right)  $. This is an infinite-dimensional spinor representation of the
above Clifford algebra. The action is given by%
\begin{align*}
\psi_{-n}  &  \mapsto\xi_{n}\ \ \ \ \ \ \ \ \ \ \text{for }n<0;\\
\psi_{n}  &  \mapsto\dfrac{\partial}{\partial\xi_{n}}%
\ \ \ \ \ \ \ \ \ \ \text{for }n>0;\\
\psi_{0}  &  \mapsto\dfrac{1}{\sqrt{2}}\left(  \dfrac{\partial}{\partial
\xi_{0}}+\xi_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{this is only
relevant if }\delta=0\right)  .
\end{align*}
Here, $\dfrac{\partial}{\partial\xi_{i}}$ follows the Koszul sign rule. This
indeed defines a representation (exercise!). From Homework Set 2 problem 2, we know:

\begin{proposition}
Let%
\[
L_{k}=\delta_{k,0}\dfrac{1-2\delta}{16}+\dfrac{1}{2}\sum\limits_{j\in
\mathbb{Z}+\delta}j:\psi_{-j}\psi_{j+k}:,
\]
where the normal ordering is defined as follows:%
\[
:\psi_{n}\psi_{m}:\ =\ \left\{
\begin{array}
[c]{c}%
-\psi_{m}\psi_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
\psi_{n}\psi_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
Then:

\textbf{(a)} $\left[  \psi_{m},L_{k}\right]  =\left(  m+\dfrac{k}{2}\right)
\psi_{m+k}$.

\textbf{(b)} $\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}%
+\delta_{n,-m}\dfrac{m^{3}-m}{24}$ (so $c=\dfrac{1}{2}$).
\end{proposition}

Now this representation $V_{\delta}$ of $\operatorname*{Vir}$ is unitary. In
fact, consider the Hermitian form under which all monomials in $\psi_{i}$ are
orthonormal (positive definite). Then it is easy to see that $\psi_{j}^{\dag
}=\psi_{-j}$. Thus, $L_{n}^{\dag}=L_{-n}$.

But these $V_{\delta}$ are reducible, since the $L_{n}$ preserve parity:
$V_{\delta}=V_{\delta}^{+}\oplus V_{\delta}^{-}$.

\begin{theorem}
$V_{\delta}^{+}$ and $V_{\delta}^{-}$ are irreducible Virasoro modules.
\end{theorem}

We will not prove this.

What are the highest weights?

First consider the case $\delta=0$. The highest-weight vector of $V_{\delta
}^{+}$ is $1$, with weight $\left(  \dfrac{1}{16},\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{0}$, with weight $\left(  \dfrac{1}{16},\dfrac
{1}{2}\right)  $. Thus, $V_{\delta}^{+}\cong V_{\delta}^{-}$ by action of
$\psi_{0}$ (since $\psi_{0}^{2}=\dfrac{1}{2}$).

Now consider the case $\delta=\dfrac{1}{2}$. The highest-weight vector of
$V_{\delta}^{+}$ is $1$, with weight $\left(  0,\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{1/2}$, with weight $\left(  \dfrac{1}{2}%
,\dfrac{1}{2}\right)  $.

\begin{corollary}
The representation $L_{h,\dfrac{1}{2}}$ is unitary if $h=0$, $h=\dfrac{1}{16}$
or $h=\dfrac{1}{2}$. (In physics: Ising model.)
\end{corollary}

We will not prove:

\begin{proposition}
This is an only-if as well.
\end{proposition}

General answer for $c<1$: for $c=1-\dfrac{6}{\left(  m+2\right)  \left(
m+3\right)  }$ for $m\in\mathbb{N}$, there are finitely many $h$ where
$L_{h,c}$ is unitary. For other values of $c$, there are no such values.

\begin{definition}
The \textit{character }$\operatorname*{ch}\nolimits_{V}\left(  q\right)  $ of
a $\operatorname*{Vir}$-module $V$ from category $\mathcal{O}^{+}$ is
$\operatorname*{Tr}\nolimits_{V}\left(  q^{L_{0}}\right)  =\sum\left(  \dim
V_{\lambda}\right)  q^{\lambda}$ for $V_{\lambda}=$generalized eigenspace of
$L_{0}$ with eigenvalue $\lambda$.
\end{definition}

This is related to the old definition of character [how?]

What are the characters of the above modules? Since $V_{\delta}^{+}%
=\wedge\left(  \xi_{1},\xi_{2},\xi_{3},...\right)  ^{+}$, we have%
\[
\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
=q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}\right)
...=q^{1/16}\prod\limits_{n\geq1}\left(  1+q^{n}\right)
\]
(because
\begin{align*}
2\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{0}}\left(  q\right)  =q^{1/16}\left(
1+1\right)  \left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}%
\right)  ...\\
&  =2q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(
1+q^{3}\right)  ...
\end{align*}
).

Now%
\begin{align*}
\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)
+\operatorname*{ch}\nolimits_{L_{\dfrac{1}{2},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{\dfrac{1}{2}}}\left(  q\right)  =\left(
1+q^{1/2}\right)  \left(  1+q^{3/2}\right)  \left(  1+q^{5/2}\right)  ...\\
&  =\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  .
\end{align*}
Thus, $\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)  $ is
the integer part of the product $\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}%
}\left(  1+q^{n}\right)  $, and $\operatorname*{ch}\nolimits_{L_{\dfrac{1}%
{2},\dfrac{1}{2}}}\left(  q\right)  $ is the half-integer part of the product
$\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  $.

\subsection{Representations of $\mathfrak{gl}_{\infty}$}

What is $\mathfrak{gl}_{\infty}$? Here is a first version of $\mathfrak{gl}%
_{\infty}$ (there will be different ones):

\begin{definition}
We define $\mathfrak{gl}_{\infty}$ to be the vector space of infinite matrices
with rows and columns labeled by integers (not only positive integers) such
that only finitely many entries are nonzero. This vector space $\mathfrak{gl}%
_{\infty}$ is an associative algebra \textit{without unit} (by matrix
multiplication); we can thus make $\mathfrak{gl}_{\infty}$ into a Lie algebra
by the commutator in this associative algebra.
\end{definition}

We will study the representations of this $\mathfrak{gl}_{\infty}$. The theory
of these representations will extend the well-known (Schur-Weyl) theory of
representations of $\mathfrak{gl}_{n}$.

\begin{definition}
The \textit{vector representation} $V$ of $\mathfrak{gl}_{\infty}$ is defined
as the vector space $\mathbb{C}^{\left(  \mathbb{Z}\right)  }=\left\{  \left(
x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }\mid\ x_{i}\in\mathbb{C}\text{; only
finitely many }x_{i}\text{ are nonzero}\right\}  $.

For every $j\in\mathbb{Z}$, let $v_{j}$ be the vector $\left(  \delta
_{i,j}\right)  _{i\in\mathbb{Z}}\in V$. Then, $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ is a basis of $V$.
\end{definition}

\begin{Convention}
When we draw infinite matrices with rows and columns are labeled by integers,
the index of the rows is supposed to increase as we go from left to right, and
the index of the columns is supposed to increase as we go from top to bottom.
\end{Convention}

We can consider $\wedge^{i}V$ for every $i\in\mathbb{N}$. More generally, we
have the so-called \textit{Schur modules}:

\begin{definition}
If $\pi\in\operatorname*{Irr}S_{n}$, then we can define a representation
$S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ by $S_{\pi}\left(
V\right)  =\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes
n}\right)  $. This $S_{\pi}\left(  V\right)  $ is called the $\pi$\textit{-th
Schur module} of $V$.
\end{definition}

\begin{proposition}
For every $\pi\in\operatorname*{Irr}S_{n}$, the representation $S_{\pi}\left(
V\right)  $ of $\mathfrak{gl}_{\infty}$ is irreducible.
\end{proposition}

On the other hand, we can define so-called \textit{highest-weight
representations}. Before we do so, let us make $\mathfrak{gl}_{\infty}$ into a
graded Lie algebra:

\begin{definition}
For every $i\in\mathbb{Z}$, let $\mathfrak{gl}_{\infty}^{i}$ be the subspace
of $\mathfrak{gl}_{\infty}$ which consists of matrices which have nonzero
entries only the $i$-th diagonal. (The $i$\textit{-th diagonal} consists of
the entries in the $\left(  \alpha,\beta\right)  $-th places with
$\beta-\alpha=i$.)

Then, $\mathfrak{gl}_{\infty}=\bigoplus\limits_{i\in\mathbb{Z}}\mathfrak{gl}%
_{\infty}^{i}$, and this makes $\mathfrak{gl}_{\infty}$ into a $\mathbb{Z}%
$-graded Lie algebra. Note that $\mathfrak{gl}_{0}$ is abelian. Let
$\mathfrak{gl}_{\infty}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\mathfrak{gl}_{\infty}$,
so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}\mathfrak{gl}%
_{\infty}^{i}$ is the space of all strictly lower-triangular matrices in
$\mathfrak{gl}_{\infty}$, the subspace $\mathfrak{h}=\mathfrak{gl}_{\infty
}^{0}$ is the space of all diagonal matrices in $\mathfrak{gl}_{\infty}$, and
the subspace $\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\mathfrak{gl}_{\infty
}^{i}$ is the space of all strictly upper-triangular matrices in
$\mathfrak{gl}_{\infty}$.
\end{definition}

\begin{definition}
For every $i,j\in\mathbb{Z}$, let $E_{i,j}$ be the matrix (with rows and
columns labeled by integers) whose $\left(  i,j\right)  $-th entry is $1$ and
whose all other entries are $0$. Then, $\left(  E_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$ is a basis of $\mathfrak{gl}_{\infty}$.
\end{definition}

\begin{definition}
For every $\lambda\in\mathfrak{h}^{\ast}$, let $M_{\lambda}$ be the
highest-weight Verma module $M_{\lambda}^{+}$ (as defined in Definition
\ref{def.verma}). Let $J_{\lambda}=\operatorname*{Ker}\left(  \cdot
,\cdot\right)  \subseteq M_{\lambda}$ be the maximal proper submodule. Let
$L_{\lambda}$ be the quotient module $M_{\lambda}\diagup J_{\lambda
}=M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$; then, $L_{\lambda}$
is irreducible (as we know).
\end{definition}

\begin{definition}
Unitary structure: $E_{i,j}^{\dag}=E_{j,i}$ (thus, $\dag$ is transposition of
matrix $\circ$ complex conjugation).
\end{definition}

Very important: For usual $\mathfrak{gl}_{n}$, Schur modules = highest-weight
modules up to det:

For $\mathfrak{gl}_{n}$, every finite-dimensional irreducible representation
and any unitary representation is of the form $S_{\pi}\left(  V_{n}\right)
\otimes\left(  \wedge^{n}\left(  V_{n}^{\ast}\right)  \right)  ^{\otimes j}$
for some $j\in\mathbb{N}$, where $V_{n}$ is the $\mathfrak{gl}_{n}$-module
$\mathbb{C}^{n}$.

Nothing like this is true for $\mathfrak{gl}_{\infty}$. Instead, exterior
powers of $V$ and highest-weight representations live in different worlds.
This is because $V$ is composed of infinite-dimensional vectors which have "no
top or bottom"; $V$ has no highest or lowest weight and does not lie in
category $\mathcal{O}^{+}$ or $\mathcal{O}^{-}$.

This is important, because many beautiful properties of representations of
$\mathfrak{gl}_{n}$ come from the equality of the highest-weight and Schur
module representations.

A way to marry these two worlds is by considering so-called
\textit{semiinfinite wedges}.

\begin{definition}
The space $\wedge^{\dfrac{\infty}{2}}V$ is the free vector space with basis
given by formal infinite wedge products $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$ where $i_{0}>i_{1}>i_{2}>...$ and $i_{k+1}=i_{k}-1$ for
all sufficiently large $k$. (Such wedge products are said to be
\textit{semiinfinite}.)
\end{definition}

This definition can be replaced by a somewhat more functorial one, which
doesn't use the basis $\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ of $V$
anymore. But it still needs a topology on $V$ (which makes $V$ locally
linearly compact), and some working with formal Laurent series. It proceeds
through the semiinfinite Grassmannian, and will not be done in these lectures.

$\mathbb{C}\left(  \left(  t\right)  \right)  $ field of formal Laurent series

$\operatorname*{Gr}=\left\{  U\subseteq\mathbb{C}\left(  \left(  t\right)
\right)  \ \mid\ \left(  U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]
\right]  \text{ and }\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[
\left[  n\right]  \right]  \right)  \right)  <\infty\right)  \text{ for some
sufficiently high }n\right\}  $.

Call $\operatorname*{sdim}U=\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[
\left[  n\right]  \right]  \right)  \right)  -n$ (independent of $n$ !)

This Grassmannian is the disjoint union $\coprod\operatorname*{Gr}%
\nolimits_{n}$.

There is something called a determinant line bundle on $\operatorname*{Gr}$.
The space of semiinfinite wedges is then defined as the space of regular
sections of this line bundle (in the sense of algebraic geometry).

Back to topic. [Book by Pressley and Segal about loop groups for explanations.]

The space $\wedge^{\dfrac{\infty}{2}}V$ is countably dimensional. More
precisely:%
\begin{align*}
\wedge^{\dfrac{\infty}{2}}V  &  =\bigoplus\limits_{n\in\mathbb{Z}}%
\wedge^{\dfrac{\infty}{2},m}V,\ \ \ \ \ \ \ \ \ \ \text{where}\\
\wedge^{\dfrac{\infty}{2},m}V  &  =\operatorname*{span}\left\{  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for
sufficiently large }m\right\}  .
\end{align*}
The space $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for sufficiently
large }m\right\}  $, which is easily seen to be countable. We will see later
that this basis can be naturally labeled by partitions (of all integers, not
just of $m$).

The Lie algebra $\mathfrak{gl}_{\infty}$ acts on the space $\wedge
^{\dfrac{\infty}{2}}V$ by the usual Leibniz rule:%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\]
The elements on the right have to be interpreted in a way such that $\wedge$
is antisymmetric and multilinear. Explicitly:%
\[
E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }j\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge\left(  v_{i}\text{ instead of }%
v_{j}\right)  \wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }j=i_{k}%
\end{array}
\right.  ,
\]
and use antisymmetry.

\begin{proposition}
\label{prop.Lomegam}The $\mathfrak{gl}_{\infty}$-module $\wedge^{\dfrac
{\infty}{2},m}V$ is the irreducible highest-weight representation
$L_{\omega_{m}}$ of $\mathfrak{gl}_{\infty}$ with highest weight $\omega
_{m}=\left(  ...,1,1,0,0,...\right)  $, where the last $1$ is on place $m$ and
the first $0$ is on place $m+1$. Moreover, $L_{\omega_{m}}$ is unitary.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Lomegam}.} Define a $w_{m}\in
\wedge^{\dfrac{\infty}{2},m}V$ by $w_{m}=v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...$. Then, $\mathfrak{n}_{+}\cdot w_{m}=0$. (In fact, if
$E_{i,j}\in\mathfrak{n}_{+}$ then $i<j$ and thus indiced are replaced by
smaller indices...) Moreover, every $h\in\mathfrak{h}$ satisfies
$hw_{m}=\omega_{m}\left(  h\right)  w_{m}$ (in fact, test at $h=E_{i,i}$).
Also, $w_{m}$ generates the $\mathfrak{gl}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$. Thus, $\wedge^{\dfrac{\infty}{2},m}V$ is a
highest-weight representation with highest weight $\omega_{m}$.

Next let us prove that it is unitary. This will yield that it is
irreducible.\footnote{We could also show the irreducibility more directly, by
showing that every sum of wedges can be used to get back $w_{m}$.}

Note that we can grade $\wedge^{\dfrac{\infty}{2},m}V$ by%
\[
\deg\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=-\sum\limits_{k}\left(  i_{k}+k-m\right)  .
\]
(Note that $\left(  i_{k}+k-m\right)  _{k}$ will be our partition.)

The unitarity is because the form in which the wedges are orthonormal is
$\dag$-invariant. Thus, irreducible. (We used Lemma \ref{lem.unitrick}.)
Proposition \ref{prop.Lomegam} is proven.

\begin{corollary}
For every finite sum $\sum\limits_{i}k_{i}\omega_{i}$ with $k_{i}\in
\mathbb{N}$, the representation $L_{\sum\limits_{i}k_{i}\omega_{i}}$ is unitary.
\end{corollary}

\textit{Proof.} Take the module $\bigotimes\limits_{i}L_{\omega_{i}}^{\otimes
k_{i}}$, and let $v$ be the tensor product of their respective highest weight
vectors. Let $L$ be the submodule generated by $v$. Then, $L$ is a
highest-weight module, and is unitary since it is a submodule of a unitary
module. Hence it is irreducible, and thus $L\cong L_{\sum\limits_{i}%
k_{i}\omega_{i}}$, qed.

\subsection{$\overline{\mathfrak{gl}_{\infty}}$}

Now we want to enlarge $\mathfrak{gl}_{\infty}$. (Remember that $\mathfrak{gl}%
_{\infty}$ is too small - it doesn't even contain the identity matrix.)

\begin{definition}
We define $\overline{\mathfrak{a}_{\infty}}$ to be the vector space of
infinite matrices with rows and columns labeled by integers (not only positive
integers) such that only finitely many \textbf{diagonals} are nonzero. This is
an associative algebra with $1$, and thus, by the commutator, a Lie algebra.
\end{definition}

We can think of the elements of $\overline{\mathfrak{a}_{\infty}}$ as
difference operators:

Consider $V$ as the space of sequences\footnote{In the following, "sequences"
means "sequences labeled by integers".} with finitely many nonzero entries,
and let $T:V\rightarrow V$ be the linear map given by $\left(  Tx\right)
_{n}=x_{n+1}$ for all $n\in\mathbb{Z}$. This map $T$ is called the
\textit{shift operator}. A \textit{difference operator} is an operator of the
form $A=\sum\limits_{i=p}^{q}a_{i}\left(  n\right)  T^{i}$, where $p$ and $q$
are some integers, and $a_{i}:\mathbb{Z}\rightarrow\mathbb{C}$ are some
functions. Then, $\overline{\mathfrak{a}_{\infty}}$ is the algebra of all such
operators. (These operators also act on the space of \textit{all} sequences,
not only on the space of sequence with finitely many nonzero entries.)

Note that $\overline{\mathfrak{a}_{\infty}}$ is no longer countably
dimensional. The family $\left(  E_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{N}^{2}}$ is no longer a vector space basis, but it is a topological
basis in an appropriately defined topology.

Note on topologies:

\begin{itemize}
\item Topology on $\overline{\mathfrak{a}_{\infty}}$: A sequence $\left(
A_{n}\right)  $ goes to a limit $A$ if for any vector $v\in\wedge
^{\dfrac{\infty}{2},m}V$, we have $A_{n}v=Av$ for sufficiently large $n$.

\item Topology on $\mathfrak{gl}_{\infty}$: Subspace of $\overline
{\mathfrak{a}_{\infty}}$. It is dense.

\item Topology on $\wedge^{\dfrac{\infty}{2},m}V$: discrete.
\end{itemize}

Let $\rho$ be the representation of $\mathfrak{gl}_{\infty}$ on $\wedge
^{\dfrac{\infty}{2},m}V$. Can we extend $\rho$ by continuity to a
representation of $\overline{\mathfrak{a}_{\infty}}$?

What this means in elementary terms: We have $\overline{\mathfrak{a}_{\infty}%
}=\bigoplus\limits_{i\in\mathbb{Z}}\mathfrak{a}_{\infty}^{i}$, where
$\mathfrak{a}_{\infty}^{i}$ are the matrices with nonzero entries only on the
$i$-th diagonal. Each $\mathfrak{a}_{\infty}^{i}$ has weak topology (i. e.,
stabilization on every term). Then, $\mathfrak{gl}_{\infty}^{i}$ in dense in
$\mathfrak{a}_{\infty}^{i}$. Can we extend $\rho$ by continuity to a
representation of $\overline{\mathfrak{a}_{\infty}}$?

Answer: Almost, but no (usually). We need a central extension.

For $i\neq0$, a typical element $X\in\mathfrak{a}_{\infty}^{i}$ is of the form
$X=\sum\limits_{j\in\mathbb{Z}}a_{j}E_{j,j+i}$ with $a_{j}\in\mathbb{C}$. Now
define $\rho\left(  x\right)  v=\sum\limits_{j\in\mathbb{Z}}a_{j}\rho\left(
E_{j,j+i}\right)  v$ for every $v\in\wedge^{\dfrac{\infty}{2},m}V$; this is a
finite sum (prove [...]) and thus makes sense.

But when $i=0$, we run into a problem: $\rho\left(  \sum\limits_{j\in
\mathbb{Z}}a_{j}E_{j,j}\right)  v=\sum\limits_{j\in\mathbb{Z}}a_{j}\rho\left(
E_{j,j}\right)  v$ is an infinite sum and thus makes no sense.

We modify:%
\[
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  .
\]
Then it is easy to see that $\widehat{\rho}$ extends to $\overline
{\mathfrak{a}_{\infty}}$ continuously as a linear map.

The following theorem will be a homework problem:

\begin{theorem}
\label{thm.japan}For any $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\overline{\mathfrak{a}_{\infty}}$, we have $\widehat{\rho}\left(  \left[
A,B\right]  \right)  -\left[  \widehat{\rho}\left(  A\right)  ,\widehat{\rho
}\left(  B\right)  \right]  =\alpha\left(  A,B\right)  $ where $\alpha\left(
A,B\right)  $ is a scalar depending on $A$ and $B$ (thus a $2$-cocycle). This
$\alpha\left(  A,B\right)  $ can be computed as follows: Write $A$ and $B$ as
block matrices $A=\left(
\begin{array}
[c]{cc}%
A_{11} & A_{12}\\
A_{21} & A_{22}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
B_{11} & B_{12}\\
B_{21} & B_{22}%
\end{array}
\right)  $, where the blocks are separated as follows:

- The left blocks contain the $j$-th columns for all $j\leq0$; the right
blocks contain the $j$-th columns for all $j>0$.

- The upper blocks contain the $i$-th rows for all $i\leq0$; the lower blocks
contain the $i$-th rows for all $i>0$.

Then, $\alpha\left(  A,B\right)  =\operatorname*{Tr}\left(  -B_{12}%
A_{21}+A_{12}B_{21}\right)  $. (This trace makes sense because the matrices
$A_{12}$, $B_{21}$, $A_{21}$, $B_{12}$ have only finitely many nonzero entries.)
\end{theorem}

This $2$-cocycle $\alpha$ is called the \textit{Japanese cocycle}.

We are going to prove soon (Proposition \ref{prop.japan.nontr} and Corollary
\ref{cor.japan.triv}) that $\alpha$ is a nontrivial $2$-cocycle, but its
restriction to $\mathfrak{gl}_{\infty}$ is trivial. This is a strange
situation for a dense Lie subalgebra!

\begin{corollary}
The $\alpha$ defined in Theorem \ref{thm.japan} is a $2$-cocycle on
$\overline{\mathfrak{a}_{\infty}}$.

We define $\mathfrak{a}_{\infty}$ as the $1$-dimensional central extension
$\widehat{\overline{\mathfrak{a}_{\infty}}}_{\alpha}$ of $\overline
{\mathfrak{a}_{\infty}}$ by $\mathbb{C}$ using this cocycle $\alpha$ (see
Definition \ref{def.centex} for what this means).
\end{corollary}

\begin{corollary}
\label{cor.japan.triv}The restriction of $\alpha$ to $\mathfrak{gl}_{\infty
}\times\mathfrak{gl}_{\infty}$ is a $2$-coboundary.
\end{corollary}

\textit{Proof of Corollary \ref{cor.japan.triv}.} Let $J=\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & -I_{\infty}%
\end{array}
\right)  \in\overline{\mathfrak{a}_{\infty}}$. Define a linear map
$f:\mathfrak{gl}_{\infty}\rightarrow\mathbb{C}$ by $f\left(  A\right)
=\operatorname*{Tr}\left(  JA\right)  $. Then, any $A\in\mathfrak{gl}_{\infty
}$ and $B\in\mathfrak{gl}_{\infty}$ satisfy $\alpha\left(  A,B\right)
=f\left(  \left[  A,B\right]  \right)  $, because:

$\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
\ast & \ast\\
\ast & \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}A_{12}%
\end{array}
\right)  $, so that
\[
\operatorname*{Tr}\left(  J\left[  A,B\right]  \right)  =-\operatorname*{Tr}%
\left(  \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}A_{12}\right)
=\operatorname*{Tr}\left(  -A_{21}B_{12}+B_{21}A_{12}\right)  =\alpha\left(
A,B\right)  .
\]
Proof of Corollary \ref{cor.japan.triv}.

But note that this proof does not extend to $\overline{\mathfrak{a}_{\infty}}%
$, because $f$ does not continuously extend to $\overline{\mathfrak{a}%
_{\infty}}$.

\begin{proposition}
\label{prop.japan.nontr}The $2$-cocycle $\alpha$ itself is not a $2$-coboundary.
\end{proposition}

\textit{Proof of Proposition \ref{prop.japan.nontr}.} Let $T$ be the shift
operator defined above. The span $\left\langle T^{j}\ \mid\ j\in
\mathbb{Z}\right\rangle $ is an abelian Lie subalgebra of $\overline
{\mathfrak{a}_{\infty}}$ (isomorphic to the quotient $\overline{\mathcal{A}}$
of the Heisenberg algebra $\mathcal{A}$ by its center). Any $2$-coboundary
must become zero when restricted onto an abelian Lie subalgebra. But $\alpha$,
restricted onto the span $\left\langle T^{j}\ \mid\ j\in\mathbb{Z}%
\right\rangle $, does not become $0$, since%
\[
\alpha\left(  T^{i},T^{j}\right)  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq-j;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }i=-j
\end{array}
\right.  .
\]
Proposition \ref{prop.japan.nontr} is thus proven.

From this proof, we get an embedding $\mathcal{A}\rightarrow\mathfrak{a}%
_{\infty}$ which lifts $\overline{\mathcal{A}}\rightarrow\overline
{\mathfrak{a}_{\infty}}$ and sends $K$ to $K$.

Now we can extend the theory for $\mathfrak{gl}_{\infty}$ that we discussed
last time to $\mathfrak{a}_{\infty}$. Namely, $\wedge^{\dfrac{\infty}{2}%
,m}V=L_{\omega_{m}}$ is unitary. Also, if $k_{1},k_{2},...,k_{n}\in\mathbb{N}%
$, then the representation $L_{k_{1}\omega_{1}+k_{2}\omega_{2}+...+k_{n}%
\omega_{n}}$ of $\mathfrak{a}_{\infty}$ is unitary.

We can also embed $\operatorname*{Vir}$ into $\mathfrak{a}_{\infty}$, and not
just in one way, but in infinitely many ways depending on two parameters:

\begin{proposition}
Let $\alpha,\beta\in\mathbb{C}$. Let $V_{\alpha,\beta}$ be the space of formal
expressions $gz^{\alpha}\left(  dz\right)  ^{\beta}$, where $g$ is a Laurent
polynomial in the variable $z$. (Such formal expressions can be seen as
"tensor fields" of rank $\beta$ and branching $\alpha$ on the punctured
complex plane.) According to Homework Set 1 exercise 1, the formula
\begin{equation}
f\partial\circ\left(  gz^{\alpha}\left(  dz\right)  ^{\beta}\right)  =\left(
fg^{\prime}+\alpha z^{-1}fg+\beta f^{\prime}g\right)  z^{\alpha}\left(
dz\right)  ^{\beta} \label{ex1.1.1}%
\end{equation}
defines an action of $W$ on $V_{\alpha,\beta}$. Thus, $V_{\alpha,\beta}$
becomes a $\operatorname*{Vir}$-module with $C$ acting as $0$.

For every $k\in\mathbb{Z}$, let $v_{k}=z^{-k+\alpha}\left(  dz\right)
^{\beta}\in V_{\alpha,\beta}$. (This is a different convention than the one
used in the homework!)

According to the homework (but with our notations),%
\[
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  n+1\right)  \right)  v_{k-m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\]
Thus, if we write $L_{m}$ as a matrix with respect to the basis $\left(
v_{k}\right)  _{k\in\mathbb{Z}}$ of $V_{\alpha,\beta}$, then this matrix lies
in $\overline{\mathfrak{a}_{\infty}}$ (in fact, its only nonzero diagonal is
the $m$-th one).

This defines an inclusion $\overline{\varphi_{\alpha,\beta}}:W\rightarrow
\overline{\mathfrak{a}_{\infty}}$. This lifts to a map $\widehat{W}%
\rightarrow\mathfrak{a}_{\infty}$, where $\widehat{W}$ is the central
extension of $W$ defined by $\alpha$. But what is $\widehat{W}$ ? Exercise
(homework):%
\[
\alpha\left(  L_{m},L_{n}\right)  =\delta_{n,-m}\left(  \dfrac{n^{3}-n}%
{12}c_{\beta}+2nh_{\alpha,\beta}\right)  ,
\]
where
\[
c_{\beta}=-12\beta^{2}+12\beta-2\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ h_{\alpha,\beta}=\dfrac{1}{2}\alpha\left(  \alpha
+2\beta-1\right)  .
\]
So we redefine $L_{0}$ to $\widehat{L_{0}}=L_{0}+h_{\alpha,\beta}K$. Then,
$L_{n}$ for $n\neq0$ and $\widehat{L_{0}}$ satisfy the usual Virasoro
relations with $C=c_{\beta}$. Thus, the map%
\begin{align*}
L_{n}  &  \mapsto L_{n},\\
L_{0}  &  \mapsto\widehat{L_{0}},\\
C  &  \mapsto c_{\beta}K
\end{align*}
is a homomorphism $\varphi_{\alpha,\beta}:\operatorname*{Vir}\rightarrow
\mathfrak{a}_{\infty}$, and $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is
a Virasoro module with central charge $c=c_{\beta}$ (via the map
$\varphi_{\alpha,\beta}$). This $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$
is called the \textit{module of semiinfinite forms}. The vector $\psi
_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ has highest degree (namely,
$0$).

We have $L_{i}\psi_{m}=0$ for $i>0$, and we have $L_{0}\psi_{m}=\dfrac{1}%
{2}\left(  \alpha-m\right)  \left(  \alpha+2\beta-1-m\right)  \psi_{m}$.
(Proof: Homework exercise.)
\end{proposition}

\begin{corollary}
We have a homomorphism%
\begin{align*}
M_{\lambda}  &  \rightarrow\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta},\\
v_{\lambda}  &  \mapsto\psi_{m}%
\end{align*}
of Virasoro modules, where%
\[
\lambda=\left(  \dfrac{1}{2}\left(  \alpha-m\right)  \left(  \alpha
+2\beta-1-m\right)  ,-12\beta^{2}+12\beta-2\right)  .
\]

\end{corollary}

We will see that this is an isomorphism for generic $\lambda$. For concrete
$\lambda$ it is not always one, and can have a rather complicated kernel.

What is the character of $\wedge^{\dfrac{\infty}{2},m}V$ ? For $d\geq0$, we
have%
\[
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  =\left\langle
v_{i_{0}}\wedge v_{i_{1}}\wedge...\ \mid\ d=\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  \right\rangle .
\]
This defines a grading on $\wedge^{\dfrac{\infty}{2},m}V$ by%
\[
\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\geq0}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  .
\]


If we set $\lambda_{k}=i_{k}+k-m$, then $\lambda=\left(  \lambda_{0}%
,\lambda_{1},\lambda_{2},...\right)  $ is a partition of $d$ (that is,
$\lambda_{0}\geq\lambda_{1}\geq\lambda_{2}\geq...$ and $\sum\limits_{k\geq
0}\lambda_{k}=d$).

Conversely, given a partition $\lambda$, set $i_{k}=\lambda_{k}-k+m$ and
obtain the monomial $v_{i_{0}}\wedge v_{i_{1}}\wedge...$.

This is a bijection between monomials and partitions. Hence:

\begin{proposition}
We have $\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right)  =p\left(  d\right)  $, so that%
\begin{align*}
\operatorname*{ch}\nolimits_{\wedge^{\dfrac{\infty}{2},m}V}:  &  =\sum_{d}%
\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]
\right)  q^{d}\\
&  =\sum_{d}p\left(  d\right)  q^{d}=\dfrac{1}{\left(  1-q\right)  \left(
1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}.
\end{align*}

\end{proposition}

\begin{proposition}
\label{prop.wedge.fock}As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}%
{2},m}V$ is isomorphic to the Fock module $F_{m}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.wedge.fock}.} We have $a_{i}\psi_{m}=0$
for all $i>0$ (by degree considerations). Also,%
\begin{align*}
a_{0}\psi_{m}  &  =\mathbf{1}\psi_{m}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\mathbf{1}\text{ is the identity matrix in }\mathfrak{a}_{\infty}\text{, but
it does not have to}\\
\text{act by }1\text{ (since it is a Lie algebra representation)}%
\end{array}
\right) \\
&  =\mathbf{1}\cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge
v_{1}\wedge v_{0}\wedge v_{-1}\wedge...\\
&  =\sum\limits_{i\in\mathbb{Z}}E_{i,i}\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}\wedge....
\end{align*}
To this sum, the $E_{i,i}$ with $i>m$ and the $E_{i,i}$ with $i\leq0$ do not
contribute. The only contributions come from $E_{i,i}$ with $m\geq i>0$. All
these contributions equal $1$, so $a_{0}\psi_{m}=m\psi_{m}$.

So (by a lemma that we have used many times: the lemma about Sing) we have a
homomorphism $\sigma_{m}:F_{m}\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ with
$\sigma_{m}\left(  1\right)  =\psi_{m}$. [We are using that $F_{m}$ is a Verma
module for $\mathcal{A}$.] This $\sigma_{m}$ is injective (since $F_{m}$ is
irreducible) and preserves grading. But $\operatorname*{ch}\nolimits_{F_{m}%
}\left(  q\right)  =\dfrac{1}{\left(  1-q\right)  \left(  1-q^{2}\right)
\left(  1-q^{3}\right)  \cdots}=\operatorname*{ch}\nolimits_{\wedge
^{\dfrac{\infty}{2},m}V}$, so $\sigma_{m}$ is a map between vector spaces of
the same dimension in each degree. Thus, $\sigma_{m}$ (being injective) must
be an isomorphism. Qed.

Note that Proposition \ref{prop.wedge.fock} is strange: it gives an
isomorphism between bosons (Fock module) and fermions (infinite wedge),
something unheard of in finite-dimensional contexts.

\begin{definition}
We write $\mathcal{B}^{\left(  m\right)  }=F_{m}$. We write $\mathcal{B}%
=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }$. We write
$\mathcal{F}^{\left(  m\right)  }=\wedge^{\dfrac{\infty}{2},m}V$. We write
$\mathcal{F}=\bigoplus\limits_{m}\mathcal{F}^{\left(  m\right)  }$.

We write $\sigma_{m}:\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m\right)  }$. We write $\sigma=\bigoplus\limits_{m}%
\mathcal{\sigma}_{m}:\mathcal{B}\rightarrow\mathcal{F}$ (isomorphism). This
$\sigma$ is called the \textit{Boson-Fermion Correspondence}.
\end{definition}

Note that we can do the same for the Virasoro algebra: If $M_{\lambda}$ is
irreducible, then the homomorphism $M_{\lambda}\rightarrow\wedge
^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is an isomorphism. And we know that
$\operatorname*{Vir}$ is nondegenerate, so $M_{\lambda}$ is irreducible for
Weil-generic $\lambda$.

\begin{corollary}
For generic $\alpha$ and $\beta$, the $\operatorname*{Vir}$-module
$\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is irreducible.
\end{corollary}

But now, back to Boson-Fermion Correspondence:

How to extend the action of $\mathcal{A}$ on $\mathcal{B}$ to an action of
$\mathfrak{a}_{\infty}$ explicitly? (We mean: How to describe the action of
$\mathcal{A}$ on $\mathcal{B}$ obtained by transporting the action of
$\mathcal{A}$ on $\mathcal{F}$ through $\sigma$ ?)

This question is answered using the so-called \textit{vertex operator
construction}.

But first, some easier things:

Wedging/contraction operators:

\textit{Wedging operators: }$\widehat{v_{i}}:\mathcal{F}^{\left(  m\right)
}\rightarrow\mathcal{F}^{\left(  m+1\right)  }$, $\widehat{v_{i}}\cdot
\psi=v_{i}\wedge\psi$. (We need to apply alternation to $v_{i}\wedge\psi$ to
make the indices decrease again.)

\textit{Contraction operators:} $\overset{\vee}{v_{i}}:\mathcal{F}^{\left(
m\right)  }\rightarrow\mathcal{F}^{\left(  m-1\right)  }$,%

\[
\overset{\vee}{v_{i}}\psi=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }\psi\text{ has no }v_{i};\\
\psi\text{ with excised }v_{i}\text{ and appropriate sign,}%
\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  .
\]
These operators satisfy the usual relations:%
\begin{align*}
\widehat{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\widehat{v_{i}}  &
=0,\ \ \ \ \ \ \ \ \ \ \overset{\vee}{v_{i}}\overset{\vee}{v_{j}%
}+\overset{\vee}{v_{j}}\overset{\vee}{v_{i}}=0,\\
\overset{\vee}{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\overset{\vee}{v_{i}}  &
=\delta_{i,j}.
\end{align*}


Let us call $\xi_{i}=\widehat{v_{i}}$ and $\xi_{i}^{\ast}=\overset{\vee
}{v_{i}}$. Then, $\rho\left(  E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}$ and
\[
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  .
\]


The $\xi_{i}$ and $\xi_{i}^{\ast}$ are called \textit{fermionic operators}.

So what are the $\xi_{i}$ in terms of $a_{j}$ ?

\subsection{The vertex operator construction}

We write $\mathbb{C}\left[  z,z^{-1},x_{1},x_{2},...\right]  =\bigoplus
\limits_{m}z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ for $\mathcal{B}%
=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }$, with $\mathcal{B}%
^{\left(  m\right)  }=z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $.

Note that we give the $x$'es degree $-1$.

Note also that $z$ is an iso of $\mathcal{A}_{0}$-modules, but not of
$\mathcal{A}$-modules.

The Boson-Fermion correspondence goes like this:%
\[
\mathcal{F}=\bigoplus\limits_{m}\mathcal{F}^{\left(  m\right)  }%
\overset{\sigma=\bigoplus\limits_{m}\sigma_{m}}{\leftarrow}\mathcal{B}%
=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }.
\]
On $\mathcal{F}$ there are operators $\widehat{v_{i}}=\xi_{i}$, $\overset{\vee
}{v_{i}}=\xi_{i}^{\ast}$, $\rho\left(  E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}%
$, $\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  $. We must find the corresponding operators on $\mathcal{B}$.

Introduce the quantum fields%
\begin{align*}
X\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}%
,\ \ \ \ \ \ \ \ \ \ X^{\ast}\left(  u\right)  =\sum\limits_{n\in\mathbb{Z}%
}\xi_{n}^{\ast}u^{-n},\\
\Gamma\left(  u\right)   &  =\sigma^{-1}\circ X\left(  u\right)  \circ
\sigma,\ \ \ \ \ \ \ \ \ \ \Gamma^{\ast}\left(  u\right)  =\sigma^{-1}\circ
X^{\ast}\left(  u\right)  \circ\sigma.
\end{align*}
Note that $\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{B}^{\left(  m+1\right)  }$ and $\Gamma^{\ast}\left(
u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow\mathcal{B}^{\left(
m-1\right)  }$, because this is how the $\xi_{n}$ and $\xi_{n}^{\ast}$ act.

\begin{theorem}
\label{thm.euler}We have%
\begin{align*}
\Gamma\left(  u\right)   &  =u^{m+1}z\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  ;\\
\Gamma^{\ast}\left(  u\right)   &  =u^{-m}z^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
Here, $\exp A$ means $1+A+\dfrac{A^{2}}{2!}+\dfrac{A^{3}}{3!}+...$ for any $A$
for which this series makes any sense.
\end{theorem}

Why do these formulas make sense?

Claim: For any $v\in\mathcal{B}^{\left(  m\right)  }$, we have $RHS\cdot
v\in\mathcal{B}^{\left(  m+1\text{ resp. }m-1\right)  }\left(  \left(
u\right)  \right)  $.

Indeed, $\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \left(
v\right)  \in B\left[  u^{-1}\right]  $ (since $v$ has degree $-d$ for some
$d$, and $a_{j}$ raise degree by $j$, so only finitely many terms remain), so
that $\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)
\in B\left(  \left(  u\right)  \right)  $.

\begin{remark}
Morally speaking, $\Gamma\left(  u\right)  $ is $uz:\exp\left(  \int a\left(
u\right)  du\right)  :$, and $\Gamma^{\ast}\left(  u\right)  $ is $z^{-1}%
:\exp\left(  -\int a\left(  u\right)  du\right)  :$. What does this mean?
$a\left(  u\right)  =\sum\limits_{m}a_{m}u^{-m-1}$, thus $\int a\left(
u\right)  du=\sum\limits_{m\neq0}\dfrac{a_{m}}{m}u^{-m}+a_{0}\log u$.
Exponentiating this \textbf{in the normal ordering}, we get%
\begin{align*}
&  \underbrace{\exp\left(  a_{0}\log u\right)  }_{\substack{=u^{a_{0}}%
=u^{m}\\\text{(since }a_{0}\text{ acts by }0\text{ on }\mathcal{B}^{\left(
m\right)  }\text{)}}}\cdot\exp\left(  \sum\limits_{m<0}\dfrac{a_{m}}{m}%
u^{-m}\right)  \cdot\exp\left(  \sum\limits_{m>0}\dfrac{a_{m}}{m}u^{-m}\right)
\\
&  =u^{m}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
This is reminiscent of Euler's formula $y=c\exp\left(  \int a\left(  u\right)
du\right)  $ for the solution $y$ of the differential equation $y^{\prime}=ay$.
\end{remark}

Before we can show Theorem \ref{thm.euler}, let us prove a lemma:

\begin{lemma}
We have $\left[  a_{j},\Gamma\left(  u\right)  \right]  =u^{j}\Gamma\left(
u\right)  $ and $\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]
=u^{j}\Gamma^{\ast}\left(  u\right)  $ (or maybe $u^{-j}\Gamma^{\ast}\left(
u\right)  $).
\end{lemma}

\textit{Proof.} We will only prove the first formula (the second one is analogous).

Under the BFC, $a_{j}$ corresponds to $T^{j}=\sum\limits_{i}:\xi_{i}\xi
_{i+j}^{\ast}:$.

In the fermionic setting,%
\begin{align*}
\left[  T^{j},X\left(  u\right)  \right]   &  =\left[  \sum\limits_{i}:\xi
_{i}\xi_{i+j}^{\ast}:\ ,\ \sum\limits_{m}\xi_{m}u^{m}\right] \\
&  =\sum\limits_{m}\xi_{m-j}u^{m}=u^{j}\underbrace{\sum\limits_{m}\xi
_{m-j}u^{m-j}}_{=X\left(  u\right)  }=u^{j}X\left(  u\right)  .
\end{align*}
Now, conjugate by $\sigma$.

Lemma proven.

\textit{Proof of Theorem \ref{thm.euler}.} Define $\Gamma_{+}\left(  u\right)
=\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $. Then,%
\begin{align*}
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &  =u^{i}\Gamma_{+}\left(
u\right)  \ \ \ \ \ \ \ \ \ \ \text{if }i<0.
\end{align*}
To check the second of these two equalities (the first is trivial), notice
that
\begin{align*}
\left[  a_{i},\exp\left(  -\dfrac{a_{-i}}{-i}u^{i}\right)  \right]   &
=u^{i}\exp\left(  -\dfrac{a_{-i}}{-i}u^{i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{using }\left[  a_{i},a_{-i}\right]
=i\right)  .
\end{align*}
Now consider $\Delta\left(  u\right)  :=\Gamma\left(  u\right)  \Gamma
_{+}\left(  u\right)  ^{-1}z^{-1}$. Since $\left[  a_{0},z\right]  =z$ and
$\left[  a_{i},z\right]  =0$ for $i\neq0$, from the above we get%
\[
\left[  a_{i},\Delta\left(  u\right)  \right]  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\leq0;\\
u^{i}\Delta\left(  u\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }i>0
\end{array}
\right.
\]
(using the standard formula $\left[  a,b^{-1}\right]  =-b^{-1}\left[
a,b\right]  b^{-1}$). In particular, $\left[  a_{i},\Delta\left(  u\right)
\right]  =0$ if $i\leq0$. Thus, $\Delta\left(  u\right)  $ is completely
determined by $\Delta\left(  u\right)  v_{m}$. Let us compute $\Delta\left(
u\right)  v_{m}$. This is a series in $u$ whose coefficients are polynomials
in $x_{1},x_{2},x_{3},...$. Denote this series by $Q$.

We will use the representation%
\begin{align*}
a_{-i}  &  \mapsto ix_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i>0;\\
a_{i}  &  \mapsto\dfrac{\partial}{\partial x_{i}}\ \ \ \ \ \ \ \ \ \ \text{for
}i>0,\\
K  &  \mapsto1
\end{align*}
of $\mathcal{A}_{0}$.

If $i>0$, then $a_{i}\Delta\left(  u\right)  v_{m}=u^{i}\Delta\left(
u\right)  v_{m}$. In other words, $\dfrac{\partial Q}{\partial x_{i}}=u^{i}Q$.
So we get $Q\left(  u,x_{1},x_{2},x_{3},...\right)  =f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  $ for some Laurent series
$f\left(  u\right)  \in\mathbb{C}\left(  \left(  u\right)  \right)  $. Thus,
$\Delta\left(  u\right)  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  $. Thus, $\Gamma\left(  u\right)  =f\left(
u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $. It
remains to show that $f\left(  u\right)  =u^{m+1}$.

In the fermionic space, with $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}%
\wedge...$, we have%
\[
\left\langle \psi_{m+1}^{\ast},X\left(  u\right)  \psi_{m}\right\rangle
=\left\langle \psi_{m+1}^{\ast},\sum\limits_{i}\widehat{v_{i}}u^{i}\psi
_{m}\right\rangle =u^{m+1}%
\]
but%
\[
\left\langle \psi_{m+1}^{\ast},X\left(  u\right)  \psi_{m}\right\rangle
=\left\langle v_{m+1}^{\ast},\Gamma\left(  u\right)  v_{m}\right\rangle
=f\left(  u\right)  .
\]


\begin{corollary}
We have%
\[
\rho\left(  \sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)  =\sum\limits_{i}%
u^{i}v^{-j}\xi_{i}\xi_{j}^{\ast}=X\left(  u\right)  X^{\ast}\left(  v\right)
,
\]
thus%
\begin{align*}
\sigma^{-1}\circ\rho\left(  \sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)
\circ\sigma &  =\sigma^{-1}\circ X\left(  u\right)  X^{\ast}\left(  v\right)
\circ\sigma=\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}

\end{corollary}

\textit{Proof.}
\begin{align*}
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)   &  =u^{m+1}%
v^{-m-1}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}%
{j}a_{-j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
.
\end{align*}
We need to reorder the second and the third exponential on the right hand side
of this. We can reorder them using the following fact: When $\left[
A,B\right]  =C$ and $\left[  C,A\right]  =0$ and $\left[  C,B\right]  =0$,
then $\exp A\cdot\exp B=\exp B\cdot\exp A\cdot\exp C$. (This is an exercise.)
Thus, this becomes%
\begin{align*}
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)   &  =u^{m+1}v^{-m}%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  \cdot\exp\left(  \left[  \sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j},\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right]  \right)  \exp\left(
\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)  .
\end{align*}
But%
\begin{align*}
\exp\left(  \left[  \sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j},\sum
\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right]  \right)   &  =\exp\left(
\sum\limits_{j>0}\dfrac{1}{j}\left(  \dfrac{v}{u}\right)  ^{j}\right)
=\exp\left(  -\log\left(  1-\dfrac{v}{u}\right)  \right) \\
&  =\dfrac{1}{1-\dfrac{v}{u}},
\end{align*}
which is what gives the $\dfrac{1}{1-\dfrac{v}{u}}$ term.

Next question: What is $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  $ ? This turns out to be a Schur polynomial.

We first define elementary Schur polynomials:

\begin{definition}
We define $S_{k}\in\mathbb{Q}\left[  x_{1},x_{2},x_{3},...\right]  $ by%
\[
\sum\limits_{k\geq0}S_{k}\left(  x\right)  z^{k}=\exp\left(  \sum
\limits_{j\geq1}x_{i}z^{i}\right)  .
\]

\end{definition}

For example, $S_{0}\left(  x\right)  =1$, $S_{1}\left(  x\right)  =x_{1}$,
$S_{2}\left(  x\right)  =\dfrac{x_{1}^{2}}{2}+x_{2}$, $...$.

Note that these are not symmetric polynomials. These are the representations
of complete symmetric functions in terms of the $\dfrac{p_{i}}{i}$.

Complete symmetric functions: $h_{k}\left(  y_{1},y_{2},...,y_{N}\right)
=\sum\limits_{\substack{p_{1},p_{2},...,p_{N}\in\mathbb{N};\\p_{1}%
+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}$.

\begin{proposition}
We have%
\[
\sum\limits_{k\geq0}z^{k}h_{k}\left(  y\right)  =\prod\limits_{j=1}^{N}%
\dfrac{1}{1-zy_{j}}.
\]

\end{proposition}

\begin{proposition}
\label{prop.h_k.as.schur}If $x_{j}=\dfrac{y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}%
}{j}$, then $h_{k}\left(  y\right)  =S_{k}\left(  x\right)  $.
\end{proposition}

\textit{Proof.}
\begin{align*}
\sum\limits_{k\geq0}S_{k}\left(  x\right)  z^{k}  &  =\exp\left(
\sum\limits_{j\geq1}x_{i}z^{i}\right)  =\exp\left(  \sum\limits_{j\geq1}%
\dfrac{y_{1}^{i}+y_{2}^{i}+...+y_{N}^{i}}{i}z^{i}\right) \\
&  =\prod\limits_{j}\exp\left(  \sum\limits_{i\geq1}\dfrac{y_{j}^{i}z^{i}}%
{i}\right)  =\prod\limits_{j}\exp\left(  -\log\left(  1-y_{j}z\right)  \right)
\\
&  =\prod\limits_{j}\dfrac{1}{1-y_{j}z}=\sum\limits_{k\geq0}z^{k}h_{k}\left(
y\right)  .
\end{align*}


\begin{definition}
Let $\lambda=\left(  \lambda_{1},\lambda_{2},...,\lambda_{m}\right)  $ be a
partition, so that $\lambda_{1}\geq\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers.

We define $S_{\lambda}\left(  x\right)  \in\mathbb{Q}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to be the polynomial%
\[
\det\left(
\begin{array}
[c]{ccccc}%
s_{\lambda_{1}}\left(  x\right)  & s_{\lambda_{1}+1}\left(  x\right)  &
s_{\lambda_{1}+2}\left(  x\right)  & ... & s_{\lambda_{1}+m-1}\left(  x\right)
\\
s_{\lambda_{2}-1}\left(  x\right)  & s_{\lambda_{2}}\left(  x\right)  &
s_{\lambda_{2}+1}\left(  x\right)  & ... & s_{\lambda_{2}+m-2}\left(  x\right)
\\
s_{\lambda_{3}-2}\left(  x\right)  & s_{\lambda_{3}-1}\left(  x\right)  &
s_{\lambda_{3}}\left(  x\right)  & ... & s_{\lambda_{3}+m-3}\left(  x\right)
\\
... & ... & ... & ... & ...\\
s_{\lambda_{m}-m+1}\left(  x\right)  & s_{\lambda_{m}-m+2}\left(  x\right)  &
s_{\lambda_{m}-m+3}\left(  x\right)  & ... & s_{\lambda_{m}}\left(  x\right)
\end{array}
\right)  ,
\]
where $s_{j}$ denotes $0$ if $j<0$. (Note that this does not depend on
trailing zeroes in the partition; in other words, $S_{\left(  \lambda
_{1},\lambda_{2},...,\lambda_{m}\right)  }\left(  x\right)  =S_{\left(
\lambda_{1},\lambda_{2},...,\lambda_{m},0,0,...,0\right)  }\left(  x\right)  $
for any number of zeroes.)

If $N\geq m$, and we denote by $\Lambda$ the $N$-tuple $\left(  \lambda
_{1},\lambda_{2},...,\lambda_{m},0,0,...,0\right)  $, then we have an
irreducible representation $V_{\Lambda}$ of $\mathfrak{gl}\left(  N\right)  $
and of $\operatorname*{GL}\left(  N\right)  $. Then, a diagonal matrix
$Y=\operatorname*{diag}\left(  y_{1},y_{2},...,y_{N}\right)  \in
\operatorname*{GL}\left(  N\right)  $ has character $\chi_{\Lambda}\left(
y_{1},y_{2},...,y_{N}\right)  =\operatorname*{Tr}\mid_{V_{\lambda}}\left(
Y\right)  $.
\end{definition}

\begin{proposition}
We have $\chi_{\Lambda}\left(  y_{1},y_{2},...,y_{N}\right)  =S_{\lambda
}\left(  x\right)  $ where $x_{j}=\dfrac{y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}}%
{j}$.
\end{proposition}

This generalizes Proposition \ref{prop.h_k.as.schur} (in fact, set
$\lambda=\left(  k\right)  $ and notice that $V_{\Lambda}=S^{k}\mathbb{C}^{N}$).

\begin{theorem}
\label{thm.schur}Whenever $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\mathcal{F}^{\left(  0\right)  }$ (with $i_{0}>i_{1}>i_{2}>...$),
we have $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =S_{\lambda}\left(  x\right)  $ where $\lambda=\left(
i_{0},i_{1}+1,i_{2}+2,...\right)  $ (this $\lambda$ is indeed a partition
since $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\in\mathcal{F}%
^{\left(  0\right)  }$).
\end{theorem}

Before we give a proof of this theorem, let us lift the $\mathfrak{gl}%
_{\infty}$-action on $\mathcal{F}$ to $\operatorname*{GL}\left(
\infty\right)  $. But what is $\operatorname*{GL}\left(  \infty\right)  $ ?

\begin{definition}
We let $\operatorname*{GL}\left(  \infty\right)  $ denote the set
$\operatorname*{id}+\mathfrak{gl}_{\infty}$. In other words,
$\operatorname*{GL}\left(  \infty\right)  $ is the set of all infinite
matrices (infinite in both directions) which are equal to the infinite
identity matrix $\operatorname*{id}$ in all but finitely many entries.
\end{definition}

We know that $\mathfrak{gl}_{\infty}$ acts on $\mathcal{F}$ by representation
$\rho$.

We now define an action $\rho$ (we call it $\rho$ again) of
$\operatorname*{GL}\left(  \infty\right)  $ on $\mathcal{F}$ as follows:

For $A\in\operatorname*{GL}\left(  \infty\right)  $, let $A\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =Av_{i_{0}}\wedge
Av_{i_{1}}\wedge Av_{i_{2}}\wedge...$. This, when expanded, gives a sum, but
only a finite sum, since $Av_{k}=v_{k}$ for sufficiently small $k$.

Let us write this action as a matrix: Write%
\[
\rho\left(  A\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum C\left(  A\right)  _{i_{0},i_{1},i_{2},...}%
^{j_{0},j_{1},j_{2},...}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\]
Then, $C\left(  A\right)  _{i_{0},i_{1},i_{2},...}^{j_{0},j_{1},j_{2}%
,...}=\det\left(  A_{i_{0},i_{1},i_{2},...}^{j_{0},j_{1},j_{2},...}\right)  $.
Here, $A_{i_{0},i_{1},i_{2},...}^{j_{0},j_{1},j_{2},...}$ denotes the matrix
which consists of the $i_{0}$-th, the $i_{1}$-th, the $i_{2}$-th etc. rows and
of the $j_{0}$-th, the $j_{1}$-th, the $j_{2}$-th etc. columns of $A$. This
matrix $A_{i_{0},i_{1},i_{2},...}^{j_{0},j_{1},j_{2},...}$ is semi-infinite in
the sense that it is an identity matrix up to finitely many entries; hence,
the determinant $\det\left(  A_{i_{0},i_{1},i_{2},...}^{j_{0},j_{1},j_{2}%
,...}\right)  $ makes sense.

We are now ready to prove Theorem \ref{thm.schur}.

\textit{Proof of Theorem \ref{thm.schur}.} Let us denote by $\mathbf{1}$ the
unity of $\mathcal{B}^{0}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $.

Fix a sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ with $i_{0}%
>i_{1}>i_{2}>...$. Denote $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  $ by $P\left(  x\right)  $, where $x$ denotes the
whole collection of variables $x_{1},x_{2},x_{3},...$. We need to show that
$P\left(  x\right)  =S_{\lambda}\left(  x\right)  $.

It suffices to show that $\left(  \mathbf{1},\exp\left(  y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+...\right)  P\left(  x\right)  \right)  =\left(
\mathbf{1},\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
S_{\lambda}\left(  x\right)  \right)  $, where $\left(  \cdot,\cdot\right)  $
denotes the $\mathcal{A}$-invariant bilinear form on the Fock space (as a
Verma module) (here we use the involution on $\mathcal{A}$, else this form
would be on two different spaces), and where $y$ are new variables. In fact,
this will show that $\left(  \mathbf{1},a_{1}^{n_{1}}a_{2}^{n_{2}}...P\left(
x\right)  \right)  =\left(  \mathbf{1},a_{1}^{n_{1}}a_{2}^{n_{2}}%
...S_{\lambda}\left(  x\right)  \right)  $, which will rewrite as $\left(
a_{-1}^{n_{1}}a_{-2}^{n_{2}}...\mathbf{1},P\left(  x\right)  \right)  =\left(
a_{-1}^{n_{1}}a_{-2}^{n_{2}}...\mathbf{1},S_{\lambda}\left(  x\right)
\right)  $, and $\left(  a_{-1}^{n_{1}}a_{-2}^{n_{2}}...\mathbf{1}\right)
_{\left(  n_{1},n_{2},...\right)  }$ is a basis and the form is nondegenerate
(because the Fock module is an irreducible Verma module, or alternatively for
the reason that $\left(  a_{-1}^{n_{1}}a_{-2}^{n_{2}}...\mathbf{1}%
,a_{-1}^{m_{1}}a_{-2}^{m_{2}}...\mathbf{1}\right)  =\delta_{n_{1},m_{1}}%
\delta_{n_{2},m_{2}}...\prod\limits_{i}i^{n_{i}}\prod\limits_{i}n_{i}!$).

Let us now compute $\left(  \mathbf{1},\exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  P\left(  x\right)  \right)  $. In fact, every
polynomial $R$ satisfies%
\begin{align*}
&  \left(  \mathbf{1},\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
R\left(  x\right)  \right) \\
&  =\left(  \mathbf{1},\exp\left(  y_{1}\dfrac{\partial}{\partial x_{1}}%
+y_{2}\dfrac{\partial}{\partial x_{2}}+y_{3}\dfrac{\partial}{\partial x_{3}%
}+...\right)  R\left(  x\right)  \right) \\
&  =\left(  \mathbf{1},R\left(  x+y\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\exp\left(  y_{1}\dfrac{\partial}{\partial x_{1}}+y_{2}%
\dfrac{\partial}{\partial x_{2}}+y_{3}\dfrac{\partial}{\partial x_{3}%
}+...\right)  R\left(  x\right)  =R\left(  x+y\right) \\
\text{by a multivariate Taylor formula}%
\end{array}
\right) \\
&  =R\left(  x+y\right)  \mid_{x=0}=R\left(  y\right)  .
\end{align*}
Thus, $\left(  \mathbf{1},\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  S_{\lambda}\left(  x\right)  \right)  =S_{\lambda}\left(
y\right)  $.

So our job is to show that $\left(  \mathbf{1},\exp\left(  y_{1}a_{1}%
+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(  x\right)  \right)  =S_{\lambda
}\left(  y\right)  $.

Now consider the matrix $T$ which has $1$'s on the diagonal right above the
main one, and $0$'s everywhere else. Then, $a_{i}=T^{i}$ (this is how we
embedded the Heisenberg algebra in $\overline{\mathfrak{a}_{\infty}}$).

Thus,%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right) \\
&  =\exp\left(  y_{1}T^{1}+y_{2}T^{2}+y_{3}T^{3}+...\right)  =\sum
\limits_{k\in\mathbb{N}}s_{k}\left(  y\right)  T^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\exp\left(  y_{1}z^{1}+y_{2}z^{2}+y_{3}z^{3}+...\right)
=\sum\limits_{k\in\mathbb{N}}s_{\lambda}\left(  y\right)  T^{k}\\
\text{as a generating function identity}%
\end{array}
\right) \\
&  =\left(  s_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}=:A.
\end{align*}
Modulo some cheating, we can view $A$ as an element of $\operatorname*{GL}%
\left(  \infty\right)  $ (actually, not really, but of some completion of
$\operatorname*{GL}\left(  \infty\right)  $ over the ring $\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  $).

Now we go to $\mathcal{F}$ (by means of $\sigma$), and find%
\begin{align*}
&  \left(  \mathbf{1},\underbrace{\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  }_{=A}P\left(  x\right)  \right) \\
&  =\left(  \mathbf{1},AP\left(  x\right)  \right)  =\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...,A\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the minor of }A\text{ which is exactly the same determinant}\\
\text{that was used to define }S_{\lambda}\left(  y\right)
\end{array}
\right)  =S_{\lambda}\left(  y\right)  .
\end{align*}
Theorem \ref{thm.schur} is proven.

\subsection{Applications to integrable systems}

Let us show how these things can be applied.

\textbf{Korteweg-de Vries equation:} It is the equation $u_{t}=\dfrac{3}%
{2}uu_{x}-\dfrac{1}{4}u_{xxx}$ on a function $u\left(  t,x\right)  $.

We will discuss several interesting solutions of this equation. Here is the
most basic family of solutions:%
\[
u\left(  t\right)  =\dfrac{2a^{2}}{\cosh^{2}\left(  a\left(  x+a^{2}t\right)
\right)  }.
\]
These are so-called "traveling wave solutions". It is a peculiar kind of wave:
it has only one bump; it is therefore called a \textit{soliton} (or
\textit{solitary wave}). Such waves never occur in linear systems. Note that
when we speak of "wave", we are imagining a graph with the x-axis showing $t$
and the y-axis showing $u\left(  t\right)  $. The $x$ is the "time" parameter,
so when we speak of "traveling wave", we mean that it "travels" when $x$ moves.

These kind of waves were invented by J. S. Russell in 1834, describing the
motion of water in a shallow canal (tsunami waves are similar). The first
models for these waves were found by Korteweg-de Vries in 1895.

The term $-\dfrac{1}{4}u_{xxx}$ in the Korteweg-de Vries equation
$u_{t}=\dfrac{3}{2}uu_{x}-\dfrac{1}{4}u_{xxx}$ is called the
\textit{dispersion term}.

\textbf{Exercise:} Solve the equation $u_{t}=\dfrac{3}{2}uu_{x}$. (Note that
these waves develop shocks, in contrast to Korteweg-de Vries equation.)

The Korteweg-de Vries equation is famous for having lots of explicit solutions
(unexpectedly for a nonlinear partial differential equation). We will
construct some of them using infinite-dimensional Lie algebras. (There are
many other ways to construct solutions. In some sense, all of mathematics is
related to its solutions.)

\textbf{Generalization:} The \textbf{Kadomtsev-Petviashvili equation}
$u_{yy}=\left(  u_{t}-\dfrac{3}{2}uu_{x}-\dfrac{1}{4}u_{xxx}\right)  _{x}$
(or, after some rescaling, $\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(
\partial_{t}u-\dfrac{3}{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)
$) on a function $u\left(  t,x,y\right)  $.

We will also find some solutions to this equation. (Abbreviated as KP equation.)

We are going to use the \textit{infinite Grassmannian} for this. First, recall
what the \textit{finite Grassmannian} is:

Let $V$ be the $\mathbb{C}$-vector space $\mathbb{C}^{n}$. Recall that
$\wedge^{k}V$ is a representation of $\operatorname*{GL}\left(  V\right)  $
with a highest weight vector $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Denote
by $\Omega$ the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $.

\begin{proposition}
We have $\Omega=\left\{  x\in\wedge^{k}V\text{ nonzero}\ \mid\ x=x_{1}\wedge
x_{2}\wedge...\wedge x_{k}\text{ for some }x_{i}\in V\right\}  $. Also,
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\neq0$ if and only if $x_{1}$, $x_{2}$,
$...$, $x_{k}$ are linearly independent.
\end{proposition}

\textit{Proof.} Very easy.

Now define the Pl\"{u}cker embedding%
\begin{align*}
\operatorname*{Pl}:\operatorname*{Gr}\left(  k,V\right)   &  \rightarrow
\mathbb{P}\left(  \wedge^{k}V\right)  ,\\
\left(
\begin{array}
[c]{c}%
k\text{-dimensional subspace of }V\\
\text{with basis }x_{1},x_{2},...,x_{k}%
\end{array}
\right)   &  \mapsto\left(
\begin{array}
[c]{c}%
\text{projection of }x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge
^{k}V\diagdown\left\{  0\right\} \\
\text{on }\mathbb{P}\left(  \wedge^{k}V\right)
\end{array}
\right)  .
\end{align*}
It is easy to see that this does not depend on the choice of basis
$x_{1},x_{2},...,x_{k}$. It is called the \textit{Pl\"{u}cker embedding}. Its
image is $\operatorname*{Im}\operatorname*{Pl}=\Omega\diagup\left(
\text{scalars}\right)  $.

\begin{proposition}
This map $\operatorname*{Pl}$ is injective.
\end{proposition}

This is an exercise.

Thus, $\operatorname*{Gr}\left(  k,V\right)  \cong\Omega\diagup\left(
\text{scalars}\right)  $. (For algebraic geometers: $\Omega$ is the total
space of the determinant bundle on $\operatorname*{Gr}\left(  k,V\right)  $
(but only the nonzero elements).)

We are now going to describe the image $\operatorname*{Im}\operatorname*{Pl}$
by algebraic equations. These equations go under the name \textit{Pl\"{u}cker
relations}.

Define $\widehat{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ and
$\overset{\vee}{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k-1}V$.

\begin{theorem}
\label{thm.plu}Let $S=\sum\limits_{i}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}:\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes
\wedge^{k-1}V$. This does not depend on the choice of a basis and is
$\operatorname*{GL}\left(  V\right)  $-invariant.

Now, a nonzero element $\tau\in\wedge^{k}V$ belongs to $\Omega$ if and only if
$S\left(  \tau\otimes\tau\right)  =0$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.plu}.}

\textbf{1)} First let us show that if $\tau\in\Omega$, then $S\left(
\tau\otimes\tau\right)  =0$.

In order to show this, it is enough to prove that $S\left(  \tau\otimes
\tau\right)  =0$ holds for $\tau=v_{1}\wedge v_{2}\wedge...\wedge v_{k}$
(since $S$ is $\operatorname*{GL}\left(  V\right)  $-invariant).

But this is obvious, because for every $i$, either $\widehat{v_{i}}$ or
$\overset{\vee}{v_{i}}$ kills $\tau$.

\textbf{2)} Let us now prove that if $S\left(  \tau\otimes\tau\right)  =0$,
then $\tau\in\Omega$.

(There is a combinatorial proof of this in the infinite setting in the
Kac-Raina book, but we will make a different proof here.)

Define $E\subseteq V$ to be the set $\left\{  v\in V\ \mid\ v\wedge
\tau=0\right\}  $. Define $E^{\prime}\subseteq V^{\ast}$ to be the set
$\left\{  f\in V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  $ (where
$\overset{\vee}{f}$ is the contraction with $f$; this is the same as $i_{f}$).

We know that $\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}=0$ for
all $v\in E$ and $f\in E^{\prime}$. But $\left(  \overset{\vee}{f}%
\widehat{v}+\widehat{v}\overset{\vee}{f}\right)  \tau=f\left(  v\right)  \tau
$, so this yields $f\left(  v\right)  =0$. Thus, $E\subseteq E^{\prime\perp}$.

Let $m=\dim E$ and $r=\dim E^{\prime\perp}$. Pick a basis $\left(
e_{i}\right)  $ of $V$ compatible with these subspaces.

Since $S$ did not depend on the basis,%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =\sum\limits_{i=1}^{m}%
\underbrace{\widehat{e_{i}}\tau}_{=0}\otimes\overset{\vee}{e_{i}^{\ast}}%
\tau+\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee
}{e_{i}^{\ast}}\tau+\sum\limits_{i=r+1}^{n}\widehat{e_{i}}\tau\otimes
\underbrace{\overset{\vee}{e_{i}^{\ast}}\tau}_{\substack{=0\\\text{(since
}i>r\text{ and thus }\\e_{i}\in E^{\prime}\text{ or something like that)}}}\\
&  =\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee
}{e_{i}^{\ast}}\tau.
\end{align*}
Thus, $S\left(  \tau\otimes\tau\right)  =0$ becomes $\sum\limits_{i=m+1}%
^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}}\tau=0$. But
$\widehat{e_{i}}\tau$ are linearly independent. Hence, $\overset{\vee
}{e_{i}^{\ast}}\tau=0$ for any $i\in\left\{  m+1,m+2,...,r\right\}  $. But
this can only happen if there are no such $i$. Thus, $m=r$. Hence, $E\subseteq
E^{\prime\perp}$ yields $E=E^{\prime\perp}$.

Since $\widehat{e_{i}}\tau=0$ for $i\leq m$ and since $\overset{\vee
}{e_{i}^{\ast}}\tau=0$ for $i>m$, we have $\tau=c\cdot e_{1}\wedge e_{2}%
\wedge...\wedge e_{m}$, and thus $m=k$. Proof complete.

We can rewrite Theorem \ref{thm.plu} in coordinates:

Let $E\subseteq\mathbb{C}^{n}$ have a basis $\left(  x_{1},x_{2}%
,...,x_{k}\right)  =\left(
\begin{array}
[c]{cccc}%
x_{11} &  & ... & x_{1k}\\
... &  &  & ...\\
&  &  & \\
x_{n1} &  & ... & x_{nk}%
\end{array}
\right)  $.

For every $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k$, let $P_{I}$ be the corresponding minor of this matrix (the
one with rows ...).

\textbf{Exercise:} Show that $S\left(  \tau\otimes\tau\right)  =0$ is
equivalent to%
\begin{align*}
&  \text{for all }I,J\subseteq\left\{  1,2,...,n\right\}  \text{ with
}\left\vert I\right\vert =k-1\text{ and }\left\vert J\right\vert
=k+1\text{,}\\
&  \text{we have }\sum_{j\in J;\ j\notin I}P_{I\cup\left\{  j\right\}
}P_{J\diagdown\left\{  j\right\}  }\left(  -1\right)  ^{\nu\left(  j\right)
}=0,
\end{align*}
where $\nu\left(  j\right)  $ is defined in such a way that $j$ is the
$\nu\left(  j\right)  $-th element of $J$.

\textit{Example:} If $n=4$ and $k=2$, then $P_{12}P_{34}+P_{14}P_{23}%
-P_{13}P_{24}=0$ (where $ij$ stands for $\left\{  i,j\right\}  $).

Now to the infinite Grassmanian:

$\mathcal{F}^{\left(  0\right)  }\supseteq\psi_{0}=v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...$.

\begin{definition}
$\Omega=\operatorname*{GL}\left(  \infty\right)  \psi_{0}$.
\end{definition}

\begin{proposition}
For all $\left(  i_{0},i_{1},i_{2},...\right)  $ such that $i_{k}+k=0$ for
large enough $k$, we have $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\Omega$.
\end{proposition}

\textit{Proof.} There exists a permutation $\sigma:\mathbb{Z}\rightarrow
\mathbb{Z}$ which moves only finitely many numbers (so $\sigma\in
\operatorname*{GL}\left(  \infty\right)  $) and $i_{k}=\sigma\left(
-k\right)  $. Thus, $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...=\sigma\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  =\sigma
\psi_{0}\in\operatorname*{GL}\left(  \infty\right)  \psi_{0}=\Omega$.

\begin{theorem}
Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be nonzero. Then, $\tau\in
\Omega$ if and only if $S\left(  \tau\otimes\tau\right)  =0$, where
$S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}%
}^{\ast}:\mathcal{F}^{\left(  0\right)  }\otimes\mathcal{F}^{\left(  0\right)
}\rightarrow\mathcal{F}^{\left(  1\right)  }\otimes\mathcal{F}^{\left(
-1\right)  }$.
\end{theorem}

[Maybe in Theorem \ref{thm.plu}, the $\overset{\vee}{v_{i}}$ should be
$\overset{\vee}{v_{i}}^{\ast}$.]

Denote $\Omega\diagup\mathbb{C}^{\times}$ by $\operatorname*{Gr}$; this is
called the \textit{semiinfinite Grassmannian}.

Think of the space $V$ as $\mathbb{C}\left[  t,t^{-1}\right]  $ (by
identifying $v_{i}$ with $t^{-i}$). Then, $\left\langle v_{0},v_{-1}%
,v_{-2},...\right\rangle =\mathbb{C}\left[  t\right]  $.

\textbf{Exercise:} Then, $\operatorname*{Gr}$ is the set%
\[
\left\{  E\subseteq V\ \mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  t\right]  \text{ for sufficiently large
}N,\\
\text{and }\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)
=N\text{ for sufficiently large }N
\end{array}
\right)  \right\}  .
\]
(Note that when the relations $E\supseteq t^{N}\mathbb{C}\left[  t\right]  $
and $\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N$ hold
for \textit{some} $N$, it is easy to see that they also hold for all greater
$N$.)

We can also replace $\mathbb{C}\left[  t,t^{-1}\right]  $ with $\mathbb{C}%
\left(  \left(  t\right)  \right)  $ (the formal Laurent series), and then
\[
\operatorname*{Gr}=\left\{  E\subseteq V\ \mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ for
sufficiently large }N,\\
\text{and }\dim\left(  E\diagup t^{N}\mathbb{C}\left[  \left[  t\right]
\right]  \right)  =N\text{ for sufficiently large }N
\end{array}
\right)  \right\}  .
\]


For any $E\in\operatorname*{Gr}$, there exists some $N\in\mathbb{N}$ such that
$t^{N}\mathbb{C}\left[  t\right]  \subseteq E\subseteq t^{-N}\mathbb{C}\left[
t\right]  $, so that the quotient $E\diagup t^{N}\mathbb{C}\left[  t\right]
\subseteq t^{-N}\mathbb{C}\left[  t\right]  \diagup t^{N}\mathbb{C}\left[
t\right]  \cong\mathbb{C}^{2N}$.

Thus, $\operatorname*{Gr}=\bigcup\limits_{N\geq1}\operatorname*{Gr}\left(
N,2N\right)  $ (a nested union). (By a variation of this construction,
$\operatorname*{Gr}=\bigcup\limits_{N\geq1}\bigcup\limits_{M\geq
1}\operatorname*{Gr}\left(  N,N+M\right)  $.)

Now, how do we actually use these things to solve KP equations and other
integral systems?

Recall the fermionic sums $X\left(  u\right)  =\sum\limits_{i}\xi_{i}u^{i}$
and $X^{\ast}\left(  u\right)  =\sum\limits_{i}\xi_{i}^{\ast}u^{-i}$.

We can rewrite $S\left(  \tau\otimes\tau\right)  =0$ as
\[
\left(  \text{constant term of }X\left(  u\right)  \tau\otimes X^{\ast}\left(
u\right)  \tau\right)  =0.
\]
So in bosonic space, we get
\[
\left(  \text{constant term of }\Gamma\left(  u\right)  \tau\otimes
\Gamma^{\ast}\left(  u\right)  \tau\right)  =0.
\]
Now, $\tau\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  =F=\mathcal{B}%
^{\left(  0\right)  }$. Implement $\mathcal{B}^{\left(  0\right)  }%
\otimes\mathcal{B}^{\left(  0\right)  }$ as $\mathbb{C}\left[  x_{1}^{\prime
},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime
},x_{3}^{\prime\prime},...\right]  $.

We have\footnote{In the following, $\sum\limits_{j}$ will always mean
$\sum\limits_{j\geq1}$.}%
\begin{align*}
&  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\\
&  =u\exp\left(  \sum\limits_{j}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  \exp\left(  -\sum\limits_{j}\dfrac{u^{-j}}%
{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  \tau\left(  x^{\prime}\right)
\tau\left(  x^{\prime\prime}\right)  .
\end{align*}
Make the substitution $\left\{
\begin{array}
[c]{c}%
x^{\prime}=x-y;\\
x^{\prime\prime}=x+y
\end{array}
\right.  $ (then, $x^{\prime}-x^{\prime\prime}=-2y$ and $\partial_{x^{\prime}%
}-\partial_{x^{\prime\prime}}=-\partial_{y}$). Then, this becomes%
\[
=u\exp\left(  -2\sum\limits_{j}u^{j}y_{j}\right)  \exp\left(  -\sum
\limits_{j}\dfrac{u^{-j}}{j}\partial_{y_{j}}\right)  \tau\left(  x-y\right)
\tau\left(  x+y\right)  .
\]


\textbf{Notation:} For three polynomials $P,f,g$, define $A\left(
P,f,g\right)  \left(  x\right)  =\left(  P\left(  \partial z\right)  \left(
f\left(  x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}$.

\textbf{Example:} If $P\left(  w\right)  =w_{1}$ (the first variable), then
$A\left(  P,f,g\right)  \left(  x\right)  =-\dfrac{\partial f}{\partial x_{1}%
}g+\dfrac{\partial g}{\partial x_{1}}f$.

\begin{lemma}
For any three polynomials $P,f,g$, we have $A\left(  P,f,g\right)  =A\left(
P_{-},g,f\right)  $, where $P_{-}\left(  w\right)  =P\left(  -w\right)  $.
\end{lemma}

\begin{corollary}
For any two polynomials $P$ and $f$, we have $A\left(  P,f,f\right)  =0$ if
$P$ is odd.
\end{corollary}

This is clear from the definition.

\begin{theorem}
[Hirota bilinear relations]Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be a
nonzero vector. Then, $\tau\in\Omega$ if and only if%
\[
A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{x}\right)  \exp\left(  \sum\limits_{s\geq1}y_{s}x_{s}\right)
,\tau,\tau\right)  =0,
\]
where $\widetilde{x}$ denotes $\left(  x_{1},\dfrac{x_{2}}{2},\dfrac{x_{3}}%
{3},...\right)  $ and where $y_{1},y_{2},y_{3},...$ are additional variables
(which are considered as constants for our polynomials, so they should be
"taken out of" $A$). (Recall that $S_{j}$ are the elementary Schur polynomials.)

Note that every coefficient of $\sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{x}\right)  \exp\left(  \sum
\limits_{s\geq1}y_{s}x_{s}\right)  $ as a power series in the $y$'s is a
honest polynomial (finite!!), so this stuff makes sense.
\end{theorem}

\textit{Proof of Theorem.} Let $\operatorname*{CT}$ mean constant term. We
introduce a new variable $t$. We know%
\begin{align*}
&  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\\
&  =u\exp\left(  -2\sum\limits_{j}u^{j}y_{j}\right)  \exp\left(
-\sum\limits_{j}\dfrac{u^{-j}}{j}\partial_{y_{j}}\right)  \tau\left(
x-y\right)  \tau\left(  x+y\right)  .
\end{align*}
Thus%
\begin{align*}
&  \operatorname*{CT}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast
}\left(  u\right)  \tau\right) \\
&  =\operatorname*{CT}\left(  u\exp\left(  -2\sum\limits_{j}u^{j}y_{j}\right)
\exp\left(  -\sum\limits_{j}\dfrac{u^{-j}}{j}\partial_{y_{j}}\right)
\tau\left(  x-y\right)  \tau\left(  x+y\right)  \right) \\
&  =\operatorname*{CT}\left(  u\underbrace{\exp\left(  -2\sum\limits_{j}%
u^{j}y_{j}\right)  }_{=\sum\limits_{j=0}^{\infty}s_{j}\left(  -2y\right)
u^{j}}\underbrace{\exp\left(  -\sum\limits_{j}\dfrac{u^{-j}}{j}\partial
_{y_{j}}\right)  }_{=\sum\limits_{\ell=0}^{\infty}s_{j}\left(
\widetilde{\partial_{y}}\right)  u^{-\ell}}\tau\left(  x+y+t\right)
\tau\left(  x-y-t\right)  \right)  \mid_{t=0}\\
&  =\operatorname*{CT}\left(  u\left(  \sum\limits_{j=0}^{\infty}s_{j}\left(
-2y\right)  u^{j}\right)  \left(  \sum\limits_{\ell=0}^{\infty}s_{j}\left(
\widetilde{\partial_{y}}\right)  u^{-\ell}\right)  \tau\left(  x+y+t\right)
\tau\left(  x-y-t\right)  \right)  \mid_{t=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{y}}\right)  \tau\left(  x+y+t\right)  \tau\left(
x-y-t\right)  \mid_{t=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{t}}\right)  \tau\left(  x+y+t\right)  \tau\left(
x-y-t\right)  \mid_{t=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have replaced }%
\widetilde{\partial_{y}}\text{ by }\widetilde{\partial_{t}}\text{, since
}y\text{ and }t\text{ only appear as }y+t\right) \\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{t}}\right)  \exp\left(  \sum\limits_{s}y_{s}%
\dfrac{\partial}{\partial_{t_{s}}}\right)  \tau\left(  x+t\right)  \tau\left(
x-t\right)  \mid_{t=0}\\
&  =\text{qed.}%
\end{align*}


So let us take the monomials in $y$ in%
\[
A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{x}\right)  \exp\left(  \sum\limits_{s\geq1}y_{s}x_{s}\right)
,\tau,\tau\right)  =0
\]
and see what equations we get.

Monomial $1$ in $y_{j}$: We get $A\left(  s_{1}\left(  x\right)  ,\tau
,\tau\right)  =0$. But this trivially holds always (since $s_{1}\left(
x\right)  =x_{1}$ is odd).

Monomial $y_{r}$ in $y_{j}$ (where $r\geq1$):

Denote $\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{x}\right)  \exp\left(  \sum\limits_{s\geq1}y_{s}x_{s}\right)  $ by
$G\left(  x,y\right)  $.

\textbf{Easy to see:} The coefficient of $y_{r}$ in $G\left(  x,y\right)  $ is
$x_{1}x_{r}-2S_{r+1}\left(  \widetilde{x}\right)  $. Denote this by
$T_{r}\left(  x\right)  $.

We have $T_{1}\left(  x\right)  =x_{2}$, $T_{2}\left(  x\right)
=-\dfrac{x_{1}^{3}}{3}-\dfrac{2x_{3}}{3}$ and $T_{3}\left(  x\right)
=\dfrac{x_{1}x_{3}}{3}-\dfrac{x_{4}}{2}-\dfrac{x_{2}^{2}}{4}-\dfrac{x_{1}^{4}%
}{12}-\dfrac{x_{1}^{2}x_{2}}{2}$. Note that $T_{3}\left(  x\right)  $ is not odd!!!

So we get the equation $A\left(  T_{3},\tau,\tau\right)  =0$. This rewrites
as
\begin{equation}
\left(  \partial_{w_{1}}^{4}+3\partial_{w_{2}}^{2}-4\partial_{w_{1}}%
\partial_{w_{3}}\right)  \tau\left(  x-w\right)  \tau\left(  x+w\right)
\mid_{w=0}=0. \label{KdV.star}%
\end{equation}
This does not yet look like a PDE in any usual form. We will now transform it
into one.

We make the substitution $x_{1}=x$, $x_{2}=y$, $x_{3}=t$, $x_{m}=c_{m}$ for
$m\geq4$, and $u=2\partial_{x}^{2}\log\tau$.

\begin{proposition}
$\tau\left(  x,y,t,c_{4},c_{5},..\right)  $ satisfies (\ref{KdV.star}) if and
only if $u$ satisfies the KP equation
\[
\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(  \partial_{t}u-\dfrac{3}%
{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)  .
\]

\end{proposition}

\textit{Proof.} Optional homework exercise.

Thus, we know that any element of $\operatorname*{Gr}$ gives rise to a
solution of the KP equation. Since we know how to produce elements of
$\operatorname*{Gr}$, we thus know how to produce solutions of the KP equation!

We cannot hope to find \textit{all} solutions explicitly (since they depend on
boundary conditions, and these can be arbitrarily nonexplicit), but we will
use this to find a dense subset of them.

Some of the solutions will not depend on $y$, and, with some work, give rise
to solution of KdV (Korteweg-de Vries).

The equations corresponding to $y_{4}$, $y_{5}$, $...$ correspond to the
\textit{KP hierarchy} of higher-order PDEs. There is no point in writing them
up explicitly; they become more and more complicated.

\begin{corollary}
Let $\lambda$ be a partition. Then, $2\partial_{x}^{2}\log\left(  S_{\lambda
}\left(  x,y,t,c_{4},c_{5},...\right)  \right)  $ is a solution of the KP
equation (and of the whole KP hierarchy).
\end{corollary}

\textit{Proof.} We know that the wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$ is in $\Omega$, so that $S_{\lambda}\in\Omega$ (in the
bosonic language). Qed.

Now we will construct other solutions (which are called multisoliton solutions).

Recall the series%
\begin{align*}
\Gamma\left(  u,v\right)   &  =u:\Gamma\left(  u\right)  \Gamma^{\ast}\left(
v\right)  :\\
&  =\exp\left(  \sum\limits_{j\geq1}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)
\exp\left(  -\sum\limits_{j\geq1}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}


\begin{proposition}
\label{prop.KdV.grassm}If $\tau\in\Omega$ and $a\in\mathbb{C}$, then%
\[
\left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\in\Omega_{u,v},
\]
where%
\[
\Omega_{u,v}=\left\{  \tau\in\mathcal{B}^{\left(  0\right)  }\left(  \left(
u,v\right)  \right)  \ \mid\ S\left(  \tau\otimes\tau\right)  =0\right\}  .
\]

\end{proposition}

\begin{corollary}
We have
\begin{align*}
&  \left(  1+a_{1}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(
1+a_{2}\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a_{n}%
\Gamma\left(  u_{n},v_{n}\right)  \right)  \mathbf{1}\\
&  \in\Omega
\end{align*}
(in fact, in an appropriate $\Omega_{u_{1},v_{1},u_{2},v_{2},...}$ rather than
in $\Omega$ itself).
\end{corollary}

\textit{Idea of proof of Proposition.} We will prove $\Gamma\left(
u,v\right)  ^{2}=0$, but we will have to make sense of a term like
$\Gamma\left(  u,v\right)  $ in order to define this. Thus, $1+a\Gamma\left(
u,v\right)  $ will become $\exp\left(  a\Gamma\left(  u,v\right)  \right)  $.

We will formalize this proof later.

But first, here is the punchline of this:

\begin{proposition}
If $\tau=\left(  1+a_{1}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(
1+a_{2}\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a_{n}%
\Gamma\left(  u_{n},v_{n}\right)  \right)  \mathbf{1}$, then $2\partial
_{x}^{2}\log\tau$ is given by a convergent series and defines a solution of KP
depending on the parameters $a_{i}$, $u_{i}$ and $v_{i}$.
\end{proposition}

This solution is called an $n$\textit{-soliton solution}.

For $n=1$, we have%
\[
\tau=\left(  1+a\Gamma\left(  u,v\right)  \right)  \mathbf{1}=1+a\exp\left(
\left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)  y+\left(  u^{3}%
-v^{3}\right)  t+\left(  u^{4}-v^{4}\right)  c_{4}+...\right)  .
\]
Absorb the $c_{i}$ parameters into a single constant $c$, which can be
absorbed into $a$. So we get%
\[
\tau=1+a\exp\left(  \left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)
y+\left(  u^{3}-v^{3}\right)  t\right)  .
\]
This $\tau$ satisfies
\[
2\partial_{x}^{2}\log\tau=\dfrac{\left(  u-v\right)  ^{2}}{2}\dfrac{1}%
{\cosh^{2}\left(  \dfrac{1}{2}\left(  \left(  u-v\right)  x+\left(
u^{2}-v^{2}\right)  y+\left(  u^{3}-v^{3}\right)  \tau\right)  \right)  }.
\]
Call this function $U$. To make it independent of $y$ (so we get a solution of
KdV equation), we set $v=-u$, and this becomes%
\[
U=\dfrac{2u^{2}}{\cosh^{2}\left(  ux+u^{3}t\right)  }.
\]
This is exactly the soliton solution of KdV.

But let us now give the promised proof of Proposition \ref{prop.KdV.grassm}.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that
$\Gamma\left(  u,v\right)  =:\Gamma\left(  u\right)  \Gamma^{\ast}\left(
v\right)  :$. We can show:

\begin{lemma}
\label{lem.KdV.GG}We have%
\[
\Gamma\left(  u\right)  \Gamma\left(  v\right)  =\left(  u-v\right)
\cdot\left.  :\Gamma\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\left(
u-v\right)  \cdot\left.  :\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(
v\right)  :\right.  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.KdV.GG}.} We have%
\[
...\exp\left(  \sum_{j}\dfrac{a_{j}}{j}u^{-j}\right)  \exp\left(  \sum
_{k}\dfrac{a_{-k}}{k}v^{k}\right)  ...
\]
and we have to switch these two terms. We get something like%
\[
\exp\left(  -\log\left(  1-\dfrac{v}{u}\right)  \right)  =\dfrac{1}%
{1-\dfrac{v}{u}}=\dfrac{u}{u-v}.
\]
Etc.

We can generalize this: If $\varepsilon=1$ or $\varepsilon=-1$, we can define
$\Gamma_{\varepsilon}$ by $\Gamma_{+1}=\Gamma$ and $\Gamma_{-1}=\Gamma^{\ast}%
$. Then,

\begin{proposition}
We have%
\[
\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}}\left(
u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)  =\prod
\limits_{i<j}\left(  u_{i}-u_{j}\right)  ^{\varepsilon_{i}\varepsilon_{j}%
}\left.  :\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon
_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
:\right.  .
\]
Here, series are being expanded in the region where $\left\vert u_{1}%
\right\vert >\left\vert u_{2}\right\vert >...>\left\vert u_{n}\right\vert $.
\end{proposition}

\begin{corollary}
The matrix elements of $\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)
\Gamma_{\varepsilon_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}%
}\left(  u_{n}\right)  $ (this means expressions of the form $\left(  w^{\ast
},\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}%
}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
w\right)  $ with $w\in\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}%
\in\mathcal{B}^{\left(  0\right)  \ast}$ (where $^{\ast}$ means restricted
dual); a priori, these are only series) are series which converge to rational
functions of the form%
\[
P\left(  u\right)  \cdot\prod\limits_{i<j}\left(  u_{i}-u_{j}\right)
^{\varepsilon_{i}\varepsilon_{j}},\ \ \ \ \ \ \ \ \ \ \text{where }%
P\in\mathbb{C}\left[  u_{1}^{\pm1},u_{2}^{\pm1},...,u_{n}^{\pm1}\right]  .
\]

\end{corollary}

This follows from the Proposition since matrix elements of normal ordered
products are Laurent polynomials.

\begin{corollary}
We have $\Gamma\left(  u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)
=\dfrac{\left(  u^{\prime}-u\right)  \left(  v^{\prime}-v\right)  }{\left(
v^{\prime}-u\right)  \left(  u^{\prime}-v\right)  }\left.  :\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  :\right.  $.
\end{corollary}

Here, we cancelled $u-v$ and $u^{\prime}-v^{\prime}$ which is okay because our
rational functions lie in an integral domain.

As a corollary of this corollary, we have:

\begin{corollary}
If $u\neq v$, then $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  =0$. By which we mean that for any $w\in
\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}\in\mathcal{B}^{\left(
0\right)  \ast}$, we have $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\left(  w^{\ast},\Gamma\left(  u^{\prime
},v^{\prime}\right)  \Gamma\left(  u,v\right)  w\right)  =0$ as a rational function.
\end{corollary}

Informally, this can be written $\left(  \Gamma\left(  u,v\right)  \right)
^{2}=0$. But this does not really make sense in a formal sense since we are
not supposed to take squares of such power series.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that our idea was
to use $1+a\Gamma=\exp\left(  a\Gamma\right)  $ since $\Gamma^{2}=0$. But this
is not rigorous since we cannot speak of $\Gamma^{2}$. So here is the actual proof:

We have (abbreviating $\Gamma\left(  u,v\right)  $ by $\Gamma$ occasionally)%
\begin{align*}
&  S\left(  \left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\otimes\left(
1+a\Gamma\left(  u,v\right)  \right)  \tau\right) \\
&  =\underbrace{S\left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}\tau\in\Omega\text{)}}}+a\underbrace{S\left(  \Gamma\otimes1+1\otimes
\Gamma\right)  \left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}S\text{ commutes with }\mathfrak{gl}_{\infty}\text{,}\\\text{and coefficients
of }\Gamma\text{ are in }\mathfrak{gl}_{\infty}\text{,}\\\text{and }S\left(
\tau\otimes\tau\right)  =0\text{)}}}+a^{2}S\left(  \Gamma\otimes\Gamma\right)
\left(  \tau\otimes\tau\right) \\
&  =a^{2}S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right)
.
\end{align*}
Remains to prove that $S\left(  \Gamma\otimes\Gamma\right)  \left(
\tau\otimes\tau\right)  =0$.

We have
\begin{align*}
&  S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right) \\
&  =\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime}\rightarrow
v}}\dfrac{1}{2}S\left(  \Gamma\left(  u,v\right)  \tau\otimes\Gamma\left(
u^{\prime},v^{\prime}\right)  \tau+\Gamma\left(  u^{\prime},v^{\prime}\right)
\tau\otimes\Gamma\left(  u,v\right)  \tau\right) \\
&  =\dfrac{1}{2}\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime
}\rightarrow v}}\underbrace{S\left(  \Gamma\left(  u^{\prime},v^{\prime
}\right)  \otimes1+1\otimes\Gamma\left(  u^{\prime},v^{\prime}\right)
\right)  \left(  \Gamma\left(  u,v\right)  \otimes1+1\otimes\Gamma\left(
u,v\right)  \right)  \left(  \tau\otimes\tau\right)  }%
_{\substack{=0\\\text{(since }S\text{ commutes with these things)}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2}\lim\limits_{\substack{u^{\prime
}\rightarrow u;\\v^{\prime}\rightarrow v}}S\left(  \underbrace{\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  }_{\rightarrow
0}\otimes1+1\otimes\underbrace{\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  }_{\rightarrow0}\right)  \left(  \tau\otimes
\tau\right) \\
&  =0.
\end{align*}
This proves Proposition \ref{prop.KdV.grassm}.

\subsection{Representations of $\operatorname*{Vir}$ revisited}

We now come back to the representation theory of the Virasoro algebra
$\operatorname*{Vir}$.

Recall that to every pair $\lambda=\left(  c,h\right)  $, we can attach a
Verma module $M_{\lambda}^{+}=M_{c,h}^{+}$ over $\operatorname*{Vir}$. We will
denote this module by $M_{\lambda}=M_{c,h}$, and its $v_{\lambda}^{+}$ by
$v_{\lambda}$.

This module $M_{\lambda}$ has a symmetric bilinear form $\left(  \cdot
,\cdot\right)  :M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$
such that $\left(  v_{\lambda},v_{\lambda}\right)  =1$ and $\left(
L_{n}v,w\right)  =\left(  v,L_{-n}w\right)  $ for all $n\in\mathbb{N}$, $v\in
M_{\lambda}$ and $w\in M_{\lambda}$. This form is called the
\textit{Shapovalov form}, and is obtained from the invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ by means of the
involution on $\operatorname*{Vir}$.

Also, if $\lambda\in\mathbb{R}^{2}$, the module $M_{\lambda}^{+}$ has a
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ satisfying the same conditions.

We recall that $M_{\lambda}$ has a unique irreducible quotient $L_{\lambda}$.
We have asked questions about when it is unitary, etc.. We will try to answer
some of these questions today.

\begin{Convention}
Let us change the grading of the Virasoro algebra $\operatorname*{Vir}$ to
$\deg L_{i}=-i$. Correspondingly, $M_{\lambda}$ becomes $M_{\lambda}%
=\bigoplus\limits_{n\geq0}M_{\lambda}\left[  n\right]  $.
\end{Convention}

For any $n\geq0$, we have the polynomial $\det\nolimits_{n}\left(  c,h\right)
$ which is the determinant of the contravariant form $\left(  \cdot
,\cdot\right)  $ in degree $n$. This polynomial is defined up to a constant
scalar. Let us recall how it is defined:

Let $\left(  a_{j}\right)  $ be a basis of $U\left(  \operatorname*{Vir}%
\nolimits_{-}\right)  \left[  n\right]  $ (where $\operatorname*{Vir}%
\nolimits_{-}$ is $\left\langle L_{-1},L_{-2},L_{-3},...\right\rangle $; this
is now the \textbf{positive} part of $\operatorname*{Vir}$). Then,
$\det\nolimits_{n}\left(  c,h\right)  =\det\left(  \left(  a_{I}v_{\lambda
},a_{J}v_{\lambda}\right)  _{I,J}\right)  $. If we change the basis by a
matrix $S$, the determinant multiplies by $\left(  \det S\right)  ^{2}$.

For a Hermitian form, we can do the same when $\left(  c,h\right)  $ is real,
but then $\det\nolimits_{n}\left(  c,h\right)  $ is defined up to a
\textbf{positive} scalar, because now the determinant multiplies by
$\left\vert \det S\right\vert ^{2}$. Hence it makes sense to say that
$\det\nolimits_{n}\left(  c,h\right)  >0$.

\begin{proposition}
We have $\det\nolimits_{n}\left(  c,h\right)  =0$ if and only if there exists
a singular vector $w\neq0$ in $M_{c,h}$ of degree $\leq n$ and $>0$.

In particular, if $\det\nolimits_{n}\left(  c,h\right)  =0$, then
$\det\nolimits_{n+1}\left(  c,h\right)  =0$.
\end{proposition}

In fact, we will see that $\det\nolimits_{n+1}$ is divisible by $\det
\nolimits_{n}$.

\textit{Proof of Proposition.} Apparently this is supposed to follow from
something we did.

We recall examples:%
\begin{align*}
\det\nolimits_{1}  &  =2h,\\
\det\nolimits_{2}  &  =2h\left(  16h^{2}+2hc-10h+c\right)  .
\end{align*}
Also recall that $M_{c,h}$ is irreducible if and only if every positive $n$
satisfies $\det\nolimits_{n}\left(  c,h\right)  \neq0$.

\begin{proposition}
Let $\left(  c,h\right)  \in\mathbb{R}^{2}$. If $M_{c,h}$ is unitary, then
$\det\nolimits_{n}\left(  c,h\right)  >0$ for all positive $n$.

More generally, if $L_{c,h}\left[  n\right]  \cong M_{c,h}\left[  n\right]  $
for some positive $n$, and $L_{c,h}$ is unitary, then $\det\nolimits_{n}%
\left(  c,h\right)  >0$.
\end{proposition}

\textit{Proof of Proposition.} A positive-definite Hermitian matrix has
positive determinant.

\begin{theorem}
\label{thm.kac.leader}Fix $c$. Regard $\det\nolimits_{m}\left(  c,h\right)  $
as a polynomial in $h$. Then,%
\[
\det\nolimits_{m}\left(  c,h\right)  =K\cdot h^{\sum\limits_{\substack{r,s\geq
1;\\rs\leq m}}p\left(  m-rs\right)  }+\left(  \text{lower terms}\right)
\]
for some nonzero constant $K$ (which depends on the choice of the basis).
\end{theorem}

\textit{Proof.} We computed before the leading term of $\det\nolimits_{m}$ for
any graded Lie algebra.

$\left\langle L_{-k}^{m_{k}}...L_{-1}^{m_{1}}v_{\lambda},L_{-k}^{n_{k}%
}...L_{-1}^{n_{1}}v_{\lambda}\right\rangle $: the main contribution to the
leading term comes from diagonal.

What degree in $h$ do we get?

If $\mu$ is a partition of $m$, we can write $m=1k_{1}\left(  \mu\right)
+2k_{2}\left(  \mu\right)  +...$, where $k_{i}\left(  \mu\right)  $ is the
number of times $i$ occurs in $\mu$.

$\left(  L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v,L_{-\ell}^{k_{\ell}}%
...L_{-1}^{k_{1}}v\right)  =\left(  v,L_{1}^{k_{1}}...L_{\ell}^{k_{\ell}%
}L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v\right)  $.

So $\mu$ contributes $k_{1}+...+k_{\ell}$ to the exponent of $h$.

So we conclude that the total exponent of $h$ is $\sum\limits_{\mu\vdash
m}\sum\limits_{i}k_{i}\left(  \mu\right)  $.

The rest is easy combinatorics:

Let $m\left(  r,s\right)  $ denote the number of partitions of $m$ in which
$r$ occurs exactly $s$ times. Then, $m\left(  r,s\right)  =p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  $. Thus, with $m$ and
$r$ fixed, $\sum\limits_{s}p\left(  m-rs\right)  =\sum\limits_{s}sm\left(
r,s\right)  $ (since Abel/telescope).

So our job is to show that $\sum\limits_{\mu\vdash m}\sum\limits_{i}%
k_{i}\left(  \mu\right)  =\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}sm\left(  r,s\right)  $. But $\sum\limits_{\substack{s\geq1;\\s\leq
m}}sm\left(  r,s\right)  $ is the total number of occurences of $r$ in all
partition of $m.$ Summed over $r$, it yields the total number of parts of all
partitions of $m$. But this is also $\sum\limits_{\mu\vdash m}\sum
\limits_{i}k_{i}\left(  \mu\right)  $, qed.

We now quote a theorem which was proved independently by Kac and Feigin-Fuchs:

\begin{theorem}
Suppose $rs\leq m$. Then, if%
\[
h=h_{r,s}\left(  c\right)  :=\dfrac{1}{48}\left(  \left(  13-c\right)  \left(
r^{2}+s^{2}\right)  +\sqrt{\left(  c-1\right)  \left(  c-25\right)  }\left(
r^{2}-s^{2}\right)  -24rs-2+2c\right)  ,
\]
then $\det\nolimits_{m}\left(  c,h\right)  =0$. (This is true for any of the
branches of the square root.)
\end{theorem}

\begin{theorem}
\label{thm.kac.thm1}If $h=h_{r,s}\left(  c\right)  $, then $M_{c,h}$ has a
nonzero singular vector in degree $1\leq d\leq rs$.
\end{theorem}

\begin{theorem}
[Kac, also proved by Feigin-Fuchs]\label{thm.kac.thm2}We have%
\[
\det\nolimits_{m}\left(  c,h\right)  =K_{m}\cdot\prod
\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(  c\right)
\right)  ^{p\left(  m-rs\right)  },
\]
where $K_{m}$ is some constant. Note that we should choose the same branch of
the square root in $\sqrt{\left(  c-1\right)  \left(  c-25\right)  }$ for
$h_{r,s}$ and $h_{s,r}$. The square roots "cancel out" and give way to a
polynomial in $h$ and $c$.
\end{theorem}

To prove these, we will use the following lemma:

\begin{lemma}
\label{lem.kac.linalg}Let $A\left(  t\right)  $ be a polynomial in one
variable $t$ with values in $\operatorname*{End}V$, where $V$ is a
finite-dimensional vector space. Suppose that $\dim\operatorname*{Ker}\left(
A\left(  0\right)  \right)  \geq n$. Then, $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.kac.linalg}.} Pick a basis $e_{1}%
,e_{2},...,e_{m}$ of $V$ such that the first $n$ vectors $e_{1},e_{2}%
,...,e_{n}$ are in $\operatorname*{Ker}\left(  A\left(  0\right)  \right)  $.
Then, the matrix of $A\left(  t\right)  $ in this basis has first $n$ columns
divisible by $t$, so that its determinant $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.

\textit{Proof of Theorem \ref{thm.kac.thm2}.} Let $A=A\left(  h\right)  $ be
the matrix of the contravariant form in degree $m$, considered as a polynomial
in $h$. If $h=h_{r,s}\left(  c\right)  $, we have a singular vector $w$ in
degree $1\leq d\leq rs$ (by Theorem \ref{thm.kac.thm1}), which generates a
Verma submodule $M_{c,h^{\prime}}\subseteq M_{c,h}$ (by Homework Set 3 problem
1) (the $c$ is the same since $c$ is central and thus acts by the same number
on all vectors).

So $M_{c,h}\left[  m\right]  \supseteq M_{c,h^{\prime}}\left[  m-d\right]  $.
We also have $\dim\left(  M_{c,h^{\prime}}\left[  m-d\right]  \right)
=p\left(  m-d\right)  \geq p\left(  m-rs\right)  $ (since $d\leq rs$) and
$M_{c,h^{\prime}}\left[  m-d\right]  \subseteq\operatorname*{Ker}\left(
\cdot,\cdot\right)  $ (when $h=h_{r,s}$). Hence, $\dim\left(
\operatorname*{Ker}\left(  \cdot,\cdot\right)  \right)  \geq p\left(
m-rs\right)  $. By Lemma \ref{lem.kac.linalg}, this yields that $\det
\nolimits_{m}\left(  c,h\right)  $ is divisible by $\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$.

But it is easy to see that for Weil-generic $c$, the $h-h_{r,s}\left(
c\right)  $ are different, so that $\det\nolimits_{m}\left(  c,h\right)  $ is
divisible by $\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(
h-h_{r,s}\left(  c\right)  \right)  ^{p\left(  m-rs\right)  }$. But by Theorem
\ref{thm.kac.leader}, the leading term of $\det\nolimits_{m}\left(
c,h\right)  $ is $K\cdot h^{\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}p\left(  m-rs\right)  }$, which has exactly the same degree. So
$\det\nolimits_{m}\left(  c,h\right)  $ is a constant multiple of
$\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$. Theorem \ref{thm.kac.thm2} is proven.

We will not prove Theorem \ref{thm.kac.thm1}, since we do not have the tools
for that.

\begin{corollary}
The module $M_{c,h}$ is irreducible if and only if $\left(  c,h\right)  $ does
not lie on the lines
\[
h-h_{r,r}\left(  c\right)  =0\ \Longleftrightarrow\ h+\left(  r^{2}-1\right)
\left(  c-1\right)  /24=0
\]
and quadrics (in fact, hyperbolas if we are over $\mathbb{R}$)%
\begin{align*}
&  \ \left(  h-h_{r,s}\left(  c\right)  \right)  \left(  h-h_{s,r}\left(
c\right)  \right)  =0\\
&  \Longleftrightarrow\ \left(  h-\dfrac{\left(  r-s\right)  ^{2}}{4}\right)
^{2}+\dfrac{h}{24}\left(  c-1\right)  \left(  r^{2}+s^{2}-2\right)  +\dfrac
{1}{576}\left(  r^{2}-1\right)  \left(  s^{2}-1\right)  \left(  c-1\right)
^{2}\\
&  \ \ \ \ \ \ \ \ \ \ +\dfrac{1}{48}\left(  c-1\right)  \left(  r-s\right)
^{2}\left(  rs+1\right)  =0.
\end{align*}

\end{corollary}

\begin{corollary}
\label{cor.kac.irred}\textbf{(1)} Let $h\geq0$ and $c\geq1$. Then, $L_{c,h}$
is unitary.

\textbf{(2)} Let $h>0$ and $c>1$. Then, $M_{c,h}\cong L_{c,h}$, so that
$M_{c,h}$ is rreducible.
\end{corollary}

\textit{Proof of Corollary \ref{cor.kac.irred}.} \textbf{(2)} Lines and
hyperbolas do not pass through the region.

For part \textbf{(1)} we need a lemma:

\begin{lemma}
If $\mathfrak{g}$ is a graded Lie algebra (with $\dim\mathfrak{g}_{i}%
\neq\infty$) with a real structure $\dag$, and $U\subseteq\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ is the set of $\lambda$ such that $L_{\lambda}$ is
unitary, then $U$ is closed in the usual metric.
\end{lemma}

\textit{Proof of Lemma.} It follows from the fact that if $\left(
A_{n}\right)  $ is a sequence of positive definite Hermitian matrices, and
$\lim\limits_{n\rightarrow\infty}A_{n}=A_{\infty}$, then $A_{\infty}$ is
nonnegative definite.

Okay, sorry, we are not going to use this lemma; we will derive the special
case we need.

Now I claim that if $h>0$ and $c>1$, then $L_{c,h}=M_{c,h}$ is unitary. We
know this is true for some points of this region (namely, the ones "above the
zigzag line"). Then everything follows from the fact that if $A\left(
t\right)  $ is a continuous family of nondegenerate Hermitian matrices
parametrized by $t\in\left[  0,1\right]  $ such that $A\left(  0\right)  >0$,
then $A\left(  t\right)  >0$ for every $t$. (This fact is because the
signature of a nondegenerate Hermitian matrix is a continuous map to a
discrete set, and thus constant on connected components.)

e. g., consider $M_{1,h}$ as a limit of $M_{1+\dfrac{1}{n},h}$ (this is
irreducible for large $n$).

So the matrix of the form in $M_{1,h}\left[  m\right]  $ is a limit of the
matrices for $M_{1+\dfrac{1}{n},h}\left[  m\right]  $. So the matrix for
$M_{1,h}\left[  m\right]  $ is $\geq0$. But kernel lies in $J_{1,h}\left[
m\right]  $, so the form on $L_{1,h}\left[  m\right]  =\left(  M_{1,h}\diagup
J_{1,h}\right)  \left[  m\right]  $ is strictly positive.

By analyzing the Kac curves, we can show (although \textit{we} will
\textit{not} show) that in the region $0\leq c<1$, there are only countably
many points where we possibly can have unitarity:

$c\left(  m\right)  =1-\dfrac{6}{\left(  m+2\right)  \left(  m+3\right)  };$

$h_{r,s}\left(  m\right)  =\dfrac{\left(  \left(  m+3\right)  r-\left(
m+2\right)  s\right)  ^{2}-1}{4\left(  m+2\right)  \left(  m+3\right)  }$ with
$1\leq r\leq s\leq m+1$.

for $m\geq0$.

In fact we will show that at these points we indeed have unitary representations.

\begin{proposition}
\textbf{(1)} If $c\geq0$ and $L_{c,h}$ is unitary, then $h=0$.

\textbf{(2)} We have $L_{0,h}=M_{0,h}$ if and only if $h\neq\dfrac{m^{2}%
-1}{24}$ for all $m\geq0$.

\textbf{(3)} We have $L_{1,h}=M_{1,h}$ if and only if $h\neq\dfrac{m^{2}}{24}$
for all $m\geq0$.
\end{proposition}

\textit{Proof.} \textbf{(2)} and \textbf{(3)} follow immediately from the Kac
determinant formula. For \textbf{(1)}, just compute $\det\left(
\begin{array}
[c]{cc}%
\left(  L_{-N}^{2}v,L_{-N}^{2}v\right)  & \left(  L_{-N}^{2}v,L_{-2N}v\right)
\\
\left(  L_{-2N}v,L_{-N}^{2}v\right)  & \left(  L_{-2N}v,L_{-2N}v\right)
\end{array}
\right)  =4N^{3}h^{2}\left(  8h-5N\right)  $ (this is $<0$ for high enough $N$
as long as $h\neq0$), so that the only possibility for unitarity is $h=0$.

\section{Affine Lie algebras}

Set $L\mathfrak{gl}_{n}=\mathfrak{gl}_{n}\left[  t,t^{-1}\right]  $ the loop
algebra. This acts on $\mathbb{C}^{n}\left[  t,t^{-1}\right]  $. But we can
identify $\mathbb{C}^{n}\left[  t,t^{-1}\right]  $ with $V:=\mathbb{C}%
^{\infty}$ as follows: Let $\left(  e_{1},e_{2},...,e_{n}\right)  $ be a basis
of $\mathbb{C}^{n}$. Then we can write $v_{i-kn}=e_{i}t^{k}$ for $i\in\left\{
1,2,...,n\right\}  $ and $k\in\mathbb{Z}$. We thus get an embedding of
$L\mathfrak{gl}_{n}$ into $\overline{\mathfrak{a}_{\infty}}$, namely: Let
$a\left(  t\right)  \in L\mathfrak{gl}_{n}$ be a Laurent polynomial. Write
$a\left(  t\right)  $ as $a\left(  t\right)  =\sum\limits_{k\in\mathbb{Z}%
}a_{k}t^{k}$ with $a_{k}\in\mathfrak{gl}_{n}$. Then, the corresponding matrix
in $\overline{\mathfrak{a}_{\infty}}$ is a block-diagonal matrix such that the
$i$-th block diagonal is filled with $a_{i}$'s. This looks like follows:%
\[
\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  .
\]
Recall that $\overline{\mathfrak{a}_{\infty}}$ has a central extension
$\mathfrak{a}_{\infty}=\overline{\mathfrak{a}_{\infty}}\oplus\mathbb{C}K$,
which is defined using the cocycle $\alpha$.

\begin{proposition}
The restriction of $\alpha$ to $L\mathfrak{gl}_{n}$ is the $2$-cocycle
defining the affine Lie algebra $\widehat{\mathfrak{gl}_{n}}=L\mathfrak{gl}%
_{n}\oplus\mathbb{C}K$ corresponding to the trace form $\left(  a,b\right)
=\operatorname*{Tr}\left(  ab\right)  $ for $a,b\in\mathfrak{gl}_{n}$.
\end{proposition}

\textit{Proof.} Recall that%
\[
\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\operatorname*{Res}\nolimits_{t=0}\left(  da\left(  t\right)  ,b\left(
t\right)  \right)  =\sum\limits_{k\in\mathbb{Z}}k\operatorname*{Tr}\left(
a_{k}b_{-k}\right)
\]
(where $a=\sum\limits_{k\in\mathbb{Z}}a_{k}t^{k}$ and $b=\sum\limits_{k\in
\mathbb{Z}}b_{k}t^{k}$).

[...] [verification that this is the same as the restriction of $\alpha$ is
now easy.]

Thus, we have an inclusion $\widehat{\mathfrak{gl}_{n}}\subseteq
\mathfrak{a}_{\infty}$ which lifts the inclusion $L\mathfrak{gl}_{n}%
\subseteq\overline{\mathfrak{a}_{\infty}}$ and sends $K$ to $K$.

Similarly, we can get an inclusion $\widehat{\mathfrak{sl}_{n}}\subseteq
\mathfrak{a}_{\infty}$ which lifts the inclusion $L\mathfrak{sl}_{n}%
\subseteq\overline{\mathfrak{a}_{\infty}}$.

So $\mathcal{B}^{\left(  m\right)  }\cong\mathcal{F}^{\left(  m\right)  }$ is
a module over $\widehat{\mathfrak{gl}_{n}}$ and $\widehat{\mathfrak{sl}_{n}}$
at level $1$ (this means that $K$ acts as $1$).

Now we give a definition pertaining to general affine Lie algebras:

\begin{definition}
If $\widehat{\mathfrak{g}}=L\mathfrak{g}\oplus\mathbb{C}K$ is an affine Lie
algebra, then there exists a unique derivation $d:\widehat{\mathfrak{g}%
}\rightarrow\widehat{\mathfrak{g}}$ such that $d\left(  a\left(  t\right)
\right)  =ta^{\prime}\left(  t\right)  $ (so that $d\left(  at^{m}\right)
=mat^{m-1}$) and $d\left(  K\right)  =0$.

Set $\widetilde{\mathfrak{g}}=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$ (this
makes sense since $\mathbb{C}d$ acts on $\widehat{\mathfrak{g}}$). Clearly,
$\widetilde{\mathfrak{g}}=\mathbb{C}d\oplus\widehat{\mathfrak{g}}$ as vector
space. The Lie algebra $\widetilde{\mathfrak{g}}$ is graded by taking the
grading of $\widehat{\mathfrak{g}}$ and additionally giving $d$ the degree $0$.
\end{definition}

\begin{proposition}
There exists a unique extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{B}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$, where $\psi
_{m}=\sigma^{-1}\left(  v_{m}\wedge v_{m-1}\wedge...\right)  \in
\mathcal{B}^{m}$ is the highest-weight vector. This $d$ should act on each
monomial by a uniquely determined eigenvalue. [WTF does this mean?]
\end{proposition}

\textit{Proof.} ???

Let us recall highest weight theory for $\widetilde{\mathfrak{gl}_{n}}$.

We have $\widetilde{\mathfrak{gl}_{n}}=\widetilde{\mathfrak{n}_{-}}%
\oplus\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}_{+}}$. Here,
$\widetilde{\mathfrak{h}}=\mathbb{C}K\oplus\mathbb{C}d\oplus\mathfrak{h}$
where $\mathfrak{h}$ are the diagonal matrices. Further,
$\widetilde{\mathfrak{n}_{+}}=\mathfrak{n}_{+}\oplus t\mathfrak{gl}_{n}\left[
t\right]  $ (where $\mathfrak{n}_{+}$ are the strictly upper-triangular
matrices) and $\widetilde{\mathfrak{n}_{-}}=\mathfrak{n}_{-}\oplus
t^{-1}\mathfrak{gl}_{n}\left[  t^{-1}\right]  $ (where $\mathfrak{n}_{-}$ are
the strictly lower-triangular matrices). This is a graded Lie algebra with
$\deg t=n$ and $\deg E_{i,j}=j-i$.

Define the weights $\widetilde{\omega}_{m}$ by%
\begin{align*}
\widetilde{\omega}_{m}\left(  E_{i,i}\right)   &  =\left(  \text{the number of
all }j\in\mathbb{Z}\text{ such that }j\equiv i\operatorname{mod}n\text{ and
}1\leq j\leq m\right)  ;\\
\widetilde{\omega}_{m}\left(  K\right)   &  =1;\\
\widetilde{\omega}_{m}\left(  d\right)   &  =0.
\end{align*}
Note that $i$ is in $\left\{  1,2,...,n\right\}  $ here; these $E_{i,i}$
denote the $E_{i,i}^{\mathfrak{gl}_{n}}$ (the elementary matrices of
$\mathfrak{gl}_{n}$), not the $E_{i,i}^{\mathfrak{gl}_{\infty}}$ (the
elementary matrices of $\mathfrak{gl}_{\infty}$).

Note that we can rewrite the definition of $\widetilde{\omega}_{m}\left(
E_{i,i}\right)  $ as $\widetilde{\omega}_{m}\left(  E_{i,i}\right)  =\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\leq\overline{m};\\
0,\text{ if }i>\overline{m}%
\end{array}
\right.  +\dfrac{m-\overline{m}}{n}$, where $\overline{m}$ is the element of
$\left\{  0,1,...,n-1\right\}  $ satisfying $m\equiv\overline{m}%
\operatorname{mod}n$. Or something like this. [Check this.]

\begin{proposition}
The $\widetilde{\mathfrak{gl}_{n}}$-representation $\left(  \mathcal{B}%
^{\left(  m\right)  },\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}%
}\right)  $ is irreducible with highest weight $\widetilde{\omega}_{m}$.
\end{proposition}

\textit{Proof.} \textbf{1)} It is easy to check that $\widetilde{\mathfrak{n}%
_{+}}\psi_{m}=0$ and $x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)
\psi_{m}$ if $x\in\widetilde{\mathfrak{h}}$.

\textit{Proof.} Proving that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$ is easy,
since $\widetilde{\mathfrak{n}_{+}}$ embeds into $\mathfrak{a}_{\infty}$ as
strictly upper-triangular matrices.

In order to prove that $x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)
\psi_{m}$ if $x\in\widetilde{\mathfrak{h}}$, we must show that $E_{i,i}%
^{\mathfrak{gl}_{n}}\psi_{m}=\widetilde{\omega}_{m}\left(  E_{i,i}%
^{\mathfrak{gl}_{n}}\right)  \psi_{m}$ for every $i\in\left\{
1,2,...,n\right\}  $.

Use $E_{i,i}^{\mathfrak{gl}_{n}}=\sum\limits_{j\equiv i\operatorname{mod}%
n}E_{j,j}^{\mathfrak{gl}_{\infty}}$ to conclude that $\widehat{\rho}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  =\left\lfloor \dfrac{m}{n}\right\rfloor
+\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\leq m;\\
0,\text{ if }i>m
\end{array}
\right.  $.

\textbf{2)} Irreducibility: First, we need to prove that the Heisenberg
algebra is contained in $\widetilde{\mathfrak{gl}_{n}}$.

Let $T=\left(
\begin{array}
[c]{cccccc}%
... & ... & ... & ... & ... & ...\\
... & 0 & 1 & 0 & 0 & ...\\
... & 0 & 0 & 1 & 0 & ...\\
... & 0 & 0 & 0 & 1 & ...\\
... & 0 & 0 & 0 & 0 & ...\\
... & ... & ... & ... & ... & ...
\end{array}
\right)  $ (this is the matrix which has $1$'s on the main diagonal and $0$'s
everywhere else). Clearly, $T\in\overline{\mathfrak{a}_{\infty}}$. We want to
prove that $T$ lies in $L\mathfrak{gl}_{n}\subseteq\overline{\mathfrak{a}%
_{\infty}}$.

Let $a_{0}=\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & ... & 0\\
0 & 0 & 1 & ... & 0\\
0 & 0 & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & 0
\end{array}
\right)  $ (the $n\times n$ matrix defined in the same way as to $T$).

Let $a_{1}=\left(
\begin{array}
[c]{ccccc}%
0 & 0 & 0 & ... & 0\\
0 & 0 & 0 & ... & 0\\
0 & 0 & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & 0\\
1 & 0 & 0 & ... & 0
\end{array}
\right)  $ (the $n\times n$ matrix which has a $1$ in its lowermost leftmost
corner, and $0$'s everywhere else).

Then, $T=a_{0}+ta_{1}$. Then, $T^{m}\in L\mathfrak{gl}_{n}$ for all
$m\in\mathbb{N}$. It is easy to see that $T^{-1}\in L\mathfrak{gl}_{n}$ as
well, so that $T^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{Z}$.

Now it is clear that we can embed the Heisenberg algebra $\mathcal{A}$ into
$\widehat{\mathfrak{gl}_{n}}$ by sending $t^{n}$ to $T^{n}$ and sending $K$ to
$K$. By this embedding, $\mathcal{B}^{\left(  m\right)  }$ becomes a module
over the Heisenberg algebra, and this is exactly the one we know.

Now use that the Fock space is irreducible as a Heisenberg module.

Qed.

Note that the restriction to $\widehat{\mathfrak{sl}_{n}}$ of the
representation $\mathcal{B}^{\left(  m\right)  }$ is not irreducible, since
$\left[  T^{n},\widehat{\mathfrak{sl}_{n}}\right]  =0$ (because $T^{n}%
=\operatorname*{diag}\underbrace{\left(  t,t,...,t\right)  }_{n\text{ times
}t}=I_{n}t$).

But $\psi_{m}$ is the highest weight vector with highest weight $\omega
_{\overline{m}}$. Let us look at how this representation $\mathcal{B}^{\left(
m\right)  }$ decomposes.

Let $h_{i}=E_{i,i}^{\mathfrak{gl}_{n}}-E_{i+1,i+1}^{\mathfrak{gl}_{n}}$ for
$i\in\left\{  1,2,...,n-1\right\}  $, and let $h_{0}=K-h_{1}-h_{2}%
-...-h_{n-1}$. This is a basis of $\widetilde{\mathfrak{h}}\cap
\widetilde{\mathfrak{gl}_{n}}$.

We have $\omega_{m}\left(  h_{i}\right)  =\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\equiv m\operatorname{mod}n;\\
0,\text{ if }i\not \equiv m\operatorname{mod}n
\end{array}
\right.  $ for all $i\in\left\{  0,1,...,n-1\right\}  $.

Consider $\widehat{\mathfrak{gl}_{n}}=\left(  \widehat{\mathfrak{sl}_{n}%
}\oplus\mathcal{A}_{n}\right)  \diagup\left(  K_{1}-K_{2}\right)  $, where
$\mathcal{A}_{n}$ is the $n$-Heisenberg algebra, and where $K_{1}$ and $K_{2}$
denote the $K$'s of $\widehat{\mathfrak{sl}_{n}}$ and $\mathcal{A}_{n}$, respectively.

Since $\left[  T^{ni},\widehat{\mathfrak{sl}_{n}}\right]  =0$ for every
integer $i$, the Lie algebra $\widehat{\mathfrak{sl}_{n}}$ acts on the space
$\mathcal{B}_{0}^{\left(  m\right)  }=\left\{  v\in\mathcal{B}^{\left(
m\right)  }\ \mid\ T^{ni}v=0\text{ for all }i>0\right\}  $. Recalling that
$\mathcal{B}^{m}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ with
$T^{ni}\sim\dfrac{\partial}{\partial x_{ni}}$, we have $\mathcal{B}%
_{0}^{\left(  m\right)  }\cong\mathbb{C}\left[  x_{j}\ \mid\ n\nmid j\right]
$.

\begin{theorem}
\label{thm.B0m}This $\mathcal{B}_{0}^{\left(  m\right)  }$ is an irreducible
$\widehat{\mathfrak{sl}_{n}}$-module (or $\widetilde{\mathfrak{sl}_{n}}%
$-module; this doesn't matter) with highest weight $\omega_{m}$ (this means
that $\mathcal{B}_{0}^{\left(  m\right)  }\cong L_{\omega_{m}}$) and depends
only on $m$ modulo $n$. Moreover, $\mathcal{B}^{\left(  m\right)  }%
\cong\mathcal{B}_{0}^{\left(  m\right)  }\otimes F_{m}$, where $F_{m}$ is the
appropriate Fock module over $\mathcal{A}_{n}$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.B0m}.} We clearly have such a decomposition
as vector spaces, $F_{m}=\mathbb{C}\left[  x_{n},x_{2n},x_{3n},...\right]  $.
Each of the two Lie algebras acts in its own factor: $\mathcal{A}_{n}$ acts in
$F_{m}$, and $\widehat{\mathfrak{gl}_{n}}$ commutes with $\mathcal{A}_{n}$.
Since the tensor product is irreducible, each factor is irreducible, so that
$\mathcal{B}_{0}^{\left(  m\right)  }$ is irreducible.

We can now classify unitary highest-weight representations of
$\widehat{\mathfrak{sl}_{n}}$:

\begin{proposition}
The highest weight representation $L_{\omega_{m}}$ is unitary for each
$m\in\left\{  0,1,...,n-1\right\}  $.
\end{proposition}

\textit{Proof.} The contravariant Hermitian form on $L_{\omega_{m}}$ is the
restriction of the form on $\mathcal{B}^{\left(  m\right)  }$.

\begin{corollary}
If $k_{0},k_{1},...,k_{n-1}$ are nonnegative integers, then $L_{k_{0}%
\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}}$ is unitary (of level
$k_{0}+k_{1}+...+k_{n-1}$).
\end{corollary}

\textit{Proof.} The tensor product $L_{\omega_{0}}^{\otimes k_{0}}\otimes
L_{\omega_{1}}^{\otimes k_{1}}\otimes...\otimes L_{\omega_{n-1}}^{\otimes
k_{n-1}}$ is unitary (being a tensor product of unitary representations), and
thus is a direct sum of irreducible representations. Clearly, $L_{k_{0}%
\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}}$ is a summand of this
module, and thus also unitary, qed.

\begin{theorem}
\label{thm.sln.unitaries}These are the only unitary highest weight
representations of $\widehat{\mathfrak{sl}_{n}}$.
\end{theorem}

To prove this, first a lemma:

\begin{lemma}
Let $L_{\lambda}$ be a unitary representation of $\mathfrak{sl}_{2}$ with
$e^{\dag}=f$, $f^{\dag}=e$ and $h^{\dag}=h$. Then, $\lambda\in\mathbb{Z}_{+}$
(where we identify $\lambda$ with $\lambda\left(  h\right)  $), and all such
$L_{\lambda}$ are indeed unitary.
\end{lemma}

\textit{Proof of Lemma.} Let $v_{\lambda}=v_{\lambda}^{+}$. We have $\left(
f^{m}v_{\lambda},f^{m}v_{\lambda}\right)  =m!\left(  \lambda-m+1\right)
...\lambda$ (we already proved this long ago, for the bilinear rather than the
Hermitian form, but this is clearly just a rewriting of that result due to
$e^{\dag}=f$). For this to be nonnegative for any $m\in\mathbb{N}$, we need
$L_{\lambda}$ to be finite-dimensional. For finite-dimensional, $f^{\lambda
}v_{\lambda}=0$ and it is positive.

\begin{corollary}
If $\mathfrak{g}$ is a Lie algebra with antilinear antiinvolution $\dag$ and
$\mathfrak{sl}_{2}\subseteq\mathfrak{g}$, and if $\dag\mid_{\mathfrak{sl}_{2}%
}$ sends $e,f,h$ to $f,e,h$, and if $V$ is a unitary representation of
$\mathfrak{g}$, and if some $v\in V$ satisfies $ev=0$ and $hv=\lambda v$, then
$\lambda\in\mathbb{Z}_{+}$.
\end{corollary}

\textit{Proof of Theorem \ref{thm.sln.unitaries}.} For every $i\in\left\{
0,1,...,n-1\right\}  $, we have an $\mathfrak{sl}_{2}$-subalgebra:%
\begin{align*}
h_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i,i}-E_{i+1,i+1},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
K+E_{n,n}-E_{1,1},\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.  ,\\
e_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i,i+1},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
E_{n,1}t,\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.  ;\\
f_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i+1,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
E_{1,n}t^{-1},\ \ \ \ \ \ \ \ \ \ \text{if }t=0
\end{array}
\right.
\end{align*}
\footnote{Here, $E_{i,j}$ means $E_{i,j}^{\mathfrak{gl}_{n}}$.} (these form an
$\mathfrak{sl}_{2}$-triple, as can be easily checked). These satisfy
$e_{i}^{\dag}=f_{i}$, $f_{i}^{\dag}=e_{i}$ and $h_{i}^{\dag}=h_{i}$. Thus, if
$L_{\lambda}$ is a unitary representation of $\widehat{\mathfrak{sl}_{n}}$,
then $\lambda\left(  h_{i}\right)  \in\mathbb{Z}_{+}$. But $\omega_{i}$ are a
basis for the weights, and namely the dual basis to the basis of the $h_{i}$.
Thus, $\lambda=\sum\limits_{i=0}^{n-1}\lambda\left(  h_{i}\right)  \omega_{i}%
$. Hence, $\lambda=\sum\limits_{i=0}^{n-1}k_{i}\omega_{i}$ with $k_{i}%
\in\mathbb{Z}_{+}$. Qed.

\begin{remark}
Relation between $\widehat{\mathfrak{sl}_{n}}$-modules and $\mathfrak{sl}_{n}$-modules:

Let $L_{\lambda}$ be a unitary $\widehat{\mathfrak{sl}_{n}}$-module, with
$\lambda=k_{0}\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}$.

Then, $U\left(  \mathfrak{sl}_{n}\right)  v_{\lambda}=L_{\overline{\lambda}}$
where $\overline{\lambda}=k_{1}\omega_{1}+k_{2}\omega_{2}+...+k_{n-1}%
\omega_{n-1}$ is a weight for $\mathfrak{sl}_{n}$. And if the level of
$L_{\lambda}$ was $k$, then we must have $k_{1}+k_{2}+...+k_{n-1}\leq k$.
\end{remark}

\subsection{The Sugawara construction}

We will now study the Sugawara construction. It constructs a
$\operatorname*{Vir}$ action on a $\widehat{\mathfrak{g}}$-module (under some
conditions), and it generalizes the action of $\operatorname*{Vir}$ on the
$\mu$-Fock representation $F_{\mu}$.

\begin{definition}
\label{def.sugawara}Let $\mathfrak{g}$ be a finite-dimensional $\mathbb{C}%
$-Lie algebra with an invariant symmetric bilinear form $\left(  \cdot
,\cdot\right)  $. (This form needs not be nondegenerate; it is even allowed to
be $0$.)

Consider the affine Lie algebra $\widehat{\mathfrak{g}}=\mathfrak{g}\left[
t,t^{-1}\right]  \oplus\mathbb{C}K$ defined through the $2$-cocycle
\[
\omega\left(  a,b\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
a^{\prime}\left(  t\right)  ,b\left(  t\right)  \right)  dt.
\]
(This is the $2$-cocycle $\omega$ in Definition \ref{def.loop}. We just
slightly rewrote the definition.)

Let $\operatorname*{Kil}$ denote the Killing form on $\mathfrak{g}$, defined
by
\[
\operatorname*{Kil}\left(  a,b\right)  =\operatorname*{Tr}\left(
\operatorname*{ad}\left(  a\right)  \cdot\operatorname*{ad}\left(  b\right)
\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in\mathfrak{g}.
\]


An element $k\in\mathbb{C}$ is said to be \textit{non-critical for }$\left(
\mathfrak{g},\left(  \cdot,\cdot\right)  \right)  $ if and only if the form
$k\cdot\left(  \cdot,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$ is nondegenerate.
\end{definition}

\begin{definition}
\label{def.sugawara.M}Let $M$ be a $\widehat{\mathfrak{g}}$-module.

We say that $M$ is \textit{admissible} if for every $v\in M$, there exists
some $N\in\mathbb{N}$ such that every integer $n\geq N$ and every
$a\in\mathfrak{g}$ satisfy $at^{n}\cdot v=0$.

If $k\in\mathbb{C}$, then we say that $M$ is \textit{of level }$k$ if
$K\mid_{M}=k\cdot\operatorname*{id}$.
\end{definition}

The following theorem is one of the most important facts about affine Lie algebras:

\begin{theorem}
[Sugawara construction]\label{thm.sugawara}Let us work in the situation of
Definition \ref{def.sugawara}.

Let $k\in\mathbb{C}$ be non-critical for $\left(  \mathfrak{g},\left(
\cdot,\cdot\right)  \right)  $. Let $M$ be an admissible
$\widehat{\mathfrak{g}}$-module of level $k$. Let $B\subseteq\mathfrak{g}$ be
a basis orthonormal with respect to the form $k\left(  \cdot,\cdot\right)
+\dfrac{1}{2}\operatorname*{Kil}$.

For every $x\in\mathfrak{g}$ and $n\in\mathbb{Z}$, let us denote by $x_{n}$
the element $xt^{n}\in\widehat{\mathfrak{g}}$.

Then, the operators%
\[
L_{n}:=\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.
\]
on $M$ are well-defined and give rise to a $\operatorname*{Vir}$%
-representation on $M$ with central charge%
\[
C=k\cdot\sum\limits_{a\in B}\left(  a,a\right)  .
\]
Let us explain what the normal ordering here means: it is defined by%
\[
\left.  :a_{m}a_{\ell}:\right.  =\left\{
\begin{array}
[c]{c}%
a_{m}a_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq\ell;\\
a_{\ell}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>\ell
\end{array}
\right.  .
\]
Note that the $L_{n}$ do not depend on the choice of orthonormal basis.

Moreover, these formulas (for $L_{n}$ and $C$) extend the action of
$\widehat{\mathfrak{g}}$ on $M$ to an action of $\operatorname*{Vir}%
\ltimes\widehat{\mathfrak{g}}$, so they satisfy $L_{n}\circ a_{m}=-ma_{n+m}$
and $L_{n}\circ K=0$.
\end{theorem}

Before we prove this, an elementary lemma on Killing forms of
finite-dimensional Lie algebras:

\begin{lemma}
\label{lem.sugawara.Kil}Let $\mathfrak{g}$ be a finite-dimensional Lie
algebra. Denote by $\operatorname*{Kil}$ the Killing form of $\mathfrak{g}$.
Let $n\in\mathbb{N}$ and $p_{1},p_{2},...,p_{n}\in\mathfrak{g}$ and
$q_{1},q_{2},...,q_{n}\in\mathfrak{g}$ be such that the tensor $\sum
\limits_{i=1}^{n}p_{i}\otimes q_{i}\in\mathfrak{g}\otimes\mathfrak{g}$ is
$\mathfrak{g}$-invariant. Then, $\sum\limits_{i=1}^{n}\left[  \left[
b,p_{i}\right]  ,q_{i}\right]  =\sum\limits_{i=1}^{n}\operatorname*{Kil}%
\left(  b,p_{i}\right)  q_{i}$ for every $b\in\mathfrak{g}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sugawara.Kil}.} Let $\left(  c_{1}%
,c_{2},...,c_{m}\right)  $ be a basis of the vector space $\mathfrak{g}$, and
let $\left(  c_{1}^{\ast},c_{2}^{\ast},...,c_{m}^{\ast}\right)  $ be the dual
basis of $\mathfrak{g}^{\ast}$. Then, every $i\in\left\{  1,2,...,n\right\}  $
satisfies
\[
\operatorname*{Kil}\left(  b,p_{i}\right)  =\operatorname*{Tr}\left(  \left(
\operatorname*{ad}b\right)  \circ\left(  \operatorname*{ad}p_{i}\right)
\right)  =\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left(  \left(
\operatorname*{ad}b\right)  \circ\left(  \operatorname*{ad}p_{i}\right)
\right)  \left(  c_{j}\right)  \right)  =\sum\limits_{j=1}^{m}c_{j}^{\ast
}\left(  \left[  b,\left[  p_{i},c_{j}\right]  \right]  \right)  .
\]
Hence,%
\begin{align*}
\sum\limits_{i=1}^{n}\operatorname*{Kil}\left(  b,p_{i}\right)  q_{i}  &
=\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left[
b,\left[  p_{i},c_{j}\right]  \right]  \right)  q_{i}=\sum\limits_{j=1}%
^{m}\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[  b,\left[  p_{i}%
,c_{j}\right]  \right]  \right)  q_{i}\\
&  =-\sum\limits_{j=1}^{m}\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  \left[  q_{i},c_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sum\limits_{i=1}^{n}p_{i}\otimes q_{i}\text{ is }\mathfrak{g}%
\otimes\mathfrak{g}\text{-invariant, so that}\\
\sum\limits_{i=1}^{n}\left[  p_{i},c_{j}\right]  \otimes q_{i}=\sum
\limits_{i=1}^{n}p_{i}\otimes\left[  q_{i},c_{j}\right]  \text{ for every
}j\in\left\{  1,2,...,m\right\}
\end{array}
\right) \\
&  =-\sum\limits_{j=1}^{m}\sum\limits_{i=1}^{n}\left[  q_{i},c_{j}^{\ast
}\left(  \left[  b,p_{i}\right]  \right)  c_{j}\right]  =-\sum\limits_{i=1}%
^{n}\left[  q_{i},\underbrace{\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  c_{j}}_{=\left[  b,p_{i}\right]  }\right] \\
&  =-\sum\limits_{i=1}^{n}\left[  q_{i},\left[  b,p_{i}\right]  \right]
=\sum\limits_{i=1}^{n}\left[  \left[  b,p_{i}\right]  ,q_{i}\right]  ,
\end{align*}
which proves Lemma \ref{lem.sugawara.Kil}.

\textit{Proof of Theorem \ref{thm.sugawara}.} Let us first show that $\left[
b_{r},L_{n}\right]  =-b_{n+r}$ for every $b\in\mathfrak{g}$ and $r,n\in
\mathbb{Z}$.

Indeed, we must be careful here with infinite sums, since not even formal
algebra allows us to manipulate infinite sums like $\sum\limits_{m\in
\mathbb{Z}}\left[  b,a\right]  _{r+m}a_{n-m}$ (for good reasons: these are
divergent in every meaning of this word). While we were working in the
Heisenberg algebra $\mathcal{A}$ (which can be written as
$\widehat{\mathfrak{g}}$ for $\mathfrak{g}$ being the trivial Lie algebra
$\mathbb{C}$), these infinite sums made sense due to all of their addends
being $0$ (since $\left[  b,a\right]  =0$ for all $a$ and $b$ lying in the
trivial Lie algebra $\mathbb{C}$). But this was an exception rather than the
rule, and now we need to take care.

We have (with $\lim\limits_{N\rightarrow\infty}$ means
pointwise\footnote{pointwise = acting on each element of $M$} stabilization)%
\begin{align*}
\left[  b_{r},L_{n}\right]   &  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty
}\sum\limits_{a\in B}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}\underbrace{\left[  b_{r},\left.  :a_{m}a_{n-m}:\right.  \right]
}_{\substack{=\left[  b_{r},a_{m}a_{n-m}\right]  \\\text{(since }a_{m}%
a_{n-m}\text{ differs from }\left.  :a_{m}a_{n-m}:\right.  \\\text{only by a
scalar multiple of }K\text{, and scalar multiples of }K\text{ are central)}%
}}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\underbrace{\left[
b_{r},a_{m}a_{n-m}\right]  }_{=\left[  b_{r},a_{m}\right]  a_{n-m}%
+a_{m}\left[  b_{r},a_{n-m}\right]  }\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\underbrace{\left[  b_{r},a_{m}\right]  }_{=\left[  b,a\right]  _{r+m}%
+K\omega\left(  b_{r},a_{m}\right)  }a_{n-m}+a_{m}\underbrace{\left[
b_{r},a_{n-m}\right]  }_{=\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{n-m}\right)  }\right) \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}%
+a_{m}\left[  b,a\right]  _{n+r-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)
\right) \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\left(  \sum\limits_{a\in
B}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here we are using sufficiently large
}N\right)  .
\end{align*}
But since $\sum\limits_{a\in B}a\otimes a$ is $\mathfrak{g}$-invariant in
$\mathfrak{g}\otimes\mathfrak{g}$ (because the basis $B$ is orthonormal with
respect to a $\mathfrak{g}$-invariant bilinear form), we have $\sum
\limits_{a\in B}\left(  \left[  b,a\right]  \otimes a+a\otimes\left[
b,a\right]  \right)  =0$ and thus $\sum\limits_{a\in B}\left(  \left[
b,a\right]  _{\ell}\otimes a_{s}+a_{\ell}\otimes\left[  b,a\right]
_{s}\right)  =0$ for all $\ell,s\in\mathbb{Z}$. Thus, the above computation
becomes
\begin{align*}
&  \left[  b_{r},L_{n}\right] \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}\right) \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}-\left[  b,a\right]  _{m}a_{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}\right) \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}-\left[  b,a\right]  _{m}a_{n+r-m}\right)
+\sum\limits_{a\in B}rk\cdot\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in
B}\underbrace{\left(  \sum\limits_{\left\vert m-r-\dfrac{n}{2}\right\vert \leq
N}\left[  b,a\right]  _{m}a_{n+r-m}-\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left[  b,a\right]  _{m}a_{n+r-m}\right)  }%
_{\substack{=-\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}%
+N-r}\left[  b,a\right]  _{m}a_{n+r-m}\\\text{(by a generalized telescope
trick)}}}+\sum\limits_{a\in B}rk\cdot\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
-\underbrace{\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}}_{\substack{\rightarrow0\text{ for }N\rightarrow
\infty\\\text{(because convergence is pointwise,}\\\text{and }M\text{ is
admissible)}}}+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}%
\underbrace{\left[  b,a\right]  _{m}a_{n+r-m}}_{=a_{n+r-m}\left[  b,a\right]
_{m}+\left[  \left[  b,a\right]  _{m},a_{n+r-m}\right]  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{a\in B}rk\cdot\left(  b,a\right)
a_{n+r}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  a_{n+r-m}\left[
b,a\right]  _{m}+\underbrace{\left[  \left[  b,a\right]  _{m},a_{n+r-m}%
\right]  }_{=\left[  \left[  b,a\right]  ,a\right]  _{n+r}+\omega\left(
\left[  b,a\right]  _{m},a_{n+r-m}\right)  }\right)  +\sum\limits_{a\in
B}rk\cdot\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  a_{n+r-m}\left[
b,a\right]  _{m}+\left[  \left[  b,a\right]  ,a\right]  _{n+r}+\omega\left(
\left[  b,a\right]  _{m},a_{n+r-m}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{a\in B}rk\cdot\left(  b,a\right)
a_{n+r}.
\end{align*}


Now recall that $\left(  \left[  b,a\right]  ,a\right)  =0$ (since $\left(
\left[  b,a\right]  ,c\right)  +\left(  a,\left[  b,c\right]  \right)  =0$ due
to the $\mathfrak{g}$-invariance of $\left(  \cdot,\cdot\right)  $, and we can
set $c=a$ in this to get $2\left(  \left[  b,a\right]  ,a\right)  =0$). Hence,
$\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)  =0$. The above
computation thus becomes%
\begin{align*}
&  \left[  b_{r},L_{n}\right] \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(
\underbrace{a_{n+r-m}\left[  b,a\right]  _{m}}_{\substack{\rightarrow0\text{
as }N\rightarrow\infty\\\text{(again for reasons of admissibility)}}}+\left[
\left[  b,a\right]  ,a\right]  _{n+r}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{a\in B}rk\cdot\left(  b,a\right)
a_{n+r}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in
B}\underbrace{\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[
\left[  b,a\right]  ,a\right]  _{n+r}}_{=r\left[  \left[  b,a\right]
,a\right]  _{n+r}}+\sum\limits_{a\in B}rk\cdot\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}\sum\limits_{a\in B}r\left[  \left[  b,a\right]  ,a\right]
_{n+r}+\sum\limits_{a\in B}rk\cdot\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}rt^{n+r}\left(  \dfrac{1}{2}\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(  b,a\right)
a\right)  .
\end{align*}
Now, we claim that $\dfrac{1}{2}\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(  b,a\right)  a=b$.

\textit{Proof of this claim:} We have%
\[
b=k\sum\limits_{a\in B}\left(  b,a\right)  a+\dfrac{1}{2}\sum\limits_{a\in
B}\operatorname*{Kil}\left(  b,a\right)  a.
\]
(\textit{Proof:} If $\left\langle \cdot,\cdot\right\rangle $ is any
nondegenerate inner product on a finite-dimensional vector space $V$ and $B$
is an orthonormal basis with respect to that product, then any vector $b\in V$
is equal to $\sum\limits_{a\in B}\left\langle b,a\right\rangle a$; we are now
applying this fact to the form $k\left(  \cdot,\cdot\right)  +\dfrac{1}%
{2}\operatorname*{Kil}$.)

Therefore it suffices to prove that $\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  =\sum\limits_{a\in B}\operatorname*{Kil}\left(
b,a\right)  a$. This follows from Lemma \ref{lem.sugawara.Kil} (applied to the
invariant tensor $\sum\limits_{a\in B}a\otimes a$ in lieu of $\sum
\limits_{i=1}^{n}p_{i}\otimes q_{i}$).

Thus we have proven the $L_{n}\circ a_{m}=-ma_{n+m}$ relation. Obviously,
$L_{n}\circ K=0$ since $K$ acts as a scalar.

Now we must prove the Virasoro relations $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}+\left(  \text{something}\right)  $.

Let $R_{n,m}=\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$. Since
$\left[  L_{n},a_{r}\right]  =-ra_{n+r}$, it is clear that $R_{n,m}$ commutes
with $a_{r}$ for all $a$ and $r$. In particular, this yields that $R_{n,m}$
commutes with $L_{0}$ (since $L_{0}$ is made out of $a_{r}$'s). But $\left[
L_{0},R_{n,m}\right]  =\left(  n+m\right)  R_{n,m}$ (since $\left[
a_{r},L_{0}\right]  =ra_{r}$ and thus $\left[  L_{n},L_{0}\right]  =nL_{n}$).
Hence, $R_{n,m}=0$ whenever $n+m\neq0$.

The only case that remains to be checked is $n+m=0$. So we need to compute
$\left[  L_{n},L_{-n}\right]  -2nL_{0}$.

We write%
\begin{align*}
&  \left[  L_{n},L_{-n}\right]  -2nL_{0}=R_{n,-n}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left(
-m\right)  a_{n+m}a_{-n-m}+\left(  n+m\right)  a_{m}a_{-m}\right)  -2nL_{0}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{3n}{2}\right\vert \leq N}\left(  -m+n\right)
a_{m}a_{-m}+\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
n+m\right)  a_{m}a_{-m}\right)  -2nL_{0}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{3n}{2}-N}\left(
m+n\right)  a_{m}a_{-m}}_{\rightarrow0}+\sum\limits_{\dfrac{n}{2}+N\leq
m<\dfrac{3n}{2}+N}\left(  -m+n\right)  a_{m}a_{-m}+\sum\limits_{1\leq
m\leq\dfrac{n}{2}+N}2nmk\left(  a,a\right)  \right) \\
&  =\operatorname*{constant}.
\end{align*}
We calculate this constant: It is enough to calculate it for $n=1$ and for
$n=2$. For $n=1$, it is $\dfrac{1}{2}\left(  -N\left(  N+1\right)  +N\left(
N+1\right)  \right)  \cdot k\sum\limits_{a\in B}\left(  a,a\right)  =0$. For
$n=2$, it is
\[
\dfrac{1}{2}\underbrace{\left(  -N\left(  N+2\right)  -\left(  N+1\right)
\left(  N+3\right)  +2\left(  N+2\right)  \left(  N+1\right)  \right)  }%
_{=1}\cdot k\sum\limits_{a\in B}\left(  a,a\right)  =\dfrac{1}{2}\cdot
k\sum\limits_{a\in B}\left(  a,a\right)  .
\]
Thus, $R_{1,-1}=0$ and $R_{2,-2}=\dfrac{1}{2}\cdot k\sum\limits_{a\in
B}\left(  a,a\right)  $. As a consequence, $R_{n,-n}=\dfrac{n^{3}-n}{12}%
k\sum\limits_{a\in B}\left(  a,a\right)  $. (We could have also obtained this
by direct computation.) This proves that, with $C=k\sum\limits_{a\in B}\left(
a,a\right)  $, we get a representation of $\operatorname*{Vir}$.

We are now going to specialize these results to the case of $\mathfrak{g}$
being simple. In this case, the so-called \textit{dual Coxeter number} of the
simple Lie algebra $\mathfrak{g}$ comes into play. Let us explain what this is:

\begin{definition}
\label{def.dualcox}Let $\mathfrak{g}$ be a simple Lie algebra. Let $\theta$ be
the maximal root of $\mathfrak{g}$. (In other words, let $\theta$ be the
highest weight of the adjoint representation of $\mathfrak{g}$.) Let
$\rho=\dfrac{1}{2}\sum\limits_{\substack{\alpha\text{ root of }\mathfrak{g}%
\text{;}\\\alpha>0}}\alpha$ be the half-sum of all positive roots. The
\textit{dual Coxeter number} $h^{\vee}$ of $\mathfrak{g}$ is defined by
$h^{\vee}=1+\left(  \theta,\rho\right)  $. It is easy to show that $h^{\vee}$
is a positive integer.
\end{definition}

\begin{definition}
\label{def.standform}Let $\mathfrak{g}$ be a simple Lie algebra. The
\textit{standard form} on $\mathfrak{g}$ will mean the scaling of the Killing
form under which $\left(  \alpha,\alpha\right)  $ (under the inverse form)
equals $2$ for long roots $\alpha$. (We do not care to define what a long root
is, but it is enough to say that the maximal root $\theta$ is a long root, and
this is clearly enough to define the standard form.)

(The inverse form means the form on $\mathfrak{g}^{\ast}=\mathfrak{h}^{\ast
}\oplus\mathfrak{n}_{+}^{\ast}\oplus\mathfrak{n}_{-}^{\ast}$ obtained by
dualizing the Killing form on $\mathfrak{g}=\mathfrak{h}\oplus\mathfrak{n}%
_{+}\oplus\mathfrak{n}_{-}$ using itself.)

We are going to denote the standard form by $\left(  \cdot,\cdot\right)  $.
\end{definition}

\begin{lemma}
\label{lem.dualcox}Let $B$ be an orthonormal basis of $\mathfrak{g}$ with
respect to the standard form. Let $C=\sum\limits_{a\in B}a^{2}\in U\left(
\mathfrak{g}\right)  $. This element $C$ is known to be central in $U\left(
\mathfrak{g}\right)  $ (this is easily checked), and is called the
\textit{quadratic Casimir}.

Then:

\textbf{(1)} For every $\lambda\in\mathfrak{h}^{\ast}$, the element $C\in
U\left(  \mathfrak{g}\right)  $ acts on $L_{\lambda}$ by $\left(
\lambda,\lambda+2\rho\right)  \cdot\operatorname*{id}$. (Here, $L_{\lambda}$
means $L_{\lambda}^{+}$, but actually can be replaced by any highest-weight
module with highest weight $\lambda$.)

\textbf{(2)} The element $C\in U\left(  \mathfrak{g}\right)  $ acts on the
adjoint representation $\mathfrak{g}$ by $2h^{\vee}\cdot\operatorname*{id}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.dualcox}.} If $\left(  b_{i}\right)  _{i\in
I}$ is any basis of $\mathfrak{g}$, and $\left(  b_{i}^{\ast}\right)  _{i\in
I}$ is the dual basis of $\mathfrak{g}$ with respect to the standard form
$\left(  \cdot,\cdot\right)  $, then%
\begin{equation}
C=\sum\limits_{i\in I}b_{i}b_{i}^{\ast}. \label{pf.dualcox.Csum}%
\end{equation}


Let us refine the triangular decomposition $\mathfrak{g}=\mathfrak{h}%
\oplus\mathfrak{n}_{+}\oplus\mathfrak{n}_{-}$ to $\mathfrak{g}=\mathfrak{h}%
\oplus\left(  \bigoplus\limits_{\alpha>0}\mathfrak{g}_{\alpha}\right)
\oplus\left(  \bigoplus\limits_{\alpha<0}\mathfrak{g}_{\alpha}\right)  $,
where $\mathfrak{g}_{\alpha}=\mathbb{C}e_{\alpha}$ for roots $\alpha>0$, and
$\mathfrak{g}_{-\alpha}=\mathbb{C}f_{\alpha}$ for roots $\alpha>0$. (This is
standard theory of simple Lie algebras.) Normalize the $f_{\alpha}$ so that
$\left(  e_{\alpha},f_{\alpha}\right)  =1$.

Fix an orthonormal basis $\left(  x_{r}\right)  _{i\in\left\{
1,2,...,r\right\}  }$ of $\mathfrak{h}$. Extend it to a basis $\left(
x_{r}\right)  _{i\in\left\{  1,2,...,r\right\}  }\cup\left(  e_{\alpha
}\right)  _{\alpha>0}\cup\left(  f_{\alpha}\right)  _{\alpha>0}$ (where the
index $\alpha$ runs over positive roots only). Since%
\begin{align*}
\left(  e_{\alpha},x_{i}\right)   &  =\left(  f_{\alpha},x_{i}\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,...,r\right\}  \text{
and roots }\alpha>0;\\
\left(  e_{\alpha},f_{\beta}\right)   &  =0\ \ \ \ \ \ \ \ \ \ \text{for any
two distinct roots }\alpha>0\text{ and }\beta>0;\\
\left(  e_{\alpha},e_{\gamma}\right)   &  =\left(  f_{\alpha},f_{\gamma
}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for any roots }\alpha>0\text{ and
}\gamma>0;\\
\left(  x_{i},x_{j}\right)   &  =\delta_{i,j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i\in\left\{  1,2,...,r\right\}  \text{ and }j\in\left\{  1,2,...,r\right\}
;\\
\left(  e_{\alpha},f_{\alpha}\right)   &  =\left(  f_{\alpha},e_{\alpha
}\right)  =1\ \ \ \ \ \ \ \ \ \ \text{for any root }\alpha>0\text{,}%
\end{align*}
we see that $\left(  x_{r}\right)  _{i\in\left\{  1,2,...,r\right\}  }%
\cup\left(  f_{\alpha}\right)  _{\alpha>0}\cup\left(  e_{\alpha}\right)
_{\alpha>0}$ is the dual basis to this basis $\left(  x_{r}\right)
_{i\in\left\{  1,2,...,r\right\}  }\cup\left(  e_{\alpha}\right)  _{\alpha
>0}\cup\left(  f_{\alpha}\right)  _{\alpha>0}$. Thus, (\ref{pf.dualcox.Csum})
yields%
\[
C=\sum\limits_{i=1}^{n}x_{i}^{2}+\sum\limits_{\alpha>0}\left(  f_{\alpha
}e_{\alpha}+e_{\alpha}f_{\alpha}\right)  ,
\]
so that (denoting $v_{\lambda}^{+}$ by $v_{\lambda}$) we have%
\begin{align*}
Cv_{\lambda}  &  =\underbrace{\sum\limits_{i=1}^{n}\lambda\left(
x_{i}\right)  ^{2}}_{=\left(  \lambda,\lambda\right)  }v_{\lambda}%
+\sum\limits_{\alpha>0}\left(  f_{\alpha}\underbrace{e_{\alpha}v_{\lambda}%
}_{=0}+\underbrace{e_{\alpha}f_{\alpha}}_{=f_{\alpha}e_{\alpha}+\left[
e_{\alpha},f_{\alpha}\right]  }v_{\lambda}\right) \\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha>0}\left(
f_{\alpha}e_{\alpha}+\left[  e_{\alpha},f_{\alpha}\right]  \right)
v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}f_{\alpha}\underbrace{e_{\alpha}v_{\lambda}}_{=0}+\sum\limits_{\alpha
>0}\underbrace{\left[  e_{\alpha},f_{\alpha}\right]  }_{=h_{\alpha}}%
v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}\underbrace{h_{\alpha}}_{=\lambda\left(  h_{\alpha}\right)  =\left(
\lambda,\alpha\right)  }v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha>0}\left(
\lambda,\alpha\right)  v_{\lambda}=\underbrace{\left(  \left(  \lambda
,\lambda\right)  +\sum\limits_{\alpha>0}\left(  \lambda,\alpha\right)
\right)  }_{=\left(  \lambda,\lambda+2\rho\right)  }v_{\lambda}=\left(
\lambda,\lambda+2\rho\right)  v_{\lambda}.
\end{align*}
Thus, every $a\in U\left(  \mathfrak{g}\right)  $ satisfies
\begin{align*}
Cav_{\lambda}  &  =a\underbrace{Cv_{\lambda}}_{=\left(  \lambda,\lambda
+2\rho\right)  v_{\lambda}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }C\text{ is
central in }U\left(  \mathfrak{g}\right)  \right) \\
&  =\left(  \lambda,\lambda+2\rho\right)  av_{\lambda}.
\end{align*}
Hence, $C$ acts as $\left(  \lambda,\lambda+2\rho\right)  \cdot
\operatorname*{id}$ on $L_{\lambda}$ (because every element of $L_{\lambda}$
has the form $av_{\lambda}$ for some $a\in U\left(  \mathfrak{g}\right)  $).
This proves Lemma \ref{lem.dualcox} \textbf{(1)}.

We have $\mathfrak{g}=L_{\theta}$, and thus Lemma \ref{lem.dualcox}
\textbf{(1)} yields%
\[
C\mid_{L_{\theta}}=\left(  \theta,\theta+2\rho\right)  =\underbrace{\left(
\theta,\theta\right)  }_{=2}+2\left(  \theta,\rho\right)  =2+2\left(
\theta,\rho\right)  =2h^{\vee}.
\]
This proves Lemma \ref{lem.dualcox} \textbf{(2)}.

Here is a little table of dual Coxeter numbers, depending on the root system
type of $\mathfrak{g}$:

For $A_{n-1}$, we have $h^{\vee}=n$.

For $B_{n}$, we have $h^{\vee}=2n-1$.

For $C_{n}$, we have $h^{\vee}=n+1$.

For $D_{n}$, we have $h^{\vee}=2n-2$.

For $E_{6}$, we have $h^{\vee}=12$.

For $E_{7}$, we have $h^{\vee}=18$.

For $E_{8}$, we have $h^{\vee}=30$.

For $F_{4}$, we have $h^{\vee}=9$.

For $G_{2}$, we have $h^{\vee}=4$.

Every Lie theorist is supposed to remember these by heart.

\begin{lemma}
\label{lem.dualcox.kil}Let $\mathfrak{g}$ be a simple Lie algebra. Then,%
\[
\operatorname*{Kil}\left(  a,b\right)  =2h^{\vee}\cdot\left(  a,b\right)
\ \ \ \ \ \ \ \ \ \ \text{for any }a,b\in\mathfrak{g}.
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.dualcox.kil}.} Let $B$ be an orthonormal basis
of $\mathfrak{g}$ with respect to the standard form. Then, $C=\sum
\limits_{a\in B}a^{2}$, so that
\[
\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  C\right)  =\sum\limits_{a\in
B}\underbrace{\operatorname*{Tr}\left(  \left(  \operatorname*{ad}a\right)
\circ\left(  \operatorname*{ad}a\right)  \right)  }_{=\operatorname*{Kil}%
\left(  a,a\right)  }=\sum\limits_{a\in B}\operatorname*{Kil}\left(
a,a\right)  .
\]
Comparing this with%
\begin{align*}
\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  C\right)   &  =2h^{\vee
}\underbrace{\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(
\operatorname*{id}\right)  }_{=\dim\mathfrak{g}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }C\mid_{\mathfrak{g}}=2h^{\vee}\operatorname*{id}\text{ by Lemma
\ref{lem.dualcox} \textbf{(2)}}\right) \\
&  =2h^{\vee}\dim\mathfrak{g}=2h^{\vee}\sum\limits_{a\in B}\left(  a,a\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  a,a\right)  =1\text{ for
every }a\in B\right)  ,
\end{align*}
this yields $\sum\limits_{a\in B}\operatorname*{Kil}\left(  a,a\right)
=2h^{\vee}\sum\limits_{a\in B}\left(  a,a\right)  $. Since
$\operatorname*{Kil}$ is a scalar multiple of $\left(  \cdot,\cdot\right)  $
(because there is only one $\mathfrak{g}$-invariant symmetric bilinear form on
$\mathfrak{g}$ up to scaling), this yields $\operatorname*{Kil}=2h^{\vee}%
\cdot\left(  \cdot,\cdot\right)  $. Lemma \ref{lem.dualcox.kil} is proven.

So let us now look at the Sugawara construction when $\mathfrak{g}$ is simple.
First of all, $k$ is noncritical if and only if $k\neq-h^{\vee}$. (The value
$k=-h^{\vee}$ is called the \textit{critical level}.)

If $B^{\prime}$ is an orthonormal basis under $\left(  \cdot,\cdot\right)  $
(rather than under $k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}%
\operatorname*{Kil}=\left(  k+h^{\vee}\right)  \left(  \cdot,\cdot\right)  $),
then we have%
\begin{align*}
L_{n}  &  =\dfrac{1}{2\left(  k+h^{\vee}\right)  }\sum\limits_{a\in B^{\prime
}}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.
\ \ \ \ \ \ \ \ \ \ \text{and}\\
C  &  =\dfrac{k}{k+h^{\vee}}\sum\limits_{a\in B^{\prime}}\left(  a,a\right)
=\dfrac{k}{k+h^{\vee}}\left\vert B^{\prime}\right\vert =\dfrac{k\dim
\mathfrak{g}}{k+h^{\vee}}.
\end{align*}
In particular, we have an internal grading on any module which is a quotient
of $M_{\lambda}^{+}$ by eigenvalues of $L_{0}$. This is a grading by complex
numbers, since eigenvalues of $L_{0}$ are not necessarily integers. (I have
not checked whether this works for more general modules than quotients of
$M_{\lambda}^{+}$, like admissible modules.)

What happens at the critical level $k=-h^{\vee}$ ? The above formulas with
$k+h^{\vee}$ in the denominators clearly don't work at this level anymore. We
can, however, remove the denominators, i. e., consider the operators%
\[
T_{n}=\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  .
\]
Then, the same calculation as we did in the proof of Theorem
\ref{thm.sugawara} tell us that these $T_{n}$ satisfy $\left[  T_{n}%
,a_{m}\right]  =0$ and $\left[  T_{n},T_{m}\right]  =0$; they are thus central
"elements" of $U\left(  \widehat{\mathfrak{g}}\right)  $ (except that they are
not actually elements of $U\left(  \widehat{\mathfrak{g}}\right)  $, but of
some completion acting on admissible modules).

We can construct a module $M_{\lambda}\diagup\left(  \sum\limits_{m\geq
1}\left(  \left(  T_{m}-\gamma_{m}\right)  M\right)  \right)  $ over
$\widehat{\mathfrak{g}}$ (where $\gamma_{1},\gamma_{2},...,\gamma_{m}$ are
complex numbers), which does not have a grading. So, at the critical level, we
do not get gradings anymore. This is one reason why representations at the
critical level are considered more difficult than those at non-critical levels.

\subsection{The Sugawara construction and unitarity}

We now will show that the Sugawara construction preserves unitarity:

\begin{proposition}
Consider the situation of Theorem \ref{thm.sugawara}. If $M$ is a unitary
admissible module for $\widehat{\mathfrak{g}}$, then $M$ is a unitary
$\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$-module. (We recall that the
Virasoro algebra had its unitary structure given by $L_{n}^{\dag}=L_{-n}$.)
\end{proposition}

But for $M$ to be unitary for $\widehat{\mathfrak{g}}$, we need $k\in
\mathbb{Z}_{+}$ (this is easy to prove; we proved it for $\mathfrak{sl}_{n}$,
and the general case is similar). Since for $k=0$, there is only the trivial
representation, we really must require $k\geq1$ to get something interesting.
And since $c=\dfrac{k\dim\mathfrak{g}}{k+h^{\vee}}$, the $c$ is then $\geq1$,
since $\dim\mathfrak{g}\geq1+h^{\vee}$. These modules are already known to us
to be unitary, so this construction does not help us in constructing new
unitary modules.

But there is a way to amend this by a variation of the Sugawara construction:
the Goddard-Kent-Olive construction.

\subsection{The Goddard-Kent-Olive construction (a.k.a. the coset
construction)}

\begin{definition}
Let $\mathfrak{g}$ and $\mathfrak{p}$ be two finite-dimensional Lie algebras
such that $\mathfrak{g}\supseteq\mathfrak{p}$. Assume that $\mathfrak{g}$ has
a nondegenerate $\mathfrak{g}$-invariant form $\left(  \cdot,\cdot\right)  $.
We can restrict this form to $\mathfrak{p}$, and have $\widehat{\mathfrak{g}%
}\supseteq\widehat{\mathfrak{p}}$. Choose a level $k$ which is noncritical for
both $\mathfrak{g}$ and $\mathfrak{p}$.

If $M$ is an admissible $\widehat{\mathfrak{g}}$-module at level $k$, then on
$M$ we have two Virasoro actions: One is obtained from the
$\widehat{\mathfrak{g}}$-action, and one which is obtained from the
$\widehat{\mathfrak{p}}$-action. We will denote these actions by $\left(
L_{i}^{\mathfrak{g}}\right)  _{i\in\mathbb{Z}}$ and $\left(  L_{i}%
^{\mathfrak{p}}\right)  _{i\in\mathbb{Z}}$, respectively, and we will denote
their central charges by $c_{\mathfrak{g}}$ and $c_{\mathfrak{p}}$, respectively.
\end{definition}

\begin{theorem}
\label{thm.goddardkentolive}Let $L_{i}=L_{i}^{\mathfrak{g}}-L_{i}%
^{\mathfrak{p}}$ for all $i\in\mathbb{Z}$. Then, $\left(  L_{i}\right)
_{i\in\mathbb{Z}}$ is a $\operatorname*{Vir}$-action on $M$ with central
charge $c=c_{\mathfrak{g}}-c_{\mathfrak{p}}$.

Also, $\left[  L_{n},\widehat{p}\right]  =0$ for all $\widehat{p}%
\in\widehat{\mathfrak{g}}$.

Moreover, $\left[  L_{n},L_{m}^{\mathfrak{p}}\right]  =0$ for all
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.goddardkentolive}.} Suppose $p\in
\mathfrak{p}$. Then,%
\[
\left[  L_{n},p_{m}\right]  =\underbrace{\left[  L_{n}^{\mathfrak{g}}%
,p_{m}\right]  }_{=-mp_{m+n}}-\underbrace{\left[  L_{n}^{\mathfrak{p}}%
,p_{m}\right]  }_{=-mp_{m+n}}=\left(  -mp_{m+n}\right)  -\left(
-mp_{m+n}\right)  =0.
\]
Thus, $L_{n}$ commutes with $L_{m}^{\mathfrak{p}}$ (because we defined
$L_{m}^{\mathfrak{p}}$ as a noncommutative power series in the $p_{i}$'s for
various $p$ and $i$). In other words, $\left[  L_{n},L_{m}^{\mathfrak{p}%
}\right]  =0$. Hence,
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =\left[  L_{n},L_{m}^{\mathfrak{g}}%
-L_{m}^{\mathfrak{p}}\right]  =\left[  L_{n},L_{m}^{\mathfrak{g}}\right]
-\underbrace{\left[  L_{n},L_{m}^{\mathfrak{p}}\right]  }_{=0}=\left[
L_{n},L_{m}^{\mathfrak{g}}\right] \\
&  =\left[  L_{n}^{\mathfrak{g}}-L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}%
}\right]  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
-\underbrace{\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}}\right]
}_{\substack{=\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}}-L_{m}%
^{\mathfrak{p}}\right]  +\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}%
}\right]  \\\text{(since }L_{m}^{\mathfrak{g}}=\left(  L_{m}^{\mathfrak{g}%
}-L_{m}^{\mathfrak{p}}\right)  +L_{m}^{\mathfrak{p}}\text{)}}}\\
&  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]  -\left[
L_{n}^{\mathfrak{p}},\underbrace{L_{m}^{\mathfrak{g}}-L_{m}^{\mathfrak{p}}%
}_{=L_{m}}\right]  -\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}}\right]
\\
&  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
-\underbrace{\left[  L_{n}^{\mathfrak{p}},L_{m}\right]  }_{=0}-\left[
L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}}\right] \\
&  =\underbrace{\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
}_{=\left(  n-m\right)  L_{n+m}^{\mathfrak{g}}-\dfrac{n^{3}-n}{12}%
c_{\mathfrak{g}}\delta_{n,-m}}-\underbrace{\left[  L_{n}^{\mathfrak{p}}%
,L_{m}^{\mathfrak{p}}\right]  }_{=\left(  n-m\right)  L_{n+m}^{\mathfrak{p}%
}-\dfrac{n^{3}-n}{12}c_{\mathfrak{p}}\delta_{n,-m}}\\
&  =\left(  n-m\right)  \underbrace{\left(  L_{n+m}^{\mathfrak{g}}%
-L_{n+m}^{\mathfrak{p}}\right)  }_{=L_{n+m}}-\dfrac{n^{3}-n}{12}%
\underbrace{\left(  c_{\mathfrak{g}}-c_{\mathfrak{p}}\right)  }_{=c}%
\delta_{n,-m}\\
&  =\left(  n-m\right)  L_{n+m}-\dfrac{n^{3}-n}{12}c\delta_{n,m}.
\end{align*}
This proves that we actually have a $\operatorname*{Vir}$-action.

From $\left[  L_{n},p_{m}\right]  =0$, we get $\left[  L_{n},\widehat{p}%
\right]  =0$. Theorem \ref{thm.goddardkentolive} is proven.

\begin{example}
Let $\mathfrak{a}$ be a simple Lie algebra. Let $\mathfrak{g}=\mathfrak{a}%
\oplus\mathfrak{a,}$ and let $\mathfrak{p}=\mathfrak{a}_{\operatorname*{diag}%
}\subseteq\mathfrak{a}\oplus\mathfrak{a}$.

Let $V^{\prime}$ and $V^{\prime\prime}$ be admissible $\widehat{\mathfrak{a}}%
$-modules at levels $k^{\prime}$ and $k^{\prime\prime}$ with central charges
$c_{\mathfrak{a}}^{\prime}$ and $c_{\mathfrak{a}}^{\prime\prime}$.

Then, $V=V^{\prime}\otimes V^{\prime\prime}$ is a $\widehat{\mathfrak{g}}%
$-module with central charge $c_{\mathfrak{g}}=c_{\mathfrak{a}}^{\prime
}+c_{\mathfrak{a}}^{\prime\prime}=\dfrac{k^{\prime}\dim\mathfrak{a}}%
{k^{\prime}+h^{\vee}}+\dfrac{k^{\prime\prime}\dim\mathfrak{a}}{k^{\prime
\prime}+h^{\vee}}$. The action is given by $L_{i}=L_{i}^{\prime}+L_{i}%
^{\prime\prime}$.

Since $\widehat{\mathfrak{p}}=\widehat{\mathfrak{a}}$ acts on $V^{\prime
}\otimes V^{\prime\prime}$ by diagonal action, the level is $k^{\prime
}+k^{\prime\prime}$ and the central charge is%
\[
c_{\mathfrak{p}}=\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime}+k^{\prime
\prime}+h^{\vee}}\dim\mathfrak{a}.
\]
Thus, the central charge $c$ of the $\operatorname*{Vir}$-action given by
Theorem \ref{thm.goddardkentolive} is%
\begin{align*}
c  &  =c_{\mathfrak{a}}^{\prime}+c_{\mathfrak{a}}^{\prime\prime}%
-c_{\mathfrak{p}}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}%
}+\dfrac{k^{\prime\prime}\dim\mathfrak{a}}{k^{\prime\prime}+h^{\vee}}%
-\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime}+k^{\prime\prime}+h^{\vee}}%
\dim\mathfrak{a}\\
&  =\left(  \dfrac{k^{\prime}}{k^{\prime}+h^{\vee}}+\dfrac{k^{\prime\prime}%
}{k^{\prime\prime}+h^{\vee}}-\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime
}+k^{\prime\prime}+h^{\vee}}\right)  \dim\mathfrak{a}.
\end{align*}


For example, let $\mathfrak{a}=\mathfrak{sl}_{2}$, so that $h^{\vee}=2$, and
let $k^{\prime}=1$ and $k^{\prime\prime}=m$ with $m\geq1$. In this case, we
get%
\[
c=3\left(  \dfrac{1}{3}+\dfrac{m}{m+2}-\dfrac{m+1}{m+3}\right)  =1-\dfrac
{6}{\left(  m+2\right)  \left(  m+3\right)  }.
\]
So we get unitary representations for these values of $c$.
\end{example}

\subsection{Kac-Moody Lie algebras}

\subsubsection{Simple Lie algebras: a summary}

The Kac-Moody Lie algebras form a class of Lie algebras which contains (or
extends?) simple and affine Lie algebras, but also many more. Before we start
studying them, let us recall some facts about simple Lie algebras:

Let $\mathfrak{g}$ be a finite simple Lie algebra over $\mathbb{C}$. A
\textit{Cartan subalgebra} $\mathfrak{h}$ of $\mathfrak{g}$ is a maximal
commutative subalgebra which consists of semisimple\footnote{An element of a
Lie algebra is said to be \textit{semisimple} if and only if its action on the
adjoint representation is a semisimple operator.} elements. There are usually
many Cartan subalgebras of $\mathfrak{g}$, but they are all conjugate under
the action of the corresponding Lie group $G$ (which satisfies $\mathfrak{g}%
=\operatorname*{Lie}G$, and can be defined as the connected component of the
identity in the group $\operatorname*{Aut}\mathfrak{g}$). Thus, there is no
loss of generality in picking one such subalgebra. So pick a Cartan subalgebra
$\mathfrak{h}$ of $\mathfrak{g}$. We denote the dimension $\dim\mathfrak{h}$
by $n$ and also by $\operatorname*{rank}\mathfrak{g}$.

For every $\alpha\in\mathfrak{h}^{\ast}$, we can define a vector subspace
$\mathfrak{g}_{\alpha}$ of $\mathfrak{g}$ by $\mathfrak{g}_{\alpha}=\left\{
a\in\mathfrak{g}\ \mid\ \left[  h,a\right]  =\alpha\left(  h\right)  a\text{
for all }h\in\mathfrak{h}\right\}  $. It can be shown that $\mathfrak{g}%
_{0}=\mathfrak{h}$. Now we can write $\mathfrak{g}=\mathfrak{h}\oplus
\bigoplus\limits_{\alpha\in\Delta}\mathfrak{g}_{\alpha}$ for some subset
$\Delta$ of $\mathfrak{h}^{\ast}\diagdown\left\{  0\right\}  $ (which is
chosen in such a way that $\mathfrak{g}_{\alpha}\neq0$ for all $\alpha
\in\Delta$). This subset $\Delta$ is called the \textit{root system} of
$\mathfrak{g}$. It is known that each $\mathfrak{g}_{\alpha}$ is
one-dimensional, given as $\mathfrak{g}_{\alpha}=\mathbb{C}e_{\alpha}$ for
some particular $e_{\alpha}\in\mathfrak{g}$.

We want to use the decomposition $\mathfrak{g}=\mathfrak{h}\oplus
\bigoplus\limits_{\alpha\in\Delta}\mathfrak{g}_{\alpha}$ in order to construct
a triangular decomposition of $\mathfrak{g}$. This can be done with the
grading which we constructed in Proposition \ref{prop.grad.g}, but let us do
it again now, with more elementary means: Fix an $\overline{h}\in\mathfrak{h}$
such that every $\alpha\in\Delta$ satisfies $\alpha\left(  \overline
{h}\right)  \in\mathbb{R}\diagdown\left\{  0\right\}  $ (it can be seen that
such $\overline{h}$ exists). Define $\Delta_{+}=\left\{  \alpha\in\Delta
\ \mid\ \alpha\left(  \overline{h}\right)  >0\right\}  $ and $\Delta
_{-}=\left\{  \alpha\in\Delta\ \mid\ \alpha\left(  \overline{h}\right)
<0\right\}  $. Then, $\Delta$ is the union of two disjoint subsets $\Delta
_{+}$ and $\Delta_{-}$, and we have $\Delta_{+}=-\Delta_{-}$. The triangular
decomposition of $\mathfrak{g}$ is now defined as $\mathfrak{g}=\mathfrak{n}%
_{-}\oplus\mathfrak{h}\oplus\mathfrak{n}_{+}$, where $\mathfrak{n}%
_{-}=\bigoplus\limits_{\alpha\in\Delta_{-}}\mathfrak{g}_{\alpha}$ and
$\mathfrak{n}_{+}=\bigoplus\limits_{\alpha\in\Delta_{+}}\mathfrak{g}_{\alpha}$.

Let us now construct the grading on $\mathfrak{g}$ which yields this
triangular decomposition. This grading was already constructed in Proposition
\ref{prop.grad.g}, but now we are going to do this in detail:

We define the \textit{simple roots} of $\mathfrak{g}$ as the elements of
$\Delta^{+}$ which cannot be written as sums of elements of $\Delta_{+}$
except in the trivial way. It can be shown that there are exactly $n$ of these
simple roots, and they form a basis of $\mathfrak{h}^{\ast}$. Denote these
simple roots as $\alpha_{1}$, $\alpha_{2}$, $...$, $\alpha_{n}$. Every root
$\alpha\in\Delta_{+}$ can now be written in the form $\alpha=\sum
\limits_{i=1}^{n}k_{i}\left(  \alpha\right)  \alpha_{i}$ for a unique
$n$-tuple $\left(  k_{1}\left(  \alpha\right)  ,k_{2}\left(  \alpha\right)
,...,k_{n}\left(  \alpha\right)  \right)  $ of nonnegative integers.

For all $\alpha,\beta\in\Delta$ with $\alpha+\beta\notin\Delta$, we have
$\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}\right]  =0$. For all
$\alpha,\beta\in\Delta$ with $\alpha+\beta\in\Delta$, we have $\left[
\mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}\right]  \subseteq\mathfrak{g}%
_{\alpha+\beta}$. In particular, for every $\alpha\in\Delta$, we have $\left[
\mathfrak{g}_{\alpha},\mathfrak{g}_{-\alpha}\right]  \subseteq\mathfrak{h}$.
Better yet, we can show that for every $\alpha\in\Delta$, there exists some
nonzero $h_{\alpha}\in\mathfrak{h}$ such that $\left[  \mathfrak{g}_{\alpha
},\mathfrak{g}_{-\alpha}\right]  =\mathbb{C}h_{\alpha}$.

For every $i\in\left\{  1,2,...,n\right\}  $, pick generators $e_{i}$ of
$\mathfrak{g}_{\alpha_{i}}$ and $f_{i}$ of $\mathfrak{g}_{-\alpha_{i}}$.

Normalze $e_{i}$ and $f_{i}$ in such a way that $\left[  h_{i},e_{i}\right]
=2e_{i}$ and $\left[  h_{i},f_{i}\right]  =-2f_{i}$, where $h_{i}=\left[
e_{i},f_{i}\right]  $. This $h_{i}$ will, of course, lie in $\mathfrak{h}$ and
be a scalar multiple of $h_{\alpha_{i}}$. We can renormalize $h_{\alpha_{i}}$
in such a way that $h_{i}=h_{\alpha_{i}}$.

\begin{proposition}
\label{prop.serre-gen.1}\textbf{1)} The family $\left(  h_{1},h_{2}%
,...,h_{n}\right)  $ is a basis of $\mathfrak{h}$.

\textbf{2)} The Lie algebra $\mathfrak{g}$ is generated (as a Lie algebra) by
the $e_{i}$, $f_{i}$ and $h_{i}$, and the following relations hold:%
\begin{align*}
\left[  h_{i},h_{j}\right]   &  =0;\\
\left[  h_{i},e_{j}\right]   &  =\alpha_{j}\left(  h_{i}\right)  e_{j}%
=a_{i,j}e_{j};\\
\left[  h_{i},f_{j}\right]   &  =-\alpha_{j}\left(  h_{i}\right)  f_{j};\\
\left[  e_{i},f_{j}\right]   &  =\delta_{i,j}h_{i}.
\end{align*}
(This does not mean that no more relations hold. In fact, there are additional
relations, the so-called Serre relations; we will see them later.)

Here, we denote $\alpha_{j}\left(  h_{i}\right)  $ by $a_{i,j}$. Then, the
$n\times n$ matrix $A=\left(  a_{i,j}\right)  $ is called the \textit{Cartan
matrix} of $\mathfrak{g}$.

Define an isomorphism $\mathfrak{h}\rightarrow\mathfrak{h}^{\ast}$ using the
standard invariant form on $\mathfrak{g}$ restricted to $\mathfrak{h}$ (we
don't care how this form is scaled, so we can just take the Killing form).
This isomorphism sends $h_{i}$ to $\alpha_{i}^{\vee}=\dfrac{2\alpha_{i}%
}{\left(  \alpha_{i},\alpha_{i}\right)  }$. Thus, $a_{i,j}=\alpha_{j}\left(
h_{i}\right)  =\dfrac{2\left(  \alpha_{j},\alpha_{i}\right)  }{\left(
\alpha_{i},\alpha_{i}\right)  }$.

The elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ are called \textit{Chevalley
generators} of $\mathfrak{g}$.

\textbf{Properties of the matrix }$A$\textbf{:}

\textbf{1)} We have $a_{i,i}=2$ for all $i$.

\textbf{2)} We have $a_{i,j}\leq0$ and $a_{i,j}\in\mathbb{Z}$. Also,
$a_{i,j}=0$ if and only if $a_{j,i}=0$.

\textbf{3)} The matrix $A$ is indecomposable (i. e., if conjugation of $A$ by
a permutation matrix brings $A$ into a block-diagonal form $\left(
\begin{array}
[c]{cc}%
A_{1} & 0\\
0 & A_{2}%
\end{array}
\right)  $, then either $A_{1}$ or $A_{2}$ is a $0\times0$ matrix).

\textbf{4)} The matrix $A$ is positive. Here is what we mean by this: There
exists a diagonal $n\times n$ matrix with positive diagonal entries such that
$DA$ is a symmetric and positive definite matrix.
\end{proposition}

\begin{theorem}
An $n\times n$ matrix satisfies these four properties if and only if it is a
Cartan matrix of a simple Lie algebras.
\end{theorem}

Such matrices (and thus, simple finite-dimensional Lie algebras) can be
encoded by so-called \textit{Dynkin diagrams}. The \textit{Dynkin diagram} of
the simple Lie algebra $\mathfrak{g}$ is the graph with vertex set $\left\{
1,2,...,n\right\}  $, and the following rules for edges:

If $a_{i,j}=0$, then the vertices $i$ and $j$ are not connected by an edge.

If $a_{i,j}=a_{j,i}=-1$, then the vertices $i$ and $j$ are connected by
exactly one edge.

If $a_{i,j}=-2$ and $a_{j,i}=-1$, then the vertices $i$ and $j$ are connected
by two directed edges from $j$ to $i$.

If $a_{i,j}=-3$ and $a_{j,i}=-1$, then the vertices $i$ and $j$ are connected
by three directed edges from $j$ to $i$.

Classification of simple finite-dimensional Lie algebras by Dynkin diagrams:

$A_{n}=\mathfrak{sl}\left(  n+1\right)  $ for $n\geq1$; the Dynkin diagram is
$\circ-\circ-\circ...\circ-\circ$.

$B_{n}=\mathfrak{so}\left(  2n+1\right)  $ for $n\geq2$; the Dynkin diagram is
$\circ-\circ-\circ...\circ=\circ$. (Note that $\mathfrak{so}\left(  3\right)
\cong\mathfrak{sl}\left(  2\right)  $.)

$C_{n}=\mathfrak{sp}\left(  2n\right)  $ for $n\geq2$; the Dynkin diagram is
$\circ-\circ-\circ...\circ\equiv\circ$. (Note that $\mathfrak{sp}\left(
2\right)  \cong\mathfrak{sl}\left(  2\right)  $ and $\mathfrak{sp}\left(
4\right)  \cong\mathfrak{so}\left(  5\right)  $.)

$D_{n}=\mathfrak{so}\left(  2n\right)  $ for $n\geq4$; the Dynkin diagram is
$\circ-\circ-\circ...\circ<_{\circ}^{\circ}$. (Note that $\mathfrak{so}\left(
4\right)  \cong\mathfrak{sl}\left(  2\right)  \oplus\mathfrak{sl}\left(
2\right)  $ and $\mathfrak{so}\left(  6\right)  \cong\mathfrak{sl}\left(
4\right)  $.)

Exceptional Lie algebras:

$E_{6}$; the Dynkin diagram is $\circ-\circ-\overset{\overset{\circ}{\mid
}}{\circ}-\circ-\circ$.

$E_{7}$; the Dynkin diagram is $\circ-\circ-\overset{\overset{\circ}{\mid
}}{\circ}-\circ-\circ-\circ$.

$E_{8}$; the Dynkin diagram is $\circ-\circ-\overset{\overset{\circ}{\mid
}}{\circ}-\circ-\circ-\circ-\circ$.

$F_{4}$; the Dynkin diagram is $\circ-\circ=>\circ-\circ$.

$G_{2}$; the Dynkin diagram is $\circ<\equiv\circ$.

Now to the Serre relations, which we have not yet written down:

\begin{theorem}
\label{thm.serre-gen.2}\textbf{(a)} Let $i\neq j$. Then, in $\mathfrak{g}$, we
have $\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)  ^{1-a_{i,j}%
}e_{j}=0$ and $\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}=0$. These relations are called the \textit{Serre relations}.

\textbf{(b)} Moreover, together with the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  h_{i},h_{j}\right]  =0;\\
\left[  h_{i},e_{j}\right]  =a_{i,j}e_{j};\\
\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j};\\
\left[  e_{i},f_{j}\right]  =\delta_{i,j}h_{i}%
\end{array}
\right.  \label{nonserre-relations}%
\end{equation}
of Proposition \ref{prop.serre-gen.1}, these relations form a set of defining
relations. If $\widetilde{\mathfrak{g}}$ is $\operatorname*{FreeLie}\left(
h_{i},f_{i},e_{i}\right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  $, then $\widetilde{\mathfrak{g}}%
\diagup\left(  \text{Serre relations}\right)  \cong\mathfrak{g}$.
\end{theorem}

We give a partial \textit{Proof of Theorem \ref{thm.serre-gen.2}.}
\textbf{(a)} Consider $\mathfrak{g}$ as a $\left(  \mathfrak{sl}_{2}\right)
_{i}$-module, where $\left(  \mathfrak{sl}_{2}\right)  _{i}=\left\langle
e_{i},f_{i},h_{i}\right\rangle $. Then, $\left(  \operatorname*{ad}\left(
e_{i}\right)  \right)  f_{j}=\left[  e_{i},f_{j}\right]  =0$ and $\left(
\operatorname*{ad}\left(  h_{i}\right)  \right)  f_{j}=\left[  h_{i}%
,f_{j}\right]  =-a_{i,j}f_{i,j}$. Thus, $f_{j}$ generates a highest-weight
module over $\left(  \mathfrak{sl}_{2}\right)  _{i}$ with highest weight
$-a_{i,j}$. This module is finite-dimensional, so it has weights $-a_{i,j}$,
$-a_{i,j}-2$, $...$, $a_{i,j}$ (listed from highest to lowest). But $\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}$ has weight
$-a_{i,j}-2\left(  1-a_{i,j}\right)  =a_{i,j}-2$. Hence, $\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}=0$.
Similarly, $\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)
^{1-a_{i,j}}e_{j}=0$. Theorem \ref{thm.serre-gen.2} \textbf{(a)} is thus proven.

We are not going to prove Theorem \ref{thm.serre-gen.2} \textbf{(b)} here.

\subsubsection{Kac-Moody Lie algebras}

Now forget about our simple Lie algebra $\mathfrak{g}$. Suppose that
$A=\left(  a_{i,j}\right)  $ is any $n\times n$ matrix of complex numbers.

Let $Q$ be the free abelian group generated by $n$ symbols $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{n}$ (that is, $Q=\mathbb{Z}\alpha_{1}%
+\mathbb{Z}\alpha_{2}+...+\mathbb{Z}\alpha_{n}$).

We call $Q$ a \textit{root lattice}.

\begin{definition}
A \textit{contragredient Lie algebra} corresponding to $A$ is a $\mathbb{C}%
$-Lie algebra $\mathfrak{g}$ which is (as a Lie algebra) generated by some
elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$,
$h_{1}$, $h_{2}$, $...$, $h_{n}$ which satisfy the following three conditions:

\textbf{(1)} These elements satisfy the relations (\ref{nonserre-relations}).

\textbf{(2)} The Lie algebra $\mathfrak{g}$ is $Q$-graded. This means that
$\mathfrak{g}=\bigoplus\limits_{\alpha\in Q}\mathfrak{g}_{\alpha}$ for some
subspaces $\mathfrak{g}_{\alpha}$ satisfying $\left[  \mathfrak{g}_{\alpha
},\mathfrak{g}_{\beta}\right]  \subseteq\mathfrak{g}_{\alpha+\beta}$,
$\mathfrak{g}_{0}=\mathfrak{h}=\left\langle h_{1},h_{2},...,h_{n}\right\rangle
$ and $\mathfrak{g}_{\alpha_{i}}=\mathbb{C}e_{i}$ and $\mathfrak{g}%
_{-\alpha_{i}}=\mathbb{C}f_{i}$.

\textbf{(3)} Every $\mathbb{Q}$-graded nonzero ideal in $\mathfrak{g}$ has a
nonzero intersection with $\mathfrak{h}$.
\end{definition}

Note that the condition \textbf{(3)} is satisfied for simple
finite-dimensional $\mathfrak{g}$, so simple finite-dimensional Lie algebras
are contragredient.

\begin{theorem}
If $A$ is fixed, then there exists a unique (up to isomorphism) contragredient
Lie algebra $\mathfrak{g}$ corresponding to $A$. If $A$ is a Cartan matrix,
then this Lie algebra $\mathfrak{g}$ is finite-dimensional and simple.
\end{theorem}

\textit{Proof.} Let $\widetilde{\mathfrak{g}}$ be the Lie algebra
$\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)  $. Then, let us prove
that $\widetilde{\mathfrak{g}}$ already satisfies conditions \textbf{(1)} and
\textbf{(2)}. In fact, we make $\widetilde{\mathfrak{g}}$ into a $Q$-graded
Lie algebra by setting $\deg\left(  e_{i}\right)  =\alpha_{i}$, $\deg\left(
f_{i}\right)  =-\alpha_{i}$ and $\deg\left(  h_{i}\right)  =0$.

Let $\mathfrak{h}=\left\langle h_{1},h_{2},...,h_{n}\right\rangle $ (linear span).

To prove the rest, consider the Lie algebra $\widetilde{\mathfrak{g}}^{\prime
}=\operatorname*{FreeLie}\left(  e_{i}\right)  \oplus\mathfrak{h}%
\oplus\operatorname*{FreeLie}\left(  f_{i}\right)  $. Grade this in the
obvious way, so that $\operatorname*{FreeLie}\left(  e_{i}\right)
=\widetilde{\mathfrak{n}}_{+}^{\prime}$ and $\operatorname*{FreeLie}\left(
f_{i}\right)  =\widetilde{\mathfrak{n}}_{-}^{\prime}$. Make this a Lie algebra
by the Leibniz rule, and check that this indeed gives a Lie algebra.

We have a surjective homomorphism $\phi:\widetilde{\mathfrak{g}}%
\rightarrow\widetilde{\mathfrak{g}}^{\prime}$ (since $\widetilde{\mathfrak{g}%
}^{\prime}$ satisfies conditions \textbf{(1)} and \textbf{(2)}). We want to
prove that this $\phi$ is an isomorphism.

\textbf{Claim:} We can write $\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}%
}_{+}\oplus\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}}_{-}$, where
$\widetilde{\mathfrak{n}}_{+}$ is generated by the $e_{i}$ and where
$\widetilde{\mathfrak{n}}_{-}$ is generated by the $f_{i}$, and where
$\widetilde{\mathfrak{h}}$ is generated and actually spanned by $h_{i}$.

\textit{Proof of the Claim:} Any commutator can be written as a commutator of
only $e_{i}$, or of only $f_{i}$, or it lies in $\mathfrak{h}$. (This is
rather clear from the relations (\ref{nonserre-relations}).) Thus, we get a
decomposition $\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}}%
_{+}+\widetilde{\mathfrak{h}}+\widetilde{\mathfrak{n}}_{-}$. Since the addends
live in different degrees, this is a direct sum, and thus we conclude that
$\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}}_{-}$.

Hence, $\phi$ is an isomorphism (since $\widetilde{\mathfrak{n}}_{+}^{\prime}$
and $\widetilde{\mathfrak{n}}_{-}^{\prime}$ are free Lie algebras).

So we have proven that $\widetilde{\mathfrak{g}}$ satisfies conditions
\textbf{(1)} and \textbf{(2)}. Now, in order to find a Lie algebra
$\mathfrak{g}$ which additionally satisfies \textbf{(3)}, we do something
cheap: Let $I$ be the sum of all $Q$-graded ideals in $\widetilde{\mathfrak{g}%
}$ which have zero intersection with $\mathfrak{h}$. Let $\mathfrak{g}%
=\widetilde{\mathfrak{g}}\diagup I$.

We now claim that $\mathfrak{g}$ is a contragredient Lie algebra corresponding
to $A$. To prove this, we must check that conditions \textbf{(1)},
\textbf{(2)} and \textbf{(3)} are satisfied. We know that conditions
\textbf{(1)} and \textbf{(2)} hold (since they hold for $\mathfrak{g}$, and
since "intersection with $\mathfrak{h}$" is the same as "projection onto the
$0$-th graded component" due to our grading!), so it only remains to check
\textbf{(3)}.

If $J\subseteq\widetilde{\mathfrak{g}}\diagup I$ is a graded ideal such that
$J\cap\mathfrak{h}=0$, then the preimage $\widetilde{J}$ of $J$ in
$\widetilde{\mathfrak{g}}$ is also such an ideal and properly contains $J$,
which is a contradiction. So $\mathfrak{g}$ is a contragredient Lie algebra
corresponding to $A$.

If $\mathfrak{g}^{\prime}$ is another contragredient Lie algebra corresponding
to $A$, then we have a surjective map $\widetilde{\mathfrak{g}}\rightarrow
\mathfrak{g}^{\prime}$, and $I$ is killed by this map, so we get a surjective
map $\psi:\mathfrak{g}\rightarrow\mathfrak{g}^{\prime}$. The kernel
$\operatorname*{Ker}\psi$ is a $Q$-graded ideal, so that $\operatorname*{Ker}%
\psi\cap\mathfrak{h}=0$, and thus (by condition \textbf{(3)}) we have
$\operatorname*{Ker}\psi=0$ and thus $\psi$ is a graded isomorphism.


\end{document}