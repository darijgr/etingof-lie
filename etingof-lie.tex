%\usepackage{bigfoot}
%\usepackage{fixltx2e}
%\renewcommand{\footnote}[1]{\footnotemark \footnotetext{#1}}
% This paper comes in two versions:
% The long version:
% \includecomment{verlong}
% \excludecomment{vershort}
% The short version:
% \excludecomment{verlong}
% \includecomment{vershort}

\documentclass
[numbers=enddot,12pt,final,onecolumn,german,notitlepage]{scrartcl}%
\usepackage[all,cmtip]{xy}
\usepackage{lscape}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{amsthm}
\usepackage{graphicx}%
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Monday, January 14, 2013 16:15:10}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newcommand{\Ker}{\operatorname*{Ker}}
\newcommand{\id}{\operatorname*{id}}
\newcommand{\inc}{\operatorname*{inc}}
\newcommand{\gr}{\operatorname*{gr}}
\newcommand{\Hom}{\operatorname*{Hom}}
\newcommand{\calA}{\mathcal A}
\newcommand{\arinj}{\ar@{_{(}->}}
\newcommand{\arsurj}{\ar@{->>}}
\newcommand{\arelem}{\ar@{|->}}
\newcommand{\fraka}{\mathfrak{a}}
\newcommand{\frakb}{\mathfrak{b}}
\newcommand{\frakc}{\mathfrak{c}}
\newcommand{\PBW}{\operatorname*{PBW}}
\newcommand{\xycs}{\xymatrixcolsep}
\newcommand{\xyrs}{\xymatrixrowsep}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[subsection]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{impnt}[theo]{Important Notice}
\newenvironment{impnot}[1][]
{\begin{impnt}[#1]\begin{leftbar}\color{blue}}
{\end{leftbar}\end{impnt}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{exe}[theo]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exe}[#1]\begin{leftbar}}
{\end{leftbar}\end{exe}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{Convention}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{Warning}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{example}[theo]{Example}
\voffset=-0.5cm
\hoffset=-0.7cm
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\setlength\textheight{24cm}
\setlength\textwidth{15.5cm}
\begin{document}

\title{18.747: Infinite-dimensional Lie algebras (Spring term 2012 at MIT)}
\author{Pavel Etingof\\Scribed by Darij Grinberg}
\date{Version 0.39 (\today) (not proofread!)}
\maketitle
\tableofcontents

\begin{noncompile}
\textbf{TO-DO\ LIST:}

[footnotes correct order]
\end{noncompile}

\subsection{Version notes}

\textbf{Only Chapters 1 and 2 of these notes are currently anywhere near
completion.} Chapter 3 is done in parts, but most material is still very
sketchy and/or wrong. Chapter 4 is, right now, an unusable mess.

These notes are mostly based on what is being said and written on the
blackboard in the lectures, and less so on Pavel Etingof's handwritten notes
posted on \newline\texttt{http://www-math.mit.edu/\symbol{126}etingof/} . At
the moment, they lag behind Etingof's handwritten notes, but are more
detailed. But Etingof's notes contain some remarks that are not in (and might
never be added to) these notes!

Thanks to Pavel Etingof for his patience in explaining me things until I
actually understand them. Thanks to Dorin Boger for finding mistakes.

\subsection{Introduction}

The standard text on infinite-dimensional Lie algebras (although we will not
really follow it) is:

\begin{itemize}
\item V. G. Kac, A. K. Raina, \textit{(Bombay Lectures on) Highest Weight
Representations of Infinite Dimensional Lie Algebras}, World Scientific 1987.
\end{itemize}

Further recommended sources are:

\begin{itemize}
\item Victor G. Kac, \textit{Infinite dimensional Lie algebras}, Third
Edition, CUP 1995.

\item B. L. Feigin, A. Zelevinsky, \textit{Representations of contragredient
Lie algebras and the Kac-Macdonald identities}, a paper in: Representations of
Lie groups and Lie algebras (Budapest, 1971), pp. 25-77, Akad. Kiad\'{o},
Budapest, 1985.
\end{itemize}

The goal of these lectures is to discuss the structure and the representation
theory (mainly the latter) of the most important infinite-dimensional Lie
algebras. And we are also going to show some connections of this subject to
other fields of mathematics (such as conformal field theory and the theory of
integrable systems).

\begin{verlong}
Also, there are connections to the theory of quantum groups, but we won't get
to them.
\end{verlong}

Basic algebra will be used, but also occasionally some results from the
representation theory of finite-dimensional Lie algebras.

The biggest difference between the theory of finite-dimensional Lie algebras
and that of infinite-dimensional ones is that in the finite-dimensional case,
we have a complete picture -- we can classify simple Lie algebras, etc. --,
whereas in the infinite-dimensional one there are lots and lots of simple Lie
algebras and we have no real hope to classify them; what we can do is study
some very specific classes and families. The main classes of Lie algebras that
we will study in this course are:

\textbf{1.} The Heisenberg algebra (aka oscillator algebra).

\textbf{2.} The Virasoro algebra.

\textbf{3.} Kac-Moody algebras (this class contains semisimple Lie algebras
and also affine Lie algebras, which are central extensions of $\mathfrak{g}%
\left[  t,t^{-1}\right]  $ where $\mathfrak{g}$ is simple finite-dimensional).

\textbf{4.} The Lie algebra $\mathfrak{gl}_{\infty}$ and some variations on it.

\subsection{General conventions}

We will almost always work over $\mathbb{C}$ in this course. All algebras are
over $\mathbb{C}$ unless specified otherwise. Characteristic $p$ is too
complicated for us, although very interesting. Sometimes we will work over
$\mathbb{R}$, and occasionally even over rings (as auxiliary constructions
require this).

Some remarks on notation:

\begin{itemize}
\item In the following, $\mathbb{N}$ will always denote the set $\left\{
0,1,2,...\right\}  $ (and not $\left\{  1,2,3,...\right\}  $).

\item When a Lie algebra $\mathfrak{g}$ acts on a vector space $M$, we will
denote the image of an element $m\in M$ under the action of an element
$a\in\mathfrak{g}$ by any of the three notations $am$, $a\cdot m$ and
$a\rightharpoonup m$. (One day, I will probably come to an agreement with
myself and decide which of these notations to use, but for now expect to see
each of them in this text. Some authors also use the notation $a\circ m$ for
the image of $m$ under the action of $a$, but we won't use this notation.)
\end{itemize}

\section{The main examples}

\subsection{The Heisenberg algebra}

We start with the definition of the Heisenberg algebra. Before we formulate
it, let us introduce polynomial differential forms on $\mathbb{C}^{\times}$
(in the algebraic sense):

\begin{definition}
\label{def.diffform}Recall that $\mathbb{C}\left[  t,t^{-1}\right]  $ denotes
the $\mathbb{C}$-algebra of Laurent polynomials in the variable $t$ over
$\mathbb{C}$.

Consider the free $\mathbb{C}\left[  t,t^{-1}\right]  $-module on the basis
$\left(  dt\right)  $ (where $dt$ is just a symbol). The elements of this
module are called \textit{polynomial differential forms on }$\mathbb{C}%
^{\times}$. Thus, polynomial differential forms on $\mathbb{C}^{\times}$ are
just formal expressions of the form $fdt$ where $f\in\mathbb{C}\left[
t,t^{-1}\right]  $.

Whenever $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ is a Laurent polynomial, we
define a polynomial differential form $dg$ by $dg=g^{\prime}dt$. This notation
$dg$ does not conflict with the previously defined notation $dt$ (which was a
symbol), because the polynomial $t$ satisfies $t^{\prime}=1$.
\end{definition}

\begin{definition}
\label{def.res}For every polynomial differential form $fdt$ on $\mathbb{C}%
^{\times}$ (with $f\in\mathbb{C}\left[  t,t^{-1}\right]  $), we define a
complex number $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ to be
the coefficient of the Laurent polynomial $f$ before $t^{-1}$. In other words,
we define $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ to be
$a_{-1}$, where $f$ is written as $\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$
(with $a_{i}\in\mathbb{C}$ for all $i\in\mathbb{Z}$).

This number $\operatorname*{Res}\nolimits_{t=0}\left(  fdt\right)  $ is called
the \textit{residue} of the form $fdt$ at $0$.
\end{definition}

(The same definition could have been done for Laurent series instead of
Laurent polynomials, but this would require us to consider a slightly
different notion of differential forms, and we do not want to do this here.)

\begin{remark}
\label{rmk.res}\textbf{(a)} Every Laurent polynomial $f\in\mathbb{C}\left[
t,t^{-1}\right]  $ satisfies $\operatorname*{Res}\nolimits_{t=0}\left(
df\right)  =0$.

\textbf{(b)} Every Laurent polynomial $f\in\mathbb{C}\left[  t,t^{-1}\right]
$ satisfies $\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)  =0$.
\end{remark}

\textit{Proof of Remark \ref{rmk.res}.} \textbf{(a)} Write $f$ in the form
$\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ (with $b_{i}\in\mathbb{C}$ for all
$i\in\mathbb{Z}$). Then, $f^{\prime}=\sum\limits_{i\in\mathbb{Z}}ib_{i}%
t^{i-1}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}$. Now,
$df=f^{\prime}dt$, so that
\begin{align*}
\operatorname*{Res}\nolimits_{t=0}\left(  df\right)   &  =\operatorname*{Res}%
\nolimits_{t=0}\left(  f^{\prime}dt\right)  =\left(  \text{the coefficient of
the Laurent polynomial }f^{\prime}\text{ before }t^{-1}\right) \\
&  =\underbrace{\left(  -1+1\right)  }_{=0}b_{-1+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }f^{\prime}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)
b_{i+1}t^{i}\right) \\
&  =0,
\end{align*}
proving Remark \ref{rmk.res} \textbf{(a)}.

\textbf{(b)} \textit{First proof of Remark \ref{rmk.res} \textbf{(b)}:} By the
Leibniz identity, $\left(  f^{2}\right)  ^{\prime}=ff^{\prime}+f^{\prime
}f=2ff^{\prime}$, so that $ff^{\prime}=\dfrac{1}{2}\left(  f^{2}\right)
^{\prime}$ and thus $f\underbrace{df}_{=f^{\prime}dt}=\underbrace{ff^{\prime}%
}_{=\dfrac{1}{2}\left(  f^{2}\right)  ^{\prime}}dt=\dfrac{1}{2}%
\underbrace{\left(  f^{2}\right)  ^{\prime}dt}_{=d\left(  f^{2}\right)
}=\dfrac{1}{2}d\left(  f^{2}\right)  $. Thus,
\[
\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)  =\operatorname*{Res}%
\nolimits_{t=0}\left(  \dfrac{1}{2}d\left(  f^{2}\right)  \right)  =\dfrac
{1}{2}\underbrace{\operatorname*{Res}\nolimits_{t=0}\left(  d\left(
f^{2}\right)  \right)  }_{\substack{=0\text{ (by Remark \ref{rmk.res}
\textbf{(a)},}\\\text{applied to }f^{2}\text{ instead of }f\text{)}}}=0,
\]
and Remark \ref{rmk.res} \textbf{(b)} is proven.

\textit{Second proof of Remark \ref{rmk.res} \textbf{(b)}:} Write $f$ in the
form $\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ (with $b_{i}\in\mathbb{C}$ for
all $i\in\mathbb{Z}$). Then, $f^{\prime}=\sum\limits_{i\in\mathbb{Z}}%
ib_{i}t^{i-1}=\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}$.
Now,%
\[
ff^{\prime}=\left(  \sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}\right)  \left(
\sum\limits_{i\in\mathbb{Z}}\left(  i+1\right)  b_{i+1}t^{i}\right)
=\sum\limits_{n\in\mathbb{Z}}\left(  \sum\limits_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\i+j=n}}b_{i}\cdot\left(  j+1\right)
b_{j+1}\right)  t^{n}%
\]
(by the definition of the product of Laurent polynomials). Also,
$df=f^{\prime}dt$, so that%
\begin{align*}
\operatorname*{Res}\nolimits_{t=0}\left(  fdf\right)   &  =\operatorname*{Res}%
\nolimits_{t=0}\left(  ff^{\prime}dt\right)  =\left(  \text{the coefficient of
the Laurent polynomial }ff^{\prime}\text{ before }t^{-1}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2}%
;\\i+j=-1}}b_{i}\cdot\left(  j+1\right)  b_{j+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }ff^{\prime}=\sum\limits_{n\in\mathbb{Z}}\left(  \sum
\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\i+j=n}}b_{i}%
\cdot\left(  j+1\right)  b_{j+1}\right)  t^{n}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2}%
;\\i+j=0}}b_{i}\cdot jb_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
substituted }\left(  i,j\right)  \text{ for }\left(  i,j+1\right)  \text{ in
the sum}\right) \\
&  =\sum\limits_{j\in\mathbb{Z}}b_{-j}\cdot jb_{j}=\underbrace{\sum
\limits_{\substack{j\in\mathbb{Z};\\j<0}}b_{-j}\cdot jb_{j}}_{\substack{=\sum
\limits_{\substack{j\in\mathbb{Z};\\j>0}}b_{-\left(  -j\right)  }\cdot\left(
-j\right)  b_{-j}\\\text{(here, we substituted }j\text{ for }-j\text{ in the
sum)}}}+\underbrace{b_{-0}\cdot0b_{0}}_{=0}+\sum\limits_{\substack{j\in
\mathbb{Z};\\j>0}}b_{-j}\cdot jb_{j}\\
&  =\sum\limits_{\substack{j\in\mathbb{Z};\\j>0}}\underbrace{b_{-\left(
-j\right)  }\cdot\left(  -j\right)  b_{-j}}_{=b_{j}\left(  -j\right)
b_{-j}=-b_{-j}\cdot jb_{j}}+\sum\limits_{\substack{j\in\mathbb{Z}%
;\\j>0}}b_{-j}\cdot jb_{j}=\sum\limits_{\substack{j\in\mathbb{Z}%
;\\j>0}}\left(  -b_{-j}\cdot jb_{j}\right)  +\sum\limits_{\substack{j\in
\mathbb{Z};\\j>0}}b_{-j}\cdot jb_{j}=0.
\end{align*}
This proves Remark \ref{rmk.res} \textbf{(b)}.

Note that the first proof of Remark \ref{rmk.res} \textbf{(b)} made use of the
fact that $2$ is invertible in $\mathbb{C}$, whereas the second proof works
over any commutative ring instead of $\mathbb{C}$.

Now, finally, we define the Heisenberg algebra:

\begin{definition}
\label{def.osc}The \textit{oscillator algebra} $\mathcal{A}$ is the vector
space $\mathbb{C}\left[  t,t^{-1}\right]  \oplus\mathbb{C}$ endowed with the
Lie bracket%
\[
\left[  \left(  f,\alpha\right)  ,\left(  g,\beta\right)  \right]  =\left(
0,\operatorname*{Res}\nolimits_{t=0}\left(  gdf\right)  \right)  .
\]
Since this Lie bracket satisfies the Jacobi identity (because the definition
quickly yields that $\left[  \left[  x,y\right]  ,z\right]  =0$ for all
$x,y,z\in\mathcal{A}$) and is skew-symmetric (due to Remark \ref{rmk.res}
\textbf{(b)}), this $\mathcal{A}$ is a Lie algebra.

This oscillator algebra $\mathcal{A}$ is also known as the \textit{Heisenberg
algebra}.
\end{definition}

Thus, $\mathcal{A}$ has a basis
\[
\left\{  a_{n}\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  K\right\}  ,
\]
where $a_{n}=\left(  t^{n},0\right)  $ and $K=\left(  0,1\right)  $. The
bracket is given by%
\begin{align*}
\left[  a_{n},K\right]   &  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{thus, }K\text{
is central}\right)  ;\\
\left[  a_{n},a_{m}\right]   &  =n\delta_{n,-m}K
\end{align*}
(in fact, $\left[  a_{n},a_{-n}\right]  =\operatorname*{Res}\nolimits_{t=0}%
\left(  t^{-n}dt^{n}\right)  K=\operatorname*{Res}\nolimits_{t=0}\left(
nt^{-1}dt\right)  K=nK$). Thus, $\mathcal{A}$ is a $1$-dimensional central
extension of the abelian Lie algebra $\mathbb{C}\left[  t,t^{-1}\right]  $;
this means that we have a short exact sequence%
\[
\xymatrix{
0 \ar[r] & \mathbb C K \ar[r] & \mathcal A \ar[r] & \mathbb C\left[t,t^{-1}\right] \ar[r] & 0
},
\]
where $\mathbb{C}K$ is contained in the center of $\mathcal{A}$ and where
$\mathbb{C}\left[  t,t^{-1}\right]  $ is an abelian Lie algebra.

Note that $\mathcal{A}$ is a $2$-nilpotent Lie algebra. Also note that the
center of $\mathcal{A}$ is spanned by $a_{0}$ and $K$.

\subsection{The Witt algebra}

The next introductory example will be the Lie algebra of vector fields:

\begin{definition}
Consider the free $\mathbb{C}\left[  t,t^{-1}\right]  $-module on the basis
$\left(  \partial\right)  $ (where $\partial$ is just a symbol). This module,
regarded as a $\mathbb{C}$-vector space, will be denoted by $W$. Thus, the
elements of $W$ are formal expressions of the form $f\partial$ where
$f\in\mathbb{C}\left[  t,t^{-1}\right]  $. (Thus, $W\cong\mathbb{C}\left[
t,t^{-1}\right]  $.)

Define a Lie bracket on the $\mathbb{C}$-vector space $W$ by%
\[
\left[  f\partial,g\partial\right]  =\left(  fg^{\prime}-gf^{\prime}\right)
\partial\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }g\in\mathbb{C}\left[  t,t^{-1}\right]  .
\]
This Lie bracket is easily seen to be skew-symmetric and satisfy the Jacobi
identity. Thus, it makes $W$ into a Lie algebra. This Lie algebra is called
the \textit{Witt algebra}.

The elements of $W$ are called \textit{polynomial vector fields on
}$\mathbb{C}^{\times}$.

The symbol $\partial$ is often denoted by $\dfrac{d}{dt}$.
\end{definition}

\begin{remark}
It is not by chance that $\partial$ is also known as $\dfrac{d}{dt}$. In fact,
this notation allows us to view the elements of $W$ as actual polynomial
vector fields on $\mathbb{C}^{\times}$ in the sense of algebraic geometry over
$\mathbb{C}$. The Lie bracket of the Witt algebra $W$ is then exactly the
usual Lie bracket of vector fields (because if $f\in\mathbb{C}\left[
t,t^{-1}\right]  $ and $g\in\mathbb{C}\left[  t,t^{-1}\right]  $ are two
Laurent polynomials, then a simple application of the Leibniz rule shows that
the commutator of the differential operators $f\dfrac{d}{dt}$ and $g\dfrac
{d}{dt}$ is indeed the differential operator $\left(  fg^{\prime}-gf^{\prime
}\right)  \dfrac{d}{dt}$).
\end{remark}

A basis of the Witt algebra $W$ is $\left\{  L_{n}\ \mid\ n\in\mathbb{Z}%
\right\}  $, where $L_{n}$ means $-t^{n+1}\dfrac{d}{dt}=-t^{n+1}\partial$.
(Note that some other references like to define $L_{n}$ as $t^{n+1}\partial$
instead, thus getting a different sign in many formulas.) It is easy to see
that the Lie bracket of the Witt algebra is given on this basis by
\[
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ and }m\in
\mathbb{Z}.
\]


\subsection{A digression: Lie groups (and the absence thereof)}

Let us make some remarks about the relationship between Lie algebras and Lie
groups. In analysis and geometry, linearizations (tangent spaces etc.) usually
only give a crude approximation of non-linear things (manifolds etc.). This is
what makes the theory of Lie groups special: The linearization of a
finite-dimensional Lie group (i. e., its corresponding Lie algebra) carries
very much information about the Lie group. The relation between
finite-dimensional Lie groups and finite-dimensional Lie algebras is almost a
one-to-one correspondence (at least if we restrict ourselves to simply
connected Lie groups). This correspondence breaks down in the
infinite-dimensional case. There are lots of important infinite-dimensional
Lie groups, but their relation to Lie algebras is not as close as in the
finite-dimensional case anymore. One example for this is that there is no Lie
group corresponding to the Witt algebra $W$. There are a few things that come
close to such a Lie group:

We can consider the real subalgebra $W_{\mathbb{R}}$ of $W$, consisting of the
vector fields in $W$ which are tangent to $S^{1}$ (the unit circle in
$\mathbb{C}$). This is a real Lie algebra satisfying $W_{\mathbb{R}}%
\otimes_{\mathbb{R}}\mathbb{C}\cong W$ (thus, $W_{\mathbb{R}}$ is what is
called a \textit{real form} of $W$). And we can say that
$\widehat{W_{\mathbb{R}}}=\operatorname*{Lie}\left(  \operatorname*{Diff}%
S^{1}\right)  $ for some kind of completion $\widehat{W_{\mathbb{R}}}$ of
$W_{\mathbb{R}}$ (although $W_{\mathbb{R}}$ itself is not the Lie algebra of
any Lie group).\footnote{Here is how this completion $\widehat{W_{\mathbb{R}}%
}$ is defined exactly: Notice that%
\[
W_{\mathbb{R}}=\left\{  \varphi\left(  \theta\right)  \dfrac{d}{dt}\ \mid\
\begin{array}
[c]{c}%
\varphi\text{ is a trigonometric polynomial, i. e.,}\\
\varphi\left(  \theta\right)  =a_{0}+\sum\limits_{n}a_{n}\cos n\theta
+\sum\limits_{n}b_{n}\sin n\theta\\
\text{where both sums are finite}%
\end{array}
\right\}  .
\]
Now, define the completion $\widehat{W_{\mathbb{R}}}$ by%
\[
\widehat{W_{\mathbb{R}}}=\left\{  \varphi\left(  \theta\right)  \dfrac{d}%
{dt}\ \mid\
\begin{array}
[c]{c}%
\varphi\left(  \theta\right)  =a_{0}+\sum\limits_{n}a_{n}\cos n\theta
+\sum\limits_{n}b_{n}\sin n\theta\\
\text{where both sums are infinite sums with rapidly}\\
\text{decreasing coefficients}%
\end{array}
\right\}  .
\]
} Now if we take two one-parameter families%
\begin{align*}
g_{s}  &  \in\operatorname*{Diff}S^{1},\ \ \ \ \ \ \ \ \ \ g_{s}\mid
_{s=0}=\operatorname*{id},\ \ \ \ \ \ \ \ \ \ g_{s}^{\prime}\mid_{s=0}%
=\varphi;\\
h_{u}  &  \in\operatorname*{Diff}S^{1},\ \ \ \ \ \ \ \ \ \ h_{u}\mid
_{u=0}=\operatorname*{id},\ \ \ \ \ \ \ \ \ \ h_{u}^{\prime}\mid_{u=0}=\psi,
\end{align*}
then%
\begin{align*}
g_{s}\left(  \theta\right)   &  =\theta+s\varphi\left(  \theta\right)
+O\left(  s^{2}\right)  ;\\
h_{u}\left(  \theta\right)   &  =\theta+u\psi\left(  \theta\right)  +O\left(
u^{2}\right)  ;\\
\left(  g_{s}\circ h_{u}\circ g_{s}^{-1}\circ h_{u}^{-1}\right)  \left(
\theta\right)   &  =\theta+su\left(  \varphi\psi^{\prime}-\psi\varphi^{\prime
}\right)  \left(  \theta\right)  +\left(  \text{cubic terms in }s\text{ and
}u\text{ and higher}\right)  .
\end{align*}
So we get something like the standard Lie-group-Lie-algebra correspondence,
but only for the completion of the real part. For the complex one, some people
have done some work yielding something like Lie semigroups (the so-called
"semigroup of annuli" of G. Segal), but no Lie groups.

Anyway, this was a digression, just to show that we don't have Lie groups
corresponding to our Lie algebras. Still, this should not keep us from
heuristically thinking of Lie algebras as linearizations of Lie groups. We can
even formalize this heuristic, by using the purely algebraic notion of formal groups.

\subsection{The Witt algebra acts on the Heisenberg algebra by derivations}

Let's return to topic. The following proposition is a variation on a
well-known theme:

\begin{proposition}
\label{prop.commutator.derivs}Let $\mathfrak{n}$ be a Lie algebra. Let
$f:\mathfrak{n}\rightarrow\mathfrak{n}$ and $g:\mathfrak{n}\rightarrow
\mathfrak{n}$ be two derivations of $\mathfrak{n}$. Then, $\left[  f,g\right]
$ is a derivation of $\mathfrak{n}$. (Here, the Lie bracket is to be
understood as the Lie bracket on $\operatorname*{End}\mathfrak{n}$, so that we
have $\left[  f,g\right]  =f\circ g-g\circ f$.)
\end{proposition}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.commutator.derivs}.} Let $a\in
\mathfrak{n}$ and $b\in\mathfrak{n}$. Since $f$ is a derivation, we have
$f\left(  \left[  a,b\right]  \right)  =\left[  f\left(  a\right)  ,b\right]
+\left[  a,f\left(  b\right)  \right]  $. Thus,%
\begin{align*}
\left(  g\circ f\right)  \left(  \left[  a,b\right]  \right)   &  =g\left(
\underbrace{f\left(  \left[  a,b\right]  \right)  }_{=\left[  f\left(
a\right)  ,b\right]  +\left[  a,f\left(  b\right)  \right]  }\right)
=g\left(  \left[  f\left(  a\right)  ,b\right]  +\left[  a,f\left(  b\right)
\right]  \right) \\
&  =\underbrace{g\left(  \left[  f\left(  a\right)  ,b\right]  \right)
}_{\substack{=\left[  g\left(  f\left(  a\right)  \right)  ,b\right]  +\left[
f\left(  a\right)  ,g\left(  b\right)  \right]  \\\text{(since }g\text{ is a
derivation)}}}+\underbrace{g\left(  \left[  a,f\left(  b\right)  \right]
\right)  }_{\substack{=\left[  g\left(  a\right)  ,f\left(  b\right)  \right]
+\left[  a,g\left(  f\left(  b\right)  \right)  \right]  \\\text{(since
}g\text{ is a derivation)}}}\\
&  =\left[  \underbrace{g\left(  f\left(  a\right)  \right)  }_{=\left(
g\circ f\right)  \left(  a\right)  },b\right]  +\left[  f\left(  a\right)
,g\left(  b\right)  \right]  +\left[  g\left(  a\right)  ,f\left(  b\right)
\right]  +\left[  a,\underbrace{g\left(  f\left(  b\right)  \right)
}_{=\left(  g\circ f\right)  \left(  b\right)  }\right] \\
&  =\left[  \left(  g\circ f\right)  \left(  a\right)  ,b\right]  +\left[
f\left(  a\right)  ,g\left(  b\right)  \right]  +\left[  g\left(  a\right)
,f\left(  b\right)  \right]  +\left[  a,\left(  g\circ f\right)  \left(
b\right)  \right]  .
\end{align*}
The same argument, with $f$ and $g$ replaced by $g$ and $f$, shows that%
\[
\left(  f\circ g\right)  \left(  \left[  a,b\right]  \right)  =\left[  \left(
f\circ g\right)  \left(  a\right)  ,b\right]  +\left[  g\left(  a\right)
,f\left(  b\right)  \right]  +\left[  f\left(  a\right)  ,g\left(  b\right)
\right]  +\left[  a,\left(  f\circ g\right)  \left(  b\right)  \right]  .
\]
Thus,%
\begin{align*}
\underbrace{\left[  f,g\right]  }_{=f\circ g-g\circ f}\left(  \left[
a,b\right]  \right)   &  =\left(  f\circ g-g\circ f\right)  \left(  \left[
a,b\right]  \right) \\
&  =\underbrace{\left(  f\circ g\right)  \left(  \left[  a,b\right]  \right)
}_{=\left[  \left(  f\circ g\right)  \left(  a\right)  ,b\right]  +\left[
g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  f\left(  a\right)
,g\left(  b\right)  \right]  +\left[  a,\left(  f\circ g\right)  \left(
b\right)  \right]  }-\underbrace{\left(  g\circ f\right)  \left(  \left[
a,b\right]  \right)  }_{=\left[  \left(  g\circ f\right)  \left(  a\right)
,b\right]  +\left[  f\left(  a\right)  ,g\left(  b\right)  \right]  +\left[
g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  a,\left(  g\circ
f\right)  \left(  b\right)  \right]  }\\
&  =\left(  \left[  \left(  f\circ g\right)  \left(  a\right)  ,b\right]
+\left[  g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  f\left(
a\right)  ,g\left(  b\right)  \right]  +\left[  a,\left(  f\circ g\right)
\left(  b\right)  \right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ -\left(  \left[  \left(  g\circ f\right)  \left(
a\right)  ,b\right]  +\left[  f\left(  a\right)  ,g\left(  b\right)  \right]
+\left[  g\left(  a\right)  ,f\left(  b\right)  \right]  +\left[  a,\left(
g\circ f\right)  \left(  b\right)  \right]  \right) \\
&  =\underbrace{\left[  \left(  f\circ g\right)  \left(  a\right)  ,b\right]
-\left[  \left(  g\circ f\right)  \left(  a\right)  ,b\right]  }_{=\left[
\left(  f\circ g\right)  \left(  a\right)  -\left(  g\circ f\right)  \left(
a\right)  ,b\right]  }+\underbrace{\left[  a,\left(  f\circ g\right)  \left(
b\right)  \right]  -\left[  a,\left(  g\circ f\right)  \left(  b\right)
\right]  }_{=\left[  a,\left(  f\circ g\right)  \left(  b\right)  -\left(
g\circ f\right)  \left(  b\right)  \right]  }\\
&  =\left[  \underbrace{\left(  f\circ g\right)  \left(  a\right)  -\left(
g\circ f\right)  \left(  a\right)  }_{=\left(  f\circ g-g\circ f\right)
\left(  a\right)  },b\right]  +\left[  a,\underbrace{\left(  f\circ g\right)
\left(  b\right)  -\left(  g\circ f\right)  \left(  b\right)  }_{=\left(
f\circ g-g\circ f\right)  \left(  b\right)  }\right] \\
&  =\left[  \underbrace{\left(  f\circ g-g\circ f\right)  }_{=\left[
f,g\right]  }\left(  a\right)  ,b\right]  +\left[  a,\underbrace{\left(
f\circ g-g\circ f\right)  }_{=\left[  f,g\right]  }\left(  b\right)  \right]
\\
&  =\left[  \left[  f,g\right]  \left(  a\right)  ,b\right]  +\left[
a,\left[  f,g\right]  \left(  b\right)  \right]  .
\end{align*}
We have thus proven that any $a\in\mathfrak{n}$ and $b\in\mathfrak{n}$ satisfy
$\left[  f,g\right]  \left(  \left[  a,b\right]  \right)  =\left[  \left[
f,g\right]  \left(  a\right)  ,b\right]  +\left[  a,\left[  f,g\right]
\left(  b\right)  \right]  $. In other words, $\left[  f,g\right]  $ is a
derivation. This proves Proposition \ref{prop.commutator.derivs}.
\end{verlong}

\begin{definition}
For every Lie algebra $\mathfrak{g}$, we will denote by $\operatorname*{Der}%
\mathfrak{g}$ the Lie subalgebra $\left\{  f\in\operatorname*{End}%
\mathfrak{g}\ \mid\ f\text{ is a derivation}\right\}  $ of
$\operatorname*{End}\mathfrak{g}$. (This is well-defined because Proposition
\ref{prop.commutator.derivs} shows that $\left\{  f\in\operatorname*{End}%
\mathfrak{g}\ \mid\ f\text{ is a derivation}\right\}  $ is a Lie subalgebra of
$\operatorname*{End}\mathfrak{g}$.) We call $\operatorname*{Der}\mathfrak{g}$
the \textit{Lie algebra of derivations} of $\mathfrak{g}$.
\end{definition}

\begin{lemma}
\label{lem.WtoDerA}There is a natural homomorphism $\eta:W\rightarrow
\operatorname*{Der}\mathcal{A}$ of Lie algebras given by
\[
\left(  \eta\left(  f\partial\right)  \right)  \left(  g,\alpha\right)
=\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }\alpha\in\mathbb{C}.
\]

\end{lemma}

\textit{First proof of Lemma \ref{lem.WtoDerA}.} Lemma \ref{lem.WtoDerA} can
be proven by direct calculation:

For every $f\partial\in W$, the map%
\[
\mathcal{A}\rightarrow\mathcal{A},\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)
\mapsto\left(  fg^{\prime},0\right)
\]
is a derivation of $\mathcal{A}$\ \ \ \ \footnote{\textit{Proof.} Let
$f\partial$ be an element of $W$. (In other words, let $f$ be an element of
$\mathbb{C}\left[  t,t^{-1}\right]  $.) Let $\tau$ denote the map%
\[
\mathcal{A}\rightarrow\mathcal{A},\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)
\mapsto\left(  fg^{\prime},0\right)  .
\]
Then, we must prove that $\tau$ is a derivation of $\mathcal{A}$.
\par
In fact, first it is clear that $\tau$ is $\mathbb{C}$-linear. Moreover, any
$\left(  u,\beta\right)  \in\mathcal{A}$ and $\left(  v,\gamma\right)
\in\mathcal{A}$ satisfy%
\begin{align*}
\tau\left(  \underbrace{\left[  \left(  u,\beta\right)  ,\left(
v,\gamma\right)  \right]  }_{=\left(  0,\operatorname*{Res}\nolimits_{t=0}%
\left(  vdu\right)  \right)  }\right)   &  =\tau\left(  0,\operatorname*{Res}%
\nolimits_{t=0}\left(  vdu\right)  \right)  =\left(  f0,0\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\right) \\
&  =\left(  0,0\right)
\end{align*}
and%
\begin{align*}
&  \left[  \underbrace{\tau\left(  u,\beta\right)  }_{=\left(  fu^{\prime
},0\right)  },\left(  v,\gamma\right)  \right]  +\left[  \left(
u,\beta\right)  ,\underbrace{\tau\left(  v,\gamma\right)  }_{=\left(
fv^{\prime},0\right)  }\right] \\
&  =\underbrace{\left[  \left(  fu^{\prime},0\right)  ,\left(  v,\gamma
\right)  \right]  }_{=\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(
vd\left(  fu^{\prime}\right)  \right)  \right)  }+\underbrace{\left[  \left(
u,\beta\right)  ,\left(  fv^{\prime},0\right)  \right]  }_{=\left(
0,\operatorname*{Res}\nolimits_{t=0}\left(  fv^{\prime}du\right)  \right)  }\\
&  =\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(  vd\left(  fu^{\prime
}\right)  \right)  \right)  +\left(  0,\operatorname*{Res}\nolimits_{t=0}%
\left(  fv^{\prime}du\right)  \right) \\
&  =\left(  0,\operatorname*{Res}\nolimits_{t=0}\left(  vd\left(  fu^{\prime
}\right)  +fv^{\prime}du\right)  \right)  =\left(  0,\operatorname*{Res}%
\nolimits_{t=0}\left(  d\left(  vfu^{\prime}\right)  \right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }v\underbrace{d\left(  fu^{\prime}\right)  }_{=\left(  fu^{\prime
}\right)  ^{\prime}dt}+fv^{\prime}\underbrace{du}_{=u^{\prime}dt}=v\left(
fu^{\prime}\right)  ^{\prime}dt+fv^{\prime}u^{\prime}dt\\
=\left(  v\left(  fu^{\prime}\right)  ^{\prime}+fv^{\prime}u^{\prime}\right)
dt=\underbrace{\left(  v\left(  fu^{\prime}\right)  ^{\prime}+v^{\prime
}\left(  fu^{\prime}\right)  \right)  }_{=\left(  vfu^{\prime}\right)
^{\prime}}dt=\left(  vfu^{\prime}\right)  ^{\prime}dt=d\left(  vfu^{\prime
}\right)
\end{array}
\right) \\
&  =\left(  0,0\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since Remark
\ref{rmk.res} \textbf{(a)} (applied to }vfu^{\prime}\text{ instead of
}f\text{) yields }\operatorname*{Res}\nolimits_{t=0}\left(  d\left(
vfu^{\prime}\right)  \right)  =0\right)  ,
\end{align*}
so that $\tau\left(  \left[  \left(  u,\beta\right)  ,\left(  v,\gamma\right)
\right]  \right)  =\left[  \tau\left(  u,\beta\right)  ,\left(  v,\gamma
\right)  \right]  +\left[  \left(  u,\beta\right)  ,\tau\left(  v,\gamma
\right)  \right]  $. Thus, $\tau$ is a derivation of $\mathcal{A}$, qed.},
thus lies in $\operatorname*{Der}\mathcal{A}$. Hence, we can define a map
$\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ by%
\[
\eta\left(  f\partial\right)  =\left(  \mathcal{A}\rightarrow\mathcal{A}%
,\ \ \ \ \ \ \ \ \ \ \left(  g,\alpha\right)  \mapsto\left(  fg^{\prime
},0\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[
t,t^{-1}\right]  .
\]
In other words, we can define a map $\eta:W\rightarrow\operatorname*{Der}%
\mathcal{A}$ by%
\[
\left(  \eta\left(  f\partial\right)  \right)  \left(  g,\alpha\right)
=\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{ and }\alpha\in\mathbb{C}.
\]
Now, it remains to show that this map $\eta$ is a homomorphism of Lie algebras.

In fact, any $f_{1}\in\mathbb{C}\left[  t,t^{-1}\right]  $ and $f_{2}%
\in\mathbb{C}\left[  t,t^{-1}\right]  $ and any $g\in\mathbb{C}\left[
t,t^{-1}\right]  $ and $\alpha\in\mathbb{C}$ satisfy%
\[
\left(  \eta\left(  \underbrace{\left[  f_{1}\partial,f_{2}\partial\right]
}_{=\left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  \partial}\right)
\right)  \left(  g,\alpha\right)  =\left(  \eta\left(  \left(  f_{1}%
f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  \partial\right)  \right)  \left(
g,\alpha\right)  =\left(  \left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime
}\right)  g^{\prime},0\right)
\]
and%
\begin{align*}
&  \left[  \eta\left(  f_{1}\partial\right)  ,\eta\left(  f_{2}\partial
\right)  \right]  \left(  g,\alpha\right) \\
&  =\left(  \eta\left(  f_{1}\partial\right)  \right)  \underbrace{\left(
\left(  \eta\left(  f_{2}\partial\right)  \right)  \left(  g,\alpha\right)
\right)  }_{=\left(  f_{2}g^{\prime},0\right)  }-\left(  \eta\left(
f_{2}\partial\right)  \right)  \underbrace{\left(  \left(  \eta\left(
f_{1}\partial\right)  \right)  \left(  g,\alpha\right)  \right)  }_{=\left(
f_{1}g^{\prime},0\right)  }\\
&  =\underbrace{\left(  \eta\left(  f_{1}\partial\right)  \right)  \left(
f_{2}g^{\prime},0\right)  }_{=\left(  f_{1}\left(  f_{2}g^{\prime}\right)
^{\prime},0\right)  }-\underbrace{\left(  \eta\left(  f_{2}\partial\right)
\right)  \left(  f_{1}g^{\prime},0\right)  }_{=\left(  f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right)  }=\left(  f_{1}\left(
f_{2}g^{\prime}\right)  ^{\prime},0\right)  -\left(  f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right) \\
&  =\left(  f_{1}\left(  f_{2}g^{\prime}\right)  ^{\prime}-f_{2}\left(
f_{1}g^{\prime}\right)  ^{\prime},0\right)  =\left(  \left(  f_{1}%
f_{2}^{\prime}-f_{2}f_{1}^{\prime}\right)  g^{\prime},0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }f_{1}\underbrace{\left(  f_{2}g^{\prime}\right)  ^{\prime}%
}_{=f_{2}^{\prime}g^{\prime}+f_{2}g^{\prime\prime}}-f_{2}\underbrace{\left(
f_{1}g^{\prime}\right)  ^{\prime}}_{=f_{1}^{\prime}g^{\prime}+f_{1}%
g^{\prime\prime}}=f_{1}\left(  f_{2}^{\prime}g^{\prime}+f_{2}g^{\prime\prime
}\right)  -f_{2}\left(  f_{1}^{\prime}g^{\prime}+f_{1}g^{\prime\prime}\right)
\\
=f_{1}f_{2}^{\prime}g^{\prime}+f_{1}f_{2}g^{\prime\prime}-f_{2}f_{1}^{\prime
}g^{\prime}-f_{1}f_{2}g^{\prime\prime}=f_{1}f_{2}^{\prime}g^{\prime}%
-f_{2}f_{1}^{\prime}g^{\prime}=\left(  f_{1}f_{2}^{\prime}-f_{2}f_{1}^{\prime
}\right)  g^{\prime}%
\end{array}
\right)  ,
\end{align*}
so that $\left(  \eta\left(  \left[  f_{1}\partial,f_{2}\partial\right]
\right)  \right)  \left(  g,\alpha\right)  =\left[  \eta\left(  f_{1}%
\partial\right)  ,\eta\left(  f_{2}\partial\right)  \right]  \left(
g,\alpha\right)  $. Thus, any $f_{1}\in\mathbb{C}\left[  t,t^{-1}\right]  $
and $f_{2}\in\mathbb{C}\left[  t,t^{-1}\right]  $ satisfy $\eta\left(  \left[
f_{1}\partial,f_{2}\partial\right]  \right)  )=\left[  \eta\left(
f_{1}\partial\right)  ,\eta\left(  f_{2}\partial\right)  \right]  $. This
proves that $\eta$ is a Lie algebra homomorphism, and thus Lemma
\ref{lem.WtoDerA} is proven.

\textit{Second proof of Lemma \ref{lem.WtoDerA}.} Let us really interpret the
elements of $W$ as vector fields on $\mathbb{C}^{\times}$. The bracket
$\left[  \cdot,\cdot\right]  $ in $\mathcal{A}$ was defined in an invariant
way:%
\[
\left[  f,g\right]  =\operatorname*{Res}\nolimits_{t=0}\left(  gdf\right)
=\dfrac{1}{2\pi i}\oint\limits_{\left\vert z\right\vert =1}%
gdf\ \ \ \ \ \ \ \ \ \ \left(  \text{by Cauchy's residue theorem}\right)
\]
is an integral of a $1$-form, thus invariant under diffeomorphisms, thus
invariant under "infinitesimal diffeomorphisms" such as the ones given by
elements of $W$. Thus, Lemma \ref{lem.WtoDerA} becomes obvious.

The first of these two proofs is obviously the more straightforward one (and
generalizes better to fields other than $\mathbb{C}$), but it does not offer
any explanation why Lemma \ref{lem.WtoDerA} is more than a mere coincidence.
Meanwhile, the second proof gives Lemma \ref{lem.WtoDerA} a philosophical
reason to be true.

\subsection{The Virasoro algebra}

In representation theory, one often doesn't encounter representations of $W$
directly, but instead one finds representations of a $1$-dimensional central
extension of $W$ called the Virasoro algebra. I will now construct this
extension and show that it is the only one (up to isomorphism of extensions).

Let us recollect the theory of central extensions of Lie algebras (more
precisely, the $1$-dimensional ones):

\begin{definition}
\label{def.centex}If $L$ is a Lie algebra, then a $1$-dimensional central
extension of $L$ is a Lie algebra $\widehat{L}$ along with an exact sequence%
\begin{equation}
0\rightarrow\mathbb{C}\rightarrow\widehat{L}\rightarrow L\rightarrow0,
\label{def.2-cocyc.es}%
\end{equation}
where $\mathbb{C}$ is central in $\widehat{L}$. Since all exact sequences of
vector spaces split, we can pick a splitting of this exact sequence on the
level of vector spaces, and thus identify $\widehat{L}$ with $L\oplus
\mathbb{C}$ as a vector space (not as a Lie algebra). Upon this
identification, the Lie bracket of $\widehat{L}$ can be written as%
\begin{equation}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]  =\left(
\left[  a,b\right]  ,\omega\left(  a,b\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for }a\in L\text{, }\alpha\in\mathbb{C}\text{, }b\in
L\text{, }\beta\in\mathbb{C}, \label{def.2-cocyc.form}%
\end{equation}
for some skew-symmetric bilinear form $\omega:L\times L\rightarrow\mathbb{C}$.
(We can also write this skew-symmetric bilinear form $\omega:L\times
L\rightarrow\mathbb{C}$ as a linear form $\wedge^{2}L\rightarrow\mathbb{C}$.)
But $\omega$ cannot be a completely arbitrary skew-symmetric bilinear form. It
needs to satisfy the so-called $2$\textit{-cocycle condition}%
\begin{equation}
\omega\left(  \left[  a,b\right]  ,c\right)  +\omega\left(  \left[
b,c\right]  ,a\right)  +\omega\left(  \left[  c,a\right]  ,b\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }a,b,c\in L. \label{def.2-cocyc.eq}%
\end{equation}
This condition comes from the requirement that the bracket in $\widehat{L}$
have to satisfy the Jacobi identity.

In the following, a $2$\textit{-cocycle on }$L$ will mean a skew-symmetric
bilinear form $\omega:L\times L\rightarrow\mathbb{C}$ (not necessarily
obtained from a central extension!) which satisfies the equation
(\ref{def.2-cocyc.eq}). (The name "$2$-cocycle" comes from Lie algebra
cohomology, where $2$-cocycles are indeed the cocycles in the $2$nd degree.)
Thus, we have assigned a $2$-cocycle on $L$ to every $1$-dimensional central
extension of $L$ (although the assignment depended on the splitting).

Conversely, if $\omega$ is any $2$-cocycle on $L$, then we can define a
central extension $\widehat{L}_{\omega}=L\oplus\mathbb{C}$ (whose Lie bracket
is defined by (\ref{def.2-cocyc.form})). Thus, every $2$-cocycle on $L$
canonically determines a $1$-dimensional central extension of $L$.

However, our assignment of the $2$-cocycle $\omega$ to the central extension
$\widehat{L}$ was not canonical, but depended on the splitting of the exact
sequence (\ref{def.2-cocyc.es}). If we change the splitting by some $\xi\in
L^{\ast}$, then $\omega$ is changed by $d\xi$ (this means that $\omega$ is
being replaced by $\omega+d\xi$), where $d\xi$ is the $2$-cocycle on $L$
defined by
\[
d\xi\left(  a,b\right)  =\xi\left(  \left[  a,b\right]  \right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in L.
\]
The $2$-cocycle $d\xi$ is called a $2$\textit{-coboundary}. As a conclusion,
$1$-dimensional central extensions of $L$ are parametrized up to isomorphism
by%
\[
\left(  2\text{-cocycles}\right)  \diagup\left(  2\text{-coboundaries}\right)
=H^{2}\left(  L\right)  .
\]
(Note that "up to isomorphism" means "up to isomorphism of extensions" here,
not "up to isomorphism of Lie algebras".) The vector space $H^{2}\left(
L\right)  $ is called the $2$\textit{-nd cohomology space} (or just the $2$-nd
cohomology) of the Lie algebra $L$.
\end{definition}

\begin{theorem}
\label{thm.H^2(W)}The space $H^{2}\left(  W\right)  $ is $1$-dimensional and
is spanned by the residue class of the $2$-cocycle $\omega$ given by%
\[
\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z}.
\]

\end{theorem}

Note that in this theorem, we could have replaced the factor $\dfrac{n^{3}%
-n}{6}$ by $n^{3}-n$ (since the vector space spanned by a vector obviously
doesn't change if we rescale the vector by a scalar factor), or even by
$n^{3}$ (since the $2$-cocycle $\left(  L_{n},L_{m}\right)  \mapsto
n\delta_{n,-m}$ is a coboundary, and two $2$-cocycles which differ by a
coboundary give the same residue class in $H^{2}\left(  W\right)  $). But we
prefer $\dfrac{n^{3}-n}{6}$ since this is closer to how this class appears in
representation theory (and, also, comes up in the proof below).

\textit{Proof of Theorem \ref{thm.H^2(W)}.} First of all, it is easy to prove
by computation that the bilinear form $\omega:W\times W\rightarrow\mathbb{C}$
given by%
\[
\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z}%
\]
is indeed a $2$-cocycle. Now, let us prove that every $2$-cocycle on $W$ is
congruent to a multiple of $\omega$ modulo the $2$-coboundaries.

Let $\beta$ be a $2$-cocycle on $W$. We must prove that $\beta$ is congruent
to a multiple of $\omega$ modulo the $2$-coboundaries.

Pick $\xi\in W^{\ast}$ such that $\xi\left(  L_{n}\right)  =\dfrac{1}{n}%
\beta\left(  L_{n},L_{0}\right)  $ for all $n\neq0$ (such a $\xi$ clearly
exists, but is not unique since we have complete freedom in choosing
$\xi\left(  L_{0}\right)  $). Let $\widetilde{\beta}$ be the $2$-cocycle
$\beta-d\xi$. Then,
\[
\widetilde{\beta}\left(  L_{n},L_{0}\right)  =\underbrace{\beta\left(
L_{n},L_{0}\right)  }_{\substack{=n\xi\left(  L_{n}\right)  \\\text{(since
}\xi\left(  L_{n}\right)  =\dfrac{1}{n}\beta\left(  L_{n},L_{0}\right)
\text{)}}}-\xi\left(  \underbrace{\left[  L_{n},L_{0}\right]  }_{=nL_{n}%
}\right)  =n\xi\left(  L_{n}\right)  -\xi\left(  nL_{n}\right)  =0
\]
for every $n\neq0$. Thus, by replacing $\beta$ by $\widetilde{\beta}$, we can
WLOG assume that $\beta\left(  L_{n},L_{0}\right)  =0$ for every $n\neq0$.
This clearly also holds for $n=0$ since $\beta$ is skew-symmetric. Hence,
$\beta\left(  X,L_{0}\right)  =0$ for every $X\in W$. Now, by the $2$-cocycle
condition, we have%
\[
\beta\left(  \left[  L_{0},L_{m}\right]  ,L_{n}\right)  +\beta\left(  \left[
L_{n},L_{0}\right]  ,L_{m}\right)  +\beta\left(  \left[  L_{m},L_{n}\right]
,L_{0}\right)  =0
\]
for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Thus,%
\begin{align*}
0  &  =\beta\left(  \underbrace{\left[  L_{0},L_{m}\right]  }_{=-mL_{m}}%
,L_{n}\right)  +\beta\left(  \underbrace{\left[  L_{n},L_{0}\right]
}_{=nL_{n}},L_{m}\right)  +\underbrace{\beta\left(  \left[  L_{m}%
,L_{n}\right]  ,L_{0}\right)  }_{=0\text{ (since }\beta\left(  X,L_{0}\right)
=0\text{ for every }X\in W\text{)}}\\
&  =-m\beta\left(  L_{m},L_{n}\right)  +n\underbrace{\beta\left(  L_{n}%
,L_{m}\right)  }_{=-\beta\left(  L_{m},L_{n}\right)  }=-\left(  m+n\right)
\beta\left(  L_{m},L_{n}\right)
\end{align*}
for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Hence, for all $n\in\mathbb{Z}$
and $m\in\mathbb{Z}$ with $n+m\neq0$, we have $\beta\left(  L_{m}%
,L_{n}\right)  =0$. In other words, there exists some sequence $\left(
b_{n}\right)  _{n\in\mathbb{Z}}\in\mathbb{C}^{\mathbb{Z}}$ such that
\begin{equation}
\beta\left(  L_{n},L_{m}\right)  =b_{n}\delta_{n,-m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\label{thm.H^2(W).pf.2}%
\end{equation}
This sequence satisfies
\begin{equation}
b_{-n}=-b_{n}\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}
\label{thm.H^2(W).pf.1}%
\end{equation}
(since $\beta$ is skew-symmetric and thus $\beta\left(  L_{n},L_{-n}\right)
=-\beta\left(  L_{-n},L_{n}\right)  $) and thus, in particular, $b_{0}=0$. We
will now try to get a recursive equation for this sequence.

Let $m$, $n$ and $p$ be three integers satisfying $m+n+p=0$. Then, the
$2$-cocycle condition yields%
\[
\beta\left(  \left[  L_{p},L_{n}\right]  ,L_{m}\right)  +\beta\left(  \left[
L_{m},L_{p}\right]  ,L_{n}\right)  +\beta\left(  \left[  L_{n},L_{m}\right]
,L_{p}\right)  =0.
\]
Due to%
\begin{align*}
\beta\left(  \underbrace{\left[  L_{p},L_{n}\right]  }_{=\left(  p-n\right)
L_{p+n}},L_{m}\right)   &  =\left(  p-n\right)  \underbrace{\beta\left(
L_{p+n},L_{m}\right)  }_{\substack{=-\beta\left(  L_{m},L_{p+n}\right)
\\\text{(since }\beta\text{ is skew-symmetric)}}}=-\left(  p-n\right)
\underbrace{\beta\left(  L_{m},L_{p+n}\right)  }_{\substack{=b_{m}%
\delta_{m,-\left(  p+n\right)  }\\\text{(by (\ref{thm.H^2(W).pf.2}))}}}\\
&  =-\left(  p-n\right)  b_{m}\underbrace{\delta_{m,-\left(  p+n\right)  }%
}_{\substack{=1\\\text{(since }m+n+p=0\text{)}}}=-\left(  p-n\right)  b_{m}%
\end{align*}
and the two cyclic permutations of this equality, this rewrites as%
\[
\left(  -\left(  p-n\right)  b_{m}\right)  +\left(  -\left(  m-p\right)
b_{n}\right)  +\left(  -\left(  n-m\right)  b_{p}\right)  =0.
\]
In other words,%
\begin{equation}
\left(  n-m\right)  b_{p}+\left(  m-p\right)  b_{n}+\left(  p-n\right)
b_{m}=0. \label{thm.H^2(W).pf.3}%
\end{equation}


Now define a form $\xi_{0}\in W^{\ast}$ by $\xi_{0}\left(  L_{0}\right)  =1$
and $\xi_{0}\left(  L_{i}\right)  =0$ for all $i\neq0$.

By replacing $\beta$ with $\beta-\dfrac{b_{1}}{2}d\xi_{0}$, we can assume WLOG
that $b_{1}=0$.

Now let $n\in\mathbb{Z}$ be arbitrary. Setting $m=1$ and $p=-\left(
n+1\right)  $ in (\ref{thm.H^2(W).pf.3}) (this is allowed since $1+n+\left(
-\left(  n+1\right)  \right)  =0$), we get%
\[
\left(  n-1\right)  b_{-\left(  n+1\right)  }+\left(  1-\left(  -\left(
n+1\right)  \right)  \right)  b_{n}+\left(  n-1\right)  b_{1}=0.
\]
Thus,%
\begin{align*}
0  &  =\left(  n-1\right)  \underbrace{b_{-\left(  n+1\right)  }}%
_{=-b_{n+1}\text{ (by (\ref{thm.H^2(W).pf.1}))}}+\underbrace{\left(  1-\left(
-\left(  n+1\right)  \right)  \right)  }_{=n+2}b_{n}+\left(  n-1\right)
\underbrace{b_{1}}_{=0}\\
&  =-\left(  n-1\right)  b_{n+1}+\left(  n+2\right)  b_{n},
\end{align*}
so that $\left(  n-1\right)  b_{n+1}=\left(  n+2\right)  b_{n}$. This
recurrence equation rewrites as $b_{n+1}=\dfrac{n+2}{n-1}b_{n}$ for $n\geq2$.
Thus, by induction we see that every $n\geq2$ satisfies%
\[
b_{n}=\dfrac{n+1}{n-2}\cdot\dfrac{n}{n-3}\cdot\dfrac{n-1}{n-4}\cdot
...\cdot\dfrac{4}{1}b_{2}=\dfrac{\left(  n+1\right)  \cdot n\cdot...\cdot
4}{\left(  n-2\right)  \cdot\left(  n-3\right)  \cdot...\cdot1}b_{2}%
=\dfrac{\left(  n+1\right)  \left(  n-1\right)  n}{6}b_{2}=\dfrac{n^{3}-n}%
{6}b_{2}.
\]
But $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$ also holds for $n=1$ (since $b_{1}=0$ and
$\dfrac{1^{3}-1}{6}=0$) and for $n=0$ (since $b_{0}=0$ and $\dfrac{0^{3}-0}%
{6}=0$). Hence, $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$ holds for every $n\geq0$. By
(\ref{thm.H^2(W).pf.1}), we conclude that $b_{n}=\dfrac{n^{3}-n}{6}b_{2}$
holds also for every $n\leq0$. Thus, every $n\in\mathbb{Z}$ satisfies
$b_{n}=\dfrac{n^{3}-n}{6}b_{2}$. From (\ref{thm.H^2(W).pf.2}), we thus see
that $\beta$ is a scalar multiple of $\omega$.

We thus have proven that every $2$-cocycle $\beta$ on $W$ is congruent to a
multiple of $\omega$ modulo the $2$-coboundaries. This yields that the space
$H^{2}\left(  W\right)  $ is \textit{at most }$1$-dimensional and is spanned
by the residue class of the $2$-cocycle $\omega$. In order to complete the
proof of Theorem \ref{thm.H^2(W)}, we have yet to prove that $H^{2}\left(
W\right)  $ is indeed $1$-dimensional (and not $0$-dimensional), i. e., that
the $2$-cocycle $\omega$ is \textit{not} a $2$-coboundary. But this is
easy\footnote{\textit{Proof.} Assume the contrary. Then, the $2$-cocycle
$\omega$ is a $2$-coboundary. This means that there exists a linear map
$\xi:W\rightarrow\mathbb{C}$ such that $\omega=d\xi$. Pick such a $\xi$. Then,%
\[
\omega\left(  L_{2},L_{-2}\right)  =\left(  d\xi\right)  \left(  L_{2}%
,L_{-2}\right)  =\xi\left(  \underbrace{\left[  L_{2},L_{-2}\right]
}_{=4L_{0}}\right)  =4\xi\left(  L_{0}\right)
\]
and%
\[
\omega\left(  L_{1},L_{-1}\right)  =\left(  d\xi\right)  \left(  L_{1}%
,L_{-1}\right)  =\xi\left(  \underbrace{\left[  L_{1},L_{-1}\right]
}_{=2L_{0}}\right)  =2\xi\left(  L_{0}\right)  .
\]
Hence,%
\[
2\underbrace{\omega\left(  L_{1},L_{-1}\right)  }_{=2\xi\left(  L_{0}\right)
}=4\xi\left(  L_{0}\right)  =\omega\left(  L_{2},L_{-2}\right)  .
\]
But this contradicts with the equalities $\omega\left(  L_{1},L_{-1}\right)
=0$ and $\omega\left(  L_{2},L_{-2}\right)  =1$ (which easily follow from the
definition of $\omega$). This contradiction shows that our assumption was
wrong, and thus the $2$-cocycle $\omega$ is not a $2$-coboundary, qed.}. The
proof of Theorem \ref{thm.H^2(W)} is thus complete.

The $2$-cocycle $\dfrac{1}{2}\omega$ (where $\omega$ is the $2$-cocycle
introduced in Theorem \ref{thm.H^2(W)}) gives a central extension of the Witt
algebra $W$: the so-called Virasoro algebra. Let us recast the definition of
this algebra in elementary terms:

\begin{definition}
The \textit{Virasoro algebra} $\operatorname*{Vir}$ is defined as the vector
space $W\oplus\mathbb{C}$ with Lie bracket defined by%
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}%
-n}{12}\delta_{n,-m}C;\\
\left[  L_{n},C\right]   &  =0,
\end{align*}
where $L_{n}$ denotes $\left(  L_{n},0\right)  $ for every $n\in\mathbb{Z}$,
and where $C$ denotes $\left(  0,1\right)  $. Note that $\left\{  L_{n}%
\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  C\right\}  $ is a basis of
$\operatorname*{Vir}$.
\end{definition}

If we change the denominator $12$ to any other nonzero complex number, we get
a Lie algebra isomorphic to $\operatorname*{Vir}$ (it is just a rescaling of
$C$). It is easy to show that the Virasoro algebra is not isomorphic to the
Lie-algebraic direct sum $W\oplus\mathbb{C}$. Thus, $\operatorname*{Vir}$ is
the unique (up to Lie algebra isomorphism) nontrivial $1$-dimensional central
extension of $W$.

\subsection{Affine Lie algebras}

We now define the notion

Now let us talk about affine Lie algebras:

\begin{definition}
\label{def.loop}Let $\mathfrak{g}$ be a Lie algebra.

\textbf{(a)} The $\mathbb{C}$-Lie algebra $\mathfrak{g}$ induces (by extension
of scalars) a $\mathbb{C}\left[  t,t^{-1}\right]  $-Lie algebra $\mathbb{C}%
\left[  t,t^{-1}\right]  \otimes\mathfrak{g}=\left\{  \sum\limits_{i}%
a_{i}t^{i}\ \mid\ a_{i}\in\mathfrak{g}\right\}  $. This Lie algebra
$\mathbb{C}\left[  t,t^{-1}\right]  \otimes\mathfrak{g}$, considered as a
$\mathbb{C}$-Lie algebra, will be called the \textit{loop algebra} of
$\mathfrak{g}$, and denoted by $\mathfrak{g}\left[  t,t^{-1}\right]  $.

\textbf{(b)} Let $\left(  \cdot,\cdot\right)  $ be a symmetric bilinear form
on $\mathfrak{g}$ (that is, a bilinear map $\mathfrak{g}\times\mathfrak{g}%
\rightarrow\mathbb{C}$) which is $\mathfrak{g}$-invariant (this means that
$\left(  \left[  a,b\right]  ,c\right)  +\left(  b,\left[  a,c\right]
\right)  =0$ for all $a,b,c\in\mathfrak{g}$).

Then, we can define a $2$-cocycle $\omega$ on the loop algebra $\mathfrak{g}%
\left[  t,t^{-1}\right]  $ by%
\begin{equation}
\omega\left(  f,g\right)  =\sum\limits_{i\in\mathbb{Z}}i\left(  f_{i}%
,g_{-i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }f\in\mathfrak{g}\left[
t,t^{-1}\right]  \text{ and }g\in\mathfrak{g}\left[  t,t^{-1}\right]
\label{loop.w}%
\end{equation}
(where we write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$
with $f_{i}\in\mathfrak{g}$, and where we write $g$ in the form $g=\sum
\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in\mathfrak{g}$).

Proving that $\omega$ is a $2$-cocycle is an exercise. So we can define a
$1$-dimensional central extension $\mathfrak{g}\left[  t,t^{-1}\right]
_{\omega}=\mathfrak{g}\left[  t,t^{-1}\right]  \oplus\mathbb{C}$ with bracket
defined by $\omega$.

We are going to abbreviate $\mathfrak{g}\left[  t,t^{-1}\right]  _{\omega}$ by
$\widehat{\mathfrak{g}}_{\omega}$, or, more radically, by
$\widehat{\mathfrak{g}}$.
\end{definition}

\begin{remark}
The equation (\ref{loop.w}) can be rewritten in the (laconical but suggestive)
form $\omega\left(  f,g\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
df,g\right)  $. Here, $\left(  df,g\right)  $ is to be understood as follows:
Extend the bilinear form $\left(  \cdot,\cdot\right)  :\mathfrak{g}%
\times\mathfrak{g}\rightarrow\mathbb{C}$ to a bilinear form $\left(
\cdot,\cdot\right)  :\mathfrak{g}\left[  t,t^{-1}\right]  \times
\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}\left[
t,t^{-1}\right]  $ by setting
\[
\left(  at^{i},bt^{j}\right)  =\left(  a,b\right)  t^{i+j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }b\in
\mathfrak{g}\text{, }i\in\mathbb{Z}\text{ and }j\in\mathbb{Z}.
\]
Also, for every $f\in\mathfrak{g}\left[  t,t^{-1}\right]  $, define the
"derivative" $f^{\prime}$ of $f$ to be the element $\sum\limits_{i\in
\mathbb{Z}}if_{i}t^{i-1}$ of $\mathfrak{g}\left[  t,t^{-1}\right]  $ (where we
write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$ with
$f_{i}\in\mathfrak{g}$). In analogy to the notation $dg=gdt$ which we
introduced in Definition \ref{def.diffform}, set $\left(  df,g\right)  $ to
mean the polynomial differential form $\left(  f^{\prime},g\right)  dt$ for
any $f\in\mathfrak{g}\left[  t,t^{-1}\right]  $ and $g\in\mathfrak{g}\left[
t,t^{-1}\right]  $. Then, it is very easy to see that $\operatorname*{Res}%
\nolimits_{t=0}\left(  df,g\right)  =\sum\limits_{i\in\mathbb{Z}}i\left(
f_{i},g_{-i}\right)  $ (where we write $f$ in the form $f=\sum\limits_{i\in
\mathbb{Z}}f_{i}t^{i}$ with $f_{i}\in\mathfrak{g}$, and where we write $g$ in
the form $g=\sum\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in
\mathfrak{g}$), so that we can rewrite (\ref{loop.w}) as $\omega\left(
f,g\right)  =\operatorname*{Res}\nolimits_{t=0}\left(  df,g\right)  $.
\end{remark}

We already know one example of the construction in Definition \ref{def.loop}:

\begin{remark}
If $\mathfrak{g}$ is the abelian Lie algebra $\mathbb{C}$, and $\left(
\cdot,\cdot\right)  $ is the bilinear form $\mathbb{C}\times\mathbb{C}%
\rightarrow\mathbb{C},\ \left(  x,y\right)  \mapsto xy$, then the $2$-cocycle
$\omega$ on the loop algebra $\mathbb{C}\left[  t,t^{-1}\right]  $ is given by%
\[
\omega\left(  f,g\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
gdf\right)  =\sum\limits_{i\in\mathbb{Z}}if_{i}g_{-i}%
\ \ \ \ \ \ \ \ \ \ \text{for every }f,g\in\mathbb{C}\left[  t,t^{-1}\right]
\]
(where we write $f$ in the form $f=\sum\limits_{i\in\mathbb{Z}}f_{i}t^{i}$
with $f_{i}\in\mathbb{C}$, and where we write $g$ in the form $g=\sum
\limits_{i\in\mathbb{Z}}g_{i}t^{i}$ with $g_{i}\in\mathbb{C}$). Hence, in this
case, the central extension $\mathfrak{g}\left[  t,t^{-1}\right]  _{\omega
}=\widehat{\mathfrak{g}}_{\omega}$ is precisely the Heisenberg algebra
$\mathcal{A}$ as introduced in Definition \ref{def.osc}.
\end{remark}

The main example that we will care about is when $\mathfrak{g}$ is a simple
finite-dimensional Lie algebra and $\left(  \cdot,\cdot\right)  $ is the
unique (up to scalar) invariant symmetric bilinear form (i. e., a multiple of
the Killing form). In this case, the Lie algebra $\widehat{\mathfrak{g}%
}=\widehat{\mathfrak{g}}_{\omega}$ is called an \textit{affine Lie algebra}.

\begin{theorem}
\label{thm.H^2(gtt)}If $\mathfrak{g}$ is a simple finite-dimensional Lie
algebra, then $H^{2}\left(  \mathfrak{g}\left[  t,t^{-1}\right]  \right)  $ is
$1$-dimensional and spanned by the cocycle $\omega$ corresponding to $\left(
\cdot,\cdot\right)  $.
\end{theorem}

\begin{corollary}
\label{cor.g_w^hat}If $\mathfrak{g}$ is a simple finite-dimensional Lie
algebra, then the Lie algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $ has a
unique (up to isomorphism of Lie algebras, not up to isomorphism of
extensions) nontrivial $1$-dimensional central extension
$\widehat{\mathfrak{g}}_{\omega}$.
\end{corollary}

\begin{definition}
\label{def.kac}The Lie algebra $\widehat{\mathfrak{g}}_{\omega}$ defined in
Corollary \ref{cor.g_w^hat} (for $\left(  \cdot,\cdot\right)  $ being the
Killing form of $\mathfrak{g}$) is called the \textit{affine Kac-Moody
algebra} corresponding to $\mathfrak{g}$. (Or, more precisely, the
\textit{untwisted affine Kac-Moody algebra} corresponding to $\mathfrak{g}$.)
\end{definition}

In order to prepare for the proof of Theorem \ref{thm.H^2(gtt)}, we recollect
some facts from the cohomology of Lie algebras:

\begin{definition}
\label{def.semidir}Let $\mathfrak{g}$ be a Lie algebra. Let $M$ be a
$\mathfrak{g}$-module. We define the \textit{semidirect product}
$\mathfrak{g}\ltimes M$ to be the Lie algebra which, as a vector space, is
$\mathfrak{g}\oplus M$, but whose Lie bracket is defined by%
\begin{align*}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]   &
=\left(  \left[  a,b\right]  ,a\rightharpoonup\beta-b\rightharpoonup
\alpha\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }a\in\mathfrak{g}\text{, }%
\alpha\in M\text{, }b\in\mathfrak{g}\text{ and }\beta\in M\right.  .
\end{align*}
(The symbol $\rightharpoonup$ means action here; i. e., a term like
$c\rightharpoonup m$ (with $c\in\mathfrak{g}$ and $m\in M$) means the action
of $c$ on $m$.) Thus, the canonical injection $\mathfrak{g}\rightarrow
\mathfrak{g}\ltimes M,$ $a\mapsto\left(  a,0\right)  $ is a Lie algebra
homomorphism, and so is the canonical projection $\mathfrak{g}\ltimes
M\rightarrow\mathfrak{g},$ $\left(  a,\alpha\right)  \mapsto a$. Also, $M$ is
embedded into $\mathfrak{g}\ltimes M$ by the injection $M\rightarrow
\mathfrak{g}\ltimes M,$ $\alpha\mapsto\left(  0,\alpha\right)  $; this makes
$M$ an abelian Lie subalgebra of $\mathfrak{g}\ltimes M$.
\end{definition}

All statements made in Definition \ref{def.semidir} (including the tacit
statement that the Lie bracket on $\mathfrak{g}\ltimes M$ defined in
Definition \ref{def.semidir} satisfies antisymmetry and the Jacobi identity)
are easy to verify by computation. The semidirect product that we have just
defined is not the most general notion of a semidirect product. We will later
(Definition \ref{def.semidir.lielie}) define a more general one, where $M$
itself may have a Lie algebra structure and this structure has an effect on
that of $\mathfrak{g}\ltimes M$. But for now, Definition \ref{def.semidir}
suffices for us.

\begin{definition}
Let $\mathfrak{g}$ be a Lie algebra. Let $M$ be a $\mathfrak{g}$-module.

\textbf{(a)} A $1$\textit{-cocycle} \textit{of }$\mathfrak{g}$\textit{ with
coefficients in }$M$ is a linear map $\eta:\mathfrak{g}\rightarrow M$ such
that%
\[
\eta\left(  \left[  a,b\right]  \right)  =a\rightharpoonup\eta\left(
b\right)  -b\rightharpoonup\eta\left(  a\right)  \ \ \ \ \ \ \ \ \ \ \text{for
all }a\in\mathfrak{g}\text{ and }b\in\mathfrak{g}.
\]
(The symbol $\rightharpoonup$ means action here; i. e., a term like
$c\rightharpoonup m$ (with $c\in\mathfrak{g}$ and $m\in M$) means the action
of $c$ on $m$.)

It is easy to see (and known) that $1$-cocycles of $\mathfrak{g}$ with
coefficients in $M$ are in bijection with Lie algebra homomorphisms
$\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M$. This bijection sends every
$1$-cocycle $\eta$ to the map $\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,$
$a\mapsto\left(  a,\eta\left(  a\right)  \right)  $.

Notice that $1$-cocycles of $\mathfrak{g}$ with coefficients in the
$\mathfrak{g}$-module $\mathfrak{g}$ are exactly the same as derivations of
$\mathfrak{g}$.

\textbf{(b)} A $1$\textit{-coboundary of }$\mathfrak{g}$ \textit{with
coefficients in }$M$ means a linear map $\eta:\mathfrak{g}\rightarrow M$ which
has the form $a\mapsto a\rightharpoonup m$ for some $m\in M$. Every
$1$-coboundary of $\mathfrak{g}$ with coefficients in $M$ is a $1$-cocycle.

\textbf{(c)} The space of $1$-cocycles of $\mathfrak{g}$ with coefficients in
$M$ is denoted by $Z^{1}\left(  \mathfrak{g},M\right)  $. The space of
$1$-coboundaries of $\mathfrak{g}$ with coefficients in $M$ is denoted by
$B^{1}\left(  \mathfrak{g},M\right)  $. We have $B^{1}\left(  \mathfrak{g}%
,M\right)  \subseteq Z^{1}\left(  \mathfrak{g},M\right)  $. The quotient space
$Z^{1}\left(  \mathfrak{g},M\right)  \diagup B^{1}\left(  \mathfrak{g}%
,M\right)  $ is denoted by $H^{1}\left(  \mathfrak{g},M\right)  $ is called
the $1$\textit{-st cohomology space} of $\mathfrak{g}$\textit{ with
coefficients in }$M$.

Of course, these spaces $Z^{1}\left(  \mathfrak{g},M\right)  $, $B^{1}\left(
\mathfrak{g},M\right)  $ and $H^{1}\left(  \mathfrak{g},M\right)  $ are but
particular cases of more general constructions $Z^{i}\left(  \mathfrak{g}%
,M\right)  $, $B^{i}\left(  \mathfrak{g},M\right)  $ and $H^{i}\left(
\mathfrak{g},M\right)  $ which are defined for every $i\in\mathbb{N}$. (In
particular, $H^{0}\left(  \mathfrak{g},M\right)  $ is the subspace $\left\{
m\in M\ \mid\ a\rightharpoonup m=0\text{ for all }a\in\mathfrak{g}\right\}  $
of $M$, and often denoted by $M^{\mathfrak{g}}$.) The spaces $H^{i}\left(
\mathfrak{g},M\right)  $ (or, more precisely, the functors assigning these
spaces to every $\mathfrak{g}$-module $M$) can be understood as the so-called
derived functors of the functor $M\mapsto M^{\mathfrak{g}}$. However, we won't
use $H^{i}\left(  \mathfrak{g},M\right)  $ for any $i$ other than $1$ here.

We record a relation between $H^{1}\left(  \mathfrak{g},M\right)  $ and the
$\operatorname*{Ext}$ bifunctor:%
\[
H^{1}\left(  \mathfrak{g},M\right)  =\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)  .
\]
More generally, $\operatorname*{Ext}\nolimits_{\mathfrak{g}}^{1}\left(
N,M\right)  =H^{1}\left(  \mathfrak{g},\operatorname*{Hom}%
\nolimits_{\mathbb{C}}\left(  N,M\right)  \right)  $ for any two
$\mathfrak{g}$-modules $N$ and $M$.
\end{definition}

\begin{theorem}
[Whitehead]\label{thm.white}If $\mathfrak{g}$ is a simple finite-dimensional
Lie algebra, and $M$ is a finite-dimensional $\mathfrak{g}$-module, then
$H^{1}\left(  \mathfrak{g},M\right)  =0$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.white}.} Since $\mathfrak{g}$ is a simple
Lie algebra, Weyl's theorem says that finite-dimensional $\mathfrak{g}%
$-modules are completely reducible. Hence, if $N$ and $M$ are
finite-dimensional $\mathfrak{g}$-modules, we have $\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  N,M\right)  =0$. In particular,
$\operatorname*{Ext}\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)
=0$. Since $H^{1}\left(  \mathfrak{g},M\right)  =\operatorname*{Ext}%
\nolimits_{\mathfrak{g}}^{1}\left(  \mathbb{C},M\right)  $, this yields
$H^{1}\left(  \mathfrak{g},M\right)  =0$. Theorem \ref{thm.white} is thus proven.

\begin{lemma}
\label{lem.Z^1}Let $\omega$ be a $2$-cocycle on a Lie algebra $\mathfrak{g}$.
Let $\mathfrak{g}_{0}\subseteq\mathfrak{g}$ be a Lie subalgebra, and
$M\subseteq\mathfrak{g}$ be a $\mathfrak{g}_{0}$-submodule. Then, $\omega
\mid_{\mathfrak{g}_{0}\times M}$, when considered as a map $\mathfrak{g}%
_{0}\rightarrow M^{\ast}$, belongs to $Z^{1}\left(  \mathfrak{g}_{0},M^{\ast
}\right)  $.
\end{lemma}

The proof of Lemma \ref{lem.Z^1} is a straightforward manipulation of formulas:

\textit{Proof of Lemma \ref{lem.Z^1}.} Let $\eta$ denote the $2$-cocycle
$\omega\mid_{\mathfrak{g}_{0}\times M}$, considered as a map $\mathfrak{g}%
_{0}\rightarrow M^{\ast}$. Thus, $\eta$ is defined by%
\[
\eta\left(  x\right)  =\left(  M\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ y\mapsto\omega\left(  x,y\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}_{0}.
\]
Hence,
\begin{equation}
\left(  \eta\left(  x\right)  \right)  \left(  y\right)  =\omega\left(
x,y\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}_{0}\text{ and
}y\in M. \label{pf.Z^1.eta}%
\end{equation}


Thus, any $a\in\mathfrak{g}_{0}$, $b\in\mathfrak{g}_{0}$ and $c\in M$ satisfy
$\left(  \eta\left(  \left[  a,b\right]  \right)  \right)  \left(  c\right)
=\omega\left(  \left[  a,b\right]  ,c\right)  $ and%
\begin{align*}
&  \left(  a\rightharpoonup\eta\left(  b\right)  -b\rightharpoonup\eta\left(
a\right)  \right)  \left(  c\right) \\
&  =\underbrace{\left(  a\rightharpoonup\eta\left(  b\right)  \right)  \left(
c\right)  }_{\substack{=-\left(  \eta\left(  b\right)  \right)  \left(
\left[  a,c\right]  \right)  \\\text{(by the definition of the dual of a
}\mathfrak{g}_{0}\text{-module)}}}-\underbrace{\left(  b\rightharpoonup
\eta\left(  a\right)  \right)  \left(  c\right)  }_{\substack{=-\left(
\eta\left(  a\right)  \right)  \left(  \left[  b,c\right]  \right)
\\\text{(by the definition of the dual of a }\mathfrak{g}_{0}\text{-module)}%
}}\\
&  =\left(  -\underbrace{\left(  \eta\left(  b\right)  \right)  \left(
\left[  a,c\right]  \right)  }_{\substack{=\omega\left(  b,\left[  a,c\right]
\right)  \\\text{(by (\ref{pf.Z^1.eta}))}}}\right)  -\left(
-\underbrace{\left(  \eta\left(  a\right)  \right)  \left(  \left[
b,c\right]  \right)  }_{\substack{=\omega\left(  a,\left[  b,c\right]
\right)  \\\text{(by (\ref{pf.Z^1.eta}))}}}\right)  =\left(  -\omega\left(
b,\left[  a,c\right]  \right)  \right)  -\left(  -\omega\left(  a,\left[
b,c\right]  \right)  \right) \\
&  =-\omega\left(  b,\underbrace{\left[  a,c\right]  }_{=-\left[  c,a\right]
}\right)  +\omega\left(  a,\left[  b,c\right]  \right)  =\underbrace{\omega
\left(  b,\left[  c,a\right]  \right)  }_{\substack{=-\omega\left(  \left[
c,a\right]  ,b\right)  \\\text{(since }\omega\text{ is antisymmetric)}%
}}+\underbrace{\omega\left(  a,\left[  b,c\right]  \right)  }%
_{\substack{=-\omega\left(  \left[  b,c\right]  ,a\right)  \\\text{(since
}\omega\text{ is antisymmetric)}}}\\
&  =-\omega\left(  \left[  c,a\right]  ,b\right)  -\omega\left(  \left[
b,c\right]  ,a\right)  =\omega\left(  \left[  a,b\right]  ,c\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.2-cocyc.eq})}\right)  ,
\end{align*}
so that $\left(  \eta\left(  \left[  a,b\right]  \right)  \right)  \left(
c\right)  =\left(  a\rightharpoonup\eta\left(  b\right)  -b\rightharpoonup
\eta\left(  a\right)  \right)  \left(  c\right)  $. Thus, any $a\in
\mathfrak{g}_{0}$ and $b\in\mathfrak{g}_{0}$ satisfy $\eta\left(  \left[
a,b\right]  \right)  =a\rightharpoonup\eta\left(  b\right)  -b\rightharpoonup
\eta\left(  a\right)  $. This shows that $\eta$ is a $1$-cocycle, i. e.,
belongs to $Z^{1}\left(  \mathfrak{g}_{0},M^{\ast}\right)  $. Lemma
\ref{lem.Z^1} is proven.

\textit{Proof of Theorem \ref{thm.H^2(gtt)}.} First notice that any
$a,b,c\in\mathfrak{g}$ satisfy%
\begin{equation}
\left(  \left[  a,b\right]  ,c\right)  =\left(  \left[  b,c\right]  ,a\right)
=\left(  \left[  c,a\right]  ,b\right)  \label{thm.H^2(gtt).pf.0}%
\end{equation}
\footnote{\textit{Proof.} First of all, any $a,b,c\in\mathfrak{g}$ satisfy%
\begin{align*}
\left(  \left[  a,b\right]  ,c\right)   &  =\left(  a,\left[  b,c\right]
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the form }\left(  \cdot
,\cdot\right)  \text{ is invariant}\right) \\
&  =\left(  \left[  b,c\right]  ,a\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the form }\left(  \cdot,\cdot\right)  \text{ is symmetric}\right)
.
\end{align*}
Applying this to $b,c,a$ instead of $a,b,c$, we obtain $\left(  \left[
b,c\right]  ,a\right)  =\left(  \left[  c,a\right]  ,b\right)  $. Hence,
$\left(  \left[  a,b\right]  ,c\right)  =\left(  \left[  b,c\right]
,a\right)  =\left(  \left[  c,a\right]  ,b\right)  $, so that
(\ref{thm.H^2(gtt).pf.0}) is proven.}. Moreover,%
\begin{equation}
\text{there exist }a,b,c\in\mathfrak{g}\text{ such that }\left(  \left[
a,b\right]  ,c\right)  =\left(  \left[  b,c\right]  ,a\right)  =\left(
\left[  c,a\right]  ,b\right)  \neq0. \label{thm.H^2(gtt).pf.00}%
\end{equation}
\footnote{\textit{Proof.} Since $\mathfrak{g}$ is simple, we have $\left[
\mathfrak{g},\mathfrak{g}\right]  =\mathfrak{g}$ and thus $\left(  \left[
\mathfrak{g},\mathfrak{g}\right]  ,\mathfrak{g}\right)  =\left(
\mathfrak{g},\mathfrak{g}\right)  \neq0$ (since the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate). Hence, there exist $a,b,c\in\mathfrak{g}$
such that $\left(  \left[  a,b\right]  ,c\right)  \neq0$. The rest is handled
by (\ref{thm.H^2(gtt).pf.0}).} This will be used later in our proof; but as
for now, forget about these $a,b,c$.

It is easy to see that the $2$-cocycle $\omega$ on $\mathfrak{g}\left[
t,t^{-1}\right]  $ defined by (\ref{loop.w}) is not a $2$%
-coboundary.\footnote{\textit{Proof.} Assume the contrary. Then, this
$2$-cocycle $\omega$ is a coboundary, i. e., there exists a linear map
$\xi:\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$ such that
$\omega=d\xi$.
\par
Now, pick some $a\in\mathfrak{g}$ and $b\in\mathfrak{g}$ such that $\left(
a,b\right)  \neq0$ (this is possible since the form $\left(  \cdot
,\cdot\right)  $ is nondegenerate). Then,%
\[
\underbrace{\omega}_{=d\xi}\left(  at,bt^{-1}\right)  =\left(  d\xi\right)
\left(  at,bt^{-1}\right)  =\xi\left(  \underbrace{\left[  at,bt^{-1}\right]
}_{=\left[  a,b\right]  }\right)  =\xi\left(  \left[  a,b\right]  \right)
\]
and%
\[
\underbrace{\omega}_{=d\xi}\left(  a,b\right)  =\left(  d\xi\right)  \left(
a,b\right)  =\xi\left(  \left[  a,b\right]  \right)  ,
\]
so that $\omega\left(  at,bt^{-1}\right)  =\omega\left(  a,b\right)  $. But by
the definition of $\omega$, we easily see that $\omega\left(  at,bt^{-1}%
\right)  =1\underbrace{\left(  a,b\right)  }_{\neq0}\neq0$ and $\omega\left(
a,b\right)  =0\left(  a,b\right)  =0$, which yields a contradiction.}

Now let us consider the structure of $\mathfrak{g}\left[  t,t^{-1}\right]  $.
We have $\mathfrak{g}\left[  t,t^{-1}\right]  =\bigoplus\limits_{n\in
\mathbb{Z}}\mathfrak{g}t^{n}\supseteq\mathfrak{g}t^{0}=\mathfrak{g}$. This is,
actually, an inclusion of Lie algebras. So $\mathfrak{g}$ is a Lie subalgebra
of $\mathfrak{g}\left[  t,t^{-1}\right]  $, and $\mathfrak{g}t^{n}$ is a
$\mathfrak{g}$-submodule of $\mathfrak{g}\left[  t,t^{-1}\right]  $ isomorphic
to $\mathfrak{g}$ for every $n\in\mathbb{Z}$.

Let $\omega$ be an arbitrary $2$-cocycle on $\mathfrak{g}\left[
t,t^{-1}\right]  $ (not necessarily the one defined by (\ref{loop.w})).

Let $n\in\mathbb{Z}$. Then, $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}%
$, when considered as a map $\mathfrak{g}\rightarrow\left(  \mathfrak{g}%
t^{n}\right)  ^{\ast}$, belongs to $Z^{1}\left(  \mathfrak{g},\left(
\mathfrak{g}t^{n}\right)  ^{\ast}\right)  $ (by Lemma \ref{lem.Z^1}, applied
to $\mathfrak{g}$, $\mathfrak{g}t^{n}$ and $\mathfrak{g}\left[  t,t^{-1}%
\right]  $ instead of $\mathfrak{g}_{0}$, $M$ and $\mathfrak{g}$), i. e., is a
$1$-cocycle. But by Theorem \ref{thm.white}, we have $H^{1}\left(
\mathfrak{g},\left(  \mathfrak{g}t^{n}\right)  ^{\ast}\right)  =0$, so this
rewrites as $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}\in B^{1}\left(
\mathfrak{g},\left(  \mathfrak{g}t^{n}\right)  ^{\ast}\right)  $. In other
words, there exists some $\xi_{n}\in\left(  \mathfrak{g}t^{n}\right)  ^{\ast}$
such that $\omega\mid_{\mathfrak{g}\times\mathfrak{g}t^{n}}=d\xi_{n}$. Pick
such a $\xi_{n}$. Thus,%
\[
\omega\left(  a,bt^{n}\right)  =\underbrace{\left(  \omega\mid_{\mathfrak{g}%
\times\mathfrak{g}t^{n}}\right)  }_{=d\xi_{n}}\left(  a,bt^{n}\right)
=\left(  d\xi_{n}\right)  \left(  a,bt^{n}\right)  =\xi_{n}\left(  \left[
a,bt^{n}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in
\mathfrak{g}.
\]


Define a map $\xi:\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathbb{C}$
by requiring that $\xi\mid_{\mathfrak{g}t^{n}}=\xi_{n}$ for every
$n\in\mathbb{Z}$.

Now, let $\widetilde{\omega}=\omega-d\xi$. Then,%
\[
\widetilde{\omega}\left(  x,y\right)  =\omega\left(  x,y\right)  -\xi\left(
\left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
x,y\in\mathfrak{g}\left[  t,t^{-1}\right]  .
\]
Replace $\omega$ by $\widetilde{\omega}$ (this doesn't change the residue
class of $\omega$ in $H^{2}\left(  \mathfrak{g}\left[  t,t^{-1}\right]
\right)  $, since $\widetilde{\omega}$ differs from $\omega$ by a
$2$-coboundary). By doing this, we have reduced to a situation when
\[
\omega\left(  a,bt^{n}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
a,b\in\mathfrak{g}\text{ and }n\in\mathbb{Z}.
\]
\footnote{But all the $\xi$-freedom has been used up in this reduction - i.
e., if the new $\omega$ is nonzero, then the original $\omega$ was not a
$2$-coboundary. This gives us an alternative way of proving that the
$2$-cocycle $\omega$ on $\mathfrak{g}\left[  t,t^{-1}\right]  $ defined by
(\ref{loop.w}) is not a $2$-coboundary.} Since $\omega$ is antisymmetric, this
yields%
\begin{equation}
\omega\left(  bt^{n},a\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
a,b\in\mathfrak{g}\text{ and }n\in\mathbb{Z}. \label{thm.H^2(gtt).pf.1}%
\end{equation}


Now, fix some $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Since $\omega$ is a
$2$-cocycle, the $2$-cocycle condition yields%
\begin{align*}
0  &  =\omega\left(  \underbrace{\left[  a,bt^{n}\right]  }_{=\left[
a,b\right]  t^{n}},ct^{m}\right)  +\omega\left(  \underbrace{\left[
ct^{m},a\right]  }_{\substack{=\left[  c,a\right]  t^{m}\\=-\left[
a,c\right]  t^{m}}},bt^{n}\right)  +\omega\left(  \underbrace{\left[
bt^{n},ct^{m}\right]  }_{=\left[  b,c\right]  t^{n+m}},a\right) \\
&  =\omega\left(  \left[  a,b\right]  t^{n},ct^{m}\right)  +\underbrace{\omega
\left(  -\left[  a,c\right]  t^{m},bt^{n}\right)  }_{=\omega\left(
bt^{n},\left[  a,c\right]  t^{m}\right)  }+\underbrace{\omega\left(  \left[
b,c\right]  t^{n+m},a\right)  }_{\substack{=0\\\text{(by
(\ref{thm.H^2(gtt).pf.1}))}}}\\
&  =\omega\left(  \left[  a,b\right]  t^{n},ct^{m}\right)  +\omega\left(
bt^{n},\left[  a,c\right]  t^{m}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a,b,c\in\mathfrak{g}\text{.}%
\end{align*}
In other words, the bilinear form on $\mathfrak{g}$ given by $\left(
b,c\right)  \mapsto\omega\left(  bt^{n},ct^{m}\right)  $ is $\mathfrak{g}%
$-invariant. But every $\mathfrak{g}$-invariant bilinear form on
$\mathfrak{g}$ must be a multiple of our bilinear form $\left(  \cdot
,\cdot\right)  $ (since $\mathfrak{g}$ is simple, and thus the space of all
$\mathfrak{g}$-invariant bilinear forms on $\mathfrak{g}$ is $1$%
-dimensional\footnote{and spanned by the Killing form}). Hence, there exists
some constant $\gamma_{n,m}\in\mathbb{C}$ (depending on $n$ and $m$) such
that
\begin{equation}
\omega\left(  bt^{n},ct^{m}\right)  =\gamma_{n,m}\cdot\left(  b,c\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }b,c\in\mathfrak{g}.
\label{thm.H^2(gtt).pf.2}%
\end{equation}
It is easy to see that%
\begin{equation}
\gamma_{n,m}=-\gamma_{m,n}\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z},
\label{thm.H^2(gtt).pf.3}%
\end{equation}
since the bilinear form $\omega$ is skew-symmetric whereas the bilinear form
$\left(  \cdot,\cdot\right)  $ is symmetric.

Now, for any $m\in\mathbb{Z}$, $n\in\mathbb{Z}$ and $p\in\mathbb{Z}$, the
$2$-cocycle condition yields%
\[
\omega\left(  \left[  at^{n},bt^{m}\right]  ,ct^{p}\right)  +\omega\left(
\left[  bt^{m},ct^{p}\right]  ,at^{n}\right)  +\omega\left(  \left[
ct^{p},at^{n}\right]  ,bt^{m}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all
}a,b,c\in\mathfrak{g}.
\]
Due to%
\[
\omega\left(  \underbrace{\left[  at^{n},bt^{m}\right]  }_{=\left[
a,b\right]  t^{n+m}},ct^{p}\right)  =\omega\left(  \left[  a,b\right]
t^{n+m},ct^{p}\right)  =\gamma_{n+m,p}\cdot\left(  \left[  a,b\right]
,c\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.H^2(gtt).pf.2}%
)}\right)
\]
and the two cyclic permutations of this identity, this rewrites as%
\[
\gamma_{n+m,p}\cdot\left(  \left[  a,b\right]  ,c\right)  +\gamma_{m+p,n}%
\cdot\left(  \left[  b,c\right]  ,a\right)  +\gamma_{p+n,m}\cdot\left(
\left[  c,a\right]  ,b\right)  =0.
\]
Since this holds for all $a,b,c\in\mathfrak{g}$, we can use
(\ref{thm.H^2(gtt).pf.00}) to transform this into%
\[
\gamma_{n+m,p}+\gamma_{m+p,n}+\gamma_{p+n,m}=0.
\]
Due to (\ref{thm.H^2(gtt).pf.3}), this rewrites as%
\[
\gamma_{n,m+p}+\gamma_{m,p+n}+\gamma_{p,m+n}=0.
\]
Denoting by $s$ the sum $m+n+p$, we can rewrite this as%
\[
\gamma_{n,s-n}+\gamma_{m,s-m}-\gamma_{m+n,s-m-n}=0.
\]
In other words, for fixed $s\in\mathbb{Z}$, the function $\mathbb{Z}%
\rightarrow\mathbb{C},$ $n\mapsto\gamma_{n,s-n}$ is additive. Hence,
$\gamma_{n,s-n}=n\gamma_{1,s-1}$ and $\gamma_{s-n,n}=\left(  s-n\right)
\gamma_{1,s-1}$ for every $n\in\mathbb{Z}$. Thus,
\begin{align*}
\left(  s-n\right)  \gamma_{1,s-1}  &  =\gamma_{s-n,n}=-\gamma_{n,s-n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.H^2(gtt).pf.3})}\right) \\
&  =-n\gamma_{1,s-1}\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}%
\end{align*}
Hence, $s\gamma_{1,s-1}=0$. Thus, for every $s\neq0$, we conclude that
$\gamma_{1,s-1}=0$ and hence $\gamma_{n,s-n}=n\underbrace{\gamma_{1,s-1}}%
_{=0}=0$ for every $n\in\mathbb{Z}$. In other words, $\gamma_{n,m}=0$ for
every $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfying $n+m\neq0$.

What happens for $s=0$ ? For $s=0$, the equation $\gamma_{n,s-n}%
=n\gamma_{1,s-1}$ becomes $\gamma_{n,-n}=n\gamma_{1,-1}$.

Thus we have proven that $\gamma_{n,m}=0$ for every $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$ satisfying $n+m\neq0$, and that every $n\in\mathbb{Z}$
satisfies $\gamma_{n,-n}=n\gamma_{1,-1}$.

Hence, the form $\omega$ must be a scalar multiple of the form which sends
every $\left(  f,g\right)  $ to $\operatorname*{Res}\nolimits_{t=0}%
\underbrace{\left(  df,g\right)  }_{\text{scalar-valued }1\text{-form}}%
=\sum\limits_{i\in\mathbb{Z}}i\left(  f_{i},g_{-i}\right)  $. We have thus
proven that every $2$-cocycle $\omega$ is a scalar multiple of the $2$-cocycle
$\omega$ defined by (\ref{loop.w}) modulo the $2$-coboundaries. Since we also
know that the $2$-cocycle $\omega$ defined by (\ref{loop.w}) is not a
$2$-coboundary, this yields that the space $H^{2}\left(  \mathfrak{g}\left[
t,t^{-1}\right]  \right)  $ is $1$-dimensional and spanned by the residue
class of the $2$-cocycle $\omega$ defined by (\ref{loop.w}). This proves
Theorem \ref{thm.H^2(gtt)}.

\section{Representation theory: generalities}

\subsection{Representation theory: general facts}

The first step in the representation theory of any objects (groups, algebras,
etc.) is usually proving some kind of Schur's lemma. There is one form of
Schur's lemma that holds almost tautologically: This is the form that claims
that every morphism between irreducible representations is either $0$ or an
isomorphism.\footnote{There are also variations on this assertion:
\par
\textbf{1)} Every morphism from an irreducible representation to a
representation is either $0$ or injective.
\par
\textbf{2)} Every morphism from a representation to an irreducible
representation is either $0$ or surjective.
\par
Both of these variations follow very easily from the definition of
"irreducible".} However, the more often used form of Schur's lemma is a bit
different: It claims that, over an algebraically closed field, every
endomorphism of a finite-dimensional irreducible representation is a scalar
multiple of the identity map. This is usually proven using eigenvalues, and
this proof depends on the fact that eigenvalues exist; this (in general)
requires the irreducible representation to be \textit{finite-dimensional}.
Hence, it should not come as a surprise that this latter form of Schur's lemma
does not generally hold for infinite-dimensional representations. This makes
this lemma not particularly useful in the case of infinite-dimensional Lie
algebras. But we still can show the following version of Schur's lemma over
$\mathbb{C}$:

\begin{lemma}
[Dixmier's Lemma]\label{lem.dix}Let $A$ be an algebra over $\mathbb{C}$, and
let $V$ be an irreducible $A$-module of countable dimension. Then, any
$A$-module homomorphism $\phi:V\rightarrow V$ is a scalar multiple of the identity.
\end{lemma}

This lemma is called \textit{Dixmier's lemma}, and its proof is similar to the
famous proof of the Nullstellensatz over $\mathbb{C}$ using the uncountability
of $\mathbb{C}$.

\textit{Proof of Lemma \ref{lem.dix}.} Let $D=\operatorname*{End}%
\nolimits_{A}V$. Then, $D$ is a division algebra (in fact, the endomorphism
ring of an irreducible representation always is a division algebra).

For any nonzero $v\in V$, we have $Av=V$ (otherwise, $Av$ would be a nonzero
proper $A$-submodule of $V$, contradicting the fact that $V$ is irreducible
and thus does not have any such submodules). In other words, for any nonzero
$v\in V$, every element of $V$ can be written as $av$ for some $a\in A$. Thus,
for any nonzero $v\in V$, any element $\phi\in D$ is completely determined by
$\phi\left(  v\right)  $ (because $\phi\left(  av\right)  =a\phi\left(
v\right)  $ for every $a\in A$, so that the value $\phi\left(  v\right)  $
uniquely determines the value of $\phi\left(  av\right)  $ for every $a\in A$,
and thus (since we know that every element of $V$ can be written as $av$ for
some $a\in A$) every value of $\phi$ is uniquely determined). Thus, we have an
embedding of $D$ into $V$. Hence, $D$ is countably-dimensional (since $V$ is
countably-dimensional). But a countably-dimensional division algebra $D$ over
$\mathbb{C}$ must be $\mathbb{C}$ itself\footnote{\textit{Proof.} Indeed,
assume the contrary. So there exists some $\phi\in D$ not belonging to
$\mathbb{C}$. Then, $\phi$ is transcendental over $\mathbb{C}$, so that
$\mathbb{C}\left(  \phi\right)  \subseteq D$ is the field of rational
functions in one variable $\phi$ over $\mathbb{C}$. Now, $\mathbb{C}\left(
\phi\right)  $ contains the rational function $\dfrac{1}{\phi-\lambda}$ for
every $\lambda\in\mathbb{C}$, and these rational functions for varying
$\lambda$ are linearly independent. Since $\mathbb{C}$ is uncountable, we thus
have an uncountable linearly independent set of elements of $\mathbb{C}\left(
\phi\right)  $, contradicting the fact that $\mathbb{C}\left(  \phi\right)  $
is a subspace of the countably-dimensional space $D$, qed.}, so that
$D=\mathbb{C}$, and this is exactly what we wanted to show. Lemma
\ref{lem.dix} is proven.

Note that Lemma \ref{lem.dix} is a general fact, not particular to Lie
algebras; however, it is not as general as it seems: It really makes use of
the uncountability of $\mathbb{C}$, not just of the fact that $\mathbb{C}$ is
an algebraically closed field of characteristic $0$. It would be wrong if we
would replace $\mathbb{C}$ by (for instance) the algebraic closure of
$\mathbb{Q}$.

\begin{remark}
\label{rem.dix}Let $A$ be a countably-dimensional algebra over $\mathbb{C}$,
and let $V$ be an irreducible $A$-module. Then, $V$ itself is countably dimensional.
\end{remark}

\textit{Proof of Remark \ref{rem.dix}.} For any nonzero $v\in V$, we have
$Av=V$ (by the same argument as in the proof of Lemma \ref{lem.dix}), and thus
$\dim\left(  Av\right)  =\dim V$. Since $\dim\left(  Av\right)  \leq\dim A$,
we thus have $\dim V=\dim\left(  Av\right)  \leq\dim A$, so that $V$ has
countable dimension (since $A$ has countable dimension). This proves Remark
\ref{rem.dix}.

\begin{corollary}
\label{cor.dix2}Let $A$ be an algebra over $\mathbb{C}$, and let $V$ be an
irreducible $A$-module of countable dimension. Let $C$ be a central element of
$A$. Then, $C\mid_{V}$ is a scalar (i. e., a scalar multiple of the identity map).
\end{corollary}

\textit{Proof of Corollary \ref{cor.dix2}.} Since $C$ is central, the element
$C$ commutes with any element of $A$. Thus, $C\mid_{V}$ is an $A$-module
homomorphism, and hence (by Lemma \ref{lem.dix}, applied to $\phi=C\mid_{V}$)
a scalar multiple of the identity. This proves Corollary \ref{cor.dix2}.

\subsection{Representations of the Heisenberg algebra $\mathcal{A}$}

\subsubsection{General remarks}

Consider the oscillator algebra (aka Heisenberg algebra) $\mathcal{A}%
=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle +\left\langle
K\right\rangle $. Recall that%
\begin{align*}
\left[  a_{i},a_{j}\right]   &  =i\delta_{i,-j}K\ \ \ \ \ \ \ \ \ \ \text{for
any }i,j\in\mathbb{Z};\\
\left[  K,a_{i}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for any }%
i\in\mathbb{Z}.
\end{align*}


Let us try to classify the irreducible $\mathcal{A}$-modules.

Let $V$ be an irreducible $\mathcal{A}$-module. Then, $V$ is
countably-dimensional (by Remark \ref{rem.dix}, since $U\left(  \mathcal{A}%
\right)  $ is countably-dimensional), so that by Corollary \ref{cor.dix2}, the
endomorphism $K\mid_{V}$ is a scalar (because $K$ is a central element of
$\mathcal{A}$ and thus also a central element of $U\left(  \mathcal{A}\right)
$).

If $K\mid_{V}=0$, then $V$ is a module over the Lie algebra $\mathcal{A}%
\diagup\mathbb{C}K=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle $.
But since $\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\right\rangle $ is an
abelian Lie algebra, irreducible modules over $\left\langle a_{i}\ \mid
\ i\in\mathbb{Z}\right\rangle $ are $1$-dimensional (again by Corollary
\ref{cor.dix2}), so that $V$ must be $1$-dimensional in this case. Thus, the
case when $K\mid_{V}=0$ is not an interesting case.

Now consider the case when $K\mid_{V}=k\neq0$. Then, we can WLOG assume that
$k=1$, because the Lie algebra $\mathcal{A}$ has an automorphism sending $K$
to $\lambda K$ for any arbitrary $\lambda\neq0$ (this automorphism is given by
$a_{i}\mapsto\lambda a_{i}$ for $i>0$, and $a_{i}\mapsto a_{i}$ for $i\leq0$).

We are thus interested in irreducible representations $V$ of $\mathcal{A}$
satisfying $K\mid_{V}=1$. These are in an obvious 1-to-1 correspondence with
irreducible representations of $U\left(  \mathcal{A}\right)  \diagup\left(
K-1\right)  $.

\begin{proposition}
\label{prop.K-1}We have an algebra isomorphism%
\[
\xi:U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  \rightarrow
D\left(  x_{1},x_{2},x_{3},...\right)  \otimes\mathbb{C}\left[  x_{0}\right]
,
\]
where $D\left(  x_{1},x_{2},x_{3},...\right)  $ is the algebra of differential
operators in the variables $x_{1}$, $x_{2}$, $x_{3}$, $...$ with polynomial
coefficients. This isomorphism is given by%
\begin{align*}
\xi\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{0}\right)   &  =x_{0}.
\end{align*}

\end{proposition}

Note that we are sloppy with notation here: Since $\xi$ is a homomorphism from
$U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  $ (rather than
$U\left(  \mathcal{A}\right)  $), we should write $\xi\left(  \overline
{a_{-i}}\right)  $ instead of $\xi\left(  a_{-i}\right)  $, etc.. We are using
the same letters to denote elements of $U\left(  \mathcal{A}\right)  $ and
their residue classes in $U\left(  \mathcal{A}\right)  \diagup\left(
K-1\right)  $, and are relying on context to keep them apart. We hope that the
reader will forgive us this abuse of notation.

\textit{Proof of Proposition \ref{prop.K-1}.} It is clear\footnote{from the
universal property of the universal enveloping algebra, and the universal
property of the quotient algebra} that there exists a unique algebra
homomorphism $\xi:U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)
\rightarrow D\left(  x_{1},x_{2},x_{3},...\right)  $ satisfying%
\begin{align*}
\xi\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1;\\
\xi\left(  a_{0}\right)   &  =x_{0}.
\end{align*}
It is also clear that this $\xi$ is surjective (since all the generators
$x_{i}$, $\dfrac{\partial}{\partial x_{i}}$ and $x_{0}$ of the algebra
$D\left(  x_{1},x_{2},x_{3},...\right)  \otimes\mathbb{C}\left[  x_{0}\right]
$ are in its image).

In the following, a map $\varphi:A\rightarrow\mathbb{N}$ (where $A$ is some
set) is said to be \textit{finitely supported} if all but finitely many $a\in
A$ satisfy $\varphi\left(  a\right)  =0$. Sequences (finite, infinite, or
two-sided infinite) are considered as maps (from finite sets, $\mathbb{N}$ or
$\mathbb{Z}$, or occasionally other sets). Thus, a sequence is finitely
supported if and only if all but finitely many of its elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

By the easy part of the Poincar\'{e}-Birkhoff-Witt theorem (this is the part
which states that the increasing monomials \textit{span} the universal
enveloping algebra\footnote{The hard part says that these increasing monomials
are linearly independent.}), the family\footnote{Here, $\overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}$ denotes the product
$...a_{-2}^{n_{-2}}a_{-1}^{n_{-1}}a_{0}^{n_{0}}a_{1}^{n_{1}}a_{2}^{n_{2}}...$.
(This product is infinite, but still has a value since only finitely many
$n_{i}$ are nonzero.)}%
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}\cdot
K^{m}\right)  _{\left(  ...,n_{-2},n_{-1},n_{0},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}},\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}\right)  $. Hence,
the family
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}%
}\right)  _{\left(  ...,n_{-2},n_{-1},n_{0},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}}}%
\]
is a spanning set of $U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)
$, and since this family maps to a linearly independent set under $\xi$ (this
is very easy to see), it follows that $\xi$ is injective. Thus, $\xi$ is an
isomorphism, so that Proposition \ref{prop.K-1} is proven.

\begin{definition}
\label{def.A0}Define a vector subspace $\mathcal{A}_{0}$ of $\mathcal{A}$ by
$\mathcal{A}_{0}=\left\langle a_{i}\ \mid\ i\in\mathbb{Z}\diagdown\left\{
0\right\}  \right\rangle +\left\langle K\right\rangle $.
\end{definition}

\begin{proposition}
\label{prop.A0}This subspace $\mathcal{A}_{0}$ is a Lie subalgebra of
$\mathcal{A}$, and $\mathbb{C}a_{0}$ is also a Lie subalgebra of $\mathcal{A}%
$. We have $\mathcal{A}=\mathcal{A}_{0}\oplus\mathbb{C}a_{0}$ as Lie algebras.
Hence,%
\[
U\left(  \mathcal{A}\right)  \diagup\left(  K-1\right)  =U\left(
\mathcal{A}_{0}\oplus\mathbb{C}a_{0}\right)  \diagup\left(  K-1\right)
\cong\underbrace{\left(  U\left(  \mathcal{A}_{0}\right)  \diagup\left(
K-1\right)  \right)  }_{\cong D\left(  x_{1},x_{2},x_{3},...\right)  }%
\otimes\underbrace{\mathbb{C}\left[  a_{0}\right]  }_{\cong\mathbb{C}\left[
x_{0}\right]  }%
\]
(since $K\in\mathcal{A}_{0}$). Here, the isomorphism $U\left(  \mathcal{A}%
_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(  x_{1},x_{2}%
,x_{3},...\right)  $ is defined as follows: In analogy to Proposition
\ref{prop.K-1}, we have an algebra isomorphism%
\[
\widetilde{\xi}:U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)
\rightarrow D\left(  x_{1},x_{2},x_{3},...\right)
\]
given by%
\begin{align*}
\widetilde{\xi}\left(  a_{-i}\right)   &  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for
}i\geq1;\\
\widetilde{\xi}\left(  a_{i}\right)   &  =i\dfrac{\partial}{\partial x_{i}%
}\ \ \ \ \ \ \ \ \ \ \text{for }i\geq1.
\end{align*}

\end{proposition}

The proof of Proposition \ref{prop.A0} is analogous to that of Proposition
\ref{prop.K-1} (where it is not completely straightforward).

\subsubsection{The Fock space}

From Proposition \ref{prop.A0}, we know that $U\left(  \mathcal{A}_{0}\right)
\diagup\left(  K-1\right)  \cong D\left(  x_{1},x_{2},x_{3},...\right)
\subseteq\operatorname*{End}\left(  \mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  \right)  $. Hence, we have a $\mathbb{C}$-algebra
homomorphism $U\left(  \mathcal{A}_{0}\right)  \rightarrow\operatorname*{End}%
\left(  \mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \right)  $. This makes
$\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ into a representation of
the Lie algebra $\mathcal{A}_{0}$. Let us state this as a corollary:

\begin{corollary}
\label{cor.fock}The Lie algebra $\mathcal{A}_{0}$ has a representation
$F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ which is given by
\begin{align*}
a_{-i}  &  \mapsto x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto i\dfrac{\partial}{\partial x_{i}}%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where "$a_{-i}\mapsto x_{i}$" is just shorthand for "$a_{-i}\mapsto\left(
\text{multiplication by }x_{i}\right)  $"). For every $\mu\in\mathbb{C}$, we
can upgrade $F$ to a representation $F_{\mu}$ of $\mathcal{A}$ by adding the
condition that $a_{0}\mid_{F_{\mu}}=\mu\cdot\operatorname*{id}$.
\end{corollary}

\begin{definition}
\label{def.fock}The representation $F$ of $\mathcal{A}_{0}$ introduced in
Corollary \ref{cor.fock} is called the \textit{Fock module} or the
\textit{Fock representation}. For every $\mu\in\mathbb{C}$, the representation
$F_{\mu}$ of $\mathcal{A}$ introduced in Corollary \ref{cor.fock} will be
called the $\mu$\textit{-Fock representation} of $\mathcal{A}$. The vector
space $F$ itself is called the \textit{Fock space}.
\end{definition}

Let us now define some gradings to make these infinite-dimensional spaces more manageable:

\begin{definition}
Let us grade the vector space $\mathcal{A}$ by $\mathcal{A}=\bigoplus
\limits_{n\in\mathbb{Z}}\mathcal{A}\left[  n\right]  $, where $\mathcal{A}%
\left[  n\right]  =\left\langle a_{n}\right\rangle $ for $n\neq0$, and where
$\mathcal{A}\left[  0\right]  =\left\langle a_{0},K\right\rangle $.\ This is a
grading of a Lie algebra, i. e., we have $\left[  \mathcal{A}\left[  n\right]
,\mathcal{A}\left[  m\right]  \right]  \subseteq\mathcal{A}\left[  n+m\right]
$ for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$.
\end{definition}

Note that we are denoting the $n$-th graded component of $\mathcal{A}$ by
$\mathcal{A}\left[  n\right]  $ rather than $\mathcal{A}_{n}$, since otherwise
the notation $\mathcal{A}_{0}$ would have two different meanings.

\begin{definition}
\label{def.fock.grad}We grade the polynomial algebra $F$ by setting
$\deg\left(  x_{i}\right)  =-i$ for each $i$. Thus, $F=\bigoplus
\limits_{n\geq0}F\left[  -n\right]  $, where $F\left[  -n\right]  $ is the
space of polynomials of degree $-n$, where the degree is our degree defined by
$\deg\left(  x_{i}\right)  =-i$ (so that, for instance, $x_{1}^{2}+x_{2}$ is
homogeneous of degree $-2$). With this grading, $\dim\left(  F\left[
-n\right]  \right)  $ is the number $p\left(  n\right)  $ of all partitions of
$n$. Hence,%
\[
\sum\limits_{n\geq0}\dim\left(  F\left[  -n\right]  \right)  q^{n}%
=\sum\limits_{n\geq0}p\left(  n\right)  q^{n}=\dfrac{1}{\left(  1-q\right)
\left(  1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}=\dfrac{1}%
{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }%
\]
in the ring of power series $\mathbb{Z}\left[  \left[  q\right]  \right]  $.

We use the same grading for $F_{\mu}$ for every $\mu\in\mathbb{C}$. That is,
we define the grading on $F_{\mu}$ by $F_{\mu}\left[  n\right]  =F\left[
n\right]  $ for every $n\in\mathbb{Z}$.
\end{definition}

The action of $\mathcal{A}$ on $F_{\mu}$ is now graded, i. e., it maps
$\mathcal{A}\left[  n\right]  \otimes F_{\mu}\left[  m\right]  $ to $F_{\mu
}\left[  n+m\right]  $. This makes $F_{\mu}$ into a $\mathbb{Z}$-graded
$\mathcal{A}$-module.

\begin{remark}
\label{rmk.fockgrad}Some people prefer to grade $F_{\mu}$ somewhat differently
from $F$: namely, they shift the grading for $F_{\mu}$ by $\dfrac{\mu^{2}}{2}%
$, so that $\deg1=-\dfrac{\mu^{2}}{2}$ in $F_{\mu}$, and generally $F_{\mu
}\left[  z\right]  =F\left[  \dfrac{\mu^{2}}{2}+z\right]  $ (as vector spaces)
for every $z\in\mathbb{C}$. Of course, with this new grading, $F_{\mu}$ still
is a graded $\mathcal{A}$-module, although this is a grading by complex
numbers rather than integers (in general). (The advantage of this grading is
that we will eventually find an operator whose eigenspace to the eigenvalue
$n$ is $F_{\mu}\left[  n\right]  =F\left[  \dfrac{\mu^{2}}{2}+n\right]  $ for
every $n\in\mathbb{C}$.)

With this grading, the equality $\sum\limits_{n\geq0}\dim\left(  F\left[
-n\right]  \right)  q^{n}=\dfrac{1}{\prod\limits_{i\geq1}\left(
1-q^{i}\right)  }$ rewrites as $\sum\limits_{n\in\mathbb{C}}\dim\left(
F_{\mu}\left[  -n\right]  \right)  q^{n+\dfrac{\mu^{2}}{2}}=\dfrac{q^{\mu^{2}%
}}{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }$, if we allow power series
with complex exponents. We define a "power series" $\operatorname*{ch}\left(
F_{\mu}\right)  $ by%
\[
\operatorname*{ch}\left(  F_{\mu}\right)  =\sum\limits_{n\in\mathbb{C}}%
\dim\left(  F_{\mu}\left[  -n\right]  \right)  q^{n+\dfrac{\mu^{2}}{2}}%
=\dfrac{q^{\mu^{2}}}{\prod\limits_{i\geq1}\left(  1-q^{i}\right)  }.
\]
But we will not use this grading; instead we will use the grading defined in
Definition \ref{def.fock.grad}.
\end{remark}

\begin{proposition}
\label{prop.F.irrep}The representation $F$ is an irreducible representation of
$\mathcal{A}_{0}$.
\end{proposition}

\begin{lemma}
\label{lem.F.P1=P}For every $P\in F$, we have
\[
P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1=P\ \ \ \ \ \ \ \ \ \ \text{in
}F.
\]
(Here, the term $P\left(  a_{-1},a_{-2},a_{-3},...\right)  $ denotes the
evaluation of the polynomial $P$ at $\left(  x_{1},x_{2},x_{3},...\right)
=\left(  a_{-1},a_{-2},a_{-3},...\right)  $. This evaluation is a well-defined
element of $U\left(  \mathcal{A}_{0}\right)  $, since the elements $a_{-1}$,
$a_{-2}$, $a_{-3}$, $...$ of $U\left(  \mathcal{A}_{0}\right)  $ commute.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.F.P1=P}.} For every $Q\in F$, let
$\operatorname*{mult}Q$ denote the map $F\rightarrow F,$ $R\mapsto QR$. (In
Proposition \ref{prop.K-1}, we abused notations and denoted this map simply by
$Q$; but we will not do this in this proof.) Then, by the definition of $\xi$,
we have $\xi\left(  a_{-i}\right)  =\operatorname*{mult}\left(  x_{i}\right)
$ for every $i\geq1$.

Since we have defined an endomorphism $\operatorname*{mult}Q\in
\operatorname*{End}F$ for every $Q\in F$, we thus obtain a map
$\operatorname*{mult}:F\rightarrow\operatorname*{End}F$. This map
$\operatorname*{mult}$ is an algebra homomorphism (since it describes the
action of $F$ on the $F$-module $F$).

Let $P\in F$. Since $\xi$ is an algebra homomorphism, and thus commutes with
polynomials, we have
\begin{align*}
&  \xi\left(  P\left(  a_{-1},a_{-2},a_{-3},...\right)  \right) \\
&  =P\left(  \xi\left(  a_{-1}\right)  ,\xi\left(  a_{-2}\right)  ,\xi\left(
a_{-3}\right)  ,...\right)  =P\left(  \operatorname*{mult}\left(
x_{1}\right)  ,\operatorname*{mult}\left(  x_{2}\right)  ,\operatorname*{mult}%
\left(  x_{3}\right)  ,...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\xi\left(  a_{-i}\right)
=\operatorname*{mult}\left(  x_{i}\right)  \text{ for every }i\geq1\right) \\
&  =\operatorname*{mult}\left(  \underbrace{P\left(  x_{1},x_{2}%
,x_{3},...\right)  }_{=P}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{mult}\text{ is an algebra homomorphism,}\\
\text{and thus commutes with polynomials}%
\end{array}
\right) \\
&  =\operatorname*{mult}P.
\end{align*}
Thus,%
\[
P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1=\left(  \operatorname*{mult}%
P\right)  \left(  1\right)  =P\cdot1=P.
\]
This proves Lemma \ref{lem.F.P1=P}.

\textit{Proof of Proposition \ref{prop.F.irrep}.} \textbf{1)} The
representation $F$ is generated by $1$ as a $U\left(  \mathcal{A}_{0}\right)
$-module (due to Lemma \ref{lem.F.P1=P}). In other words, $F=U\left(
\mathcal{A}_{0}\right)  \cdot1$.

\textbf{2)} If $P\in F$ and $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}%
...x_{k}^{m_{k}}$ is a monomial in $P$ of degree $\deg P$, with $\alpha\neq0$,
then $\dfrac{1}{\alpha}\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac
{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}%
!}P=1$\ \ \ \ \footnote{\textit{Proof.} When we apply the differential
operator $\dfrac{1}{\alpha}\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}%
\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}%
}{m_{k}!}$ to $P$, all monomials $\beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}%
}...x_{k}^{n_{k}}$ with at least one $n_{\ell}$ being smaller than the
corresponding $m_{\ell}$ are annihilated (because if $n_{\ell}<m_{\ell}$ for
some $\ell$, then $\dfrac{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac
{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}%
!}\left(  \beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}\right)  =0$).
Hence, the only monomials in $P$ which survive under this operator are
monomials of the form $\beta\cdot x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}$
with each $n_{\ell}$ being $\geq$ to the corresponding $m_{\ell}$. But since
$m_{1}+m_{2}+...+m_{k}=\deg P$ (because $\alpha\cdot x_{1}^{m_{1}}x_{2}%
^{m_{2}}...x_{k}^{m_{k}}$ is a monomial of degree $\deg P$), the only such
monomial in $P$ is $\alpha\cdot x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$
(because for every other monomial of the form $\beta\cdot x_{1}^{n_{1}}%
x_{2}^{n_{2}}...x_{k}^{n_{k}}$ with each $n_{\ell}$ being $\geq$ to the
corresponding $m_{\ell}$, the sum $n_{1}+n_{2}+...+n_{k}$ must be greater than
$m_{1}+m_{2}+...+m_{k}=\deg P$, and thus such a monomial cannot occur in $P$).
Hence, the only monomial in $P$ which survives is the monomial $\alpha\cdot
x_{1}^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}$. This monomial clearly gets mapped
to $1$ by the differential operator $\dfrac{1}{\alpha}\dfrac{\partial_{x_{1}%
}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}!}...\dfrac
{\partial_{x_{k}}^{m_{k}}}{m_{k}!}$. Thus, $\dfrac{1}{\alpha}\dfrac
{\partial_{x_{1}}^{m_{1}}}{m_{1}!}\dfrac{\partial_{x_{2}}^{m_{2}}}{m_{2}%
!}...\dfrac{\partial_{x_{k}}^{m_{k}}}{m_{k}!}P=1$, qed.} and thus $1\in
U\left(  \mathcal{A}_{0}\right)  \cdot P$. Combined with \textbf{1)}, this
yields that for every nonzero $P\in F$, the representation $F$ is generated by
$P$ as a $U\left(  \mathcal{A}_{0}\right)  $-module (since $F=U\left(
\mathcal{A}_{0}\right)  \cdot\underbrace{1}_{\in U\left(  \mathcal{A}%
_{0}\right)  \cdot P}\subseteq U\left(  \mathcal{A}_{0}\right)  \cdot U\left(
\mathcal{A}_{0}\right)  \cdot P=U\left(  \mathcal{A}_{0}\right)  \cdot P$).
Consequently, $F$ is irreducible. Proposition \ref{prop.F.irrep} is proven.

\begin{proposition}
\label{prop.V=F}Let $V$ be an irreducible $\mathcal{A}_{0}$-module on which
$K$ acts as $1$. Assume that for any $v\in V$, the space $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v$ is finite-dimensional, and the $a_{i}$
with $i>0$ act on it by nilpotent operators. Then, $V\cong F$ as
$\mathcal{A}_{0}$-modules.
\end{proposition}

Before we prove this, a simple lemma:

\begin{lemma}
\label{lem.V=F}Let $V$ be an $\mathcal{A}_{0}$-module. Let $u\in V$ be such
that $a_{i}u=0$ for all $i>0$, and such that $Ku=u$. Then, there exists a
homomorphism $\eta:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =u$. (This homomorphism $\eta$ is unique, although we
won't need this.)
\end{lemma}

We give two proofs of this lemma. The first one is conceptual and gives us a
glimpse into the more general theory (it proceeds by constructing an
$\mathcal{A}_{0}$-module $\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$, which is an example
of what we will later call a Verma highest-weight module in Definition
\ref{def.verma}). The second one is down-to-earth and proceeds by direct
construction and computation.

\textit{First proof of Lemma \ref{lem.V=F}.} Define a vector subspace
$\mathcal{A}_{0}^{+}$ of $\mathcal{A}_{0}$ by $\mathcal{A}_{0}^{+}%
=\left\langle a_{i}\ \mid\ i\text{ positive integer}\right\rangle $. It is
clear that the internal direct sum $\mathbb{C}K\oplus\mathcal{A}_{0}^{+}$ is
well-defined and an abelian Lie subalgebra of $\mathcal{A}_{0}$. We can make
$\mathbb{C}$ into an $\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
$-module by setting%
\begin{align*}
K\lambda &  =\lambda\ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in
\mathbb{C};\\
a_{i}\lambda &  =0\ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in
\mathbb{C}\text{ and every positive integer }i.
\end{align*}
Now, consider the $\mathcal{A}_{0}$-module $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}=U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }\mathbb{C}$. Denote the element
$1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\in
U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }\mathbb{C}$ of this module by $1$.

We will now show the following important property of this module:
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{For any }\mathcal{A}_{0}\text{-module }T\text{, and any }t\in T\text{
satisfying }\left(  a_{i}t=0\text{ for all }i>0\right)  \text{ and
}Kt=t\text{,}\\
\text{there exists a homomorphism }\overline{\eta}_{T,t}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow T\text{ of }\mathcal{A}_{0}\text{-modules such that
}\overline{\eta}_{T,t}\left(  1\right)  =t
\end{array}
\right)  . \label{lem.V=F.pf.A1}%
\end{equation}
Once this is proven, we will (by considering $\overline{\eta}_{F,1}$) show
that $\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}}\mathbb{C}\cong F$, so this property will translate into the
assertion of Lemma \ref{lem.V=F}.

\textit{Proof of (\ref{lem.V=F.pf.A1}).} Let $\tau:\mathbb{C}\rightarrow T$ be
the map which sends every $\lambda\in\mathbb{C}$ to $\lambda t\in T$. Then,
$\tau$ is $\mathbb{C}$-linear and satisfies%
\[
\tau\underbrace{\left(  K\lambda\right)  }_{=\lambda}=\tau\left(
\lambda\right)  =\lambda\underbrace{t}_{=Kt}=\lambda\cdot Kt=K\cdot
\underbrace{\lambda t}_{=\tau\left(  \lambda\right)  }=K\cdot\tau\left(
\lambda\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in\mathbb{C}%
\]
and%
\begin{align*}
\tau\underbrace{\left(  a_{i}\lambda\right)  }_{=0}  &  =\tau\left(  0\right)
=0=\lambda\cdot\underbrace{0}_{=a_{i}t}=\lambda\cdot a_{i}t=a_{i}%
\cdot\underbrace{\lambda t}_{=\tau\left(  \lambda\right)  }=a_{i}\tau\left(
\lambda\right) \\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in\mathbb{C}\text{ and every
positive integer }i\text{.}%
\end{align*}
Thus, $\tau$ is a $\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
$-module map. In other words, $\tau\in\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  $.

By Frobenius reciprocity, we have%
\[
\operatorname*{Hom}\nolimits_{\mathcal{A}_{0}}\left(  \operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C},T\right)  \cong\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  .
\]
The preimage of $\tau\in\operatorname*{Hom}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}\left(  \mathbb{C},\operatorname*{Res}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}T\right)  $
under this isomorphism is an $\mathcal{A}_{0}$-module map $\overline{\eta
}_{T,t}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow T$ such that
\begin{align*}
\overline{\eta}_{T,t}\underbrace{\left(  1\right)  }_{=1\otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1}  &  =\overline{\eta}%
_{T,t}\left(  1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
}1\right)  =1\underbrace{\tau\left(  1\right)  }_{=1t=t}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the proof of Frobenius reciprocity}%
\right) \\
&  =1t=t.
\end{align*}
Hence, there exists a homomorphism $\overline{\eta}_{T,t}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow T$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{T,t}\left(  1\right)  =t$. This proves (\ref{lem.V=F.pf.A1}).

It is easy to see that the element $1\in F$ satisfies $\left(  a_{i}1=0\text{
for all }i>0\right)  $ and $K1=1$. Thus, (\ref{lem.V=F.pf.A1}) (applied to
$T=F$ and $t=1$) yields that there exists a homomorphism $\overline{\eta
}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}$-modules such
that $\overline{\eta}_{F,1}\left(  1\right)  =1$. This homomorphism
$\overline{\eta}_{F,1}$ is clearly surjective, since
\begin{align*}
F  &  =U\left(  \mathcal{A}_{0}\right)  \cdot\underbrace{1}_{=\overline{\eta
}_{F,1}\left(  1\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{as proven in the
proof of Proposition \ref{prop.F.irrep}}\right) \\
&  =U\left(  \mathcal{A}_{0}\right)  \cdot\overline{\eta}_{F,1}\left(
1\right)  =\overline{\eta}_{F,1}\left(  U\left(  \mathcal{A}_{0}\right)
\cdot1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\overline{\eta}%
_{F,1}\text{ is an }\mathcal{A}_{0}\text{-module map}\right) \\
&  \subseteq\operatorname{Im}\overline{\eta}_{F,1}.
\end{align*}


Now we will prove that this homomorphism $\overline{\eta}_{F,1}$ is injective.

In the following, a map $\varphi:A\rightarrow\mathbb{N}$ (where $A$ is any
set) is said to be \textit{finitely supported} if all but finitely many $a\in
A$ satisfy $\varphi\left(  a\right)  =0$. Sequences (finite, infinite, or
two-sided infinite) are considered as maps (from finite sets, $\mathbb{N}$ or
$\mathbb{Z}$, or occasionally other sets). Thus, a sequence is finitely
supported if and only if all but finitely many of its elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.

By the easy part of the Poincar\'{e}-Birkhoff-Witt theorem (this is the part
which states that the increasing monomials \textit{span} the universal
enveloping algebra), the family\footnote{Here, $\overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}$
denotes the product $...a_{-2}^{n_{-2}}a_{-1}^{n_{-1}}a_{1}^{n_{1}}%
a_{2}^{n_{2}}...$. (This product is infinite, but still has a value since only
finitely many $n_{i}$ are nonzero.)}%
\[
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  _{\left(  ...,n_{-2}%
,n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  },\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}_{0}\right)  $.

Hence, the family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  \otimes
_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  },\ m\in\mathbb{N}}%
\]
is a spanning set of the vector space $U\left(  \mathcal{A}_{0}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }%
\mathbb{C}=\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}$.

Let us first notice that this family is redundant: Each of its elements is
contained in the smaller family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }}.
\]
\footnote{This is because any sequence $\left(  ...,n_{-2},n_{-1},n_{1}%
,n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}%
\diagdown\left\{  0\right\}  }$ and any $m\in\mathbb{N}$ satisfy%
\begin{align*}
&  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\cdot K^{m}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown
\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }\underbrace{\left(  K^{m}1\right)
}_{\substack{=1\\\text{(by repeated application of }K1=1\text{)}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K^{m}\in U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  \right) \\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown
\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1.
\end{align*}
} Hence, this smaller family is also a spanning set of the vector space
$\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}$.

This smaller family is still redundant: Every of its elements corresponding to
a sequence $\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$
satisfying $n_{1}+n_{2}+n_{3}+...>0$ is zero\footnote{\textit{Proof.} Let
$\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$ be a
sequence satisfying $n_{1}+n_{2}+n_{3}+...>0$. Then, the sequence $\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  $ is finitely supported (as it is an
element of $\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{
0\right\}  }$), so that only finitely many $n_{i}$ are nonzero.
\par
There exists some positive integer $\ell$ satisfying $n_{\ell}>0$ (since
$n_{1}+n_{2}+n_{3}+...>0$). Let $j$ be the greatest such $\ell$ (this is
well-defined, since only finitely many $n_{i}$ are nonzero).
\par
Since $j$ is the greatest positive integer $\ell$ satisfying $n_{\ell}>0$, it
is clear that $j$ is the greatest integer $\ell$ satisfying $n_{\ell}>0$. In
other words, $a_{j}^{n_{j}}$ is the rightmost factor in the product
$\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}}}a_{i}^{n_{i}}$ which is
not equal to $1$. Thus,%
\[
\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}
}}a_{i}^{n_{i}}=\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}%
\cdot\underbrace{a_{j}^{n_{j}}}_{\substack{=a_{j}^{n_{j}-1}a_{j}\\\text{(since
}n_{j}>0\text{)}}}=\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}\cdot
a_{j}^{n_{j}-1}a_{j},
\]
so that%
\begin{align*}
\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1  &  =\left(  \overset{\rightarrow
}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{  0\right\}  \diagdown\left\{
j\right\}  }}a_{i}^{n_{i}}\cdot a_{j}^{n_{j}-1}a_{j}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\\
&  =\overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  \diagdown\left\{  j\right\}  }}a_{i}^{n_{i}}\cdot a_{j}^{n_{j}%
-1}\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)
}\underbrace{a_{j}1}_{\substack{=0\\\text{(since }j>0\text{, so that}%
\\a_{j}1=j\dfrac{\partial}{\partial x_{j}}1=0\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\in U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  \right) \\
&  =0.
\end{align*}
We have thus proven that every sequence $\left(  ...,n_{-2},n_{-1},n_{1}%
,n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}%
\diagdown\left\{  0\right\}  }$ satisfying $n_{1}+n_{2}+n_{3}+...>0$ satisfies
$\left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}\diagdown\left\{
0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1=0$, qed.}, and zero elements in a spanning set
are automatically redundant. Hence, we can replace this smaller family by the
even smaller family%
\begin{align*}
&  \left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }\text{; we do \textit{not} have
}n_{1}+n_{2}+n_{3}+...>0}\\
&  =\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(
\mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(
...,n_{-2},n_{-1},n_{1},n_{2},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\mathbb{Z}\diagdown\left\{  0\right\}  }\text{; }n_{1}=n_{2}=n_{3}=...=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the condition }\left(  \text{we do \textit{not} have }n_{1}%
+n_{2}+n_{3}+...>0\right) \\
\text{is equivalent to the condition }\left(  n_{1}=n_{2}=n_{3}=...=0\right)
\\
\text{(because }n_{i}\in\mathbb{N}\text{ for all }i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  \text{)}%
\end{array}
\right)  ,
\end{align*}
and we still have a spanning set of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$.

Clearly, sequences $\left(  ...,n_{-2},n_{-1},n_{1},n_{2},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\mathbb{Z}\diagdown\left\{  0\right\}  }$
satisfying $n_{1}=n_{2}=n_{3}=...=0$ are in 1-to-1 correspondence with
sequences $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$. Hence, we can
reindex the above family as follows:
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}.
\]
So we have proven that the family%
\[
\left(  \left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \otimes_{U\left(  \mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}%
\]
is a spanning set of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$.
But the map $\overline{\eta}_{F,1}$ sends this family to%
\begin{align*}
&  \left(  \overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right)  _{\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }}\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}%
\end{align*}
\footnote{\textit{Proof.} Let $\left(  ...,n_{-2},n_{-1}\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$ be
arbitrary. Then,%
\begin{align*}
&  \overline{\eta}_{F,1}\left(  \underbrace{\left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1}_{=\left(
\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}%
a_{i}^{n_{i}}\right)  \left(  1\otimes_{U\left(  \mathbb{C}K\oplus
\mathcal{A}_{0}^{+}\right)  }1\right)  }\right) \\
&  =\overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \left(
1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right) \\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \overline{\eta}_{F,1}%
\underbrace{\left(  1\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}%
^{+}\right)  }1\right)  }_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\overline{\eta}_{F,1}\text{ is an }\mathcal{A}_{0}\text{-module map}\right)
\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  \underbrace{\overline{\eta
}_{F,1}\left(  1\right)  }_{=1}=\left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)  1=\left(
\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
}x_{-i}^{n_{i}}\right)  1\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because each }a_{i}\text{ with negative
}i\text{ acts on }F\text{ by multiplication with }x_{-i}\right) \\
&  =\overset{\rightarrow}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
}x_{-i}^{n_{i}}=\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }%
x_{-i}^{n_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }F\text{ is
commutative}\right)  .
\end{align*}
Now forget that we fixed $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$. We thus have shown
that every $\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }$ satisfies
$\overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow}{\prod
\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
=\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }x_{-i}^{n_{i}}$. Thus,%
\begin{align*}
&  \left(  \overline{\eta}_{F,1}\left(  \left(  \overset{\rightarrow
}{\prod\limits_{i\in\left\{  ...,-3,-2,-1\right\}  }}a_{i}^{n_{i}}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }1\right)
\right)  _{\left(  ...,n_{-2},n_{-1}\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  ...,-3,-2,-1\right\}  }}\\
&  =\left(  \overset{\rightarrow}{\prod\limits_{i\in\left\{
...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(  ...,n_{-2}%
,n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }},
\end{align*}
qed.}. Since the family $\left(  \overset{\rightarrow}{\prod\limits_{i\in
\left\{  ...,-3,-2,-1\right\}  }}x_{-i}^{n_{i}}\right)  _{\left(
...,n_{-2},n_{-1}\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
...,-3,-2,-1\right\}  }}$ is a basis of the vector space $F$ (in fact, this
family consists of all monomials of the polynomial ring $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  =F$), we thus conclude that $\overline{\eta
}_{F,1}$ sends a spanning family of the vector space $\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$
to a basis of the vector space $F$. Thus, $\overline{\eta}_{F,1}$ must be
injective\footnote{Here we are using the following trivial fact from linear
algebra: If a linear map $\varphi:V\rightarrow W$ sends a spanning family of
the vector space $V$ to a basis of the vector space $W$ (as families, not just
as sets), then this map $\varphi$ must be injective.}.

Altogether, we now know that $\overline{\eta}_{F,1}$ is a surjective and
injective $\mathcal{A}_{0}$-module map. Thus, $\overline{\eta}_{F,1}$ is an
isomorphism of $\mathcal{A}_{0}$-modules.

Now, apply (\ref{lem.V=F.pf.A1}) to $T=V$ and $t=u$. This yields that there
exists a homomorphism $\overline{\eta}_{V,u}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{V,u}\left(  1\right)  =u$.

Now, the composition $\overline{\eta}_{V,u}\circ\overline{\eta}_{F,1}^{-1}$ is
a homomorphism $F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that
\[
\left(  \overline{\eta}_{V,u}\circ\overline{\eta}_{F,1}^{-1}\right)  \left(
1\right)  =\overline{\eta}_{V,u}\underbrace{\left(  \overline{\eta}_{F,1}%
^{-1}\left(  1\right)  \right)  }_{\substack{=1\\\text{(since }\overline{\eta
}_{F,1}\left(  1\right)  =1\text{)}}}=\overline{\eta}_{V,u}\left(  1\right)
=u.
\]
Thus, there exists a homomorphism $\eta:F\rightarrow V$ of $\mathcal{A}_{0}%
$-modules such that $\eta\left(  1\right)  =u$ (namely, $\eta=\overline{\eta
}_{V,u}\circ\overline{\eta}_{F,1}^{-1}$). This proves Lemma \ref{lem.V=F}.

\textit{Second proof of Lemma \ref{lem.V=F}.} Let $\eta$ be the map
$F\rightarrow V$ which sends every polynomial $P\in F=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ to $P\left(  a_{-1},a_{-2},a_{-3},...\right)
\cdot u\in V$.\ \ \ \ \footnote{Note that the term $P\left(  a_{-1}%
,a_{-2},a_{-3},...\right)  $ denotes the evaluation of the polynomial $P$ at
$\left(  x_{1},x_{2},x_{3},...\right)  =\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  $. This evaluation is a well-defined element of $U\left(
\mathcal{A}_{0}\right)  $, since the elements $a_{-1}$, $a_{-2}$, $a_{-3}$,
$...$ of $U\left(  \mathcal{A}_{0}\right)  $ commute.} This map $\eta$ is
clearly $\mathbb{C}$-linear, and satisfies $\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u$. In order to prove that $\eta$ is an
$\mathcal{A}_{0}$-module homomorphism, we must prove that
\begin{equation}
\eta\left(  a_{i}P\right)  =a_{i}\eta\left(  P\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{Z}\diagdown\left\{
0\right\}  \text{ and }P\in F \label{lem.V=F.pf.1}%
\end{equation}
and that%
\begin{equation}
\eta\left(  KP\right)  =K\eta\left(  P\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }P\in F. \label{lem.V=F.pf.2}%
\end{equation}


First we show that%
\begin{equation}
Kv=v\ \ \ \ \ \ \ \ \ \ \text{for every }v\in U\left(  \mathcal{A}_{0}\right)
\cdot u. \label{lem.V=F.pf.3}%
\end{equation}


\textit{Proof of (\ref{lem.V=F.pf.3}).} Since $K$ lies in the center of the
Lie algebra $\mathcal{A}_{0}$, it is clear that $K$ lies in the center of the
universal enveloping algebra $U\left(  \mathcal{A}_{0}\right)  $. Thus,
$Kx=xK$ for every $x\in U\left(  \mathcal{A}_{0}\right)  $.

Now let $v\in U\left(  \mathcal{A}_{0}\right)  \cdot u$. Then, there exists
some $x\in U\left(  \mathcal{A}_{0}\right)  $ such that $v=xu$. Thus,
$Kv=Kxu=x\underbrace{Ku}_{=u}=xu=v$. This proves (\ref{lem.V=F.pf.3}).

\textit{Proof of (\ref{lem.V=F.pf.2}).} Since $K$ acts as the identity on $F$,
we have $KP=P$ for every $P\in F$. Thus, for every $P\in F$, we have%
\[
\eta\left(  KP\right)  =\eta\left(  P\right)  =K\eta\left(  P\right)
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{lem.V=F.pf.3}) (applied to }v=\eta\left(  P\right)  \text{)
yields }K\eta\left(  P\right)  =\eta\left(  P\right) \\
\text{(because }\eta\left(  P\right)  \in\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u\text{)}%
\end{array}
\right)  .
\]
This proves (\ref{lem.V=F.pf.2}).

\textit{Proof of (\ref{lem.V=F.pf.1}).} Let $i\in\mathbb{Z}\diagdown\left\{
0\right\}  $. If $i<0$, then (\ref{lem.V=F.pf.1}) is pretty much obvious
(because in this case, $a_{i}$ acts as $x_{-i}$ on $F$, so that $a_{i}%
P=x_{-i}P$ and thus%
\[
\eta\left(  a_{i}P\right)  =\eta\left(  x_{-i}P\right)  =\left(
x_{-i}P\right)  \left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u=a_{i}%
\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}_{=\eta\left(
P\right)  }=a_{i}\eta\left(  P\right)
\]
for every $P\in F$). Hence, from now on, we can WLOG assume that $i$ is not
$<0$. Assume this. Then, $i\geq0$, so that $i>0$ (since $i\in\mathbb{Z}%
\diagdown\left\{  0\right\}  $).

In order to prove the equality (\ref{lem.V=F.pf.1}) for all $P\in F$, it is
enough to prove it for the case when $P$ is a monomial of the form
$x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}$ for some $m\in\mathbb{N}$ and some
$\left(  \ell_{1},\ell_{2},...,\ell_{m}\right)  \in\left\{  1,2,3,...\right\}
^{m}$.\ \ \ \ \footnote{This is because such monomials generate $F$ as a
$\mathbb{C}$-vector space, and because the equality (\ref{lem.V=F.pf.1}) is
linear in $P$.} In other words, in order to prove the equality
(\ref{lem.V=F.pf.1}), it is enough to prove that%
\begin{equation}
\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}\right)
\right)  =a_{i}\eta\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{m}}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}\text{ and every }\left(
\ell_{1},\ell_{2},...,\ell_{m}\right)  \in\left\{  1,2,3,...\right\}  ^{m}.
\label{lem.V=F.pf.4}%
\end{equation}


Thus, let us now prove (\ref{lem.V=F.pf.4}). In fact, we are going to prove
(\ref{lem.V=F.pf.4}) by induction over $m$. The induction base is very easy
(using $a_{i}1=i\dfrac{\partial}{\partial x_{i}}1=0$ and $a_{i}u=0$) and thus
left to the reader. For the induction step, fix some positive $M\in\mathbb{N}%
$, and assume that (\ref{lem.V=F.pf.4}) is already proven for $m=M-1$. Our
task is now to prove (\ref{lem.V=F.pf.4}) for $m=M$.

So let $\left(  \ell_{1},\ell_{2},...,\ell_{M}\right)  \in\left\{
1,2,3,...\right\}  ^{M}$ be arbitrary. Denote by $Q$ the polynomial
$x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}}$. Then, $x_{\ell_{1}}Q=x_{\ell_{1}%
}x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}}=x_{\ell_{1}}x_{\ell_{2}}%
...x_{\ell_{M}}$.

Since (\ref{lem.V=F.pf.4}) is already proven for $m=M-1$, we can apply
(\ref{lem.V=F.pf.4}) to $M-1$ and $\left(  \ell_{2},\ell_{3},...,\ell
_{M}\right)  $ instead of $m$ and $\left(  \ell_{1},\ell_{2},...,\ell
_{m}\right)  $. We obtain $\eta\left(  a_{i}\left(  x_{\ell_{2}}x_{\ell_{3}%
}...x_{\ell_{M}}\right)  \right)  =a_{i}\eta$ $\left(  x_{\ell_{2}}x_{\ell
_{3}}...x_{\ell_{M}}\right)  $. Since $x_{\ell_{2}}x_{\ell_{3}}...x_{\ell_{M}%
}=Q$, this rewrites as $\eta\left(  a_{i}Q\right)  =a_{i}\eta\left(  Q\right)
$.

Since any $x\in\mathcal{A}_{0}$ and $y\in\mathcal{A}_{0}$ satisfy
$xy=yx+\left[  x,y\right]  $ (by the definition of $U\left(  \mathcal{A}%
_{0}\right)  $), we have%
\[
a_{i}a_{-\ell_{1}}=a_{-\ell_{1}}a_{i}+\underbrace{\left[  a_{i},a_{-\ell_{1}%
}\right]  }_{=i\delta_{i,-\left(  -\ell_{1}\right)  }K}=a_{-\ell_{1}}%
a_{i}+i\underbrace{\delta_{i,-\left(  -\ell_{1}\right)  }}_{=\delta
_{i,\ell_{1}}}K=a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K.
\]


On the other hand, by the definition of $\eta$, every $P\in F$ satisfies the
two equalities $\eta\left(  P\right)  =P\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  \cdot u$ and%
\begin{align}
\eta\left(  x_{\ell_{1}}P\right)   &  =\underbrace{\left(  x_{\ell_{1}%
}P\right)  \left(  a_{-1},a_{-2},a_{-3},...\right)  }_{=a_{-\ell_{1}}\cdot
P\left(  a_{-1},a_{-2},a_{-3},...\right)  }\cdot u=a_{-\ell_{1}}%
\cdot\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}%
_{=\eta\left(  P\right)  }\nonumber\\
&  =a_{-\ell_{1}}\cdot\eta\left(  P\right)  . \label{lem.V=F.pf.5}%
\end{align}


Since $a_{i}$ acts on $F$ as $i\dfrac{\partial}{\partial x_{i}}$, we have
$a_{i}\left(  x_{\ell_{1}}Q\right)  =i\dfrac{\partial}{\partial x_{i}}\left(
x_{\ell_{1}}Q\right)  $ and $a_{i}Q=i\dfrac{\partial}{\partial x_{i}}Q$. Now,%
\begin{align*}
a_{i}\left(  \underbrace{x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}}%
_{=x_{\ell_{1}}Q}\right)   &  =a_{i}\left(  x_{\ell_{1}}Q\right)
=i\dfrac{\partial}{\partial x_{i}}\left(  x_{\ell_{1}}Q\right)  =i\left(
\left(  \dfrac{\partial}{\partial x_{i}}x_{\ell_{1}}\right)  Q+x_{\ell_{1}%
}\left(  \dfrac{\partial}{\partial x_{i}}Q\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the Leibniz rule}\right) \\
&  =i\underbrace{\left(  \dfrac{\partial}{\partial x_{i}}x_{\ell_{1}}\right)
}_{=\delta_{i,\ell_{1}}}Q+x_{\ell_{1}}\cdot\underbrace{i\dfrac{\partial
}{\partial x_{i}}Q}_{=a_{i}Q}=i\delta_{i,\ell_{1}}Q+x_{\ell_{1}}\cdot
a_{i}Q=x_{\ell_{1}}\cdot a_{i}Q+i\delta_{i,\ell_{1}}Q,
\end{align*}
so that%
\begin{align*}
\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}\right)
\right)   &  =\eta\left(  x_{\ell_{1}}\cdot a_{i}Q+i\delta_{i,\ell_{1}%
}Q\right)  =\underbrace{\eta\left(  x_{\ell_{1}}\cdot a_{i}Q\right)
}_{\substack{=a_{-\ell_{1}}\cdot\eta\left(  a_{i}Q\right)  \\\text{(by
(\ref{lem.V=F.pf.5}), applied to }P=a_{i}Q\text{)}}}+i\delta_{i,\ell_{1}}%
\eta\left(  Q\right) \\
&  =a_{-\ell_{1}}\cdot\underbrace{\eta\left(  a_{i}Q\right)  }_{=a_{i}%
\eta\left(  Q\right)  }+i\delta_{i,\ell_{1}}\eta\left(  Q\right)
=a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}}%
\eta\left(  Q\right)  .
\end{align*}
Compared to%
\begin{align*}
a_{i}\eta\left(  \underbrace{x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}}%
}_{=x_{\ell_{1}}Q}\right)   &  =a_{i}\underbrace{\eta\left(  x_{\ell_{1}%
}Q\right)  }_{\substack{=a_{-\ell_{1}}\cdot\eta\left(  Q\right)  \\\text{(by
(\ref{lem.V=F.pf.5}), applied to }P=Q\text{)}}}=\underbrace{a_{i}a_{-\ell_{1}%
}}_{=a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K}\cdot\eta\left(  Q\right) \\
&  =\left(  a_{-\ell_{1}}a_{i}+i\delta_{i,\ell_{1}}K\right)  \cdot\eta\left(
Q\right)  =a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}%
}\underbrace{K\eta\left(  Q\right)  }_{\substack{=\eta\left(  Q\right)
\\\text{(by (\ref{lem.V=F.pf.3}), applied to }v=\eta\left(  Q\right)
\\\text{(since }\eta\left(  Q\right)  \in\eta\left(  F\right)  \subseteq
U\left(  \mathcal{A}_{0}\right)  \cdot u\text{))}}}\\
&  =a_{-\ell_{1}}\cdot a_{i}\eta\left(  Q\right)  +i\delta_{i,\ell_{1}}%
\eta\left(  Q\right)  ,
\end{align*}
this yields $\eta\left(  a_{i}\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}%
}\right)  \right)  =a_{i}\eta\left(  x_{\ell_{1}}x_{\ell_{2}}...x_{\ell_{M}%
}\right)  $. Since we have proven this for every $\left(  \ell_{1},\ell
_{2},...,\ell_{M}\right)  \in\left\{  1,2,3,...\right\}  ^{M}$, we have thus
proven (\ref{lem.V=F.pf.4}) for $m=M$. This completes the induction step, and
thus the induction proof of (\ref{lem.V=F.pf.4}) is complete. As we have seen
above, this proves (\ref{lem.V=F.pf.1}).

From (\ref{lem.V=F.pf.1}) and (\ref{lem.V=F.pf.2}), it is clear that $\eta$ is
$\mathcal{A}_{0}$-linear (since $\mathcal{A}_{0}$ is spanned by the $a_{i}$
for $i\in\mathbb{Z}\diagdown\left\{  0\right\}  $ and $K$). Since $\eta\left(
1\right)  =u$ is obvious, this proves Lemma \ref{lem.V=F}.

\textit{Proof of Proposition \ref{prop.V=F}.} Pick some nonzero vector $v\in
V$. Let $W=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \cdot v$. Then, by
the condition, we have $\dim W<\infty$, and $a_{i}:W\rightarrow W$ are
commuting nilpotent operators\footnote{Of course, when we write $a_{i}%
:W\rightarrow W$, we don't mean the elements $a_{i}$ of $\mathcal{A}_{0}$
themselves, but their actions on $W$.}. Hence, $\bigcap\limits_{i\geq
1}\operatorname*{Ker}a_{i}\neq0\ \ \ \ $\footnote{Here, we are using the
following linear-algebraic fact:
\par
If $T$ is a nonzero finite-dimensional vector space over an algebraically
closed field, and if $b_{1}$, $b_{2}$, $b_{3}$, $...$ are commuting linear
maps $T\rightarrow T$, then there exists a nonzero common eigenvector of
$b_{1}$, $b_{2}$, $b_{3}$, $...$. If $b_{1}$, $b_{2}$, $b_{3}$, $...$ are
nilpotent, this yields $\bigcap\limits_{i\geq1}\operatorname*{Ker}b_{i}\neq0$
(since any eigenvector of a nilpotent map must lie in its kernel).}. Hence,
there exists some nonzero $u\in\bigcap\limits_{i\geq1}\operatorname*{Ker}%
a_{i}$. Pick such a $u$. Then, $a_{i}u=0$ for all $i>0$, and $Ku=u$ (since $K$
acts as $1$ on $V$). Thus, there exists a homomorphism $\eta:F\rightarrow V$
of $\mathcal{A}_{0}$-modules such that $\eta\left(  1\right)  =u$ (by Lemma
\ref{lem.V=F}). Since both $F$ and $V$ are irreducible and $\eta\neq0$, this
yields that $\eta$ is an isomorphism. This proves Proposition \ref{prop.V=F}.

\subsubsection{Classification of $\mathcal{A}_{0}$-modules with locally
nilpotent action of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $}

\begin{proposition}
\label{prop.V=F(X)U}Let $V$ be any $\mathcal{A}_{0}$-module having a locally
nilpotent action of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $. (Here,
we say that the $\mathcal{A}_{0}$-module $V$ has a \textit{locally nilpotent
action of$\mathbb{\ }$}$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ if
for any $v\in V$, the space $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]
\cdot v$ is finite-dimensional, and the $a_{i}$ with $i>0$ act on it by
nilpotent operators.) Assume that $K$ acts as $1$ on $V$. Assume that for
every $v\in V$, there exists some $N\in\mathbb{N}$ such that for every $n\geq
N$, we have $a_{n}v=0$. Then, $V\cong F\otimes U$ as $\mathcal{A}_{0}$-modules
for some vector space $U$. (The vector space $U$ is not supposed to carry any
$\mathcal{A}_{0}$-module structure.)
\end{proposition}

\begin{remark}
From Proposition \ref{prop.V=F(X)U}, we cannot remove the condition that for
every $v\in V$, there exists some $N\in\mathbb{N}$ such that for every $n\geq
N$, we have $a_{n}v=0$. In fact, here is a counterexample of how Proposition
\ref{prop.V=F(X)U} can fail without this condition:

Let $V$ be the representation $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]
\left[  y\right]  \diagup\left(  y^{2}\right)  $ of $\mathcal{A}_{0}$ given
by
\begin{align*}
a_{-i}  &  \mapsto x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto i\dfrac{\partial}{\partial x_{i}}%
+y\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where we are being sloppy and abbreviating the residue class $\overline{y}%
\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \left[  y\right]
\diagup\left(  y^{2}\right)  $ by $y$, and similarly all other residue
classes). We have an exact sequence%
\[%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%0 \ar[r] & F \ar[r]^i & V \ar[r]^{\pi} & F \ar[r] & 0
%}}}%
%BeginExpansion
\xymatrix{
0 \ar[r] & F \ar[r]^i & V \ar[r]^{\pi} & F \ar[r] & 0
}%
%EndExpansion
\]
of $\mathcal{A}_{0}$-modules, where the map $i:F\rightarrow V$ is given by%
\[
i\left(  P\right)  =yP\ \ \ \ \ \ \ \ \ \ \text{for every }p\in F=\mathbb{C}%
\left[  x_{1},x_{2},x_{3},...\right]  ,
\]
and the map $\pi:V\rightarrow F$ is the canonical projection $V\rightarrow
V\diagup\left(  y\right)  \cong F$. Thus, $V$ is an extension of $F$ by $F$.
It is easily seen that $V$ has a locally nilpotent action of$\ \mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  $. But $V$ is not isomorphic to
$F\otimes U$ as $\mathcal{A}_{0}$-modules for any vector space $U$, since
there is a vector $v\in V$ satisfying $V=U\left(  \mathcal{A}_{0}\right)
\cdot v$ (for example, $v=1$), whereas there is no vector $v\in F\otimes U$
satisfying $F\otimes U=U\left(  \mathcal{A}_{0}\right)  \cdot v$ if $\dim
U>1$, and the case $\dim U\leq1$ is easily ruled out (in this case, $\dim U$
would have to be $1$, so that $V$ would be $\cong F$ and thus irreducible, and
thus the homomorphisms $i$ and $\pi$ would have to be isomorphisms, which is absurd).
\end{remark}

Before we prove Proposition \ref{prop.V=F(X)U}, we need to define the notion
of complete coflags:

\begin{definition}
Let $k$ be a field. Let $V$ be a $k$-vector space. Let $W$ be a vector
subspace of $V$. Assume that $\dim\left(  V\diagup W\right)  <\infty$. Then, a
\textbf{complete coflag from }$V$ \textbf{to }$W$ will mean a sequence
$\left(  V_{0},V_{1},...,V_{N}\right)  $ of vector subspaces of $V$ (with $N$
being an integer) satisfying the following conditions:

- We have $V_{0}\supseteq V_{1}\supseteq...\supseteq V_{N}$.

- Every $i\in\left\{  0,1,...,N\right\}  $ satisfies $\dim\left(  V\diagup
V_{i}\right)  =i$.

- We have $V_{0}=V$ and $V_{N}=W$.

(Note that the third of these conditions follows from the second one and thus
is superfluous, but has been given for the sake of making the meaning of "from
$V$ to $W$" intuitively clear.)

We will also denote the complete coflag $\left(  V_{0},V_{1},...,V_{N}\right)
$ by $V=V_{0}\supseteq V_{1}\supseteq...\supseteq V_{N}=W$.
\end{definition}

It is clear that if $k$ is a field, $V$ is a $k$-vector space, and $W$ is a
vector subspace of $V$ satisfying $\dim\left(  V\diagup W\right)  <\infty$,
then a complete coflag from $V$ to $W$ exists.\footnote{In fact, it is known
that the finite-dimensional vector space $V\diagup W$ has a complete flag
$\left(  F_{0},F_{1},...,F_{N}\right)  $; now, if we let $p$ be the canonical
projection $V\rightarrow V\diagup W$, then $\left(  p^{-1}\left(
F_{N}\right)  ,p^{-1}\left(  F_{N-1}\right)  ,...,p^{-1}\left(  F_{0}\right)
\right)  $ is easily seen to be a complete coflag from $V$ to $W$.}

\begin{definition}
Let $k$ be a field. Let $V$ be a $k$-algebra. Let $W$ be a vector subspace of
$V$. Let $\mathfrak{i}$ be an ideal of $V$. Then, an $\mathfrak{i}%
$\textbf{-coflag from }$V$\textbf{ to }$W$ means a complete coflag $\left(
V_{0},V_{1},...,V_{N}\right)  $ from $V$ to $W$ such that
\[
\text{every }i\in\left\{  0,1,...,N-1\right\}  \text{ satisfies }%
\mathfrak{i}\cdot V_{i}\subseteq V_{i+1}.
\]

\end{definition}

\begin{lemma}
\label{lem.V=F(X)U.coflags}Let $k$ be a field. Let $B$ be a commutative
$k$-algebra. Let $I$ be an ideal of $B$ such that the $k$-vector space
$B\diagup I$ is finite-dimensional. Let $\mathfrak{i}$ be an ideal of $B$. Let
$M\in\mathbb{N}$. Then, there exists an $\mathfrak{i}$-coflag from $B$ to
$\mathfrak{i}^{M}+I$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.V=F(X)U.coflags}.} We will prove Lemma
\ref{lem.V=F(X)U.coflags} by induction over $M$:

\textit{Induction base:} Lemma \ref{lem.V=F(X)U.coflags} is trivial in the
case when $M=0$, because $\underbrace{\mathfrak{i}^{0}}_{=B}+I=B+I=B$. This
completes the induction base.

\textit{Induction base:} Let $m\in\mathbb{N}$. Assume that Lemma
\ref{lem.V=F(X)U.coflags} is proven in the case when $M=m$. We now must prove
Lemma \ref{lem.V=F(X)U.coflags} in the case when $M=m+1$.

Since Lemma \ref{lem.V=F(X)U.coflags} is proven in the case when $M=m$, there
exists an $\mathfrak{i}$-coflag $\left(  J_{0},J_{1},...,J_{K}\right)  $ from
$B$ to $\mathfrak{i}^{m}+I$. This $\mathfrak{i}$-coflag clearly is a complete
coflag from $B$ to $\mathfrak{i}^{m}+I$.

Since
\begin{align*}
\dim\left(  \left(  \mathfrak{i}^{m}+I\right)  \diagup\left(  \mathfrak{i}%
^{m+1}+I\right)  \right)   &  \leq\dim\left(  B\diagup\left(  \mathfrak{i}%
^{m+1}+I\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{because }\left(  \mathfrak{i}%
^{m}+I\right)  \diagup\left(  \mathfrak{i}^{m+1}+I\right)  \text{ injects into
}B\diagup\left(  \mathfrak{i}^{m+1}+I\right)  \right) \\
&  \leq\dim\left(  B\diagup I\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}B\diagup\left(  \mathfrak{i}^{m+1}+I\right)  \text{ is a quotient of
}B\diagup I\right) \\
&  <\infty\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B\diagup I\text{ is
finite-dimensional}\right)  ,
\end{align*}
there exists a complete coflag $\left(  U_{0},U_{1},...,U_{P}\right)  $ from
$\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$.

Since $\left(  U_{0},U_{1},...,U_{P}\right)  $ is a complete coflag from
$\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$, we have $U_{0}=\mathfrak{i}%
^{m}+I$, and each of the vector spaces $U_{0}$, $U_{1}$, $...$, $U_{P}$
contains $\mathfrak{i}^{m+1}+I$ as a subspace.

Also, every $i\in\left\{  0,1,...,P\right\}  $ satisfies $U_{i}\subseteq
\mathfrak{i}^{m}+I$ (again since $\left(  U_{0},U_{1},...,U_{P}\right)  $ is a
complete coflag from $\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$).

Since $\left(  J_{0},J_{1},...,J_{K}\right)  $ is a complete coflag from $B$
to $\mathfrak{i}^{m}+I$, while $\left(  U_{0},U_{1},...,U_{P}\right)  $ is a
complete coflag from $\mathfrak{i}^{m}+I$ to $\mathfrak{i}^{m+1}+I$, it is
clear that
\[
\left(  J_{0},J_{1},...,J_{K},U_{1},U_{2},...,U_{P}\right)  =\left(
J_{0},J_{1},...,J_{K-1},U_{0},U_{1},...,U_{P}\right)
\]
is a complete coflag from $B$ to $\mathfrak{i}^{m+1}+I$. We now will prove
that this complete coflag
\[
\left(  J_{0},J_{1},...,J_{K},U_{1},U_{2},...,U_{P}\right)  =\left(
J_{0},J_{1},...,J_{K-1},U_{0},U_{1},...,U_{P}\right)
\]
actually is an $\mathfrak{i}$-coflag.

In order to prove this, we must show the following two assertions:

\textit{Assertion 1:} Every $i\in\left\{  0,1,...,K-1\right\}  $ satisfies
$\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}$.

\textit{Assertion 2:} Every $i\in\left\{  0,1,...,P-1\right\}  $ satisfies
$\mathfrak{i}\cdot U_{i}\subseteq U_{i+1}$.

Assertion 1 follows directly from the fact that $\left(  J_{0},J_{1}%
,...,J_{K}\right)  $ is an $\mathfrak{i}$-coflag.

Assertion 2 follows from the fact that $\mathfrak{i}\cdot\underbrace{U_{i}%
}_{\subseteq\mathfrak{i}^{m}+I}\subseteq\mathfrak{i}\cdot\left(
\mathfrak{i}^{m}+I\right)  \subseteq\underbrace{\mathfrak{i}\cdot
\mathfrak{i}^{m}}_{=\mathfrak{i}^{m+1}}+\underbrace{\mathfrak{i}\cdot
I}_{\substack{\subseteq I\\\text{(since }I\text{ is an ideal)}}}\subseteq
\mathfrak{i}^{m+1}+I\subseteq U_{i+1}$ (because we know that each of the
vector spaces $U_{0}$, $U_{1}$, $...$, $U_{P}$ contains $\mathfrak{i}^{m+1}+I$
as a subspace, so that (in particular) $\mathfrak{i}^{m+1}+I\subseteq U_{i+1}$).

Hence, both Assertions 1 and 2 are proven, and we conclude that
\[
\left(  J_{0},J_{1},...,J_{K},U_{1},U_{2},...,U_{P}\right)  =\left(
J_{0},J_{1},...,J_{K-1},U_{0},U_{1},...,U_{P}\right)
\]
is an $\mathfrak{i}$-coflag. This is clearly an $\mathfrak{i}$-coflag from $B$
to $\mathfrak{i}^{m+1}+I$. Thus, there exists an $\mathfrak{i}$-coflag from
$B$ to $\mathfrak{i}^{m+1}+I$. This proves Lemma \ref{lem.V=F(X)U.coflags} in
the case when $M=m+1$. The induction step is complete, and with it the proof
of Lemma \ref{lem.V=F(X)U.coflags}.

\textit{Proof of Proposition \ref{prop.V=F(X)U}.} Let $v\in V$ be arbitrary.
Let $I_{v}\subseteq\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ be the
annihilator of $v$. Then, the canonical $\mathbb{C}$-algebra map
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \rightarrow
\operatorname*{End}\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]
\cdot v\right)  $ (this map comes from the action of the $\mathbb{C}$-algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ on $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v$) gives rise to an \textit{injective}
map $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}%
\rightarrow\operatorname*{End}\left(  \mathbb{C}\left[  a_{1},a_{2}%
,a_{3},...\right]  \cdot v\right)  $. Since this map is injective, we have
$\dim\left(  \mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
I_{v}\right)  \leq\dim\left(  \operatorname*{End}\left(  \mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \cdot v\right)  \right)  <\infty$ (since
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \cdot v$ is
finite-dimensional). In other words, the vector space $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$ is finite-dimensional.

Let $W$ be the $\mathcal{A}_{0}$-submodule of $V$ generated by $v$. In other
words, let $W=U\left(  \mathcal{A}_{0}\right)  \cdot v$. Then, $W$ is a
quotient of $U\left(  \mathcal{A}_{0}\right)  $ (as an $\mathcal{A}_{0}%
$-module). Since $K$ acts as $1$ on $W$, it follows that $W$ is a quotient of
$U\left(  \mathcal{A}_{0}\right)  \diagup\left(  K-1\right)  \cong D\left(
x_{1},x_{2},x_{3},...\right)  $. Since $I_{v}$ annihilates $v$, it follows
that $W$ is a quotient of $D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  $. Let us denote the
$\mathcal{A}_{0}$-module $D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  $ by $\widetilde{W}$.

We now will prove that $\widetilde{W}$ is a finite-length $\mathcal{A}_{0}%
$-module with all composition factors isomorphic to $F$.\ \ \ \ \footnote{We
can even prove that there are exactly $\dim\left(  \mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  \diagup I_{v}\right)  $ composition factors.}

Let $\mathfrak{i}$ be the ideal $\left(  a_{1},a_{2},a_{3},...\right)  $ of
the commutative algebra $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $.

Since $I_{v}$ is an ideal of the commutative algebra $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $, the quotient $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  \diagup I_{v}$ is an algebra. For every
$q\in\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $, let $\overline{q}$ be
the projection of $q$ onto the quotient algebra $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  \diagup I_{v}$. Let also $\overline{\mathfrak{i}}$ be
the projection of the ideal $\mathfrak{i}$ onto the quotient algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$. Clearly,
$\overline{\mathfrak{i}}=\left(  \overline{a_{1}},\overline{a_{2}}%
,\overline{a_{3}},...\right)  $.

For every $j>0$, there exists some $i\in\mathbb{N}$ such that $a_{j}^{i}v=0$
(since $V$ has a locally nilpotent action of $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  $). Hence, for every $j>0$, the element
$\overline{a_{j}}$ of $\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup
I_{v}$ is nilpotent (because there exists some $i\in\mathbb{N}$ such that
$a_{j}^{i}v=0$, and thus this $i$ satisfies $a_{j}^{i}\in I_{v}$, so that
$\overline{a_{j}}^{i}=0$). Hence, the ideal $\overline{\mathfrak{i}}$ is
generated by nilpotent generators (since $\overline{\mathfrak{i}}=\left(
\overline{a_{1}},\overline{a_{2}},\overline{a_{3}},...\right)  $). Since we
also know that $\overline{\mathfrak{i}}$ is finitely generated (since
$\overline{\mathfrak{i}}$ is an ideal of the finite-dimensional algebra
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup I_{v}$), it follows
that $\overline{\mathfrak{i}}$ is generated by \textit{finitely many}
nilpotent generators. But if an ideal of a commutative ring is generated by
finitely many nilpotent generators, it must be nilpotent. Thus, $\overline
{\mathfrak{i}}$ is nilpotent. In other words, there exists some $M\in
\mathbb{N}$ such that $\overline{\mathfrak{i}}^{M}=0$. Consider this $M$.
Since $\overline{\mathfrak{i}}^{M}=0$, we have $\mathfrak{i}^{M}\subseteq
I_{v}$ and thus $\mathfrak{i}^{M}+I_{v}=I_{v}$.

Now, Lemma \ref{lem.V=F(X)U.coflags} (applied to $k=\mathbb{C}$,
$B=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ and $I=I_{v}$) yields
that there exists an $\mathfrak{i}$-coflag from $\mathbb{C}\left[  a_{1}%
,a_{2},a_{3},...\right]  $ to $\mathfrak{i}^{M}+I_{v}$. Denote this
$\mathfrak{i}$-coflag by $\left(  J_{0},J_{1},...,J_{N}\right)  $. Since
$\mathfrak{i}^{M}+I_{v}=I_{v}$, this $\mathfrak{i}$-coflag $\left(
J_{0},J_{1},...,J_{N}\right)  $ thus is an $\mathfrak{i}$-coflag from
$\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ to $I_{v}$. Thus, $\left(
J_{0},J_{1},...,J_{N}\right)  $ is a complete coflag from $\mathbb{C}\left[
a_{1},a_{2},a_{3},...\right]  $ to $I_{v}$. In other words:

\begin{itemize}
\item We have $J_{0}\supseteq J_{1}\supseteq...\supseteq J_{N}$.

\item Every $i\in\left\{  0,1,...,N\right\}  $ satisfies $\dim\left(
\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  \diagup J_{i}\right)  =i$.

\item We have $J_{0}=\mathbb{C}\left[  a_{1},a_{2},a_{3},...\right]  $ and
$J_{N}=I_{v}$.
\end{itemize}

Besides, since $\left(  J_{0},J_{1},...,J_{N}\right)  $ is an $\mathfrak{i}%
$-coflag, we have%
\begin{equation}
\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  0,1,...,N-1\right\}  . \label{prop.V=F(X)U.pf.2}%
\end{equation}


For every $i\in\left\{  0,1,...,N\right\}  $, let $D_{i}=D\left(  x_{1}%
,x_{2},...\right)  \cdot J_{i}$. Then,%
\[
D_{0}=D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{0}}_{=\mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  }=D\left(  x_{1},x_{2},...\right)
\]
and%
\[
D_{N}=D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{N}}_{=I_{v}%
}=D\left(  x_{1},x_{2},...\right)  \cdot I_{v}.
\]
Hence, $D_{0}\diagup D_{N}=D\left(  x_{1},x_{2},...\right)  \diagup\left(
D\left(  x_{1},x_{2},...\right)  I_{v}\right)  =\widetilde{W}$.

Now, we are going to prove that%
\begin{equation}
D_{i}\diagup D_{i+1}\cong F\text{ or }D_{i}\diagup D_{i+1}%
=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  0,1,...,N-1\right\}
\label{prop.V=F(X)U.pf.3}%
\end{equation}
(where $\cong$ means isomorphism of $\mathcal{A}_{0}$-modules).

\textit{Proof of (\ref{prop.V=F(X)U.pf.3}).} Let $i\in\left\{
0,1,...,N-1\right\}  $. Since $\dim\left(  \mathbb{C}\left[  a_{1},a_{2}%
,a_{3},...\right]  \diagup J_{i}\right)  =i$ and $\dim\left(  \mathbb{C}%
\left[  a_{1},a_{2},a_{3},...\right]  \diagup J_{i+1}\right)  =i+1$, there
exists some $u\in J_{i}$ such that $J_{i}=u+J_{i+1}$. Consider this $u$. By
abuse of notation, we also use the letter $u$ to denote the element $1\cdot
u\in D\left(  x_{1},x_{2},...\right)  \cdot J_{i}=D_{i}$. Then,
\begin{align*}
D_{i}  &  =D\left(  x_{1},x_{2},...\right)  \cdot\underbrace{J_{i}%
}_{=u+J_{i+1}}=D\left(  x_{1},x_{2},...\right)  \cdot\left(  u+J_{i+1}\right)
\\
&  =D\left(  x_{1},x_{2},...\right)  \cdot u+\underbrace{D\left(  x_{1}%
,x_{2},...\right)  \cdot J_{i+1}}_{=D_{i+1}}=D\left(  x_{1},x_{2},...\right)
\cdot u+D_{i+1}.
\end{align*}
Thus,
\[
D_{i}\diagup D_{i+1}=D\left(  x_{1},x_{2},...\right)  \cdot u^{\prime},
\]
where $u^{\prime}$ denotes the residue class of $u\in D_{i}$ modulo $D_{i+1}$.
For every $j>0$, we have $\underbrace{a_{j}}_{\in\mathfrak{i}}\underbrace{u}%
_{\in J_{i}}\in\mathfrak{i}\cdot J_{i}\subseteq J_{i+1}$ (by
(\ref{prop.V=F(X)U.pf.2})) and thus $a_{j}u\in D\left(  x_{1},x_{2}%
,...\right)  \cdot J_{i+1}=D_{i+1}$. In other words, for every $j>0$, we have
$a_{j}u^{\prime}=0$. Also, it is pretty clear that $Ku^{\prime}=u^{\prime}$.
Thus, Lemma \ref{lem.V=F} (applied to $D_{i}\diagup D_{i+1}$ and $u^{\prime}$
instead of $V$ and $u$) yields that there exists a homomorphism $\eta
:F\rightarrow D_{i}\diagup D_{i+1}$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =u^{\prime}$. This homomorphism $\eta$ must be
surjective\footnote{since its image is $\eta\left(  \underbrace{F}_{=D\left(
x_{1},x_{2},...\right)  \cdot1}\right)  =D\left(  x_{1},x_{2},...\right)
\cdot\underbrace{\eta\left(  1\right)  }_{=u^{\prime}}=D\left(  x_{1}%
,x_{2},...\right)  \cdot u^{\prime}=D_{i}\diagup D_{i+1}$}, and thus
$D_{i}\diagup D_{i+1}$ is a factor module of $F$. Since $F$ is irreducible,
this yields that $D_{i}\diagup D_{i+1}\cong F$ or $D_{i}\diagup D_{i+1}=0$.
This proves (\ref{prop.V=F(X)U.pf.3}).

Now, clearly, the $\mathcal{A}_{0}$-module $\widetilde{W}=D_{0}\diagup D_{N}$
is filtered by the $\mathcal{A}_{0}$-modules $D_{i}\diagup D_{N}$ for
$i\in\left\{  0,1,...,N\right\}  $. Due to (\ref{prop.V=F(X)U.pf.3}), the
subquotients of this filtration are all $\cong F$ or $=0$, so that
$\widetilde{W}$ is a finite-length $\mathcal{A}_{0}$-module with all
composition factors isomorphic to $F$ (since $F$ is irreducible).

Since $W$ is a quotient module of $\widetilde{W}$, this yields that $W$ must
also be a finite-length $\mathcal{A}_{0}$-module with all composition factors
isomorphic to $F$.

Now forget that we fixed $v$. We have thus shown that for every $v\in V$, the
$\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of $V$
(this submodule is what we called $W$) is a finite-length module with
composition factors isomorphic to $F$.

By the assumption (that for every $v\in V$, there exists some $N\in\mathbb{N}$
such that for every $n\geq N$, we have $a_{n}v=0$), we can define an action of
$E=\sum\limits_{i>0}a_{-i}a_{i}\in\widehat{\mathcal{A}}$ (the so-called
\textit{Euler field}) on $V$. Note that $E$ acts on $V$ in a locally finite
way (this means that for any $v\in V$, the space $\mathbb{C}\left[  E\right]
\cdot v$ is finite-dimensional)\footnote{\textit{Proof.} Notice that $E$ acts
on $F$ as $\sum\limits_{i>0}ix_{i}\dfrac{\partial}{\partial x_{i}}$, and thus
$E$ acts on $F$ in a locally finite way (since the differential operator
$\sum\limits_{i>0}ix_{i}\dfrac{\partial}{\partial x_{i}}$ preserves the
degrees of polynomials), and thus also on $V$ (because for every $v\in V$, the
$\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of $V$
is a finite-length module with composition factors isomorphic to $F$).}. Now,
let us notice that the eigenvalues of the map $E\mid_{V}:V\rightarrow V$ (this
is the action of $E$ on $V$) are nonnegative
integers.\footnote{\textit{Proof.} Let $\rho$ be an eigenvalue of $E\mid_{V}$.
Then, there exists some nonzero eigenvector $v\in V$ to the eigenvalue $\rho$.
Consider this $v$. Clearly, $\rho$ must thus also be an eigenvalue of
$E\mid_{U\left(  \mathcal{A}_{0}\right)  \cdot v}$ (because $v$ is a nonzero
eigenvector of $E\mid_{V}$ to the eigenvalue $\rho$ and lies in $U\left(
\mathcal{A}_{0}\right)  \cdot v$). But the eigenvalues of $E\mid_{U\left(
\mathcal{A}_{0}\right)  \cdot v}$ are nonnegative integers (since we know that
the $\mathcal{A}_{0}$-submodule $U\left(  \mathcal{A}_{0}\right)  \cdot v$ of
$V$ is a finite-length module with composition factors isomorphic to $F$, and
we can easily check that the eigenvalues of $E\mid_{F}$ are nonnegative
integers). Hence, $\rho$ is a nonnegative integer. We have thus shown that
every eigenvalue of $E\mid_{V}$ is a nonnegative integer, qed.} Hence, we can
write $V$ as $V=\bigoplus\limits_{j\geq0}V\left[  j\right]  $, where $V\left[
j\right]  $ is the generalized eigenspace of $E\mid_{V}$ with eigenvalue $j$
for every $j\in\mathbb{N}$.

If some $v\in V$ satisfies $a_{i}v=0$ for all $i>0$, then $Ev=0$ and thus
$v\in V\left[  0\right]  $.

Conversely, if $v\in V\left[  0\right]  $, then $a_{i}v=0$ for all
$i>0$.\ \ \ \ \footnote{\textit{Proof.} Let $v\in V\left[  0\right]  $. Let
$j$ be positive.
\par
It is easy to check that $a_{-i}a_{i}a_{j}=a_{j}a_{-i}a_{i}-i\delta_{i,j}%
a_{i}$ for any positive $i$ (here, we use that $j>0$). Since $E=\sum
\limits_{i>0}a_{-i}a_{i}$, we have%
\begin{align*}
Ea_{j}  &  =\sum\limits_{i>0}\underbrace{a_{-i}a_{i}a_{j}}_{=a_{j}a_{-i}%
a_{i}-i\delta_{i,j}a_{i}}=\sum\limits_{i>0}\left(  a_{j}a_{-i}a_{i}%
-i\delta_{i,j}a_{i}\right) \\
&  =a_{j}\underbrace{\sum\limits_{i>0}a_{-i}a_{i}}_{=E}-\underbrace{\sum
\limits_{i>0}i\delta_{i,j}a_{i}}_{=ja_{j}}=a_{j}E-ja_{j},
\end{align*}
so that $\left(  E+j\right)  a_{j}=a_{j}E$. This yields (by induction over
$m$) that $\left(  E+j\right)  ^{m}a_{j}=a_{j}E^{m}$ for every $m\in
\mathbb{N}$.
\par
Now, since $v\in V\left[  0\right]  =\left(  \text{generalized eigenspace of
}E\mid_{V}\text{ with eigenvalue }0\right)  $, there exists an $m\in
\mathbb{N}$ such that $E^{m}v=0$. Consider this $m$. Then, from $\left(
E+j\right)  ^{m}a_{j}=a_{j}E^{m}$, we obtain $\left(  E+j\right)  ^{m}%
a_{j}v=a_{j}E^{m}v=0$, so that
\[
a_{j}v\in\left(  \text{generalized eigenspace of }E\mid_{V}\text{ with
eigenvalue }-j\right)  =0
\]
(because the eigenvalues of the map $E\mid_{V}:V\rightarrow V$ are nonnegative
integers, whereas $-j$ is not). In other words, $a_{j}v=0$.
\par
We have thus proven that $a_{j}v=0$ for every positive $j$. In other words,
$a_{i}v=0$ for all $i>0$, qed.}

So we conclude that $V\left[  0\right]  =\operatorname*{Ker}E=\bigcap
\limits_{i\geq1}\operatorname*{Ker}a_{i}$.

Now, $F\otimes V\left[  0\right]  $ is an $\mathcal{A}_{0}$-module (where
$\mathcal{A}_{0}$ acts only on the $F$ tensorand, where $V\left[  0\right]  $
is considered just as a vector space). We will now construct an isomorphism
$F\otimes V\left[  0\right]  \rightarrow V$ of $\mathcal{A}_{0}$-modules. This
will prove Proposition \ref{prop.V=F(X)U}.

For every $v\in V\left[  0\right]  $, there exists a homomorphism $\eta
_{v}:F\rightarrow V$ of $\mathcal{A}_{0}$-modules such that $\eta_{v}\left(
1\right)  =v$ (according to Lemma \ref{lem.V=F}, applied to $v$ instead of $u$
(since $a_{i}v=0$ for all $i>0$ and $Kv=v$)). Consider these homomorphisms
$\eta_{v}$ for various $v$. Clearly, every $v\in V\left[  0\right]  $ and
$P\in F$ satisfy%
\begin{align*}
\eta_{v}\left(  P\right)   &  =\eta_{v}\left(  P\left(  a_{-1},a_{-2}%
,a_{-3},...\right)  \cdot1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}P=P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot1\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  \underbrace{\eta_{v}\left(
1\right)  }_{=v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\eta_{v}\text{ is an
}\mathcal{A}_{0}\text{-module map}\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  v.
\end{align*}
Hence, we can define a $\mathbb{C}$-linear map $\rho:F\otimes V\left[
0\right]  \rightarrow V$ by%
\[
\rho\left(  P\otimes v\right)  =\eta_{v}\left(  P\right)  =P\left(
a_{-1},a_{-2},a_{-3},...\right)  v\ \ \ \ \ \ \ \ \ \ \text{for any }P\in
F\text{ and }v\in V\left[  0\right]  .
\]
This map $\rho$ is an $\mathcal{A}_{0}$-module map (because $\eta_{v}$ is an
$\mathcal{A}_{0}$-module map for every $v\in V\left[  0\right]  $).

The restriction of the map $\rho$ to the subspace $\mathbb{C}\cdot1\otimes
V\left[  0\right]  $ of $F\otimes V\left[  0\right]  $ is injective (since it
maps every $1\otimes v$ to $v$). Hence, the map $\rho$ is
injective\footnote{This follows from the following general
representation-theoretical fact (applied to $A=U\left(  \mathcal{A}%
_{0}\right)  $, $I=F$, $R=V\left[  0\right]  $, $S=V$, $i=1$ and $\phi=\rho$):
\par
Let $A$ be a $\mathbb{C}$-algebra. Let $I$ be an irreducible $A$-module, and
let $S$ be an $A$-module. Let $R$ be a vector space. Let $i\in I$ be nonzero.
Let $\phi:I\otimes R\rightarrow S$ be an $A$-module homomorphism such that the
restriction of $\phi$ to $\mathbb{C}i\otimes R$ is injective. Then, $\phi$ is
injective.}. Also, considering the quotient $\mathcal{A}_{0}$-module
$V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  $, we notice that
$E\mid_{V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  }$ has only
strictly positive eigenvalues (since $\rho\left(  F\otimes V\left[  0\right]
\right)  \supseteq V\left[  0\right]  $, so that all eigenvectors of
$E\mid_{V}$ to eigenvalue $0$ have been killed when factoring modulo
$\rho\left(  F\otimes V\left[  0\right]  \right)  $), and thus $V\diagup
\rho\left(  F\otimes V\left[  0\right]  \right)  =0$%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, $V\diagup
\rho\left(  F\otimes V\left[  0\right]  \right)  \neq0$. Thus, there exists
some nonzero $w\in V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  $.
Write $w$ as $\overline{v}$, where $v$ is an element of $V$ and $\overline{v}$
denotes the residue class of $v$ modulo $\rho\left(  F\otimes V\left[
0\right]  \right)  $. As we know, the $\mathcal{A}_{0}$-submodule $U\left(
\mathcal{A}_{0}\right)  \cdot v$ of $V$ is a finite-length module with
composition factors isomorphic to $F$. Thus, the $\mathcal{A}_{0}$-module
$U\left(  \mathcal{A}_{0}\right)  \cdot w$ (being a quotient module of
$U\left(  \mathcal{A}_{0}\right)  \cdot v$) must also be a finite-length
module with composition factors isomorphic to $F$. Hence, there exists a
submodule of $U\left(  \mathcal{A}_{0}\right)  \cdot w$ isomorphic to $F$
(since $w\neq0$ and thus $U\left(  \mathcal{A}_{0}\right)  \cdot w\neq0$).
This submodule contains a nonzero eigenvector of $E$ to eigenvalue $0$
(because $F$ contains a nonzero eigenvector of $E$ to eigenvalue $0$, namely
$1$). This is a contradiction to the fact that $E\mid_{V\diagup\rho\left(
F\otimes V\left[  0\right]  \right)  }$ has only strictly positive
eigenvalues. This contradiction shows that our assumption was wrong, so we do
have $V\diagup\rho\left(  F\otimes V\left[  0\right]  \right)  =0$, qed.}. In
other words, $V=\rho\left(  F\otimes V\left[  0\right]  \right)  $, so that
$\rho$ is surjective. Since $\rho$ is an injective and surjective
$\mathcal{A}_{0}$-module map, we conclude that $\rho$ is an $\mathcal{A}_{0}%
$-module isomorphism. Thus, $V\cong F\otimes V\left[  0\right]  $ as
$\mathcal{A}_{0}$-modules. This proves Proposition \ref{prop.V=F(X)U}.

\subsubsection{Remark on $\mathcal{A}$-modules}

We will not use this until much later, but here is an analogue of Lemma
\ref{lem.V=F} for $\mathcal{A}$ instead of $\mathcal{A}_{0}$:

\begin{lemma}
\label{lem.V=F.A}Let $V$ be an $\mathcal{A}$-module. Let $\mu\in\mathbb{C}$.
Let $u\in V$ be such that $a_{i}u=0$ for all $i>0$, such that $a_{0}u=\mu u$,
and such that $Ku=u$. Then, there exists a homomorphism $\eta:F_{\mu
}\rightarrow V$ of $\mathcal{A}$-modules such that $\eta\left(  1\right)  =u$.
(This homomorphism $\eta$ is unique, although we won't need this.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.V=F.A}.} Let $\eta$ be the map $F\rightarrow
V$ which sends every polynomial $P\in F=\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to $P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u\in
V$.\ \ \ \ \footnote{Note that the term $P\left(  a_{-1},a_{-2},a_{-3}%
,...\right)  $ denotes the evaluation of the polynomial $P$ at $\left(
x_{1},x_{2},x_{3},...\right)  =\left(  a_{-1},a_{-2},a_{-3},...\right)  $.
This evaluation is a well-defined element of $U\left(  \mathcal{A}_{0}\right)
$, since the elements $a_{-1}$, $a_{-2}$, $a_{-3}$, $...$ of $U\left(
\mathcal{A}_{0}\right)  $ commute.} Just as in the Second proof of Lemma
\ref{lem.V=F}, we can show that $\eta$ is an $\mathcal{A}_{0}$-module
homomorphism $F\rightarrow V$ such that $\eta\left(  1\right)  =u$. We are now
going to prove that this $\eta$ is also a homomorphism $F_{\mu}\rightarrow V$
of $\mathcal{A}$-modules. Clearly, in order to prove this, it is enough to
show that $\eta\left(  a_{0}P\right)  =a_{0}\eta\left(  P\right)  $ for all
$P\in F_{\mu}$.

Let $P\in F_{\mu}$. Since $a_{0}$ acts as multiplication by $\mu$ on $F_{\mu}%
$, we have $a_{0}P=\mu P$.

On the other hand, by the definition of $\eta$, we have $\eta\left(  P\right)
=P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u$, so that%
\begin{align*}
a_{0}\eta\left(  P\right)   &  =a_{0}P\left(  a_{-1},a_{-2},a_{-3},...\right)
\cdot u=P\left(  a_{-1},a_{-2},a_{-3},...\right)  a_{0}\cdot u\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{0}\text{ lies in the center of }\mathcal{A}\text{, and thus in
the center of }U\left(  \mathcal{A}\right)  \text{,}\\
\text{and thus }a_{0}P\left(  a_{-1},a_{-2},a_{-3},...\right)  =P\left(
a_{-1},a_{-2},a_{-3},...\right)  a_{0}%
\end{array}
\right) \\
&  =P\left(  a_{-1},a_{-2},a_{-3},...\right)  \underbrace{a_{0}u}_{=\mu u}%
=\mu\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  \cdot u}%
_{=\eta\left(  P\right)  }=\mu\eta\left(  P\right) \\
&  =\eta\left(  \underbrace{\mu P}_{=a_{0}P}\right)  =\eta\left(
a_{0}P\right)  .
\end{align*}
Thus, we have shown that $\eta\left(  a_{0}P\right)  =a_{0}\eta\left(
P\right)  $ for all $P\in F_{\mu}$. This completes the proof of Lemma
\ref{lem.V=F.A}.

\subsubsection{A rescaled version of the Fock space}

Here is a statement very similar to Corollary \ref{cor.fock}:

\begin{corollary}
\label{cor.focktilde}The Lie algebra $\mathcal{A}_{0}$ has a representation
$\widetilde{F}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ which is
given by
\begin{align*}
a_{-i}  &  \mapsto ix_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\geq1;\\
a_{i}  &  \mapsto\dfrac{\partial}{\partial x_{i}}\ \ \ \ \ \ \ \ \ \ \text{for
every }i\geq1,\\
K  &  \mapsto1
\end{align*}
(where "$a_{-i}\mapsto ix_{i}$" is just shorthand for "$a_{-i}\mapsto\left(
\text{multiplication by }ix_{i}\right)  $"). For every $\mu\in\mathbb{C}$, we
can upgrade $\widetilde{F}$ to a representation $\widetilde{F}_{\mu}$ of
$\mathcal{A}$ by adding the condition that $a_{0}\mid_{\widetilde{F}_{\mu}%
}=\mu\cdot\operatorname*{id}$.
\end{corollary}

Note that the $\mathcal{A}_{0}$-module structure on $\widetilde{F}$ differs
from that on $F$ by a different choice of "where to put the $i$ factor": in
$F$ it is in the action of $a_{i}$, while in $\widetilde{F}$ it is in the
action of $a_{-i}$ (where $i\geq1$).

\begin{definition}
\label{def.focktilde}The representation $\widetilde{F}$ of $\mathcal{A}_{0}$
introduced in Corollary \ref{cor.focktilde} will be called the
\textit{rescaled Fock module} or the \textit{rescaled Fock representation}.
For every $\mu\in\mathbb{C}$, the representation $\widetilde{F}_{\mu}$ of
$\mathcal{A}$ introduced in Corollary \ref{cor.focktilde} will be called the
\textit{rescaled }$\mu$\textit{-Fock representation} of $\mathcal{A}$. The
vector space $\widetilde{F}$ itself, of course, is the same as the vector
space $F$ of Corollary \ref{cor.fock}, and thus we simply call it the Fock space.
\end{definition}

\begin{proposition}
\label{prop.resc}Let $\operatorname*{resc}:\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  \rightarrow\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]
$ be the $\mathbb{C}$-algebra homomorphism which sends $x_{i}$ to $ix_{i}$ for
every $i\in\left\{  1,2,3,...\right\}  $. (This homomorphism exists and is
unique by the universal property of the polynomial algebra. It is clear that
$\operatorname*{resc}$ multiplies every monomial by some scalar.)

\textbf{(a)} Then, $\operatorname*{resc}$ is an $\mathcal{A}_{0}$-module
isomorphism $F\rightarrow\widetilde{F}$. Thus, $F\cong\widetilde{F}$ as
$\mathcal{A}_{0}$-modules.

\textbf{(b)} Let $\mu\in\mathbb{C}$. Then, $\operatorname*{resc}$ is an
$\mathcal{A}$-module isomorphism $F_{\mu}\rightarrow\widetilde{F}_{\mu}$.
Thus, $F_{\mu}\cong\widetilde{F}_{\mu}$ as $\mathcal{A}$-modules.
\end{proposition}

Corollary \ref{cor.focktilde} and Proposition \ref{prop.resc} are both very
easy to prove: It is best to prove Proposition \ref{prop.resc} first (without
yet knowing that $\widetilde{F}$ and $\widetilde{F}_{\mu}$ are really an
$\mathcal{A}_{0}$-module and an $\mathcal{A}$-module, respectively), and then
use it to derive Corollary \ref{cor.focktilde} from Corollary \ref{cor.fock}
by means of $\operatorname*{resc}$. We leave all details to the reader.

The modules $\widetilde{F}$ and $F$ aren't that much different: They are
isomorphic by an isomorphism which has diagonal form with respect to the
monomial bases (due to Proposition \ref{prop.resc}). Nevertheless, it pays off
to use different notations for them so as not to let confusion arise. We are
going to work with $F$ most of the time, except when $\widetilde{F}$ is easier
to handle.

\subsubsection{An involution on $\mathcal{A}$ and a bilinear form on the Fock
space}

The following fact is extremely easy to prove:

\begin{proposition}
\label{prop.A.omega}Define a $\mathbb{C}$-linear map $\omega:\mathcal{A}%
\rightarrow\mathcal{A}$ by setting
\begin{align*}
\omega\left(  K\right)   &  =-K\ \ \ \ \ \ \ \ \ \ \text{and}\\
\omega\left(  a_{i}\right)   &  =-a_{-i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\mathbb{Z}.
\end{align*}
Then, $\omega$ is an automorphism of the Lie algebra $\mathcal{A}$. Also,
$\omega$ is an involution (this means that $\omega^{2}=\operatorname*{id}$).
Moreover, $\omega\left(  \mathcal{A}\left[  i\right]  \right)  =\mathcal{A}%
\left[  -i\right]  $ for all $i\in\mathbb{Z}$. Finally, $\omega\mid
_{\mathcal{A}\left[  0\right]  }=-\operatorname*{id}$.
\end{proposition}

Now, let us make a few conventions:

\begin{Convention}
\label{conv.fin}In the following, a map $\varphi:A\rightarrow\mathbb{N}$
(where $A$ is some set) is said to be \textit{finitely supported} if all but
finitely many $a\in A$ satisfy $\varphi\left(  a\right)  =0$. Sequences
(finite, infinite, or two-sided infinite) are considered as maps (from finite
sets, $\mathbb{N}$ or $\mathbb{Z}$, or occasionally other sets). Thus, a
sequence is finitely supported if and only if all but finitely many of its
elements are zero.

If $A$ is a set, then $\mathbb{N}_{\operatorname*{fin}}^{A}$ will denote the
set of all finitely supported maps $A\rightarrow\mathbb{N}$.
\end{Convention}

\begin{proposition}
\label{prop.A.contravariantform}Define a $\mathbb{C}$-bilinear form $\left(
\cdot,\cdot\right)  :F\times F\rightarrow\mathbb{C}$ by setting%
\begin{align*}
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)   &  =\prod\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}%
}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!\\
&  \ \ \ \ \ \ \ \ \ \ \left.
\begin{array}
[c]{c}%
\text{for all sequences }\left(  n_{1},n_{2},n_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\\
\text{and }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }%
\end{array}
\right.  .
\end{align*}
(This is well-defined, because each of the infinite products $\prod
\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}}$, $\prod\limits_{i=1}^{\infty
}i^{n_{i}}$ and $\prod\limits_{i=1}^{\infty}n_{i}!$ has only finitely many
terms distinct from $1$, and thus is well-defined.)

\textbf{(a)} This form $\left(  \cdot,\cdot\right)  $ is symmetric and nondegenerate.

\textbf{(b)} Every polynomial $P\in F=\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ satisfies $\left(  1,P\right)  =P\left(
0,0,0,...\right)  $.

\textbf{(c)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in F_{\mu}$ and
$Q\in F_{\mu}$ satisfy $\left(  xP,Q\right)  =-\left(  P,\omega\left(
x\right)  Q\right)  $, where $xP$ and $\omega\left(  x\right)  Q$ are
evaluated in the $\mathcal{A}$-module $F_{\mu}$.

\textbf{(d)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in F_{\mu}$ and
$Q\in F_{\mu}$ satisfy $\left(  P,xQ\right)  =-\left(  \omega\left(  x\right)
P,Q\right)  $, where $xQ$ and $\omega\left(  x\right)  P$ are evaluated in the
$\mathcal{A}$-module $F_{\mu}$.

\textbf{(e)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in
\widetilde{F}_{\mu}$ and $Q\in\widetilde{F}_{\mu}$ satisfy $\left(
xP,Q\right)  =-\left(  P,\omega\left(  x\right)  Q\right)  $, where $xP$ and
$\omega\left(  x\right)  Q$ are evaluated in the $\mathcal{A}$-module
$\widetilde{F}_{\mu}$.

\textbf{(f)} Let $\mu\in\mathbb{C}$. Any $x\in\mathcal{A}$, $P\in
\widetilde{F}_{\mu}$ and $Q\in\widetilde{F}_{\mu}$ satisfy $\left(
P,xQ\right)  =-\left(  \omega\left(  x\right)  P,Q\right)  $, where $xQ$ and
$\omega\left(  x\right)  P$ are evaluated in the $\mathcal{A}$-module
$\widetilde{F}_{\mu}$.
\end{proposition}

We are going to put the form $\left(  \cdot,\cdot\right)  $ from this
proposition into a broader context in Proposition \ref{prop.invol.A}; indeed,
we will see that it is an example of a contravariant form on a Verma module of
a Lie algebra with involution. ("Contravariant" means that $\left(
av,w\right)  =-\left(  v,\omega\left(  a\right)  w\right)  $ and $\left(
v,aw\right)  =-\left(  \omega\left(  a\right)  v,w\right)  $ for all $a$ in
the Lie algebra and $v$ and $w$ in the module. In the case of our form
$\left(  \cdot,\cdot\right)  $, the contravariantness of the form follows from
Proposition \ref{prop.A.contravariantform} \textbf{(c)} and \textbf{(d)}.)

\textit{Proof of Proposition \ref{prop.A.contravariantform}.} \textbf{(a)} For
any sequences $\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ and $\left(
m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, we have%
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  =\prod\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}%
}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!
\]
and%
\[
\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...,x_{1}^{n_{1}}x_{2}^{n_{2}%
}x_{3}^{n_{3}}...\right)  =\prod\limits_{i=1}^{\infty}\delta_{m_{i},n_{i}%
}\cdot\prod\limits_{i=1}^{\infty}i^{m_{i}}\cdot\prod\limits_{i=1}^{\infty
}m_{i}!.
\]
These two terms are equal in the case when $\left(  n_{1},n_{2},n_{3}%
,...\right)  \neq\left(  m_{1},m_{2},m_{3},...\right)  $ (because in this
case, they are both $0$ due to the presence of the $\prod\limits_{i=1}%
^{\infty}\delta_{n_{i},m_{i}}$ and $\prod\limits_{i=1}^{\infty}\delta
_{m_{i},n_{i}}$ factors), and are clearly equal in the case when $\left(
n_{1},n_{2},n_{3},...\right)  =\left(  m_{1},m_{2},m_{3},...\right)  $ as
well. Hence, these two terms are always equal. In other words, any sequences
$\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ and $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfy
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  =\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...,x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right)  .
\]
This proves that the form $\left(  \cdot,\cdot\right)  $ is symmetric.

The space $F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ has a basis
consisting of monomials. With respect to this basis, the form $\left(
\cdot,\cdot\right)  $ is represented by a diagonal matrix (because whenever
$\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ and $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ are
distinct, we have
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}%
}x_{3}^{m_{3}}...\right)  =\underbrace{\prod\limits_{i=1}^{\infty}%
\delta_{n_{i},m_{i}}}_{\substack{=0\\\text{(since }\left(  n_{1},n_{2}%
,n_{3},...\right)  \neq\left(  m_{1},m_{2},m_{3},...\right)  \text{)}}%
}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!=0
\]
), whose diagonal entries are all nonzero (since every $\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$ satisfies%
\[
\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{n_{1}}x_{2}^{n_{2}%
}x_{3}^{n_{3}}...\right)  =\prod\limits_{i=1}^{\infty}\underbrace{\delta
_{n_{i},n_{i}}}_{=1}\cdot\prod\limits_{i=1}^{\infty}\underbrace{i^{n_{i}}%
}_{\neq0}\cdot\prod\limits_{i=1}^{\infty}\underbrace{n_{i}!}_{\neq0}\neq0
\]
). Hence, this form is nondegenerate. Proposition
\ref{prop.A.contravariantform} \textbf{(a)} is proven.

\textbf{(b)} We must prove that every polynomial $P\in F=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ satisfies $\left(  1,P\right)  =P\left(
0,0,0,...\right)  $. In order to show this, it is enough to check that every
\textbf{monomial} $P\in F=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $
satisfies $\left(  1,P\right)  =P\left(  0,0,0,...\right)  $ (because the
equation $\left(  1,P\right)  =P\left(  0,0,0,...\right)  $ is linear in $P$,
and because the monomials span $F$). In other words, we must check that every
$\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ satisfies $\left(  1,x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)  =\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...\right)  \left(  0,0,0,...\right)  $. But this is easy:%
\begin{align*}
\left(  \underbrace{1}_{=x_{1}^{0}x_{2}^{0}x_{3}^{0}...},x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)   &  =\left(  x_{1}^{0}x_{2}^{0}x_{3}%
^{0}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)  =\prod
\limits_{i=1}^{\infty}\underbrace{\delta_{0,m_{i}}}_{=0^{m_{i}}}\cdot
\prod\limits_{i=1}^{\infty}\underbrace{i^{0}}_{=1}\cdot\prod\limits_{i=1}%
^{\infty}\underbrace{0!}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  \cdot
,\cdot\right)  \right) \\
&  =\prod\limits_{i=1}^{\infty}0^{m_{i}}=0^{m_{1}}0^{m_{2}}0^{m_{3}%
}...=\left(  x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)  \left(
0,0,0,...\right)  ,
\end{align*}
qed. Proposition \ref{prop.A.contravariantform} \textbf{(b)} is proven.

\textbf{(c)} We must prove that any $x\in\mathcal{A}$, $P\in F_{\mu}$ and
$Q\in F_{\mu}$ satisfy $\left(  xP,Q\right)  =-\left(  P,\omega\left(
x\right)  Q\right)  $. Since this equation is linear in each of $x$, $P$ and
$Q$, we can WLOG assume that $x$ is an element of the basis $\left\{
a_{n}\ \mid\ n\in\mathbb{Z}\right\}  \cup\left\{  K\right\}  $ of
$\mathcal{A}$ and that $P$ and $Q$ are monomials (since monomials span $F$).
So let us assume this.

Since $x$ is an element of the basis $\left\{  a_{n}\ \mid\ n\in
\mathbb{Z}\right\}  \cup\left\{  K\right\}  $ of $\mathcal{A}$, we have either
$x=a_{j}$ for some $j\in\mathbb{Z}$, or $x=K$. Since the latter case is
trivial (in fact, when $x=K$, then%
\[
\left(  xP,Q\right)  =\left(  KP,Q\right)  =\left(  P,Q\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }1\text{ on }F_{\mu
}\text{, so that }KP=P\right)
\]
and%
\begin{align*}
-\left(  P,\omega\left(  \underbrace{x}_{=K}\right)  Q\right)   &  =-\left(
P,\underbrace{\omega\left(  K\right)  }_{=-K}Q\right)  =-\left(  P,-KQ\right)
=\left(  P,KQ\right)  =\left(  P,Q\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }1\text{ on
}F_{\mu}\text{, so that }KQ=Q\right)  ,
\end{align*}
so that $\left(  xP,Q\right)  =-\left(  P,\omega\left(  x\right)  Q\right)  $
is proven), we can WLOG assume that we are in the former case, i. e., that
$x=a_{j}$ for some $j\in\mathbb{Z}$. Assume this, and consider this $j$.

Since $P$ is a monomial, there exists a $\left(  n_{1},n_{2},n_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$. Consider this $\left(
n_{1},n_{2},n_{3},...\right)  $.

Since $Q$ is a monomial, there exists a $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$Q=x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$. Consider this $\left(
m_{1},m_{2},m_{3},...\right)  $.

We must prove that $\left(  xP,Q\right)  =-\left(  P,\omega\left(  x\right)
Q\right)  $. Since $\left(  xP,Q\right)  =\left(  a_{j}P,Q\right)  $ (because
$x=a_{j}$) and $-\left(  P,\omega\left(  x\right)  Q\right)  =\left(
P,a_{-j}Q\right)  $ (because $-\left(  P,\omega\left(  \underbrace{x}_{=a_{j}%
}\right)  Q\right)  =-\left(  P,\underbrace{\omega\left(  a_{j}\right)
}_{=-a_{-j}}Q\right)  =-\left(  P,-a_{-j}Q\right)  =\left(  P,a_{-j}Q\right)
$), this rewrites as $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $.
Hence, we must only prove that $\left(  a_{j}P,Q\right)  =\left(
P,a_{-j}Q\right)  $.

We will distinguish between three cases:

\textit{Case 1:} We have $j\geq1$.

\textit{Case 2:} We have $j=0$.

\textit{Case 3:} We have $j\leq-1$.

First, let us consider Case 1. In this case, by the definition of $F_{\mu}$,
we know that $a_{j}$ acts on $F_{\mu}$ as $j\dfrac{\partial}{\partial x_{j}}$,
whereas $a_{-j}$ acts on $F_{\mu}$ as multiplication by $x_{j}$. Hence,
$a_{j}P=j\dfrac{\partial}{\partial x_{j}}P$ and $a_{-j}Q=x_{j}Q$.

Since $Q=x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$, we have $x_{j}%
Q=x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}}...$,
where the sequence $\left(  m_{1}^{\prime},m_{2}^{\prime},m_{3}^{\prime
},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}$ is defined by%
\[
m_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
m_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
m_{i}+1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,3,...\right\}
.
\]
Note that this definition immediately yields $m_{j}^{\prime}=m_{j}+1\geq1$, so
that $\delta_{0,m_{j}^{\prime}}=0$.

As a consequence of the definition of $\left(  m_{1}^{\prime},m_{2}^{\prime
},m_{3}^{\prime},...\right)  $, we have $m_{i}^{\prime}-m_{i}=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  $ for every $i\in\left\{  1,2,3,...\right\}  $.

Now, $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is easily proven
when $n_{j}=0$\ \ \ \ \footnote{\textit{Proof.} Assume that $n_{j}=0$. Then,
$P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ is a monomial that does not
involve the indeterminate $n_{j}$; hence, $\dfrac{\partial}{\partial x_{j}%
}P=0$, so that $a_{j}P=j\underbrace{\dfrac{\partial}{\partial x_{j}}P}_{=0}%
=0$, and thus $\left(  a_{j}P,Q\right)  =\left(  0,Q\right)  =0$. On the other
hand, since $n_{j}=0$, we have $\delta_{n_{j},m_{j}^{\prime}}=\delta
_{0,m_{j}^{\prime}}=0$ and thus $\prod\limits_{i=1}^{\infty}\delta
_{n_{i},m_{i}^{\prime}}=0$ (since the product $\prod\limits_{i=1}^{\infty
}\delta_{n_{i},m_{i}^{\prime}}$ contains the factor $\delta_{n_{j}%
,m_{j}^{\prime}}$). Now, since $P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$
and $a_{-j}Q=x_{j}Q=x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}%
^{m_{3}^{\prime}}...$, we have%
\begin{align*}
\left(  P,a_{-j}Q\right)   &  =\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}%
}...,x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}%
}...\right) \\
&  =\underbrace{\prod\limits_{i=1}^{\infty}\delta_{n_{i},m_{i}^{\prime}}}%
_{=0}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}\cdot\prod\limits_{i=1}^{\infty
}n_{i}!\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(
\cdot,\cdot\right)  \right) \\
&  =0=\left(  a_{j}P,Q\right)  .
\end{align*}
Hence, $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is proven when
$n_{j}=0$.}. Hence, for the remaining part of Case 1, we can WLOG assume that
$n_{j}\neq0$. Let us assume this. Then, $n_{j}\geq1$. Hence, since
$P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$, we have $\dfrac{\partial
}{\partial x_{j}}P=n_{j}x_{1}^{n_{1}^{\prime}}x_{2}^{n_{2}^{\prime}}%
x_{3}^{n_{3}^{\prime}}...$, where the sequence $\left(  n_{1}^{\prime}%
,n_{2}^{\prime},n_{3}^{\prime},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ is defined by%
\[
n_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
n_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
n_{i}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,3,...\right\}
.
\]
From this definition, it is clear that the sequence $\left(  n_{1}^{\prime
},n_{2}^{\prime},n_{3}^{\prime},...\right)  $ differs from the sequence
$\left(  n_{1},n_{2},n_{3},...\right)  $ only in the $j$-th term. Hence, the
product $\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}$ differs from the
product $\prod\limits_{i=1}^{\infty}i^{n_{i}}$ only in the $j$-th factor.
Thus,%
\begin{align*}
\dfrac{\prod\limits_{i=1}^{\infty}i^{n_{i}}}{\prod\limits_{i=1}^{\infty
}i^{n_{i}^{\prime}}}  &  =\dfrac{j^{n_{j}}}{j^{n_{j}^{\prime}}}=\dfrac
{j^{n_{j}}}{j^{n_{j}-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n_{j}%
^{\prime}=n_{j}-1\text{ by the definition of }\left(  n_{1}^{\prime}%
,n_{2}^{\prime},n_{3}^{\prime},...\right)  \right) \\
&  =j,
\end{align*}
so that $\prod\limits_{i=1}^{\infty}i^{n_{i}}=j\prod\limits_{i=1}^{\infty
}i^{n_{i}^{\prime}}$. A similar argument (using the products $\prod
\limits_{i=1}^{\infty}n_{i}^{\prime}$ and $\prod\limits_{i=1}^{\infty}n_{i}$
instead of the products $\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}$ and
$\prod\limits_{i=1}^{\infty}i^{n_{i}}$) shows that $\prod\limits_{i=1}%
^{\infty}n_{i}!=n_{j}\prod\limits_{i=1}^{\infty}n_{i}^{\prime}!$.

As a consequence of the definition of $\left(  n_{1}^{\prime},n_{2}^{\prime
},n_{3}^{\prime},...\right)  $, we have $n_{i}-n_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  $ for every $i\in\left\{  1,2,3,...\right\}  $. Thus, every
$i\in\left\{  1,2,3,...\right\}  $ satisfies%
\[
n_{i}-n_{i}^{\prime}=\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq j;\\
1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j
\end{array}
\right.  =m_{i}^{\prime}-m_{i},
\]
so that $n_{i}-m_{i}^{\prime}=n_{i}^{\prime}-m_{i}$, so that $\delta
_{n_{i}-m_{i}^{\prime},0}=\delta_{n_{i}^{\prime}-m_{i},0}$.

Now, since $P=x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ and $a_{-j}%
Q=x_{j}Q=x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}%
}...$, we have%
\begin{align*}
\left(  P,a_{-j}Q\right)   &  =\left(  x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}%
}...,x_{1}^{m_{1}^{\prime}}x_{2}^{m_{2}^{\prime}}x_{3}^{m_{3}^{\prime}%
}...\right) \\
&  =\prod\limits_{i=1}^{\infty}\underbrace{\delta_{n_{i},m_{i}^{\prime}}%
}_{=\delta_{n_{i}-m_{i}^{\prime},0}=\delta_{n_{i}^{\prime}-m_{i},0}%
=\delta_{n_{i}^{\prime},m_{i}}}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}}%
\cdot\prod\limits_{i=1}^{\infty}n_{i}!\ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\left(  \cdot,\cdot\right)  \right) \\
&  =\prod\limits_{i=1}^{\infty}\delta_{n_{i}^{\prime},m_{i}}\cdot
\underbrace{\prod\limits_{i=1}^{\infty}i^{n_{i}}}_{=j\prod\limits_{i=1}%
^{\infty}i^{n_{i}^{\prime}}}\cdot\underbrace{\prod\limits_{i=1}^{\infty}%
n_{i}!}_{=n_{j}\prod\limits_{i=1}^{\infty}n_{i}^{\prime}!}=jn_{j}\cdot
\prod\limits_{i=1}^{\infty}\delta_{n_{i}^{\prime},m_{i}}\cdot\prod
\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}\cdot\prod\limits_{i=1}^{\infty}%
n_{i}^{\prime}!.
\end{align*}
Compared with%
\begin{align*}
\left(  a_{j}P,Q\right)   &  =\left(  jn_{j}x_{1}^{n_{1}^{\prime}}x_{2}%
^{n_{2}^{\prime}}x_{3}^{n_{3}^{\prime}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}P=j\underbrace{\dfrac
{\partial}{\partial x_{j}}P}_{=n_{j}x_{1}^{n_{1}^{\prime}}x_{2}^{n_{2}%
^{\prime}}x_{3}^{n_{3}^{\prime}}...}=jn_{j}x_{1}^{n_{1}^{\prime}}x_{2}%
^{n_{2}^{\prime}}x_{3}^{n_{3}^{\prime}}...\text{ and }Q=x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right) \\
&  =jn_{j}\underbrace{\left(  x_{1}^{n_{1}^{\prime}}x_{2}^{n_{2}^{\prime}%
}x_{3}^{n_{3}^{\prime}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...\right)
}_{\substack{=\prod\limits_{i=1}^{\infty}\delta_{n_{i}^{\prime},m_{i}}%
\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}\cdot\prod\limits_{i=1}%
^{\infty}n_{i}^{\prime}!\\\text{(by the definition of }\left(  \cdot
,\cdot\right)  \text{)}}}=jn_{j}\cdot\prod\limits_{i=1}^{\infty}\delta
_{n_{i}^{\prime},m_{i}}\cdot\prod\limits_{i=1}^{\infty}i^{n_{i}^{\prime}}%
\cdot\prod\limits_{i=1}^{\infty}n_{i}^{\prime}!,
\end{align*}
this yields $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $. Thus,
$\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is proven in Case 1. In
other words, we have shown that%
\begin{equation}
\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)
\ \ \ \ \ \ \ \ \ \ \text{for every integer }j\geq1\text{ and any monomials
}P\text{ and }Q\text{.} \label{pf.A.contravariantform.1}%
\end{equation}


In Case 2, proving $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is
trivial (since $a_{0}$ acts on $F_{\mu}$ as $\mu\cdot\operatorname*{id}$).

Now, let us consider Case 3. In this case, $j\leq-1$, so that $-j\geq1$. Thus,
(\ref{pf.A.contravariantform.1}) (applied to $-j$, $Q$ and $P$ instead of $j$,
$P$ and $Q$) yields $\left(  a_{-j}Q,P\right)  =\left(  Q,a_{-\left(
-j\right)  }P\right)  $. Now, since $\left(  \cdot,\cdot\right)  $ is
symmetric, we have $\left(  a_{j}P,Q\right)  =\left(  Q,\underbrace{a_{j}%
}_{=a_{-\left(  -j\right)  }}P\right)  =\left(  Q,a_{-\left(  -j\right)
}P\right)  =\left(  a_{-j}Q,P\right)  =\left(  P,a_{-j}Q\right)  $ (again
since $\left(  \cdot,\cdot\right)  $ is symmetric). Thus, $\left(
a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is proven in Case 3.

We have now proven $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $ is
each of the cases 1, 2 and 3. Since no other cases can occur, this completes
the proof of $\left(  a_{j}P,Q\right)  =\left(  P,a_{-j}Q\right)  $. As we
have explained above, this proves Proposition \ref{prop.A.contravariantform}
\textbf{(c)}.

\textbf{(d)} Let $x\in\mathcal{A}$, $P\in F_{\mu}$ and $Q\in F_{\mu}$. Since
the form $\left(  \cdot,\cdot\right)  $ is symmetric, we have $\left(
P,xQ\right)  =\left(  xQ,P\right)  $ and $\left(  \omega\left(  x\right)
P,Q\right)  =\left(  Q,\omega\left(  x\right)  P\right)  $. Proposition
\ref{prop.A.contravariantform} \textbf{(c)} (applied to $P$ and $Q$ instead of
$Q$ and $P$) yields $\left(  xQ,P\right)  =-\left(  Q,\omega\left(  x\right)
P\right)  $. Thus, $\left(  P,xQ\right)  =\left(  xQ,P\right)
=-\underbrace{\left(  Q,\omega\left(  x\right)  P\right)  }_{=\left(
\omega\left(  x\right)  P,Q\right)  }=-\left(  \omega\left(  x\right)
P,Q\right)  $. This proves Proposition \ref{prop.A.contravariantform}
\textbf{(d)}.

\textbf{(e)} and \textbf{(f)} The proofs of Proposition
\ref{prop.A.contravariantform} \textbf{(e)} and \textbf{(f)} are analogous to
those of Proposition \ref{prop.A.contravariantform} \textbf{(c)} and
\textbf{(d)}, respectively, and thus will be omitted.

\subsection{Representations of the Virasoro algebra $\operatorname*{Vir}$}

We now come to the Virasoro algebra $\operatorname*{Vir}$. First, some notations:

\begin{definition}
\textbf{(a)} The notion "\textit{Virasoro module}" will be a synonym for
"$\operatorname*{Vir}$-module". Similarly, "Virasoro action" means
"$\operatorname*{Vir}$-action".

\textbf{(b)} Let $c\in\mathbb{C}$. A $\operatorname*{Vir}$-module $M$ is said
to have \textit{central charge }$c$ if and only if the element $C$ of
$\operatorname*{Vir}$ acts as $c\cdot\operatorname*{id}$ on $M$.
\end{definition}

Note that not every $\operatorname*{Vir}$-module has a central charge (and the
zero module has infinitely many central charges), but Corollary \ref{cor.dix2}
yields that every irreducible $\operatorname*{Vir}$-module of countable
dimension has a (unique) central charge.

There are lots and lots of Virasoro modules in mathematics, and we will
encounter them as this course progresses; the more complicated among them will
require us to introduce a lot of machinery like Verma modules, semiinfinite
wedges and affine Lie algebras. For now, we define one of the simplest
families of representations of $\operatorname*{Vir}$: the "chargeless"
$\operatorname*{Vir}$-modules $V_{\alpha,\beta}$ parametrized by pairs of
complex numbers $\left(  \alpha,\beta\right)  $.

\begin{proposition}
\label{prop.Vab.1}Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Let
$V_{\alpha,\beta}$ be the vector space of formal expressions of the form
$gz^{\alpha}\left(  dz\right)  ^{\beta}$ with $g\in\mathbb{C}\left[
z,z^{-1}\right]  $ (where $\mathbb{C}\left[  z,z^{-1}\right]  $ is the ring of
Laurent polynomials in the variable $z$). (Formally, this vector space
$V_{\alpha,\beta}$ is defined to be a copy of the $\mathbb{C}$-vector space
$\mathbb{C}\left[  z,z^{-1}\right]  $, but in which the element corresponding
to any $g\in\mathbb{C}\left[  z,z^{-1}\right]  $ is denoted by $gz^{\alpha
}\left(  dz\right)  ^{\beta}$. For a geometric intuition, the elements of
$V_{\alpha,\beta}$ can be seen as "tensor fields" of rank $\beta$ and
branching $\alpha$ on the punctured complex plane $\mathbb{C}^{\times}$.)

\textbf{(a)} The formula
\begin{equation}
f\partial\rightharpoonup\left(  gz^{\alpha}\left(  dz\right)  ^{\beta}\right)
=\left(  fg^{\prime}+\alpha z^{-1}fg+\beta f^{\prime}g\right)  z^{\alpha
}\left(  dz\right)  ^{\beta} \label{ex1.1.1}%
\end{equation}
defines an action of $W$ on $V_{\alpha,\beta}$. Thus, $V_{\alpha,\beta}$
becomes a $\operatorname*{Vir}$-module with $C$ acting as $0$. (In other
words, $V_{\alpha,\beta}$ becomes a $\operatorname*{Vir}$-module with central
charge $0$.)

\textbf{(b)} For every $k\in\mathbb{Z}$, let $v_{k}=z^{-k+\alpha}\left(
dz\right)  ^{\beta}\in V_{\alpha,\beta}$. Here, for any $\ell\in\mathbb{Z}$,
the term $z^{\ell+\alpha}\left(  dz\right)  ^{\beta}$ denotes $z^{\ell
}z^{\alpha}\left(  dz\right)  ^{\beta}$. Then,%
\begin{equation}
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{Z}\text{ and }k\in\mathbb{Z}.
\label{ex1.1.2.var}%
\end{equation}

\end{proposition}

Note that Proposition \ref{prop.Vab.1} was Homework Set 1 exercise 1, but the
notation $v_{k}$ had a slightly different meaning in Homework Set 1 exercise 1
than it has here.

The proof of this proposition consists of straightforward computations. We
give it for the sake of completeness, slightly simplifying the calculation by
introducing auxiliary functions.

\textit{Proof of Proposition \ref{prop.Vab.1}.} \textbf{(a)} In order to prove
Proposition \ref{prop.Vab.1} \textbf{(a)}, we must show that the formula
(\ref{ex1.1.1}) defines an action of $W$ on $V_{\alpha,\beta}$.

It is clear that $\left(  fg^{\prime}+\alpha z^{-1}fg+\beta f^{\prime
}g\right)  z^{\alpha}\left(  dz\right)  ^{\beta}$ depends linearly on each of
$f$ and $g$. Hence, we must only prove that, with the definition
(\ref{ex1.1.1}), we have
\begin{equation}
\left[  f\partial,g\partial\right]  \rightharpoonup\left(  hz^{\alpha}\left(
dz\right)  ^{\beta}\right)  =f\partial\rightharpoonup\left(  g\partial
\rightharpoonup\left(  hz^{\alpha}\left(  dz\right)  ^{\beta}\right)  \right)
-g\partial\rightharpoonup\left(  f\partial\rightharpoonup\left(  hz^{\alpha
}\left(  dz\right)  ^{\beta}\right)  \right)  \label{sol1.1.1}%
\end{equation}
for any Laurent polynomials $f$, $g$ and $h$ in $\mathbb{C}\left[
z,z^{-1}\right]  $.

So let $f$, $g$ and $h$ be any three Laurent polynomials in $\mathbb{C}\left[
z,z^{-1}\right]  $. Denote by $p$ the Laurent polynomial $h^{\prime}+\alpha
z^{-1}h$. Denote by $q$ the Laurent polynomial $fg^{\prime}-gf^{\prime}$.
Then,\footnote{In the following computations, terms like $f\left(  u\right)  $
(where $u$ is a subterm, usually a complicated one) have to be understood as
$f\cdot u$ (the product of $f$ with $u$) and not as $f\left(  u\right)  $ (the
Laurent polynomial $f$ applied to $u$).}%
\begin{equation}
f\left(  g^{\prime}h\right)  ^{\prime}-g\left(  f^{\prime}h\right)  ^{\prime
}=q^{\prime}h+qh^{\prime} \label{sol1.1.fgh}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol1.1.fgh}):} Since $q=fg^{\prime
}-gf^{\prime}$, we have $qh=\left(  fg^{\prime}-gf^{\prime}\right)
h=fg^{\prime}h-gf^{\prime}h=f\left(  g^{\prime}h\right)  -g\left(  f^{\prime
}h\right)  $, so that%
\begin{align*}
\left(  qh\right)  ^{\prime}  &  =\left(  f\left(  g^{\prime}h\right)
-g\left(  f^{\prime}h\right)  \right)  ^{\prime}=\underbrace{\left(  f\left(
g^{\prime}h\right)  \right)  ^{\prime}}_{\substack{=f^{\prime}\left(
g^{\prime}h\right)  +f\left(  g^{\prime}h\right)  ^{\prime}\\\text{(by the
Leibniz rule)}}}-\underbrace{\left(  g\left(  f^{\prime}h\right)  \right)
^{\prime}}_{\substack{=g^{\prime}\left(  f^{\prime}h\right)  +g\left(
f^{\prime}h\right)  ^{\prime}\\\text{(by the Leibniz rule)}}}\\
&  =\underbrace{f^{\prime}\left(  g^{\prime}h\right)  }_{=f^{\prime}g^{\prime
}h}+f\left(  g^{\prime}h\right)  ^{\prime}-\underbrace{g^{\prime}\left(
f^{\prime}h\right)  }_{=f^{\prime}g^{\prime}h}+g\left(  f^{\prime}h\right)
^{\prime}\\
&  =f^{\prime}g^{\prime}h+f\left(  g^{\prime}h\right)  ^{\prime}-f^{\prime
}g^{\prime}h+g\left(  f^{\prime}h\right)  ^{\prime}=f\left(  g^{\prime
}h\right)  ^{\prime}-g\left(  f^{\prime}h\right)  ^{\prime}.
\end{align*}
Since $\left(  qh\right)  ^{\prime}=q^{\prime}h+qh^{\prime}$ (by the Leibniz
rule), this rewrites as $q^{\prime}h+qh^{\prime}=f\left(  g^{\prime}h\right)
^{\prime}-g\left(  f^{\prime}h\right)  ^{\prime}$. This proves
(\ref{sol1.1.fgh}).} and%
\begin{align}
&  f\underbrace{\left(  gp\right)  ^{\prime}}_{\substack{=g^{\prime
}p+gp^{\prime}\\\text{(by the Leibniz rule)}}}-g\underbrace{\left(  fp\right)
^{\prime}}_{\substack{=f^{\prime}p+fp^{\prime}\\\text{(by the Leibniz rule)}%
}}\nonumber\\
&  =\underbrace{f\left(  g^{\prime}p+gp^{\prime}\right)  }_{=fg^{\prime
}p+fgp^{\prime}}-\underbrace{g\left(  f^{\prime}p+fp^{\prime}\right)
}_{=gf^{\prime}p+gfp^{\prime}=gf^{\prime}p+fgp^{\prime}}\nonumber\\
&  =fg^{\prime}p+fgp^{\prime}-gf^{\prime}p-fgp^{\prime}=fg^{\prime
}p-gf^{\prime}p=\underbrace{\left(  fg^{\prime}-gf^{\prime}\right)  }%
_{=q}p=qp. \label{sol1.1.fgh2}%
\end{align}


Also,%
\begin{align*}
\underbrace{\left[  f\partial,g\partial\right]  }_{=\left(  fg^{\prime
}-gf^{\prime}\right)  \partial}\rightharpoonup\left(  hz^{\alpha}\left(
dz\right)  ^{\beta}\right)   &  =\underbrace{\left(  fg^{\prime}-gf^{\prime
}\right)  }_{=q}\partial\rightharpoonup\left(  hz^{\alpha}\left(  dz\right)
^{\beta}\right)  =q\partial\rightharpoonup\left(  hz^{\alpha}\left(
dz\right)  ^{\beta}\right) \\
&  =\left(  \underbrace{qh^{\prime}+\alpha z^{-1}qh}_{\substack{=q\left(
h^{\prime}+\alpha z^{-1}h\right)  =qp\\\text{(since }h^{\prime}+\alpha
z^{-1}h=p\text{)}}}+\beta q^{\prime}h\right)  z^{\alpha}\left(  dz\right)
^{\beta}=\left(  qp+\beta q^{\prime}h\right)  z^{\alpha}\left(  dz\right)
^{\beta}.
\end{align*}
Moreover, $gh^{\prime}+\alpha z^{-1}gh=g\underbrace{\left(  h^{\prime}+\alpha
z^{-1}h\right)  }_{=p}=gp$, and
\[
g\partial\rightharpoonup\left(  hz^{\alpha}\left(  dz\right)  ^{\beta}\right)
=\left(  \underbrace{gh^{\prime}+\alpha z^{-1}gh}_{=gp}+\beta g^{\prime
}h\right)  z^{\alpha}\left(  dz\right)  ^{\beta}=\left(  gp+\beta g^{\prime
}h\right)  z^{\alpha}\left(  dz\right)  ^{\beta},
\]
so that%
\begin{align}
&  f\partial\rightharpoonup\underbrace{\left(  g\partial\rightharpoonup\left(
hz^{\alpha}\left(  dz\right)  ^{\beta}\right)  \right)  }_{=\left(  gp+\beta
g^{\prime}h\right)  z^{\alpha}\left(  dz\right)  ^{\beta}}\nonumber\\
&  =f\partial\rightharpoonup\left(  \left(  gp+\beta g^{\prime}h\right)
z^{\alpha}\left(  dz\right)  ^{\beta}\right) \nonumber\\
&  =\left(  f\underbrace{\left(  gp+\beta g^{\prime}h\right)  ^{\prime}%
}_{=\left(  gp\right)  ^{\prime}+\beta\left(  g^{\prime}h\right)  ^{\prime}%
}+\underbrace{\alpha z^{-1}f\left(  gp+\beta g^{\prime}h\right)  }_{=\alpha
z^{-1}fgp+\alpha\beta z^{-1}fg^{\prime}h}+\underbrace{\beta f^{\prime}\left(
gp+\beta g^{\prime}h\right)  }_{=\beta f^{\prime}gp+\beta^{2}f^{\prime
}g^{\prime}h}\right)  z^{\alpha}\left(  dz\right)  ^{\beta}\nonumber\\
&  =\left(  \underbrace{f\left(  \left(  gp\right)  ^{\prime}+\beta\left(
g^{\prime}h\right)  ^{\prime}\right)  }_{=f\left(  gp\right)  ^{\prime}+\beta
f\left(  g^{\prime}h\right)  ^{\prime}}+\alpha z^{-1}fgp+\alpha\beta
z^{-1}fg^{\prime}h+\beta f^{\prime}gp+\beta^{2}f^{\prime}g^{\prime}h\right)
z^{\alpha}\left(  dz\right)  ^{\beta}\nonumber\\
&  =\left(  f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime}h\right)
^{\prime}+\alpha z^{-1}fgp+\alpha\beta z^{-1}fg^{\prime}h+\beta f^{\prime
}gp+\beta^{2}f^{\prime}g^{\prime}h\right)  z^{\alpha}\left(  dz\right)
^{\beta}. \label{sol1.1.2.var}%
\end{align}


Since the roles of $f$ and $g$ in our situation are symmetric, we can
interchange $f$ and $g$ in (\ref{sol1.1.2.var}), and obtain%
\begin{align}
&  g\partial\rightharpoonup\left(  f\partial\rightharpoonup\left(  hz^{\alpha
}\left(  dz\right)  ^{\beta}\right)  \right) \nonumber\\
&  =\left(  g\left(  fp\right)  ^{\prime}+\beta g\left(  f^{\prime}h\right)
^{\prime}+\alpha z^{-1}gfp+\alpha\beta z^{-1}gf^{\prime}h+\beta g^{\prime
}fp+\beta^{2}g^{\prime}f^{\prime}h\right)  z^{\alpha}\left(  dz\right)
^{\beta}\nonumber\\
&  =\left(  g\left(  fp\right)  ^{\prime}+\beta g\left(  f^{\prime}h\right)
^{\prime}+\alpha z^{-1}fgp+\alpha\beta z^{-1}gf^{\prime}h+\beta g^{\prime
}fp+\beta^{2}f^{\prime}g^{\prime}h\right)  z^{\alpha}\left(  dz\right)
^{\beta}. \label{sol1.1.3}%
\end{align}


Thus,%
\begin{align*}
&  \underbrace{f\partial\rightharpoonup\left(  g\partial\rightharpoonup\left(
hz^{\alpha}\left(  dz\right)  ^{\beta}\right)  \right)  }_{\substack{=\left(
f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime}h\right)  ^{\prime
}+\alpha z^{-1}fgp+\alpha\beta z^{-1}fg^{\prime}h+\beta f^{\prime}gp+\beta
^{2}f^{\prime}g^{\prime}h\right)  z^{\alpha}\left(  dz\right)  ^{\beta
}\\\text{(by (\ref{sol1.1.2.var}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\underbrace{g\partial
\rightharpoonup\left(  f\partial\rightharpoonup\left(  hz^{\alpha}\left(
dz\right)  ^{\beta}\right)  \right)  }_{\substack{=\left(  g\left(  fp\right)
^{\prime}+\beta g\left(  f^{\prime}h\right)  ^{\prime}+\alpha z^{-1}%
fgp+\alpha\beta z^{-1}gf^{\prime}h+\beta g^{\prime}fp+\beta^{2}f^{\prime
}g^{\prime}h\right)  z^{\alpha}\left(  dz\right)  ^{\beta}\\\text{(by
(\ref{sol1.1.3}))}}}\\
&  =\left(  f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime}h\right)
^{\prime}+\alpha z^{-1}fgp+\alpha\beta z^{-1}fg^{\prime}h+\beta f^{\prime
}gp+\beta^{2}f^{\prime}g^{\prime}h\right)  z^{\alpha}\left(  dz\right)
^{\beta}\\
&  \ \ \ \ \ \ \ \ \ \ -\left(  g\left(  fp\right)  ^{\prime}+\beta g\left(
f^{\prime}h\right)  ^{\prime}+\alpha z^{-1}fgp+\alpha\beta z^{-1}gf^{\prime
}h+\beta g^{\prime}fp+\beta^{2}f^{\prime}g^{\prime}h\right)  z^{\alpha}\left(
dz\right)  ^{\beta}\\
&  =\left(  \left(  f\left(  gp\right)  ^{\prime}+\beta f\left(  g^{\prime
}h\right)  ^{\prime}+\alpha z^{-1}fgp+\alpha\beta z^{-1}fg^{\prime}h+\beta
f^{\prime}gp+\beta^{2}f^{\prime}g^{\prime}h\right)  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  -\left(  g\left(
fp\right)  ^{\prime}+\beta g\left(  f^{\prime}h\right)  ^{\prime}+\alpha
z^{-1}fgp+\alpha\beta z^{-1}gf^{\prime}h+\beta g^{\prime}fp+\beta^{2}%
f^{\prime}g^{\prime}h\right)  \right)  z^{\alpha}\left(  dz\right)  ^{\beta}\\
&  =\left(  \underbrace{f\left(  gp\right)  ^{\prime}-g\left(  fp\right)
^{\prime}}_{\substack{=qp\\\text{(by (\ref{sol1.1.fgh2}))}}}+\underbrace{\beta
f\left(  g^{\prime}h\right)  ^{\prime}-\beta g\left(  f^{\prime}h\right)
^{\prime}}_{=\beta\left(  f\left(  g^{\prime}h\right)  ^{\prime}-g\left(
f^{\prime}h\right)  ^{\prime}\right)  }+\underbrace{\alpha\beta z^{-1}%
fg^{\prime}h-\alpha\beta z^{-1}gf^{\prime}h}_{=\alpha\beta z^{-1}\left(
fg^{\prime}-gf^{\prime}\right)  h}+\underbrace{\beta f^{\prime}gp-\beta
g^{\prime}fp}_{=\beta\left(  f^{\prime}g-g^{\prime}f\right)  p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ z^{\alpha}\left(  dz\right)  ^{\beta}\\
&  =\left(  qp+\beta\underbrace{\left(  f\left(  g^{\prime}h\right)  ^{\prime
}-g\left(  f^{\prime}h\right)  ^{\prime}\right)  }_{\substack{=q^{\prime
}h+qh^{\prime}\\\text{(by (\ref{sol1.1.fgh}))}}}+\alpha\beta z^{-1}%
\underbrace{\left(  fg^{\prime}-gf^{\prime}\right)  }_{=q}h+\beta
\underbrace{\left(  f^{\prime}g-g^{\prime}f\right)  }%
_{\substack{=-q\\\text{(since }q=fg^{\prime}-gf^{\prime}=g^{\prime}%
f-f^{\prime}g\text{)}}}p\right)  z^{\alpha}\left(  dz\right)  ^{\beta}\\
&  =\left(  qp+\underbrace{\beta\left(  q^{\prime}h+qh^{\prime}\right)
}_{=\beta q^{\prime}h+\beta qh^{\prime}}+\alpha\beta z^{-1}%
qh+\underbrace{\beta\left(  -q\right)  p}_{=-\beta qp}\right)  z^{\alpha
}\left(  dz\right)  ^{\beta}\\
&  =\left(  qp+\beta q^{\prime}h+\underbrace{\beta qh^{\prime}+\alpha\beta
z^{-1}qh}_{=\beta q\left(  h^{\prime}+\alpha z^{-1}h\right)  }-\beta
qp\right)  z^{\alpha}\left(  dz\right)  ^{\beta}\\
&  =\left(  qp+\beta q^{\prime}h+\beta q\underbrace{\left(  h^{\prime}+\alpha
z^{-1}h\right)  }_{=p}-\beta qp\right)  z^{\alpha}\left(  dz\right)  ^{\beta
}\\
&  =\left(  qp+\beta q^{\prime}h+\underbrace{\beta qp-\beta qp}_{=0}\right)
z^{\alpha}\left(  dz\right)  ^{\beta}=\left(  qp+\beta q^{\prime}h\right)
z^{\alpha}\left(  dz\right)  ^{\beta}=\left[  f\partial,g\partial\right]
\rightharpoonup\left(  hz^{\alpha}\left(  dz\right)  ^{\beta}\right)  .
\end{align*}


Thus, (\ref{sol1.1.1}) is proven for any Laurent polynomials $f$, $g$ and $h$.
This proves that the formula (\ref{ex1.1.1}) defines an action of $W$ on
$V_{\alpha,\beta}$. Hence, $V_{\alpha,\beta}$ becomes a $W$-module, i. e., a
$\operatorname*{Vir}$-module with $C$ acting as $0$. (In other words,
$V_{\alpha,\beta}$ becomes a $\operatorname*{Vir}$-module with central charge
$0$.) This proves Proposition \ref{prop.Vab.1} \textbf{(a)}.

\textbf{(b)} We only need to prove (\ref{ex1.1.2.var}).

Let $m\in\mathbb{Z}$ and $k\in\mathbb{Z}$. Then, $v_{k}=z^{-k+\alpha}\left(
dz\right)  ^{\beta}=z^{-k}z^{\alpha}\left(  dz\right)  ^{\beta}$ and
$v_{k-m}=z^{-\left(  k-m\right)  +\alpha}\left(  dz\right)  ^{\beta}%
=z^{m-k}z^{\alpha}\left(  dz\right)  ^{\beta}$. Thus,%
\begin{align*}
&  \underbrace{L_{m}}_{=-z^{m+1}\partial}\rightharpoonup\underbrace{v_{k}%
}_{=z^{-k}z^{\alpha}\left(  dz\right)  ^{\beta}}\\
&  =\left(  -z^{m+1}\partial\right)  \rightharpoonup\left(  z^{-k}z^{\alpha
}\left(  dz\right)  ^{\beta}\right) \\
&  =\left(  -z^{m+1}\underbrace{\left(  z^{-k}\right)  ^{\prime}}%
_{=-kz^{-k-1}}+\underbrace{\alpha z^{-1}\left(  -z^{m+1}\right)  z^{-k}%
}_{=-\alpha z^{-1}z^{m+1}z^{-k}}+\beta\underbrace{\left(  -z^{m+1}\right)
^{\prime}}_{=-\left(  m+1\right)  z^{m}}z^{-k}\right)  z^{\alpha}\left(
dz\right)  ^{\beta}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{ex1.1.1}), applied to
}f=-z^{m+1}\text{ and }g=z^{-k}\right) \\
&  =\left(  -\left(  -k\right)  \underbrace{z^{m+1}z^{-k-1}}_{=z^{m-k}}%
-\alpha\underbrace{z^{-1}z^{m+1}z^{-k}}_{=z^{\left(  -1\right)  +\left(
m+1\right)  +\left(  -k\right)  }=z^{m-k}}+\beta\left(  -\left(  m+1\right)
\right)  \underbrace{z^{m}z^{-k}}_{=z^{m-k}}\right)  z^{\alpha}\left(
dz\right)  ^{\beta}\\
&  =\left(  kz^{m-k}-\alpha z^{m-k}+\beta\left(  -\left(  m+1\right)  \right)
z^{m-k}\right)  z^{\alpha}\left(  dz\right)  ^{\beta}\\
&  =\underbrace{\left(  k-\alpha+\beta\left(  -\left(  m+1\right)  \right)
\right)  }_{=k-\alpha-\left(  m+1\right)  \beta}\underbrace{z^{m-k}z^{\alpha
}\left(  dz\right)  ^{\beta}}_{=v_{k-m}}=\left(  k-\alpha-\left(  m+1\right)
\beta\right)  v_{k-m}.
\end{align*}
This proves (\ref{ex1.1.2.var}). Proposition \ref{prop.Vab.1} \textbf{(b)} is proven.

\begin{remark}
\label{rmk.Vab.adj}Consider the $\operatorname*{Vir}$-module
$\operatorname*{Vir}$ (with the adjoint action). Since $\left\langle
C\right\rangle $ is a submodule of $\operatorname*{Vir}$, we obtain a
$\operatorname*{Vir}$-module $\operatorname*{Vir}\diagup\left\langle
C\right\rangle $. This $\operatorname*{Vir}$-module is isomorphic to
$V_{1,-1}$ (and thus to $V_{\alpha,-1}$ for every $\alpha\in\mathbb{Z}$). More
precisely, the $\mathbb{C}$-linear map%
\begin{align*}
\operatorname*{Vir}\diagup\left\langle C\right\rangle  &  \rightarrow
V_{1,-1},\\
\overline{L_{n}}  &  \mapsto v_{-n}%
\end{align*}
is a $\operatorname*{Vir}$-module isomorphism.
\end{remark}

The representations $V_{\alpha,\beta}$ are not all pairwise non-isomorphic,
but there are still uncountably many non-isomorphic ones among them. More precisely:

\begin{proposition}
\label{prop.Vab.iso}\textbf{(a)} For every $\ell\in\mathbb{Z}$, $\alpha
\in\mathbb{C}$ and $\beta\in\mathbb{C}$, the $\mathbb{C}$-linear map%
\begin{align*}
V_{\alpha,\beta}  &  \rightarrow V_{\alpha+\ell,\beta},\\
gz^{\alpha}\left(  dz\right)  ^{\beta}  &  \mapsto\left(  gz^{-\ell}\right)
z^{\alpha+\ell}\left(  dz\right)  ^{\beta}%
\end{align*}
is an isomorphism of $\operatorname*{Vir}$-modules. (This map sends $v_{k}$ to
$v_{k+\ell}$ for every $k\in\mathbb{Z}$.)

\textbf{(b)} For every $\alpha\in\mathbb{C}$, the $\mathbb{C}$-linear map%
\begin{align*}
V_{\alpha,0}  &  \rightarrow V_{\alpha-1,1},\\
gz^{\alpha}\left(  dz\right)  ^{0}  &  \mapsto\left(  -g^{\prime}z-\alpha
g\right)  z^{\alpha-1}\left(  dz\right)  ^{1}%
\end{align*}
is a homomorphism of $\operatorname*{Vir}$-modules. (This map sends $v_{k}$ to
$\left(  k-\alpha\right)  v_{k}$ for every $k\in\mathbb{Z}$.) If $\alpha
\notin\mathbb{Z}$, then this map is an isomorphism.

\textbf{(c)} Let $\left(  \alpha,\beta,\alpha^{\prime},\beta^{\prime}\right)
\in\mathbb{C}^{4}$. Then, $V_{\alpha,\beta}\cong V_{\alpha^{\prime}%
,\beta^{\prime}}$ as $\operatorname*{Vir}$-modules if and only if either
$\left(  \beta=\beta^{\prime}\text{ and }\alpha-\alpha^{\prime}\in
\mathbb{Z}\right)  $ or $\left(  \beta=0\text{, }\beta^{\prime}=1\text{,
}\alpha-\alpha^{\prime}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)
$ or $\left(  \beta=1\text{, }\beta^{\prime}=0\text{, }\alpha-\alpha^{\prime
}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)  $.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Vab.iso} (sketched).} \textbf{(a)} and
\textbf{(b)} Very easy and left to the reader.

\textbf{(c)} The $\Longleftarrow$ direction is handled by parts \textbf{(a)}
and \textbf{(b)}.

$\Longrightarrow:$ Assume that $V_{\alpha,\beta}\cong V_{\alpha^{\prime}%
,\beta^{\prime}}$ as $\operatorname*{Vir}$-modules. We must prove that either
$\left(  \beta=\beta^{\prime}\text{ and }\alpha-\alpha^{\prime}\in
\mathbb{Z}\right)  $ or $\left(  \beta=0\text{, }\beta^{\prime}=1\text{,
}\alpha-\alpha^{\prime}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)
$ or $\left(  \beta=1\text{, }\beta^{\prime}=0\text{, }\alpha-\alpha^{\prime
}\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)  $.

Let $\Phi$ be the $\operatorname*{Vir}$-module isomorphism $V_{\alpha,\beta
}\rightarrow V_{\alpha^{\prime},\beta^{\prime}}$.

Applying (\ref{ex1.1.2.var}) to $m=0$, we obtain%
\begin{equation}
L_{0}v_{k}=\left(  k-\alpha-\beta\right)  v_{k}\text{ in }V_{\alpha,\beta
}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}. \label{pf.Vab.iso.1}%
\end{equation}
Hence, $L_{0}$ acts on $V_{\alpha,\beta}$ as a diagonal matrix with
eigenvalues $k-\alpha-\beta$ for all $k\in\mathbb{Z}$, each eigenvalue
appearing exactly once. Similarly, applying (\ref{ex1.1.2.var}) to $0$ and
$\left(  \alpha^{\prime},\beta^{\prime}\right)  $ instead of $m$ and $\left(
\alpha,\beta\right)  $, we obtain%
\begin{equation}
L_{0}v_{k}=\left(  k-\alpha^{\prime}-\beta^{\prime}\right)  v_{k}\text{ in
}V_{\alpha^{\prime},\beta^{\prime}}\ \ \ \ \ \ \ \ \ \ \text{for every }%
k\in\mathbb{Z}. \label{pf.Vab.iso.2}%
\end{equation}
Thus, $L_{0}$ acts on $V_{\alpha^{\prime},\beta^{\prime}}$ as a diagonal
matrix with eigenvalues $k-\alpha^{\prime}-\beta^{\prime}$ for all
$k\in\mathbb{Z}$, each eigenvalue appearing exactly once.

But since $V_{\alpha,\beta}\cong V_{\alpha^{\prime},\beta^{\prime}}$ as
$\operatorname*{Vir}$-modules, the eigenvalues of $L_{0}$ acting on
$V_{\alpha,\beta}$ must be the same as the eigenvalues of $L_{0}$ acting on
$V_{\alpha^{\prime},\beta^{\prime}}$. In other words,%
\[
\left\{  k-\alpha-\beta\ \mid\ k\in\mathbb{Z}\right\}  =\left\{
k-\alpha^{\prime}-\beta^{\prime}\ \mid\ k\in\mathbb{Z}\right\}
\]
(because we know that the eigenvalues of $L_{0}$ acting on $V_{\alpha,\beta}$
are $k-\alpha-\beta$ for all $k\in\mathbb{Z}$, while the eigenvalues of
$L_{0}$ acting on $V_{\alpha^{\prime},\beta^{\prime}}$ are $k-\alpha^{\prime
}-\beta^{\prime}$ for all $k\in\mathbb{Z}$). Hence, $\left(  \alpha
+\beta\right)  -\left(  \alpha^{\prime}+\beta^{\prime}\right)  \in\mathbb{Z}$.
Since we can shift $\alpha$ by an arbitrary integer without changing the
isomorphism class of $V_{\alpha,\beta}$ (due to part \textbf{(a)}), we can
thus WLOG assume that $\alpha+\beta=\alpha^{\prime}+\beta^{\prime}$.

Let us once again look at the equality (\ref{pf.Vab.iso.1}). This equality
tells us that, for each $k\in\mathbb{Z}$, the vector $v_{k}$ is the unique (up
to scaling) eigenvector of the operator $L_{0}$ with eigenvalue $k-\alpha
-\beta$ in $V_{\alpha,\beta}$. The isomorphism $\Phi$ (being
$\operatorname*{Vir}$-linear) must map this vector $v_{k}$ to an eigenvector
of the operator $L_{0}$ with eigenvalue $k-\alpha-\beta$ in $V_{\alpha
^{\prime},\beta^{\prime}}$. Since $\alpha+\beta=\alpha^{\prime}+\beta^{\prime
}$, this eigenvalue equals $k-\alpha^{\prime}-\beta^{\prime}$. But (due to
(\ref{pf.Vab.iso.2})) the unique (up to scaling) eigenvector of the operator
$L_{0}$ with eigenvalue $k-\alpha^{\prime}-\beta^{\prime}$ in $V_{\alpha
^{\prime},\beta^{\prime}}$ is $v_{k}$. Hence, $\Phi\left(  v_{k}\right)  $
must equal $v_{k}$ up to scaling, i. e., there exists a nonzero complex number
$\lambda_{k}$ such that $\Phi\left(  v_{k}\right)  =\lambda_{k}v_{k}$.

Now, let $m\in\mathbb{Z}$ and $k\in\mathbb{Z}$. Then, in $V_{\alpha,\beta}$,
we have%
\[
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m},
\]
so that%
\begin{align*}
\Phi\left(  L_{m}v_{k}\right)   &  =\Phi\left(  \left(  k-\alpha-\beta\left(
m+1\right)  \right)  v_{k-m}\right)  =\left(  k-\alpha-\beta\left(
m+1\right)  \right)  \underbrace{\Phi\left(  v_{k-m}\right)  }_{=\lambda
_{k-m}v_{k-m}}\\
&  =\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\end{align*}
in $V_{\alpha^{\prime},\beta^{\prime}}$. Compared with%
\begin{align*}
\Phi\left(  L_{m}v_{k}\right)   &  =L_{m}\underbrace{\Phi\left(  v_{k}\right)
}_{=\lambda_{k}v_{k}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Phi\text{ is
}\operatorname*{Vir}\text{-linear}\right) \\
&  =\lambda_{k}\underbrace{L_{m}v_{k}}_{=\left(  k-\alpha^{\prime}%
-\beta^{\prime}\left(  m+1\right)  \right)  v_{k-m}}=\lambda_{k}\left(
k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)  v_{k-m}%
\end{align*}
in $V_{\alpha^{\prime},\beta^{\prime}}$, this yields%
\[
\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
=\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)
\right)  v_{k-m}.
\]
Since $v_{k-m}\neq0$, this yields%
\begin{equation}
\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)  \right)  =\lambda
_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)  .
\label{pf.Vab.iso.5a}%
\end{equation}


Now, any $m\in\mathbb{Z}$, $k\in\mathbb{Z}$ and $n\in\mathbb{Z}$ satisfy%
\begin{equation}
\lambda_{k-\left(  n+m\right)  }\left(  k-\alpha-\beta\left(  m+1\right)
\right)  =\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
n+m+1\right)  \right)  \label{pf.Vab.iso.5b}%
\end{equation}
(by (\ref{pf.Vab.iso.5a}), applied to $n+m$ instead of $m$) and%
\begin{equation}
\lambda_{k-m-n}\left(  k-m-\alpha-\beta\left(  n+1\right)  \right)
=\lambda_{k-m}\left(  k-m-\alpha^{\prime}-\beta^{\prime}\left(  n+1\right)
\right)  \label{pf.Vab.iso.5c}%
\end{equation}
(by (\ref{pf.Vab.iso.5a}), applied to $k-m$ and $n$ instead of $k$ and $m$).
Hence, any $m\in\mathbb{Z}$, $k\in\mathbb{Z}$ and $n\in\mathbb{Z}$ satisfy
\begin{align*}
&  \lambda_{k}\lambda_{k-m}\lambda_{k-m-n}\cdot\left(  k-\alpha^{\prime}%
-\beta^{\prime}\left(  n+m+1\right)  \right)  \cdot\left(  k-\alpha
-\beta\left(  m+1\right)  \right)  \cdot\left(  k-m-\alpha-\beta\left(
n+1\right)  \right) \\
&  =\underbrace{\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
n+m+1\right)  \right)  }_{\substack{=\lambda_{k-\left(  n+m\right)  }\left(
k-\alpha-\beta\left(  m+1\right)  \right)  \\\text{(by (\ref{pf.Vab.iso.5b}%
))}}}\cdot\underbrace{\lambda_{k-m}\left(  k-\alpha-\beta\left(  m+1\right)
\right)  }_{\substack{=\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime
}\left(  m+1\right)  \right)  \\\text{(by (\ref{pf.Vab.iso.5a}))}}%
}\cdot\underbrace{\lambda_{k-m-n}\left(  k-m-\alpha-\beta\left(  n+1\right)
\right)  }_{\substack{=\lambda_{k-m}\left(  k-m-\alpha^{\prime}-\beta^{\prime
}\left(  n+1\right)  \right)  \\\text{(by (\ref{pf.Vab.iso.5c}))}}}\\
&  =\lambda_{k-\left(  n+m\right)  }\left(  k-\alpha-\beta\left(  m+1\right)
\right)  \cdot\lambda_{k}\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
m+1\right)  \right)  \cdot\lambda_{k-m}\left(  k-m-\alpha^{\prime}%
-\beta^{\prime}\left(  n+1\right)  \right) \\
&  =\lambda_{k}\lambda_{k-m}\underbrace{\lambda_{k-\left(  n+m\right)  }%
}_{=\lambda_{k-m-n}}\cdot\left(  k-\alpha-\beta\left(  n+m+1\right)  \right)
\cdot\left(  k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)
\cdot\left(  k-m-\alpha^{\prime}-\beta^{\prime}\left(  n+1\right)  \right) \\
&  =\lambda_{k}\lambda_{k-m}\lambda_{k-m-n}\cdot\left(  k-\alpha-\beta\left(
n+m+1\right)  \right)  \cdot\left(  k-\alpha^{\prime}-\beta^{\prime}\left(
m+1\right)  \right)  \cdot\left(  k-m-\alpha^{\prime}-\beta^{\prime}\left(
n+1\right)  \right)  .
\end{align*}
We can divide this equality by $\lambda_{k}\lambda_{k-m}\lambda_{k-m-n}$
(since $\lambda_{i}\neq0$ for every $i\in\mathbb{Z}$, and therefore we have
$\lambda_{k}\lambda_{k-m}\lambda_{k-m-n}\neq0$), and thus obtain that any
$m\in\mathbb{Z}$, $k\in\mathbb{Z}$ and $n\in\mathbb{Z}$ satisfy
\begin{align*}
&  \left(  k-\alpha^{\prime}-\beta^{\prime}\left(  n+m+1\right)  \right)
\cdot\left(  k-\alpha-\beta\left(  m+1\right)  \right)  \cdot\left(
k-m-\alpha-\beta\left(  n+1\right)  \right) \\
&  =\left(  k-\alpha-\beta\left(  n+m+1\right)  \right)  \cdot\left(
k-\alpha^{\prime}-\beta^{\prime}\left(  m+1\right)  \right)  \cdot\left(
k-m-\alpha^{\prime}-\beta^{\prime}\left(  n+1\right)  \right)  .
\end{align*}
Since $\mathbb{Z}^{3}$ is Zariski-dense in $\mathbb{Z}^{3}$, this yields that
\begin{align*}
&  \left(  X-\alpha^{\prime}-\beta^{\prime}\left(  Y+Z+1\right)  \right)
\cdot\left(  X-\alpha-\beta\left(  Z+1\right)  \right)  \cdot\left(
X-Z-\alpha-\beta\left(  Y+1\right)  \right) \\
&  =\left(  X-\alpha-\beta\left(  Y+Z+1\right)  \right)  \cdot\left(
X-\alpha^{\prime}-\beta^{\prime}\left(  Z+1\right)  \right)  \cdot\left(
X-Z-\alpha^{\prime}-\beta^{\prime}\left(  Y+1\right)  \right)  .
\end{align*}
holds as a polynomial identity in the polynomial ring $\mathbb{C}\left[
X,Y,Z\right]  $.

If we compare coefficients before $XYZ$ in this polynomial identity, we get an
equation which easily simplifies to $\left(  \beta-\beta^{\prime}\right)
\left(  \beta+\beta^{\prime}-1\right)  =0$. If we compare coefficients before
$YZ^{2}$ in the same identity, we similarly obtain $\beta\beta^{\prime}\left(
\beta-\beta^{\prime}\right)  =0$.

If $\beta=\beta^{\prime}$, then $\alpha=\alpha^{\prime}$ (since $\alpha
+\beta=\alpha^{\prime}+\beta^{\prime}$), and thus we are done. Hence, let us
assume that $\beta\neq\beta^{\prime}$ for the rest of this proof. Then,
$\left(  \beta-\beta^{\prime}\right)  \left(  \beta+\beta^{\prime}-1\right)
=0$ simplifies to $\beta+\beta^{\prime}-1=0$, and $\beta\beta^{\prime}\left(
\beta-\beta^{\prime}\right)  =0$ simplifies to $\beta\beta^{\prime}=0$.
Combining these two equations, we see that either $\left(  \beta=0\text{ and
}\beta^{\prime}=1\right)  $ or $\left(  \beta=1\text{ and }\beta^{\prime
}=0\right)  $. Assume WLOG that $\left(  \beta=0\text{ and }\beta^{\prime
}=1\right)  $ (otherwise, just switch $\left(  \alpha,\beta\right)  $ with
$\left(  \alpha^{\prime},\beta^{\prime}\right)  $). From $\alpha+\beta
=\alpha^{\prime}+\beta^{\prime}$, we obtain $\alpha-\alpha^{\prime
}=\underbrace{\beta^{\prime}}_{=1}-\underbrace{\beta}_{=0}=1\in\mathbb{Z}$. If
we are able to prove that $\alpha\notin\mathbb{Z}$, then we can conclude that
$\left(  \beta=0\text{, }\beta^{\prime}=1\text{, }\alpha-\alpha^{\prime}%
\in\mathbb{Z}\text{ and }\alpha\notin\mathbb{Z}\right)  $, and thus we are
done. So let us show that $\alpha\notin\mathbb{Z}$.

In fact, assume the opposite. Then, $\alpha\in\mathbb{Z}$, so that $v_{\alpha
}$ is well-defined in $V_{\alpha,\beta}$ and in $V_{\alpha^{\prime}%
,\beta^{\prime}}$. Then, (\ref{ex1.1.2.var}) yields that every $m\in
\mathbb{Z}$ satisfies
\[
L_{m}v_{\alpha}=\left(  \underbrace{\alpha-\alpha}_{=0}-\underbrace{\beta
}_{=0}\left(  m+1\right)  \right)  v_{\alpha-m}=0\text{ in }V_{\alpha,\beta}.
\]
Thus, every $m\in\mathbb{Z}$ satisfies $\Phi\left(  L_{m}v_{\alpha}\right)
=\Phi\left(  0\right)  =0$, so that $0=\Phi\left(  L_{m}v_{\alpha}\right)
=L_{m}\underbrace{\Phi\left(  v_{\alpha}\right)  }_{=\lambda_{\alpha}%
v_{\alpha}}=\lambda_{\alpha}L_{m}v_{\alpha}$ in $V_{\alpha^{\prime}%
,\beta^{\prime}}$, and thus $0=L_{m}v_{\alpha}$ in $V_{\alpha^{\prime}%
,\beta^{\prime}}$ (since $\lambda_{\alpha}\neq0$). But since
(\ref{ex1.1.2.var}) yields%
\[
L_{m}v_{\alpha}=\left(  \alpha-\alpha^{\prime}-\underbrace{\beta^{\prime}%
}_{=1}\left(  m+1\right)  \right)  \underbrace{v_{\alpha-\alpha}}_{=v_{0}%
}=\left(  \alpha-\alpha^{\prime}-\left(  m+1\right)  \right)  v_{0}\text{ in
}V_{\alpha^{\prime},\beta^{\prime}},
\]
this rewrites as $0=\left(  \alpha-\alpha^{\prime}-\left(  m+1\right)
\right)  v_{0}$, so that $0=\alpha-\alpha^{\prime}-\left(  m+1\right)  $. But
this cannot hold for every $m\in\mathbb{Z}$. This contradiction shows that our
assumption (that $\alpha\in\mathbb{Z}$) was wrong. Thus, $\alpha
\notin\mathbb{Z}$, and our proof of the $\Longrightarrow$ direction is finally
done. Proposition \ref{prop.Vab.iso} \textbf{(c)} is finally proven.

Proving Proposition \ref{prop.Vab.iso} was one part of Homework Set 1 exercise
2; the other was the following:

\begin{proposition}
\label{prop.Vab.irr}Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Then,
the $\operatorname*{Vir}$-module $V_{\alpha,\beta}$ is not irreducible if and
only if $\left(  \alpha\in\mathbb{Z}\text{ and }\beta\in\left\{  0,1\right\}
\right)  $.
\end{proposition}

We will not prove this; the interested reader is referred to Proposition 1.1
in \S 1.2 of Kac-Raina.

\subsection{Some consequences of Poincar\'{e}-Birkhoff-Witt}

We will now spend some time with generalities on Lie algebras and their
universal enveloping algebras. These generalities will be applied later, and
while these applications could be substituted by concrete computations, it
appears to me that it is better for the sake of clarity to do them generally
in here.

\begin{proposition}
\label{prop.U(X)U}Let $k$ be a field. Let $\mathfrak{c}$ be a $k$-Lie algebra.
Let $\mathfrak{a}$ and $\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$
such that $\mathfrak{a}+\mathfrak{b}=\mathfrak{c}$. Notice that $\mathfrak{a}%
\cap\mathfrak{b}$ is also a Lie subalgebra of $\mathfrak{c}$.

Let $\rho:U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \rightarrow U\left(
\mathfrak{c}\right)  $ be the $k$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
U\left(  \mathfrak{a}\right)  \text{ and }\beta\in U\left(  \mathfrak{b}%
\right)
\]
(this is clearly well-defined). Then, $\rho$ is an isomorphism of filtered
vector spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and of right
$U\left(  \mathfrak{b}\right)  $-modules.
\end{proposition}

\begin{corollary}
\label{cor.U(X)U}Let $k$ be a field. Let $\mathfrak{c}$ be a $k$-Lie algebra.
Let $\mathfrak{a}$ and $\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$
such that $\mathfrak{a}\oplus\mathfrak{b}=\mathfrak{c}$ (as vector spaces, not
necessarily as Lie algebras). Let $\rho:U\left(  \mathfrak{a}\right)
\otimes_{k}U\left(  \mathfrak{b}\right)  \rightarrow U\left(  \mathfrak{c}%
\right)  $ be the $k$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \mathfrak{a}\right)
\text{ and }\beta\in U\left(  \mathfrak{b}\right)
\]
(this is clearly well-defined). Then, $\rho$ is an isomorphism of filtered
vector spaces, of left $U\left(  \mathfrak{a}\right)  $-modules and of right
$U\left(  \mathfrak{b}\right)  $-modules.
\end{corollary}

We give two proofs of Proposition \ref{prop.U(X)U}. They are very similar
(both use the Poincar\'{e}-Birkhoff-Witt theorem, albeit different versions
thereof). The first is more conceptual (and more general), while the second is
more down-to-earth.

\textit{First proof of Proposition \ref{prop.U(X)U}.} For any Lie algebra
$\mathfrak{u}$, we have a $k$-algebra homomorphism $\operatorname*{PBW}%
\nolimits_{\mathfrak{u}}:S\left(  \mathfrak{u}\right)  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{u}\right)  \right)  $ which sends
$u_{1}u_{2}...u_{\ell}$ to $\overline{u_{1}u_{2}...u_{\ell}}\in
\operatorname*{gr}\nolimits_{\ell}\left(  U\left(  \mathfrak{u}\right)
\right)  $ for every $\ell\in\mathbb{N}$ and every $u_{1},u_{2},...,u_{\ell
}\in\mathfrak{u}$. This homomorphism $\operatorname*{PBW}%
\nolimits_{\mathfrak{u}}$ is an isomorphism due to the
Poincar\'{e}-Birkhoff-Witt theorem.

\begin{verlong}
It is rather clear that $\operatorname*{gr}\left(  U\left(  \mathfrak{a}%
\right)  \right)  $ and $\operatorname*{gr}\left(  U\left(  \mathfrak{b}%
\right)  \right)  $ are $\operatorname*{gr}\left(  U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  \right)  $-modules (since $U\left(  \mathfrak{a}%
\right)  $ and $U\left(  \mathfrak{b}\right)  $ are filtered $U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  $-modules)
\end{verlong}

We can define a $k$-algebra homomorphism $f:\operatorname*{gr}\left(  U\left(
\mathfrak{a}\right)  \right)  \otimes_{\operatorname*{gr}\left(  U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \right)  }\operatorname*{gr}\left(
U\left(  \mathfrak{b}\right)  \right)  \rightarrow\operatorname*{gr}\left(
U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \right)  $ by
\[
f\left(  \overline{u}\otimes_{\operatorname*{gr}\left(  U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \right)  }\overline{v}\right)
=\overline{u\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }v}%
\in\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)
\]
for any $k\in\mathbb{N}$, any $\ell\in\mathbb{N}$, any $u\in U_{\leq k}\left(
\mathfrak{a}\right)  $ and $v\in U_{\leq\ell}\left(  \mathfrak{b}\right)  $.
This $f$ is easily seen to be well-defined. Moreover, $f$ is
surjective\footnote{To show this, either notice that the image of $f$ contains
a generating set of $\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)  $ (because the definition of $f$ easily rewrites
as
\par%
\[
f\left(  \overline{\alpha_{1}\alpha_{2}...\alpha_{k}}\otimes
_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
\right)  }\overline{\beta_{1}\beta_{2}...\beta_{\ell}}\right)  =\overline
{\alpha_{1}\alpha_{2}...\alpha_{k}\otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\beta_{1}\beta_{2}...\beta_{\ell}}\in
\operatorname*{gr}\nolimits_{k+\ell}\left(  U\left(  \mathfrak{a}\right)
\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \right)
\]
for any $k\in\mathbb{N}$, any $\ell\in\mathbb{N}$, any $\alpha_{1},\alpha
_{2},...,\alpha_{k}\in\mathfrak{a}$ and $\beta_{1},\beta_{2},...,\beta_{\ell
}\in\mathfrak{b}$), or prove the more general fact that for any $\mathbb{Z}%
_{+}$-filtered algebra $A$, any filtered right $A$-module $M$ and any filtered
left $A$-module $N$, the canonical map%
\begin{align*}
\operatorname*{gr}\left(  M\right)  \otimes_{\operatorname*{gr}\left(
A\right)  }\operatorname*{gr}\left(  N\right)   &  \rightarrow
\operatorname*{gr}\left(  M\otimes_{A}N\right)  ,\\
\overline{\mu}\otimes_{\operatorname*{gr}\left(  A\right)  }\overline{\nu}  &
\mapsto\overline{\mu\otimes_{A}\nu}\in\operatorname*{gr}\nolimits_{m+n}\left(
M\otimes_{A}N\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{for all }\mu\in
M_{m}\text{ and }\nu\in N_{n}\text{, for all }m,n\in\mathbb{N}\right)
\end{align*}
is well-defined and surjective (this is easy to prove).}.

It is easy to see that the isomorphisms $\operatorname*{PBW}%
\nolimits_{\mathfrak{a}}:S\left(  \mathfrak{a}\right)  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)  \right)  $,
$\operatorname*{PBW}\nolimits_{\mathfrak{b}}:S\left(  \mathfrak{b}\right)
\rightarrow\operatorname*{gr}\left(  U\left(  \mathfrak{b}\right)  \right)  $
and $\operatorname*{PBW}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}:S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \rightarrow\operatorname*{gr}\left(
U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \right)  $ are "compatible" with
each other in the sense that the diagrams%
\[%
%TCIMACRO{\TeXButton{X}{\xycs{12pc}
%\xymatrix{
%S\left(\fraka\right) \otimes S\left(\fraka\cap\frakb\right) \ar[r]^-{\text
%{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\fraka\right)}
%\ar[d]_{\PBW_{\fraka}\otimes\PBW_{\fraka\cap\frakb}}^{\cong} & S\left
%(\fraka\right) \ar[d]_{\PBW_{\fraka}}^{\cong} \\
%\gr\left(U\left(\fraka\right)\right) \otimes\gr\left(U\left(\fraka\cap
%\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
%\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\fraka\right)\right)}
%& \gr\left(U\left(\fraka\right)\right)
%}}}%
%BeginExpansion
\xycs{12pc}
\xymatrix{
S\left(\fraka\right) \otimes S\left(\fraka\cap\frakb\right) \ar[r]^-{\text
{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\fraka\right)}
\ar[d]_{\PBW_{\fraka}\otimes\PBW_{\fraka\cap\frakb}}^{\cong} & S\left
(\fraka\right) \ar[d]_{\PBW_{\fraka}}^{\cong} \\
\gr\left(U\left(\fraka\right)\right) \otimes\gr\left(U\left(\fraka\cap
\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\fraka\right)\right)}
& \gr\left(U\left(\fraka\right)\right)
}%
%EndExpansion
\]
and%
\[%
%TCIMACRO{\TeXButton{X}{\xycs{12pc}
%\xymatrix{
%S\left(\fraka\cap\frakb\right) \otimes S\left(\frakb\right) \ar[r]^-{\text
%{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\frakb\right)}
%\ar[d]_{\PBW_{\fraka\cap\frakb}\otimes\PBW_{\frakb}}^{\cong} & S\left
%(\frakb\right) \ar[d]_{\PBW_{\frakb}}^{\cong} \\
%\gr\left(U\left(\fraka\cap\frakb\right)\right) \otimes\gr\left(U\left
%(\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
%\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\frakb\right)\right)}
%& \gr\left(U\left(\frakb\right)\right)
%}}}%
%BeginExpansion
\xycs{12pc}
\xymatrix{
S\left(\fraka\cap\frakb\right) \otimes S\left(\frakb\right) \ar[r]^-{\text
{action of }S\left(\fraka\cap\frakb\right)\text{ on }S\left(\frakb\right)}
\ar[d]_{\PBW_{\fraka\cap\frakb}\otimes\PBW_{\frakb}}^{\cong} & S\left
(\frakb\right) \ar[d]_{\PBW_{\frakb}}^{\cong} \\
\gr\left(U\left(\fraka\cap\frakb\right)\right) \otimes\gr\left(U\left
(\frakb\right)\right) \ar[r]_-{\text{action of }\gr\left(U\left(\fraka
\cap\frakb\right)\right)\text{ on }\gr\left(U\left(\frakb\right)\right)}
& \gr\left(U\left(\frakb\right)\right)
}%
%EndExpansion
\]
commute\footnote{This is pretty easy to see from the definition of
$\operatorname*{PBW}\nolimits_{\mathfrak{u}}$.}. Hence, they give rise to an
isomorphism%
\begin{align*}
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)   &  \rightarrow
\operatorname*{gr}\left(  U\left(  \mathfrak{a}\right)  \right)
\otimes_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  \right)  }\operatorname*{gr}\left(  U\left(  \mathfrak{b}\right)
\right)  ,\\
\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }\beta &
\mapsto\left(  \operatorname*{PBW}\nolimits_{\mathfrak{a}}\alpha\right)
\otimes_{\operatorname*{gr}\left(  U\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  \right)  }\left(  \operatorname*{PBW}\nolimits_{\mathfrak{b}}%
\beta\right)  .
\end{align*}
Denote this isomorphism by $\left(  \operatorname*{PBW}\nolimits_{\mathfrak{a}%
}\right)  \otimes_{\operatorname*{PBW}\nolimits_{\mathfrak{a}\cap\mathfrak{b}%
}}\left(  \operatorname*{PBW}\nolimits_{\mathfrak{b}}\right)  $.

Finally, let $\sigma:S\left(  \mathfrak{a}\right)  \otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)
\rightarrow S\left(  \mathfrak{c}\right)  $ be the vector space homomorphism
defined by%
\[
\sigma\left(  \alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
S\left(  \mathfrak{a}\right)  \text{ and }\beta\in S\left(  \mathfrak{b}%
\right)  .
\]
This $\sigma$ is rather obviously an algebra homomorphism. Now, it is easy to
see that $\sigma$ is an algebra isomorphism\footnote{\textit{First proof that
}$\sigma$\textit{ is an algebra isomorphism:} Since every subspace of a vector
space has a complementary subspace, we can find a subspace $\mathfrak{d}$ of
$\mathfrak{a}$ such that $\mathfrak{a}=\mathfrak{d}\oplus\left(
\mathfrak{a}\cap\mathfrak{b}\right)  $. Consider such a $\mathfrak{d}$.
\par
Since $\mathfrak{a}=\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  =\mathfrak{d}+\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $, the
fact that $\mathfrak{c}=\mathfrak{a}+\mathfrak{b}$ rewrites as $\mathfrak{c}%
=\mathfrak{d}+\underbrace{\left(  \mathfrak{a}\cap\mathfrak{b}\right)
+\mathfrak{b}}_{\substack{=\mathfrak{b}\\\text{(since }\mathfrak{a}%
\cap\mathfrak{b}\subseteq\mathfrak{b}\text{)}}}=\mathfrak{d}+\mathfrak{b}$.
Combined with $\underbrace{\mathfrak{d}}_{\substack{=\mathfrak{d}%
\cap\mathfrak{a}\\\text{(since }\mathfrak{d}\subseteq\mathfrak{a}\text{)}%
}}\cap\mathfrak{b}\subseteq\mathfrak{d}\cap\mathfrak{a}\cap\mathfrak{b}=0$
(since $\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $ is a
well-defined internal direct sum), this yields $\mathfrak{c}=\mathfrak{d}%
\oplus\mathfrak{b}$.
\par
Recall a known fact from multilinear algebra: Any two vector spaces $U$ and
$V$ satisfy $S\left(  U\oplus V\right)  \cong S\left(  U\right)  \otimes
_{k}S\left(  V\right)  $ by the canonical algebra isomorphism. Hence,
$S\left(  \mathfrak{d}\oplus\mathfrak{b}\right)  \cong S\left(  \mathfrak{d}%
\right)  \otimes_{k}S\left(  \mathfrak{b}\right)  $.
\par
But $\mathfrak{a}=\mathfrak{d}\oplus\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  $ yields $S\left(  \mathfrak{a}\right)  =S\left(  \mathfrak{d}%
\oplus\left(  \mathfrak{a}\cap\mathfrak{b}\right)  \right)  \cong S\left(
\mathfrak{d}\right)  \otimes_{k}S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
$ (by the above-quoted known fact). Hence,%
\begin{align*}
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)   &  \cong\left(  S\left(
\mathfrak{d}\right)  \otimes_{k}S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
\right)  \otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right) \\
&  \cong S\left(  \mathfrak{d}\right)  \otimes_{k}\underbrace{\left(  S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  \right)  }_{=S\left(
\mathfrak{b}\right)  }=S\left(  \mathfrak{d}\right)  \otimes_{k}S\left(
\mathfrak{b}\right)  \cong S\left(  \underbrace{\mathfrak{d}\oplus
\mathfrak{b}}_{=\mathfrak{c}}\right)  =S\left(  \mathfrak{c}\right)  .
\end{align*}
Thus we have constructed an algebra isomorphism $S\left(  \mathfrak{a}\right)
\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right)  \rightarrow S\left(  \mathfrak{c}\right)  $. If we track
down what happens to elements of $\mathfrak{d}$, $\mathfrak{a}\cap
\mathfrak{b}$ and $\mathfrak{b}$ under this isomorphism, we notice that they
just get sent to themselves, so this isomorphism must coincide with $\sigma$
(since two algebra homomorphisms coinciding on a set generators of algebra
must be equal). Thus, $\sigma$ is an algebra isomorphism, qed.
\par
\textit{Second proof that }$\sigma$\textit{ is an algebra isomorphism:} Define
a map $\tau:\mathfrak{c}\rightarrow S\left(  \mathfrak{a}\right)
\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }S\left(
\mathfrak{b}\right)  $ as follows: For every $c\in\mathfrak{c}$, let
$\tau\left(  c\right)  $ be $a\otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }b$, where we have written $c$ in the form $c=a+b$ with
$a\in\mathfrak{a}$ and $b\in\mathfrak{b}$ (in fact, we can write $c$ this way,
because $\mathfrak{c}=\mathfrak{a}+\mathfrak{b}$). This map $\tau$ is
well-defined, because the value of $a\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }b$ depends only on $c$ and not on the exact values of $a$ and $b$ in
the decomposition $c=a+b$. (In fact, if $c=a+b$ and $c=a^{\prime}+b^{\prime}$
are two different ways to decompose $c$ into a sum of an element of
$\mathfrak{a}$ with an element of $\mathfrak{b}$, then $a+b=c=a^{\prime
}+b^{\prime}$, so that $a-a^{\prime}=b^{\prime}-b$, thus $a-a^{\prime}%
\in\mathfrak{a}\cap\mathfrak{b}$ (because $a-a^{\prime}\in\mathfrak{a}$ and
$a-a^{\prime}=b^{\prime}-b\in\mathfrak{b}$), so that%
\begin{align*}
&  \underbrace{a}_{=a^{\prime}+\left(  a-a^{\prime}\right)  }\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }b\\
&  =\left(  a^{\prime}+\left(  a-a^{\prime}\right)  \right)  \otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }1+1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }b\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+\underbrace{\left(  a-a^{\prime}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }1}_{\substack{=1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\left(  a-a^{\prime}\right)  \\\text{(since
}a-a^{\prime}\in\mathfrak{a}\cap\mathfrak{b}\subseteq S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  \text{)}}}+1\otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }b\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
\underbrace{\left(  a-a^{\prime}\right)  }_{=b^{\prime}-b}+1\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }b\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+\underbrace{1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\left(  b^{\prime}-b\right)  +1\otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }b}_{=1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\left(  \left(  b^{\prime}-b\right)  +b\right)  }\\
&  =a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
\underbrace{\left(  \left(  b^{\prime}-b\right)  +b\right)  }_{=b^{\prime}%
}=a^{\prime}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }b^{\prime}.
\end{align*}
)
\par
It is also easy to see that $\tau$ is a linear map. Thus, by the universal
property of the symmetric algebra, the map $\tau:\mathfrak{c}\rightarrow
S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  $ gives rise to a
$k$-algebra homomorphism $\widehat{\tau}:S\left(  \mathfrak{c}\right)
\rightarrow S\left(  \mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  $ that lifts $\tau$.
\par
Any $\alpha\in\mathfrak{a}$ satisfies%
\begin{align*}
\left(  \widehat{\tau}\circ\sigma\right)  \left(  \alpha\otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }1\right)   &  =\widehat{\tau}\left(
\underbrace{\sigma\left(  \alpha\otimes_{S\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }1\right)  }_{\substack{=\alpha1\\\text{(by the
definition of }\sigma\text{)}}}\right)  =\widehat{\tau}\left(  \alpha1\right)
=\widehat{\tau}\left(  \alpha\right)  =\tau\left(  \alpha\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widehat{\tau}\text{ lifts }%
\tau\right) \\
&  =\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }%
1+1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }0\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\tau\text{, since }\alpha=\alpha+0\text{ is a
decomposition of}\\
\text{ }\alpha\text{ into a sum of an element of }\mathfrak{a}\text{ with an
element of }\mathfrak{b}%
\end{array}
\right) \\
&  =\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }1.
\end{align*}
In other words, the map $\widehat{\tau}\circ\sigma$ fixes all tensors of the
form $\alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }1$ with
$\alpha\in\mathfrak{a}$. Similarly, the map $\widehat{\tau}\circ\sigma$ fixes
all tensors of the form $1\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\beta$ with $\beta\in\mathfrak{b}$. Thus, there is a generating set
of the $k$-algebra $S\left(  \mathfrak{a}\right)  \otimes_{S\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }S\left(  \mathfrak{b}\right)  $ such
that the map $\widehat{\tau}\circ\sigma$ fixes all elements of this set
(because $\left\{  \alpha\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }1\ \mid\ \alpha\in\mathfrak{a}\right\}  \cup\left\{  1\otimes
_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }\beta\ \mid\ \beta
\in\mathfrak{b}\right\}  $ is a generating set of the $k$-algebra $S\left(
\mathfrak{a}\right)  \otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}S\left(  \mathfrak{b}\right)  $). Since this map $\widehat{\tau}\circ\sigma$
is a $k$-algebra homomorphism (because $\widehat{\tau}$ and $\sigma$ are
$k$-algebra homomorphisms), this yields that the map $\widehat{\tau}%
\circ\sigma$ is the identity (since a $k$-algebra homomorphism which fixes a
generating set of its domain must be the identity). In other words, we have
shown that $\widehat{\tau}\circ\sigma=\operatorname*{id}$. A slightly
different but similarly simple argument shows that $\sigma\circ\widehat{\tau
}=\operatorname*{id}$. Combining $\sigma\circ\widehat{\tau}=\operatorname*{id}%
$ with $\widehat{\tau}\circ\sigma=\operatorname*{id}$, we conclude that
$\widehat{\tau}$ is an inverse to $\sigma$, so that $\sigma$ is an algebra
isomorphism, qed.}.

Now, it is easy to see (by elementwise checking) that the diagram%
\[%
%TCIMACRO{\TeXButton{X}{\xymatrixcolsep{11pc}
%\xymatrix{
%\gr\left(U\left(\fraka\right)\right) \otimes_{\gr\left(U\left(\fraka\cap
%\frakb\right)\right)} \gr\left(U\left(\frakb\right)\right) \ar[d]^{f}
%& S\left(\fraka\right) \otimes_{S\left(\fraka\cap\frakb\right)} S\left
%(\frakb\right) \ar[l]_-{\left(\PBW_{\fraka}\right) \otimes_{\PBW_{\fraka
%\cap\frakb}} \left(\PBW_{\frakb}\right)}^{\cong} \ar[d]_{\cong}^{\sigma} \\
%\gr\left(U\left(\fraka\right) \otimes_{U\left(\fraka\cap\frakb\right)}
%U\left(\frakb\right)\right) \ar[dr]_{\gr\rho} & S\left(\frakc\right
%) \ar[d]^{\PBW_{\frakc}}_{\cong} \\
%& \gr\left(U\left(\frakc\right)\right)
%}}}%
%BeginExpansion
\xymatrixcolsep{11pc}
\xymatrix{
\gr\left(U\left(\fraka\right)\right) \otimes_{\gr\left(U\left(\fraka\cap
\frakb\right)\right)} \gr\left(U\left(\frakb\right)\right) \ar[d]^{f}
& S\left(\fraka\right) \otimes_{S\left(\fraka\cap\frakb\right)} S\left
(\frakb\right) \ar[l]_-{\left(\PBW_{\fraka}\right) \otimes_{\PBW_{\fraka
\cap\frakb}} \left(\PBW_{\frakb}\right)}^{\cong} \ar[d]_{\cong}^{\sigma} \\
\gr\left(U\left(\fraka\right) \otimes_{U\left(\fraka\cap\frakb\right)}
U\left(\frakb\right)\right) \ar[dr]_{\gr\rho} & S\left(\frakc\right
) \ar[d]^{\PBW_{\frakc}}_{\cong} \\
& \gr\left(U\left(\frakc\right)\right)
}%
%EndExpansion
\]
is commutative.\footnote{In fact, if we follow the pure tensor $\alpha
_{1}\alpha_{2}...\alpha_{k}\otimes_{S\left(  \mathfrak{a}\cap\mathfrak{b}%
\right)  }\beta_{1}\beta_{2}...\beta_{\ell}$ (with $k\in\mathbb{N}$, $\ell
\in\mathbb{N}$, $\alpha_{1},\alpha_{2},...,\alpha_{k}\in\mathfrak{a}$ and
$\beta_{1},\beta_{2},...,\beta_{\ell}\in\mathfrak{b}$) through this diagram,
we get $\overline{\alpha_{1}\alpha_{2}...\alpha_{k}\beta_{1}\beta_{2}%
...\beta_{\ell}}\in\operatorname*{gr}\nolimits_{k+\ell}\left(  \mathfrak{c}%
\right)  $ both ways.} Hence, $\left(  \operatorname*{gr}\rho\right)  \circ f$
is an isomorphism, so that $f$ is injective. Since $f$ is also surjective,
this yields that $f$ is an isomorphism. Thus, $\operatorname*{gr}\rho$ is an
isomorphism (since $\left(  \operatorname*{gr}\rho\right)  \circ f$ is an
isomorphism). Since $\rho$ is a filtered map and $\operatorname*{gr}\rho$ is
an isomorphism, it follows that $\rho$ is an isomorphism of filtered vector
spaces. Hence, $\rho$ is an isomorphism of filtered vector spaces, of left
$U\left(  \mathfrak{a}\right)  $-modules and of right $U\left(  \mathfrak{b}%
\right)  $-modules (since it is clear that $\rho$ is a homomorphism of
$U\left(  \mathfrak{a}\right)  $-left modules and of $U\left(  \mathfrak{b}%
\right)  $-right modules). This proves Proposition \ref{prop.U(X)U}.

\textit{Second proof of Proposition \ref{prop.U(X)U}.} Let $\left(
z_{i}\right)  _{i\in I}$ be a basis of the $k$-vector space $\mathfrak{a}%
\cap\mathfrak{b}$. We extend this basis to a basis $\left(  z_{i}\right)
_{i\in I}\cup\left(  x_{j}\right)  _{j\in J}$ of the $k$-vector space
$\mathfrak{a}$ and to a basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
y_{\ell}\right)  _{\ell\in L}$ of the $k$-vector space $\mathfrak{b}$. Then,
$\left(  z_{i}\right)  _{i\in I}\cup\left(  x_{j}\right)  _{j\in J}\cup\left(
y_{\ell}\right)  _{\ell\in L}$ is a basis of the $k$-vector space
$\mathfrak{c}$. We endow this basis with a total ordering in such a way that
every $x_{j}$ is smaller than every $z_{i}$, and that every $z_{i}$ is smaller
than every $y_{\ell}$. By the Poincar\'{e}-Birkhoff-Witt theorem, we have a
basis of $U\left(  \mathfrak{c}\right)  $ consisting of increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}\cup\left(  y_{\ell}\right)  _{\ell\in L}$. On the
other hand, again by the Poincar\'{e}-Birkhoff-Witt theorem, we have a basis
of $U\left(  \mathfrak{a}\right)  $ consisting of increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}$. Note that the $z_{i}$ accumulate at the right end of
these products, while the $x_{j}$ accumulate at the left end (because we
defined the total ordering in such a way that every $x_{j}$ is smaller than
every $z_{i}$). Hence, $U\left(  \mathfrak{a}\right)  $ is a free right
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $-module, with a basis (over
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  $, not over $k$) consisting of
increasing products of elements of the basis $\left(  x_{j}\right)  _{j\in J}%
$. Combined with the fact that $U\left(  \mathfrak{b}\right)  $ is a free
$k$-vector space with a basis consisting of increasing products of elements of
the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(  y_{\ell}\right)
_{\ell\in L}$ (again by Poincar\'{e}-Birkhoff-Witt), this yields that
$U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap
\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  $ is a free $k$-vector
space with a basis consisting of tensors of the form%
\begin{align*}
&  \left(  \text{some increasing product of elements of the basis }\left(
x_{j}\right)  _{j\in J}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\left(  \text{some increasing product of elements of the basis }\left(
z_{i}\right)  _{i\in I}\cup\left(  y_{\ell}\right)  _{\ell\in L}\right)  .
\end{align*}
The map $\rho$ clearly maps such terms bijectively into increasing products of
elements of the basis $\left(  z_{i}\right)  _{i\in I}\cup\left(
x_{j}\right)  _{j\in J}\cup\left(  y_{\ell}\right)  _{\ell\in L}$. Hence,
$\rho$ maps a basis of $U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  $
bijectively to a basis of $U\left(  \mathfrak{c}\right)  $. Thus, $\rho$ is an
isomorphism of vector spaces. Moreover, since both of our bases were
filtered\footnote{A basis $\mathcal{B}$ of a filtered vector space $V$ is said
to be \textit{filtered} if for every $n\in\mathbb{N}$, the subfamily of
$\mathcal{B}$ consisting of those elements of $\mathcal{B}$ lying in the
$n$-th filtration of $V$ is a basis of the $n$-th filtration of $V$.}, and
$\rho$ respects this filtration on the bases, we can even conclude that $\rho$
is an isomorphism of filtered vector spaces. Since it is clear that $\rho$ is
a homomorphism of $U\left(  \mathfrak{a}\right)  $-left modules and of
$U\left(  \mathfrak{b}\right)  $-right modules, it follows that $\rho$ is an
isomorphism of filtered vector spaces, of left $U\left(  \mathfrak{a}\right)
$-modules and of right $U\left(  \mathfrak{b}\right)  $-modules. This proves
Proposition \ref{prop.U(X)U}.

\textit{Proof of Corollary \ref{cor.U(X)U}.} Corollary \ref{cor.U(X)U}
immediately follows from Proposition \ref{prop.U(X)U} (since $\mathfrak{a}%
\oplus\mathfrak{b}=\mathfrak{c}$ yields $\mathfrak{a}\cap\mathfrak{b}=0$, thus
$U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  =U\left(  0\right)  =k$).

\begin{remark}
\label{rmk.U(X)U}While we have required $k$ to be a field in Proposition
\ref{prop.U(X)U} and Corollary \ref{cor.U(X)U}, these two results hold in more
general situations as well. For instance, Proposition \ref{prop.U(X)U} holds
whenever $k$ is a commutative ring, as long as $\mathfrak{a}$, $\mathfrak{b}$
and $\mathfrak{a}\cap\mathfrak{b}$ are free $k$-modules, and $\mathfrak{a}%
\cap\mathfrak{b}$ is a direct summand of $\mathfrak{a}$ as a $k$-module. In
fact, the first proof of Proposition \ref{prop.U(X)U} works in this situation
(because the Poincar\'{e}-Birkhoff-Witt theorem holds for free modules). In a
more restrictive situation (namely, when $\mathfrak{a}\cap\mathfrak{b}$ is a
free $k$-module, and a direct summand of each of $\mathfrak{a}$ and
$\mathfrak{b}$, with the other two summands also being free), the second proof
of Proposition \ref{prop.U(X)U} works as well. As for Corollary
\ref{cor.U(X)U}, it holds whenever $k$ is a commutative ring, as long as
$\mathfrak{a}$ and $\mathfrak{b}$ are free $k$-modules.

This generality is more than enough for most applications of Proposition
\ref{prop.U(X)U} and Corollary \ref{cor.U(X)U}. Yet we can go even further
using the appropriate generalizations of the Poincar\'{e}-Birkhoff-Witt
theorem (for these, see, e. g., P. J. Higgins, \textit{Baer Invariants and the
Birkhoff-Witt theorem}, J. of Alg. 11, pp. 469-482, (1969), \newline%
\texttt{http://www.sciencedirect.com/science/article/pii/0021869369900866\ }).
\end{remark}

\subsection{$\mathbb{Z}$-graded Lie algebras and Verma modules}

Let us show some general results about representations of $\mathbb{Z}$-graded
Lie algebras -- particularly of \textit{nondegenerate} $\mathbb{Z}$-graded Lie
algebras. This is a notion that encompasses many of the concrete Lie algebras
that we want to study (among others, $\mathcal{A}$, $\mathcal{A}_{0}$, $W$ and
$\operatorname*{Vir}$), and thus by proving the properties of nondegenerate
$\mathbb{Z}$-graded Lie algebras now we can avoid proving them separately in
many different cases.

\begin{definition}
\label{def.gradLie}A $\mathbb{Z}$\textit{-graded Lie algebra} is a Lie algebra
$\mathfrak{g}$ with a decomposition $\mathfrak{g}=\bigoplus\limits_{n\in
\mathbb{Z}}\mathfrak{g}_{n}$ (as a vector space) such that $\left[
\mathfrak{g}_{n},\mathfrak{g}_{m}\right]  \subseteq\mathfrak{g}_{n+m}$ for all
$n,m\in\mathbb{Z}$.
\end{definition}

Note that if $\mathfrak{g}=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}$
is a $\mathbb{Z}$-graded Lie algebra, then $\bigoplus\limits_{n<0}%
\mathfrak{g}_{n}$, $\mathfrak{g}_{0}$ and $\bigoplus\limits_{n>0}%
\mathfrak{g}_{n}$ are Lie subalgebras of $\mathfrak{g}$.

\begin{definition}
\label{def.gradLienondeg}A $\mathbb{Z}$-graded Lie algebra $\mathfrak{g}%
=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}$ is said to be
\textit{nondegenerate} if

\textbf{(1)} the vector space $\mathfrak{g}_{n}$ is finite-dimensional for
every $n\in\mathbb{Z}$;

\textbf{(2)} the Lie algebra $\mathfrak{g}_{0}$ is abelian;

\textbf{(3)} for every positive integer $n$, for generic $\lambda
\in\mathfrak{g}_{0}^{\ast}$, the bilinear form $\mathfrak{g}_{n}%
\times\mathfrak{g}_{-n}\rightarrow\mathbb{C},\ \left(  a,b\right)
\mapsto\lambda\left(  \left[  a,b\right]  \right)  $ is nondegenerate.
("Generic $\lambda$" means "$\lambda$ lying in some dense open subset of
$\mathfrak{g}_{0}^{\ast}$ with respect to the Zariski topology". This subset
can depend on $n$.)
\end{definition}

Note that condition \textbf{(3)} in Definition \ref{def.gradLienondeg} implies
that $\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}%
_{-n}\right)  $ for all $n\in\mathbb{Z}$.

Here are some examples:

\begin{proposition}
The Lie algebras $\mathcal{A}$, $\mathcal{A}_{0}$, $W$ and
$\operatorname*{Vir}$ are nondegenerate (with the usual gradings).
\end{proposition}

\begin{proposition}
\label{prop.grad.g}Let $\mathfrak{g}$ be a finite-dimensional simple Lie
algebra. The following is a reasonable (although non-canonical) way to define
a grading on $\mathfrak{g}$:

Using a Cartan subalgebra and the roots of $\mathfrak{g}$, we can present the
Lie algebra $\mathfrak{g}$ as a Lie algebra with generators $e_{1}$, $e_{2}$,
$...$, $e_{m}$, $f_{1}$, $f_{2}$, $...$, $f_{m}$, $h_{1}$, $h_{2}$, $...$,
$h_{m}$ (the so-called Chevalley generators) and some relations (among them
the Serre relations). Then, we can define a grading on $\mathfrak{g}$ by
setting $\deg\left(  e_{i}\right)  =1$, $\deg\left(  f_{i}\right)  =-1$ and
$\deg\left(  h_{i}\right)  =0$ and continuing this grading in such a way that
$\mathfrak{g}$ becomes a graded Lie algebra. This grading is non-canonical,
but it makes $\mathfrak{g}$ into a nondegenerate graded Lie algebra.
\end{proposition}

\begin{proposition}
If $\mathfrak{g}$ is a finite-dimensional simple Lie algebra, then the loop
algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $ and the affine Kac-Moody
algebra $\widehat{\mathfrak{g}}=\mathfrak{g}\left[  t,t^{-1}\right]
\oplus\mathbb{C}K$ can be graded as follows:

Fix Chevalley generators for $\mathfrak{g}$ and grade $\mathfrak{g}$ as in
Proposition \ref{prop.grad.g}. Now let $\theta$ be the maximal root of
$\mathfrak{g}$, i. e., the highest weight of the adjoint representation of
$\mathfrak{g}$. Let $e_{\theta}$ and $f_{\theta}$ be the root elements
corresponding to $\theta$. The \textit{Coxeter number} of $\mathfrak{g}$ is
defined as $\deg\left(  e_{\theta}\right)  +1$, and denoted by $h$. Now let us
grade $\widehat{\mathfrak{g}}$ by setting $\deg K=0$ and $\deg\left(
at^{m}\right)  =\deg a+mh$ for every homogeneous $a\in\mathfrak{g}$ and every
$m\in\mathbb{Z}$. This grading satisfies $\deg\left(  f_{\theta}t\right)  =1$
and $\deg\left(  e_{\theta}t^{-1}\right)  =-1$. Moreover, the map
$\mathfrak{g}\left[  t,t^{-1}\right]  \rightarrow\mathfrak{g}\left[
t,t^{-1}\right]  ,\ x\mapsto xt$ is homogeneous of degree $h$; this is often
informally stated as "$\deg t=h$" (although $t$ itself is not an element of
$\widehat{\mathfrak{g}}$). It is easy to see that the elements of
$\widehat{\mathfrak{g}}$ of positive degree span $\mathfrak{n}_{+}\oplus
t\mathfrak{g}\left[  t\right]  $.

The graded Lie algebra $\widehat{\mathfrak{g}}$ is nondegenerate. The loop
algebra $\mathfrak{g}\left[  t,t^{-1}\right]  $, however, is not (with the
grading defined in the same way).
\end{proposition}

If $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie algebra, we can write%
\[
\mathfrak{g}=\bigoplus\limits_{n\in\mathbb{Z}}\mathfrak{g}_{n}=\bigoplus
\limits_{n<0}\mathfrak{g}_{n}\oplus\mathfrak{g}_{0}\oplus\bigoplus
\limits_{n>0}\mathfrak{g}_{n}.
\]
We denote $\bigoplus\limits_{n<0}\mathfrak{g}_{n}$ by $\mathfrak{n}_{-}$ and
we denote $\bigoplus\limits_{n>0}\mathfrak{g}_{n}$ by $\mathfrak{n}_{+}$. We
also denote $\mathfrak{g}_{0}$ by $\mathfrak{h}$. Then, $\mathfrak{n}_{-}$,
$\mathfrak{n}_{+}$ and $\mathfrak{h}$ are Lie subalgebras of $\mathfrak{g}$,
and the above decomposition rewrites as $\mathfrak{g}=\mathfrak{n}_{-}%
\oplus\mathfrak{h}\oplus\mathfrak{n}_{+}$ (but this is, of course, not a
decomposition of Lie algebras). This is called the \textit{triangular
decomposition} of $\mathfrak{g}$.

In the following, $\mathfrak{g}$ will be a $\mathbb{Z}$-graded Lie algebra
(not necessarily nondegenerate), and we will work with the notations
introduced above.

\begin{definition}
\label{def.verma}Let $\lambda\in\mathfrak{h}^{\ast}$.

Let $\mathbb{C}_{\lambda}$ denote the $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{+}\right)  $-module which, as a $\mathbb{C}$-vector space, is
the free vector space with basis $\left(  v_{\lambda}^{+}\right)  $ (thus, a
$1$-dimensional vector space), and whose $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{+}\right)  $-action is given by%
\begin{align*}
hv_{\lambda}^{+}  &  =\lambda\left(  h\right)  v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h};\\
\mathfrak{n}_{+}v_{\lambda}^{+}  &  =0.
\end{align*}


The \textit{Verma highest-weight module }$M_{\lambda}^{+}$ \textit{of
}$\left(  \mathfrak{g},\lambda\right)  $ is defined by%
\[
M_{\lambda}^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{\lambda}.
\]
The element $1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
}v_{\lambda}^{+}$ of $M_{\lambda}^{+}$ will still be denoted by $v_{\lambda
}^{+}$ by abuse of notation, and will be called the \textit{defining vector}
of $M_{\lambda}^{+}$. Since $U\left(  \mathfrak{g}\right)  $ and
$\mathbb{C}_{\lambda}$ are graded $U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  $-modules, their tensor product $U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{\lambda}=M_{\lambda}^{+}$ becomes graded as well.

Let $\mathbb{C}_{\lambda}$ denote the $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  $-module which, as a $\mathbb{C}$-vector space, is
the free vector space with basis $\left(  v_{\lambda}^{-}\right)  $ (thus, a
$1$-dimensional vector space), and whose $\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  $-action is given by%
\begin{align*}
hv_{\lambda}^{-}  &  =\lambda\left(  h\right)  v_{\lambda}^{-}%
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h};\\
\mathfrak{n}_{-}v_{\lambda}^{-}  &  =0.
\end{align*}
(Note that we denote this $\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
$-module by $\mathbb{C}_{\lambda}$, although we already have denoted an
$\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-module by $\mathbb{C}%
_{\lambda}$. This is ambiguous, but misunderstandings are unlikely to occur
since these modules are modules over different Lie algebras, and their
restrictions to $\mathfrak{h}$ are identical.)

The \textit{Verma lowest-weight module }$M_{\lambda}^{-}$ \textit{of }$\left(
\mathfrak{g},\lambda\right)  $ is defined by%
\[
M_{\lambda}^{-}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{\lambda}.
\]
The element $1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
}v_{\lambda}^{-}$ of $M_{\lambda}^{-}$ will still be denoted by $v_{\lambda
}^{-}$ by abuse of notation, and will be called the \textit{defining vector}
of $M_{\lambda}^{-}$. Since $U\left(  \mathfrak{g}\right)  $ and
$\mathbb{C}_{\lambda}$ are graded $U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{-}\right)  $-modules, their tensor product $U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}%
_{\lambda}=M_{\lambda}^{-}$ becomes graded as well.
\end{definition}

We notice some easy facts about these modules:

\begin{proposition}
\label{prop.verma1}Let $\lambda\in\mathfrak{h}^{\ast}$.

\textbf{(a)} As a graded $\mathfrak{n}_{-}$-module, $M_{\lambda}^{+}=U\left(
\mathfrak{n}_{-}\right)  v_{\lambda}^{+}$; more precisely, there exists a
graded $\mathfrak{n}_{-}$-module isomorphism $U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{+}$ which
sends every $x\otimes t\in U\left(  \mathfrak{n}_{-}\right)  \otimes
\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{+}$. The Verma module $M_{\lambda
}^{+}$ is concentrated in nonpositive degrees:%
\[
M_{\lambda}^{+}=\bigoplus\limits_{n\geq0}M_{\lambda}^{+}\left[  -n\right]
;\ \ \ \ \ \ \ \ \ \ M_{\lambda}^{+}\left[  -n\right]  =U\left(
\mathfrak{n}_{-}\right)  \left[  -n\right]  v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\geq0.
\]
Also, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\leq-1$, we have%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]


\textbf{(b)} As a graded $\mathfrak{n}_{+}$-module, $M_{\lambda}^{-}=U\left(
\mathfrak{n}_{+}\right)  v_{\lambda}^{-}$; more precisely, there exists a
graded $\mathfrak{n}_{+}$-module isomorphism $U\left(  \mathfrak{n}%
_{+}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{-}$ which
sends every $x\otimes t\in U\left(  \mathfrak{n}_{+}\right)  \otimes
\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{-}$. The Verma module $M_{\lambda
}^{-}$ is concentrated in nonnegative degrees:%
\[
M_{\lambda}^{-}=\bigoplus\limits_{n\geq0}M_{\lambda}^{-}\left[  n\right]
;\ \ \ \ \ \ \ \ \ \ M_{\lambda}^{-}\left[  n\right]  =U\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  v_{\lambda}^{-}\ \ \ \ \ \ \ \ \ \ \text{for
every }n\geq0.
\]
Also, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\geq1$, we have%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{-}\left[  n\right]  \right)
q^{n}=\dfrac{1}{\prod\limits_{j\geq1}\left(  1-q^{j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.verma1}.} \textbf{(a)} Let
$\rho:U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}}U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \rightarrow U\left(  \mathfrak{g}%
\right)  $ be the $\mathbb{C}$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \mathfrak{n}_{-}\right)
\text{ and }\beta\in U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
\]
(this is clearly well-defined). By Corollary \ref{cor.U(X)U} (applied to
$\mathfrak{a}=\mathfrak{n}_{-}$, $\mathfrak{b}=\mathfrak{h}\oplus
\mathfrak{n}_{+}$ and $\mathfrak{c}=\mathfrak{g}$), this $\rho$ is an
isomorphism of filtered\footnote{Filtered by the usual filtration on the
universal enveloping algebra of a Lie algebra. This filtration does not take
into account the grading on $\mathfrak{n}_{-}$, $\mathfrak{h}\oplus
\mathfrak{n}_{+}$ and $\mathfrak{g}$.} vector spaces, of left $U\left(
\mathfrak{n}_{-}\right)  $-modules and of right $U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  $-modules. Also, it is a graded linear
map\footnote{Here we \textit{do} take into account the grading on
$\mathfrak{n}_{-}$, $\mathfrak{h}\oplus\mathfrak{n}_{+}$ and $\mathfrak{g}$.}
(this is clear from its definition), and thus an isomorphism of graded vector
spaces (because if a vector space isomorphism of graded vector spaces is a
graded linear map, then it must be an isomorphism of graded vector
spaces\footnote{If you are wondering why this statement is more than a
blatantly obvious tautology, let me add some clarifications:
\par
A \textit{graded linear map} is a morphism in the category of graded vector
spaces. What I am stating here is that if a vector space isomorphism between
graded vector spaces is at the same time a morphism in the category of graded
vector spaces, then it must be an \textit{isomorphism} in the category of
graded vector spaces. This is very easy to show, but not a self-evident
tautology. In fact, the analogous assertion about filtered vector spaces (i.
e., the assertion that if a vector space isomorphism between filtered vector
spaces is at the same time a morphism in the category of filtered vector
spaces, then it must be an \textit{isomorphism} in the category of filtered
vector spaces) is wrong.}). Altogether, $\rho$ is an isomorphism of graded
filtered vector spaces, of left $U\left(  \mathfrak{n}_{-}\right)  $-modules
and of right $U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-modules.
Hence,%
\begin{align*}
M_{\lambda}^{+}  &  =\underbrace{U\left(  \mathfrak{g}\right)  }%
_{\substack{\cong U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}%
}U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \\\text{(by the
isomorphism }\rho\text{)}}}\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}\cong\left(  U\left(  \mathfrak{n}%
_{-}\right)  \otimes_{\mathbb{C}}U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  \right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}\\
&  \cong U\left(  \mathfrak{n}_{-}\right)  \otimes_{\mathbb{C}}%
\underbrace{\left(  U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{\lambda}\right)  }_{\cong\mathbb{C}_{\lambda}}\cong U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\ \ \ \ \ \ \ \ \ \ \text{as graded
}U\left(  \mathfrak{n}_{-}\right)  \text{-modules.}%
\end{align*}
This gives us a graded $\mathfrak{n}_{-}$-module isomorphism $U\left(
\mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}\rightarrow M_{\lambda
}^{+}$ which is easily seen to send every $x\otimes t\in U\left(
\mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ to $xtv_{\lambda}^{+}$.
Hence, $M_{\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)  v_{\lambda}^{+}$.
Since $\mathfrak{n}_{-}$ is concentrated in negative degrees, it is clear that
$U\left(  \mathfrak{n}_{-}\right)  $ is concentrated in nonpositive degrees.
Hence, $U\left(  \mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ is
concentrated in nonpositive degrees, and thus the same holds for $M_{\lambda
}^{+}$ (since $M_{\lambda}^{+}\cong U\left(  \mathfrak{n}_{-}\right)
\otimes\mathbb{C}_{\lambda}$ as graded $U\left(  \mathfrak{n}_{-}\right)
$-modules). In other words, $M_{\lambda}^{+}=\bigoplus\limits_{n\geq
0}M_{\lambda}^{+}\left[  -n\right]  $.

Since the isomorphism $U\left(  \mathfrak{n}_{-}\right)  \otimes
\mathbb{C}_{\lambda}\rightarrow M_{\lambda}^{+}$ which sends every $x\otimes
t\in U\left(  \mathfrak{n}_{-}\right)  \otimes\mathbb{C}_{\lambda}$ to
$xtv_{\lambda}^{+}$ is graded, it sends $U\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  \otimes\mathbb{C}_{\lambda}=\left(  U\left(  \mathfrak{n}%
_{-}\right)  \otimes\mathbb{C}_{\lambda}\right)  \left[  -n\right]  $ to
$M_{\lambda}^{+}\left[  -n\right]  $ for every $n\geq0$. Thus, $M_{\lambda
}^{+}\left[  -n\right]  =U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]
v_{\lambda}^{+}$ for every $n\geq0$. Hence,
\begin{align*}
\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)   &  =\dim\left(
U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  v_{\lambda}^{+}\right)
=\dim\left(  U\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \right)
=\dim\left(  S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }U\left(  \mathfrak{n}_{-}\right)  \cong S\left(  \mathfrak{n}%
_{-}\right)  \text{ as graded vector spaces}\\
\text{(by the Poincar\'{e}-Birkhoff-Witt theorem)}%
\end{array}
\right)
\end{align*}
for every $n\geq0$. Hence, if $\dim\mathfrak{g}_{j}<\infty$ for all $j\leq-1$,
then%
\[
\sum\limits_{n\geq0}\dim\left(  M_{\lambda}^{+}\left[  -n\right]  \right)
q^{n}=\sum\limits_{n\geq0}\dim\left(  S\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  \right)  q^{n}=\dfrac{1}{\prod\limits_{j\leq-1}\left(
1-q^{-j}\right)  ^{\dim\left(  \left(  \mathfrak{n}_{-}\right)  _{j}\right)
}}=\dfrac{1}{\prod\limits_{j\leq-1}\left(  1-q^{-j}\right)  ^{\dim
\mathfrak{g}_{j}}}.
\]
This proves Proposition \ref{prop.verma1} \textbf{(a)}.

\textbf{(b)} The proof of part \textbf{(b)} is analogous to that of
\textbf{(a)}.

This proves Proposition \ref{prop.verma1}.

We have already encountered an example of a Verma highest-weight module:

\begin{proposition}
\label{prop.fockverma}Let $\mathfrak{g}$ be the Lie algebra $\mathcal{A}_{0}$.
Consider the Fock module $F$ over the Lie algebra $\mathcal{A}_{0}$. Then,
there is a canonical isomorphism $M_{1}^{+}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules (where $1$ is the element of $\mathfrak{h}^{\ast}$ which sends $K$
to $1$) which sends $v_{1}^{+}\in M_{1}^{+}$ to $1\in F$.
\end{proposition}

\textit{First proof of Proposition \ref{prop.fockverma}.} As we showed in the
First proof of Lemma \ref{lem.V=F}, there exists a homomorphism $\overline
{\eta}_{F,1}:\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}%
_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules such that $\overline{\eta}_{F,1}\left(  1\right)  =1$. In the same
proof, we also showed that this $\overline{\eta}_{F,1}$ is an isomorphism. We
thus have an isomorphism $\overline{\eta}_{F,1}:\operatorname*{Ind}%
\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}%
\mathbb{C}\rightarrow F$ of $\mathcal{A}_{0}$-modules such that $\overline
{\eta}_{F,1}\left(  1\right)  =1$. Since%
\begin{align*}
\operatorname*{Ind}\nolimits_{\mathbb{C}K\oplus\mathcal{A}_{0}^{+}%
}^{\mathcal{A}_{0}}\mathbb{C}  &  =U\left(  \mathcal{A}_{0}\right)
\otimes_{U\left(  \mathbb{C}K\oplus\mathcal{A}_{0}^{+}\right)  }%
\mathbb{C}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathcal{A}_{0}=\mathfrak{g}%
\text{, }\mathbb{C}K=\mathfrak{h}\text{, }\mathcal{A}_{0}^{+}=\mathfrak{n}%
_{+}\text{ and }\mathbb{C}=\mathbb{C}_{1}\right) \\
&  =M_{1}^{+},
\end{align*}
and since the element $1$ of $\operatorname*{Ind}\nolimits_{\mathbb{C}%
K\oplus\mathcal{A}_{0}^{+}}^{\mathcal{A}_{0}}\mathbb{C}$ is exactly the
element $v_{1}^{+}$ of $M_{1}^{+}$, this rewrites as follows: We have an
isomorphism $\overline{\eta}_{F,1}:M_{1}^{+}\rightarrow F$ of $\mathcal{A}%
_{0}$-modules such that $\overline{\eta}_{F,1}\left(  v_{1}^{+}\right)  =1$.
This proves Proposition \ref{prop.fockverma}.

\textit{Second proof of Proposition \ref{prop.fockverma}.} It is clear from
the definition of $v_{1}^{+}$ that $a_{i}v_{1}^{+}=0$ for all $i>0$, and that
$Kv_{1}^{+}=v_{1}^{+}$. Applying Lemma \ref{lem.V=F} to $u=v_{1}^{+}$ and
$V=M_{1}^{+}$, we thus conclude that there exists a homomorphism
$\eta:F\rightarrow M_{1}^{+}$ of $\mathcal{A}_{0}$-modules such that
$\eta\left(  1\right)  =v_{1}^{+}$.

On the other hand, since $M_{1}^{+}=U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{1}$
(by the definition of $M_{1}^{+}$), we can define an $U\left(  \mathfrak{g}%
\right)  $-module homomorphism%
\[
M_{1}^{+}\rightarrow F,\ \ \ \ \ \ \ \ \ \ \alpha\otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }z\mapsto\alpha z.
\]
Since $\mathfrak{g}=\mathcal{A}_{0}$, this is an $U\left(  \mathcal{A}%
_{0}\right)  $-module homomorphism, i. e., an $\mathcal{A}_{0}$-module
homomorphism. Denote this homomorphism by $\xi$. We are going to prove that
$\eta$ and $\xi$ are mutually inverse.

Since $v_{1}^{+}=1\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)
}1$, we have%
\begin{align*}
\xi\left(  v_{1}^{+}\right)   &  =\xi\left(  1\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }1\right)  =1\cdot1\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\xi\right) \\
&  =1.
\end{align*}
Since $v_{1}^{+}=\eta\left(  1\right)  $, this rewrites as $\xi\left(
\eta\left(  1\right)  \right)  =1$. In other words, $\left(  \xi\circ
\eta\right)  \left(  1\right)  =1$. Since the vector $1$ generates the
$\mathcal{A}_{0}$-module $F$ (because Lemma \ref{lem.F.P1=P} yields
$P=\underbrace{P\left(  a_{-1},a_{-2},a_{-3},...\right)  }_{\in U\left(
\mathcal{A}_{0}\right)  }\cdot1\in U\left(  \mathcal{A}_{0}\right)  \cdot1$
for every $P\in F$), this yields that the $\mathcal{A}_{0}$-module
homomorphisms $\xi\circ\eta:F\rightarrow F$ and $\operatorname*{id}%
:F\rightarrow F$ are equal on a generating set of the $\mathcal{A}_{0}$-module
$F$. Thus, $\xi\circ\eta=\operatorname*{id}$.

Also, $\left(  \eta\circ\xi\right)  \left(  v_{1}^{+}\right)  =\eta\left(
\underbrace{\xi\left(  v_{1}^{+}\right)  }_{=1}\right)  =\eta\left(  1\right)
=v_{1}^{+}$. Since the vector $v_{1}^{+}$ generates $M_{1}^{+}$ as an
$\mathcal{A}_{0}$-module (because $M_{1}^{+}=U\left(  \mathfrak{g}\right)
\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}%
_{1}=U\left(  \mathcal{A}_{0}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{1}$), this yields that the
$\mathcal{A}_{0}$-module homomorphisms $\eta\circ\xi:M_{1}^{+}\rightarrow
M_{1}^{+}$ and $\operatorname*{id}:M_{1}^{+}\rightarrow M_{1}^{+}$ are equal
on a generating set of the $\mathcal{A}_{0}$-module $M_{1}^{+}$. Thus,
$\eta\circ\xi=\operatorname*{id}$.

Since $\eta\circ\xi=\operatorname*{id}$ and $\xi\circ\eta=\operatorname*{id}$,
the maps $\xi$ and $\eta$ are mutually inverse, so that $\xi$ is an
isomorphism $M_{1}^{+}\rightarrow F$ of $\mathcal{A}_{0}$-modules. We know
that $\xi$ sends $v_{1}^{+}$ to $\xi\left(  v_{1}^{+}\right)  =1$. Thus, there
is a canonical isomorphism $M_{1}^{+}\rightarrow F$ of $\mathcal{A}_{0}%
$-modules which sends $v_{1}^{+}\in M_{1}^{+}$ to $1\in F$. Proposition
\ref{prop.fockverma} is proven.

In analogy to the Second proof of Proposition \ref{prop.fockverma}, we can show:

\begin{proposition}
\label{prop.fockverma.A}Let $\mathfrak{g}$ be the Lie algebra $\mathcal{A}$.
Let $\mu\in\mathbb{C}$. Consider the $\mu$-Fock module $F_{\mu}$ over the Lie
algebra $\mathcal{A}$. Then, there is a canonical isomorphism $M_{1,\mu}%
^{+}\rightarrow F_{\mu}$ of $\mathcal{A}$-modules (where $\left(
1,\mu\right)  $ is the element of $\mathfrak{h}^{\ast}$ which sends $K$ to $1$
and $a_{0}$ to $\mu$) which sends $v_{1,\mu}^{+}\in M_{1,\mu}^{+}$ to $1\in
F_{\mu}$.
\end{proposition}

\subsection{\label{subsect.invform}The invariant bilinear form on Verma
modules}

\subsubsection{The invariant bilinear form}

The study of the Verma modules rests on a $\mathfrak{g}$-bilinear form which
connects a highest-weight Verma module with a lowest-weight Verma module for
the opposite weight. First, let us prove its existence and basic properties:

\begin{proposition}
\label{prop.invform}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra,
and $\lambda\in\mathfrak{h}^{\ast}$.

\textbf{(a)} There exists a unique $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =1$ (where we denote this
bilinear form by $\left(  \cdot,\cdot\right)  $).

\textbf{(b)} This form has degree $0$. (This means that if we consider this
bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ as
a linear map $M_{\lambda}^{+}\otimes M_{-\lambda}^{-}\rightarrow\mathbb{C}$,
then it is a graded map, where $M_{\lambda}^{+}\otimes M_{-\lambda}^{-}$ is
graded as a tensor product of graded vector spaces, and $\mathbb{C}$ is
concentrated in degree $0$.)

\textbf{(c)} Every $\mathfrak{g}$-invariant bilinear form $M_{\lambda}%
^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ is a scalar multiple of this
form $\left(  \cdot,\cdot\right)  $.
\end{proposition}

\begin{remark}
\label{rmk.invform.1}Proposition \ref{prop.invform} still holds when the
ground field $\mathbb{C}$ is replaced by a commutative ring $k$, as long as
some rather weak conditions hold (for instance, it is enough that
$\mathfrak{n}_{-}$, $\mathfrak{n}_{+}$ and $\mathfrak{h}$ are free $k$-modules).
\end{remark}

\begin{definition}
\label{def.invform}Let $\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra,
and $\lambda\in\mathfrak{h}^{\ast}$. According to Proposition
\ref{prop.invform} \textbf{(a)}, there exists a unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$ (where we denote this bilinear form by $\left(  \cdot
,\cdot\right)  $). This form is going to be denoted by $\left(  \cdot
,\cdot\right)  _{\lambda}$ (to stress its dependency on $\lambda$). (Later we
will also denote this form by $\left(  \cdot,\cdot\right)  _{\lambda
}^{\mathfrak{g}}$ to point out its dependency on both $\lambda$ and
$\mathfrak{g}$.)
\end{definition}

To prove Proposition \ref{prop.invform}, we recall two facts about modules
over Lie algebras:

\begin{lemma}
\label{lem.pushpull}Let $\mathfrak{a}$ be a Lie algebra, and let
$\mathfrak{b}$ be a Lie subalgebra of $\mathfrak{a}$. Let $V$ be a
$\mathfrak{b}$-module, and $W$ be an $\mathfrak{a}$-module. Then, $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\cong\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ as $\mathfrak{a}$-modules (where the $W$ on the right
hand side is to be understood as $\operatorname*{Res}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}W$). More precisely, there exists a canonical $\mathfrak{a}%
$-module isomorphism $\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}V\right)  \otimes W\rightarrow\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  $ which maps
$\left(  1\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes w$ to
$1\otimes_{U\left(  \mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all
$v\in V$ and $w\in W$.
\end{lemma}

\begin{lemma}
\label{lem.IndRes}Let $\mathfrak{c}$ be a Lie algebra. Let $\mathfrak{a}$ and
$\mathfrak{b}$ be two Lie subalgebras of $\mathfrak{c}$ such that
$\mathfrak{a}+\mathfrak{b}=\mathfrak{c}$. Notice that $\mathfrak{a}%
\cap\mathfrak{b}$ is also a Lie subalgebra of $\mathfrak{c}$. Let $N$ be a
$\mathfrak{b}$-module. Then, $\operatorname*{Ind}\nolimits_{\mathfrak{a}%
\cap\mathfrak{b}}^{\mathfrak{a}}\left(  \operatorname*{Res}%
\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{b}}N\right)
\cong\operatorname*{Res}\nolimits_{\mathfrak{a}}^{\mathfrak{c}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\right)  $ as
$\mathfrak{a}$-modules.
\end{lemma}

We will give two proofs of Lemma \ref{lem.pushpull}: one which is direct and
uses Hopf algebras; the other which is more elementary but less direct.

\textit{First proof of Lemma \ref{lem.pushpull}.} Remember that $U\left(
\mathfrak{a}\right)  $ is a Hopf algebra (a cocommutative one, actually; but
we won't use this). Let us denote its antipode by $S$ and use sumfree Sweedler notation.

Recalling that $\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V=U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{b}\right)  }V$
and $\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  =U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{b}\right)  }\left(  V\otimes W\right)  $, we define a $\mathbb{C}%
$-linear map $\phi:\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}V\right)  \otimes W\rightarrow\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  $ by
$\left(  \alpha\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes
w\mapsto\alpha_{\left(  1\right)  }\otimes_{U\left(  \mathfrak{b}\right)
}\left(  v\otimes S\left(  \alpha_{\left(  2\right)  }\right)  w\right)  $.
This map is easily checked to be well-defined and $\mathfrak{a}$-linear. Also,
we define a $\mathbb{C}$-linear map $\psi:\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)
\rightarrow\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V\right)  \otimes W$ by $\alpha\otimes_{U\left(  \mathfrak{b}\right)
}\left(  v\otimes w\right)  \mapsto\left(  \alpha_{\left(  1\right)  }%
\otimes_{U\left(  \mathfrak{b}\right)  }v\right)  \otimes\alpha_{\left(
2\right)  }w$. This map is easily checked to be well-defined. It is also easy
to see that $\phi\circ\psi=\operatorname*{id}$ and $\psi\circ\phi
=\operatorname*{id}$. Hence, $\phi$ and $\psi$ are mutually inverse
isomorphisms between the $\mathfrak{a}$-modules $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W$ and
$\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $. This proves that $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W\cong%
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $ as $\mathfrak{a}$-modules. Moreover, the isomorphism $\phi:\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\rightarrow\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ is canonical and maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$. In other words, Lemma \ref{lem.pushpull} is proven.

\textit{Second proof of Lemma \ref{lem.pushpull}.} For every $\mathfrak{a}%
$-module $Y$, we have%
\begin{align*}
&  \operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W,Y\right) \\
&  =\left(  \underbrace{\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)
\otimes W,Y\right)  }_{\cong\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
}\right)  ^{\mathfrak{a}}\\
&  \cong\left(  \operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
\right)  ^{\mathfrak{a}}=\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}%
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{b}}\left(
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right) \\
&  =\left(  \underbrace{\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(
V,\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  W,Y\right)  \right)
}_{\cong\operatorname*{Hom}\nolimits_{\mathbb{C}}\left(  V\otimes W,Y\right)
}\right)  ^{\mathfrak{b}}\cong\left(  \operatorname*{Hom}\nolimits_{\mathbb{C}%
}\left(  V\otimes W,Y\right)  \right)  ^{\mathfrak{b}}\\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{b}}\left(  V\otimes W,Y\right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  ,Y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right)  .
\end{align*}
Since this isomorphism is canonical, it gives us a natural isomorphism between
the functors $\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W,-\right)  $ and $\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  ,-\right)  $. By Yoneda's lemma, this yields that $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\cong\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $ as $\mathfrak{a}$-modules. It is also rather clear that
the $\mathfrak{a}$-module isomorphism $\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W\rightarrow
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $ we have just obtained is canonical.

In order to check that this isomorphism maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$, we must retrace the proof of Yoneda's lemma. This proof proceeds by
evaluating the natural isomorphism $\operatorname*{Hom}\nolimits_{\mathfrak{a}%
}\left(  \left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}%
}V\right)  \otimes W,-\right)  \rightarrow\operatorname*{Hom}%
\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}\left(  V\otimes W\right)  ,-\right)  $ at the object
$\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  $, thus obtaining an isomorphism%
\[
\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(  \left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes W,\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  \right)
\rightarrow\operatorname*{Hom}\nolimits_{\mathfrak{a}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes
W\right)  ,\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  \right)  ,
\]
and taking the preimage of $\operatorname*{id}\in\operatorname*{Hom}%
\nolimits_{\mathfrak{a}}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{b}%
}^{\mathfrak{a}}\left(  V\otimes W\right)  ,\operatorname*{Ind}%
\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(  V\otimes W\right)  \right)  $
under this isomorphism. This preimage is our isomorphism $\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}V\right)  \otimes
W\rightarrow\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{a}}\left(
V\otimes W\right)  $. Checking that this maps $\left(  1\otimes_{U\left(
\mathfrak{b}\right)  }v\right)  \otimes w$ to $1\otimes_{U\left(
\mathfrak{b}\right)  }\left(  v\otimes w\right)  $ for all $v\in V$ and $w\in
W$ is a matter of routine now, and left to the reader. Lemma
\ref{lem.pushpull} is thus proven.

\textit{Proof of Lemma \ref{lem.IndRes}.} Let $\rho:U\left(  \mathfrak{a}%
\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)  }U\left(
\mathfrak{b}\right)  \rightarrow U\left(  \mathfrak{c}\right)  $ be the
$\mathbb{C}$-vector space homomorphism defined by%
\[
\rho\left(  \alpha\otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}\beta\right)  =\alpha\beta\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in
U\left(  \mathfrak{a}\right)  \text{ and }\beta\in U\left(  \mathfrak{b}%
\right)
\]
(this is clearly well-defined). By Proposition \ref{prop.U(X)U}, this map
$\rho$ is an isomorphism of left $U\left(  \mathfrak{a}\right)  $-modules and
of right $U\left(  \mathfrak{b}\right)  $-modules. Hence, $U\left(
\mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}\cap\mathfrak{b}\right)
}U\left(  \mathfrak{b}\right)  \cong U\left(  \mathfrak{c}\right)  $ as left
$U\left(  \mathfrak{a}\right)  $-modules and simultaneously right $U\left(
\mathfrak{b}\right)  $-modules. Now,%
\begin{align*}
\operatorname*{Ind}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}%
}\underbrace{\left(  \operatorname*{Res}\nolimits_{\mathfrak{a}\cap
\mathfrak{b}}^{\mathfrak{b}}N\right)  }_{=N\text{ (as a left }U\left(
\mathfrak{b}\right)  \text{-module)}}  &  =\operatorname*{Ind}%
\nolimits_{\mathfrak{a}\cap\mathfrak{b}}^{\mathfrak{a}}\underbrace{N}_{\cong
U\left(  \mathfrak{b}\right)  \otimes_{U\left(  \mathfrak{b}\right)  }N}%
\cong\operatorname*{Ind}\nolimits_{\mathfrak{a}\cap\mathfrak{b}}%
^{\mathfrak{a}}\left(  U\left(  \mathfrak{b}\right)  \otimes_{U\left(
\mathfrak{b}\right)  }N\right) \\
&  =U\left(  \mathfrak{a}\right)  \otimes_{U\left(  \mathfrak{a}%
\cap\mathfrak{b}\right)  }\left(  U\left(  \mathfrak{b}\right)  \otimes
_{U\left(  \mathfrak{b}\right)  }N\right) \\
&  \cong\underbrace{\left(  U\left(  \mathfrak{a}\right)  \otimes_{U\left(
\mathfrak{a}\cap\mathfrak{b}\right)  }U\left(  \mathfrak{b}\right)  \right)
}_{\cong U\left(  \mathfrak{c}\right)  }\otimes_{U\left(  \mathfrak{b}\right)
}N\cong U\left(  \mathfrak{c}\right)  \otimes_{U\left(  \mathfrak{b}\right)
}N\\
&  =\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}%
N=\operatorname*{Res}\nolimits_{\mathfrak{a}}^{\mathfrak{c}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{b}}^{\mathfrak{c}}N\right)
\ \ \ \ \ \ \ \ \ \ \text{as }\mathfrak{a}\text{-modules.}%
\end{align*}
This proves Lemma \ref{lem.IndRes}.

\textit{Proof of Proposition \ref{prop.invform}.} We have $M_{\lambda}%
^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{\lambda}=\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}%
_{\lambda}$. Thus,%
\begin{align*}
&  \operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+}\otimes
M_{-\lambda}^{-},\mathbb{C}\right) \\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  \underbrace{\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}^{\mathfrak{g}}\mathbb{C}_{\lambda}\right)  \otimes M_{-\lambda}^{-}%
}_{\substack{\cong\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus
\mathfrak{n}_{+}}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes
M_{-\lambda}^{-}\right)  \\\text{(by Lemma \ref{lem.pushpull})}}%
},\mathbb{C}\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes M_{-\lambda}^{-}\right)
,\mathbb{C}\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}\left(  \mathbb{C}_{\lambda}\otimes\underbrace{M_{-\lambda}^{-}%
}_{\substack{=U\left(  \mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{-\lambda}\\=\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\mathbb{C}%
_{-\lambda}}},\mathbb{C}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
Frobenius reciprocity}\right) \\
&  =\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\underbrace{\mathbb{C}_{\lambda}\otimes\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\mathbb{C}%
_{-\lambda}\right)  }_{\substack{\cong\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right)  \\\text{(by Lemma
\ref{lem.pushpull})}}},\mathbb{C}\right)  \cong\operatorname*{Hom}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right)  ,\mathbb{C}\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}%
}\left(  \operatorname*{Ind}\nolimits_{\mathfrak{h}}^{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}\otimes\mathbb{C}%
_{-\lambda}\right)  ,\mathbb{C}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Lemma \ref{lem.IndRes} (applied to }\mathfrak{c}=\mathfrak{g}%
\text{, }\mathfrak{a}=\mathfrak{h}\oplus\mathfrak{n}_{+}\text{, }%
\mathfrak{b}=\mathfrak{h}\oplus\mathfrak{n}_{-}\text{ and }N=\mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda}\text{)}\\
\text{yields }\operatorname*{Ind}\nolimits_{\mathfrak{h}}^{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \operatorname*{Res}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{-}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \right)  \cong\operatorname*{Res}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\left(
\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}%
}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda
}\right)  \right)  \text{,}\\
\text{which rewrites as }\operatorname*{Ind}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \cong\operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{-}}^{\mathfrak{g}}\left(
\mathbb{C}_{\lambda}\otimes\mathbb{C}_{-\lambda}\right) \\
\text{ (since we are suppressing the }\operatorname*{Res}\text{ functors),}\\
\text{ so that }\operatorname*{Ind}\nolimits_{\mathfrak{h}\oplus
\mathfrak{n}_{-}}^{\mathfrak{g}}\left(  \mathbb{C}_{\lambda}\otimes
\mathbb{C}_{-\lambda}\right)  \cong\operatorname*{Ind}\nolimits_{\mathfrak{h}%
}^{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda}%
\otimes\mathbb{C}_{-\lambda}\right)  \text{ (as }\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  \text{-modules)}%
\end{array}
\right) \\
&  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}}\left(  \mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda},\mathbb{C}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right) \\
&  \cong\mathbb{C}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbb{C}%
_{\lambda}\otimes\mathbb{C}_{-\lambda}\cong\mathbb{C}\text{ as }%
\mathfrak{h}\text{-modules (this is easy to see)}\right)  .
\end{align*}
This isomorphism $\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(
M_{\lambda}^{+}\otimes M_{-\lambda}^{-},\mathbb{C}\right)  \rightarrow
\mathbb{C}$ is easily seen to map every $\mathfrak{g}$-invariant bilinear form
$\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ (seen as a linear map $M_{\lambda}^{+}\otimes
M_{-\lambda}^{-}\rightarrow\mathbb{C}$) to the value $\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  $. Hence, there exists a unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$ (where we denote this bilinear form by $\left(  \cdot
,\cdot\right)  $), and every other $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ must be a scalar
multiple of this one. This proves Proposition \ref{prop.invform} \textbf{(a)}
and \textbf{(c)}.

Now, for the proof of \textbf{(b)}: Denote by $\left(  \cdot,\cdot\right)  $
the unique $\mathfrak{g}$-invariant bilinear form $M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  =1$. Let us now prove that this bilinear form is
of degree $0$:

Consider the antipode $S:U\left(  \mathfrak{g}\right)  \rightarrow U\left(
\mathfrak{g}\right)  $ of the Hopf algebra $U\left(  \mathfrak{g}\right)  $.
This $S$ is a graded algebra antiautomorphism satisfying $S\left(  x\right)
=-x$ for every $x\in\mathfrak{g}$. It can be explicitly described by
\[
S\left(  x_{1}x_{2}...x_{m}\right)  =\left(  -1\right)  ^{m}x_{m}%
x_{m-1}...x_{1}\ \ \ \ \ \ \ \ \ \ \text{for all }m\in\mathbb{N}\text{ and
}x_{1},x_{2},...,x_{m}\in\mathfrak{g}.
\]


We can easily see by induction (using the $\mathfrak{g}$-invariance of the
bilinear form $\left(  \cdot,\cdot\right)  $) that $\left(  v,aw\right)
=\left(  S\left(  a\right)  v,w\right)  $ for all $v\in M_{\lambda}^{+}$ and
$w\in M_{-\lambda}^{-}$ and $a\in U\left(  \mathfrak{g}\right)  $. In
particular,%
\[
\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(  b\right)
av_{\lambda}^{+},v_{-\lambda}^{-}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a\in U\left(  \mathfrak{g}\right)  \text{ and }b\in U\left(  \mathfrak{g}%
\right)  .
\]
Thus, $\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)  =\left(  S\left(
b\right)  av_{\lambda}^{+},v_{-\lambda}^{-}\right)  =0$ whenever $a$ and $b$
are homogeneous elements of $U\left(  \mathfrak{g}\right)  $ satisfying $\deg
b>-\deg a$ (this is because any two homogeneous elements $a$ and $b$ of
$U\left(  \mathfrak{g}\right)  $ satisfying $\deg b>-\deg a$ satisfy $S\left(
b\right)  av_{\lambda}^{+}=0$\ \ \ \ \footnote{\textit{Proof.} Let $a$ and $b$
be homogeneous elements of $U\left(  \mathfrak{g}\right)  $ satisfying $\deg
b>-\deg a$. Then, $\deg b+\deg a>0$, and thus the element $S\left(  b\right)
av_{\lambda}^{+}$ of $M_{\lambda}^{+}$ is a homogeneous element of positive
degree (since $\deg v_{\lambda}^{+}=0$), but the only homogeneous element of
$M_{\lambda}^{+}$ of positive degree is $0$ (since $M_{\lambda}^{+}$ is
concentrated in nonpositive degrees), so that $S\left(  b\right)  av_{\lambda
}^{+}=0$.}). In other words, whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
are integers satisfying $m>-n$, we have $\left(  av_{\lambda}^{+}%
,bv_{-\lambda}^{-}\right)  =0$ for every $a\in U\left(  \mathfrak{g}\right)
\left[  n\right]  $ and $b\in U\left(  \mathfrak{g}\right)  \left[  m\right]
$. Since $M_{\lambda}^{+}\left[  n\right]  =\left\{  av_{\lambda}^{+}%
\ \mid\ a\in U\left(  \mathfrak{g}\right)  \left[  n\right]  \right\}  $ and
$M_{-\lambda}^{-}\left[  m\right]  =\left\{  bv_{-\lambda}^{-}\ \mid\ b\in
U\left(  \mathfrak{g}\right)  \left[  m\right]  \right\}  $, this rewrites as
follows: Whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ are integers
satisfying $m>-n$, we have $\left(  M_{\lambda}^{+}\left[  n\right]
,M_{-\lambda}^{-}\left[  m\right]  \right)  =0$.

Similarly, using the formula $\left(  av,w\right)  =\left(  v,S\left(
a\right)  w\right)  $ (which holds for all $v\in M_{\lambda}^{+}$ and $w\in
M_{-\lambda}^{-}$ and $a\in U\left(  \mathfrak{g}\right)  $), we can show that
whenever $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ are integers satisfying $m<-n$,
we have $\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}^{-}\left[
m\right]  \right)  =0$.

Thus we have $\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}%
^{-}\left[  m\right]  \right)  =0$ whenever $m>-n$ and whenever $m<-n$. Hence,
$\left(  M_{\lambda}^{+}\left[  n\right]  ,M_{-\lambda}^{-}\left[  m\right]
\right)  $ can only be nonzero when $m=-n$. In other words, the form $\left(
\cdot,\cdot\right)  $ has degree $0$. This proves Proposition
\ref{prop.invform}. In this proof, we have not used any properties of
$\mathbb{C}$ other than being a commutative ring over which $\mathfrak{n}_{-}%
$, $\mathfrak{n}_{+}$ and $\mathfrak{h}$ are free modules (the latter was only
used for applying consequences of Poincar\'{e}-Birkhoff-Witt); we thus have
also verified Remark \ref{rmk.invform.1}.

\subsubsection{Generic nondegeneracy: Statement of the fact}

We will later (Theorem \ref{thm.verma}) see that the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ is nondegenerate if and only if the $\mathfrak{g}%
$-module $M_{\lambda}^{+}$ is irreducible. This makes the question of when the
form $\left(  \cdot,\cdot\right)  _{\lambda}$ is nondegenerate an important
question to study. It can, in many concrete cases, be answered by
combinatorial computations. But let us first give a general result about how
it is nondegenerate "if $\lambda$ is in sufficiently general position":

\begin{theorem}
\label{thm.invformnondeg}Assume that $\mathfrak{g}$ is a nondegenerate
$\mathbb{Z}$-graded Lie algebra.

Let $\left(  \cdot,\cdot\right)  $ be the form $\left(  \cdot,\cdot\right)
_{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$. (In
other words, let $\left(  \cdot,\cdot\right)  $ be the unique $\mathfrak{g}%
$-invariant bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  =1$. Such a form exists and is unique by Proposition
\ref{prop.invform} \textbf{(a)}.)

In every degree, the form $\left(  \cdot,\cdot\right)  $ is nondegenerate for
generic $\lambda$. More precisely: For every $n\in\mathbb{N}$, the restriction
of the form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda
}^{-}\rightarrow\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times
M_{-\lambda}^{-}\left[  n\right]  $ is nondegenerate for generic $\lambda$.

(What "generic $\lambda$" means here may depend on the degree. Thus, we cannot
claim that "for generic $\lambda$, the form $\left(  \cdot,\cdot\right)  $ is
nondegenerate in every degree"!)
\end{theorem}

The proof of this theorem will occupy the rest of Section
\ref{subsect.invform}. While the statement of Theorem \ref{thm.invformnondeg}
itself will never be used in this text, the proof involves several useful
ideas and provides good examples of how to work with Verma modules
computationally; moreover, the main auxiliary result (Proposition
\ref{prop.det.US}) will be used later in the text.

\textbf{[Note: The below proof has been written at nighttime and not been
checked for mistakes. It also has not been checked for redundancies and
readability.]}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Casting bilinear
forms on coinvariant spaces}

Before we start with the proof, a general fact from representation theory:

\begin{lemma}
\label{lem.bilform}Let $k$ be a field, and let $G$ be a finite group. Let
$\Lambda\in k\left[  G\right]  $ be the element $\sum\limits_{g\in G}g$.

Let $V$ and $W$ be representations of $G$ over $k$. Let $B:V\times
W\rightarrow k$ be a $G$-invariant bilinear form.

\textbf{(a)} Then, there exists one and only one bilinear form $B^{\prime
}:V_{G}\times W_{G}\rightarrow k$ satisfying%
\[
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W\text{.}%
\]
(Here, $\overline{v}$ denotes the projection of $v$ onto $V_{G}$, and
$\overline{w}$ denotes the projection of $w$ onto $W_{G}$.)

\textbf{(b)} Assume that $\left\vert G\right\vert $ is invertible in $k$ (in
other words, assume that $\operatorname*{char}k$ is either $0$ or coprime to
$\left\vert G\right\vert $). If the form $B$ is nondegenerate, then this form
$B^{\prime}$ is nondegenerate, too.
\end{lemma}

\textit{Proof of Lemma \ref{lem.bilform}.} Every $h\in G$ satisfies%
\begin{align*}
h\Lambda &  =h\sum\limits_{g\in G}g\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\Lambda=\sum\limits_{g\in G}g\right) \\
&  =\sum\limits_{g\in G}hg=\sum\limits_{i\in G}i\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }i\text{ for }hg\text{ in the sum, since the map}\\
G\rightarrow G,\ g\mapsto hg\text{ is a bijection}%
\end{array}
\right) \\
&  =\sum\limits_{g\in G}g=\Lambda
\end{align*}
and similarly $\Lambda h=\Lambda$.

Also,%
\begin{align*}
\sum\limits_{g\in G}g^{-1}  &  =\sum\limits_{g\in G}%
g\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }g\text{ for }g^{-1}\text{ in the sum, since the
map}\\
G\rightarrow G,\ g\mapsto g^{-1}\text{ is a bijection}%
\end{array}
\right) \\
&  =\Lambda.
\end{align*}


We further notice that the group $G$ acts trivially on the $G$-modules $k$ and
$W_{G}$ (this follows from the definitions of these modules), and thus $G$
acts trivially on $\operatorname*{Hom}\left(  W_{G},k\right)  $ as well.

For every $v\in V$, the map%
\[
W\rightarrow k,\ \ \ \ \ \ \ \ \ \ w\mapsto B\left(  \Lambda v,w\right)
\]
is clearly $G$-equivariant (since it maps $hw$ to%
\begin{align*}
B\left(  \underbrace{\Lambda}_{=h\Lambda}v,hw\right)   &  =B\left(  h\Lambda
v,hw\right)  =B\left(  \Lambda v,w\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }B\text{ is }G\text{-invariant}\right) \\
&  =hB\left(  \Lambda v,w\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}G\text{ acts trivially on }k\right)
\end{align*}
for every $h\in G$ and $w\in W$), and thus descends to a map%
\[
W_{G}\rightarrow k_{G},\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto\overline
{B\left(  \Lambda v,w\right)  }.
\]
Hence, we have obtained a map%
\[
V\rightarrow\operatorname*{Hom}\left(  W_{G},k_{G}\right)
,\ \ \ \ \ \ \ \ \ \ v\mapsto\left(  \overline{w}\mapsto\overline{B\left(
\Lambda v,w\right)  }\right)  .
\]
Since $k_{G}=k$ (because $G$ acts trivially on $k$), this rewrites as a map%
\[
V\rightarrow\operatorname*{Hom}\left(  W_{G},k\right)
,\ \ \ \ \ \ \ \ \ \ v\mapsto\left(  \overline{w}\mapsto B\left(  \Lambda
v,w\right)  \right)  .
\]


This map, too, is $G$-equivariant (since it maps $hv$ to the map%
\begin{align*}
&  \left(  W_{G}\rightarrow k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto B\left(
\underbrace{\Lambda h}_{=\Lambda}v,w\right)  \right) \\
&  =\left(  W_{G}\rightarrow k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto
B\left(  \Lambda v,w\right)  \right)  =h\left(  W_{G}\rightarrow
k,\ \ \ \ \ \ \ \ \ \ \overline{w}\mapsto B\left(  \Lambda v,w\right)  \right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }G\text{ acts trivially on
}\operatorname*{Hom}\left(  W_{G},k\right)  \right)
\end{align*}
for every $h\in G$ and $v\in V$). Thus, it descends to a map%
\[
V_{G}\rightarrow\left(  \operatorname*{Hom}\left(  W_{G},k\right)  \right)
_{G},\ \ \ \ \ \ \ \ \ \ \overline{v}\mapsto\overline{\left(  \overline
{w}\mapsto B\left(  \Lambda v,w\right)  \right)  }.
\]
Since $\left(  \operatorname*{Hom}\left(  W_{G},k\right)  \right)
_{G}=\operatorname*{Hom}\left(  W_{G},k\right)  $ (because $G$ acts trivially
on $\operatorname*{Hom}\left(  W_{G},k\right)  $), this rewrites as a map%
\[
V_{G}\rightarrow\operatorname*{Hom}\left(  W_{G},k\right)
,\ \ \ \ \ \ \ \ \ \ \overline{v}\mapsto\left(  \overline{w}\mapsto B\left(
\Lambda v,w\right)  \right)  .
\]


This map can be rewritten as a bilinear form $V_{G}\times W_{G}\rightarrow k$
which maps $\left(  \overline{v},\overline{w}\right)  $ to $B\left(  \Lambda
v,w\right)  $ for all $v\in V$ and $w\in W$. Since
\begin{align*}
B\left(  \Lambda v,w\right)   &  =B\left(  \sum\limits_{g\in G}gv,w\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Lambda=\sum\limits_{g\in G}g\right)
\\
&  =\sum\limits_{g\in G}B\left(  gv,\underbrace{w}_{=gg^{-1}w}\right)
=\sum\limits_{g\in G}\underbrace{B\left(  gv,gg^{-1}w\right)  }%
_{\substack{=B\left(  v,g^{-1}w\right)  \\\text{(since }B\text{ is
}G\text{-invariant)}}}=\sum\limits_{g\in G}B\left(  v,g^{-1}w\right) \\
&  =B\left(  v,\underbrace{\sum\limits_{g\in G}g^{-1}}_{=\Lambda}w\right)
=B\left(  v,\Lambda w\right)
\end{align*}
for all $v\in V$ and $w\in W$, we have thus proven that there exists a
bilinear form $B^{\prime}:V_{G}\times W_{G}\rightarrow k$ satisfying%
\[
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W\text{.}%
\]
The uniqueness of such a form is self-evident. This proves Lemma
\ref{lem.bilform} \textbf{(a)}.

\textbf{(b)} Assume that $\left\vert G\right\vert $ is invertible in $k$.
Assume that the form $B$ is nondegenerate.

Let $p\in V_{G}$ be such that $B^{\prime}\left(  p,W_{G}\right)  =0$. Since
$p\in V_{G}$, there exists some $v\in V$ such that $p=\overline{v}$. Consider
this $v$. Then, every $w\in W$ satisfies $B\left(  \Lambda v,w\right)  =0$
(since $B\left(  \Lambda v,w\right)  =B^{\prime}\left(  \underbrace{\overline
{v}}_{=p},\underbrace{\overline{w}}_{\in W_{G}}\right)  \in B^{\prime}\left(
p,W_{G}\right)  =0$). Hence, $\Lambda v=0$ (since $B$ is nondegenerate).

But since the projection of $V$ to $V_{G}$ is a $G$-module map, we have
\begin{align*}
\overline{\Lambda v}  &  =\Lambda\overline{v}=\sum\limits_{g\in G}%
\underbrace{g\overline{v}}_{\substack{=\overline{v}\\\text{(since }G\text{
acts}\\\text{trivially on }V_{G}\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Lambda=\sum\limits_{g\in G}g\right) \\
&  =\sum\limits_{g\in G}\overline{v}=\left\vert G\right\vert \overline{v}.
\end{align*}
Since $\left\vert G\right\vert $ is invertible in $k$, this yields
$\overline{v}=\dfrac{1}{\left\vert G\right\vert }\overline{\Lambda v}=0$
(since $\Lambda v=0$), so that $p=\overline{v}=0$.

We have thus shown that every $p\in V_{G}$ such that $B^{\prime}\left(
p,W_{G}\right)  =0$ must satisfy $p=0$. In other words, the form $B^{\prime}$
is nondegenerate. Lemma \ref{lem.bilform} \textbf{(b)} is proven.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The form $\left(
\cdot,\cdot\right)  _{\lambda}^{\circ}$}

Let us formulate some standing assumptions:

\begin{Convention}
From now on until the end of Section \ref{subsect.invform}, we let
$\mathfrak{g}$ be a $\mathbb{Z}$-graded Lie algebra, and let $\lambda
\in\mathfrak{h}^{\ast}$. We also require that $\mathfrak{g}_{0}$ is abelian
(this is condition \textbf{(2)} of Definition \ref{def.gradLienondeg}), but we
do \textit{not} require $\mathfrak{g}$ to be nondegenerate (unless we
explicitly state this).
\end{Convention}

As vector spaces, $M_{\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\lambda}^{+}\cong U\left(  \mathfrak{n}_{-}\right)  $ (where the
isomorphism maps $v_{\lambda}^{+}$ to $1$) and $M_{-\lambda}^{-}=U\left(
\mathfrak{n}_{+}\right)  v_{-\lambda}^{-}\cong U\left(  \mathfrak{n}%
_{+}\right)  $ (where the isomorphism maps $v_{-\lambda}^{-}$ to $1$). Thus,
the bilinear form $\left(  \cdot,\cdot\right)  =\left(  \cdot,\cdot\right)
_{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$
corresponds to a bilinear form $U\left(  \mathfrak{n}_{-}\right)  \times
U\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$.

For every $n\in\mathbb{N}$, let $\left(  \cdot,\cdot\right)  _{\lambda,n}$
denote the restriction of our form $\left(  \cdot,\cdot\right)  =\left(
\cdot,\cdot\right)  _{\lambda}:M_{\lambda}^{+}\times M_{-\lambda}%
^{-}\rightarrow\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times
M_{-\lambda}^{-}\left[  n\right]  $. In order to prove Theorem
\ref{thm.invformnondeg}, it is enough to prove that for every $n\in\mathbb{N}%
$, when $\mathfrak{g}$ is nondegenerate, this form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ is nondegenerate for generic $\lambda$.

We now introduce a $\mathbb{C}$-bilinear form, which will turn out to be, in
some sense, the "highest term" of the form $\left(  \cdot,\cdot\right)  $ with
respect to $\lambda$ (what this exactly means will be explained in Proposition
\ref{prop.det.US}).

\begin{proposition}
\label{prop.lambda_k}For every $k\in\mathbb{N}$, there exists one and only one
$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ by
\begin{align}
\lambda_{k}\left(  \alpha_{1}\alpha_{2}...\alpha_{k},\beta_{1}\beta
_{2}...\beta_{k}\right)   &  =\sum\limits_{\sigma\in S_{k}}\lambda\left(
\left[  \alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)
\lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  \alpha_{k},\beta_{\sigma\left(  k\right)
}\right]  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }\alpha_{1},\alpha_{2},...,\alpha_{k}%
\in\mathfrak{n}_{-}\text{ and }\beta_{1},\beta_{2},...,\beta_{k}%
\in\mathfrak{n}_{+}. \label{thm.invformnondeg.pf.lambda}%
\end{align}

\end{proposition}

Here, we are using the following convention:

\begin{Convention}
From now on until the end of Section \ref{subsect.invform}, the map
$\lambda:\mathfrak{g}_{0}\rightarrow\mathbb{C}$ is extended to a linear map
$\lambda:\mathfrak{g}\rightarrow\mathbb{C}$ by composing it with the canonical
projection $\mathfrak{g}\rightarrow\mathfrak{g}_{0}$.
\end{Convention}

\textit{First proof of Proposition \ref{prop.lambda_k} (sketched).} Let
$k\in\mathbb{N}$. The value of
\[
\sum\limits_{\sigma\in S_{k}}\lambda\left(  \left[  \alpha_{1},\beta
_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[  \alpha
_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)
\]
depends linearly on each of the $\alpha_{1},\alpha_{2},...,\alpha_{k}$ and
$\beta_{1},\beta_{2},...,\beta_{k}$, and is invariant under any permutation of
the $\alpha_{1},\alpha_{2},...,\alpha_{k}$ and under any permutation of the
$\beta_{1},\beta_{2},...,\beta_{k}$ (as is easily checked). This readily shows
that we can indeed define a $\mathbb{C}$-bilinear form $\lambda_{k}%
:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$ by (\ref{thm.invformnondeg.pf.lambda}).
This proves Proposition \ref{prop.lambda_k}.

\textit{Second proof of Proposition \ref{prop.lambda_k}.} Let $G=S_{k}$. Let
$\Lambda\in\mathbb{C}\left[  G\right]  $ be the element $\sum\limits_{g\in
S_{k}}g=\sum\limits_{\sigma\in S_{k}}\sigma=\sum\limits_{\sigma\in S_{k}%
}\sigma^{-1}$. Let $V$ and $W$ be the canonical representations $\mathfrak{n}%
_{-}^{\otimes k}$ and $\mathfrak{n}_{+}^{\otimes k}$ of $S_{k}$ (where $S_{k}$
acts by permuting the tensorands). Let $B:V\times W\rightarrow\mathbb{C}$ be
the $\mathbb{C}$-bilinear form defined as the $k$-th tensor power of the
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}_{+}%
\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  $. It is easy to see that this form is
$S_{k}$-invariant (in fact, more generally, the $k$-th tensor power of any
bilinear form is $S_{k}$-invariant). Thus, Lemma \ref{lem.bilform}
\textbf{(a)} (applied to $\mathbb{C}$ instead of $k$) yields that there exists
one and only one bilinear form $B^{\prime}:V_{G}\times W_{G}\rightarrow
\mathbb{C}$ satisfying%
\begin{equation}
B^{\prime}\left(  \overline{v},\overline{w}\right)  =B\left(  \Lambda
v,w\right)  =B\left(  v,\Lambda w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}v\in V\text{ and }w\in W \label{thm.invformnondeg.pf.B'}%
\end{equation}
(where $\overline{v}$ denotes the projection of $v$ onto $V_{G}=V_{S_{k}%
}=S^{k}\left(  \mathfrak{n}_{-}\right)  $, and $\overline{w}$ denotes the
projection of $w$ onto $W_{G}=W_{S_{k}}=S^{k}\left(  \mathfrak{n}_{+}\right)
$). Consider this form $B^{\prime}$. All $\alpha_{1},\alpha_{2},...,\alpha
_{k}\in\mathfrak{n}_{-}$ and $\beta_{1},\beta_{2},...,\beta_{k}\in
\mathfrak{n}_{+}$ satisfy%
\begin{align*}
&  B^{\prime}\left(  \alpha_{1}\alpha_{2}...\alpha_{k},\beta_{1}\beta
_{2}...\beta_{k}\right) \\
&  =B^{\prime}\left(  \overline{\alpha_{1}\otimes\alpha_{2}\otimes
...\otimes\alpha_{k}},\overline{\beta_{1}\otimes\beta_{2}\otimes
...\otimes\beta_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha_{1}\alpha_{2}...\alpha
_{k}=\overline{\alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}}\text{
and }\beta_{1}\beta_{2}...\beta_{k}=\overline{\beta_{1}\otimes\beta_{2}%
\otimes...\otimes\beta_{k}}\right) \\
&  =B\left(  \alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}%
,\Lambda\left(  \beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{thm.invformnondeg.pf.B'}),
applied to }v=\alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}\text{ and
}w=\beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right) \\
&  =B\left(  \alpha_{1}\otimes\alpha_{2}\otimes...\otimes\alpha_{k}%
,\sum\limits_{\sigma\in S_{k}}\beta_{\sigma\left(  1\right)  }\otimes
\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\Lambda=\sum\limits_{\sigma\in S_{k}}\sigma^{-1}\text{ yields
}\Lambda\left(  \beta_{1}\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)
=\sum\limits_{\sigma\in S_{k}}\underbrace{\sigma^{-1}\left(  \beta_{1}%
\otimes\beta_{2}\otimes...\otimes\beta_{k}\right)  }_{=\beta_{\sigma\left(
1\right)  }\otimes\beta_{\sigma\left(  2\right)  }\otimes...\otimes
\beta_{\sigma\left(  k\right)  }}\\
=\sum\limits_{\sigma\in S_{k}}\beta_{\sigma\left(  1\right)  }\otimes
\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }%
\end{array}
\right) \\
&  =\sum\limits_{\sigma\in S_{k}}\underbrace{B\left(  \alpha_{1}\otimes
\alpha_{2}\otimes...\otimes\alpha_{k},\beta_{\sigma\left(  1\right)  }%
\otimes\beta_{\sigma\left(  2\right)  }\otimes...\otimes\beta_{\sigma\left(
k\right)  }\right)  }_{\substack{=\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)
\\\text{(since }B\text{ is the }k\text{-th tensor power of the }%
\mathbb{C}\text{-bilinear form }\mathfrak{n}_{-}\times\mathfrak{n}%
_{+}\rightarrow\mathbb{C},\ \left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  \text{)}}}\\
&  =\sum\limits_{\sigma\in S_{k}}\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{k},\beta_{\sigma\left(  k\right)  }\right]  \right)  .
\end{align*}
Thus, there exists a $\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(
\mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}_{+}\right)
\rightarrow\mathbb{C}$ satisfying (\ref{thm.invformnondeg.pf.lambda}). But
such a form is also seen to be unique. Hence, we can indeed define a
$\mathbb{C}$-bilinear form $\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)
\times S^{k}\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$ by
(\ref{thm.invformnondeg.pf.lambda}). And, moreover,%
\begin{equation}
\text{this form }\lambda_{k}\text{ is the form }B^{\prime}\text{ satisfying
(\ref{thm.invformnondeg.pf.B'}).} \label{thm.invformnondeg.pf.B'=l_k}%
\end{equation}
Proposition \ref{prop.lambda_k} is thus proven.

\begin{definition}
\label{def.lambda_k}For every $k\in\mathbb{N}$, let $\lambda_{k}:S^{k}\left(
\mathfrak{n}_{-}\right)  \times S^{k}\left(  \mathfrak{n}_{+}\right)
\rightarrow\mathbb{C}$ be the $\mathbb{C}$-bilinear form whose existence and
uniqueness is guaranteed by Proposition \ref{prop.lambda_k}. These forms can
be added together, resulting in a bilinear form $\bigoplus\limits_{k\geq
0}\lambda_{k}:S\left(  \mathfrak{n}_{-}\right)  \times S\left(  \mathfrak{n}%
_{+}\right)  \rightarrow\mathbb{C}$. It is very easy to see that this form is
of degree $0$ (where the grading on $S\left(  \mathfrak{n}_{-}\right)  $ and
$S\left(  \mathfrak{n}_{+}\right)  $ is not the one that gives the $k$-th
symmetric power the degree $k$ for every $k\in\mathbb{N}$, but is the one
induced by the grading on $\mathfrak{n}_{-}$ and $\mathfrak{n}_{+}$). Denote
this form by $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$.
\end{definition}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Generic nondegeneracy
of $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$}

\begin{lemma}
\label{lem.lambda_k}Let $\lambda\in\mathfrak{h}^{\ast}$ be such that the
$\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}_{+}%
\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto\lambda\left(
\left[  \alpha,\beta\right]  \right)  $ is nondegenerate. Then, the form
$\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$ is nondegenerate.
\end{lemma}

\textit{Proof of Lemma \ref{lem.lambda_k}.} Let $k\in\mathbb{N}$. Introduce
the same notations as in the Second proof of Proposition \ref{prop.lambda_k}.

The $\mathbb{C}$-bilinear form $\mathfrak{n}_{-}\times\mathfrak{n}%
_{+}\rightarrow\mathbb{C},$ $\left(  \alpha,\beta\right)  \mapsto
\lambda\left(  \left[  \alpha,\beta\right]  \right)  $ is nondegenerate. Thus,
the $k$-th tensor power of this form is also nondegenerate (since all tensor
powers of a nondegenerate form are always nondegenerate). But the $k$-th
tensor power of this form is $B$. Thus, $B$ is nondegenerate. Hence, Lemma
\ref{lem.bilform} \textbf{(b)} yields that the form $B^{\prime}$ is
nondegenerate. Due to (\ref{thm.invformnondeg.pf.B'=l_k}), this yields that
the form $\lambda_{k}$ is nondegenerate.

Forget that we fixed $k$. We thus have shown that for every $k\in\mathbb{N}$,
the form $\lambda_{k}$ is nondegenerate. Thus, the direct sum $\bigoplus
\limits_{k\geq0}\lambda_{k}$ of these forms is also nondegenerate. Since
$\bigoplus\limits_{k\geq0}\lambda_{k}=\left(  \cdot,\cdot\right)  _{\lambda
}^{\circ}$, this yields that $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$
is nondegenerate. This proves Lemma \ref{lem.lambda_k}.

For every $n\in\mathbb{N}$, define $\left(  \cdot,\cdot\right)  _{\lambda
,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \times
S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  \rightarrow\mathbb{C}$ to
be the restriction of this form $\left(  \cdot,\cdot\right)  _{\lambda}%
^{\circ}=\bigoplus\limits_{k\geq0}\lambda_{k}:S\left(  \mathfrak{n}%
_{-}\right)  \times S\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}$
to $S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  \times S\left(
\mathfrak{n}_{+}\right)  \left[  n\right]  $. We now need the following
strengthening of Lemma \ref{lem.lambda_k}:

\begin{lemma}
\label{lem.lambda_k.2}Let $n\in\mathbb{N}$ and $\lambda\in\mathfrak{h}^{\ast}$
be such that the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $. Then, the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must also be nondegenerate.
\end{lemma}

\textit{Proof of Lemma \ref{lem.lambda_k.2}.} For Lemma \ref{lem.lambda_k} to
hold, we did not need $\mathfrak{g}$ to be a graded Lie algebra; we only
needed that $\mathfrak{g}$ is a graded vector space with a well-defined
bilinear map $\left[  \cdot,\cdot\right]  :\mathfrak{g}_{-k}\times
\mathfrak{g}_{k}\rightarrow\mathfrak{g}_{0}$ for every positive integer $k$.
This is a rather weak condition, and holds not only for $\mathfrak{g}$, but
also for the graded subspace $\mathfrak{g}_{-n}\oplus\mathfrak{g}_{-n+1}%
\oplus...\oplus\mathfrak{g}_{n}$ of $\mathfrak{g}$. Denote this graded
subspace $\mathfrak{g}_{-n}\oplus\mathfrak{g}_{-n+1}\oplus...\oplus
\mathfrak{g}_{n}$ by $\mathfrak{g}^{\prime}$, and let $\mathfrak{n}%
_{-}^{\prime}\oplus\mathfrak{h}^{\prime}\oplus\mathfrak{n}_{+}^{\prime}$ be
its triangular decomposition (thus, $\mathfrak{n}_{-}^{\prime}=\mathfrak{g}%
_{-n}\oplus\mathfrak{g}_{-n+1}\oplus...\oplus\mathfrak{g}_{-1}$,
$\mathfrak{h}^{\prime}=\mathfrak{g}_{0}=\mathfrak{h}$ and $\mathfrak{n}%
_{+}^{\prime}=\mathfrak{g}_{1}\oplus\mathfrak{g}_{2}\oplus...\oplus
\mathfrak{g}_{n}$). The $\mathbb{C}$-bilinear form $\mathfrak{n}_{-}^{\prime
}\times\mathfrak{n}_{+}^{\prime}\rightarrow\mathbb{C},$ $\left(  \alpha
,\beta\right)  \mapsto\lambda\left(  \left[  \alpha,\beta\right]  \right)  $
is nondegenerate (because the bilinear form $\mathfrak{g}_{-k}\times
\mathfrak{g}_{k}\rightarrow\mathbb{C},\ \left(  a,b\right)  \mapsto
\lambda\left(  \left[  a,b\right]  \right)  $ is nondegenerate for every
$k\in\left\{  1,2,...,n\right\}  $). Hence, by Lemma \ref{lem.lambda_k}, the
form $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}$ \textit{defined for
}$\mathfrak{g}^{\prime}$ \textit{instead of }$\mathfrak{g}$ is nondegenerate.
Since this form is of degree $0$, the restriction $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}$ of this form to $S\left(  \mathfrak{n}_{-}^{\prime
}\right)  \left[  -n\right]  \times S\left(  \mathfrak{n}_{+}^{\prime}\right)
\left[  n\right]  $ must also be nondegenerate\footnote{This is because if $V$
and $W$ are two graded vector spaces, and $\phi:V\times W\rightarrow
\mathbb{C}$ is a nondegenerate bilinear form of degree $0$, then for every
$n\in\mathbb{Z}$, the restriction of $\phi$ to $V\left[  -n\right]  \times
W\left[  n\right]  $ must also be nondegenerate.}. But since $S\left(
\mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]  =S\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  $\ \ \ \ \footnote{\textit{Proof.} Since
$\mathfrak{n}_{+}=\sum\limits_{i\geq1}\mathfrak{g}_{i}$, we have $S\left(
\mathfrak{n}_{+}\right)  =\sum\limits_{k\in\mathbb{N}}\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{k}\right)  \in\mathbb{N}%
^{k};\\\text{each }i_{j}\geq1}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}%
}...\mathfrak{g}_{i_{k}}$ and thus%
\[
S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  =\sum\limits_{k\in
\mathbb{N}}\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{k}\right)
\in\mathbb{N}^{k};\\\text{each }i_{j}\geq1;\\i_{1}+i_{2}+...+i_{k}%
=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
\]
(since $\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}%
}\subseteq S\left(  \mathfrak{n}_{+}\right)  \left[  i_{1}+i_{2}%
+...+i_{k}\right]  $ for all $\left(  i_{1},i_{2},...,i_{k}\right)
\in\mathbb{N}^{k}$). Similarly,%
\[
S\left(  \mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]  =\sum
\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq1;\\\text{each
}\left\vert i_{j}\right\vert \leq n;\\i_{1}+i_{2}+...+i_{k}=n}}\mathfrak{g}%
_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
\]
(because $\mathfrak{g}^{\prime}$ is obtained from $\mathfrak{g}$ by removing
all $\mathfrak{g}_{i}$ with $\left\vert i\right\vert >n$). Thus,%
\begin{align*}
S\left(  \mathfrak{n}_{+}^{\prime}\right)  \left[  n\right]   &
=\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq
1;\\\text{each }\left\vert i_{j}\right\vert \leq n;\\i_{1}+i_{2}+...+i_{k}%
=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}}...\mathfrak{g}_{i_{k}}%
=\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{k}\right)  \in\mathbb{N}^{k};\\\text{each }i_{j}\geq
1;\\i_{1}+i_{2}+...+i_{k}=n}}\mathfrak{g}_{i_{1}}\mathfrak{g}_{i_{2}%
}...\mathfrak{g}_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we removed the condition }\left(  \text{each }\left\vert
i_{j}\right\vert \leq n\right)  \text{, because it was redundant}\\
\text{(since every }\left(  i_{1},i_{2},...,i_{k}\right)  \in\mathbb{N}%
^{k}\text{ satisfying }i_{1}+i_{2}+...+i_{k}=n\text{ automatically}\\
\text{satisfies }\left(  \text{each }\left\vert i_{j}\right\vert \leq
n\right)  \text{)}%
\end{array}
\right) \\
&  =S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  ,
\end{align*}
qed.} and $S\left(  \mathfrak{n}_{-}^{\prime}\right)  \left[  -n\right]
=S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  $\ \ \ \ \footnote{for
analogous reasons}, this restriction is exactly our form $\left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  \times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
\rightarrow\mathbb{C}$ (in fact, the form is clearly given by the same
formula). Thus we have shown that our form $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]
\times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  \rightarrow
\mathbb{C}$ is nondegenerate. Lemma \ref{lem.lambda_k.2} is proven.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: $\left(  \cdot
,\cdot\right)  _{\lambda}^{\circ}$ is the "highest term" of $\left(
\cdot,\cdot\right)  _{\lambda}$}

Before we go on, let us sketch the direction in which we want to go. We want
to study how, for a fixed $n\in\mathbb{N}$, the form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ changes with $\lambda$. If $V$ and $W$ are two
finite-dimensional vector spaces \textbf{of the same dimension}, and if we
have chosen bases for these two vector spaces $V$ and $W$, then we can
represent every bilinear form $V\times W\rightarrow\mathbb{C}$ as a square
matrix with respect to these two bases, and the bilinear form is nondegenerate
if and only if this matrix has nonzero determinant. This suggests that we
study how the determinant $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ of the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$
with respect to some bases of $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ changes with $\lambda$ (and, in
particular, show that this determinant is nonzero for generic $\lambda$ when
$\mathfrak{g}$ is nondegenerate). Of course, speaking of this determinant
$\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $ only makes
sense when the bases of $M_{\lambda}^{+}\left[  -n\right]  $ and $M_{-\lambda
}^{-}\left[  n\right]  $ have the same size (since only square matrices have
determinants), but this is automatically satisfied if we have\textit{ }%
$\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}_{-n}\right)  $
for every integer $n>0$ (this condition is automatically satisfied when
$\mathfrak{g}$ is a nondegenerate $\mathbb{Z}$-graded Lie algebra, but of
course not only then).

Unfortunately, the spaces $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ themselves change with $\lambda$. Thus,
if we want to pick some bases of $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ for all $\lambda\in\mathfrak{h}^{\ast}$,
we have to pick new bases \textbf{for every }$\lambda$. If we just pick these
bases randomly, then the determinant $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ can change very unpredictably (because the determinant
depends on the choice of bases). Thus, if we want to say something interesting
about how $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $
changes with $\lambda$, then we should specify a reasonable choice of bases
for all $\lambda$. Fortunately, this is not difficult: It is enough to choose
Poincar\'{e}-Birkhoff-Witt bases for $U\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  $ and $U\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
$, and thus obtain bases $M_{\lambda}^{+}\left[  -n\right]  $ and
$M_{-\lambda}^{-}\left[  n\right]  $ due to the isomorphisms $M_{\lambda}%
^{+}\left[  -n\right]  \cong U\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  \cong U\left(
\mathfrak{n}_{+}\right)  \left[  n\right]  $. (See Convention
\ref{conv.invformnondeg.bases} for details.) With bases chosen this way, the
determinant $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  $
will depend on $\lambda$ polynomially, and we will be able to conclude some
useful properties of this polynomial.

So much for our roadmap. Let us first make a convention:

\begin{Convention}
If $V$ and $W$ are two finite-dimensional vector spaces \textbf{of the same
dimension}, and if we have chosen bases for these two vector spaces $V$ and
$W$, then we can represent every bilinear form $B:V\times W\rightarrow
\mathbb{C}$ as a square matrix with respect to these two bases. The
determinant of this matrix will be denoted by $\det B$ and called the
\textit{determinant of the form }$B$. Of course, this determinant $\det B$
depends on the bases chosen. A change of either basis induces a scaling of
$\det B$ by a \textit{nonzero} scalar. Thus, while the determinant $\det B$
itself depends on the choice of bases, the property of $\det B$ to be zero or
nonzero does \textbf{not} depend on the choice of bases.
\end{Convention}

Let us now look at how the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$ and
its determinant $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
$ depend on $\lambda$. We want to show that this dependence is polynomial. In
order to make sense of this, let us define what we mean by "polynomial" here:

\begin{definition}
\label{def.det.US.poly}Let $V$ be a finite-dimensional vector space. A
function $\phi:V\rightarrow\mathbb{C}$ is said to be a \textit{polynomial
function} (or just to be \textit{polynomial} -- but this is not the same as
being \textit{a polynomial}) if one of the following equivalent conditions holds:

\textbf{(1)} There exist a basis $\left(  \beta_{1},\beta_{2},...,\beta
_{m}\right)  $ of the dual space $V^{\ast}$ and a polynomial $P\in
\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]


\textbf{(2)} For every basis $\left(  \beta_{1},\beta_{2},...,\beta
_{m}\right)  $ of the dual space $V^{\ast}$, there exists a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]


\textbf{(3)} There exist finitely many elements $\beta_{1}$, $\beta_{2}$,
$...$, $\beta_{m}$ of the dual space $V^{\ast}$ and a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{m}\right]  $ such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =P\left(  \beta
_{1}\left(  v\right)  ,\beta_{2}\left(  v\right)  ,...,\beta_{m}\left(
v\right)  \right)  .
\]

\end{definition}

Note that this is exactly the meaning of the word "polynomial function" that
is used in Classical Invariant Theory. In our case (where the field is
$\mathbb{C}$), polynomial functions $V\rightarrow\mathbb{C}$ can be identified
with elements of the symmetric algebra $\operatorname*{S}\left(  V^{\ast
}\right)  $, and in some sense are an "obsoleted version" of the
latter.\footnote{The identification of polynomial functions $V\rightarrow
\mathbb{C}$ with elements of the symmetric algebra $\operatorname*{S}\left(
V^{\ast}\right)  $ works similarly over any \textit{infinite} field instead of
$\mathbb{C}$. It breaks down over finite fields, however (because different
elements of $\operatorname*{S}\left(  V^{\ast}\right)  $ may correspond to the
same polynomial function over a finite field).} For our goals, however,
polynomial functions are enough. Let us define the notion of
\textit{homogeneous polynomial functions}:

\begin{definition}
\label{def.det.US.poly.hom}Let $V$ be a finite-dimensional vector space.

\textbf{(a)} Let $n\in\mathbb{N}$. A polynomial function $\phi:V\rightarrow
\mathbb{C}$ is said to be \textit{homogeneous of degree }$n$ if and only if%
\[
\text{every }v\in V\text{ and every }\lambda\in\mathbb{C}\text{ satisfy }%
\phi\left(  \lambda v\right)  =\lambda^{n}\phi\left(  v\right)  .
\]


\textbf{(b)} A polynomial function $\phi:V\rightarrow\mathbb{C}$ is said to be
\textit{homogeneous} if and only if there exists some $n\in\mathbb{N}$ such
that $\phi$ is homogeneous of degree $n$.

\textbf{(c)} It is easy to see that for every polynomial function
$\phi:V\rightarrow\mathbb{C}$, there exists a unique sequence $\left(
\phi_{n}\right)  _{n\in\mathbb{N}}$ of polynomial functions $\phi
_{n}:V\rightarrow\mathbb{C}$ such that all but finitely many $n\in\mathbb{N}$
satisfy $\phi_{n}=0$, such that $\phi_{n}$ is homogeneous of degree $n$ for
every $n\in\mathbb{N}$, and such that $\phi=\sum\limits_{n\in\mathbb{N}}%
\phi_{n}$. This sequence is said to be the \textit{graded decomposition} of
$\phi$. For every $n\in\mathbb{N}$, its member $\phi_{n}$ is called the
$n$\textit{-th homogeneous component} of $\phi$. If $N$ is the highest
$n\in\mathbb{N}$ such that $\phi_{n}\neq0$, then $\phi_{N}$ is said to be the
\textit{leading term} of $\phi$.
\end{definition}

Note that Definition \ref{def.det.US.poly.hom} \textbf{(c)} defines the
"leading term" of a polynomial as its highest-degree nonzero homogeneous
component. This "leading term" may (and usually will) contain more than one
monomial, so this notion of a "leading term" is not the same as the notion of
a "leading term" commonly used, e. g., in Gr\"{o}bner basis theory.

We now state the following crucial fact:

\begin{proposition}
\label{prop.det.US}Let $n\in\mathbb{N}$. Assume that $\mathfrak{g}$ is a
nondegenerate $\mathbb{Z}$-graded Lie algebra. As a consequence,
$\dim\mathfrak{h}=\dim\left(  \mathfrak{g}_{0}\right)  \neq\infty$, so that
$\dim\left(  \mathfrak{h}^{\ast}\right)  \neq\infty$, and thus the notion of a
polynomial function $\mathfrak{h}^{\ast}\rightarrow\mathbb{C}$ is well-defined.

There is an appropriate way of choosing bases of the vector spaces $S\left(
\mathfrak{n}_{-}\right)  \left[  -n\right]  $ and $S\left(  \mathfrak{n}%
_{+}\right)  \left[  n\right]  $ and bases of the vector spaces $M_{\lambda
}^{+}\left[  -n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  $ for all
$\lambda\in\mathfrak{h}^{\ast}$ such that the following holds:

\textbf{(a)} The determinants $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ and $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ (these determinants are defined with respect to
the chosen bases of $S\left(  \mathfrak{n}_{-}\right)  \left[  -n\right]  $,
$S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  $, $M_{\lambda}%
^{+}\left[  -n\right]  $ and $M_{-\lambda}^{-}\left[  n\right]  $) depend
polynomially on $\lambda$. By this, we mean that the functions%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
and%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
are polynomial functions.

\textbf{(b)} The leading term of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]

\end{proposition}

\begin{remark}
We can extend Proposition \ref{prop.det.US} to the case when $\mathfrak{g}$ is
no longer nondegenerate. However, this requires the following changes to
Proposition \ref{prop.det.US}:

Replace the requirement that $\mathfrak{g}$ be nondegenerate by the
requirement that $\mathfrak{g}$ satisfy the conditions \textbf{(1)} and
\textbf{(2)} in Definition \ref{def.gradLienondeg} as well as the condition
that $\dim\left(  \mathfrak{g}_{n}\right)  =\dim\left(  \mathfrak{g}%
_{-n}\right)  $ for every integer $n>0$ (this condition is a weakening of
condition \textbf{(3)} in Definition \ref{def.gradLienondeg}). Replace the
claim that "The leading term of the polynomial function $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}\right)  $ is $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  $, up to multiplication by a
nonzero scalar" by the claim that "There exists some $k\in\mathbb{N}$ such
that the polynomial function $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ is the $k$-th homogeneous component of the
polynomial function $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda
,n}\right)  $, and such that the $\ell$-th homogeneous component of the
polynomial function $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda
,n}\right)  $ is $0$ for all $\ell>k$". Note that this does not imply that
$\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  $ is not
identically zero, and indeed $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\circ}\right)  $ can be identically zero.
\end{remark}

Before we prove Proposition \ref{prop.det.US}, let us show how it completes
the proof of Theorem \ref{thm.invformnondeg}:

\textit{Proof of Theorem \ref{thm.invformnondeg}.} Fix a positive
$n\in\mathbb{N}$. For generic $\lambda$, the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $ (because
$\mathfrak{g}$ is nondegenerate). Thus, for generic $\lambda$, the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must also be nondegenerate
(by Lemma \ref{lem.lambda_k.2}), so that $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. Since the leading term of
the polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
(by Proposition \ref{prop.det.US}), this yields that $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}\right)  \neq0$ for generic $\lambda$. In
other words, the form $\left(  \cdot,\cdot\right)  _{\lambda,n}$ is
nondegenerate for generic $\lambda$. But this form $\left(  \cdot
,\cdot\right)  _{\lambda,n}$ is exactly the restriction of the form $\left(
\cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow
\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times M_{-\lambda}%
^{-}\left[  n\right]  $. Hence, the restriction of the form $\left(
\cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow
\mathbb{C}$ to $M_{\lambda}^{+}\left[  -n\right]  \times M_{-\lambda}%
^{-}\left[  n\right]  $ is nondegenerate for generic $\lambda$. This proves
Theorem \ref{thm.invformnondeg}.

So all that remains to finish the proof of Theorem \ref{thm.invformnondeg} is
verifying Proposition \ref{prop.det.US}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Polynomial maps}

We already defined the notion of a polynomial function in Definition
\ref{def.det.US.poly}. Let us give a definition of a notion of a "polynomial
map" which is tailored for our proof of Theorem \ref{thm.invformnondeg}. I
cannot guarantee that it is the same as what other people call "polynomial
map", but it should be very close.

\begin{definition}
\label{def.det.US.polymap}Let $V$ be a finite-dimensional vector space. Let
$W$ be a vector space. A map $\phi:V\rightarrow W$ is said to be a
\textit{polynomial map} if and only if there exist:

- some $n\in\mathbb{N}$;

- $n$ vectors $w_{1}$, $w_{2}$, $...$, $w_{n}$ in $W$;

- $n$ polynomial functions $P_{1}$, $P_{2}$, $...$, $P_{n}$ from $V$ to
$\mathbb{C}$

such that%
\[
\text{every }v\in V\text{ satisfies }\phi\left(  v\right)  =\sum
\limits_{i=1}^{n}P_{i}\left(  v\right)  w_{i}.
\]

\end{definition}

Note that it is clear that:

\begin{itemize}
\item If $V$ is a finite-dimensional vector space and $W$ is a vector space,
then any $\mathbb{C}$-linear combination of polynomial maps $V\rightarrow W$
is a polynomial map.

\item If $V$ is a finite-dimensional vector space and $W$ is a $\mathbb{C}%
$-algebra, then any product of polynomial maps $V\rightarrow W$ is a
polynomial map.

\item If $V$ is a finite-dimensional vector space, then polynomial maps
$V\rightarrow\mathbb{C}$ are exactly the same as polynomial functions
$V\rightarrow\mathbb{C}$ (since $\mathbb{C}$-linear combinations of polynomial
functions are polynomial functions).
\end{itemize}

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The deformed Lie
algebra $\mathfrak{g}^{\varepsilon}$}

Before we go on, here is a rough plan of how we will attack Proposition
\ref{prop.det.US}:

In order to gain a foothold on $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $, we are going to consider not just one Lie algebra
$\mathfrak{g}$ but a whole family $\left(  \mathfrak{g}^{\varepsilon}\right)
_{\varepsilon\in\mathbb{C}}$ of its "deformations" at the same time. Despite
all of these deformations being isomorphic as Lie algebras with one exception,
they will give us useful information: we will show that the bilinear forms
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ they
induce, in some sense, depend "polynomially" on $\lambda$ and $\varepsilon$.
We will have to restrain from speaking directly of the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ as depending
polynomially on $\lambda$, since this makes no sense (the domain of the
bilinear form $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}$ changes with $\lambda$), but instead we will sample this form
on particular elements of the Verma modules coming from appropriately chosen
Poincar\'{e}-Birkhoff-Witt bases of $U\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $ and $U\left(  \mathfrak{n}_{+}^{\varepsilon}\right)  $. These
sampled values of the form will turn out to depend polynomially on $\lambda$
and $\varepsilon$, and thus the determinant $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\varepsilon}\right)  $ will be a polynomial
function in $\lambda$ and $\varepsilon$. This polynomial function will turn
out to have some kind of "homogeneity with respect to $\lambda$ and
$\varepsilon^{2}$" (this is not a standard notion, but see Corollary
\ref{cor.invformnondeg.polynomiality} for what exactly this means in our
context), so that the leading term of $\lambda$ will be the term with smallest
power of $\varepsilon$ (and, as it will turn out, this will be the power
$\varepsilon^{0}$, so this term will be obtainable by setting $\varepsilon$ to
$0$). Once this all is formalized and proven, we will explicitly show that
(more or less) $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}%
}=\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ (again this does not
literally hold but must be correctly interpreted), and we know the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ to be nondegenerate (by
Lemma \ref{lem.lambda_k.2}), so that the form $\left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{0}}$ will be nondegenerate, and this will quickly
yield the nondegeneracy of $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\varepsilon}\right)  $ for generic $\lambda$ and $\varepsilon$,
and thus the nondegeneracy of $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ for generic $\lambda$.

Now, to the details. Consider the situation of Proposition \ref{prop.det.US}.
In particular, this means that (from now on until the end of Section
\ref{subsect.invform}) the Lie algebra $\mathfrak{g}$ will be assumed nondegenerate.

First, let us define $\left(  \mathfrak{g}^{\varepsilon}\right)
_{\varepsilon\in\mathbb{C}}$.

For every $\varepsilon\in\mathbb{C}$, let us define a new Lie bracket $\left[
\cdot,\cdot\right]  ^{\varepsilon}$ on the vector space $\mathfrak{g}$ by the
formula%
\begin{align}
\left[  x,y\right]  ^{\varepsilon}  &  =\varepsilon\left[  x,y\right]
+\left(  1-\varepsilon\right)  \pi\left(  \left[  x,y\right]  \right)
-\varepsilon\left(  1-\varepsilon\right)  \left[  x,\pi\left(  y\right)
\right]  -\varepsilon\left(  1-\varepsilon\right)  \left[  \pi\left(
x\right)  ,y\right] \label{pf.invformnondeg.g^epsi.1}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }x\in\mathfrak{g}\text{ and }%
y\in\mathfrak{g},\nonumber
\end{align}
where $\pi$ is the canonical projection $\mathfrak{g}\rightarrow
\mathfrak{g}_{0}$. In other words, let us define a new Lie bracket $\left[
\cdot,\cdot\right]  ^{\varepsilon}$ on the vector space $\mathfrak{g}$ by%
\begin{align}
\left[  x,y\right]  ^{\varepsilon}  &  =\varepsilon^{\delta_{n,0}+\delta
_{m,0}+1-\delta_{n+m,0}}\left[  x,y\right]  \label{pf.invformnondeg.g^epsi.2}%
\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z}\text{, }m\in
\mathbb{Z}\text{, }x\in\mathfrak{g}_{n}\text{ and }y\in\mathfrak{g}%
_{m}\nonumber
\end{align}
(note that the right hand side of this equation makes sense since
$1-\delta_{n+m,0}\geq0$ for all $n\in\mathbb{Z}$ and $m\in\mathbb{Z}%
$)\ \ \ \ \footnote{Proving that these two definitions of $\left[  \cdot
,\cdot\right]  ^{\varepsilon}$ are equivalent is completely straightforward:
just assume WLOG that $x$ and $y$ are homogeneous, so that $x\in
\mathfrak{g}_{n}$ and $y\in\mathfrak{g}_{m}$ for $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, and distinguish between the following four cases:
\par
\textit{Case 1:} We have $n=0$ and $m=0$.
\par
\textit{Case 2:} We have $n\neq0$ and $m\neq0$ but $n+m=0$.
\par
\textit{Case 3:} We have $n\neq0$, $m\neq0$ and $n+m\neq0$.
\par
\textit{Case 4:} Exactly one of $n$ and $m$ is $0$.
\par
In Case 1, the assumption that $\mathfrak{g}_{0}$ is abelian must be used.}.
It is easy to prove that this Lie bracket $\left[  \cdot,\cdot\right]
^{\varepsilon}$ is antisymmetric and satisfies the Jacobi
identity\footnote{\textit{Proof.} Antisymmetry is obvious. As for the Jacobi
identity, it can be proven in a straightforward way:
\par
We must show the equality $\left[  x,\left[  y,z\right]  ^{\varepsilon
}\right]  ^{\varepsilon}+\left[  y,\left[  z,x\right]  ^{\varepsilon}\right]
^{\varepsilon}+\left[  z,\left[  x,y\right]  ^{\varepsilon}\right]
^{\varepsilon}=0$ for all $x,y,z\in\mathfrak{g}$. Since this equality is
linear in each of $x$, $y$ and $z$, it is enough to prove it for homogeneous
$x,y,z\in\mathfrak{g}$. So let $x,y,z\in\mathfrak{g}$ be homogeneous. Then,
there exist $n,m,p\in\mathbb{Z}$ such that $x\in\mathfrak{g}_{n}$,
$y\in\mathfrak{g}_{m}$ and $z\in\mathfrak{g}_{p}$. Consider these $n$, $m$ and
$p$. Then, by (\ref{pf.invformnondeg.g^epsi.2}) (applied to $y$, $z$, $m$ and
$p$ instead of $x$, $y$, $n$ and $m$), we have $\left[  y,z\right]
^{\varepsilon}=\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\left[
y,z\right]  $. Thus,%
\begin{align*}
&  \left[  x,\left[  y,z\right]  ^{\varepsilon}\right]  ^{\varepsilon}\\
&  =\left[  x,\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\left[
y,z\right]  \right]  ^{\varepsilon}=\varepsilon^{\delta_{m,0}+\delta
_{p,0}+1-\delta_{m+p,0}}\left[  x,\left[  y,z\right]  \right]  ^{\varepsilon
}\\
&  =\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}}\varepsilon
^{\delta_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because (\ref{pf.invformnondeg.g^epsi.2}) (applied to }\left[
y,z\right]  \text{ and }m+p\text{ instead of }y\text{ and }m\text{) yields}\\
\left[  x,\left[  y,z\right]  \right]  ^{\varepsilon}=\varepsilon
^{\delta_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right]  \text{ (since }\left[  y,z\right]  \in\mathfrak{g}_{m+p}\text{ (since
}y\in\mathfrak{g}_{m}\text{ and }z\in\mathfrak{g}_{p}\text{))}%
\end{array}
\right) \\
&  =\varepsilon^{\delta_{m,0}+\delta_{p,0}+1-\delta_{m+p,0}+\delta
_{n,0}+\delta_{m+p,0}+1-\delta_{n+m+p,0}}\left[  x,\left[  y,z\right]
\right]  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta
_{n+m+p,0}}\left[  x,\left[  y,z\right]  \right]  .
\end{align*}
Similarly,
\begin{align*}
\left[  y,\left[  z,x\right]  ^{\varepsilon}\right]  ^{\varepsilon}  &
=\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  y,\left[  z,x\right]  \right]  \ \ \ \ \ \ \ \ \ \ \text{and}\\
\left[  z,\left[  x,y\right]  ^{\varepsilon}\right]  ^{\varepsilon}  &
=\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  z,\left[  x,y\right]  \right]  .
\end{align*}
Adding up these three equations yields%
\begin{align*}
&  \left[  x,\left[  y,z\right]  ^{\varepsilon}\right]  ^{\varepsilon}+\left[
y,\left[  z,x\right]  ^{\varepsilon}\right]  ^{\varepsilon}+\left[  z,\left[
x,y\right]  ^{\varepsilon}\right]  ^{\varepsilon}\\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\left[  x,\left[  y,z\right]  \right]  +\varepsilon^{\delta_{n,0}%
+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}}\left[  y,\left[  z,x\right]
\right]  +\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta
_{n+m+p,0}}\left[  z,\left[  x,y\right]  \right] \\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+\delta_{p,0}+2-\delta_{n+m+p,0}%
}\underbrace{\left(  \left[  x,\left[  y,z\right]  \right]  +\left[  y,\left[
z,x\right]  \right]  +\left[  z,\left[  x,y\right]  \right]  \right)
}_{=0\text{ (since }\mathfrak{g}\text{ is a Lie algebra)}}=0.
\end{align*}
This proves the Jacobi identity for the Lie bracket $\left[  \cdot
,\cdot\right]  ^{\varepsilon}$, qed.} and is graded. Thus, this Lie bracket
$\left[  \cdot,\cdot\right]  ^{\varepsilon}$ defines a graded Lie algebra
structure on $\mathfrak{g}$. Let us denote this Lie algebra by $\mathfrak{g}%
^{\varepsilon}$. Thus, $\mathfrak{g}^{\varepsilon}$ is identical with
$\mathfrak{g}$ as a vector space, but the Lie bracket on $\mathfrak{g}%
^{\varepsilon}$ is $\left[  \cdot,\cdot\right]  ^{\varepsilon}$ rather than
$\left[  \cdot,\cdot\right]  $.

Trivially, $\mathfrak{g}^{1}=\mathfrak{g}$ (this is an actual equality, not
only an isomorphism) and $\left[  \cdot,\cdot\right]  ^{1}=\left[  \cdot
,\cdot\right]  $.

For every $\varepsilon\in\mathbb{C}$, define a $\mathbb{C}$-linear map
$J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow\mathfrak{g}$ by%
\[
J_{\varepsilon}\left(  x\right)  =\varepsilon^{1+\delta_{n,0}}%
x\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ and }%
x\in\mathfrak{g}_{n}.
\]
Then, $J_{\varepsilon}$ is a Lie algebra homomorphism\footnote{\textit{Proof.}
We must show that $J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon
}\right)  =\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(
y\right)  \right]  $ for all $x,y\in\mathfrak{g}$. In order to show this, it
is enough to prove that $J_{\varepsilon}\left(  \left[  x,y\right]
^{\varepsilon}\right)  =\left[  J_{\varepsilon}\left(  x\right)
,J_{\varepsilon}\left(  y\right)  \right]  $ for all homogeneous
$x,y\in\mathfrak{g}$ (because of linearity). So let $x,y\in\mathfrak{g}$ be
homogeneous. Thus, there exist $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ such that
$x\in\mathfrak{g}_{n}$ and $y\in\mathfrak{g}_{m}$. Consider these $n$ and $m$.
Then, $\left[  x,y\right]  \in\mathfrak{g}_{n+m}$. Now, $J_{\varepsilon
}\left(  x\right)  =\varepsilon^{1+\delta_{n,0}}x$ and $J_{\varepsilon}\left(
y\right)  =\varepsilon^{1+\delta_{m,0}}y$ by the definition of $J_{\varepsilon
}$. Thus,%
\[
\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(  y\right)
\right]  =\left[  \varepsilon^{1+\delta_{n,0}}x,\varepsilon^{1+\delta_{m,0}%
}y\right]  =\varepsilon^{1+\delta_{n,0}}\varepsilon^{1+\delta_{m,0}}\left[
x,y\right]  =\varepsilon^{2+\delta_{n,0}+\delta_{m,0}}\left[  x,y\right]  .
\]
Compared with%
\begin{align*}
J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon}\right)   &
=J_{\varepsilon}\left(  \varepsilon^{\delta_{n,0}+\delta_{m,0}+1-\delta
_{n+m,0}}\left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.invformnondeg.g^epsi.2})}\right) \\
&  =\varepsilon^{\delta_{n,0}+\delta_{m,0}+1-\delta_{n+m,0}}%
\underbrace{J_{\varepsilon}\left(  \left[  x,y\right]  \right)  }%
_{\substack{=\varepsilon^{1+\delta_{n+m,0}}\left[  x,y\right]  \\\text{(by the
definition of }J_{\varepsilon}\text{,}\\\text{since }\left[  x,y\right]
\in\mathfrak{g}_{n+m}\text{)}}}=\varepsilon^{\delta_{n,0}+\delta
_{m,0}+1-\delta_{n+m,0}}\varepsilon^{1+\delta_{n+m,0}}\left[  x,y\right] \\
&  =\varepsilon^{2+\delta_{n,0}+\delta_{m,0}}\left[  x,y\right]  ,
\end{align*}
this yields $J_{\varepsilon}\left(  \left[  x,y\right]  ^{\varepsilon}\right)
=\left[  J_{\varepsilon}\left(  x\right)  ,J_{\varepsilon}\left(  y\right)
\right]  $, qed.}. Also, $J_{\varepsilon}$ is a vector space isomorphism when
$\varepsilon\neq0$. Hence, $J_{\varepsilon}$ is a Lie algebra isomorphism when
$\varepsilon\neq0$. Moreover, $J_{1}=\operatorname*{id}$.

For every $\varepsilon\in\mathbb{C}$, we are going to denote by $\mathfrak{n}%
_{-}^{\varepsilon}$, $\mathfrak{n}_{+}^{\varepsilon}$ and $\mathfrak{h}%
^{\varepsilon}$ the vector spaces $\mathfrak{n}_{-}$, $\mathfrak{n}_{+}$ and
$\mathfrak{h}$ \textbf{as Lie subalgebras of }$\mathfrak{g}^{\varepsilon}$.
Note that $\mathfrak{h}^{\varepsilon}=\mathfrak{h}$ as Lie algebras (because
$\mathfrak{h}$ and $\mathfrak{h}^{\varepsilon}$ are abelian Lie algebras), but
the equalities $\mathfrak{n}_{-}^{\varepsilon}=\mathfrak{n}_{-}$ and
$\mathfrak{n}_{+}^{\varepsilon}=\mathfrak{n}_{+}$ hold only as equalities of
vector spaces (unless we are in some rather special situation). Since the
grading of $\mathfrak{g}^{\varepsilon}$ is the same as the grading of
$\mathfrak{g}$, the triangular decomposition of $\mathfrak{g}^{\varepsilon}$
is $\mathfrak{n}_{-}^{\varepsilon}\oplus\mathfrak{h}^{\varepsilon}%
\oplus\mathfrak{n}_{+}^{\varepsilon}$ for every $\varepsilon\in\mathbb{C}$.

Now, we are dealing with several Lie algebras on the same vector space, and we
are going to be dealing with their Verma modules. In order not to confuse
them, let us introduce a notation:

\begin{Convention}
In the following, whenever $\mathfrak{e}$ is a $\mathbb{Z}$-graded Lie
algebra, and $\lambda\in\mathfrak{e}_{0}^{\ast}$, we are going to denote by
$M_{\lambda}^{+\mathfrak{e}}$ the Verma highest-weight module of $\left(
\mathfrak{e},\lambda\right)  $, and we are going to denote by $M_{\lambda
}^{-\mathfrak{e}}$ the Verma lowest-weight module of $\left(  \mathfrak{e}%
,\lambda\right)  $. We will furthermore denote by $v_{\lambda}^{+\mathfrak{e}%
}$ the defining vector of $M_{\lambda}^{+\mathfrak{e}}$, and we will denote by
$v_{\lambda}^{-\mathfrak{e}}$ the defining vector of $M_{\lambda
}^{-\mathfrak{e}}$.

Further, we denote by $\left(  \cdot,\cdot\right)  _{\lambda}^{\mathfrak{e}}$
and $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{e}}$ the forms
$\left(  \cdot,\cdot\right)  _{\lambda}$ and $\left(  \cdot,\cdot\right)
_{\lambda,n}$ defined for the Lie algebra $\mathfrak{e}$ instead of
$\mathfrak{g}$.
\end{Convention}

Thus, for instance, the Verma highest-weight module of $\left(  \mathfrak{g}%
,\lambda\right)  $ (which we have always denoted by $M_{\lambda}^{+}$) can now
be called $M_{\lambda}^{+\mathfrak{g}}$, and thus can be discerned from the
Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$ of
$\left(  \mathfrak{g}^{\varepsilon},\lambda\right)  $.

\begin{Convention}
\label{conv.invformnondeg.bases}For every $n\in\mathbb{Z}$, let $\left(
e_{n,i}\right)  _{i\in\left\{  1,2,...,m_{n}\right\}  }$ be a basis of the
vector space $\mathfrak{g}_{n}$ (such a basis exists since $\dim\left(
\mathfrak{g}_{n}\right)  <\infty$). Then, $\left(  e_{n,i}\right)  _{\left(
n,i\right)  \in E}$ is a basis of the vector space $\mathfrak{g}$, where
$E=\left\{  \left(  n,i\right)  \ \mid\ n\in\mathbb{Z};\ i\in\left\{
1,2,...,m_{n}\right\}  \right\}  $.

For every integer $n>0$, we have $\dim\left(  \mathfrak{g}_{n}\right)  =m_{n}$
(since $\left(  e_{n,i}\right)  _{i\in\left\{  1,2,...,m_{n}\right\}  }$ is a
basis of the vector space $\mathfrak{g}_{n}$) and $\dim\left(  \mathfrak{g}%
_{-n}\right)  =m_{-n}$ (similarly), so that $m_{n}=\dim\left(  \mathfrak{g}%
_{n}\right)  =\dim\left(  \mathfrak{g}_{-n}\right)  =m_{-n}$. Of course, this
yields that $m_{n}=m_{-n}$ for every integer $n$ (whether positive or not).

We totally order the set $E$ lexicographically. Let $\operatorname*{Seq}E$ be
the set of all finite sequences of elements of $E$. For every $\mathbf{i}%
\in\operatorname*{Seq}E$ and every $\varepsilon\in\mathbb{C}$, we define an
element $e_{\mathbf{i}}^{\varepsilon}$ of $U\left(  \mathfrak{g}^{\varepsilon
}\right)  $ by%
\[
e_{\mathbf{i}}^{\varepsilon}=e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell
},i_{\ell}},\ \ \ \ \ \ \ \ \ \ \text{where we write }\mathbf{i}\text{ in the
form }\left(  \left(  n_{1},i_{1}\right)  ,\left(  n_{2},i_{2}\right)
,...,\left(  n_{\ell},i_{\ell}\right)  \right)  .
\]
For every $\mathbf{i}\in\operatorname*{Seq}E$, we define the \textit{length}
$\operatorname*{len}\mathbf{i}$ of $\mathbf{i}$ to be the number of members of
$\mathbf{i}$ (in other words, we set $\operatorname*{len}\mathbf{i}=\ell$,
where we write $\mathbf{i}$ in the form $\left(  \left(  n_{1},i_{1}\right)
,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
$), and we define the \textit{degree} $\deg\mathbf{i}$ of $\mathbf{i}$ to be
the sum $n_{1}+n_{2}+...+n_{\ell}$, where we write $\mathbf{i}$ in the form
$\left(  \left(  n_{1},i_{1}\right)  ,\left(  n_{2},i_{2}\right)  ,...,\left(
n_{\ell},i_{\ell}\right)  \right)  $. It is clear that $e_{\mathbf{i}%
}^{\varepsilon}\in U\left(  \mathfrak{g}^{\varepsilon}\right)  \left[
\deg\mathbf{i}\right]  $.

Let $\operatorname*{Seq}\nolimits_{+}E$ be the set of all
\textbf{nondecreasing} sequences $\left(  \left(  n_{1},i_{1}\right)  ,\left(
n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
\in\operatorname*{Seq}E$ such that all of $n_{1}$, $n_{2}$, $...$, $n_{\ell}$
are \textbf{positive}. By the Poincar\'{e}-Birkhoff-Witt theorem (applied to
the Lie algebra $\mathfrak{n}_{+}^{\varepsilon}$), the family $\left(
e_{\mathbf{j}}^{\varepsilon}\right)  _{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E}$ is a basis of the vector space $U\left(  \mathfrak{n}%
_{+}^{\varepsilon}\right)  $. Moreover, it is a graded basis, i. e., the
family $\left(  e_{\mathbf{j}}^{\varepsilon}\right)  _{\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ is a basis of the
vector space $U\left(  \mathfrak{n}_{+}^{\varepsilon}\right)  \left[
n\right]  $ for every $n\in\mathbb{Z}$. Hence, $\left(  e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ is a
basis of the vector space $M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[
n\right]  $ for every $n\in\mathbb{Z}$ and $\lambda\in\mathfrak{h}^{\ast}$.

Let $\operatorname*{Seq}\nolimits_{-}E$ be the set of all
\textbf{nonincreasing} sequences $\left(  \left(  n_{1},i_{1}\right)  ,\left(
n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
\in\operatorname*{Seq}E$ such that all of $n_{1}$, $n_{2}$, $...$, $n_{\ell}$
are \textbf{negative}. By the Poincar\'{e}-Birkhoff-Witt theorem (applied to
the Lie algebra $\mathfrak{n}_{-}^{\varepsilon}$), the family $\left(
e_{\mathbf{i}}^{\varepsilon}\right)  _{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E}$ is a basis of the vector space $U\left(  \mathfrak{n}%
_{-}^{\varepsilon}\right)  $. Moreover, it is a graded basis, i. e., the
family $\left(  e_{\mathbf{i}}^{\varepsilon}\right)  _{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ is a basis of the
vector space $U\left(  \mathfrak{n}_{-}^{\varepsilon}\right)  \left[
-n\right]  $ for every $n\in\mathbb{Z}$. Hence, $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ is a
basis of the vector space $M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[
-n\right]  $ for every $n\in\mathbb{Z}$ and $\lambda\in\mathfrak{h}^{\ast}$.

We can define a bijection%
\begin{align*}
E  &  \rightarrow E,\\
\left(  n,i\right)   &  \mapsto\left(  -n,m_{n}+1-i\right)
\end{align*}
(because $m_{n}=m_{-n}$ for every $n\in\mathbb{Z}$). This bijection reverses
the order on $E$. Hence, this bijection canonically induces a bijection
$\operatorname*{Seq}E\rightarrow\operatorname*{Seq}E$, which maps
$\operatorname*{Seq}\nolimits_{+}E$ to $\operatorname*{Seq}\nolimits_{-}E$ and
vice versa, and reverses the degree of every sequence while keeping the length
of every sequence invariant. One consequence of this bijection is that for
every $n\in\mathbb{Z}$, the number of all $\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals the number of all
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ satisfying$\ \deg
\mathbf{i}=-n$. Another consequence is that $\sum\limits_{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}%
\mathbf{i=}\sum\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$.

For every $n\in\mathbb{N}$, we represent the bilinear form $\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}:M_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right]  \times M_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]  \rightarrow\mathbb{C}$ by its
matrix with respect to the bases $\left(  e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\right)  _{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ and $\left(
e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)
_{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg\mathbf{j}=n}$ of
$M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right]  $ and
$M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]  $, respectively.
This is the matrix%
\[
\left(  \left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}.
\]
This matrix is a square matrix (since the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$), and its determinant is what we are going to
denote by $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  $.
\end{Convention}

A few words about tensor algebras:

\begin{Convention}
In the following, we let $T$ denote the tensor algebra functor. Hence, for
every vector space $V$, we denote by $T\left(  V\right)  $ the tensor algebra
of $V$.

We notice that $T\left(  V\right)  $ is canonically graded even if $V$ is not.
In fact, $T\left(  V\right)  =\bigoplus\limits_{i\in\mathbb{N}}V^{\otimes i}$,
so that we get a grading on $T\left(  V\right)  $ if we set $V^{\otimes i}$ to
be the $i$-th graded component of $T\left(  V\right)  $. This grading is
called the \textit{tensor length grading} on $T\left(  V\right)  $. It makes
$T\left(  V\right)  $ concentrated in nonnegative degrees.

If $V$ itself is a graded vector space, then we can also grade $T\left(
V\right)  $ by canonically extending the grading on $V$ to $T\left(  V\right)
$ (this means that the degree of a pure tensor is the sum of the degrees of
its tensorands). This grading is called the \textit{internal grading} on
$T\left(  V\right)  $. It is different from the tensor length grading (unless
$V$ is concentrated in degree $1$).

Hence, if $V$ is a graded vector space, then $T\left(  V\right)  $ becomes a
bigraded vector space (i. e., a vector space with two gradings). Let us agree
to denote by $T\left(  V\right)  \left[  n,m\right]  $ the intersection of the
$n$-th graded component in the internal grading with the $m$-th graded
component in the tensor length grading (i. e., with $V^{\otimes m}$).
\end{Convention}

Let us notice that \textbf{as vector spaces}, we have $\mathfrak{g}%
=\mathfrak{g}^{\varepsilon}$, $\mathfrak{n}_{-}=\mathfrak{n}_{-}^{\varepsilon
}$, $\mathfrak{n}_{+}=\mathfrak{n}_{+}^{\varepsilon}$ and $\mathfrak{h}%
=\mathfrak{h}^{\varepsilon}$ for every $\varepsilon\in\mathbb{C}$. Hence,
$T\left(  \mathfrak{g}\right)  =T\left(  \mathfrak{g}^{\varepsilon}\right)  $,
$T\left(  \mathfrak{n}_{-}\right)  =T\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $, $T\left(  \mathfrak{n}_{+}\right)  =T\left(  \mathfrak{n}%
_{+}^{\varepsilon}\right)  $ and $T\left(  \mathfrak{h}\right)  =T\left(
\mathfrak{h}^{\varepsilon}\right)  $.

\begin{definition}
In the following, for every Lie algebra $\mathfrak{a}$ and every element $x\in
T\left(  \mathfrak{a}\right)  $, we denote by $\operatorname*{env}%
\nolimits_{\mathfrak{a}}x$ the projection of $x$ onto the factor algebra
$U\left(  \mathfrak{a}\right)  $ of $T\left(  \mathfrak{a}\right)  $.
\end{definition}

Let us again stress that $T\left(  \mathfrak{g}\right)  =T\left(
\mathfrak{g}^{\varepsilon}\right)  $, so that $T\left(  \mathfrak{g}%
^{\varepsilon}\right)  $ does not depend on $\varepsilon$, whereas $U\left(
\mathfrak{g}^{\varepsilon}\right)  $ does. Hence, if we want to study the form
$\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{\varepsilon}}$ as it
changes with $\varepsilon$, the easiest thing to do is to study the values of
$\left(  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}a\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}b\right)
v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}$ for fixed $a\in T\left(  \mathfrak{g}\right)  =T\left(
\mathfrak{g}^{\varepsilon}\right)  $ and $b\in T\left(  \mathfrak{g}\right)
=T\left(  \mathfrak{g}^{\varepsilon}\right)  $. Here is the polynomiality
lemma that we want to have:

\begin{lemma}
\label{lem.invformnondeg.polynomiality}Let $\mathbf{i}\in\operatorname*{Seq}E$
and $\mathbf{j}\in\operatorname*{Seq}E$. Then, there exists a polynomial
function $Q_{\mathbf{i},\mathbf{j}}:\mathfrak{h}^{\ast}\times\mathbb{C}%
\rightarrow\mathbb{C}$ such that every $\lambda\in\mathfrak{h}^{\ast}$ and
every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  .
\]

\end{lemma}

To prove this lemma, we show something more general:

\begin{lemma}
\label{lem.invformnondeg.polynomiality2}For every $n\in\mathbb{Z}$ and $c\in
T\left(  \mathfrak{g}\right)  \left[  n\right]  $, there exists a polynomial
map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ such that every $\lambda\in\mathfrak{h}%
^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]

\end{lemma}

To get some intuition about Lemma \ref{lem.invformnondeg.polynomiality2},
recall that the Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ was defined as $U\left(  \mathfrak{g}^{\varepsilon}\right)
\otimes_{U\left(  \mathfrak{h}^{\varepsilon}\oplus\mathfrak{n}_{+}%
^{\varepsilon}\right)  }\mathbb{C}_{\lambda}$, but turned out to be $U\left(
\mathfrak{n}_{-}^{\varepsilon}\right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ (as a vector space), so that every term of the form
$xv_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$ with $x\in U\left(
\mathfrak{g}^{\varepsilon}\right)  $ can be reduced to the form $yv_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}$ with $y\in U\left(  \mathfrak{n}%
_{-}^{\varepsilon}\right)  $. Lemma \ref{lem.invformnondeg.polynomiality2}
says that, if $x$ is given as the projection $\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}c$ of some tensor $c\in T\left(
\mathfrak{g}\right)  \left[  n\right]  $ onto $U\left(  \mathfrak{g}%
^{\varepsilon}\right)  $, then $y$ can be found as the projection of some
tensor $d\left(  \lambda,\varepsilon\right)  \in T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ onto $U\left(  \mathfrak{n}_{-}^{\varepsilon
}\right)  $ which depends polynomially on $\lambda$ and $\varepsilon$. This is
not particularly surprising, since $y$ is found from $x$ by picking a
tensorial representation\footnote{By a "tensorial representation"\ of $x$, I
mean a tensor $c\in T\left(  \mathfrak{g}\right)  $ such that
$\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c=x$.} of $x$ and
"gradually" stratifying it\footnote{By "stratifying" a tensorial
representation of $x$, I mean writing it as a linear combination of pure
tensors, and whenever such a pure tensor has a negative tensorand (i. e., a
tensorand in $\mathfrak{n}_{-}$) standing directly before a positive tensorand
(i. e., a tensorand in $\mathfrak{n}_{+}$), applying the $xy-yx=\left[
x,y\right]  ^{\varepsilon}$ relations in $U\left(  \mathfrak{g}^{\varepsilon
}\right)  $ to move the negative tensorand past the positive one. As soon as a
positive tensorand hits the right end of the tensor, the tensor can be thrown
away since $\mathfrak{n}_{+}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$. For
instance, in Example \ref{exa.Vir} further below, we compute $L_{1}%
L_{-1}v_{\lambda}^{+}$ by stratifying the tensorial representation
$L_{1}\otimes L_{-1}$ of $L_{1}L_{-1}$, and we compute $L_{1}^{2}L_{-1}%
^{2}v_{\lambda}^{+}$ by stratifying the tensorial representation $L_{1}\otimes
L_{1}\otimes L_{-1}\otimes L_{-1}$ of $L_{1}^{2}L_{-1}^{2}$.}, and the
$\lambda$'s and $\varepsilon$'s which appear during this stratification
process don't appear "randomly", but rather appear at foreseeable places. The
following proof of Lemma \ref{lem.invformnondeg.polynomiality2} will formalize
this idea.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality2}.} First some notations:

If $n\in\mathbb{Z}$, then a tensor $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is said to be $n$\textit{-stratifiable} if there exists a
polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ such that every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]


Lemma \ref{lem.invformnondeg.polynomiality2} states that for every
$n\in\mathbb{Z}$, every tensor $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable.

We will now prove that
\begin{equation}
\text{for every }n\in\mathbb{Z}\text{ and every }m\in\mathbb{N}\text{, every
tensor }c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  \text{ is
}n\text{-stratifiable.} \label{lem.invformnondeg.polynomiality2.ind}%
\end{equation}


Before we start proving this, let us formulate two easy observations about
stratifiable tensors:

\textit{Observation 1:} For any fixed $n$, any $\mathbb{C}$-linear combination
of $n$-stratifiable tensors is $n$-stratifiable. (In fact, we can just take
the corresponding $\mathbb{C}$-linear combination of the corresponding
polynomial maps $d$.)

\textit{Observation 2:} If an integer $n$, a negative integer $\nu$, a vector
$x\in\mathfrak{g}_{\nu}$ and a tensor $y\in T\left(  \mathfrak{g}\right)
\left[  n-\nu\right]  $ are such that $y$ is $\left(  n-\nu\right)
$-stratifiable, then $x\otimes y\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable.\footnote{\textit{Proof of Observation 2.} Let
an integer $n$, a negative integer $\nu$, a vector $x\in\mathfrak{g}_{\nu}$
and a tensor $y\in T\left(  \mathfrak{g}\right)  \left[  n-\nu\right]  $ be
such that $y$ is $\left(  n-\nu\right)  $-stratifiable. Then, there exists a
polynomial map $\widetilde{d}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow
T\left(  \mathfrak{n}_{-}\right)  \left[  n-\nu\right]  $ such that every
$\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}y\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}%
\]
(by the definition of "$\left(  n-\nu\right)  $-stratifiable"). Now, define a
map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ by
\[
d\left(  \lambda,\varepsilon\right)  =x\otimes\widetilde{d}\left(
\lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(
\lambda,\varepsilon\right)  \in\mathfrak{h}^{\ast}\times\mathbb{C}.
\]
(This is well-defined, since $x\in\mathfrak{g}_{\nu}\subseteq\mathfrak{n}_{-}$
(since $\nu$ is negative).) This map $d$ is clearly polynomial (since
$\widetilde{d}$ is a polynomial map), and every $\lambda\in\mathfrak{h}^{\ast
}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\begin{align*}
\underbrace{\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x\otimes y\right)  \right)  }_{=x\cdot\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}y}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}  &  =x\cdot\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}y\right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}_{=\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}%
=\underbrace{x\cdot\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  }_{=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon
}}\left(  x\otimes\widetilde{d}\left(  \lambda,\varepsilon\right)  \right)
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  x\otimes\widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  }_{=d\left(  \lambda,\varepsilon\right)  }\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\end{align*}
Hence, $x\otimes y$ is $n$-stratifiable (by the definition of "$n$%
-stratifiable"). This proves Observation 2.}

We are now going to prove (\ref{lem.invformnondeg.polynomiality2.ind}) by
induction on $m$:

\textit{Induction base:} We have $T\left(  \mathfrak{g}\right)  \left[
n,0\right]  =\mathbb{C}\left[  n\right]  $. Hence, every tensor $c\in T\left(
\mathfrak{g}\right)  \left[  n,0\right]  $ is $n$-stratifiable (because we can
define the polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow
T\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =c\ \ \ \ \ \ \ \ \ \ \text{for all
}\left(  \lambda,\varepsilon\right)  \in\mathfrak{h}^{\ast}\times\mathbb{C}%
\]
). In other words, (\ref{lem.invformnondeg.polynomiality2.ind}) is proven for
$m=0$. In other words, the induction base is complete.

\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. We must show that
(\ref{lem.invformnondeg.polynomiality2.ind}) holds for this $m$, using the
assumption that (\ref{lem.invformnondeg.polynomiality2.ind}) holds for $m-1$
instead of $m$.

Let $n\in\mathbb{Z}$. Let $\pi_{n}:T\left(  \mathfrak{g}\right)  \rightarrow
T\left(  \mathfrak{g}\right)  \left[  n\right]  $ denote the canonical
projection of $T\left(  \mathfrak{g}\right)  $ to the $n$-th graded component
with respect to the internal grading.

Let $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $. We must prove
that $c$ is $n$-stratifiable.

We have $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  \subseteq
\mathfrak{g}^{\otimes m}$, and since the $m$-th tensor power is generated by
pure tensors, this yields that $c$ is a $\mathbb{C}$-linear combination of
pure tensors. In other words, $c$ is a $\mathbb{C}$-linear combination of
finitely many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes
x_{m}$ with $x_{1},x_{2},...,x_{m}\in\mathfrak{g}$. We can WLOG assume that,
in each of these pure tensors, the elements $x_{1},x_{2},...,x_{m}$ are
homogeneous (since otherwise we can break each of $x_{1},x_{2},...,x_{m}$ into
homogeneous components, and thus the pure tensors $x_{1}\otimes x_{2}%
\otimes...\otimes x_{m}$ break into smaller pieces which are still pure
tensors). So we can write $c$ as a $\mathbb{C}$-linear combination of finitely
many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
\textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$. If we apply the
projection $\pi_{n}$ to this, then $c$ remains invariant (since $c\in T\left(
\mathfrak{g}\right)  \left[  n,m\right]  \subseteq T\left(  \mathfrak{g}%
\right)  \left[  n\right]  $), and the terms of the form $x_{1}\otimes
x_{2}\otimes...\otimes x_{m}$ with \textbf{homogeneous }$x_{1},x_{2}%
,...,x_{m}\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$ remain invariant as well
(since they also lie in $T\left(  \mathfrak{g}\right)  \left[  n\right]  $),
whereas the terms of the form $x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
\textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ satisfying
$\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)  +...+\deg\left(
x_{m}\right)  \neq n$ are mapped to $0$ (since they lie in graded components
of $T\left(  \mathfrak{g}\right)  $ other than $T\left(  \mathfrak{g}\right)
\left[  n\right]  $). Hence, we write $c$ as a $\mathbb{C}$-linear combination
of finitely many pure tensors of the form $x_{1}\otimes x_{2}\otimes...\otimes
x_{m}$ with \textbf{homogeneous }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$
\textbf{satisfying} $\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)
+...+\deg\left(  x_{m}\right)  =n$.

Therefore, in proving (\ref{lem.invformnondeg.polynomiality2.ind}), we can
WLOG assume that $c$ \textbf{is} a pure tensor of the form $x_{1}\otimes
x_{2}\otimes...\otimes x_{m}$ with homogeneous\textbf{ }$x_{1},x_{2}%
,...,x_{m}\in\mathfrak{g}$ satisfying $\deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  =n$ (because, clearly, once Lemma
\ref{lem.invformnondeg.polynomiality2} is proven for certain values of $c\in
T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $, it must clearly also hold
for all their $\mathbb{C}$-linear combinations\footnote{due to Observation
1}). Let us now assume this.

So we have $c=x_{1}\otimes x_{2}\otimes...\otimes x_{m}$ with
homogeneous\textbf{ }$x_{1},x_{2},...,x_{m}\in\mathfrak{g}$ satisfying
$\deg\left(  x_{1}\right)  +\deg\left(  x_{2}\right)  +...+\deg\left(
x_{m}\right)  =n$. We must now prove that $c$ is $n$-stratifiable.

For every $i\in\left\{  1,2,...,m\right\}  $, let $n_{i}$ be the degree of
$x_{i}$ (this is well-defined since $x_{i}$ is homogeneous). Thus, $x_{i}%
\in\mathfrak{g}_{n_{i}}$.

We have%
\[
\deg\left(  x_{2}\right)  +\deg\left(  x_{3}\right)  +...+\deg\left(
x_{m}\right)  =\underbrace{\left(  \deg\left(  x_{1}\right)  +\deg\left(
x_{2}\right)  +...+\deg\left(  x_{m}\right)  \right)  }_{=n}-\underbrace{\deg
\left(  x_{1}\right)  }_{=n_{1}}=n-n_{1},
\]
so that $x_{2}\otimes x_{3}\otimes...\otimes x_{m}\in T\left(  \mathfrak{g}%
\right)  \left[  n-n_{1}\right]  $ and thus $x_{2}\otimes x_{3}\otimes
...\otimes x_{m}\in T\left(  \mathfrak{g}\right)  \left[  n-n_{1},m-1\right]
$. Since we have assumed that (\ref{lem.invformnondeg.polynomiality2.ind})
holds for $m-1$ instead of $m$, we can thus apply
(\ref{lem.invformnondeg.polynomiality2.ind}) to $n-n_{1}$, $m-1$ and
$x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ instead of $n$, $m$ and $c$. We
conclude that $x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ is $\left(
n-n_{1}\right)  $-stratifiable. In other words, there exists a polynomial map
$\widetilde{d}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n-n_{1}\right]  $ such that every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\]


We notice that $c=x_{1}\otimes x_{2}\otimes...\otimes x_{m}$, so that%
\begin{align}
&  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\nonumber\\
&  =x_{1}x_{2}...x_{m}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\underbrace{\left(  x_{2}x_{3}...x_{i-1}x_{i}\cdot
x_{1}\cdot x_{i+1}x_{i+2}...x_{m}-x_{2}x_{3}...x_{i}x_{i+1}\cdot x_{1}\cdot
x_{i+2}x_{i+3}...x_{m}\right)  }_{=x_{2}x_{3}...x_{i-1}x_{i}\left(
x_{1}x_{i+1}-x_{i+1}x_{1}\right)  x_{i+2}x_{i+3}...x_{m}}+x_{2}x_{3}%
...x_{m}\cdot x_{1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the sum }\sum\limits_{i=1}^{m-1}\left(  x_{2}x_{3}...x_{i-1}%
x_{i}\cdot x_{1}\cdot x_{i+1}x_{i+2}...x_{m}-x_{2}x_{3}...x_{i}x_{i+1}\cdot
x_{1}\cdot x_{i+2}x_{i+3}...x_{m}\right) \\
\text{telescopes to }x_{1}x_{2}...x_{m}-x_{2}x_{3}...x_{m}\cdot x_{1}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{i=1}^{m-1}x_{2}x_{3}...x_{i-1}x_{i}\underbrace{\left(
x_{1}x_{i+1}-x_{i+1}x_{1}\right)  }_{\substack{=\left[  x_{1},x_{i+1}\right]
^{\varepsilon}\\\text{(since we are in }U\left(  \mathfrak{g}^{\varepsilon
}\right)  \text{)}}}x_{i+2}x_{i+3}...x_{m}+x_{2}x_{3}...x_{m}\cdot
x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}x_{2}x_{3}...x_{i-1}x_{i}\underbrace{\left[
x_{1},x_{i+1}\right]  ^{\varepsilon}}_{\substack{=\varepsilon^{\delta
_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\left[  x_{1}%
,x_{i+1}\right]  \\\text{(by (\ref{pf.invformnondeg.g^epsi.2}) (applied to
}x_{1}\text{ and }x_{i+1}\text{ instead of }x\text{ and }y\text{),}%
\\\text{since }x_{1}\in\mathfrak{g}_{n_{1}}\text{ and }x_{i+1}\in
\mathfrak{g}_{n_{i+1}}\text{)}}}x_{i+2}x_{i+3}...x_{m}+x_{2}x_{3}...x_{m}\cdot
x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{x_{2}x_{3}...x_{i-1}x_{i}\left[
x_{1},x_{i+1}\right]  x_{i+2}x_{i+3}...x_{m}}_{=\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[  x_{1},x_{i+1}\right]
\otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{x_{2}x_{3}...x_{m}}_{=\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{m}\right)  }\cdot x_{1}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{i-1}\otimes
x_{i}\otimes\left[  x_{1},x_{i+1}\right]  \otimes x_{i+2}\otimes
x_{i+3}\otimes...\otimes x_{m}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
x_{1}. \label{pf.invformnondeg.polynomiality2.big}%
\end{align}


Now, for every $i\in\left\{  1,2,...,m-1\right\}  $, denote the element
$x_{2}\otimes x_{3}\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[
x_{1},x_{i+1}\right]  \otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}$
by $c_{i}$. It is easily seen that $c_{i}\in T\left(  \mathfrak{g}\right)
\left[  n,m-1\right]  $. Since \newline$c_{i}=x_{2}\otimes x_{3}%
\otimes...\otimes x_{i-1}\otimes x_{i}\otimes\left[  x_{1},x_{i+1}\right]
\otimes x_{i+2}\otimes x_{i+3}\otimes...\otimes x_{m}$, the equality
(\ref{pf.invformnondeg.polynomiality2.big}) rewrites as%
\begin{align}
&  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  c_{i}\right)  +\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes x_{3}%
\otimes...\otimes x_{m}\right)  \cdot x_{1}.
\label{pf.invformnondeg.polynomiality2.small}%
\end{align}


For every $i\in\left\{  1,2,...,m-1\right\}  $, we can apply
(\ref{lem.invformnondeg.polynomiality2.ind}) to $m-1$ and $c_{i}$ instead of
$m$ and $c$ (since $c_{i}\in T\left(  \mathfrak{g}\right)  \left[
n,m-1\right]  $, and since we have assumed that
(\ref{lem.invformnondeg.polynomiality2.ind}) holds for $m-1$ instead of $m$).
We conclude that $c_{i}$ is $n$-stratifiable for every $i\in\left\{
1,2,...,m-1\right\}  $. In other words, for every $i\in\left\{
1,2,...,m-1\right\}  $, there exists a polynomial map $\widetilde{d_{i}%
}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}%
_{-}\right)  \left[  n\right]  $ such that every $\lambda\in\mathfrak{h}%
^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\[
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
c_{i}\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)  \right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\]


We now distinguish between three cases:

\textit{Case 1:} We have $n_{1}>0$.

\textit{Case 2:} We have $n_{1}=0$.

\textit{Case 3:} We have $n_{1}<0$.

First, let us consider Case 1. In this case, $n_{1}>0$. Thus, $x_{1}%
\in\mathfrak{n}_{+}$ (since $x_{1}\in\mathfrak{g}_{n_{1}}$), so that
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in\mathfrak{n}_{+}%
^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$ and thus
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=0$. Now,
(\ref{pf.invformnondeg.polynomiality2.small}) yields%
\begin{align}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}%
+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)
+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \cdot x_{1}\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
\underbrace{x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=0}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality2.small1}%
\end{align}
If we define a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =\sum\limits_{i=1}^{m-1}\varepsilon
^{\delta_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}%
}\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \lambda,\varepsilon\right)
\in\mathfrak{h}^{\ast}\times\mathbb{C},
\]
then this map $d$ is polynomial (since $\widetilde{d_{i}}$ are polynomial maps
for all $i$), and (\ref{pf.invformnondeg.polynomiality2.small1}) becomes%
\begin{align*}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1}%
,0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  }_{=d\left(  \lambda,\varepsilon\right)
}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\end{align*}
Hence, $c$ is $n$-stratifiable (by the definition of "$n$-stratifiable").

Next, let us consider Case 2. In this case, $n_{1}=0$. Thus, $x_{1}%
\in\mathfrak{h}$ (since $x_{1}\in\mathfrak{g}_{n_{1}}$), so that
$x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\lambda\left(  x_{1}\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}$. Now,
(\ref{pf.invformnondeg.polynomiality2.small}) yields%
\begin{align}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}%
+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)
+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \cdot x_{1}\right)  v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\underbrace{\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  c_{i}\right)  \right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}+\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\left(  x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  \cdot
\underbrace{x_{1}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}_{=\lambda\left(
x_{1}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\lambda\left(  x_{1}\right)  \underbrace{\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  x_{2}\otimes
x_{3}\otimes...\otimes x_{m}\right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}}_{=\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  \widetilde{d}\left(  \lambda,\varepsilon\right)
\right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}}\nonumber\\
&  =\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}+\lambda\left(  x_{1}\right)  \left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\nonumber\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
\sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1},0}+\delta_{n_{i+1}%
,0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(  \lambda,\varepsilon
\right)  +\lambda\left(  x_{1}\right)  \widetilde{d}\left(  \lambda
,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}. \label{pf.invformnondeg.polynomiality2.small2}%
\end{align}
If we define a map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  $ by%
\[
d\left(  \lambda,\varepsilon\right)  =\sum\limits_{i=1}^{m-1}\varepsilon
^{\delta_{n_{1},0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}%
}\widetilde{d_{i}}\left(  \lambda,\varepsilon\right)  +\lambda\left(
x_{1}\right)  \widetilde{d}\left(  \lambda,\varepsilon\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \lambda,\varepsilon\right)
\in\mathfrak{h}^{\ast}\times\mathbb{C}%
\]
(this map is well-defined, since $\widetilde{d}\left(  \lambda,\varepsilon
\right)  \in T\left(  \mathfrak{n}_{-}\right)  \left[  n-n_{1}\right]
=T\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $ (due to $n_{1}=0$)),
then this map $d$ is polynomial (since $\widetilde{d_{i}}$ are polynomial maps
for all $i$, and since $\widetilde{d}$ is polynomial), and
(\ref{pf.invformnondeg.polynomiality2.small2}) becomes%
\begin{align*}
&  \left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\
&  =\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}\underbrace{\left(  \sum\limits_{i=1}^{m-1}\varepsilon^{\delta_{n_{1}%
,0}+\delta_{n_{i+1},0}+1-\delta_{n_{1}+n_{i+1},0}}\widetilde{d_{i}}\left(
\lambda,\varepsilon\right)  +\lambda\left(  x_{1}\right)  \widetilde{d}\left(
\lambda,\varepsilon\right)  \right)  }_{=d\left(  \lambda,\varepsilon\right)
}\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}.
\end{align*}
Hence, $c$ is $n$-stratifiable (by the definition of "$n$-stratifiable").

Now, let us consider Case 3. In this case, $n_{1}<0$. Thus, we can apply
Observation 2 to $x_{1}$, $x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ and
$n_{1}$ instead of $x$, $y$ and $\nu$, and conclude that $x_{1}\otimes\left(
x_{2}\otimes x_{3}\otimes...\otimes x_{m}\right)  $ is $n$-stratifiable (since
$x_{2}\otimes x_{3}\otimes...\otimes x_{m}$ is $\left(  n-n_{1}\right)
$-stratifiable). Since $x_{1}\otimes\left(  x_{2}\otimes x_{3}\otimes
...\otimes x_{m}\right)  =x_{1}\otimes x_{2}\otimes...\otimes x_{m}=c$, this
shows that $c$ is $n$-stratifiable.

Hence, in each of the cases 1, 2 and 3, we have shown that $c$ is
$n$-stratifiable. Thus, $c$ is always $n$-stratifiable.

Forget that we fixed $c$. We thus have shown that $c$ is $n$-stratifiable for
every tensor $c\in T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $. In
other words, we have proven (\ref{lem.invformnondeg.polynomiality2.ind}) for
our $m$. This completes the induction step.

Thus, (\ref{lem.invformnondeg.polynomiality2.ind}) is proven by induction.

Now, let $n\in\mathbb{Z}$. Then, every $c\in T\left(  \mathfrak{g}\right)
\left[  n\right]  $ is a $\mathbb{C}$-linear combination of elements of
$T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $ for varying
$m\in\mathbb{N}$ (since $T\left(  \mathfrak{g}\right)  \left[  n\right]
=\bigoplus\limits_{m\in\mathbb{N}}T\left(  \mathfrak{g}\right)  \left[
n,m\right]  $), and thus every $c\in T\left(  \mathfrak{g}\right)  \left[
n\right]  $ is $n$-stratifiable (since
(\ref{lem.invformnondeg.polynomiality2.ind}) shows that every element of
$T\left(  \mathfrak{g}\right)  \left[  n,m\right]  $ is $n$-stratifiable, and
due to Observation 1).

Now forget that we fixed $n$. We have thus proven that for every
$n\in\mathbb{Z}$, every $c\in T\left(  \mathfrak{g}\right)  \left[  n\right]
$ is $n$-stratifiable. In other words, we have proved Lemma
\ref{lem.invformnondeg.polynomiality2}.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality}.} We have
$e_{\mathbf{i}}^{\varepsilon}\in U\left(  \mathfrak{g}^{\varepsilon}\right)
\left[  \deg\mathbf{i}\right]  $ and thus $e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{i}\right]  $. Similarly, $e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\in M_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\left[  \deg\mathbf{j}\right]  $. Hence, if
$\deg\mathbf{i}+\deg\mathbf{j}\neq0$, then $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}\in\left(  M_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{i}\right]  ,M_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\left[  \deg\mathbf{j}\right]  \right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}=0$ (because the form $\left(  \cdot
,\cdot\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}$ is of degree $0$,
while $\deg\mathbf{i}+\deg\mathbf{j}\neq0$) and thus $\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
}^{\mathfrak{g}^{\varepsilon}}=0$. Thus, if $\deg\mathbf{i}+\deg\mathbf{j}%
\neq0$, then Lemma \ref{lem.invformnondeg.polynomiality} trivially holds
(because we can then just take $Q_{\mathbf{i},\mathbf{j}}=0$). Thus, for the
rest of the proof of Lemma \ref{lem.invformnondeg.polynomiality}, we can WLOG
assume that we \textit{don't} have $\deg\mathbf{i}+\deg\mathbf{j}\neq0$.
Hence, we have $\deg\mathbf{i}+\deg\mathbf{j}=0$.

Write the sequence $\mathbf{j}$ in the form $\left(  \left(  m_{1}%
,j_{1}\right)  ,\left(  m_{2},j_{2}\right)  ,...,\left(  m_{k},j_{k}\right)
\right)  $. Then, $e_{\mathbf{j}}^{\varepsilon}=e_{m_{1},j_{1}}e_{m_{2},j_{2}%
}...e_{m_{k},j_{k}}$ and $\deg\mathbf{j}=m_{1}+m_{2}+...+m_{k}=m_{k}%
+m_{k-1}+...+m_{1}$.

Since $e_{\mathbf{j}}^{\varepsilon}=e_{m_{1},j_{1}}e_{m_{2},j_{2}}%
...e_{m_{k},j_{k}}$, we have%
\begin{align}
&  \left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\nonumber\\
&  =\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}},e_{m_{1},j_{1}}e_{m_{2},j_{2}}...e_{m_{k},j_{k}}v_{-\lambda
}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon
}}=\left(  -1\right)  ^{k}\left(  e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}v_{\lambda}%
^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we applied the }\mathfrak{g}%
^{\varepsilon}\text{-invariance of the form }\left(  \cdot,\cdot\right)
_{\lambda}^{\mathfrak{g}^{\varepsilon}}\text{ for a total of }k\text{
times}\right) \nonumber\\
&  =\left(  \left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}v_{\lambda}%
^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.1}%
\end{align}


Write the sequence $\mathbf{i}$ in the form $\left(  \left(  n_{1}%
,i_{1}\right)  ,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell
}\right)  \right)  $. Then, $e_{\mathbf{i}}^{\varepsilon}=e_{n_{1},i_{1}%
}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ and $\deg\mathbf{i}=n_{1}%
+n_{2}+...+n_{\ell}$. Now,%
\begin{align}
&  \left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1},j_{1}%
}\cdot\underbrace{e_{\mathbf{i}}^{\varepsilon}}_{=e_{n_{1},i_{1}}%
e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}}\nonumber\\
&  =\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1}%
,j_{1}}\cdot e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\nonumber\\
&  =\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  \left(
-1\right)  ^{k}e_{m_{k},j_{k}}\otimes e_{m_{k-1},j_{k-1}}\otimes...\otimes
e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}}\otimes e_{n_{2},i_{2}}\otimes...\otimes
e_{n_{\ell},i_{\ell}}\right)  . \label{pf.invformnondeg.polynomiality.2}%
\end{align}
Denote the tensor $\left(  -1\right)  ^{k}e_{m_{k},j_{k}}\otimes
e_{m_{k-1},j_{k-1}}\otimes...\otimes e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}%
}\otimes e_{n_{2},i_{2}}\otimes...\otimes e_{n_{\ell},i_{\ell}}$ by $c$. Then,
(\ref{pf.invformnondeg.polynomiality.2}) rewrites as%
\begin{equation}
\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}}...e_{m_{1},j_{1}%
}\cdot e_{\mathbf{i}}^{\varepsilon}=\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}c. \label{pf.invformnondeg.polynomiality.3}%
\end{equation}


Since
\begin{align*}
c  &  =\left(  -1\right)  ^{k}e_{m_{k},j_{k}}\otimes e_{m_{k-1},j_{k-1}%
}\otimes...\otimes e_{m_{1},j_{1}}\otimes e_{n_{1},i_{1}}\otimes
e_{n_{2},i_{2}}\otimes...\otimes e_{n_{\ell},i_{\ell}}\\
&  \in T\left(  \mathfrak{g}\right)  \left[  m_{k}+m_{k-1}+...+m_{1}%
+n_{1}+n_{2}+...+n_{\ell}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }e_{m_{k},j_{k}}\in\mathfrak{g}_{m_{k}}\text{, }e_{m_{k-1}%
,j_{k-1}}\in\mathfrak{g}_{m_{k-1}}\text{, }...\text{, }e_{m_{1},j_{1}}%
\in\mathfrak{g}_{m_{1}}\\
\text{and }e_{n_{1},i_{1}}\in\mathfrak{g}_{n_{1}}\text{, }e_{n_{2},i_{2}}%
\in\mathfrak{g}_{n_{2}}\text{, }...\text{, }e_{n_{\ell},i_{\ell}}%
\in\mathfrak{g}_{n_{\ell}}%
\end{array}
\right) \\
&  =T\left(  \mathfrak{g}\right)  \left[  0\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{m_{k}+m_{k-1}%
+...+m_{1}}_{=\deg\mathbf{j}}+\underbrace{n_{1}+n_{2}+...+n_{\ell}}%
_{=\deg\mathbf{i}}=\deg\mathbf{j}+\deg\mathbf{i}=\deg\mathbf{i}+\deg
\mathbf{j}=0\right)  ,
\end{align*}
we can apply Lemma \ref{lem.invformnondeg.polynomiality2} to $n=0$. We
conclude that there exists a polynomial map $d:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow T\left(  \mathfrak{n}_{-}\right)  \left[
0\right]  $ such that every $\lambda\in\mathfrak{h}^{\ast}$ and every
$\varepsilon\in\mathbb{C}$ satisfy%
\begin{equation}
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}=\left(  \operatorname*{env}%
\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon
\right)  \right)  \right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.4}%
\end{equation}
Since $T\left(  \mathfrak{n}_{-}\right)  \left[  0\right]  =\mathbb{C}$
(because $\mathfrak{n}_{-}$ is concentrated in negative degrees), this
polynomial map $d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow T\left(
\mathfrak{n}_{-}\right)  \left[  0\right]  $ is a polynomial function
$d:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$. Denote this
function $d$ by $Q_{\mathbf{i},\mathbf{j}}$. Then, every $\lambda
\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy $d\left(
\lambda,\varepsilon\right)  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda
,\varepsilon\right)  $ and thus $\operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}\left(  d\left(  \lambda,\varepsilon\right)  \right)
=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  $ (since
$Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \in\mathbb{C}$).
Thus, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon
\in\mathbb{C}$ satisfy%
\begin{align}
\left(  \operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}c\right)
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}  &  =\underbrace{\left(
\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}}\left(  d\left(
\lambda,\varepsilon\right)  \right)  \right)  }_{=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  }v_{\lambda}^{+\mathfrak{g}^{\varepsilon
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.invformnondeg.polynomiality.4}%
)}\right) \nonumber\\
&  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \cdot
v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}.
\label{pf.invformnondeg.polynomiality.5}%
\end{align}


Now, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon\in
\mathbb{C}$ satisfy%
\begin{align*}
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}  &  =\left(
\underbrace{\left(  -1\right)  ^{k}e_{m_{k},j_{k}}e_{m_{k-1},j_{k-1}%
}...e_{m_{1},j_{1}}\cdot e_{\mathbf{i}}^{\varepsilon}}%
_{\substack{=\operatorname*{env}\nolimits_{\mathfrak{g}^{\varepsilon}%
}c\\\text{(by (\ref{pf.invformnondeg.polynomiality.3}))}}}v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.invformnondeg.polynomiality.1})}\right) \\
&  =\left(  \underbrace{\left(  \operatorname*{env}\nolimits_{\mathfrak{g}%
^{\varepsilon}}c\right)  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}%
}_{\substack{=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)
\cdot v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\\\text{(by
(\ref{pf.invformnondeg.polynomiality.5}))}}},v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=\left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \cdot v_{\lambda
}^{+\mathfrak{g}^{\varepsilon}},v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}\\
&  =Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)
\cdot\underbrace{\left(  v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}%
,v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}%
^{\varepsilon}}}_{=1}=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon
\right)  .
\end{align*}
This proves Lemma \ref{lem.invformnondeg.polynomiality}.

We shall now take a closer look at the polynomial function $Q_{\mathbf{i}%
,\mathbf{j}}$ of Lemma \ref{lem.invformnondeg.polynomiality}:

\begin{lemma}
\label{lem.invformnondeg.polynomiality3}Let $\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E$ and $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$. Consider
the polynomial function $Q_{\mathbf{i},\mathbf{j}}:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow\mathbb{C}$ of Lemma
\ref{lem.invformnondeg.polynomiality}. Then, every $\lambda\in\mathfrak{h}%
^{\ast}$ and every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  .
\]

\end{lemma}

Note that Lemma \ref{lem.invformnondeg.polynomiality3} does not really need
the conditions $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ and
$\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$. It is sufficient that
$\mathbf{i}\in\operatorname*{Seq}E$ is such that no element $\left(
n,i\right)  $ of the sequence $\mathbf{i}$ satisfies $n=0$, and that a similar
condition holds for $\mathbf{j}$. But since we will only use Lemma
\ref{lem.invformnondeg.polynomiality3} in the case when $\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E$ and $\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E$, we would not gain much from thus generalizing it.

\textit{Proof of Lemma \ref{lem.invformnondeg.polynomiality3}.} We recall that
the definition of $Q_{\mathbf{i},\mathbf{j}}$ said that%
\begin{equation}
\left(  e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}\lambda\in\mathfrak{h}^{\ast}\text{ and }\varepsilon\in\mathbb{C}.
\label{pf.invformnondeg.polynomiality3.1}%
\end{equation}


Let $\lambda\in\mathfrak{h}^{\ast}$ be arbitrary, and let $\varepsilon
\in\mathbb{C}$ be nonzero. Since $\varepsilon\neq0$, the Lie algebra
isomorphism $J_{\varepsilon}:\mathfrak{g}^{\varepsilon}\rightarrow
\mathfrak{g}$ exists and satisfies $\left(  \lambda/\varepsilon^{2}\right)
\circ J_{\varepsilon}=\lambda$. Hence, we have an isomorphism $J_{\varepsilon
}:\left(  \mathfrak{g}^{\varepsilon},\lambda\right)  \rightarrow\left(
\mathfrak{g},\lambda/\varepsilon^{2}\right)  $ in the category of pairs of a
$\mathbb{Z}$-graded Lie algebra and a linear form on its $0$-th graded
component (where the morphisms in this category are defined in the obvious
way). This isomorphism induces a corresponding isomorphism $M_{\lambda
}^{+\mathfrak{g}^{\varepsilon}}\rightarrow M_{\lambda/\varepsilon^{2}%
}^{+\mathfrak{g}}$ of Verma modules which sends $xv_{\lambda}^{+\mathfrak{g}%
^{\varepsilon}}$ to $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
x\right)  v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}}$ for every $x\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  $ (where $U\left(  J_{\varepsilon
}\right)  $ is the isomorphism $U\left(  \mathfrak{g}^{\varepsilon}\right)
\rightarrow U\left(  \mathfrak{g}\right)  $ canonically induced by the Lie
algebra isomorphism $\mathfrak{g}^{\varepsilon}\rightarrow\mathfrak{g}$).
Similarly, we get an isomorphism $M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\rightarrow M_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}$ of Verma modules
which sends $yv_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}$ to $\left(  U\left(
J_{\varepsilon}\right)  \right)  \left(  y\right)  v_{-\lambda/\varepsilon
^{2}}^{-\mathfrak{g}}$ for every $y\in U\left(  \mathfrak{g}^{\varepsilon
}\right)  $. Since the bilinear form $\left(  \cdot,\cdot\right)  _{\mu
}^{\mathfrak{e}}$ depends functorially on a $\mathbb{Z}$-graded Lie algebra
$\mathfrak{e}$ and a linear form $\mu:\mathfrak{e}_{0}^{\ast}\rightarrow
\mathbb{C}$, these isomorphisms leave the bilinear form invariant, i. e., we
have%
\[
\left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  x\right)
v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon
}\right)  \right)  \left(  y\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}=\left(
xv_{\lambda}^{+\mathfrak{g}^{\varepsilon}},yv_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}%
\]
for every $x\in U\left(  \mathfrak{g}^{\varepsilon}\right)  $ and $y\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  $. Applied to $x=e_{\mathbf{i}%
}^{\varepsilon}$ and $y=e_{\mathbf{j}}^{\varepsilon}$, this yields%
\begin{equation}
\left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  v_{\lambda/\varepsilon^{2}}%
^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}=\left(
e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=Q_{\mathbf{i},\mathbf{j}%
}\left(  \lambda,\varepsilon\right)  \label{pf.invformnondeg.polynomiality3.2}%
\end{equation}
(by the definition of $Q_{\mathbf{i},\mathbf{j}}$).

But we have $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  =\varepsilon^{\operatorname*{len}%
\mathbf{i}}e_{\mathbf{i}}^{1}$\ \ \ \ \footnote{\textit{Proof.} Write the
sequence $\mathbf{i}$ in the form $\left(  \left(  n_{1},i_{1}\right)
,\left(  n_{2},i_{2}\right)  ,...,\left(  n_{\ell},i_{\ell}\right)  \right)
$. Since $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$, all of the numbers
$n_{1}$, $n_{2}$, $...$, $n_{\ell}$ are negative, so that none of them is $0$.
As a consequence, $\delta_{n_{u},0}=0$ for every $u\in\left\{  1,2,...,\ell
\right\}  $. By the definition of $J_{\varepsilon}$, we have%
\begin{align*}
J_{\varepsilon}\left(  e_{n_{u},i_{u}}\right)   &  =\underbrace{\varepsilon
^{1+\delta_{n_{u},0}}}_{\substack{=\varepsilon\\\text{(since }\delta_{n_{u}%
,0}=0\text{)}}}e_{n_{u},i_{u}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}e_{n_{u},i_{u}}\in\mathfrak{g}_{n_{u}}\right) \\
&  =\varepsilon e_{n_{u},i_{u}}%
\end{align*}
for every $u\in\left\{  1,2,...,\ell\right\}  $.
\par
Now, $e_{\mathbf{i}}^{\varepsilon}$ is defined as the product $e_{n_{1},i_{1}%
}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ in $U\left(  \mathfrak{g}%
^{\varepsilon}\right)  $, and $e_{\mathbf{i}}^{1}$ is defined as the product
$e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}$ in $U\left(
\mathfrak{g}^{1}\right)  $. Hence,%
\begin{align*}
\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(  e_{\mathbf{i}%
}^{\varepsilon}\right)   &  =\left(  U\left(  J_{\varepsilon}\right)  \right)
\left(  e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }e_{\mathbf{i}}^{\varepsilon}%
=e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell},i_{\ell}}\right) \\
&  =J_{\varepsilon}\left(  e_{n_{1},i_{1}}\right)  J_{\varepsilon}\left(
e_{n_{2},i_{2}}\right)  ...J_{\varepsilon}\left(  e_{n_{\ell},i_{\ell}}\right)
\\
&  =\varepsilon e_{n_{1},i_{1}}\cdot\varepsilon e_{n_{2},i_{2}}\cdot
...\cdot\varepsilon e_{n_{\ell},i_{\ell}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }J_{\varepsilon}\left(  e_{n_{u},i_{u}}\right)  =\varepsilon
e_{n_{u},i_{u}}\text{ for every }u\in\left\{  1,2,...,\ell\right\}  \right) \\
&  =\varepsilon^{\ell}\underbrace{e_{n_{1},i_{1}}e_{n_{2},i_{2}}...e_{n_{\ell
},i_{\ell}}}_{=e_{\mathbf{i}}^{1}}=\varepsilon^{\ell}e_{\mathbf{i}}%
^{1}=\varepsilon^{\operatorname*{len}\mathbf{i}}e_{\mathbf{i}}^{1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell=\operatorname*{len}%
\mathbf{i}\text{ by the definition of }\operatorname*{len}\mathbf{i}\right)  ,
\end{align*}
qed.} and similarly $\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  =\varepsilon^{\operatorname*{len}%
\mathbf{j}}e_{\mathbf{j}}^{1}$. Hence,%
\begin{align*}
&  \left(  \left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{i}}^{\varepsilon}\right)  v_{\lambda/\varepsilon^{2}}%
^{+\mathfrak{g}},\left(  U\left(  J_{\varepsilon}\right)  \right)  \left(
e_{\mathbf{j}}^{\varepsilon}\right)  v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}}\\
&  =\left(  \varepsilon^{\operatorname*{len}\mathbf{i}}e_{\mathbf{i}}%
^{1}v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}},\varepsilon
^{\operatorname*{len}\mathbf{j}}e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}%
}=\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}\left(  e_{\mathbf{i}}^{1}v_{\lambda/\varepsilon^{2}}^{+\mathfrak{g}%
},e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}}^{-\mathfrak{g}}\right)
_{\lambda/\varepsilon^{2}}^{\mathfrak{g}}\\
&  =\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}\underbrace{\left(  e_{\mathbf{i}}^{1}v_{\lambda/\varepsilon^{2}%
}^{+\mathfrak{g}^{1}},e_{\mathbf{j}}^{1}v_{-\lambda/\varepsilon^{2}%
}^{-\mathfrak{g}^{1}}\right)  _{\lambda/\varepsilon^{2}}^{\mathfrak{g}^{1}}%
}_{\substack{=Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon
^{2},1\right)  \\\text{(by (\ref{pf.invformnondeg.polynomiality3.1}), applied
to }\lambda/\varepsilon^{1}\text{ and }1\text{ instead of }\lambda\text{ and
}\varepsilon\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}%
=\mathfrak{g}^{1}\right) \\
&  =\varepsilon^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}%
}Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  .
\end{align*}
Compared to (\ref{pf.invformnondeg.polynomiality3.2}), this yields
$Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  $. This proves Lemma
\ref{lem.invformnondeg.polynomiality3}.

Here is the consequence of Lemmas \ref{lem.invformnondeg.polynomiality} and
\ref{lem.invformnondeg.polynomiality3} that we will actually use:

\begin{corollary}
\label{cor.invformnondeg.polynomiality}Let $n\in\mathbb{N}$. Let
$\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}%
\sum\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$ (we are using the fact
that $\sum\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{i}=-n}}\operatorname*{len}\mathbf{i=}\sum
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\operatorname*{len}\mathbf{j}$, which we proved above).

Then, there exists a polynomial function $Q_{n}:\mathfrak{h}^{\ast}%
\times\mathbb{C}\rightarrow\mathbb{C}$ such that every $\lambda\in
\mathfrak{h}^{\ast}$ and every $\varepsilon\in\mathbb{C}$ satisfy%
\begin{equation}
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  =Q_{n}\left(  \lambda,\varepsilon\right)  .
\label{cor.invformnondeg.polynomiality.1}%
\end{equation}
This function $Q_{n}$ satisfies%
\[
Q_{n}\left(  \lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}%
n}Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }\lambda\in\mathfrak{h}^{\ast}\text{ and every nonzero }\varepsilon
\in\mathbb{C}.
\]

\end{corollary}

\textit{Proof of Corollary \ref{cor.invformnondeg.polynomiality}.} For any
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ satisfying $\deg
\mathbf{i}=-n$, and any $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$
satisfying $\deg\mathbf{j}=n$, consider the polynomial function $Q_{\mathbf{i}%
,\mathbf{j}}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$ of
Lemma \ref{lem.invformnondeg.polynomiality}. Define a polynomial function
$Q_{n}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$ by%
\[
Q_{n}=\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  .
\]
Then, every $\lambda\in\mathfrak{h}^{\ast}$ and every $\varepsilon
\in\mathbb{C}$ satisfy%
\begin{align*}
Q_{n}\left(  \lambda,\varepsilon\right)   &  =\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  =\det\left(  \left(  \left(  e_{\mathbf{i}}^{\varepsilon
}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}}^{\varepsilon
}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda,n}%
^{\mathfrak{g}^{\varepsilon}}\right)  _{\substack{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Lemma \ref{lem.invformnondeg.polynomiality} yields}\\
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\left(
e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}%
},e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}%
}\right)  _{\lambda}^{\mathfrak{g}^{\varepsilon}}=\left(  e_{\mathbf{i}%
}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}},e_{\mathbf{j}%
}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\right)  _{\lambda
,n}^{\mathfrak{g}^{\varepsilon}}\\
\text{(since }\deg\mathbf{i}=-n\text{ yields }e_{\mathbf{i}}^{\varepsilon}\in
U\left(  \mathfrak{g}^{\varepsilon}\right)  \left[  -n\right]  \text{ and thus
}e_{\mathbf{i}}^{\varepsilon}v_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\in
M_{\lambda}^{+\mathfrak{g}^{\varepsilon}}\left[  -n\right] \\
\text{and similarly }e_{\mathbf{j}}^{\varepsilon}v_{-\lambda}^{-\mathfrak{g}%
^{\varepsilon}}\in M_{-\lambda}^{-\mathfrak{g}^{\varepsilon}}\left[  n\right]
\text{)}%
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}%
^{\varepsilon}}\right)  .
\end{align*}
We have thus proven that every $\lambda\in\mathfrak{h}^{\ast}$ and every
$\varepsilon\in\mathbb{C}$ satisfy $\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{\varepsilon}}\right)  =Q_{n}\left(  \lambda
,\varepsilon\right)  $.

Now, it remains to show that this function $Q_{n}$ satisfies $Q_{n}\left(
\lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}n}Q_{n}\left(
\lambda/\varepsilon^{2},1\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$
and every nonzero $\varepsilon\in\mathbb{C}$. In order to do this, we let
$\lambda\in\mathfrak{h}^{\ast}$ be arbitrary and $\varepsilon\in\mathbb{C}$ be
nonzero. Then,
\begin{equation}
Q_{n}\left(  \lambda,\varepsilon\right)  =\det\left(  \left(  Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda,\varepsilon\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  =\det\left(  \left(  \varepsilon^{\operatorname*{len}\mathbf{i}%
}\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)
\label{pf.invformnondeg.polynomiality5.1}%
\end{equation}
(since Lemma \ref{lem.invformnondeg.polynomiality3} yields $Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda,\varepsilon\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}+\operatorname*{len}\mathbf{j}}Q_{\mathbf{i}%
,\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  =\varepsilon
^{\operatorname*{len}\mathbf{i}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  $ for all
$\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$ and$\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$).

Now, recall that if we multiply a row of a square matrix by some scalar, then
the determinant of the matrix is also multiplied by the same scalar. A similar
fact holds for the columns. Thus,
\begin{align*}
&  \det\left(  \left(  \varepsilon^{\operatorname*{len}\mathbf{i}}%
\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  =\left(  \prod\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}%
\mathbf{i}}\right)  \cdot\left(  \prod\limits_{\substack{\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon
^{\operatorname*{len}\mathbf{j}}\right)  \cdot\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)
\end{align*}
(because the matrix $\left(  \varepsilon^{\operatorname*{len}\mathbf{i}%
}\varepsilon^{\operatorname*{len}\mathbf{j}}Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}$ is obtained from the
matrix $\left(  Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon
^{2},1\right)  \right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}$ by multiplying every row $\mathbf{i}$ by
the scalar $\varepsilon^{\operatorname*{len}\mathbf{i}}$ and multiplying every
column $\mathbf{j}$ by the scalar $\varepsilon^{\operatorname*{len}\mathbf{j}%
}$). Hence, (\ref{pf.invformnondeg.polynomiality5.1}) becomes%
\begin{equation}
Q_{n}\left(  \lambda,\varepsilon\right)  =\left(  \prod
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}\mathbf{i}}\right)
\cdot\left(  \prod\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}\right)  \cdot\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}}\left(
\lambda/\varepsilon^{2},1\right)  \right)  _{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  .
\label{pf.invformnondeg.polynomiality5.3}%
\end{equation}


Now, since $\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\operatorname*{len}%
\mathbf{i}$, we have $\varepsilon^{\operatorname*{LEN}n}=\prod
\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}\mathbf{i}}$. Also,
since $\operatorname*{LEN}n=\sum\limits_{\substack{\mathbf{j}\in
\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{j}=-n}}\operatorname*{len}%
\mathbf{j}$, we have $\varepsilon^{\operatorname*{LEN}n}=\prod
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}}$. Thus,%
\begin{equation}
\underbrace{\left(  \prod\limits_{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon^{\operatorname*{len}%
\mathbf{i}}\right)  }_{=\varepsilon^{\operatorname*{LEN}n}}\cdot
\underbrace{\left(  \prod\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}%
}\right)  }_{=\varepsilon^{\operatorname*{LEN}n}}=\varepsilon
^{\operatorname*{LEN}n}\varepsilon^{\operatorname*{LEN}n}=\varepsilon
^{2\operatorname*{LEN}n}. \label{pf.invformnondeg.polynomiality5.6}%
\end{equation}


On the other hand, since $Q_{n}=\det\left(  \left(  Q_{\mathbf{i},\mathbf{j}%
}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}%
=-n;\ \deg\mathbf{j}=n}}\right)  $, we have%
\begin{equation}
Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  =\det\left(  \left(
Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)  \right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  . \label{pf.invformnondeg.polynomiality5.5}%
\end{equation}
Hence, (\ref{pf.invformnondeg.polynomiality5.3}) becomes%
\begin{align*}
&  Q_{n}\left(  \lambda,\varepsilon\right) \\
&  =\underbrace{\left(  \prod\limits_{\substack{\mathbf{i}\in
\operatorname*{Seq}\nolimits_{-}E;\\\deg\mathbf{i}=-n}}\varepsilon
^{\operatorname*{len}\mathbf{i}}\right)  \cdot\left(  \prod
\limits_{\substack{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}%
E;\\\deg\mathbf{j}=n}}\varepsilon^{\operatorname*{len}\mathbf{j}}\right)
}_{\substack{=\varepsilon^{2\operatorname*{LEN}n}\\\text{(by
(\ref{pf.invformnondeg.polynomiality5.6}))}}}\cdot\underbrace{\det\left(
\left(  Q_{\mathbf{i},\mathbf{j}}\left(  \lambda/\varepsilon^{2},1\right)
\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}%
=-n;\ \deg\mathbf{j}=n}}\right)  }_{\substack{=Q_{n}\left(  \lambda
/\varepsilon^{2},1\right)  \\\text{(by
(\ref{pf.invformnondeg.polynomiality5.5}))}}}\\
&  =\varepsilon^{2\operatorname*{LEN}n}\cdot Q_{n}\left(  \lambda
/\varepsilon^{2},1\right)  .
\end{align*}
We have thus proven that $Q_{n}\left(  \lambda,\varepsilon\right)
=\varepsilon^{2\operatorname*{LEN}n}Q_{n}\left(  \lambda/\varepsilon
^{2},1\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$ and every nonzero
$\varepsilon\in\mathbb{C}$. This concludes the proof of Corollary
\ref{cor.invformnondeg.polynomiality}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: On leading terms of
pseudo-homogeneous polynomial maps}

The following lemma about polynomial maps could be an easy exercise in any
algebra text. Unfortunately I do not see a quick way to prove it, so the proof
is going to take a few pages. Reading it will probably waste more of the
reader's time than proving it on her own.

\begin{lemma}
\label{lem.invformnondeg.elemen}Let $V$ be a finite-dimensional $\mathbb{C}%
$-vector space. Let $k\in\mathbb{N}$. Let $\phi:V\times\mathbb{C}%
\rightarrow\mathbb{C}$ be a polynomial function such that every $\lambda\in V$
and every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
\phi\left(  \lambda,\varepsilon\right)  =\varepsilon^{2k}\phi\left(
\lambda/\varepsilon^{2},1\right)  .
\]
Then:

\textbf{(a)} The polynomial function
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,0\right)
\]
is homogeneous of degree $k$.

\textbf{(b)} For every integer $N>k$, the $N$-th homogeneous component of the
polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,1\right)
\]
is zero.

\textbf{(c)} The $k$-th homogeneous component of the polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,1\right)
\]
is the polynomial function%
\[
V\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto\phi\left(
\lambda,0\right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.invformnondeg.elemen}.} \textbf{(a)} Let
$\left(  v_{1},v_{2},...,v_{n}\right)  $ be a basis of the vector space
$V^{\ast}$. Let $\pi_{V}:V\times\mathbb{C}\rightarrow V$ and $\pi_{\mathbb{C}%
}:V\times\mathbb{C}\rightarrow\mathbb{C}$ be the canonical projections. Then,
$\left(  v_{1}\circ\pi_{V},v_{2}\circ\pi_{V},...,v_{n}\circ\pi_{V}%
,\pi_{\mathbb{C}}\right)  $ is a basis of the vector space $\left(
V\times\mathbb{C}\right)  ^{\ast}$.

Therefore, since $\phi$ is a polynomial function, there exists a polynomial
$P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{n},X_{n+1}\right]  $ such that every
$w\in V\times\mathbb{C}$ satisfies%
\[
\phi\left(  w\right)  =P\left(  \left(  v_{1}\circ\pi_{V}\right)  \left(
w\right)  ,\left(  v_{2}\circ\pi_{V}\right)  \left(  w\right)  ,...,\left(
v_{n}\circ\pi_{V}\right)  \left(  w\right)  ,\pi_{\mathbb{C}}\left(  w\right)
\right)  .
\]
In other words, every $\left(  \lambda,\varepsilon\right)  \in V\times
\mathbb{C}$ satisfies%
\begin{equation}
\phi\left(  \lambda,\varepsilon\right)  =P\left(  v_{1}\left(  \lambda\right)
,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda\right)  ,\varepsilon
\right)  . \label{pf.invformnondeg.elemen.1}%
\end{equation}


Now, it is easy to see that for every $\left(  x_{1},x_{2},...,x_{n}\right)
\in\mathbb{C}^{n}$ and nonzero $\varepsilon\in\mathbb{C}$, we have
\begin{equation}
P\left(  x_{1},x_{2},...,x_{n},\varepsilon\right)  =\varepsilon^{2k}P\left(
x_{1}/\varepsilon^{2},x_{2}/\varepsilon^{2},...,x_{n}/\varepsilon
^{2},1\right)  . \label{pf.invformnondeg.elemen.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.invformnondeg.elemen.2}).} Let $\left(
x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ be arbitrary, and let
$\varepsilon\in\mathbb{C}$ be nonzero.
\par
Let $\lambda\in V$ be a vector satisfying%
\[
v_{i}\left(  \lambda\right)  =x_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\]
(such a vector $\lambda$ exists since $\left(  v_{1},v_{2},...,v_{n}\right)  $
is a basis of $V^{\ast}$). Then,%
\begin{align*}
P\left(  x_{1},x_{2},...,x_{n},\varepsilon\right)   &  =P\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  ,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
x_{i}=v_{i}\left(  \lambda\right)  \text{ for every }i\in\left\{
1,2,...,n\right\}  \right) \\
&  =\phi\left(  \lambda,\varepsilon\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.invformnondeg.elemen.1})}\right) \\
&  =\varepsilon^{2k}\underbrace{\phi\left(  \lambda/\varepsilon^{2},1\right)
}_{\substack{=P\left(  v_{1}\left(  \lambda/\varepsilon^{2}\right)
,v_{2}\left(  \lambda/\varepsilon^{2}\right)  ,...,v_{n}\left(  \lambda
/\varepsilon^{2}\right)  ,1\right)  \\\text{(by
(\ref{pf.invformnondeg.elemen.1}), applied to }\left(  \lambda/\varepsilon
^{2},1\right)  \text{ instead of }\left(  \lambda,\varepsilon\right)
\text{)}}}\\
&  =\varepsilon^{2k}P\left(  v_{1}\left(  \lambda/\varepsilon^{2}\right)
,v_{2}\left(  \lambda/\varepsilon^{2}\right)  ,...,v_{n}\left(  \lambda
/\varepsilon^{2}\right)  ,1\right)  =\varepsilon^{2k}P\left(  x_{1}%
/\varepsilon^{2},x_{2}/\varepsilon^{2},...,x_{n}/\varepsilon^{2},1\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v_{i}\left(  \lambda
/\varepsilon^{2}\right)  =\underbrace{v_{i}\left(  \lambda\right)  }_{=x_{i}%
}/\varepsilon^{2}=x_{i}/\varepsilon^{2}\text{ for every }i\in\left\{
1,2,...,n\right\}  \right)  .
\end{align*}
This proves (\ref{pf.invformnondeg.elemen.2}).}

Now, since $P\in\mathbb{C}\left[  X_{1},X_{2},...,X_{n},X_{n+1}\right]
\cong\left(  \mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  \right)  \left[
X_{n+1}\right]  $, we can write the polynomial $P$ as a polynomial in the
variable $X_{n+1}$ over the ring $\mathbb{C}\left[  X_{1},X_{2},...,X_{n}%
\right]  $. In other words, we can write the polynomial $P$ in the form
$P=\sum\limits_{i\in\mathbb{N}}P_{i}\cdot X_{n+1}^{i}$ for some polynomials
$P_{0}$, $P_{1}$, $P_{2}$, $...$ in $\mathbb{C}\left[  X_{1},X_{2}%
,...,X_{n}\right]  $ such that all but finitely many $i\in\mathbb{N}$ satisfy
$P_{i}=0$. Consider these $P_{0}$, $P_{1}$, $P_{2}$, $...$.

Since all but finitely many $i\in\mathbb{N}$ satisfy $P_{i}=0$, there exists a
$d\in\mathbb{N}$ such that every integer $i>d$ satisfies $P_{i}=0$. Consider
this $d$. Then, $P=\sum\limits_{i\in\mathbb{N}}P_{i}\cdot X_{n+1}^{i}%
=\sum\limits_{i=0}^{d}P_{i}\cdot X_{n+1}^{i}$ (here, we have removed all the
terms with $i>d$ from the sum, because every integer $i>d$ satisfies $P_{i}=0$
and thus $P_{i}\cdot X_{n+1}^{i}=0$).

For every $i\in\mathbb{N}$ and every $j\in\mathbb{N}$, let $Q_{i,j}$ be the
$j$-th homogeneous component of the polynomial $P_{i}$. Then, $P_{i}%
=\sum\limits_{j\in\mathbb{N}}Q_{i,j}$ for every $i\in\mathbb{N}$, and each
$Q_{i,j}$ is homogeneous of degree $j$.

Hence,%
\begin{equation}
P=\sum\limits_{i\in\mathbb{N}}\underbrace{P_{i}}_{=\sum\limits_{j\in
\mathbb{N}}Q_{i,j}}\cdot X_{n+1}^{i}=\sum\limits_{i\in\mathbb{N}}%
\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}.
\label{pf.invformnondeg.elemen.3a}%
\end{equation}


Now, we are going to show the following fact: We have%
\begin{equation}
Q_{u,v}=0\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  u,v\right)  \in
\mathbb{N}\times\mathbb{N}\text{ which don't satisfy }u+2v=2k.
\label{pf.invformnondeg.elemen.4}%
\end{equation}


\textit{Proof of (\ref{pf.invformnondeg.elemen.4}).} Let $\left(  u,v\right)
\in\mathbb{N}\times\mathbb{N}$ be such that $u+2v\neq2k$. We must prove that
$Q_{u,v}=0$.

If $u>d$, then $Q_{u,v}=0$ is clear (because $Q_{u,v}$ is the $v$-th
homogeneous component of $P_{u}$, but we have $P_{u}=0$ since $u>d$). Hence,
for the rest of the proof of $Q_{u,v}=0$, we can WLOG assume that $u\leq d$.

We have%
\[
P=\sum\limits_{i=0}^{d}\underbrace{P_{i}}_{=\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}}\cdot X_{n+1}^{i}=\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}X_{n+1}^{i}.
\]


Let $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ and
$\varepsilon\in\mathbb{C}\diagdown\left\{  0\right\}  $. Then, $\varepsilon$
is nonzero, and we have%
\begin{align*}
P\left(  x_{1},x_{2},...,x_{n},1/\varepsilon\right)   &  =\sum\limits_{i=0}%
^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)
\underbrace{\left(  1/\varepsilon\right)  ^{i}}_{=\varepsilon^{d-i}%
/\varepsilon^{d}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum
\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}\right) \\
&  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}Q_{i,j}\left(
x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}/\varepsilon^{d}=\dfrac
{1}{\varepsilon^{d}}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}%
\end{align*}
and%
\begin{align*}
P\left(  \varepsilon^{2}x_{1},\varepsilon^{2}x_{2},...,\varepsilon^{2}%
x_{n},1\right)   &  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\underbrace{Q_{i,j}\left(  \varepsilon^{2}x_{1},\varepsilon^{2}%
x_{2},...,\varepsilon^{2}x_{n}\right)  }_{\substack{=\left(  \varepsilon
^{2}\right)  ^{j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \\\text{(since
}Q_{i,j}\text{ is homogeneous of degree }j\text{)}}}\underbrace{1^{i}}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{i=0}^{d}%
\sum\limits_{j\in\mathbb{N}}Q_{i,j}X_{n+1}^{i}\right) \\
&  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}\underbrace{\left(
\varepsilon^{2}\right)  ^{j}}_{=\varepsilon^{2j}}Q_{i,j}\left(  x_{1}%
,x_{2},...,x_{n}\right)  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  .
\end{align*}
Now,%
\begin{align*}
&  \dfrac{1}{\varepsilon^{d}}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}\\
&  =P\left(  x_{1},x_{2},...,x_{n},1/\varepsilon\right)  =\left(
1/\varepsilon\right)  ^{2k}\underbrace{P\left(  x_{1}/\left(  \dfrac
{1}{\varepsilon}\right)  ^{2},x_{2}/\left(  \dfrac{1}{\varepsilon}\right)
^{2},...,x_{n}/\left(  \dfrac{1}{\varepsilon}\right)  ^{2},1\right)
}_{=P\left(  \varepsilon^{2}x_{1},\varepsilon^{2}x_{2},...,\varepsilon
^{2}x_{n},1\right)  =\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}%
}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.invformnondeg.elemen.2}),
applied to }1/\varepsilon\text{ instead of }\varepsilon\right) \\
&  =\left(  1/\varepsilon\right)  ^{2k}\sum\limits_{i=0}^{d}\sum
\limits_{j\in\mathbb{N}}\varepsilon^{2j}Q_{i,j}\left(  x_{1},x_{2}%
,...,x_{n}\right)  ,
\end{align*}
so that%
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  \varepsilon^{d-i}=\varepsilon
^{d}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}\varepsilon^{2j}%
Q_{i,j}\left(  x_{1},x_{2},...,x_{n}\right)  .
\]
For fixed $\varepsilon$, this is a polynomial identity in $\left(  x_{1}%
,x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$. Since it holds for all $\left(
x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ (as we just have shown), it
thus must hold as a formal identity, i. e., we must have%
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}\sum\limits_{j\in\mathbb{N}}%
Q_{i,j}\varepsilon^{d-i}=\varepsilon^{d}\sum\limits_{i=0}^{d}\sum
\limits_{j\in\mathbb{N}}\varepsilon^{2j}Q_{i,j}\ \ \ \ \ \ \ \ \ \ \text{in
}\mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  .
\]
Let us take the $v$-th homogeneous components of both sides of this equation.
Since each $Q_{i,j}$ is homogeneous of degree $j$, this amounts to removing
all $Q_{i,j}$ with $j\neq v$, and leaving the $Q_{i,j}$ with $j=v$ unchanged.
Thus, we obtain%
\begin{equation}
\varepsilon^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\varepsilon^{d-i}=\varepsilon
^{d}\sum\limits_{i=0}^{d}\varepsilon^{2v}Q_{i,v}\ \ \ \ \ \ \ \ \ \ \text{in
}\mathbb{C}\left[  X_{1},X_{2},...,X_{n}\right]  .
\label{pf.invformnondeg.elemen.6}%
\end{equation}


Now, let $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$ be
arbitrary again. Then, evaluating the identity
(\ref{pf.invformnondeg.elemen.6}) at $\left(  X_{1},X_{2},...,X_{n}\right)
=\left(  x_{1},x_{2},...,x_{n}\right)  $, we obtain
\[
\varepsilon^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}%
\right)  \varepsilon^{d-i}=\varepsilon^{d}\sum\limits_{i=0}^{d}\varepsilon
^{2v}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}\right)  .
\]
For fixed $\left(  x_{1},x_{2},...,x_{n}\right)  $, this is a polynomial
identity in $\varepsilon$ (since $d-i\geq0$ for all $i\in\left\{
0,1,...,d\right\}  $). Since it holds for all nonzero $\varepsilon
\in\mathbb{C}$ (as we just have shown), it thus must hold as a formal identity
(since any polynomial in one variable which evaluates to zero at all nonzero
complex numbers must be the zero polynomial). In other words, we must have%
\[
E^{2k}\sum\limits_{i=0}^{d}Q_{i,v}\left(  x_{1},x_{2},...,x_{n}\right)
E^{d-i}=E^{d}\sum\limits_{i=0}^{d}E^{2v}Q_{i,v}\left(  x_{1},x_{2}%
,...,x_{n}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }\mathbb{C}\left[  E\right]
\]
(where $\mathbb{C}\left[  E\right]  $ denotes the polynomial ring over
$\mathbb{C}$ in one variable $E$). Let us compare the coefficients of
$E^{2k+d-u}$ on both sides of this equation: The coefficient of $E^{2k+d-u}$
on the left hand side of this equation is clearly $Q_{u,v}\left(  x_{1}%
,x_{2},...,x_{n}\right)  $, while the coefficient of $E^{2k+d-u}$ on the right
hand side is $0$ (in fact, the only coefficient on the right hand side of the
equation which is not trivially zero is the coefficient of $E^{d+2v}$, but
$d+2v\neq2k+d-u$ (since $u+2v\neq2k$ and thus $2v\neq2k-u$)). Hence,
comparison yields $Q_{u,v}\left(  x_{1},x_{2},...,x_{n}\right)  =0$. Since
this holds for all $\left(  x_{1},x_{2},...,x_{n}\right)  \in\mathbb{C}^{n}$,
we thus obtain $Q_{u,v}=0$ (because any polynomial which vanishes on the whole
$\mathbb{C}^{n}$ must be the zero polynomial). This proves
(\ref{pf.invformnondeg.elemen.4}).

Now, (\ref{pf.invformnondeg.elemen.3a}) rewrites as%
\begin{align*}
P  &  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}Q_{i,j}%
X_{n+1}^{i}=\sum\limits_{u\in\mathbb{N}}\sum\limits_{v\in\mathbb{N}}%
Q_{u,v}X_{n+1}^{u}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the
indices }i\text{ and }j\text{ as }u\text{ and }v\right) \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{N}\times\mathbb{N}}%
Q_{u,v}X_{n+1}^{u}=\sum\limits_{\substack{\left(  u,v\right)  \in
\mathbb{N}\times\mathbb{N};\\u+2v=2k}}Q_{u,v}X_{n+1}^{u}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we removed from our sum all terms for }\left(  u,v\right)
\in\mathbb{N}\times\mathbb{N}\text{ which}\\
\text{don't satisfy }u+2v=2k\text{ (because (\ref{pf.invformnondeg.elemen.4})
shows that these terms}\\
\text{don't contribute anything to the sum)}%
\end{array}
\right) \\
&  =\sum\limits_{v=0}^{k}Q_{2k-2v,v}X_{n+1}^{2k-2v}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we substituted }\left(  2k-2v,v\right)  \text{ for }\left(
u,v\right)  \text{ in the sum}\right)  .
\end{align*}


Now, for every $v\in\left\{  0,1,...,k\right\}  $, let $\psi_{v}%
:V\rightarrow\mathbb{C}$ be the polynomial map defined by%
\[
\psi_{v}\left(  \lambda\right)  =Q_{2k-2v,v}\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }\lambda\in V.
\]
Then, $\psi_{v}$ is homogeneous of degree $v$ (since $Q_{2k-2v,v}$ is
homogeneous of degree $v$). In particular, this yields that $\psi_{k}$ is
homogeneous of degree $k$.

Every $\left(  \lambda,\varepsilon\right)  \in V\times\mathbb{C}$ satisfies%
\begin{align}
\phi\left(  \lambda,\varepsilon\right)   &  =P\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  ,\varepsilon\right) \nonumber\\
&  =\sum\limits_{v=0}^{k}\underbrace{Q_{2k-2v,v}\left(  v_{1}\left(
\lambda\right)  ,v_{2}\left(  \lambda\right)  ,...,v_{n}\left(  \lambda
\right)  \right)  }_{=\psi_{v}\left(  \lambda\right)  }\varepsilon
^{2k-2v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{v=0}%
^{k}Q_{2k-2v,v}X_{n+1}^{2k-2v}\right) \nonumber\\
&  =\sum\limits_{v=0}^{k}\psi_{v}\left(  \lambda\right)  \varepsilon^{2k-2v}.
\label{pf.invformnondeg.elemen.10}%
\end{align}
Applied to $\varepsilon=0$, this yields%
\[
\phi\left(  \lambda,0\right)  =\sum\limits_{v=0}^{k}\psi_{v}\left(
\lambda\right)  0^{2k-2v}=\psi_{k}\left(  \lambda\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0^{2k-2v}=0\text{ for all
}v<k\right)
\]
for every $\lambda\in V$. Hence, the polynomial function $V\rightarrow
\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,0\right)  $ equals the
polynomial function $\psi_{k}$, and thus is homogeneous of degree $k$ (since
$\psi_{k}$ is homogeneous of degree $k$). This proves Lemma
\ref{lem.invformnondeg.elemen} \textbf{(a)}.

Applying (\ref{pf.invformnondeg.elemen.10}) to $\varepsilon=1$, we obtain%
\[
\phi\left(  \lambda,1\right)  =\sum\limits_{v=0}^{k}\psi_{v}\left(
\lambda\right)  \underbrace{1^{2k-2v}}_{=1}=\sum\limits_{v=0}^{k}\psi
_{v}\left(  \lambda\right)  .
\]
Hence, the polynomial function $V\rightarrow\mathbb{C},\ \lambda\mapsto
\phi\left(  \lambda,1\right)  $ equals the sum $\sum\limits_{v=0}^{k}\psi_{v}%
$. Since we know that the polynomial function $\psi_{v}$ is homogeneous of
degree $v$ for every $v\in\left\{  0,1,...,k\right\}  $, this yields that, for
every integer $N>k$, the $N$-th homogeneous component of the polynomial
function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,1\right)
$ is zero. This proves Lemma \ref{lem.invformnondeg.elemen} \textbf{(b)}.

Finally, recall that the polynomial function $V\rightarrow\mathbb{C}%
,\ \lambda\mapsto\phi\left(  \lambda,1\right)  $ equals the sum $\sum
\limits_{v=0}^{k}\psi_{v}$, and the polynomial function $\psi_{v}$ is
homogeneous of degree $v$ for every $v\in\left\{  0,1,...,k\right\}  $. Hence,
for every $v\in\left\{  0,1,...,k\right\}  $, the $v$-th homogeneous component
of the polynomial function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(
\lambda,1\right)  $ is $\psi_{v}$. In particular, the $k$-th homogeneous
component of the polynomial function $V\rightarrow\mathbb{C},\ \lambda
\mapsto\phi\left(  \lambda,1\right)  $ is $\psi_{k}$. Since $\psi_{k}$ equals
the function $V\rightarrow\mathbb{C},\ \lambda\mapsto\phi\left(
\lambda,0\right)  $, this rewrites as follows: The $k$-th homogeneous
component of the polynomial function $V\rightarrow\mathbb{C},\ \lambda
\mapsto\phi\left(  \lambda,1\right)  $ is the function $V\rightarrow
\mathbb{C},\ \lambda\mapsto\phi\left(  \lambda,0\right)  $. This proves Lemma
\ref{lem.invformnondeg.elemen} \textbf{(c)}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: The Lie algebra
$\mathfrak{g}^{0}$}

Consider the polynomial function $Q_{n}$ of Corollary
\ref{cor.invformnondeg.polynomiality}. Due to Corollary
\ref{cor.invformnondeg.polynomiality}, it satisfies the condition of Lemma
\ref{lem.invformnondeg.elemen} for $k=\operatorname*{LEN}n$. Hence, Lemma
\ref{lem.invformnondeg.elemen} suggests that we study the Lie algebra
$\mathfrak{g}^{0}$, since this will show us what the function $\mathfrak{h}%
^{\ast}\rightarrow\mathbb{C},$ $\lambda\mapsto Q_{n}\left(  \lambda,0\right)
$ looks like.

First, let us reformulate the definition of $\mathfrak{g}^{0}$ as follows: As
a vector space, $\mathfrak{g}^{0}=\mathfrak{g}$, but the bracket on
$\mathfrak{g}^{0}$ is given by%
\begin{equation}
\left[  \cdot,\cdot\right]  ^{0}:\mathfrak{g}_{i}\otimes\mathfrak{g}%
_{j}\rightarrow\mathfrak{g}_{i+j}\text{ }\text{is }\left\{
\begin{array}
[c]{c}%
\text{zero if }i+j\neq0\text{;}\\
\text{the Lie bracket }\left[  \cdot,\cdot\right]  \text{ of }\mathfrak{g}%
\text{ if }i+j=0
\end{array}
\right.  . \label{prop.det.US.pf.-1}%
\end{equation}


It is very easy to see (from this) that $\left[  \mathfrak{n}_{-}%
,\mathfrak{n}_{-}\right]  ^{0}=0$, $\left[  \mathfrak{n}_{+},\mathfrak{n}%
_{+}\right]  ^{0}=0$, $\left[  \mathfrak{n}_{-},\mathfrak{n}_{+}\right]
^{0}=\left[  \mathfrak{n}_{+},\mathfrak{n}_{-}\right]  ^{0}\subseteq
\mathfrak{h}$ and that $\mathfrak{h}\subseteq Z\left(  \mathfrak{g}%
^{0}\right)  $.

We notice that $\mathfrak{n}_{-}^{0}=\mathfrak{n}_{-}$, $\mathfrak{n}_{+}%
^{0}=\mathfrak{n}_{+}$ and $\mathfrak{h}^{0}=\mathfrak{h}$ as vector spaces.

Since $\left[  \mathfrak{n}_{-}^{0},\mathfrak{n}_{-}^{0}\right]  ^{0}=\left[
\mathfrak{n}_{-},\mathfrak{n}_{-}\right]  ^{0}=0$, the Lie algebra
$\mathfrak{n}_{-}^{0}$ is abelian, so that $U\left(  \mathfrak{n}_{-}%
^{0}\right)  =S\left(  \mathfrak{n}_{-}^{0}\right)  =S\left(  \mathfrak{n}%
_{-}\right)  $. Similarly, $U\left(  \mathfrak{n}_{+}^{0}\right)  =S\left(
\mathfrak{n}_{+}^{0}\right)  =S\left(  \mathfrak{n}_{+}\right)  $.

We notice that%
\begin{equation}
\lambda\left(  \left[  x,y\right]  ^{0}\right)  =\lambda\left(  \left[
x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for any }x\in\mathfrak{g}\text{
and }y\in\mathfrak{g}. \label{prop.det.US.pf.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{prop.det.US.pf.0}).} Let $x\in\mathfrak{g}$
and $y\in\mathfrak{g}$. Since the equation (\ref{prop.det.US.pf.0}) is linear
in each of $x$ and $y$, we can WLOG assume that $x$ and $y$ are homogeneous
(since every element of $\mathfrak{g}$ is a sum of homogeneous elements). So
we can assume that $x\in\mathfrak{g}_{i}$ and $y\in\mathfrak{g}_{j}$ for some
$i\in\mathbb{N}$ and $j\in\mathbb{N}$. Consider these $i$ and $j$. If
$i+j\neq0$, then $\left[  x,y\right]  ^{0}=0$ (by (\ref{prop.det.US.pf.-1}))
and $\lambda\left(  \left[  x,y\right]  \right)  =0$ (since $x\in
\mathfrak{g}_{i}$ and $y\in\mathfrak{g}_{j}$ yield $\left[  x,y\right]
\in\mathfrak{g}_{i+j}$, and due to $i+j\neq0$ the form $\lambda$ annihilates
$\mathfrak{g}_{i+j}$), so that (\ref{prop.det.US.pf.0}) trivially holds in
this case. If $i+j=0$, then $\left[  x,y\right]  ^{0}=\left[  x,y\right]  $
(again by (\ref{prop.det.US.pf.-1})), and thus (\ref{prop.det.US.pf.0}) holds
in this case as well. We have thus proven (\ref{prop.det.US.pf.0}) both in the
case $i+j\neq0$ and in the case $i+j=0$. These cases cover all possibilites,
and thus (\ref{prop.det.US.pf.0}) is proven.}

In the following, we will use the form $\left(  \cdot,\cdot\right)  _{\lambda
}^{\circ}$ defined in Definition \ref{def.lambda_k}. We will only consider
this form for the Lie algebra $\mathfrak{g}$, not for the Lie algebras
$\mathfrak{g}^{\varepsilon}$ and $\mathfrak{g}^{0}$; thus we don't have any
reason to rename it as $\left(  \cdot,\cdot\right)  _{\lambda}^{\circ
\mathfrak{g}}$.

\begin{lemma}
\label{lem.invform.g^0.1}We have%
\begin{equation}
\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  a,b\right)  _{\lambda}^{\circ
}\ \ \ \ \ \ \ \ \ \ \text{for all }a\in S\left(  \mathfrak{n}_{-}\right)
\text{ and }b\in S\left(  \mathfrak{n}_{+}\right)  . \label{prop.det.US.pf.1}%
\end{equation}
Here, $av_{\lambda}^{+\mathfrak{g}^{0}}$ and $bv_{-\lambda}^{-\mathfrak{g}%
^{0}}$ are elements of $M_{\lambda}^{+\mathfrak{g}^{0}}$ and $M_{-\lambda
}^{-\mathfrak{g}^{0}}$, respectively (because $a\in S\left(  \mathfrak{n}%
_{-}\right)  =U\left(  \mathfrak{n}_{-}^{0}\right)  $ and $b\in S\left(
\mathfrak{n}_{+}\right)  =U\left(  \mathfrak{n}_{+}^{0}\right)  $).
\end{lemma}

\textit{Proof of Lemma \ref{lem.invform.g^0.1}.} Let $a\in S\left(
\mathfrak{n}_{-}\right)  $ and $b\in S\left(  \mathfrak{n}_{+}\right)  $ be
arbitrary. Since the claim that $\left(  av_{\lambda}^{+\mathfrak{g}^{0}%
},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  a,b\right)  _{\lambda}^{\circ}$ is linear in each of $a$ and $b$, we
can WLOG assume that $a=a_{1}a_{2}...a_{u}$ for some homogeneous $a_{1}%
,a_{2},...,a_{u}\in\mathfrak{n}_{-}$ and that $b=b_{1}b_{2}...b_{v}$ for some
homogeneous $b_{1},b_{2},...,b_{v}\in\mathfrak{n}_{+}$ (because every element
of $S\left(  \mathfrak{n}_{-}\right)  $ is a $\mathbb{C}$-linear combination
of products of the form $a_{1}a_{2}...a_{u}$ with homogeneous $a_{1}%
,a_{2},...,a_{u}\in\mathfrak{n}_{-}$, and because every element of $S\left(
\mathfrak{n}_{+}\right)  $ is a $\mathbb{C}$-linear combination of products of
the form $b_{1}b_{2}...b_{v}$ with homogeneous $b_{1},b_{2},...,b_{v}%
\in\mathfrak{n}_{+}$).

WLOG assume that $v\geq u$. (Else, the proof is analogous.)

Recall the equality $\left(  av_{\lambda}^{+},bv_{-\lambda}^{-}\right)
=\left(  S\left(  b\right)  av_{\lambda}^{+},v_{-\lambda}^{-}\right)  $ shown
during the proof of Proposition \ref{prop.invform}. Applied to $\mathfrak{g}%
^{0}$ instead of $\mathfrak{g}$, this yields $\left(  av_{\lambda
}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
}^{\mathfrak{g}^{0}}=\left(  S\left(  b\right)  av_{\lambda}^{+\mathfrak{g}%
^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}$.

Since $\mathfrak{h}\subseteq Z\left(  \mathfrak{g}^{0}\right)  $, we have
$\mathfrak{h}\subseteq Z\left(  U\left(  \mathfrak{g}^{0}\right)  \right)  $
(because the center of a Lie algebra always lies in the center of its
universal enveloping algebra).

Since $b=b_{1}b_{2}...b_{v}$, we have $S\left(  b\right)  =\left(  -1\right)
^{v}b_{v}b_{v-1}...b_{1}$. Combined with $a=a_{1}a_{2}...a_{u}$, this yields%
\[
S\left(  b\right)  a=\left(  -1\right)  ^{v}b_{v}b_{v-1}...b_{1}a_{1}%
a_{2}...a_{u},
\]
so that%
\begin{equation}
\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  \underbrace{S\left(  b\right)
a}_{=\left(  -1\right)  ^{v}b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}}v_{\lambda
}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
}^{\mathfrak{g}^{0}}=\left(  -1\right)  ^{v}\left(  b_{v}b_{v-1}...b_{1}%
a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}. \label{prop.det.US.pf.4}%
\end{equation}


We will now prove some identities in order to simplify the $b_{v}%
b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}$ term here.

First: In the Verma highest-weight module $M_{\lambda}^{+\mathfrak{g}^{0}}$ of
$\left(  \mathfrak{g}^{0},\lambda\right)  $, we have%
\begin{align}
\beta\alpha_{1}\alpha_{2}...\alpha_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\sum\limits_{p=1}^{\ell}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{\ell}v_{\lambda}^{+\mathfrak{g}^{0}}\label{prop.det.US.pf.3}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{N}\text{, }\alpha
_{1},\alpha_{2},...,\alpha_{\ell}\in\mathfrak{n}_{-}\text{ and }\beta
\in\mathfrak{n}_{+}\text{.}\nonumber
\end{align}
\footnote{\textit{Proof of (\ref{prop.det.US.pf.3}).} We will prove
(\ref{prop.det.US.pf.3}) by induction over $\ell$:
\par
\textit{Induction base:} For $\ell=0$, the left hand side of
(\ref{prop.det.US.pf.3}) is $\beta v_{\lambda}^{+\mathfrak{g}^{0}}=0$ (since
$\beta\in\mathfrak{n}_{+}=\mathfrak{n}_{+}^{0}$), and the right hand side of
(\ref{prop.det.US.pf.3}) is $\left(  \text{empty sum}\right)  =0$. Thus, for
$\ell=0$, the equality (\ref{prop.det.US.pf.3}) holds. This completes the
induction base.
\par
\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. Assume that
(\ref{prop.det.US.pf.3}) holds for $\ell=m-1$. We now must show that
(\ref{prop.det.US.pf.3}) holds for $\ell=m$.
\par
Let $\alpha_{1},\alpha_{2},...,\alpha_{m}\in\mathfrak{n}_{-}$ and $\beta
\in\mathfrak{n}_{+}$.
\par
Since (\ref{prop.det.US.pf.3}) holds for $\ell=m-1$, we can apply
(\ref{prop.det.US.pf.3}) to $m-1$ and $\left(  \alpha_{2},\alpha
_{3},...,\alpha_{m}\right)  $ instead of $\ell$ and $\left(  \alpha_{1}%
,\alpha_{2},...,\alpha_{\ell}\right)  $, and thus obtain%
\begin{align*}
\beta\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\sum\limits_{p=1}^{m-1}\lambda\left(  \left[  \beta,\alpha_{p+1}\right]
\right)  \alpha_{2}\alpha_{3}...\alpha_{p-1+1}\alpha_{p+1+1}\alpha
_{p+2+1}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }p\text{ for
}p+1\text{ in the sum}\right)  .
\end{align*}
\par
Now, we notice that $\beta\in\mathfrak{n}_{+}$ and $\alpha_{1}\in
\mathfrak{n}_{-}$, so that $\left[  \beta,\alpha_{1}\right]  ^{0}\in\left[
\mathfrak{n}_{+},\mathfrak{n}_{-}\right]  ^{0}\subseteq\mathfrak{h}\subseteq
Z\left(  U\left(  \mathfrak{g}^{0}\right)  \right)  $. Thus, $\left[
\beta,\alpha_{1}\right]  ^{0}\alpha_{2}\alpha_{3}...\alpha_{m}=\alpha
_{2}\alpha_{3}...\alpha_{m}\left[  \beta,\alpha_{1}\right]  ^{0}$. But since
$\left[  \beta,\alpha_{1}\right]  ^{0}\in\mathfrak{h}=\mathfrak{h}^{0}$, we
also have $\left[  \beta,\alpha_{1}\right]  ^{0}v_{\lambda}^{+\mathfrak{g}%
^{0}}=\lambda\left(  \left[  \beta,\alpha_{1}\right]  ^{0}\right)  v_{\lambda
}^{+\mathfrak{g}^{0}}=\lambda\left(  \left[  \beta,\alpha_{1}\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}$ (since $\lambda\left(  \left[  \beta
,\alpha_{1}\right]  ^{0}\right)  =\lambda\left(  \left[  \beta,\alpha
_{1}\right]  \right)  $ by (\ref{prop.det.US.pf.0})).
\par
We now compute:
\begin{align*}
\beta\alpha_{1}\alpha_{2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=\underbrace{\beta\alpha_{1}}_{\substack{=\alpha_{1}\beta+\left[  \beta
,\alpha_{1}\right]  ^{0}\\\text{(since we are in }U\left(  \mathfrak{g}%
^{0}\right)  \text{)}}}\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}=\left(  \alpha_{1}\beta+\left[  \beta,\alpha_{1}\right]
^{0}\right)  \alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}%
}\\
&  =\alpha_{1}\underbrace{\beta\alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}}_{\substack{=\sum\limits_{p=2}^{m}\lambda\left(  \left[
\beta,\alpha_{p}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}%
\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}%
}}+\underbrace{\left[  \beta,\alpha_{1}\right]  ^{0}\alpha_{2}\alpha
_{3}...\alpha_{m}}_{\substack{=\alpha_{2}\alpha_{3}...\alpha_{m}\left[
\beta,\alpha_{1}\right]  ^{0}}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\underbrace{\alpha_{1}\sum\limits_{p=2}^{m}\lambda\left(  \left[
\beta,\alpha_{p}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{p-1}%
\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}%
_{=\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}+\alpha_{2}\alpha
_{3}...\alpha_{m}\underbrace{\left[  \beta,\alpha_{1}\right]  ^{0}v_{\lambda
}^{+\mathfrak{g}^{0}}}_{\substack{=\lambda\left(  \left[  \beta,\alpha
_{1}\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}}}\\
&  =\sum\limits_{p=2}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}+\lambda\left(  \left[
\beta,\alpha_{1}\right]  \right)  \alpha_{2}\alpha_{3}...\alpha_{m}v_{\lambda
}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=1}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}\alpha_{3}...\alpha_{p-1}\alpha_{p+1}\alpha
_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\sum\limits_{p=1}^{m}\lambda\left(  \left[  \beta,\alpha_{p}\right]
\right)  \alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}%
...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}.
\end{align*}
Thus, (\ref{prop.det.US.pf.3}) holds for $\ell=m$. This completes the
induction step. Thus, (\ref{prop.det.US.pf.3}) is proven.}

Next we will show that in the Verma highest-weight module $M_{\lambda
}^{+\mathfrak{g}^{0}}$ of $\left(  \mathfrak{g}^{0},\lambda\right)  $, we have%
\begin{align}
\beta_{\ell}\beta_{\ell-1}...\beta_{1}\alpha_{1}\alpha_{2}...\alpha_{\ell
}v_{\lambda}^{+\mathfrak{g}^{0}}  &  =\left(  -1\right)  ^{\ell}%
\sum\limits_{\sigma\in S_{\ell}}\lambda\left(  \left[  \alpha_{1}%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(
\left[  \alpha_{\ell},\beta_{\sigma\left(  \ell\right)  }\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}\label{prop.det.US.pf.2}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{N}\text{, }\alpha
_{1},\alpha_{2},...,\alpha_{\ell}\in\mathfrak{n}_{-}\text{ and }\beta
_{1},\beta_{2},...,\beta_{\ell}\in\mathfrak{n}_{+}\text{.}\nonumber
\end{align}


\textit{Proof of (\ref{prop.det.US.pf.2}).} We will prove
(\ref{prop.det.US.pf.2}) by induction over $\ell$:

\textit{Induction base:} For $\ell=0$, we have $\underbrace{\beta_{\ell}%
\beta_{\ell-1}...\beta_{1}}_{\text{empty product}}\underbrace{\alpha_{1}%
\alpha_{2}...\alpha_{\ell}}_{\text{empty product}}v_{\lambda}^{+\mathfrak{g}%
^{0}}=v_{\lambda}^{+\mathfrak{g}^{0}}$ and \newline$\underbrace{\left(
-1\right)  ^{\ell}}_{=1}\underbrace{\sum\limits_{\sigma\in S_{\ell}}%
}_{\text{sum over }1\text{ element}}\underbrace{\lambda\left(  \left[
\alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(
\left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  \alpha_{\ell},\beta_{\sigma\left(  \ell\right)
}\right]  \right)  }_{\text{empty product}}v_{\lambda}^{+\mathfrak{g}^{0}%
}=v_{\lambda}^{+\mathfrak{g}^{0}}$. Thus, for $\ell=0$, the equality
(\ref{prop.det.US.pf.2}) holds. This completes the induction base.

\textit{Induction step:} Let $m\in\mathbb{N}$ be positive. Assume that
(\ref{prop.det.US.pf.2}) holds for $\ell=m-1$. We now must show that
(\ref{prop.det.US.pf.2}) holds for $\ell=m$.

Let $\alpha_{1},\alpha_{2},...,\alpha_{m}\in\mathfrak{n}_{-}$ and $\beta
_{1},\beta_{2},...,\beta_{m}\in\mathfrak{n}_{+}$.

For every $p\in\left\{  1,2,...,m\right\}  $, let $c_{p}$ denote the
permutation in $S_{m}$ which is written in row form as $\left(
1,2,...,p-1,p+1,p+2,...,m,p\right)  $. (This is the permutation with cycle
decomposition $\left(  1\right)  \left(  2\right)  ...\left(  p-1\right)
\left(  p,p+1,...,m\right)  $.) Since (\ref{prop.det.US.pf.2}) holds for
$\ell=m-1$, we can apply (\ref{prop.det.US.pf.2}) to $m-1$ and $\left(
\alpha_{c_{p}\left(  1\right)  },\alpha_{c_{p}\left(  2\right)  }%
,...,\alpha_{c_{p}\left(  m-1\right)  }\right)  $ instead of $\ell$ and
$\left(  \alpha_{1},\alpha_{2},...,\alpha_{\ell}\right)  $. This results in%
\begin{align*}
&  \beta_{m-1}\beta_{m-2}...\beta_{1}\alpha_{c_{p}\left(  1\right)  }%
\alpha_{c_{p}\left(  2\right)  }...\alpha_{c_{p}\left(  m-1\right)
}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}%
\underbrace{\lambda\left(  \left[  \alpha_{c_{p}\left(  1\right)  }%
,\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
\alpha_{c_{p}\left(  2\right)  },\beta_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  \alpha_{c_{p}\left(  m-1\right)  }%
,\beta_{\sigma\left(  m-1\right)  }\right]  \right)  }_{\substack{=\prod
\limits_{i\in\left\{  1,2,...,m-1\right\}  }\lambda\left(  \left[
\alpha_{c_{p}\left(  i\right)  },\beta_{\sigma\left(  i\right)  }\right]
\right)  =\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  c_{p}%
^{-1}\left(  i\right)  \right)  }\right]  \right)  \\\text{(here, we
substituted }i\text{ for }c_{p}\left(  i\right)  \text{ in the product)}%
}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}\prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\underbrace{\beta_{\sigma\left(  c_{p}%
^{-1}\left(  i\right)  \right)  }}_{=\beta_{\left(  \sigma\circ c_{p}%
^{-1}\right)  \left(  i\right)  }}\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m-1}}\prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\left(  \sigma\circ c_{p}%
^{-1}\right)  \left(  i\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m};\ \sigma\left(
m\right)  =m}\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\left(  \sigma\circ
c_{p}^{-1}\right)  \left(  i\right)  }\right]  \right)  v_{\lambda
}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we identified the permutations in }S_{m-1}\text{ with the
permutations}\\
\sigma\in S_{m}\text{ satisfying }\sigma\left(  m\right)  =m
\end{array}
\right) \\
&  =\left(  -1\right)  ^{m-1}\sum\limits_{\sigma\in S_{m};\ \sigma\left(
p\right)  =m}\prod\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{
p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)
}\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\sigma\text{ for
}\sigma\circ c_{p}^{-1}\text{ in the sum}\right)  .
\end{align*}


The elements $\beta_{m}$, $\beta_{m-1}$, $...$, $\beta_{1}$ all lie in
$\mathfrak{n}_{+}$ and thus commute in $U\left(  \mathfrak{g}^{0}\right)  $
(since $\left[  \mathfrak{n}_{+},\mathfrak{n}_{+}\right]  ^{0}=0$). Thus,
$\beta_{m}\beta_{m-1}...\beta_{1}=\beta_{m-1}\beta_{m-2}...\beta_{1}\beta_{m}$
in $U\left(  \mathfrak{g}^{0}\right)  $, so that%
\begin{align*}
&  \beta_{m}\beta_{m-1}...\beta_{1}\alpha_{1}\alpha_{2}...\alpha_{m}%
v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\beta_{m-1}\beta_{m-2}...\beta_{1}\underbrace{\beta_{m}\alpha_{1}%
\alpha_{2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}^{0}}}_{\substack{=\sum
\limits_{p=1}^{m}\lambda\left(  \left[  \beta_{m},\alpha_{p}\right]  \right)
\alpha_{1}\alpha_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha
_{m}v_{\lambda}^{+\mathfrak{g}^{0}}\\\text{(by (\ref{prop.det.US.pf.3}),
applied to }\beta=\beta_{m}\text{ and }\ell=m\text{)}}}\\
&  =\beta_{m-1}\beta_{m-2}...\beta_{1}\sum\limits_{p=1}^{m}\lambda\left(
\left[  \beta_{m},\alpha_{p}\right]  \right)  \alpha_{1}\alpha_{2}%
...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha_{m}v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\sum\limits_{p=1}^{m}\underbrace{\lambda\left(  \left[  \beta_{m}%
,\alpha_{p}\right]  \right)  }_{=\lambda\left(  -\left[  \alpha_{p},\beta
_{m}\right]  \right)  =-\lambda\left(  \left[  \alpha_{p},\beta_{m}\right]
\right)  }\beta_{m-1}\beta_{m-2}...\beta_{1}\underbrace{\alpha_{1}\alpha
_{2}...\alpha_{p-1}\alpha_{p+1}\alpha_{p+2}...\alpha_{m}}_{\substack{=\alpha
_{c_{p}\left(  1\right)  }\alpha_{c_{p}\left(  2\right)  }...\alpha
_{c_{p}\left(  m-1\right)  }\\\text{(by the definition of }c_{p}\text{)}%
}}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =-\sum\limits_{p=1}^{m}\lambda\left(  \left[  \alpha_{p},\beta_{m}\right]
\right)  \underbrace{\beta_{m-1}\beta_{m-2}...\beta_{1}\alpha_{c_{p}\left(
1\right)  }\alpha_{c_{p}\left(  2\right)  }...\alpha_{c_{p}\left(  m-1\right)
}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)  ^{m-1}\sum
\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)  =m}\prod\limits_{i\in
\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }\lambda\left(
\left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]  \right)
v_{\lambda}^{+\mathfrak{g}^{0}}}\\
&  =\underbrace{-\left(  -1\right)  ^{m-1}}_{=\left(  -1\right)  ^{m}}%
\sum\limits_{p=1}^{m}\sum\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)
=m}\lambda\left(  \left[  \alpha_{p},\underbrace{\beta_{m}}_{\substack{=\beta
_{\sigma\left(  p\right)  }\\\text{(since }\sigma\left(  p\right)  =m\text{)}%
}}\right]  \right)  \prod\limits_{i\in\left\{  1,2,...,m\right\}
\diagdown\left\{  p\right\}  }\lambda\left(  \left[  \alpha_{i},\beta
_{\sigma\left(  i\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m}\sum\limits_{p=1}^{m}\sum\limits_{\sigma\in
S_{m};\ \sigma\left(  p\right)  =m}\underbrace{\lambda\left(  \left[
\alpha_{p},\beta_{\sigma\left(  p\right)  }\right]  \right)  \prod
\limits_{i\in\left\{  1,2,...,m\right\}  \diagdown\left\{  p\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]
\right)  }_{\substack{=\prod\limits_{i\in\left\{  1,2,...,m\right\}  }%
\lambda\left(  \left[  \alpha_{i},\beta_{\sigma\left(  i\right)  }\right]
\right)  \\=\lambda\left(  \left[  \alpha_{1},\beta_{\sigma\left(  1\right)
}\right]  \right)  \lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  \alpha_{m}%
,\beta_{\sigma\left(  m\right)  }\right]  \right)  }}v_{\lambda}%
^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{m}\underbrace{\sum\limits_{p=1}^{m}\sum
\limits_{\sigma\in S_{m};\ \sigma\left(  p\right)  =m}}_{=\sum\limits_{\sigma
\in S_{m}}}\lambda\left(  \left[  \alpha_{1},\beta_{\sigma\left(  1\right)
}\right]  \right)  \lambda\left(  \left[  \alpha_{2},\beta_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  \alpha_{m}%
,\beta_{\sigma\left(  m\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}}\\
&  =\left(  -1\right)  ^{m}\sum\limits_{\sigma\in S_{m}}\lambda\left(  \left[
\alpha_{1},\beta_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(
\left[  \alpha_{2},\beta_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  \alpha_{m},\beta_{\sigma\left(  m\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}}.
\end{align*}
In other words, (\ref{prop.det.US.pf.2}) is proven for $\ell=m$. This
completes the induction step. Thus, the induction proof of
(\ref{prop.det.US.pf.2}) is done.

Now, back to proving $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$. Applying (\ref{prop.det.US.pf.2}) to $\ell
=u$, $\alpha_{i}=a_{i}$ and $\beta_{i}=b_{i}$, we obtain%
\[
b_{u}b_{u-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}%
=\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}.
\]
Hence, if $v>u$, then%
\begin{align*}
&  b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =b_{v}b_{v-1}...b_{u+2}b_{u+1}\underbrace{b_{u}b_{u-1}...b_{1}a_{1}%
a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)  ^{u}%
\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}}\\
&  =b_{v}b_{v-1}...b_{u+2}b_{u+1}\left(  -1\right)  ^{u}\sum\limits_{\sigma\in
S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]
\right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[
a_{1},b_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
a_{2},b_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(  \left[
a_{u},b_{\sigma\left(  u\right)  }\right]  \right)  b_{v}b_{v-1}%
...b_{u+2}\underbrace{b_{u+1}v_{\lambda}^{+\mathfrak{g}^{0}}}%
_{\substack{=0\\\text{(since }b_{u+1}\in\mathfrak{n}_{+}=\mathfrak{n}_{+}%
^{0}\text{)}}}\\
&  =0,
\end{align*}
and thus%
\begin{align*}
&  \left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{v}\left(  \underbrace{b_{v}b_{v-1}...b_{1}a_{1}%
a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=0},v_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{prop.det.US.pf.4})}\right) \\
&  =0=\left(  a,b\right)  _{\lambda}^{\circ}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the form }\left(  \cdot,\cdot\right)  _{\lambda}^{\circ}\text{
was defined as a restriction of a sum}\\
\bigoplus\limits_{k\geq0}\lambda_{k}:S\left(  \mathfrak{n}_{-}\right)  \times
S\left(  \mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}\text{ of bilinear
forms }\lambda_{k}:S^{k}\left(  \mathfrak{n}_{-}\right)  \times S^{k}\left(
\mathfrak{n}_{+}\right)  \rightarrow\mathbb{C}\text{,}\\
\text{and thus }\left(  S^{u}\left(  \mathfrak{n}_{-}\right)  ,S^{v}\left(
\mathfrak{n}_{+}\right)  \right)  _{\lambda}^{\circ}=0\text{ for }u\neq
v\text{, so that }\left(  a,b\right)  _{\lambda}^{\circ}=0\\
\text{(since }a\in S^{u}\left(  \mathfrak{n}_{-}\right)  \text{ and }b\in
S^{v}\left(  \mathfrak{n}_{+}\right)  \text{ and }u\neq v\text{)}%
\end{array}
\right)  .
\end{align*}
We thus have proven $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$ in the case when $v>u$. It remains to prove
that $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}%
^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(  a,b\right)  _{\lambda
}^{\circ}$ in the case when $v=u$. So let us assume that $v=u$. In this case,%
\begin{align*}
b_{v}b_{v-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}  &
=b_{u}b_{u-1}...b_{1}a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[
a_{1},b_{\sigma\left(  1\right)  }\right]  \right)  \lambda\left(  \left[
a_{2},b_{\sigma\left(  2\right)  }\right]  \right)  ...\lambda\left(  \left[
a_{u},b_{\sigma\left(  u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}%
^{0}},
\end{align*}
so that%
\begin{align*}
&  \left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\underbrace{\left(  -1\right)  ^{v}}_{\substack{=\left(  -1\right)
^{u}\\\text{(since }v=u\text{)}}}\left(  \underbrace{b_{v}b_{v-1}...b_{1}%
a_{1}a_{2}...a_{u}v_{\lambda}^{+\mathfrak{g}^{0}}}_{=\left(  -1\right)
^{u}\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  v_{\lambda}^{+\mathfrak{g}^{0}}},v_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\left(  -1\right)  ^{u}\left(  \left(  -1\right)  ^{u}\sum\limits_{\sigma
\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]
\right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]
\right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]
\right)  v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}^{-\mathfrak{g}^{0}%
}\right)  _{\lambda}^{\mathfrak{g}^{0}}\\
&  =\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)  ^{u}}%
_{\substack{=\left(  -1\right)  ^{u+u}=\left(  -1\right)  ^{2u}%
=1\\\text{(since }2u\text{ is even)}}}\sum\limits_{\sigma\in S_{u}}%
\lambda\left(  \left[  a_{1},b_{\sigma\left(  1\right)  }\right]  \right)
\lambda\left(  \left[  a_{2},b_{\sigma\left(  2\right)  }\right]  \right)
...\lambda\left(  \left[  a_{u},b_{\sigma\left(  u\right)  }\right]  \right)
\underbrace{\left(  v_{\lambda}^{+\mathfrak{g}^{0}},v_{-\lambda}%
^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}}_{=1}\\
&  =\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  .
\end{align*}
Compared to%
\begin{align*}
\left(  \underbrace{a}_{=a_{1}a_{2}...a_{u}},\underbrace{b}_{\substack{=b_{1}%
b_{2}...b_{v}=b_{1}b_{2}...b_{u}\\\text{(since }v=u\text{)}}}\right)
_{\lambda}^{\circ}  &  =\left(  a_{1}a_{2}...a_{u},b_{1}b_{2}...b_{u}\right)
_{\lambda}^{\circ}=\lambda_{u}\left(  a_{1}a_{2}...a_{u},b_{1}b_{2}%
...b_{u}\right) \\
&  =\sum\limits_{\sigma\in S_{u}}\lambda\left(  \left[  a_{1},b_{\sigma\left(
1\right)  }\right]  \right)  \lambda\left(  \left[  a_{2},b_{\sigma\left(
2\right)  }\right]  \right)  ...\lambda\left(  \left[  a_{u},b_{\sigma\left(
u\right)  }\right]  \right)  ,
\end{align*}
this yields $\left(  av_{\lambda}^{+\mathfrak{g}^{0}},bv_{-\lambda
}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}}=\left(
a,b\right)  _{\lambda}^{\circ}$. Now that $\left(  av_{\lambda}^{+\mathfrak{g}%
^{0}},bv_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  a,b\right)  _{\lambda}^{\circ}$ is proven in each of the cases $v>u$
and $v=u$ (and the case $v<u$ is analogous), we are done with proving
(\ref{prop.det.US.pf.1}).

This proves Proposition \ref{lem.invform.g^0.1}.

\begin{corollary}
\label{cor.invform.g^0.1}Let $n\in\mathbb{N}$. Recall that the family $\left(
e_{\mathbf{i}}^{0}\right)  _{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}%
E;\ \deg\mathbf{i}=-n}$ is a basis of the vector space $U\left(
\mathfrak{n}_{-}^{0}\right)  \left[  -n\right]  =S\left(  \mathfrak{n}%
_{-}\right)  \left[  -n\right]  $, and that the family $\left(  e_{\mathbf{j}%
}^{0}\right)  _{\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\ \deg
\mathbf{j}=n}$ is a basis of the vector space $U\left(  \mathfrak{n}_{+}%
^{0}\right)  \left[  n\right]  =S\left(  \mathfrak{n}_{+}\right)  \left[
n\right]  $. Thus, let us represent the bilinear form $\left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}:S\left(  \mathfrak{n}_{-}\right)  \left[
-n\right]  \times S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]  $ by its
matrix with respect to the bases $\left(  e_{\mathbf{i}}^{0}\right)
_{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \deg\mathbf{i}=-n}$ and
$\left(  e_{\mathbf{j}}^{0}\right)  _{\mathbf{j}\in\operatorname*{Seq}%
\nolimits_{+}E;\ \deg\mathbf{j}=n}$ of $S\left(  \mathfrak{n}_{-}\right)
\left[  -n\right]  $ and $S\left(  \mathfrak{n}_{+}\right)  \left[  n\right]
$, respectively. This is the matrix%
\[
\left(  \left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda
,n}^{\circ}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}.
\]
This matrix is a square matrix (since the number of all $\mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E$ satisfying$\ \deg\mathbf{j}=n$ equals
the number of all $\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E$
satisfying$\ \deg\mathbf{i}=-n$), and its determinant is what we are going to
denote by $\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ
}\right)  $.

Then,%
\[
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}%
}\right)  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ
}\right)  .
\]

\end{corollary}

\textit{Proof of Corollary \ref{cor.invform.g^0.1}.} For every $\mathbf{i}%
\in\operatorname*{Seq}\nolimits_{-}E$ satisfying $\deg\mathbf{i}=-n$, and
every $\mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E$ satisfying
$\deg\mathbf{j}=n$, we have%
\begin{align*}
\left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}},e_{\mathbf{j}}%
^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda,n}^{\mathfrak{g}^{0}}
&  =\left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}},e_{\mathbf{j}%
}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda}^{\mathfrak{g}^{0}%
}=\left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda}^{\circ}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.invform.g^0.1}, applied
to }a=e_{\mathbf{i}}^{0}\text{ and }b=e_{\mathbf{j}}^{0}\right) \\
&  =\left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda,n}^{\circ
}.
\end{align*}
Thus,%
\[
\det\left(  \left(  \left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}^{0}%
},e_{\mathbf{j}}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
,n}^{\mathfrak{g}^{0}}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  =\det\left(  \left(  \left(
e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)  _{\lambda,n}^{\circ}\right)
_{\substack{\mathbf{i}\in\operatorname*{Seq}\nolimits_{-}E;\ \mathbf{j}%
\in\operatorname*{Seq}\nolimits_{+}E;\\\deg\mathbf{i}=-n;\ \deg\mathbf{j}%
=n}}\right)  .
\]


Now,%
\begin{align*}
\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)   &
=\det\left(  \left(  \left(  e_{\mathbf{i}}^{0},e_{\mathbf{j}}^{0}\right)
_{\lambda,n}^{\circ}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right) \\
&  =\det\left(  \left(  \left(  e_{\mathbf{i}}^{0}v_{\lambda}^{+\mathfrak{g}%
^{0}},e_{\mathbf{j}}^{0}v_{-\lambda}^{-\mathfrak{g}^{0}}\right)  _{\lambda
,n}^{\mathfrak{g}^{0}}\right)  _{\substack{\mathbf{i}\in\operatorname*{Seq}%
\nolimits_{-}E;\ \mathbf{j}\in\operatorname*{Seq}\nolimits_{+}E;\\\deg
\mathbf{i}=-n;\ \deg\mathbf{j}=n}}\right)  =\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{0}}\right)  .
\end{align*}
This proves Corollary \ref{cor.invform.g^0.1}.

\subsubsection{Proof of Theorem \ref{thm.invformnondeg}: Joining the threads}

\textit{Proof of Proposition \ref{prop.det.US}.} Consider the polynomial
function $Q_{n}:\mathfrak{h}^{\ast}\times\mathbb{C}\rightarrow\mathbb{C}$
introduced in Corollary \ref{cor.invformnondeg.polynomiality}. Due to
Corollary \ref{cor.invformnondeg.polynomiality}, every $\lambda\in V$ and
every nonzero $\varepsilon\in\mathbb{C}$ satisfy%
\[
Q_{n}\left(  \lambda,\varepsilon\right)  =\varepsilon^{2\operatorname*{LEN}%
n}Q_{n}\left(  \lambda/\varepsilon^{2},1\right)  .
\]
Hence, we can apply Lemma \ref{lem.invformnondeg.elemen} to $V=\mathfrak{h}%
^{\ast}$, $\phi=Q_{n}$ and $k=\operatorname*{LEN}n$. Thus, we obtain the
following three observations:

\textit{Observation 1:} The polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)
\]
is homogeneous of degree $k$. (This follows from Lemma
\ref{lem.invformnondeg.elemen} \textbf{(a)}.)

\textit{Observation 2:} For every integer $N>k$, the $N$-th homogeneous
component of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is zero. (This follows from Lemma \ref{lem.invformnondeg.elemen} \textbf{(b)}.)

\textit{Observation 3:} The $k$-th homogeneous component of the polynomial
function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)  .
\]
(This follows from Lemma \ref{lem.invformnondeg.elemen} \textbf{(c)}.)

Since every $\lambda\in\mathfrak{h}^{\ast}$ satisfies%
\begin{align*}
Q_{n}\left(  \lambda,1\right)   &  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{1}}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{cor.invformnondeg.polynomiality.1}) (applied to }%
\varepsilon=1\text{)}\\
\text{yields }\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}%
^{\mathfrak{g}^{1}}\right)  =Q_{n}\left(  \lambda,1\right)
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathfrak{g}^{1}=\mathfrak{g}\text{
and thus }\left(  \cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}^{1}}=\left(
\cdot,\cdot\right)  _{\lambda,n}^{\mathfrak{g}}=\left(  \cdot,\cdot\right)
_{\lambda,n}\right)  ,
\end{align*}
the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,1\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  .
\]
This yields that%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is a polynomial function.

Since every $\lambda\in\mathfrak{h}^{\ast}$ satisfies%
\begin{align*}
Q_{n}\left(  \lambda,0\right)   &  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}^{\mathfrak{g}^{0}}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{cor.invformnondeg.polynomiality.1}) (applied to }%
\varepsilon=0\text{)}\\
\text{yields }\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}%
^{\mathfrak{g}^{0}}\right)  =Q_{n}\left(  \lambda,0\right)
\end{array}
\right) \\
&  =\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Corollary \ref{cor.invform.g^0.1}%
}\right)  ,
\end{align*}
the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda\mapsto
Q_{n}\left(  \lambda,0\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]
This yields that%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is a polynomial function. This polynomial function is not identically
zero\footnote{\textit{Proof.} Since $\mathfrak{g}$ is nondegenerate, there
exists $\lambda\in\mathfrak{h}^{\ast}$ such that the bilinear form%
\[
\mathfrak{g}_{-k}\times\mathfrak{g}_{k}\rightarrow\mathbb{C}%
,\ \ \ \ \ \ \ \ \ \ \left(  a,b\right)  \mapsto\lambda\left(  \left[
a,b\right]  \right)
\]
is nondegenerate for every $k\in\left\{  1,2,...,n\right\}  $. For such
$\lambda$, the form $\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}$ must be
nondegenerate (by Lemma \ref{lem.lambda_k.2}), so that $\det\left(  \left(
\cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. Hence, there exists
$\lambda\in\mathfrak{h}^{\ast}$ such that $\det\left(  \left(  \cdot
,\cdot\right)  _{\lambda,n}^{\circ}\right)  \neq0$. In other words, the
polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is not identically zero, qed.}.

Since $Q_{n}\left(  \lambda,1\right)  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ for every $\lambda\in\mathfrak{h}^{\ast}$, Observation
2 rewrites as follows:

\textit{Observation 2':} For every integer $n>k$, the $n$-th homogeneous
component of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is zero.

Since $Q_{n}\left(  \lambda,1\right)  =\det\left(  \left(  \cdot,\cdot\right)
_{\lambda,n}\right)  $ and $Q_{n}\left(  \lambda,0\right)  =\det\left(
\left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  $ for every
$\lambda\in\mathfrak{h}^{\ast}$, Observation 3 rewrites as follows:

\textit{Observation 3':} The $k$-th homogeneous component of the polynomial
function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)
\]
is the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)  .
\]


Combining Observations 2' and 3' and the fact that the polynomial function
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is not identically zero, we conclude that the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}^{\circ}\right)
\]
is the leading term of the polynomial function%
\[
\mathfrak{h}^{\ast}\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \lambda
\mapsto\det\left(  \left(  \cdot,\cdot\right)  _{\lambda,n}\right)  .
\]


This proves Proposition \ref{prop.det.US}.

Now that Proposition \ref{prop.det.US} is proven, the proof of Theorem
\ref{thm.invformnondeg} is also complete (because we have already proven
Theorem \ref{thm.invformnondeg} using Proposition \ref{prop.det.US}).

\subsection{The irreducible quotients of the Verma modules}

We will now use the form $\left(  \cdot,\cdot\right)  _{\lambda}$ to develop
the representation theory of $\mathfrak{g}$. In the following, we assume that
$\mathfrak{g}$ is nondegenerate.

\begin{definition}
Let $\left(  \cdot,\cdot\right)  $ denote the form $\left(  \cdot
,\cdot\right)  _{\lambda}$. Let $J_{\lambda}^{\pm}$ be the kernel of $\left(
\cdot,\cdot\right)  $ on $M_{\lambda}^{\pm}$. This is a graded $\mathfrak{g}%
$-submodule of $M_{\lambda}^{\pm}$ (since the form $\left(  \cdot
,\cdot\right)  $ is $\mathfrak{g}$-invariant). Let $L_{\lambda}^{\pm}$ be the
quotient module $M_{\lambda}^{\pm}\diagup J_{\lambda}^{\pm}$. Then, $\left(
\cdot,\cdot\right)  $ descends to a nondegenerate pairing $L_{\lambda}%
^{+}\times L_{-\lambda}^{-}\rightarrow\mathbb{C}$.
\end{definition}

\begin{remark}
For Weil-generic $\lambda$ (away from a countable union of hypersurfaces), we
have $J_{\lambda}^{\pm}=0$ (by Theorem \ref{thm.invformnondeg}) and thus
$L_{\lambda}^{\pm}=M_{\lambda}^{\pm}$.
\end{remark}

\begin{theorem}
\label{thm.verma}\textbf{(i)} The $\mathfrak{g}$-module $L_{\lambda}^{\pm}$ is irreducible.

\textbf{(ii)} The $\mathfrak{g}$-module $J_{\lambda}^{\pm}$ is the maximal
proper graded submodule of $M_{\lambda}^{\pm}$. (This means that $J_{\lambda
}^{\pm}$ contains all proper graded submodules in $M_{\lambda}^{\pm}$.)

\textbf{(iii)} Assume that there exists some $L\in\mathfrak{g}_{0}$ such that
every $n\in\mathbb{Z}$ satisfies
\[
\left(  \operatorname*{ad}L\right)  \mid_{\mathfrak{g}_{n}}=n\cdot
\operatorname*{id}\mid_{\mathfrak{g}_{n}}.
\]
(In this case it is said that \textit{the grading on }$\mathfrak{g}$
\textit{is internal}, i. e., comes from bracketing with some $L\in
\mathfrak{g}_{0}$.) Then $J_{\lambda}^{\pm}$ is the maximal proper submodule
of $M_{\lambda}^{\pm}$.
\end{theorem}

\begin{remark}
Here are two examples of cases when the grading on $\mathfrak{g}$ is internal:

\textbf{(a)} If $\mathfrak{g}$ is a simple finite-dimensional Lie algebra,
then we know (from Proposition \ref{prop.grad.g}) that choosing a Cartan
subalgebra $\mathfrak{h}$ and corresponding Chevalley generators $e_{1}$,
$e_{2}$, $...$, $e_{m}$, $f_{1}$, $f_{2}$, $...$, $f_{m}$, $h_{1}$, $h_{2}$,
$...$, $h_{m}$ of $\mathfrak{g}$ endows $\mathfrak{g}$ with a grading. This
grading is internal. In fact, in this case, we can take $L=\rho^{\vee}$, where
$\rho^{\vee}$ is defined as the element of $\mathfrak{h}$ satisfying
$\alpha_{i}\left(  \rho^{\vee}\right)  =1$ for all $i$ (where $\alpha_{i}$ are
the simple roots of $\mathfrak{g}$). Since the actions of the $\alpha_{i}$ on
$\mathfrak{h}$ are a basis of $\mathfrak{h}^{\ast}$, this $\rho^{\vee}$ is
well-defined and unique. (But it depends on the choice of $\mathfrak{h}$ and
the Chevalley generators, of course.)

\textbf{(b)} If $\mathfrak{g}=\operatorname*{Vir}$, then the grading on
$\mathfrak{g}$ is internal. In fact, in this case, we can take $L=-L_{0}$.

On the other hand, if $\mathfrak{g}$ is the affine Kac-Moody algebra
$\widehat{\mathfrak{g}}_{\omega}$ of Definition \ref{def.kac}, then the
grading on $\mathfrak{g}$ is not internal.
\end{remark}

\textit{Proof of Theorem \ref{thm.verma}.} \textbf{(i)} Let us show that
$L_{\lambda}^{-}$ is irreducible (the proof for $L_{\lambda}^{+}$ will be similar).

In fact, assume the contrary. Then, there exists a nonzero $w\in L_{\lambda
}^{-}$ such that $U\left(  \mathfrak{g}\right)  \cdot w\neq L_{\lambda}^{-}$.
Since $L_{\lambda}^{-}$ is graded by \textit{nonnegative} integers, we can
choose $w$ to have the smallest possible degree $m$ (without necessarily being
homogeneous). Clearly, $m>0$. Thus we can write $w=w_{0}+w_{1}+...+w_{m}$,
where each $w_{i}$ is homogeneous of degree $\deg w_{i}=i$ and $w_{m}\neq0$.

Let $a\in\mathfrak{g}_{j}$ for some $j<0$. Then $aw=0$ (since $\deg\left(
aw\right)  <\deg w$, but still $U\left(  \mathfrak{g}\right)  \cdot aw\neq
L_{\lambda}^{-}$ (since $U\left(  \mathfrak{g}\right)  \cdot aw\subseteq
U\left(  \mathfrak{g}\right)  \cdot w$ and $U\left(  \mathfrak{g}\right)
\cdot w\neq L_{\lambda}^{-}$), and we have chosen $w$ to have the smallest
possible degree). By homogeneity, this yields $aw_{m}=0$ (since $aw_{m}$ is
the $\left(  m+j\right)  $-th homogeneous component of $aw$).

For every $u\in L_{-\lambda}^{+}\left[  -m-j\right]  $, the term $\left(
au,w_{m}\right)  $ is well-defined (since $au\in L_{-\lambda}^{+}$ and
$w_{m}\in L_{\lambda}^{-}$). Since the form $\left(  \cdot,\cdot\right)  $ is
$\mathfrak{g}$-invariant, it satisfies $\left(  au,w_{m}\right)  =-\left(
u,\underbrace{aw_{m}}_{=0}\right)  =0$. But since $m>0$, we have $L_{-\lambda
}^{+}\left[  -m\right]  =\sum\limits_{j<0}\mathfrak{g}_{j}\cdot L_{-\lambda
}^{+}\left[  -m-j\right]  $ (because Proposition \ref{prop.verma1}
\textbf{(a)} yields $M_{-\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\lambda}^{+}$, so that $L_{-\lambda}^{+}=U\left(  \mathfrak{n}_{-}\right)
\overline{v_{\lambda}^{+}}$, thus%
\[
L_{-\lambda}^{+}\left[  -m\right]  =\underbrace{U\left(  \mathfrak{n}%
_{-}\right)  \left[  -m\right]  }_{=\sum\limits_{j<0}\left(  \mathfrak{n}%
_{-}\right)  \left[  j\right]  \cdot U\left(  \mathfrak{n}_{-}\right)  \left[
-m-j\right]  }\overline{v_{\lambda}^{+}}=\sum\limits_{j<0}\underbrace{\left(
\mathfrak{n}_{-}\right)  \left[  j\right]  }_{=\mathfrak{g}\left[  j\right]
=\mathfrak{g}_{j}}\cdot\underbrace{U\left(  \mathfrak{n}_{-}\right)  \left[
-m-j\right]  \overline{v_{\lambda}^{+}}}_{\substack{=L_{-\lambda}^{+}\left[
-m-j\right]  \\\text{(since }U\left(  \mathfrak{n}_{-}\right)  \overline
{v_{\lambda}^{+}}=L_{-\lambda}^{+}\text{)}}}=\sum\limits_{j<0}\mathfrak{g}%
_{j}\cdot L_{-\lambda}^{+}\left[  -m-j\right]
\]
). Hence, any element of $L_{-\lambda}^{+}\left[  -m\right]  $ is a linear
combination of elements of the form $au$ with $a\in\mathfrak{g}_{j}$ (for
$j<0$) and $u\in L_{-\lambda}^{+}\left[  -m-j\right]  $. Thus, since we know
that $\left(  au,w_{m}\right)  =0$ for every $a\in\mathfrak{g}_{j}$ and $u\in
L_{-\lambda}^{+}\left[  -m-j\right]  $, we conclude that $\left(  L_{-\lambda
}^{+}\left[  -m\right]  ,w_{m}\right)  =0$. As a consequence, $\left(
L_{-\lambda}^{+},w_{m}\right)  =0$ (because the form $\left(  \cdot
,\cdot\right)  :L_{-\lambda}^{+}\times L_{\lambda}^{-}\rightarrow\mathbb{C}$
is of degree $0$, and thus $\left(  L_{-\lambda}^{+}\left[  j\right]
,w_{m}\right)  =0$ for all $j\neq-m$). Since the form $\left(  \cdot
,\cdot\right)  :L_{-\lambda}^{+}\times L_{\lambda}^{-}\rightarrow\mathbb{C}$
is nondegenerate, this yields $w_{m}=0$. This is a contradiction to $w_{m}%
\neq0$. This contradiction shows that our assumption was wrong. Thus,
$L_{\lambda}^{-}$ is irreducible. Similarly, $L_{\lambda}^{+}$ is irreducible.

\textbf{(ii)} First let us prove that the $\mathfrak{g}$-module $J_{\lambda
}^{+}$ is the maximal proper graded submodule of $M_{\lambda}^{+}$.

Let $K\subseteq M_{\lambda}^{+}$ be a proper graded submodule, and let
$\overline{K}$ be its image in $L_{\lambda}^{+}$. Then, $K$ lives in strictly
negative degrees (because it is graded, so if it would have a component in
degrees $\geq0$, it would contain $v_{\lambda}^{+}$ and thus contain
everything, and thus not be proper). Hence, $\overline{K}$ also lives in
strictly negative degrees, and thus is proper. Hence, by \textbf{(i)}, we have
$\overline{K}=0$, thus $K\subseteq J_{\lambda}^{+}$. This shows that
$J_{\lambda}^{+}$ is the maximal proper graded submodule of $M_{\lambda}^{+}$.
The proof of the corresponding statement for $J_{\lambda}^{-}$ and
$M_{\lambda}^{-}$ is similar.

\textbf{(iii)} Assume that there exists some $L\in\mathfrak{g}_{0}$ such that
every $n\in\mathbb{Z}$ satisfies
\[
\left(  \operatorname*{ad}L\right)  \mid_{\mathfrak{g}_{n}}=n\cdot
\operatorname*{id}\mid_{\mathfrak{g}_{n}}.
\]
Consider this $L$. It is easy to prove (by induction) that $\left[
L,a\right]  =na$ for every $a\in U\left(  \mathfrak{g}\right)  \left[
n\right]  $.

We are now going to show that all $\mathfrak{g}$-submodules of $M_{\lambda
}^{+}$ are automatically graded.

In fact, it is easy to see that $M_{\lambda}^{+}\left[  n\right]
\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}}-\left(
\lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $ for every
$n\in\mathbb{Z}$.\ \ \ \ \footnote{\textit{Proof.} Let $n\in\mathbb{Z}$. Let
$a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  $. Then, $a\in
U\left(  \mathfrak{g}\right)  \left[  n\right]  $, so that $\left[
L,a\right]  =na$ and thus $La=aL+\underbrace{\left[  L,a\right]  }%
_{=na}=aL+na$. Thus,%
\begin{align*}
\left(  L\mid_{M_{\lambda}^{+}}\right)  \left(  av_{\lambda}^{+}\right)   &
=\underbrace{La}_{=aL+na}v_{\lambda}^{+}=\left(  aL+na\right)  v_{\lambda}%
^{+}=a\underbrace{Lv_{\lambda}^{+}}_{=\lambda\left(  L\right)  v_{\lambda}%
^{+}}+nav_{\lambda}^{+}=\lambda\left(  L\right)  av_{\lambda}^{+}%
+nav_{\lambda}^{+}\\
&  =\left(  \lambda\left(  L\right)  +n\right)  av_{\lambda}^{+},
\end{align*}
so that $av_{\lambda}^{+}\in\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}%
}-\left(  \lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $.
Forget that we fixed $a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]
$. Thus we have showed that every $a\in U\left(  \mathfrak{n}_{-}\right)
\left[  n\right]  $ satisfies $av_{\lambda}^{+}\in\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right)  $. In other words, $\left\{  av_{\lambda}^{+}%
\ \mid\ a\in U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \right\}
\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda}^{+}}-\left(
\lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)  $. Since
$\left\{  av_{\lambda}^{+}\ \mid\ a\in U\left(  \mathfrak{n}_{-}\right)
\left[  n\right]  \right\}  =U\left(  \mathfrak{n}_{-}\right)  \left[
n\right]  \cdot v_{\lambda}^{+}=M_{\lambda}^{+}\left[  n\right]  $, this
becomes $M_{\lambda}^{+}\left[  n\right]  \subseteq\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right)  $, qed.} In other words, for every $n\in\mathbb{Z}%
$, the $n$-th graded component $M_{\lambda}^{+}\left[  n\right]  $ of
$M_{\lambda}^{+}$ is contained in the eigenspace of the operator
$L\mid_{M_{\lambda}^{+}}$ for the eigenvalue $\lambda\left(  L\right)  +n$.
Now,%
\begin{align*}
M_{\lambda}^{+}  &  =\bigoplus\limits_{n\in\mathbb{Z}}M_{\lambda}^{+}\left[
n\right]  =\sum\limits_{n\in\mathbb{Z}}\underbrace{M_{\lambda}^{+}\left[
n\right]  }_{\substack{\subseteq\operatorname*{Ker}\left(  L\mid_{M_{\lambda
}^{+}}-\left(  \lambda\left(  L\right)  +n\right)  \operatorname*{id}\right)
\\=\left(  \text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for
the eigenvalue }\lambda\left(  L\right)  +n\right)  }}\\
&  \subseteq\sum\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  .
\end{align*}
Since all eigenspaces of $L\mid_{M_{\lambda}^{+}}$ are clearly contained in
$M_{\lambda}^{+}$, this rewrites as%
\[
M_{\lambda}^{+}=\sum\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  .
\]
Since eigenspaces of an operator corresponding to distinct eigenvalues are
linearly disjoint, the sum $\sum\limits_{n\in\mathbb{Z}}\left(
\text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for the
eigenvalue }\lambda\left(  L\right)  +n\right)  $ must be a direct sum, so
this becomes%
\begin{equation}
M_{\lambda}^{+}=\bigoplus\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of
the operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)  . \label{thm.verma.pf.5}%
\end{equation}
As a consequence of this, the map $L\mid_{M_{\lambda}^{+}}$ is diagonalizable,
and all of its eigenvalues belong to the set $\left\{  \lambda\left(
L\right)  +n\ \mid\ n\in\mathbb{Z}\right\}  $.

So for every $n\in\mathbb{Z}$, we have the inclusion%
\begin{align*}
M_{\lambda}^{+}\left[  n\right]   &  \subseteq\operatorname*{Ker}\left(
L\mid_{M_{\lambda}^{+}}-\left(  \lambda\left(  L\right)  +n\right)
\operatorname*{id}\right) \\
&  =\left(  \text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{
for the eigenvalue }\lambda\left(  L\right)  +n\right)  ,
\end{align*}
but the direct sum of these inclusions over all $n\in\mathbb{Z}$ is an
equality (since%
\[
\bigoplus\limits_{n\in\mathbb{Z}}M_{\lambda}^{+}\left[  n\right]  =M_{\lambda
}^{+}=\bigoplus\limits_{n\in\mathbb{Z}}\left(  \text{eigenspace of the
operator }L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(
L\right)  +n\right)
\]
by (\ref{thm.verma.pf.5})). Hence, each of these inclusions must be an
equality. In other words,
\begin{equation}
M_{\lambda}^{+}\left[  n\right]  =\left(  \text{eigenspace of the operator
}L\mid_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}.
\label{thm.verma.pf.6}%
\end{equation}


Now, let $K$ be a $\mathfrak{g}$-submodule of $M_{\lambda}^{+}$. Then,
$L\mid_{K}$ is a restriction of $L\mid_{M_{\lambda}^{+}}$ to $K$. Hence, map
$L\mid_{K}$ is diagonalizable, and all of its eigenvalues belong to the set
$\left\{  \lambda\left(  L\right)  +n\ \mid\ n\in\mathbb{Z}\right\}  $
(because we know that the map $L\mid_{M_{\lambda}^{+}}$ is diagonalizable, and
all of its eigenvalues belong to the set $\left\{  \lambda\left(  L\right)
+n\ \mid\ n\in\mathbb{Z}\right\}  $). In other words,%
\begin{align*}
K  &  =\bigoplus\limits_{n\in\mathbb{Z}}\underbrace{\left(  \text{eigenspace
of the operator }L\mid_{K}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  }_{=K\cap\left(  \text{eigenspace of the operator }L\mid
_{M_{\lambda}^{+}}\text{ for the eigenvalue }\lambda\left(  L\right)
+n\right)  }\\
&  =\bigoplus\limits_{n\in\mathbb{Z}}\left(  K\cap\underbrace{\left(
\text{eigenspace of the operator }L\mid_{M_{\lambda}^{+}}\text{ for the
eigenvalue }\lambda\left(  L\right)  +n\right)  }_{=M_{\lambda}^{+}\left[
n\right]  }\right) \\
&  =\bigoplus\limits_{n\in\mathbb{Z}}\left(  K\cap M_{\lambda}^{+}\left[
n\right]  \right)  .
\end{align*}
Hence, $K$ is graded. We thus have shown that every $\mathfrak{g}$-submodule
of $M_{\lambda}^{+}$ is graded. Similarly, every $\mathfrak{g}$-submodule of
$M_{\lambda}^{-}$ is graded. Thus, Theorem \ref{thm.verma} \textbf{(iii)}
follows from Theorem \ref{thm.verma} \textbf{(ii)}.

\begin{remark}
Theorem \ref{thm.verma} \textbf{(ii)} does not hold if the word "graded" is
removed. In fact, here is a counterexample: Let $\mathfrak{g}$ be the
3-dimensional Heisenberg algebra. (This is the Lie algebra with vector-space
basis $\left(  x,K,y\right)  $ and with Lie bracket given by $\left[
y,x\right]  =K$, $\left[  x,K\right]  =0$ and $\left[  y,K\right]  =0$. It can
be considered as a Lie subalgebra of the oscillator algebra $\mathcal{A}$
defined in Definition \ref{def.osc}.) It is easy to see that $\mathfrak{g}$
becomes a nondegenerate $\mathbb{Z}$-graded Lie algebra by setting
$\mathfrak{g}_{-1}=\left\langle x\right\rangle $, $\mathfrak{g}_{0}%
=\left\langle K\right\rangle $, $\mathfrak{g}_{1}=\left\langle y\right\rangle
$ and $\mathfrak{g}_{i}=0$ for every $i\in\mathbb{Z}\diagdown\left\{
-1,0,1\right\}  $. Then, on the Verma highest-weight module $M_{0}%
^{+}=\mathbb{C}\left[  x\right]  v_{0}^{+}$, both $K$ and $y$ act as $0$ (and
$x$ acts as multiplication with $x$), so that $Iv_{0}^{+}$ is a $\mathfrak{g}%
$-submodule of $M_{0}^{+}$ for every ideal $I\subseteq\mathbb{C}\left[
x\right]  $, but not all of these ideals are graded, and not all of them are
contained in $J_{0}^{+}$ (as can be easily checked).
\end{remark}

\begin{corollary}
\label{cor.verma.irred}For Weil-generic $\lambda$ (this means a $\lambda$
outside of countably many hypersurfaces in $\mathfrak{h}^{\ast}$), the
$\mathfrak{g}$-modules $M_{\lambda}^{+}$ and $M_{\lambda}^{-}$ are irreducible.
\end{corollary}

\begin{definition}
Let $Y$ be a $\mathfrak{g}$-module. A vector $w\in Y$ is called a
\textit{singular vector of weight }$\mu\in\mathfrak{h}^{\ast}$ (here, recall
that $\mathfrak{h}=\mathfrak{g}_{0}$) if it satisfies%
\[
hw=\mu\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\mathfrak{h}%
\]
and%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0\text{.}%
\]
We denote by $\operatorname*{Sing}\nolimits_{\mu}\left(  Y\right)  $ the space
of singular vectors of $Y$ of weight $\mu$.
\end{definition}

When people talk about "singular vectors", they usually mean nonzero singular
vectors in negative degrees. We are not going to adhere to this convention, though.

\begin{lemma}
\label{lem.singvec}Let $Y$ be a $\mathfrak{g}$-module. Then there is a
canonical isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
&  \rightarrow\operatorname*{Sing}\nolimits_{\lambda}Y,\\
\phi &  \mapsto\phi\left(  v_{\lambda}^{+}\right)  .
\end{align*}

\end{lemma}

\textit{Proof of Lemma \ref{lem.singvec}.} We have $M_{\lambda}^{+}=U\left(
\mathfrak{g}\right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}%
_{+}\right)  }\mathbb{C}_{\lambda}=\operatorname*{Ind}\nolimits_{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}_{\lambda}$, so that
\[
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
=\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  \operatorname*{Ind}%
\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}^{\mathfrak{g}}\mathbb{C}%
_{\lambda},Y\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{h}%
\oplus\mathfrak{n}_{+}}\left(  \mathbb{C}_{\lambda},Y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Frobenius reciprocity}\right)  .
\]
But $\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\mathbb{C}_{\lambda},Y\right)  \cong\operatorname*{Sing}\nolimits_{\lambda}Y$
(because every $\mathbb{C}$-linear map $\mathbb{C}_{\lambda}\rightarrow Y$ is
uniquely determined by the image of $v_{\lambda}^{+}$, and this map is a
$\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  $-module map if and only
if this image is a singular vector of $Y$ of weight $\lambda$). Thus,
$\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{h}\oplus\mathfrak{n}_{+}}\left(
\mathbb{C}_{\lambda},Y\right)  \cong\operatorname*{Sing}\nolimits_{\lambda}Y$.
If we make this isomorphism explicit, we notice that it sends every $\phi$ to
$\phi\left(  v_{\lambda}^{+}\right)  $, so that Lemma \ref{lem.singvec} is proven.

\begin{corollary}
\label{cor.singvec}The representation $M_{\lambda}^{+}$ is irreducible if and
only if it does not have nonzero singular vectors in negative degrees. Here, a
vector in $M_{\lambda}^{+}$ is said to be "in negative degrees" if its
projection on the $0$-th graded component $M_{\lambda}^{+}\left[  0\right]  $
is zero.
\end{corollary}

\textit{Proof of Corollary \ref{cor.singvec}.} $\Longleftarrow:$ Assume that
$M_{\lambda}^{+}$ does not have nonzero singular vectors in negative degrees.

We must then show that $M_{\lambda}^{+}$ is irreducible.

In fact, assume the contrary. Then, $M_{\lambda}^{+}$ is not irreducible.
Hence, there exists a nonzero \textit{homogeneous} $v\in M_{\lambda}^{+}$ such
that $U\left(  \mathfrak{g}\right)  \cdot v\neq M_{\lambda}^{+}$%
.\ \ \ \ \footnote{\textit{Proof.} Notice that $M_{\lambda}^{+}$ is a graded
$U\left(  \mathfrak{g}\right)  $-module (since $M_{\lambda}^{+}$ is a graded
$\mathfrak{g}$-module).
\par
Since $M_{\lambda}^{+}$ is not irreducible, there exists a nonzero $w\in
M_{\lambda}^{+}$ such that $U\left(  \mathfrak{g}\right)  \cdot w\neq
M_{\lambda}^{+}$. Since $M_{\lambda}^{+}$ is graded by \textit{nonpositive}
integers, we can write $w$ in the form $w=\sum\limits_{j=0}^{m}w_{j}$, where
each $w_{i}$ is homogeneous of degree $\deg w_{i}=-i$ and $m\in\mathbb{Z}$.
Now,
\begin{align*}
\underbrace{U\left(  \mathfrak{g}\right)  }_{=\sum\limits_{i\in\mathbb{Z}%
}U\left(  \mathfrak{g}\right)  \left[  i\right]  }\cdot\underbrace{w}%
_{=\sum\limits_{j=0}^{m}w_{j}}  &  =\left(  \sum\limits_{i\in\mathbb{Z}%
}U\left(  \mathfrak{g}\right)  \left[  i\right]  \right)  \cdot\left(
\sum\limits_{j=0}^{m}w_{j}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}%
\right)  \left[  i\right]  \cdot w_{j}.
\end{align*}
Hence, for every $n\in\mathbb{Z}$, we have
\begin{align*}
\left(  U\left(  \mathfrak{g}\right)  \cdot w\right)  \left[  n\right]   &
=\left(  \sum\limits_{i\in\mathbb{Z}}\sum\limits_{j=0}^{m}U\left(
\mathfrak{g}\right)  \left[  i\right]  \cdot w_{j}\right)  \left[  n\right]
=\sum\limits_{j=0}^{m}\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}U\left(
\mathfrak{g}\right)  \left[  i\right]  \cdot w_{j}\right)  }%
_{\substack{\subseteq U\left(  \mathfrak{g}\right)  \left[  i-j\right]
\\\text{(since }\deg w_{j}=-j\text{ and since}\\M_{\lambda}^{+}\text{ is a
graded }U\left(  \mathfrak{g}\right)  \text{-module)}}}\left[  n\right] \\
&  =\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}\right)  \left[  n+j\right]
\cdot w_{j}.
\end{align*}
Now, since $U\left(  \mathfrak{g}\right)  \cdot w\neq M_{\lambda}^{+}$, there
exists at least one $n\in\mathbb{Z}$ such that $\left(  U\left(
\mathfrak{g}\right)  \cdot w\right)  \left[  n\right]  \neq M_{\lambda}%
^{+}\left[  n\right]  $. Consider such an $n$. Then, $M_{\lambda}^{+}\left[
n\right]  \neq\left(  U\left(  \mathfrak{g}\right)  \cdot w\right)  \left[
n\right]  =\sum\limits_{j=0}^{m}U\left(  \mathfrak{g}\right)  \left[
n+j\right]  \cdot w_{j}$. Thus, $U\left(  \mathfrak{g}\right)  \left[
n+j\right]  \cdot w_{j}\neq M_{\lambda}^{+}\left[  n\right]  $ for all
$j\in\left\{  0,1,...,m\right\}  $. But some $j\in\left\{  0,1,...,m\right\}
$ satisfies $w_{j}\neq0$ (since $\sum\limits_{j=0}^{m}w_{j}=w\neq0$). Consider
this $j$. Then, $w_{j}$ is a nonzero homogeneous element of $M_{\lambda}^{+}$
satisfying $U\left(  \mathfrak{g}\right)  \cdot w_{j}\neq M_{\lambda}^{+}$
(because $\left(  U\left(  \mathfrak{g}\right)  \cdot w_{j}\right)  \left[
n\right]  =U\left(  \mathfrak{g}\right)  \left[  n+j\right]  \cdot w_{j}\neq
M_{\lambda}^{+}\left[  n\right]  $). This proves that there exists a nonzero
\textit{homogeneous} $v\in M_{\lambda}^{+}$ such that $U\left(  \mathfrak{g}%
\right)  \cdot v\neq M_{\lambda}^{+}$. Qed.} Consider this $v$. Then,
$U\left(  \mathfrak{g}\right)  \cdot v$ is a proper graded submodule of
$M_{\lambda}^{+}$, and thus is contained in $J_{\lambda}^{+}$. Hence,
$J_{\lambda}^{+}\neq0$.

There exist some $d\in\mathbb{Z}$ such that $J_{\lambda}^{+}\left[  d\right]
\neq0$ (since $J_{\lambda}^{+}\neq0$ and since $J_{\lambda}^{+}$ is graded).
All such $d$ are nonpositive (since $J_{\lambda}^{+}$ is nonpositively
graded). Thus, there exists a highest integer $d$ such that $J_{\lambda}%
^{+}\left[  d\right]  \neq0$. Consider this $d$. Clearly, $d<0$ (since the
bilinear form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times M_{-\lambda
}^{-}$ is obviously nondegenerate on $M_{\lambda}^{+}\left[  0\right]  \times
M_{-\lambda}^{-}\left[  0\right]  $, so that $J_{\lambda}^{+}\left[  0\right]
=0$).

Every $i>0$ satisfies
\begin{align*}
\mathfrak{g}_{i}\cdot\left(  J_{\lambda}^{+}\left[  d\right]  \right)   &
\subseteq J_{\lambda}^{+}\left[  i+d\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }J_{\lambda}^{+}\text{ is a graded }\mathfrak{g}\text{-module}%
\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i+d>d\text{, but }d\text{ was
the highest integer such that }J_{\lambda}^{+}\left[  d\right]  \neq0\right)
.
\end{align*}


By Conditions \textbf{(1)} and \textbf{(2)} of Definition
\ref{def.gradLienondeg}, the Lie algebra $\mathfrak{g}_{0}$ is abelian and
finite-dimensional. Hence, every nonzero $\mathfrak{g}_{0}$-module has a
one-dimensional submodule\footnote{\textit{Proof.} This is because of the
following fact:
\par
Every nonzero finite-dimensional module over an abelian finite-dimensional Lie
algebra has a one-dimensional submodule. (This is just a restatement of the
fact that a finite set of pairwise commuting matrices on a finite-dimensional
nonzero $\mathbb{C}$-vector space has a common nonzero eigenvector.)}. Thus,
the nonzero $\mathfrak{g}_{0}$-module $J_{\lambda}^{+}\left[  d\right]  $ has
a one-dimensional submodule. Let $w$ be the generator of this submodule. Then,
this submodule is $\left\langle w\right\rangle $.

For every $h\in\mathfrak{h}$, the vector $hw$ is a scalar multiple of $w$
(since $h\in\mathfrak{h}=\mathfrak{g}_{0}$, so that $hw$ lies in the
$\mathfrak{g}_{0}$-submodule of $J_{\lambda}^{+}\left[  d\right]  $ generated
by $w$, but this submodule is $\left\langle w\right\rangle $). Thus, we can
write $hw=\lambda_{h}w$ for some $\lambda_{h}\in\mathbb{C}$. This $\lambda
_{h}$ is uniquely determined (since $w\neq0$), so we can define a map
$\mu:\mathfrak{h}\rightarrow\mathbb{C}$ such that $\mu\left(  h\right)
=\lambda_{h}$ for every $h\in\mathfrak{h}$. This map $\mu$ is easily seen to
be $\mathbb{C}$-linear, so that we have found a $\mu\in\mathfrak{h}^{\ast}$
such that%
\[
hw=\mu\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }h\in
\mathfrak{h}.
\]
Also,%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0
\]
(since $\underbrace{a}_{\in\mathfrak{g}_{i}}\underbrace{w}_{\in J_{\lambda
}^{+}\left[  d\right]  }\in\mathfrak{g}_{i}\cdot\left(  J_{\lambda}^{+}\left[
d\right]  \right)  \subseteq0$). Thus, $w$ is a nonzero singular vector. Since
$w\in J_{\lambda}^{+}\left[  d\right]  $ and $d<0$, this vector $w$ is in
negative degrees. This contradicts to the assumption that $M_{\lambda}^{+}$
does not have nonzero singular vectors in negative degrees. This contradiction
shows that our assumption was wrong, so that $M_{\lambda}^{+}$ is irreducible.
This proves the $\Longleftarrow$ direction of Corollary \ref{cor.singvec}.

$\Longrightarrow:$ Assume that $M_{\lambda}^{+}$ is irreducible.

We must then show that $M_{\lambda}^{+}$ does not have nonzero singular
vectors in negative degrees.

Let $v$ be a singular vector of $M_{\lambda}^{+}$ in negative degrees. Let it
be a singular vector of weight $\mu$ for some $\mu\in\mathfrak{h}^{\ast}$.

By Lemma \ref{lem.singvec} (applied to $\mu$ and $M_{\lambda}^{+}$ instead of
$\lambda$ and $Y$), we have an isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\mu}^{+},M_{\lambda}%
^{+}\right)   &  \rightarrow\operatorname*{Sing}\nolimits_{\mu}\left(
M_{\lambda}^{+}\right)  ,\\
\phi &  \mapsto\phi\left(  v_{\mu}^{+}\right)  .
\end{align*}
Let $\phi$ be the preimage of $v$ under this isomorphism. Then, $v=\phi\left(
v_{\mu}^{+}\right)  $.

Since $v$ is in negative degrees, we have $v\in\sum\limits_{n<0}M_{\lambda
}^{+}\left[  n\right]  $. Now, $M_{\mu}^{+}=U\left(  \mathfrak{n}_{-}\right)
v_{\mu}^{+}=\sum\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[
m\right]  v_{\mu}^{+}$ (since $M_{\mu}^{+}$ is nonpositively graded), so that%
\begin{align*}
\phi\left(  M_{\mu}^{+}\right)   &  =\phi\left(  \sum\limits_{m\leq0}U\left(
\mathfrak{n}_{-}\right)  \left[  m\right]  v_{\mu}^{+}\right)  =\sum
\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[  m\right]
\underbrace{\phi\left(  v_{\mu}^{+}\right)  }_{=v\in\sum\limits_{n<0}%
M_{\lambda}^{+}\left[  n\right]  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\phi\in\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\mu}%
^{+},M_{\lambda}^{+}\right)  \right) \\
&  \in\sum\limits_{m\leq0}U\left(  \mathfrak{n}_{-}\right)  \left[  m\right]
\sum\limits_{n<0}M_{\lambda}^{+}\left[  n\right]  =\sum\limits_{m\leq0}%
\sum\limits_{n<0}\underbrace{U\left(  \mathfrak{n}_{-}\right)  \left[
m\right]  \cdot M_{\lambda}^{+}\left[  n\right]  }_{\substack{\subseteq
M_{\lambda}^{+}\left[  m+n\right]  \\\text{(since }M_{\lambda}^{+}\text{ is a
graded }\mathfrak{g}\text{-module)}}}\\
&  \subseteq\sum\limits_{m\leq0}\sum\limits_{n<0}M_{\lambda}^{+}\left[
m+n\right]  \subseteq\sum\limits_{r<0}M_{\lambda}^{+}\left[  r\right]  .
\end{align*}
Thus, the projection of $\phi\left(  M_{\mu}^{+}\right)  $ onto the $0$-th
degree of $M_{\lambda}^{+}$ is $0$. Hence, $\phi\left(  M_{\mu}^{+}\right)  $
is a proper $\mathfrak{g}$-submodule of $M_{\lambda}^{+}$. Therefore,
$\phi\left(  M_{\mu}^{+}\right)  =0$ (since $M_{\lambda}^{+}$ is irreducible).
Thus, $v=\phi\left(  v_{\mu}^{+}\right)  \in\phi\left(  M_{\mu}^{+}\right)
=0$, so that $v=0$.

We have thus proven: Whenever $v$ is a singular vector of $M_{\lambda}^{+}$ in
negative degrees, we have $v=0$. In other words, $M_{\lambda}^{+}$ does not
have nonzero singular vectors in negative degrees. This proves the
$\Longrightarrow$ direction of Corollary \ref{cor.singvec}.

Here is a variation on Corollary \ref{cor.singvec}:

\begin{corollary}
\label{cor.singvec.2}The representation $M_{\lambda}^{+}$ is irreducible if
and only if it does not have nonzero homogeneous singular vectors in negative degrees.
\end{corollary}

\textit{Proof of Corollary \ref{cor.singvec.2}.} $\Longrightarrow:$ This
follows from the $\Longrightarrow$ direction of Corollary \ref{cor.singvec}.

$\Longleftarrow:$ Repeat the proof of the $\Longleftarrow$ direction of
Corollary \ref{cor.singvec}, noticing that $w$ is homogeneous (since $w\in
J_{\lambda}^{+}\left[  d\right]  $).

Corollary \ref{cor.singvec.2} is thus proven.

\subsection{Highest/lowest-weight modules}

\begin{definition}
A \textit{highest-weight module} with highest weight $\lambda\in
\mathfrak{h}^{\ast}$ means a quotient $V$ of the graded $\mathfrak{g}$-module
$M_{\lambda}^{+}$ by a proper graded submodule. The projection of $v_{\lambda
}^{+}\in M_{\lambda}^{+}$ onto this quotient will be called a
\textit{highest-weight vector} of $V$. (Note that a highest-weight module may
have several highest-weight vectors: in fact, every nonzero vector in its
$0$-th graded component is a highest-weight vector.) The notion
"highest-weight representation" is also used as a synonym for "highest-weight module".

A \textit{lowest-weight module} with lowest weight $\lambda\in\mathfrak{h}%
^{\ast}$ means a quotient $V$ of the graded $\mathfrak{g}$-module $M_{\lambda
}^{-}$ by a proper graded submodule. The projection of $v_{\lambda}^{-}\in
M_{\lambda}^{-}$ onto this quotient will be called a \textit{lowest-weight
vector} of $V$. (Note that a lowest-weight module may have several
lowest-weight vectors: in fact, every nonzero vector in its $0$-th graded
component is a lowest-weight vector.) The notion "lowest-weight
representation" is also used as a synonym for "lowest-weight module".

If $Y$ is a highest-weight module with highest weight $\lambda$, then we have
an exact sequence $%
%TCIMACRO{\TeXButton{M surj Y surj L}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{+}_{\lambda}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{+}_{\lambda}
}%
%EndExpansion
$ (by Theorem \ref{thm.verma} \textbf{(ii)}).

If $Y$ is a lowest-weight module with lowest weight $\lambda$, then we have an
exact sequence $%
%TCIMACRO{\TeXButton{M surj Y surj L}{\xymatrix{
%M^{-}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{-}_{\lambda}
%}}}%
%BeginExpansion
\xymatrix{
M^{-}_{\lambda} \arsurj[r] & Y \arsurj[r] & L^{-}_{\lambda}
}%
%EndExpansion
$ (by Theorem \ref{thm.verma} \textbf{(ii)}).
\end{definition}

\subsection{Categories $\mathcal{O}^{+}$ and $\mathcal{O}^{-}$}

The category of all $\mathfrak{g}$-modules for a graded Lie algebra is
normally not particularly well-behaved: modules can be too big. One could
restrict one's attention to finite-dimensional modules, but this is often too
much of a sacrifice (e. g., the Heisenberg algebra $\mathcal{A}$ has no
finite-dimensional modules which are not direct sums of $1$-dimensional ones).
A balance between nontriviality and tamability is achieved by considering the
so-called \textit{Category }$\mathcal{O}$. Actually, there are two of these
categories, $\mathcal{O}^{+}$ and $\mathcal{O}^{-}$, which are antiequivalent
to each other (in general) and equivalent to each other (in some more
restrictive cases). There are several definitions for each of these
categories, and some of them are not even equivalent to each other, although
they mostly differ in minor technicalities. Here are the definitions that we
are going to use:

\begin{definition}
\label{def.O+}The objects of \textit{category }$\mathcal{O}^{+}$ will be
$\mathbb{C}$-graded $\mathfrak{g}$-modules $M$ such that:

\textbf{(1)} all degrees lie in a halfplane $\operatorname{Re}z<a$ and fall
into finitely many arithmetic progressions with step $1$;

\textbf{(2)} for every $d\in\mathbb{C}$, the space $M\left[  d\right]  $ is finite-dimensional.

The \textit{morphisms of category }$\mathcal{O}^{+}$ will be graded
$\mathfrak{g}$-module homomorphisms.
\end{definition}

\begin{definition}
\label{def.O-}The objects of \textit{category }$\mathcal{O}^{-}$ will be
$\mathbb{C}$-graded $\mathfrak{g}$-modules $M$ such that:

\textbf{(1)} all degrees lie in a halfplane $\operatorname{Re}z>a$ and fall
into finitely many arithmetic progressions with step $1$;

\textbf{(2)} for every $d\in\mathbb{C}$, the space $M\left[  d\right]  $ is finite-dimensional.

The \textit{morphisms of category }$\mathcal{O}^{-}$ will be graded
$\mathfrak{g}$-module homomorphisms.
\end{definition}

It is rather clear that for a nondegenerate $\mathbb{Z}$-graded Lie algebra
(or, more generally, for a $\mathbb{Z}$-graded Lie algebra satisfying
conditions \textbf{(1)} and \textbf{(2)} of Definition \ref{def.gradLienondeg}%
), the Verma highest-weight module $M_{\lambda}^{+}$ lies in category
$\mathcal{O}^{+}$ for every $\lambda\in\mathfrak{h}^{\ast}$, and the Verma
lowest-weight module $M_{\lambda}^{-}$ lies in category $\mathcal{O}^{-}$ for
every $\lambda\in\mathfrak{h}^{\ast}$.

\begin{definition}
Let $V$ and $W$ be two $\mathbb{C}$-graded vector spaces, and $x\in\mathbb{C}%
$. A map $f:V\rightarrow W$ is said to be \textit{homogeneous of degree }$x$
if and only if every $z\in\mathbb{C}$ satisfies $f\left(  V\left[  z\right]
\right)  \subseteq W\left[  z+x\right]  $. (For example, this yields that a
map is homogeneous of degree $0$ if and only if it is graded.)
\end{definition}

\begin{proposition}
\label{prop.O.irred}The irreducible modules in category $\mathcal{O}^{\pm}$
(up to homogeneous isomorphism) are $L_{\lambda}^{\pm}$ for varying
$\lambda\in\mathbb{C}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.O.irred}.} First of all, for every
$\lambda\in\mathfrak{h}^{\ast}$, the $\mathfrak{g}$-module $L_{\lambda}^{+}$
has a unique singular vector (up to scaling), and this vector is a singular
vector of weight $\lambda$.\ \ \ \ \footnote{\textit{Proof.} It is clear that
$\overline{v_{\lambda}^{+}}\in L_{\lambda}^{+}$ is a singular vector of weight
$\lambda$. Now we must prove that it is the only singular vector (up to
scaling).
\par
In fact, assume the opposite. Then, there exists a singular vector in
$L_{\lambda}^{+}$ which is not a scalar multiple of $\overline{v_{\lambda}%
^{+}}$. This singular vector must have a nonzero $d$-th homogeneous component
for some $d<0$ (because it is not a scalar multiple of $\overline{v_{\lambda
}^{+}}$), and this component itself must be a singular vector (since any
homogeneous component of a singular vector must itself be a singular vector).
So the module $L_{\lambda}^{+}$ has a nonzero homogeneous singular vector $w$
of degree $d$.
\par
Now, repeat the proof of the $\Longrightarrow$ part of Corollary
\ref{cor.singvec}, with $M_{\lambda}^{+}$ replaced by $L_{\lambda}^{+}$ (using
the fact that $L_{\lambda}^{+}$ is irreducible). As a consequence, it follows
that $L_{\lambda}^{+}$ does not have nonzero singular vectors in negative
degrees. This contradicts the fact that the module $L_{\lambda}^{+}$ has a
nonzero homogeneous singular vector $w$ of degree $d<0$. This contradiction
shows that our assumption was wrong, so that indeed, $\overline{v_{\lambda
}^{+}}$ is the only singular vector of $L_{\lambda}^{+}$ (up to scaling),
qed.} Thus, the $\mathfrak{g}$-modules $L_{\lambda}^{+}$ are pairwise
nonisomorphic for varying $\lambda$. Similarly, the $\mathfrak{g}$-modules
$L_{\lambda}^{-}$ are pairwise nonisomorphic for varying $\lambda$.

Let $Y$ be any irreducible module in category $\mathcal{O}^{+}$. We are now
going to prove that $Y\cong L_{\lambda}^{+}$ for some $\lambda\in
\mathfrak{h}^{\ast}$.

Let $d$ be a complex number such that $Y\left[  d\right]  \neq0$ and $Y\left[
d+j\right]  =0$ for all $j\geq1$. (Such a complex number exists due to
condition \textbf{(1)} in Definition \ref{def.O+}.) For every $v\in Y\left[
d\right]  $, we have $av=0$ for every $a\in\mathfrak{g}_{i}$ for every
$i>0$\ \ \ \ \footnote{\textit{Proof.} Let $i>0$ and $a\in\mathfrak{g}_{i}$.
Then, $i\geq1$. Now, $a\in\mathfrak{g}_{i}$ and $v\in Y\left[  d\right]  $
yield $av\in\mathfrak{g}_{i}\cdot Y\left[  d\right]  \subseteq Y\left[
d+i\right]  =0$ (since $Y\left[  d+j\right]  =0$ for all $j\geq1$), so that
$av=0$, qed.}.

By Conditions \textbf{(1)} and \textbf{(2)} of Definition
\ref{def.gradLienondeg}, the Lie algebra $\mathfrak{g}_{0}$ is abelian and
finite-dimensional. Hence, every nonzero $\mathfrak{g}_{0}$-module has a
one-dimensional submodule\footnote{\textit{Proof.} This is because of the
following fact:
\par
Every nonzero finite-dimensional module over an abelian finite-dimensional Lie
algebra has a one-dimensional submodule. (This is just a restatement of the
fact that a finite set of pairwise commuting matrices on a finite-dimensional
nonzero $\mathbb{C}$-vector space has a common nonzero eigenvector.)}. Thus,
the nonzero $\mathfrak{g}_{0}$-module $Y\left[  d\right]  $ has a
one-dimensional submodule. Let $w$ be the generator of this submodule. Then,
this submodule is $\left\langle w\right\rangle $.

For every $h\in\mathfrak{h}$, the vector $hw$ is a scalar multiple of $w$
(since $h\in\mathfrak{h}=\mathfrak{g}_{0}$, so that $hw$ lies in the
$\mathfrak{g}_{0}$-submodule of $Y\left[  d\right]  $ generated by $w$, but
this submodule is $\left\langle w\right\rangle $). Thus, we can write
$hw=\lambda_{h}w$ for some $\lambda_{h}\in\mathbb{C}$. This $\lambda_{h}$ is
uniquely determined (since $h\neq0$), so we can define a map $\lambda
:\mathfrak{h}\rightarrow\mathbb{C}$ such that $\lambda\left(  h\right)
=\lambda_{h}$ for every $h\in\mathfrak{h}$. This map $\lambda$ is easily seen
to be $\mathbb{C}$-linear, so that we have found a $\lambda\in\mathfrak{h}%
^{\ast}$ such that%
\[
hw=\lambda\left(  h\right)  w\ \ \ \ \ \ \ \ \ \ \text{for every }%
h\in\mathfrak{h}.
\]
Also,%
\[
aw=0\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\mathfrak{g}_{i}\text{ for every
}i>0
\]
(since $av=0$ for every $v\in Y\left[  d\right]  $ and every $a\in
\mathfrak{g}_{i}$ for every $i>0$). Thus, $w$ is a nonzero singular vector of
weight $\lambda$.

By Lemma \ref{lem.singvec}, we have an isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  M_{\lambda}^{+},Y\right)
&  \rightarrow\operatorname*{Sing}\nolimits_{\lambda}Y,\\
\phi &  \mapsto\phi\left(  v_{\lambda}^{+}\right)  .
\end{align*}
Let $\phi$ be the preimage of $w$ under this isomorphism. Then, $w=\phi\left(
v_{\lambda}^{+}\right)  $. Since $w\in Y\left[  d\right]  $, it is easy to see
that $\phi$ is a homogeneous homomorphism of degree $d$ (in fact, every
$n\in\mathbb{Z}$ satisfies $M_{\lambda}^{+}\left[  n\right]  =U\left(
\mathfrak{n}_{-}\right)  \left[  n\right]  \cdot v_{\lambda}^{+}$, so that%
\begin{align*}
\phi\left(  M_{\lambda}^{+}\left[  n\right]  \right)   &  =\phi\left(
U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \cdot v_{\lambda}%
^{+}\right)  =U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]
\cdot\underbrace{\phi\left(  v_{\lambda}^{+}\right)  }_{=w\in Y\left[
d\right]  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\phi\text{ is
}\mathfrak{g}\text{-linear}\right) \\
&  \subseteq U\left(  \mathfrak{n}_{-}\right)  \left[  n\right]  \cdot
Y\left[  d\right]  \subseteq Y\left[  n+d\right]
\end{align*}
). This homomorphism $\phi$ must be surjective, since $Y$ is irreducible.
Thus, we have a homogeneous isomorphism $M_{\lambda}^{+}\diagup\left(
\operatorname*{Ker}\phi\right)  \cong Y$. Also, $\operatorname*{Ker}\phi$ is a
proper graded submodule of $M_{\lambda}^{+}$, thus a submodule of $J_{\lambda
}^{+}$ (by Theorem \ref{thm.verma} \textbf{(ii)}). Hence, we have a projection
$M_{\lambda}^{+}\diagup\left(  \operatorname*{Ker}\phi\right)  \rightarrow
M_{\lambda}^{+}\diagup J_{\lambda}^{+}$. Since $M_{\lambda}^{+}\diagup\left(
\operatorname*{Ker}\phi\right)  \cong Y$ is irreducible, this projection must
either be an isomorphism or the zero map. It cannot be the zero map (since it
is a projection onto the nonzero module $M_{\lambda}^{+}\diagup J_{\lambda
}^{+}$), so it therefore is an isomorphism. Thus, $M_{\lambda}^{+}\diagup
J_{\lambda}^{+}\cong M_{\lambda}^{+}\diagup\left(  \operatorname*{Ker}%
\phi\right)  \cong Y$, so we have a homogeneous isomorphism $Y\cong
M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$.

We thus have showed that any irreducible module in category $\mathcal{O}^{+}$
is isomorphic to $L_{\lambda}^{+}$ for some $\lambda\in\mathfrak{h}^{\ast}$.
Similarly, the analogous assertion holds for $\mathcal{O}^{-}$. Proposition
\ref{prop.O.irred} is thus proven.

\begin{definition}
Let $M$ be a module in category $\mathcal{O}^{+}$. We define the
\textit{character} $\operatorname*{ch}M$ of $M$ as follows:

Write $M=\bigoplus\limits_{d}M\left[  d\right]  $. Then, define
$\operatorname*{ch}M$ by%
\[
\operatorname*{ch}M=\sum\limits_{d}q^{-d}\operatorname*{tr}\nolimits_{M\left[
d\right]  }\left(  e^{x}\right)  \ \ \ \ \ \ \ \ \ \ \text{as a power series
in }q
\]
for every $x\in\mathfrak{h}$. We also write $\left(  \operatorname*{ch}%
M\right)  \left(  q,x\right)  $ for this, so it becomes a formal power series
in both $q$ and $x$. (Note that this power series can contain noninteger
powers of $q$, but due to $M\in\mathcal{O}^{+}$, the exponents in these powers
are bounded from above in their real part, and fall into infinitely many
arithmetic progressions with step $1$.)
\end{definition}

\begin{proposition}
\label{prop.chVerma}Here is an example:%
\[
\left(  \operatorname*{ch}M_{\lambda}^{+}\right)  \left(  x\right)  =\dfrac
{1}{\prod\limits_{j>0}\det\nolimits_{\mathfrak{g}\left[  -j\right]  }\left(
1-q^{j}e^{\operatorname*{ad}\left(  x\right)  }\right)  }.
\]
(To prove this, use Molien's identity which states that, for every linear map
$A:V\rightarrow V$, we have%
\[
\sum\limits_{n\in\mathbb{N}}q^{n}\operatorname*{Tr}\nolimits_{S^{n}V}\left(
S^{n}A\right)  =\dfrac{1}{\det\left(  1-qA\right)  },
\]
where $S^{n}A$ denotes the $n$-th symmetric power of the operator $A$.)
\end{proposition}

Let us consider some examples:

\begin{example}
\label{exa.sl2}Let $\mathfrak{g}=\mathfrak{sl}_{2}$. We can write this Lie
algebra in terms of Chevalley generators and their relations (this is a
particular case of what we did in Proposition \ref{prop.grad.g}). The most
traditional way to do this is by setting $e=\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $, $f=\left(
\begin{array}
[c]{cc}%
0 & 0\\
1 & 0
\end{array}
\right)  $ and $h=\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & -1
\end{array}
\right)  $; then, $\mathfrak{g}$ is generated by $e$, $f$ and $h$ as a Lie
algebra, and these generators satisfy $\left[  h,e\right]  =2e$, $\left[
h,f\right]  =-2f$ and $\left[  e,f\right]  =h$. Also, $\left(  e,f,h\right)  $
is a basis of the vector space $\mathfrak{g}$. In accordance with Proposition
\ref{prop.grad.g}, we grade $\mathfrak{g}$ by setting $\deg e=1$, $\deg f=-1$
and $\deg h=0$. Then, $\mathfrak{n}_{+}=\left\langle e\right\rangle $,
$\mathfrak{n}_{-}=\left\langle f\right\rangle $ and $\mathfrak{h}=\left\langle
h\right\rangle $. Hence, linear maps $\lambda:\mathfrak{h}\rightarrow
\mathbb{C}$ are in 1-to-1 correspondence with complex numbers (namely, the
images $\lambda\left(  h\right)  $ of $h$ under these maps). Thus, we can
identify any linear map $\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ with the
image $\lambda\left(  h\right)  \in\mathbb{C}$.

Consider any $\lambda\in\mathfrak{h}^{\ast}$. Since $\mathfrak{n}%
_{-}=\left\langle f\right\rangle $, the universal enveloping algebra $U\left(
\mathfrak{n}_{-}\right)  $ is the polynomial algebra $\mathbb{C}\left[
f\right]  $, and Proposition \ref{prop.verma1} \textbf{(a)} yields
$M_{\lambda}^{+}=\underbrace{U\left(  \mathfrak{n}_{-}\right)  }%
_{=\mathbb{C}\left[  f\right]  }v_{\lambda}^{+}=\mathbb{C}\left[  f\right]
v_{\lambda}^{+}$. Similarly, $M_{-\lambda}^{-}=\mathbb{C}\left[  e\right]
v_{-\lambda}^{-}$. In order to compute the bilinear form $\left(  \cdot
,\cdot\right)  $ on $M_{\lambda}^{+}\times M_{-\lambda}^{-}$, it is thus
enough to compute $\left(  f^{n}v_{\lambda}^{+},e^{n}v_{-\lambda}^{-}\right)
$ for all $n\in\mathbb{N}$. (The values $\left(  f^{n}v_{\lambda}^{+}%
,e^{m}v_{-\lambda}^{-}\right)  $ for $n\neq m$ are zero since the form has
degree $0$.) In order to do this, we notice that $e^{n}f^{n}v_{\lambda}%
^{+}=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
v_{\lambda}^{+}$\ \ \ \ \footnote{\textit{Proof.} Here is a sketch of the
proof. (If you want to see it in details, read the proof of Lemma
\ref{lem.serre-gen.sl2} \textbf{(a)} below; this lemma yields the equality
$e^{n}f^{n}v_{\lambda}^{+}=n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  v_{\lambda}^{+}$ by substituting $x=v_{\lambda}^{+}$.)
\par
First show that $hf^{m}v_{\lambda}^{+}=\left(  \lambda-2m\right)
f^{m}v_{\lambda}^{+}$ for every $m\in\mathbb{N}$. (This follows easily by
induction over $m$, using $hf-fh=\left[  h,f\right]  =-2f$.)
\par
Next show that $ef^{n}v_{\lambda}^{+}=n\left(  \lambda-n+1\right)
f^{n-1}v_{\lambda}^{+}$ for every positive $n\in\mathbb{N}$. (This is again an
easy induction proof using the equalities $ef-fe=\left[  e,f\right]  =h$,
$hv_{\lambda}^{+}=\underbrace{\lambda\left(  h\right)  }_{=\lambda}v_{\lambda
}^{+}=\lambda v_{\lambda}^{+}$ and $ev_{\lambda}^{+}=0$, and using the
equality $hf^{m}v_{\lambda}^{+}=\left(  \lambda-2m\right)  f^{m}v_{\lambda
}^{+}$ applied to $m=n-1$.)
\par
Now show that $e^{n}f^{n}v_{\lambda}^{+}=n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  v_{\lambda}^{+}$ for every $n\in\mathbb{N}$.
(For this, again use induction.)} and thus%
\begin{align}
\left(  f^{n}v_{\lambda}^{+},e^{n}v_{-\lambda}^{-}\right)   &  =\left(
\underbrace{S\left(  e^{n}\right)  }_{=\left(  -1\right)  ^{n}e^{n}}%
f^{n}v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =\left(  \left(  -1\right)
^{n}\underbrace{e^{n}f^{n}v_{\lambda}^{+}}_{\substack{=n!\lambda\left(
\lambda-1\right)  ...\left(  \lambda-n+1\right)  v_{\lambda}^{+}}%
},v_{-\lambda}^{-}\right) \nonumber\\
&  =\left(  \left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)
...\left(  \lambda-n+1\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right)
\nonumber\\
&  =\left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  \underbrace{\left(  v_{\lambda}^{+},v_{-\lambda}%
^{-}\right)  }_{=1}\label{exa.sl2.bilinform}\\
&  =\left(  -1\right)  ^{n}n!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  .\nonumber
\end{align}
So $M_{\lambda}^{+}$ is irreducible if $\lambda\notin\mathbb{Z}_{+}$. If
$\lambda\in\mathbb{Z}_{+}$, then $J_{\lambda}^{+}=\left\langle f^{n}%
v_{\lambda}^{+}\ \mid\ n\geq\lambda+1\right\rangle =\mathbb{C}\left[
f\right]  \cdot\left(  f^{\lambda+1}v_{\lambda}^{+}\right)  $, and the
irreducible $\mathfrak{g}$-module $L_{\lambda}^{+}=\left\langle \overline
{v_{\lambda}^{+}},f\overline{v_{\lambda}^{+}},...,f^{\lambda}\overline
{v_{\lambda}^{+}}\right\rangle $ has dimension $\dim\lambda+1$%
.\ \ \ \ \footnote{If you know the representation theory of $\mathfrak{sl}%
_{2}$, you probably recognize this module $L_{\lambda}^{+}$ as the $\left(
\dim\lambda\right)  $-th symmetric power of the vector module $\mathbb{C}^{2}$
(as there is only one irreducible $\mathfrak{sl}_{2}$-module of every
dimension).}
\end{example}

\begin{example}
\label{exa.Vir}Let $\mathfrak{g}=\operatorname*{Vir}$. With the grading that
we have defined on $\operatorname*{Vir}$, we have $\mathfrak{h}=\mathfrak{g}%
_{0}=\left\langle L_{0},C\right\rangle $. Thus, linear maps $\lambda
:\mathfrak{h}\rightarrow\mathbb{C}$ can be uniquely described by the images of
$L_{0}$ and $C$ under these maps. We thus identify every linear map
$\lambda:\mathfrak{h}\rightarrow\mathbb{C}$ with the pair $\left(
\lambda\left(  L_{0}\right)  ,\lambda\left(  C\right)  \right)  $.

For every $\lambda=\left(  \lambda\left(  L_{0}\right)  ,\lambda\left(
C\right)  \right)  $, the number $\lambda\left(  L_{0}\right)  $ is denoted by
$h$ and called the \textit{conformal weight} of $\lambda$, and the number
$\lambda\left(  C\right)  $ is denoted by $c$ and called the \textit{central
charge} of $\lambda$. Thus, $\lambda$ is identified with the pair $\left(
h,c\right)  $. As a consequence, the Verma modules $M_{\lambda}^{+}$ and
$M_{\lambda}^{-}$ are often denoted by $M_{h,c}^{+}$ and $M_{h,c}^{-}$,
respectively, and the modules $L_{\lambda}^{+}$ and $L_{\lambda}^{-}$ are
often denoted by $L_{h,c}^{+}$ and $L_{h,c}^{-}$, respectively.

(Note, of course, that the central charge of $\lambda$ is the central charge
of each of the $\operatorname*{Vir}$-modules $M_{\lambda}^{+}$, $M_{\lambda
}^{-}$, $L_{\lambda}^{+}$ and $L_{\lambda}^{-}$.)

Consider any $\lambda\in\mathfrak{h}^{\ast}$. Let us compute the bilinear form
$\left(  \cdot,\cdot\right)  $ on $M_{\lambda}^{+}\times M_{-\lambda}^{-}$.
Note first that $L_{0}v_{\lambda}^{+}=\underbrace{\lambda\left(  L_{0}\right)
}_{=h}v_{\lambda}^{+}=hv_{\lambda}^{+}$ and $Cv_{\lambda}^{+}%
=\underbrace{\lambda\left(  C\right)  }_{=c}v_{\lambda}^{+}=cv_{\lambda}^{+}$.

In order to compute $\left(  L_{-1}v_{\lambda}^{+},L_{1}v_{-\lambda}%
^{-}\right)  $, we notice that
\[
\underbrace{L_{1}L_{-1}}_{=L_{-1}L_{1}+\left[  L_{1},L_{-1}\right]
}v_{\lambda}^{+}=L_{-1}\underbrace{L_{1}v_{\lambda}^{+}}_{=0}%
+\underbrace{\left[  L_{1},L_{-1}\right]  }_{=2L_{0}}v_{\lambda}%
^{+}=2\underbrace{L_{0}v_{\lambda}^{+}}_{=hv_{\lambda}^{+}}=2hv_{\lambda}%
^{+},
\]
so that%
\[
\left(  L_{-1}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)  =\left(
-\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}},v_{-\lambda}%
^{-}\right)  =\left(  -2hv_{\lambda}^{+},v_{-\lambda}^{-}\right)
=-2h\underbrace{\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  }_{=1}=-2h.
\]
Since $\left(  L_{-1}v_{\lambda}^{+}\right)  $ is a basis of $M_{\lambda}%
^{+}\left[  -1\right]  $ and $\left(  L_{1}v_{-\lambda}^{-}\right)  $ is a
basis of $M_{-\lambda}^{-}\left[  1\right]  $, this yields $\det\left(
\left(  \cdot,\cdot\right)  _{1}\right)  =2h$ (where $\left(  \cdot
,\cdot\right)  _{1}$ denotes the restriction of the form $\left(  \cdot
,\cdot\right)  $ to $M_{\lambda}^{+}\left[  -1\right]  \times M_{-\lambda}%
^{-}\left[  1\right]  $). This vanishes for $h=0$.

In degree $2$, the form is somewhat more complicated: With respect to the
basis $\left(  L_{-1}^{2}v_{\lambda}^{+},L_{-2}v_{\lambda}^{+}\right)  $ of
$M_{\lambda}^{+}\left[  -2\right]  $, and the basis $\left(  L_{1}%
^{2}v_{-\lambda}^{-},L_{2}v_{-\lambda}^{-}\right)  $ of $M_{-\lambda}%
^{-}\left[  2\right]  $, the restriction $\left(  \cdot,\cdot\right)  _{2}$ of
the form $\left(  \cdot,\cdot\right)  $ to $M_{\lambda}^{+}\left[  -2\right]
\times M_{-\lambda}^{-}\left[  2\right]  $ is given by the matrix%
\[
\left(
\begin{array}
[c]{cc}%
\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)  & \left(
L_{-1}^{2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right) \\
\left(  L_{-2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)  & \left(
L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)
\end{array}
\right)  .
\]


Let us compute, as an example, the lower right entry of this matrix, that is,
the entry $\left(  L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)  $. We
have%
\begin{align*}
\underbrace{L_{2}L_{-2}}_{=L_{-2}L_{2}+\left[  L_{2},L_{-2}\right]
}v_{\lambda}^{+}  &  =L_{-2}\underbrace{L_{2}v_{\lambda}^{+}}_{=0}%
+\underbrace{\left[  L_{2},L_{-2}\right]  }_{=4L_{0}+\dfrac{1}{2}C}v_{\lambda
}^{+}=\left(  4L_{0}+\dfrac{1}{2}C\right)  v_{\lambda}^{+}=4\underbrace{L_{0}%
v_{\lambda}^{+}}_{=hv_{\lambda}^{+}}+\dfrac{1}{2}\underbrace{Cv_{\lambda}^{+}%
}_{=cv_{\lambda}^{+}}\\
&  =4hv_{\lambda}^{+}+\dfrac{1}{2}cv_{\lambda}^{+}=\left(  4h+\dfrac{1}%
{2}c\right)  v_{\lambda}^{+},
\end{align*}
so that%
\begin{align*}
\left(  L_{-2}v_{\lambda}^{+},L_{2}v_{-\lambda}^{-}\right)   &  =\left(
-\underbrace{L_{2}L_{-2}v_{\lambda}^{+}}_{=\left(  4h+\dfrac{1}{2}c\right)
v_{\lambda}^{+}},v_{-\lambda}^{-}\right)  =\left(  -\left(  4h+\dfrac{1}%
{2}c\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =-\left(  4h+\dfrac{1}{2}c\right)  \underbrace{\left(  v_{\lambda}%
^{+},v_{-\lambda}^{-}\right)  }_{=1}=-\left(  4h+\dfrac{1}{2}c\right)  .
\end{align*}
As a further (more complicated) example, let us compute the upper left entry
of the matrix, namely $\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda
}^{-}\right)  $. We have%
\begin{align*}
L_{1}^{2}L_{-1}^{2}v_{\lambda}^{+}  &  =L_{1}\underbrace{L_{1}L_{-1}}%
_{=L_{-1}L_{1}+\left[  L_{1},L_{-1}\right]  }L_{-1}v_{\lambda}^{+}=L_{1}%
L_{-1}\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}%
+L_{1}\underbrace{\left[  L_{1},L_{-1}\right]  }_{=2L_{0}}L_{-1}v_{\lambda
}^{+}\\
&  =2h\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}%
+2L_{1}\underbrace{L_{0}L_{-1}}_{\substack{=L_{-1}L_{0}+\left[  L_{0}%
,L_{-1}\right]  \\=L_{-1}L_{0}+L_{-1}\\\text{(since }\left[  L_{0}%
,L_{-1}\right]  =L_{-1}\text{)}}}v_{\lambda}^{+}=4h^{2}v_{\lambda}^{+}%
+2L_{1}L_{-1}\underbrace{L_{0}v_{\lambda}^{+}}_{=hv_{\lambda}^{+}%
}+2\underbrace{L_{1}L_{-1}v_{\lambda}^{+}}_{=2hv_{\lambda}^{+}}\\
&  =4h^{2}v_{\lambda}^{+}+2h\underbrace{L_{1}L_{-1}v_{\lambda}^{+}%
}_{=2hv_{\lambda}^{+}}+4hv_{\lambda}^{+}=4h^{2}v_{\lambda}^{+}+4h^{2}%
v_{\lambda}^{+}+4hv_{\lambda}^{+}=\left(  8h^{2}+4h\right)  v_{\lambda}^{+}%
\end{align*}
and thus%
\begin{align*}
\left(  L_{-1}^{2}v_{\lambda}^{+},L_{1}^{2}v_{-\lambda}^{-}\right)   &
=\left(  -L_{1}L_{-1}^{2}v_{\lambda}^{+},L_{1}v_{-\lambda}^{-}\right)
=\left(  \underbrace{L_{1}^{2}L_{-1}^{2}v_{\lambda}^{+}}_{=\left(
8h^{2}+4h\right)  v_{\lambda}^{+}},v_{-\lambda}^{-}\right)  =\left(  \left(
8h^{2}+4h\right)  v_{\lambda}^{+},v_{-\lambda}^{-}\right) \\
&  =\left(  8h^{2}+4h\right)  \underbrace{\left(  v_{\lambda}^{+},v_{-\lambda
}^{-}\right)  }_{=1}=8h^{2}+4h.
\end{align*}


Similarly, we compute the other two entries of the matrix. The matrix thus
becomes%
\[
\left(
\begin{array}
[c]{cc}%
8h^{2}+4h & 6h\\
-6h & -\left(  4h+\dfrac{1}{2}c\right)
\end{array}
\right)  .
\]
The determinant of this matrix is%
\[
\det\left(  \left(  \cdot,\cdot\right)  _{2}\right)  =\left(  8h^{2}%
+4h\right)  \left(  -\left(  4h+\dfrac{1}{2}c\right)  \right)  -6h\left(
-6h\right)  =-4h\left(  \left(  2h+1\right)  \left(  4h+\dfrac{1}{2}c\right)
-9h\right)  .
\]
Notice the term $\left(  2h+1\right)  \left(  4h+\dfrac{1}{2}c\right)  -9h$:
The set of zeroes of this term is a hyperbola (an affine conic defined over
$\mathbb{R}$ which has a hyperbola as an $\mathbb{R}$-form). The determinant
of $\left(  \cdot,\cdot\right)  _{2}$ thus vanishes on the union of a line and
a hyperbola. For every point $\left(  h,c\right)  $ lying on this hyperbola,
the highest-weight module $M_{h,c}^{+}$ has a nonzero singular vector in
degree $-2$ (this means a nonzero singular vector of the form $\alpha
L_{-2}v_{\lambda}^{+}+\beta L_{-1}^{2}v_{\lambda}^{+}$ for some $\alpha
,\beta\in\mathbb{C}$).

We will later discuss $\det\left(  \left(  \cdot,\cdot\right)  _{n}\right)  $
for generic $n$. In fact, there is an explicit formula for this determinant,
namely the so-called Kac determinant formula.
\end{example}

\subsubsection{Restricted dual modules}

\begin{definition}
Let $V=\bigoplus\limits_{i\in I}V\left[  i\right]  $ be a graded vector space
(where $I$ might be $\mathbb{Z}$, $\mathbb{N}$, $\mathbb{C}$ or any other
set). The \textit{restricted dual} $V^{\vee}$ of $V$ is defined to be the
direct sum $\bigoplus\limits_{i\in I}V\left[  i\right]  ^{\ast}$. This is a
vector subspace of the dual $V^{\ast}$ of $V$, but (in general) not the same
as $V^{\ast}$ unless the direct sum is finite.

If $V\left[  i\right]  $ is finite-dimensional for every $i\in I$, then
$V^{\vee\vee}\cong V$ canonically.

If $\mathfrak{g}$ is a $\mathbb{Z}$-graded Lie algebra, and $V$ is a
$\mathbb{C}$-graded $\mathfrak{g}$-module, then $V^{\vee}$ canonically becomes
a $\mathbb{C}$-graded $\mathfrak{g}$-module. (The grading on $V^{\vee}$ is
such that $V^{\vee}\left[  -i\right]  =V\left[  i\right]  ^{\ast}$ for every
$i\in\mathbb{C}$.)
\end{definition}

It is clear that:

\begin{proposition}
We have two mutually inverse antiequivalences of categories $\mathcal{O}%
^{+}\overset{\vee}{\rightarrow}\mathcal{O}^{-}$ and $\mathcal{O}%
^{-}\overset{\vee}{\rightarrow}\mathcal{O}^{+}$, each defined by mapping every
$\mathfrak{g}$-module in one category to its restricted dual.
\end{proposition}

We can view the form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ as a linear map $M_{\lambda}%
^{+}\rightarrow\left(  M_{-\lambda}^{-}\right)  ^{\vee}$. The kernel of this
map is $J_{\lambda}^{+}$, and therefore, when $\mathfrak{g}$ is nondegenerate,
this map is an isomorphism for Weil-generic $\lambda$ (by Theorem
\ref{thm.invformnondeg}). In general, this map factors as $%
%TCIMACRO{\TeXButton{diag}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong} &
%\left(L^{-}_{-\lambda}\right)^{\vee} \arinj[r] & \left(M^{-}_{-\lambda}%
%\right)^{\vee}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong} &
\left(L^{-}_{-\lambda}\right)^{\vee} \arinj[r] & \left(M^{-}_{-\lambda}%
\right)^{\vee}
}%
%EndExpansion
$.

\subsubsection{\label{subsect.invol}Involutions}

In many applications, we are not just working with a graded Lie algebra
$\mathfrak{g}$. Very often we additionally have a degree-reversing involution:

\begin{definition}
\label{def.invol}Let $\mathfrak{g}$ be a graded Lie algebra. Let
$\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be an involutive automorphism of
the Lie algebra $\mathfrak{g}$ ("involutive" means $\omega^{2}%
=\operatorname*{id}$) such that $\omega\left(  \mathfrak{g}_{i}\right)
=\mathfrak{g}_{-i}$ for all $i\in\mathbb{Z}$ and such that $\omega
\mid_{\mathfrak{g}_{0}}=-\operatorname*{id}$. Then, for every graded
$\mathfrak{g}$-module $M$, we can define a graded $\mathfrak{g}$-module
$M^{c}$ as being the $\mathfrak{g}$-module $M^{\omega}$ with opposite grading
(i. e., the grading on $M^{c}$ is defined by $M^{c}\left[  i\right]
=M^{\omega}\left[  -i\right]  $ for every $i$). Then, we have an equivalence
of categories $\mathcal{O}^{+}\overset{\omega}{\rightarrow}\mathcal{O}^{-}$
which sends every $\mathfrak{g}$-module $M\in\mathcal{O}^{+}$ to the
$\mathfrak{g}$-module $M^{c}\in\mathcal{O}^{-}$, and the quasiinverse
equivalence of categories $\mathcal{O}^{-}\overset{\omega}{\rightarrow
}\mathcal{O}^{+}$ which does the same thing.

So the functor $\mathcal{O}^{+}\overset{\vee}{\rightarrow}\mathcal{O}%
^{-}\overset{\omega}{\rightarrow}\mathcal{O}^{+}$ is an antiequivalence,
called the \textit{functor of contragredient module}. This functor allows us
to identify $\left(  M_{-\lambda}^{-}\right)  ^{\omega}$ with $M_{\lambda}%
^{+}$ (via the isomorphism $M_{\lambda}^{+}\rightarrow\left(  M_{-\lambda}%
^{-}\right)  ^{\omega}$ which sends $x\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }v_{\lambda}^{+}$ to $\left(  U\left(
\omega\right)  \right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }v_{-\lambda}^{-}$ for every $x\in U\left(
\mathfrak{g}\right)  $), and thus to view the form $\left(  \cdot
,\cdot\right)  $ as a form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$. But this form is not $\mathfrak{g}%
$-invariant; it is contravariant; this means that any $a\in\mathfrak{g}$,
$v\in M_{\lambda}^{+}$ and $w\in M_{\lambda}^{+}$ satisfy $\left(
av,w\right)  =-\left(  v,\omega\left(  a\right)  w\right)  $ and $\left(
v,aw\right)  =-\left(  \omega\left(  a\right)  v,w\right)  $.

This form can be viewed as a linear map $M_{\lambda}^{+}\rightarrow\left(
M_{\lambda}^{+}\right)  ^{c}$, which factors into $%
%TCIMACRO{\TeXButton{diag}{\xymatrix{
%M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong}
%& \left(L^{+}_{\lambda}\right)^{c} \arinj[r] & \left(M^{+}_{\lambda}%
%\right)^{c}
%}}}%
%BeginExpansion
\xymatrix{
M^{+}_{\lambda} \arsurj[r] & L^{+}_{\lambda} \ar[r]^-{\cong}
& \left(L^{+}_{\lambda}\right)^{c} \arinj[r] & \left(M^{+}_{\lambda}%
\right)^{c}
}%
%EndExpansion
$.

Notice that this form $\left(  \cdot,\cdot\right)  $ is a contravariant form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$. Of course, this yields
that the transpose of $\left(  \cdot,\cdot\right)  $ is also such a form.
Since there exists a \textbf{unique} contravariant form $M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{\lambda}^{+}\right)  =1$ (because contravariant forms $M_{\lambda}%
^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ are in 1-to-1 correspondence
with $\mathfrak{g}$-invariant bilinear forms $M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$, and for the latter we have Proposition
\ref{prop.invform} \textbf{(a)}), this yields that the form $\left(
\cdot,\cdot\right)  $ and its transpose must be identical. In other words, the
form $\left(  \cdot,\cdot\right)  $ is symmetric.
\end{definition}

Involutive automorphisms of $\mathfrak{g}$ satisfying the conditions of
Definition \ref{def.invol} are not uncommon; here are four examples:

\begin{proposition}
\label{prop.invol.A}The $\mathbb{C}$-linear map $\omega:\mathcal{A}%
\rightarrow\mathcal{A}$ defined by $\omega\left(  K\right)  =-K$ and
$\omega\left(  a_{i}\right)  =-a_{-i}$ for every $i\in\mathbb{Z}$ is an
involutive automorphism of the Lie algebra $\mathcal{A}$. This automorphism
$\omega$ satisfies the conditions of Definition \ref{def.invol} (for
$\mathfrak{g}=\mathcal{A}$). We already know this from Proposition
\ref{prop.A.omega}. Moreover, if we let $\lambda=\left(  1,\mu\right)  $ for a
complex number $\mu$, then $M_{\lambda}^{+}\cong F_{\mu}$ (by Proposition
\ref{prop.fockverma.A}), and thus we can regard the contravariant form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ from Definition
\ref{def.invol} as a contravariant form $F_{\mu}\times F_{\mu}\rightarrow
\mathbb{C}$. This contravariant form $F_{\mu}\times F_{\mu}\rightarrow
\mathbb{C}$ is exactly the form $\left(  \cdot,\cdot\right)  $ of Proposition
\ref{prop.A.contravariantform}. (This is because the form $\left(  \cdot
,\cdot\right)  $ of Proposition \ref{prop.A.contravariantform} is
contravariant (due to Proposition \ref{prop.A.contravariantform} \textbf{(c)}
and \textbf{(d)}) and satisfies $\left(  1,1\right)  =1$.)
\end{proposition}

\begin{proposition}
The $\mathbb{C}$-linear map $\omega:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ defined by $\omega\left(  C\right)  =-C$ and
$\omega\left(  L_{i}\right)  =-L_{-i}$ for every $i\in\mathbb{Z}$ is an
involutive automorphism of the Lie algebra $\operatorname*{Vir}$. This
automorphism $\omega$ satisfies the conditions of Definition \ref{def.invol}
(for $\mathfrak{g}=\operatorname*{Vir}$).
\end{proposition}

\begin{proposition}
\label{prop.simple.omega}Let $\mathfrak{g}$ be a simple Lie algebra, graded
and presented as in Proposition \ref{prop.grad.g}. Then, there exists a unique
Lie algebra homomorphism $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$
satisfying $\omega\left(  e_{i}\right)  =-f_{i}$, $\omega\left(  h_{i}\right)
=-h_{i}$ and $\omega\left(  f_{i}\right)  =-e_{i}$ for every $i\in\left\{
1,2,...,m\right\}  $. This automorphism $\omega$ satisfies the conditions of
Definition \ref{def.invol}.
\end{proposition}

\begin{proposition}
Let $\mathfrak{g}$ be a simple finite-dimensional Lie algebra, graded and
presented as in Proposition \ref{prop.grad.g}. Let $\widehat{\mathfrak{g}}$ be
the Kac-Moody Lie algebra defined in Definition \ref{def.kac}. Let $K$ denote
the element $\left(  0,1\right)  $ of $\mathfrak{g}\left[  t,t^{-1}\right]
\otimes\mathbb{C}=\widehat{\mathfrak{g}}$.

Let $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be defined as in Proposition
\ref{prop.simple.omega}. Then, the $\mathbb{C}$-linear map $\widehat{\omega
}:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$ defined by
$\widehat{\omega}\left(  a\cdot t^{j}\right)  =\omega\left(  a\right)  t^{-j}$
for every $a\in\mathfrak{g}$ and $j\in\mathbb{N}$, and $\widehat{\omega
}\left(  K\right)  =-K$, is an involutive automorphism of the Lie algebra
$\widehat{\mathfrak{g}}$. This automorphism $\widehat{\omega}$ satisfies the
conditions of Definition \ref{def.invol} (for $\widehat{\mathfrak{g}}$ and
$\widehat{\omega}$ instead of $\mathfrak{g}$ and $\omega$).
\end{proposition}

More generally:

\begin{proposition}
Let $\mathfrak{g}$ be a graded Lie algebra with a symmetric bilinear form
$\left(  \cdot,\cdot\right)  $ of degree $0$ invariant under the Lie bracket.
Let $\widehat{\mathfrak{g}}$ be the Lie algebra defined in Definition
\ref{def.loop}. Let $K$ denote the element $\left(  0,1\right)  $ of
$\mathfrak{g}\left[  t,t^{-1}\right]  \otimes\mathbb{C}=\widehat{\mathfrak{g}%
}$. Give $\widehat{\mathfrak{g}}$ a grading which makes $K$ homogeneous of
degree $0$ and the multiplications by $t$ and $t^{-1}$ into graded maps.

Let $\omega:\mathfrak{g}\rightarrow\mathfrak{g}$ be an involutive automorphism
satisfying the conditions of Definition \ref{def.invol} (not to be confused
with the $2$-cocycle $\omega$ of Definition \ref{def.loop}). Then, the
$\mathbb{C}$-linear map $\widehat{\omega}:\widehat{\mathfrak{g}}%
\rightarrow\widehat{\mathfrak{g}}$ defined by $\widehat{\omega}\left(  a\cdot
t^{j}\right)  =\omega\left(  a\right)  t^{-j}$ for every $a\in\mathfrak{g}$
and $j\in\mathbb{N}$, and $\widehat{\omega}\left(  K\right)  =-K$, is an
involutive automorphism of the Lie algebra $\widehat{\mathfrak{g}}$. This
automorphism $\widehat{\omega}$ satisfies the conditions of Definition
\ref{def.invol} (for $\widehat{\mathfrak{g}}$ and $\widehat{\omega}$ instead
of $\mathfrak{g}$ and $\omega$).
\end{proposition}

\subsubsection{Unitary structures}

\begin{impnot}
\textbf{The parts of these notes concerned with unitary/Hermitian/real
structures are in an unfinished state and contain mistakes which I don't know
how to fix.}

For instance, if we define $\mathfrak{g}_{\mathbb{R}}$ by $\mathfrak{g}%
_{\mathbb{R}}=\left\{  a\in\mathfrak{g}\ \mid\ a^{\dag}=-a\right\}  $, and
define $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ by $\mathfrak{g}_{0\mathbb{R}%
}^{\ast}=\left\{  f\in\mathfrak{g}_{0}^{\ast}\ \mid\ f\left(  \mathfrak{g}%
_{0\mathbb{R}}\right)  \subseteq\mathbb{R}\right\}  $ (as I do below), and
define the antiinvolution $\dag:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ on $\operatorname*{Vir}$ by $L_{i}^{\dag}=L_{-i}$ for all
$i\in\mathbb{Z}$, and $C^{\dag}=C$, then $\operatorname*{Vir}%
\nolimits_{0\mathbb{R}}^{\ast}$ is \textbf{not} the set of all weights
$\left(  h,c\right)  $ satisfying $h,c\in\mathbb{R}$, but it is the set of all
weights $\left(  h,c\right)  $ satisfying $ih,ic\in\mathbb{R}$ (because the
definition of $\dag$ that we gave leads to $\operatorname*{Vir}%
\nolimits_{0\mathbb{R}}=\left\langle iC,iL_{0}\right\rangle _{\mathbb{R}}$).
This is not what we want later. Probably it is possible to fix these issues by
correcting some signs, but I do not know how. If you know a consistent way to
correct these definitions and results, please drop me a mail
(AB\texttt{@gmail.com} where A=\texttt{darij} and B=\texttt{grinberg}).
\end{impnot}

Over $\mathbb{C}$, it makes sense to study not only linear but also antilinear
maps. Sometimes, the latter actually enjoy even better properties of the
former (e. g., Hermitian forms are better behaved than complex-symmetric forms).

\begin{definition}
If $\mathfrak{g}$ and $\mathfrak{h}$ are two Lie algebras over a field $k$,
then a $k$-\textit{antihomomorphism} from $\mathfrak{g}$ to $\mathfrak{h}$
means a $k$-linear map $f:\mathfrak{g}\rightarrow\mathfrak{h}$ such that
$f\left(  \left[  x,y\right]  \right)  =-\left[  f\left(  x\right)  ,f\left(
y\right)  \right]  $ for all $x,y\in\mathfrak{g}$.
\end{definition}

\begin{definition}
In the following, an \textit{antiinvolution} of a complex Lie algebra
$\mathfrak{g}$ means an $\mathbb{R}$-antihomomorphism from $\mathfrak{g}$ to
$\mathfrak{g}$ which is simultaneously an involution.
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra. Let $\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ be an antilinear antiinvolution. This means that
$\dag$ is an $\mathbb{R}$-linear map and satisfies the relations%
\begin{align*}
\dag^{2}  &  =\operatorname*{id};\\
\left(  za\right)  ^{\dag}  &  =\overline{z}a^{\dag}%
\ \ \ \ \ \ \ \ \ \ \text{for all }z\in\mathbb{C}\text{ and }a\in
\mathfrak{g};\\
\left[  a,b\right]  ^{\dag}  &  =-\left[  a^{\dag},b^{\dag}\right]
\ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in\mathfrak{g}.
\end{align*}
(Here and in the following, we write $c^{\dag}$ for the image of an element
$c\in\mathfrak{g}$ under $\dag$.) Such a map $\dag$ is called a \textit{real
structure}, for the following reason: If $\dag$ is such a map, then we can
define a subspace $\mathfrak{g}_{\mathbb{R}}=\left\{  a\in\mathfrak{g}%
\ \mid\ a^{\dag}=-a\right\}  $ of $\mathfrak{g}$, and this $\mathfrak{g}%
_{\mathbb{R}}$ is a real Lie algebra such that $\mathfrak{g}\cong%
\mathfrak{g}_{\mathbb{R}}\otimes_{\mathbb{R}}\mathbb{C}$ as complex Lie
algebras. (It is said that $\mathfrak{g}_{\mathbb{R}}$ is a \textit{real form}
of $\mathfrak{g}$.)
\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. If
$V$ is a $\mathfrak{g}$-module, we say that $V$ is \textit{Hermitian} if $V$
is equipped with a nondegenerate Hermitian form $\left(  \cdot,\cdot\right)  $
satisfying%
\[
\left(  av,w\right)  =\left(  v,a^{\dag}w\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }v\in V\text{ and
}w\in V.
\]
The $\mathfrak{g}$-module $V$ is said to be \textit{unitary} if this form is
positive definite.
\end{definition}

The real Lie algebra $\mathfrak{g}_{\mathbb{R}}$ acts on a Hermitian module by
skew-Hermitian operators.

\begin{remark}
While we will not be studying Lie groups in this course, here are some facts
about them that explain why unitary $\mathfrak{g}$-modules are called "unitary":

If $\mathfrak{g}$ is a finite-dimensional Lie algebra, and $V$ is a unitary
$\mathfrak{g}$-module, then the Hilbert space completion of $V$ is a unitary
representation of the Lie group $G_{\mathbb{R}}=\exp\left(  \mathfrak{g}%
_{\mathbb{R}}\right)  $ corresponding to $\mathfrak{g}_{\mathbb{R}}$ by Lie's
Third Theorem. (Note that this Hilbert space completion of $V$ is $V$ itself
if $\dim V<\infty$.) This even holds for some infinite-dimensional
$\mathfrak{g}$ under sufficiently restrictive conditions.
\end{remark}

\begin{Convention}
In the following, when we will talk about real structures on graded Lie
algebras, we will always consider the situation when $\mathfrak{g}$ is a
graded Lie algebra, and the map $\dag$ reverses the degree (i. e., every
$j\in\mathbb{Z}$ satisfies $\dag\left(  \mathfrak{g}_{j}\right)
\subseteq\mathfrak{g}_{-j}$). In particular, $\dag\left(  \mathfrak{g}%
_{0}\right)  \subseteq\mathfrak{g}_{0}$. We also assume that $\mathfrak{g}%
_{0}$ is an abelian Lie algebra (but we don't need to require $\mathfrak{g}$
to be nondegenerate).
\end{Convention}

So let us consider this situation. Two definitions:

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. Let
$V$ be a $\mathfrak{g}$-module. A Hermitian form $\left(  \cdot,\cdot\right)
$ on $V$ is said to be $\dag$\textit{-invariant} if and only if%
\[
\left(  av,w\right)  =\left(  v,a^{\dag}w\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}\text{, }v\in V\text{ and
}w\in V.
\]

\end{definition}

\begin{definition}
Let $\mathfrak{g}$ be a complex Lie algebra with a real structure $\dag$. For
every $f\in\mathfrak{g}_{0}^{\ast}$, we denote by $f^{\dag}$ the map
$\mathfrak{g}_{0}\rightarrow\mathbb{C},$ $x\mapsto\overline{f\left(  x^{\dag
}\right)  }$ (this map $f^{\dag}$ is easily seen to be $\mathbb{C}$-linear).
Let $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ be the subset $\left\{  f\in
\mathfrak{g}_{0}^{\ast}\ \mid\ f^{\dag}=-f\right\}  $ of $\mathfrak{g}%
_{0}^{\ast}$. Then, it is easily seen that%
\[
\mathfrak{g}_{0\mathbb{R}}^{\ast}=\left\{  f\in\mathfrak{g}_{0}^{\ast}%
\ \mid\ f\left(  \mathfrak{g}_{0\mathbb{R}}\right)  \subseteq\mathbb{R}%
\right\}  .
\]
Hence, we get an $\mathbb{R}$-bilinear form $\mathfrak{g}_{0\mathbb{R}}^{\ast
}\times\mathfrak{g}_{0\mathbb{R}}\rightarrow\mathbb{R},$ $\left(  f,a\right)
\mapsto f\left(  a\right)  $, which enables us to identify $\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ with the dual space of the $\mathbb{R}$-vector space
$\mathfrak{g}_{0\mathbb{R}}$. (More precisely, we have an isomorphism from
$\mathfrak{g}_{0\mathbb{R}}^{\ast}$ to the dual space of the $\mathbb{R}%
$-vector space $\mathfrak{g}_{0\mathbb{R}}$. This isomorphism sends every
$f\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$ to the map $f\mid_{\mathfrak{g}%
_{0\mathbb{R}}}$ (with target restricted to $\mathbb{R}$), and conversely, the
preimage of any $\mathbb{R}$-linear map $F:\mathfrak{g}_{0\mathbb{R}%
}\rightarrow\mathbb{R}$ is the $\mathbb{C}$-linear map $f\in\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ given by%
\[
f\left(  a\right)  =F\left(  \dfrac{a-a^{\dag}}{2}\right)  +iF\left(
\dfrac{a+a^{\dag}}{2i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathfrak{g}_{0}.
\]
)

The elements of $\mathfrak{g}_{0\mathbb{R}}^{\ast}$ are said to be
\textit{real}.
\end{definition}

\begin{proposition}
\label{prop.M+l.unitary}If $\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$, then
the $\mathfrak{g}$-module $M_{\lambda}^{+}$ carries a $\dag$-invariant
Hermitian form satisfying $\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.M+l.unitary}.} In the following,
whenever $U$ is a $\mathbb{C}$-vector space, we will denote by $\overline{U}$
the $\mathbb{C}$-vector space which is identical to $U$ as a set, but with the
$\mathbb{C}$-vector structure twisted by complex conjugation.

The antilinear $\mathbb{R}$-Lie algebra homomorphism $-\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ can be viewed as a $\mathbb{C}$-Lie algebra
homomorphism $-\dag:\mathfrak{g}\rightarrow\overline{\mathfrak{g}}$, and thus
induces a $\mathbb{C}$-algebra homomorphism $U\left(  -\dag\right)  :U\left(
\mathfrak{g}\right)  \rightarrow U\left(  \overline{\mathfrak{g}}\right)  $.
Since $U\left(  \overline{\mathfrak{g}}\right)  \cong\overline{U\left(
\mathfrak{g}\right)  }$ canonically as $\mathbb{C}$-algebras (because taking
the universal enveloping algebra commutes with base change)\footnote{Warning:
This isomorphism $U\left(  \overline{\mathfrak{g}}\right)  \rightarrow
\overline{U\left(  \mathfrak{g}\right)  }$ sends $i\cdot1_{U\left(
\overline{\mathfrak{g}}\right)  }$ to $-i\cdot1_{U\left(  \mathfrak{g}\right)
}$.}, we can thus consider this $U\left(  -\dag\right)  $ as a $\mathbb{C}%
$-algebra homomorphism $U\left(  \mathfrak{g}\right)  \rightarrow
\overline{U\left(  \mathfrak{g}\right)  }$. This, in turn, can be viewed as an
antilinear $\mathbb{R}$-algebra homomorphism $U\left(  -\dag\right)  :U\left(
\mathfrak{g}\right)  \rightarrow U\left(  \mathfrak{g}\right)  $.

Let $\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$. Let $\left(  M_{-\lambda
}^{-}\right)  ^{-\dag}$ be the $\mathfrak{g}$-module $M_{-\lambda}^{-}$
twisted by the isomorphism $-\dag:\mathfrak{g}\rightarrow\mathfrak{g}$ of
$\mathbb{R}$-Lie algebras. Then, $\left(  M_{-\lambda}^{-}\right)  ^{-\dag}$
is a module over the $\mathbb{R}$-Lie algebra $\mathfrak{g}$, but not a module
over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$, since it satisfies $\left(
za\right)  \rightharpoonup v=\overline{z}\left(  a\rightharpoonup v\right)  $
(rather than $\left(  za\right)  \rightharpoonup v=z\left(  a\rightharpoonup
v\right)  $) for all $z\in\mathbb{C}$, $a\in\mathfrak{g}$ and $v\in
M_{-\lambda}^{-}$ (where $\rightharpoonup$ denotes the action of
$\mathfrak{g}$). However, this can be easily transformed into a $\mathbb{C}%
$-Lie algebra action: Namely, $\overline{\left(  M_{-\lambda}^{-}\right)
^{-\dag}}$ is a module over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$.

We have an isomorphism%
\begin{align*}
\overline{\left(  M_{-\lambda}^{-}\right)  ^{-\dag}}  &  \rightarrow
M_{\lambda}^{+},\\
x\otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda
}^{-}  &  \mapsto U\left(  -\dag\right)  \left(  x\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}%
\end{align*}
of modules over the $\mathbb{C}$-Lie algebra $\mathfrak{g}$%
.\ \ \ \ \footnote{Here are some details on the definition of this
isomorphism:
\par
As $\mathbb{R}$-vector spaces, $\overline{\left(  M_{-\lambda}^{-}\right)
^{-\dag}}=M_{-\lambda}^{-}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }\mathbb{C}_{-\lambda}$ and
$M_{\lambda}^{+}=U\left(  \mathfrak{g}\right)  \otimes_{U\left(
\mathfrak{h}\oplus\mathfrak{n}_{-}\right)  }\mathbb{C}_{\lambda}$. Hence, we
can define an $\mathbb{R}$-linear map $\overline{\left(  M_{-\lambda}%
^{-}\right)  ^{-\dag}}\rightarrow M_{\lambda}^{+}$ that sends $x\otimes
_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda}^{-}$ to
$U\left(  -\dag\right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}$ for every $x\in
U\left(  \mathfrak{g}\right)  $ and $z\in\mathbb{C}$ if we are able to show
that
\[
U\left(  -\dag\right)  \left(  xw\right)  \otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{-}\right)  }\overline{z}v_{\lambda}^{+}=U\left(
-\dag\right)  \left(  x\right)  \otimes_{U\left(  \mathfrak{h}\oplus
\mathfrak{n}_{-}\right)  }\overline{wz}v_{\lambda}^{+}%
\ \ \ \ \ \ \ \ \ \ \text{for all }x\in U\left(  \mathfrak{g}\right)  \text{,
}w\in U\left(  \mathfrak{h}\oplus\mathfrak{n}_{+}\right)  \text{ and }%
z\in\mathbb{C}.
\]
But showing this is rather easy (left to the reader), and thus we get an
$\mathbb{R}$-linear map $\overline{\left(  M_{-\lambda}^{-}\right)  ^{-\dag}%
}\rightarrow M_{\lambda}^{+}$ that sends $x\otimes_{U\left(  \mathfrak{h}%
\oplus\mathfrak{n}_{+}\right)  }zv_{-\lambda}^{-}$ to $U\left(  -\dag\right)
\left(  x\right)  \otimes_{U\left(  \mathfrak{h}\oplus\mathfrak{n}_{-}\right)
}\overline{z}v_{\lambda}^{+}$ for every $x\in U\left(  \mathfrak{g}\right)  $
and $z\in\mathbb{C}$. This map is easily seen to be $\mathfrak{g}$-linear and
$\mathbb{C}$-linear, so it is a homomorphism of modules over $\mathbb{C}$-Lie
algebra $\mathfrak{g}$. Showing that it is an isomorphism is easy as well (one
just has to construct its inverse).} Hence, $M_{-\lambda}^{-}\cong%
\overline{\left(  M_{\lambda}^{+}\right)  ^{-\dag}}$.

Hence, our bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}%
\rightarrow\mathbb{C}$ can be viewed as a bilinear form $M_{\lambda}^{+}%
\times\overline{M_{\lambda}^{+}}\rightarrow\mathbb{C}$, id est, as a
sesquilinear form $M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}%
$. This sesquilinear form is the unique sesquilinear Hermitian form
$M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying
$\left(  v_{\lambda}^{+},v_{\lambda}^{+}\right)  =1$\ \ \ \ \footnote{This can
be easily derived from Proposition \ref{prop.invform} \textbf{(a)}, which
claims that our form $\left(  \cdot,\cdot\right)  :M_{\lambda}^{+}\times
M_{-\lambda}^{-}\rightarrow\mathbb{C}$ is the unique $\mathfrak{g}$-invariant
bilinear form $M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$
satisfying $\left(  v_{\lambda}^{+},v_{-\lambda}^{-}\right)  =1$.}. As a
consequence, this sesquilinear form can be easily seen to be Hermitian
symmetric, i. e., to satisfy%
\[
\left(  v,w\right)  =\overline{\left(  w,v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for all }v\in M_{\lambda}^{+}\text{ and }w\in
M_{\lambda}^{+}.
\]
\footnote{In fact, the form which sends $v\times w$ to $\overline{\left(
w,v\right)  }$ is also a sesquilinear Hermitian form $M_{\lambda}^{+}\times
M_{\lambda}^{+}\rightarrow\mathbb{C}$ satisfying $\left(  v_{\lambda}%
^{+},v_{\lambda}^{+}\right)  =1$, so that by uniqueness, it must be identical
with the form which sends $v\times w$ to $\left(  v,w\right)  $.}

However, this form can be degenerate. Its kernel is $J_{\lambda}^{+}$, so it
descends to a nondegenerate Hermitian form on $L_{\lambda}^{+}$. Thus, we get:

\begin{proposition}
\label{prop.hermitian.lambdareal}If $\lambda$ is real (this means that
$\lambda\in\mathfrak{g}_{0\mathbb{R}}^{\ast}$), then $L_{\lambda}^{+}$ carries
a $\dag$-invariant nondegenerate Hermitian form. Different degrees in
$L_{\lambda}^{+}$ are orthogonal with respect to this form.
\end{proposition}

A reasonable (and, in most cases, difficult and interesting) question to ask
is the following: For which $\lambda$ is $L_{\lambda}^{+}$ unitary?

We are going to address this question in some cases and give hints in some
others, leaving many more unanswered.

First, let us give several examples of complex Lie algebras $\mathfrak{g}$
with antilinear antiinvolutions $\dag:\mathfrak{g}\rightarrow\mathfrak{g}$:

\begin{proposition}
We can define an antilinear map $\dag:\mathcal{A}\rightarrow\mathcal{A}$ by
$K^{\dag}=K$ and\ $a_{i}^{\dag}=a_{-i}$ for all $i\in\mathbb{Z}$. This map is
an antilinear antiinvolution of the Heisenberg algebra $\mathcal{A}$.
\end{proposition}

\begin{proposition}
One can define an antilinear map $\dag:\mathfrak{sl}_{2}\rightarrow
\mathfrak{sl}_{2}$ by$\ e^{\dag}=f,\ f^{\dag}=e,\ h^{\dag}=h$. This map is an
antilinear antiinvolution of the Lie algebra $\mathfrak{sl}_{2}$.
\end{proposition}

More generally:

\begin{proposition}
Let $\mathfrak{g}$ be a simple finite-dimensional Lie algebra. Using the
Chevalley generators $e_{1}$, $e_{2}$, $...$, $e_{m}$, $f_{1}$, $f_{2}$,
$...$, $f_{m}$, $h_{1}$, $h_{2}$, $...$, $h_{m}$ of Proposition
\ref{prop.grad.g}, we can define an antilinear map $\dag:\mathfrak{g}%
\rightarrow\mathfrak{g}$ by $e_{i}^{\dag}=f_{i},$ $f_{i}^{\dag}=e_{i}%
,\ h_{i}^{\dag}=h_{i}$ for all $i\in\left\{  1,2,...,m\right\}  $. This map is
an antilinear antiinvolution of the Lie algebra $\mathfrak{g}$.
\end{proposition}

\begin{proposition}
We can define an antilinear map $\dag:\operatorname*{Vir}\rightarrow
\operatorname*{Vir}$ by $L_{i}^{\dag}=L_{-i}$ for all $i\in\mathbb{Z}$, and
$C^{\dag}=C$. This map is an antilinear antiinvolution of the Virasoro algebra
$\operatorname*{Vir}$.
\end{proposition}

\begin{proposition}
If $\mathfrak{g}$ is a Lie algebra with an antilinear antiinvolution
$\dag:\mathfrak{g}\rightarrow\mathfrak{g}$ and with a symmetric bilinear form
$\left(  \cdot,\cdot\right)  $ of degree $0$ invariant under the Lie bracket,
then we can define an antilinear map $\dag:\widehat{\mathfrak{g}}%
\rightarrow\widehat{\mathfrak{g}}$ (where $\widehat{\mathfrak{g}}$ is the Lie
algebra defined in Definition \ref{def.loop}) by $\left(  at^{n}\right)
^{\dag}=a^{\dag}\cdot t^{-n}$ for every $a\in\mathfrak{g}$ and $t\in
\mathbb{Z}$, and by $K^{\dag}=K$ (where $K$ denotes the element $\left(
0,1\right)  $ of $\mathfrak{g}\left[  t,t^{-1}\right]  \otimes\mathbb{C}%
=\widehat{\mathfrak{g}}$). This map $\dag$ is an antilinear involution of the
Lie algebra $\widehat{\mathfrak{g}}$.
\end{proposition}

As for examples of Hermitian modules: The $\operatorname*{Vir}$-module
$L_{h,c}^{+}$ (see Example \ref{exa.Vir} for the definition of this module)
for $h,c\in\mathbb{R}$ has a $\dag$-invariant nondegenerate Hermitian form.
(This is because the requirement $h,c\in\mathbb{R}$ forces the form
$\lambda\in\mathfrak{g}_{0}^{\ast}$ which corresponds to the pair $\left(
h,c\right)  $ to lie in $\mathfrak{g}_{0\mathbb{R}}^{\ast}$, and thus we can
apply Proposition \ref{prop.hermitian.lambdareal}.)

But now, back to the general case:

\begin{proposition}
\label{prop.unitrick}Let $V$ be a unitary representation in Category
$\mathcal{O}^{+}$. Then, $V$ is completely reducible (i. e., the
representation $V$ is a direct sum of irreducible representations).
\end{proposition}

To prove this, we will use a lemma:

\begin{lemma}
\label{lem.unitrick}If $V$ is a highest-weight representation, and $V$ has a
nondegenerate $\dag$-invariant Hermitian form, then $V$ is irreducible. (We
recall that a "highest-weight representation" means a quotient of $M_{\lambda
}^{+}$ by a proper graded submodule for some $\lambda$.)
\end{lemma}

\textit{Proof of Lemma \ref{lem.unitrick}.} Let $V$ be a highest-weight
representation having a nondegenerate $\dag$-invariant Hermitian form. Since
$V$ is a highest-weight representation, $V$ is a quotient of $M_{\lambda}^{+}$
by a proper graded submodule $P$ for some $\lambda$. The nondegenerate $\dag
$-invariant Hermitian form on $V$ thus induces a $\dag$-invariant Hermitian
form on $M_{\lambda}^{+}$ whose kernel is $P$. It is easy to see that
$\lambda$ is real. Thus, this $\dag$-invariant Hermitian form on $M_{\lambda
}^{+}$ can be rewritten as a $\mathfrak{g}$-invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$, which still has
kernel $P$. Such a form is unique up to scaling (by Proposition
\ref{prop.invform} \textbf{(c)}), and thus must be the form defined in
Proposition \ref{prop.invform} \textbf{(a)}. But the kernel of this form is
$J_{\lambda}^{+}$. Thus, the kernel of this form is, at the same time, $P$ and
$J_{\lambda}^{+}$. Hence, $P=J_{\lambda}^{+}$, so that $V=L_{\lambda}^{+}$
(since $V$ is the quotient of $M_{\lambda}^{+}$ by $P$), and thus $V$ is
irreducible. Lemma \ref{lem.unitrick} is proven.

\textit{Proof of Proposition \ref{prop.unitrick}.} Take a nonzero homogeneous
vector $v\in V$ of maximal degree. ("Maximal" means "maximal in real part".
Such a maximal degree exists by the definition of Category $\mathcal{O}^{+}$.)
Let $v$ be an eigenvector of $\mathfrak{g}_{0}$ with eigenvalue $\lambda$.
Consider the submodule of $V$ generated by $v$. This submodule is
highest-weight (since $\mathfrak{g}_{j}v=0$ for $j>0$). Hence, by Lemma
\ref{lem.unitrick}, this submodule is irreducible and therefore $\cong
L_{\lambda_{1}}^{+}$ for some $\lambda_{1}\in\mathfrak{h}^{\ast}$. Let $V_{1}$
be the orthogonal complement of $L_{\lambda_{1}}^{+}$. Then, $V=L_{\lambda
_{1}}^{+}\oplus V_{1}$. Now take a vector in $V_{1}$ etc. etf.. Since the
degrees of $V$ lie in finitely many arithmetic progressions, and homogeneous
subspaces have finite dimension, this process is exhaustive, so we obtain
$V=L_{\lambda_{1}}^{+}\oplus L_{\lambda_{2}}^{+}\oplus...$.

\begin{remark}
In this decomposition, every irreducible object of Category $\mathcal{O}^{+}$
occurs finitely many times.
\end{remark}

\section{Representation theory: concrete examples}

\subsection{Some lemmata about exponentials and commutators}

This subsection is devoted to some elementary lemmata about power series and
iterated commutators over noncommutative rings. These lemmata are well-known
in geometrical contexts (in these contexts they tend to appear in Lie groups
textbooks), but here we will formulate and prove them purely algebraically. We
will not use these lemmata until Theorem \ref{thm.euler}, but I prefer to put
them here in order not to interrupt the flow of representation-theoretical
arguments later.

We start with easy things:

\begin{lemma}
\label{lem.powerseries1}Let $K$ be a commutative ring. If $\alpha$ and $\beta$
are two elements of a topological $K$-algebra $R$ such that $\left[
\alpha,\beta\right]  $ commutes with $\beta$, then $\left[  \alpha,P\left(
\beta\right)  \right]  =\left[  \alpha,\beta\right]  \cdot P^{\prime}\left(
\beta\right)  $ for every power series $P\in K\left[  \left[  X\right]
\right]  $ for which the series $P\left(  \beta\right)  $ and $P^{\prime
}\left(  \beta\right)  $ converge.
\end{lemma}

\textit{Proof of Lemma \ref{lem.powerseries1}.} Let $\gamma=\left[
\alpha,\beta\right]  $. Then, $\gamma$ commutes with $\beta$ (since we know
that $\left[  \alpha,\beta\right]  $ commutes with $\beta$), so that
$\gamma\beta=\beta\gamma$.

Write $P$ in the form $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ for some
$\left(  u_{0},u_{1},u_{2},...\right)  \in K^{\mathbb{N}}$. Then, $P^{\prime
}=\sum\limits_{i=1}^{\infty}iu_{i}X^{i-1}$, so that $P^{\prime}\left(
\beta\right)  =\sum\limits_{i=1}^{\infty}iu_{i}\beta^{i-1}$. On the other
hand, $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ shows that $P\left(
\beta\right)  =\sum\limits_{i=0}^{\infty}u_{i}\beta^{i}$ and thus
\[
\left[  \alpha,P\left(  \beta\right)  \right]  =\left[  \alpha,\sum
\limits_{i=0}^{\infty}u_{i}\beta^{i}\right]  =\sum\limits_{i=0}^{\infty}%
u_{i}\left[  \alpha,\beta^{i}\right]  =u_{0}\underbrace{\left[  \alpha
,\beta^{0}\right]  }_{\substack{=0\\\text{(since }\beta^{0}=1\in Z\left(
R\right)  \text{)}}}+\sum\limits_{i=1}^{\infty}u_{i}\left[  \alpha,\beta
^{i}\right]  =\sum\limits_{i=1}^{\infty}u_{i}\left[  \alpha,\beta^{i}\right]
.
\]


Now, it is easy to prove that every positive $i\in\mathbb{N}$ satisfies
$\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$%
\ \ \ \ \footnote{\textit{Proof.} We will prove this by induction over $i$:
\par
\textit{Induction base:} For $i=1$, we have $\left[  \alpha,\beta^{i}\right]
=\left[  \alpha,\beta^{1}\right]  =\left[  \alpha,\beta\right]  =\gamma$ and
$\underbrace{i}_{=1}\gamma\underbrace{\beta^{i-1}}_{=\beta^{1-1}=1}=\gamma$,
so that $\left[  \alpha,\beta^{i}\right]  =\gamma=i\gamma\beta^{i-1}$. This
proves $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for $i=1$, and
thus the induction base is complete.
\par
\textit{Induction step:} Let $j\in\mathbb{N}$ be positive. Assume that
$\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ is proven for $i=j$. We
must then prove $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for
$i=j+1$.
\par
Since $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ is proven for
$i=j$, we have $\left[  \alpha,\beta^{j}\right]  =j\gamma\beta^{j-1}$.
\par
Now,
\begin{align*}
\left[  \alpha,\underbrace{\beta^{j+1}}_{=\beta\beta^{j}}\right]   &  =\left[
\alpha,\beta\beta^{j}\right]  =\alpha\beta\beta^{j}-\beta\beta^{j}%
\alpha=\underbrace{\left(  \alpha\beta\beta^{j}-\beta\alpha\beta^{j}\right)
}_{=\left(  \alpha\beta-\beta\alpha\right)  \beta^{j}}+\underbrace{\left(
\beta\alpha\beta^{j}-\beta\beta^{j}\alpha\right)  }_{=\beta\left(  \alpha
\beta^{j}-\beta^{j}\alpha\right)  }\\
&  =\underbrace{\left(  \alpha\beta-\beta\alpha\right)  }_{=\left[
\alpha,\beta\right]  =\gamma}\beta^{j}+\beta\underbrace{\left(  \alpha
\beta^{j}-\beta^{j}\alpha\right)  }_{=\left[  \alpha,\beta^{j}\right]
=j\gamma\beta^{j-1}}=\gamma\beta^{j}+\beta j\gamma\beta^{j-1}=\gamma\beta
^{j}+j\underbrace{\beta\gamma}_{=\gamma\beta}\beta^{j-1}\\
&  =\gamma\beta^{j}+j\gamma\underbrace{\beta\beta^{j-1}}_{=\beta^{j}}%
=\gamma\beta^{j}+j\gamma\beta^{j}=\left(  j+1\right)  \gamma\beta^{j}=\left(
j+1\right)  \gamma\beta^{\left(  j+1\right)  -1}.
\end{align*}
In other words, $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ holds
for $i=j+1$. This completes the induction step, and thus by induction we have
proven that $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for every
positive $i\in\mathbb{N}$.}. Hence,%
\[
\left[  \alpha,P\left(  \beta\right)  \right]  =\sum\limits_{i=1}^{\infty
}u_{i}\underbrace{\left[  \alpha,\beta^{i}\right]  }_{=i\gamma\beta^{i-1}%
}=\sum\limits_{i=1}^{\infty}u_{i}i\gamma\beta^{i-1}=\underbrace{\gamma
}_{=\left[  \alpha,\beta\right]  }\underbrace{\sum\limits_{i=1}^{\infty}%
iu_{i}\beta^{i-1}}_{=P^{\prime}\left(  \beta\right)  }=\left[  \alpha
,\beta\right]  \cdot P^{\prime}\left(  \beta\right)  .
\]
Lemma \ref{lem.powerseries1} is proven.

\begin{corollary}
\label{cor.powerseries2}If $\alpha$ and $\beta$ are two elements of a
topological $\mathbb{Q}$-algebra $R$ such that $\left[  \alpha,\beta\right]  $
commutes with $\beta$, then $\left[  \alpha,\exp\beta\right]  =\left[
\alpha,\beta\right]  \cdot\exp\beta$ whenever the power series $\exp\beta$ converges.
\end{corollary}

\textit{Proof of Corollary \ref{cor.powerseries2}.} Applying Lemma
\ref{lem.powerseries1} to $P=\exp X$ and $K=\mathbb{Q}$, and recalling that
$\exp^{\prime}=\exp$, we obtain $\left[  \alpha,\exp\beta\right]  =\left[
\alpha,\beta\right]  \cdot\exp\beta$. This proves Corollary
\ref{cor.powerseries2}.

In Lemma \ref{lem.powerseries1} and Corollary \ref{cor.powerseries2}, we had
to require convergence of certain power series in order for the results to
make sense. In the following, we will prove some results for which such
requirements are not sufficient anymore\footnote{At least they are not
sufficient for my proofs...}; instead we need more global conditions. A
standard condition to require in such cases is that all the elements to which
we apply power series lie in some ideal $I$ of $R$ such that $R$ is complete
and Hausdorff with respect to the $I$-adic topology. Under this condition,
things work nicely, due to the following fact (which is one part of the
universal property of the power series ring $K\left[  \left[  X\right]
\right]  $):

\begin{proposition}
\label{prop.K[[X]].univ}Let $K$ be a commutative ring. Let $R$ be a
$K$-algebra, and $I$ be an ideal of $R$ such that $R$ is complete and
Hausdorff with respect to the $I$-adic topology. Then, for every power series
$P\in K\left[  \left[  X\right]  \right]  $ and every $\alpha\in I$, there is
a well-defined element $P\left(  \alpha\right)  \in R$ (which is defined as
the limit $\lim\limits_{n\rightarrow\infty}\sum\limits_{i=0}^{n}u_{i}%
\alpha^{i}$ (with respect to the $I$-adic topology), where the power series
$P$ is written in the form $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ for some
$\left(  u_{0},u_{1},u_{2},...\right)  \in K^{\mathbb{N}}$). For every
$\alpha\in I$, the map $K\left[  \left[  X\right]  \right]  \rightarrow R$
which sends every $P\in K\left[  \left[  X\right]  \right]  $ to $P\left(
\alpha\right)  $ is a continuous $K$-algebra homomorphism (where the topology
on $K\left[  \left[  X\right]  \right]  $ is the standard one, and the
topology on $R$ is the $I$-adic one).
\end{proposition}

\begin{theorem}
\label{thm.exp(u+v)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an ideal
of $R$ such that $R$ is complete and Hausdorff with respect to the $I$-adic
topology. Let $\alpha\in I$ and $\beta\in I$ be such that $\alpha\beta
=\beta\alpha$. Then, $\exp\alpha$, $\exp\beta$ and $\exp\left(  \alpha
+\beta\right)  $ are well-defined (by Proposition \ref{prop.K[[X]].univ}) and
satisfy $\exp\left(  \alpha+\beta\right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  $.
\end{theorem}

\textit{Proof of Theorem \ref{thm.exp(u+v)}.} Comparing%
\[
\exp\left(  \alpha+\beta\right)  =\sum\limits_{n=0}^{\infty}\dfrac{\left(
\alpha+\beta\right)  ^{n}}{n!}=\sum\limits_{n=0}^{\infty}\dfrac{1}%
{n!}\underbrace{\left(  \alpha+\beta\right)  ^{n}}_{=\sum\limits_{i=0}%
^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i}}=\sum\limits_{n=0}^{\infty}\dfrac
{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i}%
\]
with%
\begin{align*}
\underbrace{\left(  \exp\alpha\right)  }_{=\sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}}\cdot\underbrace{\left(  \exp\beta\right)  }%
_{=\sum\limits_{j=0}^{\infty}\dfrac{\beta^{j}}{j!}}  &  =\left(
\sum\limits_{i=0}^{\infty}\dfrac{\alpha^{i}}{i!}\right)  \cdot\left(
\sum\limits_{j=0}^{\infty}\dfrac{\beta^{j}}{j!}\right)  =\sum\limits_{i=0}%
^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{\alpha^{i}\beta^{j}}{i!j!}%
=\sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{1}{i!j!}\alpha
^{i}\beta^{j}\\
&  =\underbrace{\sum\limits_{i=0}^{\infty}\sum\limits_{n=i}^{\infty}}%
_{=\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}}\underbrace{\dfrac
{1}{i!\left(  n-i\right)  !}}_{\substack{=\dfrac{1}{n!}\dbinom{n}%
{i}\\\text{(since }\dbinom{n}{i}=\dfrac{n!}{i!\left(  n-i\right)  !}\text{)}%
}}\alpha^{i}\beta^{n-i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n\text{ for
}i+j\text{ in the second sum}\right) \\
&  =\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}\dfrac{1}{n!}\dbinom{n}%
{i}\alpha^{i}\beta^{n-i}=\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}%
\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i},
\end{align*}
we obtain $\exp\left(  \alpha+\beta\right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  $. This proves Theorem \ref{thm.exp(u+v)}.

\begin{corollary}
\label{cor.exp(-w)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an ideal
of $R$ such that $R$ is complete and Hausdorff with respect to the $I$-adic
topology. Let $\gamma\in I$. Then, $\exp\gamma$ and $\exp\left(
-\gamma\right)  $ are well-defined (by Proposition \ref{prop.K[[X]].univ}) and
satisfy $\left(  \exp\gamma\right)  \cdot\left(  \exp\left(  -\gamma\right)
\right)  =1$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.exp(-w)}.} By Theorem \ref{thm.exp(u+v)}
(applied to $\alpha=\gamma$ and $\beta=-\gamma$), we have $\exp\left(
\gamma+\left(  -\gamma\right)  \right)  =\left(  \exp\gamma\right)
\cdot\left(  \exp\left(  -\gamma\right)  \right)  $, thus%
\[
\left(  \exp\gamma\right)  \cdot\left(  \exp\left(  -\gamma\right)  \right)
=\exp\underbrace{\left(  \gamma+\left(  -\gamma\right)  \right)  }_{=0}%
=\exp0=1.
\]
This proves Corollary \ref{cor.exp(-w)}.

\begin{theorem}
\label{thm.exp(a)bexp(-a)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an
ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$. Denote by $\operatorname*{ad}\alpha$ the
map $R\rightarrow R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[
\alpha,x\right]  $ denotes the commutator $\alpha x-x\alpha$).

\textbf{(a)} Then, the infinite series $\sum\limits_{n=0}^{\infty}%
\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}$ converges pointwise
(i. e., for every $x\in R$, the infinite series $\sum\limits_{n=0}^{\infty
}\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}\left(  x\right)  $
converges). Denote the value of this series by $\exp\left(  \operatorname*{ad}%
\alpha\right)  $.

\textbf{(b)} We have $\left(  \exp\alpha\right)  \cdot\beta\cdot\left(
\exp\left(  -\alpha\right)  \right)  =\left(  \exp\left(  \operatorname*{ad}%
\alpha\right)  \right)  \left(  \beta\right)  $ for every $\beta\in R$.
\end{theorem}

To prove this, we will use a lemma:

\begin{lemma}
\label{lem.exp(a)bexp(-a)}Let $R$ be a ring. Let $\alpha$ and $\beta$ be
elements of $R$. Denote by $\operatorname*{ad}\alpha$ the map $R\rightarrow
R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[  \alpha,x\right]  $
denotes the commutator $\alpha x-x\alpha$). Let $n\in\mathbb{N}$. Then,
\[
\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)
=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(  -\alpha\right)
^{n-i}.
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.exp(a)bexp(-a)}.} Let $L_{\alpha}$ denote the
map $R\rightarrow R,$ $x\mapsto\alpha x$. Let $R_{\alpha}$ denote the map
$R\rightarrow R,\ x\mapsto x\alpha$. Then, every $x\in R$ satisfies%
\[
\left(  L_{\alpha}-R_{\alpha}\right)  \left(  x\right)  =\underbrace{L_{\alpha
}\left(  x\right)  }_{\substack{=\alpha x\\\text{(by the definition of
}L_{\alpha}\text{)}}}-\underbrace{R_{\alpha}\left(  x\right)  }%
_{\substack{=x\alpha\\\text{(by the definition of }R_{\alpha}\text{)}}}=\alpha
x-x\alpha=\left[  \alpha,x\right]  =\left(  \operatorname*{ad}\alpha\right)
\left(  x\right)  .
\]
Hence, $L_{\alpha}-R_{\alpha}=\operatorname*{ad}\alpha$.

Also, every $x\in R$ satisfies%
\[
\left(  L_{\alpha}\circ R_{\alpha}\right)  \left(  x\right)  =L_{\alpha
}\underbrace{\left(  R_{\alpha}\left(  x\right)  \right)  }%
_{\substack{=x\alpha\\\text{(by the definition of }R_{\alpha}\text{)}%
}}=L_{\alpha}\left(  x\alpha\right)  =\alpha x\alpha
\]
(by the definition of $L_{\alpha}$) and%
\[
\left(  R_{\alpha}\circ L_{\alpha}\right)  \left(  x\right)  =R_{\alpha
}\underbrace{\left(  L_{\alpha}\left(  x\right)  \right)  }_{\substack{=\alpha
x\\\text{(by the definition of }L_{\alpha}\text{)}}}=R_{\alpha}\left(  \alpha
x\right)  =\alpha x\alpha
\]
(by the definition of $R_{\alpha}$), so that $\left(  L_{\alpha}\circ
R_{\alpha}\right)  \left(  x\right)  =\left(  R_{\alpha}\circ L_{\alpha
}\right)  \left(  x\right)  $. Hence, $L_{\alpha}\circ R_{\alpha}=R_{\alpha
}\circ L_{\alpha}$. In other words, the maps $L_{\alpha}$ and $R_{\alpha}$
commute. Thus, we can apply the binomial formula to $L_{\alpha}$ and
$R_{\alpha}$, and conclude that $\left(  L_{\alpha}-R_{\alpha}\right)
^{n}=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}%
^{i}\circ R_{\alpha}^{n-i}$. Since $L_{\alpha}-R_{\alpha}=\operatorname*{ad}%
\alpha$, this rewrites as $\left(  \operatorname*{ad}\alpha\right)  ^{n}%
=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}%
^{i}\circ R_{\alpha}^{n-i}$.

Now, it is easy to see (by induction over $j$) that
\begin{equation}
L_{\alpha}^{j}y=\alpha^{j}y\ \ \ \ \ \ \ \ \ \ \text{for every }j\in
\mathbb{N}\text{ and }y\in R. \label{pf.exp(a)bexp(-a).1}%
\end{equation}
Also, it is easy to see (by induction over $j$) that
\begin{equation}
R_{\alpha}^{j}y=y\alpha^{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in
\mathbb{N}\text{ and }y\in R. \label{pf.exp(a)bexp(-a).2}%
\end{equation}


Now, since $\left(  \operatorname*{ad}\alpha\right)  ^{n}=\sum\limits_{i=0}%
^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}^{i}\circ R_{\alpha}%
^{n-i}$, we have%
\begin{align*}
\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)   &
=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}%
\underbrace{\left(  L_{\alpha}^{i}\circ R_{\alpha}^{n-i}\right)  \left(
\beta\right)  }_{\substack{=L_{\alpha}^{i}\left(  R_{\alpha}^{n-i}%
\beta\right)  =\alpha^{i}R_{\alpha}^{n-i}\beta\\\text{(by
(\ref{pf.exp(a)bexp(-a).1}), applied to }j=i\text{ and }y=R_{\alpha}%
^{n-i}\beta\text{)}}}\\
&  =\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}\alpha
^{i}\underbrace{R_{\alpha}^{n-i}\beta}_{\substack{=\beta\alpha^{n-i}%
\\\text{(by (\ref{pf.exp(a)bexp(-a).2}), applied to }j=n-i\text{ and }%
y=\beta\text{)}}}\\
&  =\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}\alpha^{i}%
\beta\alpha^{n-i}=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(
-\alpha\right)  ^{n-i}.
\end{align*}
This proves Lemma \ref{lem.exp(a)bexp(-a)}.

\textit{Proof of Theorem \ref{thm.exp(a)bexp(-a)}.} \textbf{(a)} For every
$x\in R$ and every $n\in\mathbb{N}$, we have $\left(  \operatorname*{ad}%
\alpha\right)  ^{n}\left(  x\right)  \in I^{n}$ (this can be easily proven by
induction over $n$, using the fact that $I$ is an ideal) and thus
$\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}\left(  x\right)
=\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
x\right)  }_{\in I^{n}}\in I^{n}$. Hence, for every $x\in R$, the infinite
series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  x\right)  $ converges (because $R$ is complete
and Hausdorff with respect to the $I$-adic topology). In other words, the
infinite series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}$ converges pointwise. Theorem
\ref{thm.exp(a)bexp(-a)} \textbf{(a)} is proven.

\textbf{(b)} Let $\beta\in R$. By the definition of of $\exp\left(
\operatorname*{ad}\alpha\right)  $, we have%
\begin{align*}
\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)   &  =\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  \beta\right)  =\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
\beta\right)  }_{\substack{=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}%
\beta\left(  -\alpha\right)  ^{n-i}\\\text{(by Lemma \ref{lem.exp(a)bexp(-a)}%
)}}}\\
&  =\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}%
{i}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}.
\end{align*}
Compared with%
\begin{align*}
\underbrace{\left(  \exp\alpha\right)  }_{=\sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}}\cdot\beta\cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  }_{=\sum\limits_{j=0}^{\infty}\dfrac{\left(
-\alpha\right)  ^{j}}{j!}}  &  =\left(  \sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}\right)  \cdot\beta\cdot\left(  \sum\limits_{j=0}%
^{\infty}\dfrac{\left(  -\alpha\right)  ^{j}}{j!}\right) \\
&  =\sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{\alpha^{i}%
\beta\left(  -\alpha\right)  ^{j}}{i!j!}=\sum\limits_{i=0}^{\infty}%
\sum\limits_{j=0}^{\infty}\dfrac{1}{i!j!}\alpha^{i}\beta\left(  -\alpha
\right)  ^{j}\\
&  =\underbrace{\sum\limits_{i=0}^{\infty}\sum\limits_{n=i}^{\infty}}%
_{=\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}}\underbrace{\dfrac
{1}{i!\left(  n-i\right)  !}}_{\substack{=\dfrac{1}{n!}\dbinom{n}%
{i}\\\text{(since }\dbinom{n}{i}=\dfrac{n!}{i!\left(  n-i\right)  !}\text{)}%
}}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n\text{ for
}i+j\text{ in the second sum}\right) \\
&  =\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}\dfrac{1}{n!}\dbinom{n}%
{i}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}=\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(
-\alpha\right)  ^{n-i},
\end{align*}
this yields $\left(  \exp\alpha\right)  \cdot\beta\cdot\left(  \exp\left(
-\alpha\right)  \right)  =\left(  \exp\left(  \operatorname*{ad}\alpha\right)
\right)  \left(  \beta\right)  $. This proves Theorem \ref{thm.exp(a)bexp(-a)}
\textbf{(b)}.

\begin{corollary}
\label{cor.exp(a)exp(b)exp(-a)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$
be an ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$. Denote by $\operatorname*{ad}\alpha$ the
map $R\rightarrow R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[
\alpha,x\right]  $ denotes the commutator $\alpha x-x\alpha$).

As we know from Theorem \ref{thm.exp(a)bexp(-a)} \textbf{(a)}, the infinite
series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}$ converges pointwise. Denote the value of this series
by $\exp\left(  \operatorname*{ad}\alpha\right)  $.

We have $\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  =\exp\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  $ for every $\beta\in I$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.exp(a)exp(b)exp(-a)}.} Corollary
\ref{cor.exp(-w)} (applied to $\gamma=-\alpha$) yields $\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\left(  -\left(  -\alpha\right)
\right)  \right)  =1$. Since $-\left(  -\alpha\right)  =\alpha$, this rewrites
as $\left(  \exp\left(  -\alpha\right)  \right)  \cdot\left(  \exp
\alpha\right)  =1$.

Let $\beta\in I$. Let $T$ denote the map $R\rightarrow R,\ x\mapsto\left(
\exp\alpha\right)  \cdot x\cdot\left(  \exp\left(  -\alpha\right)  \right)  $.
Clearly, this map $T$ is $\mathbb{Q}$-linear. It also satisfies%
\begin{align*}
T\left(  1\right)   &  =\left(  \exp\alpha\right)  \cdot1\cdot\left(
\exp\left(  -\alpha\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }T\right) \\
&  =\left(  \exp\alpha\right)  \cdot\left(  \exp\left(  -\alpha\right)
\right)  =1,
\end{align*}
and any $x\in R$ and $y\in R$ satisfy%
\begin{align*}
\underbrace{T\left(  x\right)  }_{\substack{=\left(  \exp\alpha\right)  \cdot
x\cdot\left(  \exp\left(  -\alpha\right)  \right)  \\\left(  \text{by the
definition of }T\right)  }}\cdot\underbrace{T\left(  y\right)  }%
_{\substack{=\left(  \exp\alpha\right)  \cdot y\cdot\left(  \exp\left(
-\alpha\right)  \right)  \\\left(  \text{by the definition of }T\right)  }}
&  =\left(  \exp\alpha\right)  \cdot x\cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\alpha\right)  }_{=1}\cdot
y\cdot\left(  \exp\left(  -\alpha\right)  \right) \\
&  =\left(  \exp\alpha\right)  \cdot xy\cdot\left(  \exp\left(  -\alpha
\right)  \right)  =T\left(  xy\right)
\end{align*}
(since $T\left(  xy\right)  =\left(  \exp\alpha\right)  \cdot xy\cdot\left(
\exp\left(  -\alpha\right)  \right)  $ by the definition of $T$). Hence, $T$
is a $\mathbb{Q}$-algebra homomorphism. Also, $T$ is continuous (with respect
to the $I$-adic topology). Thus, $T$ is a continuous $\mathbb{Q}$-algebra
homomorphism, and hence commutes with the application of power series. Thus,
$T\left(  \exp\beta\right)  =\exp\left(  T\left(  \beta\right)  \right)  $.
But since $T\left(  \exp\beta\right)  =\left(  \exp\alpha\right)  \cdot\left(
\exp\beta\right)  \cdot\left(  \exp\left(  -\alpha\right)  \right)  $ (by the
definition of $T$) and%
\begin{align*}
T\left(  \beta\right)   &  =\left(  \exp\alpha\right)  \cdot\beta\cdot\left(
\exp\left(  -\alpha\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }T\right) \\
&  =\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem
\ref{thm.exp(a)bexp(-a)} \textbf{(b)}}\right)  ,
\end{align*}
this rewrites as $\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  =\exp\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  $. This proves Corollary \ref{cor.exp(a)exp(b)exp(-a)}.

\begin{lemma}
\label{lem.powerseries3}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an
ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$ and $\beta\in I$. Assume that $\left[
\alpha,\beta\right]  $ commutes with each of $\alpha$ and $\beta$. Then,
$\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  $.
\end{lemma}

First we give two short proofs of this lemma.

\textit{First proof of Lemma \ref{lem.powerseries3}.} Define the map
$\operatorname*{ad}\alpha$ as in Corollary \ref{cor.exp(a)exp(b)exp(-a)}.
Then, $\left(  \operatorname*{ad}\alpha\right)  ^{2}\left(  \beta\right)
=\left[  \alpha,\left[  \alpha,\beta\right]  \right]  =0$ (since $\left[
\alpha,\beta\right]  $ commutes with $\alpha$). Hence, $\left(
\operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)  =0$ for every
integer $n\geq2$. Now, by the definition of $\exp\left(  \operatorname*{ad}%
\alpha\right)  $, we have%
\begin{align*}
\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)   &  =\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  \beta\right)  =\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
\beta\right) \\
&  =\underbrace{\dfrac{1}{0!}}_{=1}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{0}}_{=\operatorname*{id}}\left(  \beta\right)
+\underbrace{\dfrac{1}{1!}}_{=1}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{1}}_{=\operatorname*{ad}\alpha}\left(  \beta\right)
+\sum\limits_{n=2}^{\infty}\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}\left(  \beta\right)  }_{\substack{=0\\\text{(since }%
n\geq2\text{)}}}\\
&  =\underbrace{\operatorname*{id}\left(  \beta\right)  }_{=\beta
}+\underbrace{\left(  \operatorname*{ad}\alpha\right)  \left(  \beta\right)
}_{=\left[  \alpha,\beta\right]  }+\underbrace{\sum\limits_{n=2}^{\infty
}\dfrac{1}{n!}0}_{=0}=\beta+\left[  \alpha,\beta\right]  .
\end{align*}
By Corollary \ref{cor.exp(a)exp(b)exp(-a)}, we now have%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \cdot\left(
\exp\left(  -\alpha\right)  \right)  =\exp\underbrace{\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  }_{=\beta+\left[  \alpha,\beta\right]  }=\exp\left(  \beta+\left[
\alpha,\beta\right]  \right)  .
\]
But $\beta$ and $\left[  \alpha,\beta\right]  $ commute, so that $\beta\left[
\alpha,\beta\right]  =\left[  \alpha,\beta\right]  \beta$. Hence, Theorem
\ref{thm.exp(u+v)} (applied to $\beta$ and $\left[  \alpha,\beta\right]  $
instead of $\alpha$ and $\beta$) yields $\exp\left(  \beta+\left[
\alpha,\beta\right]  \right)  =\left(  \exp\beta\right)  \cdot\left(
\exp\left[  \alpha,\beta\right]  \right)  $.

On the other hand,%
\begin{align*}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \cdot\left(
\exp\left(  -\alpha\right)  \right)  \cdot\left(  \exp\underbrace{\alpha
}_{=-\left(  -\alpha\right)  }\right)   &  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  \cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\left(  -\left(  -\alpha\right)
\right)  \right)  }_{\substack{=1\\\text{(by Corollary \ref{cor.exp(-w)},
applied to }\gamma=-\alpha\text{)}}}\\
&  =\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  .
\end{align*}
Compared with%
\[
\underbrace{\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  }_{=\exp\left(
\beta+\left[  \alpha,\beta\right]  \right)  =\left(  \exp\beta\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }\cdot\left(
\exp\alpha\right)  =\left(  \exp\beta\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  \cdot\left(  \exp\alpha\right)  ,
\]
this yields%
\begin{equation}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)
\cdot\left(  \exp\alpha\right)  . \label{pf.powerseries3.4}%
\end{equation}


Besides, $\alpha$ and $\left[  \alpha,\beta\right]  $ commute, so that
$\alpha\left[  \alpha,\beta\right]  =\left[  \alpha,\beta\right]  \alpha$.
Hence, Theorem \ref{thm.exp(u+v)} (applied to $\left[  \alpha,\beta\right]  $
instead of $\beta$) yields $\exp\left(  \alpha+\left[  \alpha,\beta\right]
\right)  =\left(  \exp\alpha\right)  \cdot\left(  \exp\left[  \alpha
,\beta\right]  \right)  $.

On the other hand, $\alpha$ and $\left[  \alpha,\beta\right]  $ commute, so
that $\left[  \alpha,\beta\right]  \alpha=\alpha\left[  \alpha,\beta\right]
$. Hence, Theorem \ref{thm.exp(u+v)} (applied to $\left[  \alpha,\beta\right]
$ and $\alpha$ instead of $\alpha$ and $\beta$) yields $\exp\left(  \left[
\alpha,\beta\right]  +\alpha\right)  =\left(  \exp\left[  \alpha,\beta\right]
\right)  \cdot\left(  \exp\alpha\right)  $.

Thus, $\left(  \exp\left[  \alpha,\beta\right]  \right)  \cdot\left(
\exp\alpha\right)  =\exp\underbrace{\left(  \left[  \alpha,\beta\right]
+\alpha\right)  }_{=\alpha+\left[  \alpha,\beta\right]  }=\exp\left(
\alpha+\left[  \alpha,\beta\right]  \right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $. Now,
(\ref{pf.powerseries3.4}) becomes%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\underbrace{\left(  \exp\left[  \alpha,\beta\right]
\right)  \cdot\left(  \exp\alpha\right)  }_{=\left(  \exp\alpha\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }=\left(  \exp
\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  .
\]
This proves Lemma \ref{lem.powerseries3}.

\textit{Second proof of Lemma \ref{lem.powerseries3}.} Clearly, $\left[
\beta,\alpha\right]  =-\left[  \alpha,\beta\right]  $ commutes with each of
$\alpha$ and $\beta$ (since $\left[  \alpha,\beta\right]  $ commutes with each
of $\alpha$ and $\beta$).

The Baker-Campbell-Hausdorff formula has the form%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\exp\left(
\alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  +\left(  \text{higher
terms}\right)  \right)  ,
\]
where the "higher terms" on the right hand side mean $\mathbb{Q}$-linear
combinations of nested Lie brackets of three or more $\alpha$'s and $\beta$'s.
Since $\left[  \alpha,\beta\right]  $ commutes with each of $\alpha$ and
$\beta$, all of these higher terms are zero, and thus the
Baker-Campbell-Hausdorff formula simplifies to%
\begin{equation}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\exp\left(
\alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  .
\label{pf.powerseries3.6}%
\end{equation}
Applying this to $\beta$ and $\alpha$ instead of $\alpha$ and $\beta$, we
obtain%
\[
\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  =\exp\left(
\beta+\alpha+\dfrac{1}{2}\left[  \beta,\alpha\right]  \right)  .
\]
Since $\left[  \beta,\alpha\right]  =-\left[  \alpha,\beta\right]  $, this
becomes%
\begin{equation}
\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  =\exp\left(
\beta+\alpha+\dfrac{1}{2}\underbrace{\left[  \beta,\alpha\right]  }_{=-\left[
\alpha,\beta\right]  }\right)  =\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[
\alpha,\beta\right]  \right)  . \label{pf.powerseries3.8}%
\end{equation}


Now, $\left[  \alpha,\beta\right]  $ commutes with each of $\alpha$ and
$\beta$ (by the assumptions of the lemma) and also with $\left[  \alpha
,\beta\right]  $ itself (clearly). Hence, $\left[  \alpha,\beta\right]  $
commutes with $\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  $. In
other words, $\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]
\right)  \left[  \alpha,\beta\right]  =\left[  \alpha,\beta\right]  \left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  $. Hence,
Theorem \ref{thm.exp(u+v)} (applied to $\beta+\alpha-\dfrac{1}{2}\left[
\alpha,\beta\right]  $ and $\left[  \alpha,\beta\right]  $ instead of $\alpha$
and $\beta$) yields $\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  +\left[  \alpha,\beta\right]  \right)  =\left(  \exp\left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  \right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $. Now,%
\begin{align*}
\underbrace{\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)
}_{\substack{=\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  \right)  \\\text{(by (\ref{pf.powerseries3.8}))}}}\cdot\left(
\exp\left[  \alpha,\beta\right]  \right)   &  =\left(  \exp\left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  \right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right) \\
&  =\exp\underbrace{\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  +\left[  \alpha,\beta\right]  \right)  }_{=\alpha+\beta
+\dfrac{1}{2}\left[  \alpha,\beta\right]  }\\
&  =\exp\left(  \alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)
=\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\end{align*}
(by (\ref{pf.powerseries3.6})). Lemma \ref{lem.powerseries3} is proven.

We are going to also present a third, very elementary (term-by-term) proof of
Lemma \ref{lem.powerseries3}. It relies on the following proposition, which
can also be applied in some other contexts (e. g., computing in universal
enveloping algebras):

\begin{proposition}
\label{prop.powerseries3.fin}Let $R$ be a ring. Let $\alpha\in R$ and
$\beta\in R$. Assume that $\left[  \alpha,\beta\right]  $ commutes with each
of $\alpha$ and $\beta$. Then, for every $i\in\mathbb{N}$ and $j\in\mathbb{N}%
$, we have%
\[
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha
,\beta\right]  ^{k}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.powerseries3.fin}.} Let $\gamma$ denote
$\left[  \alpha,\beta\right]  $. Then, $\gamma$ commutes with each of $\alpha$
and $\beta$ (since $\left[  \alpha,\beta\right]  $ commutes with each of
$\alpha$ and $\beta$). In other words, $\gamma\alpha=\alpha\gamma$ and
$\gamma\beta=\beta\gamma$.

As we showed in the proof of Lemma \ref{lem.powerseries1}, every positive
$i\in\mathbb{N}$ satisfies $\left[  \alpha,\beta^{i}\right]  =i\gamma
\beta^{i-1}$. Since $\gamma=\left[  \alpha,\beta\right]  $, this rewrites as
follows:
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\left[  \alpha,\beta
^{i}\right]  =i\left[  \alpha,\beta\right]  \beta^{i-1}.
\label{pf.powerseries3.fin.1}%
\end{equation}


Since $\left[  \beta,\alpha\right]  =-\underbrace{\left[  \alpha,\beta\right]
}_{=\gamma}=-\gamma$, we see that $\underbrace{\left[  \beta,\alpha\right]
}_{=-\gamma}\alpha=-\underbrace{\gamma\alpha}_{=\alpha\gamma}=-\alpha
\gamma=\alpha\underbrace{\left(  -\gamma\right)  }_{=\left[  \beta
,\alpha\right]  }=\alpha\left[  \beta,\alpha\right]  $ and
$\underbrace{\left[  \beta,\alpha\right]  }_{=-\gamma}\beta
=-\underbrace{\gamma\beta}_{=\beta\gamma}=-\beta\gamma=\beta
\underbrace{\left(  -\gamma\right)  }_{=\left[  \beta,\alpha\right]  }%
=\beta\left[  \beta,\alpha\right]  $. In other words, $\left[  \beta
,\alpha\right]  $ commutes with each of $\alpha$ and $\beta$. Therefore, the
roles of $\alpha$ and $\beta$ are symmetric, and thus we can apply
(\ref{pf.powerseries3.fin.1}) to $\beta$ and $\alpha$ instead of $\alpha$ and
$\beta$, and conclude that%
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\left[  \beta,\alpha
^{i}\right]  =i\left[  \beta,\alpha\right]  \alpha^{i-1}.
\label{pf.powerseries3.fin.1a}%
\end{equation}
Thus, every positive $i\in\mathbb{N}$ satisfies $\beta\alpha^{i}-\alpha
^{i}\beta=\left[  \beta,\alpha^{i}\right]  =i\underbrace{\left[  \beta
,\alpha\right]  }_{=-\gamma}\alpha^{i-1}=-i\gamma\alpha^{i-1}$, so that
$\beta\alpha^{i}=\alpha^{i}\beta-i\gamma\alpha^{i-1}$ and thus $\alpha
^{i}\beta=\beta\alpha^{i}+i\gamma\alpha^{i-1}$. We have thus proven that%
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\alpha^{i}\beta
=\beta\alpha^{i}+i\gamma\alpha^{i-1}. \label{pf.powerseries3.fin.1b}%
\end{equation}


Now, we are going to prove that every $i\in\mathbb{N}$ and $j\in\mathbb{N}$
satisfy%
\begin{equation}
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}.
\label{pf.powerseries3.fin.2}%
\end{equation}


\textit{Proof of (\ref{pf.powerseries3.fin.2}):} We will prove
(\ref{pf.powerseries3.fin.2}) by induction over $i$:

\textit{Induction base:} Let $j\in\mathbb{N}$ be arbitrary. For $i=0$, we have
$\alpha^{j}\beta^{i}=\alpha^{j}\underbrace{\beta^{0}}_{=1}=\alpha^{j}$ and
\begin{align*}
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}%
{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}  &  =\underbrace{\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq0;\ k\leq j}}}_{=\sum\limits_{k\in
\left\{  0\right\}  }}k!\dbinom{0}{k}\dbinom{j}{k}\beta^{0-k}\alpha
^{j-k}\gamma^{k}=\sum\limits_{k\in\left\{  0\right\}  }k!\dbinom{0}{k}%
\dbinom{j}{k}\beta^{0-k}\alpha^{j-k}\gamma^{k}\\
&  =\underbrace{0!}_{=1}\underbrace{\dbinom{0}{0}}_{=1}\underbrace{\dbinom
{j}{0}}_{=1}\underbrace{\beta^{0-0}}_{=1}\underbrace{\alpha^{j-0}}%
_{=\alpha^{j}}\underbrace{\gamma^{0}}_{=1}=\alpha^{j}.
\end{align*}
Hence, for $i=0$, we have $\alpha^{j}\beta^{i}=\alpha^{j}=\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}{k}%
\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}$. Thus,
(\ref{pf.powerseries3.fin.2}) holds for $i=0$, so that the induction base is complete.

\textit{Induction step:} Let $u\in\mathbb{N}$. Assume that
(\ref{pf.powerseries3.fin.2}) holds for $i=u$. We must now prove that
(\ref{pf.powerseries3.fin.2}) holds for $i=u+1$.

Since (\ref{pf.powerseries3.fin.2}) holds for $i=u$, we have%
\begin{equation}
\alpha^{j}\beta^{u}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\mathbb{N}.
\label{pf.powerseries3.fin.3}%
\end{equation}


Now, let $j\in\mathbb{N}$ be positive. Then, $j-1\in\mathbb{N}$. Now,%
\begin{align}
&  \alpha^{j}\underbrace{\beta^{u+1}}_{=\beta\beta^{u}}\nonumber\\
&  =\underbrace{\alpha^{j}\beta}_{\substack{=\beta\alpha^{j}+j\gamma
\alpha^{j-1}\\\text{(by (\ref{pf.powerseries3.fin.1b}),}\\\text{applied to
}j\text{ instead of }i\text{)}}}\beta^{u}=\left(  \beta\alpha^{j}%
+j\gamma\alpha^{j-1}\right)  \beta^{u}=\beta\alpha^{j}\beta^{u}%
+j\underbrace{\gamma\alpha^{j-1}\beta^{u}}_{\substack{=\alpha^{j-1}\beta
^{u}\gamma\\\text{(since }\gamma\text{ commutes with}\\\text{each of }%
\beta\text{ and }\alpha\text{)}}}\nonumber\\
&  =\beta\underbrace{\alpha^{j}\beta^{u}}_{\substack{=\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}{k}%
\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}\\\text{(by
(\ref{pf.powerseries3.fin.3}))}}}+j\underbrace{\alpha^{j-1}\beta^{u}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma
^{k}\\\text{(by (\ref{pf.powerseries3.fin.3}), applied to }j-1\\\text{instead
of }j\text{ (since }j-1\in\mathbb{N}\text{))}}}\gamma\nonumber\\
&  =\beta\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}%
}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}+j\left(
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom{u}%
{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k}\right)  \gamma
\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\underbrace{\beta\beta^{u-k}}_{=\beta^{u+1-k}}\alpha
^{j-k}\gamma^{k}+j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\underbrace{\gamma
^{k}\gamma}_{=\gamma^{k+1}}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}+j\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom{u}%
{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +j\sum\limits_{\substack{k\in\mathbb{N};\ k\geq
1;\\k\leq u+1;\ k\leq j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}%
{k-1}\underbrace{\beta^{u-\left(  k-1\right)  }}_{=\beta^{u+1-k}%
}\underbrace{\alpha^{j-1-\left(  k-1\right)  }}_{=\alpha^{j-k}}%
\underbrace{\gamma^{\left(  k-1\right)  +1}}_{=\gamma^{k}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k-1\text{ for
}k\text{ in the second sum}\right) \nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}+j\sum
\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq j}}\left(
k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j}. \label{pf.powerseries3.fin.4}%
\end{align}
Let us separately simplify the two sums on the right hand side of this equation.

First of all, every $k\in\mathbb{N}$ which satisfies $k\leq u+1$ and $k\leq j$
but does \textbf{not} satisfy $k\leq u$ must satisfy \newline$k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=0$ (because this $k$ does
not satisfy $k\leq u$, so that we have $k>u$, and thus $\dbinom{u}{k}=0$).
Thus, $\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ \left(  \text{not
}k\leq u\right)  ;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq
u+1;\ \left(  \text{not }k\leq u\right)  ;\ k\leq j}}0=0$. Hence,%
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\nonumber\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq
u;\ k\leq j}}}_{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}%
}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
+\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ \left(
\text{not }k\leq u\right)  ;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}}_{=0}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.5}%
\end{align}


Besides, every $k\in\mathbb{N}$ which satisfies $k\leq u+1$ and $k\leq j$ but
does \textbf{not} satisfy $k\geq1$ must satisfy \newline$\left(  k-1\right)
!\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=0$
(because this $k$ does not satisfy $k\geq1$, so that we have $k<1$, and thus
$\dbinom{u}{k-1}=0$). Thus, \newline$\sum\limits_{\substack{k\in
\mathbb{N};\ \left(  \text{not }k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}\left(
k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N};\ \left(  \text{not }%
k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}0=0$. Hence,%
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N}\\k\leq u+1;\ k\leq j}}\left(
k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{k\in\mathbb{N}%
;\ \left(  \text{not }k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}\left(
k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j}}_{=0}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}. \label{pf.powerseries3.fin.6}%
\end{align}


Also, notice that every $k\in\mathbb{N}$ satisfies%
\begin{align}
&  k!\underbrace{\dbinom{u}{k}\dbinom{j}{k}}_{=\dbinom{j}{k}\dbinom{u}{k}%
}+j\left(  k-1\right)  !\underbrace{\dbinom{u}{k-1}\dbinom{j-1}{k-1}%
}_{=\dbinom{j-1}{k-1}\dbinom{u}{k-1}}\nonumber\\
&  =k!\underbrace{\dbinom{j}{k}}_{=\dfrac{j!}{k!\left(  j-k\right)  !}}%
\dbinom{u}{k}+j\left(  k-1\right)  !\underbrace{\dbinom{j-1}{k-1}}%
_{=\dfrac{\left(  j-1\right)  !}{\left(  k-1\right)  !\left(  j-k\right)  !}%
}\dbinom{u}{k-1}\nonumber\\
&  =\underbrace{k!\dfrac{j!}{k!\left(  j-k\right)  !}}_{=\dfrac{j!}{\left(
j-k\right)  !}}\dbinom{u}{k}+\underbrace{j\left(  k-1\right)  !\dfrac{\left(
j-1\right)  !}{\left(  k-1\right)  !\left(  j-k\right)  !}}_{\substack{=\dfrac
{j\left(  j-1\right)  !}{\left(  j-k\right)  !}=\dfrac{j!}{\left(  j-k\right)
!}\\\text{(since }j\left(  j-1\right)  !=j!\text{)}}}\dbinom{u}{k-1}%
\nonumber\\
&  =\dfrac{j!}{\left(  j-k\right)  !}\dbinom{u}{k}+\dfrac{j!}{\left(
j-k\right)  !}\dbinom{u}{k-1}=\underbrace{\dfrac{j!}{\left(  j-k\right)  !}%
}_{\substack{=k!\dbinom{j}{k}\\\text{(since }\dbinom{j}{k}=\dfrac
{j!}{k!\left(  j-k\right)  !}\text{)}}}\underbrace{\left(  \dbinom{u}%
{k}+\dbinom{u}{k-1}\right)  }_{\substack{=\dbinom{u+1}{k}\\\text{(by the
recurrence equation}\\\text{of the binomial coefficients)}}}\nonumber\\
&  =k!\dbinom{j}{k}\dbinom{u+1}{k}=k!\dbinom{u+1}{k}\dbinom{j}{k}.
\label{pf.powerseries3.fin.binom}%
\end{align}


Now, (\ref{pf.powerseries3.fin.4}) becomes%
\begin{align*}
\alpha^{j}\beta^{u+1}  &  =\underbrace{\sum\limits_{\substack{k\in
\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}}_{\substack{=\sum\limits_{\substack{k\in
\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}\\\text{(by (\ref{pf.powerseries3.fin.5}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +j\underbrace{\sum\limits_{\substack{k\in
\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq j}}\left(  k-1\right)  !\dbinom
{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}}_{\substack{=\sum
\limits_{\substack{k\in\mathbb{N}\\k\leq u+1;\ k\leq j}}\left(  k-1\right)
!\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}\\\text{(by (\ref{pf.powerseries3.fin.6}))}}}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  \ \ \ \ \ \ \ \ \ \ +j\sum\limits_{\substack{k\in\mathbb{N}\\k\leq
u+1;\ k\leq j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}%
}\underbrace{\left(  k!\dbinom{u}{k}\dbinom{j}{k}+j\left(  k-1\right)
!\dbinom{u}{k-1}\dbinom{j-1}{k-1}\right)  }_{\substack{=k!\dbinom{u+1}%
{k}\dbinom{j}{k}\\\text{(by (\ref{pf.powerseries3.fin.binom}))}}}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}%
}k!\dbinom{u+1}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\end{align*}


Now, forget that we fixed $j$. We thus have shown that%
\begin{equation}
\alpha^{j}\beta^{u+1}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq
u+1;\ k\leq j}}k!\dbinom{u+1}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j} \label{pf.powerseries3.fin.9}%
\end{equation}
holds for every positive $j\in\mathbb{N}$. Since it is easy to see that
(\ref{pf.powerseries3.fin.9}) also holds for $j=0$ (the proof is similar to
our induction base above), this yields that (\ref{pf.powerseries3.fin.9})
holds for every $j\in\mathbb{N}$. In other words, (\ref{pf.powerseries3.fin.2}%
) holds for $i=u+1$. Thus, the induction step is complete. Hence, we have
proven (\ref{pf.powerseries3.fin.2}) by induction over $i$.

Since $\gamma=\left[  \alpha,\beta\right]  $, the (now proven) identity
(\ref{pf.powerseries3.fin.2}) rewrites as%
\[
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left.
\underbrace{\gamma}_{=\left[  \alpha,\beta\right]  }\right.  ^{k}%
=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}%
{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}.
\]
Proposition \ref{prop.powerseries3.fin} is thus proven.

\textit{Third proof of Lemma \ref{lem.powerseries3}.} By the definition of the
exponential, we have $\exp\left[  \alpha,\beta\right]  =\sum\limits_{k\in
\mathbb{N}}\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}$, $\exp\alpha
=\sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}{j!}$ and $\exp\beta
=\sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}$. Multiplying the last two
of these three equalities, we obtain%
\begin{align*}
&  \left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right) \\
&  =\left(  \sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}{j!}\right)
\cdot\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\right)
=\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}%
}{j!}\cdot\dfrac{\beta^{i}}{i!}=\sum\limits_{i\in\mathbb{N}}\sum
\limits_{j\in\mathbb{N}}\dfrac{1}{i!j!}\underbrace{\alpha^{j}\beta^{i}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha
,\beta\right]  ^{k}\\\text{(by Proposition \ref{prop.powerseries3.fin})}}}\\
&  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{1}%
{i!j!}\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom
{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}}_{=\sum
\limits_{k\in\mathbb{N}}\sum\limits_{\substack{i\in\mathbb{N};\\k\leq i}%
}\sum\limits_{\substack{j\in\mathbb{N};\\k\leq j}}}\underbrace{\dfrac{1}%
{i!j!}k!\dbinom{i}{k}\dbinom{j}{k}}_{\substack{=\dfrac{1}{\left(  i-k\right)
!\left(  j-k\right)  !k!}\\\text{(by easy computations)}}}\beta^{i-k}%
\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}\\
&  =\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{i\in\mathbb{N};\\k\leq
i}}\sum\limits_{\substack{j\in\mathbb{N};\\k\leq j}}\dfrac{1}{\left(
i-k\right)  !\left(  j-k\right)  !k!}\beta^{i-k}\alpha^{j-k}\left[
\alpha,\beta\right]  ^{k}=\sum\limits_{k\in\mathbb{N}}\sum\limits_{i\in
\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{1}{i!j!k!}\beta^{i}\alpha
^{j}\left[  \alpha,\beta\right]  ^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }i\text{ for }i-k\text{ in the second sum,}\\
\text{and we substituted }j\text{ for }j-k\text{ in the third sum}%
\end{array}
\right) \\
&  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\sum
\limits_{k\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\cdot\dfrac{\alpha^{j}}{j!}%
\cdot\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}=\underbrace{\left(
\sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\right)  }_{=\exp\beta}%
\cdot\underbrace{\left(  \sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}%
{j!}\right)  }_{=\exp\alpha}\cdot\underbrace{\left(  \sum\limits_{k\in
\mathbb{N}}\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}\right)  }%
_{=\exp\left[  \alpha,\beta\right]  }\\
&  =\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(
\exp\left[  \alpha,\beta\right]  \right)  .
\end{align*}
This proves Lemma \ref{lem.powerseries3} once again.

\subsection{\label{subsect.fockvir}Representations of $\operatorname*{Vir}$ on
$F_{\mu}$}

Let us define the "full-fledged" version of the Lie-algebraic semidirect
product, although it will not be central to what we will later do:

\begin{definition}
\label{def.semidir.lielie}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a vector space equipped with both a Lie algebra structure
and a $\mathfrak{g}$-module structure.

\textbf{(a)} Let $\rho:\mathfrak{g}\rightarrow\operatorname*{End}\mathfrak{h}$
be the map representing the action of $\mathfrak{g}$ on $\mathfrak{h}$. We say
that $\mathfrak{g}$ \textit{acts on }$\mathfrak{h}$\textit{ by derivations} if
$\rho\left(  \mathfrak{g}\right)  \subseteq\operatorname*{Der}\mathfrak{h}$,
or, equivalently, if the map%
\[
\mathfrak{h}\rightarrow\mathfrak{h},\ \ \ \ \ \ \ \ \ \ x\mapsto
a\rightharpoonup x
\]
is a derivation for every $a\in\mathfrak{g}$. (Here and in the following, the
symbol $\rightharpoonup$ means action; i. e., a term like $c\rightharpoonup h$
(with $c\in\mathfrak{g}$ and $h\in\mathfrak{h}$) means the action of $c$ on
$h$.)

\textbf{(b)} Assume that $\mathfrak{g}$ acts on $\mathfrak{h}$ by derivations.
Then, we define the \textit{semidirect product} $\mathfrak{g}\ltimes
\mathfrak{h}$ to be the Lie algebra which, as a vector space, is
$\mathfrak{g}\oplus\mathfrak{h}$, but whose Lie bracket is defined by%
\begin{align*}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]   &
=\left(  \left[  a,b\right]  ,\left[  \alpha,\beta\right]  +a\rightharpoonup
\beta-b\rightharpoonup\alpha\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }a\in\mathfrak{g}\text{, }%
\alpha\in\mathfrak{h}\text{, }b\in\mathfrak{g}\text{ and }\beta\in
\mathfrak{h}\right.  .
\end{align*}


Thus, the canonical injection $\mathfrak{g}\rightarrow\mathfrak{g}%
\ltimes\mathfrak{h},$ $a\mapsto\left(  a,0\right)  $ is a Lie algebra
homomorphism, and so is the canonical projection $\mathfrak{g}\ltimes
\mathfrak{h}\rightarrow\mathfrak{g},$ $\left(  a,\alpha\right)  \mapsto a$.
Also, the canonical injection $\mathfrak{h}\rightarrow\mathfrak{g}%
\ltimes\mathfrak{h},$ $\alpha\mapsto\left(  0,\alpha\right)  $ is a Lie
algebra homomorphism.
\end{definition}

All statements made in Definition \ref{def.semidir.lielie} (including the
tacit statement that the Lie bracket on $\mathfrak{g}\ltimes\mathfrak{h}$
defined in Definition \ref{def.semidir.lielie} satisfies antisymmetry and the
Jacobi identity) are easy to verify by computation.

\begin{remark}
If $\mathfrak{g}$ is a Lie algebra, and $\mathfrak{h}$ is an \textbf{abelian}
Lie algebra with any $\mathfrak{g}$-module structure, then $\mathfrak{g}$
automatically acts on $\mathfrak{h}$ by derivations (because any endomorphism
of the vector space $\mathfrak{h}$ is a derivation), and thus Definition
\ref{def.semidir.lielie} \textbf{(b)} defines a semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$. In this case, this semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$ coincides with the semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$ defined in Definition \ref{def.semidir}
(applied to $M=\mathfrak{h}$). However, when $\mathfrak{h}$ is not abelian,
the semidirect product $\mathfrak{g}\ltimes\mathfrak{h}$ defined in Definition
\ref{def.semidir.lielie} (in general) differs from that defined in Definition
\ref{def.semidir} (since the former depends on the Lie algebra structure on
$\mathfrak{h}$, while the latter does not). Care must therefore be taken when
speaking of semidirect products.
\end{remark}

Let us now return to considering the Witt and Heisenberg algebras.

From Lemma \ref{lem.WtoDerA}, we know a Lie algebra action of $W$ on
$\mathcal{A}$. Thus, we get a semidirect product $W\ltimes\mathcal{A}$
(defined as in Definition \ref{def.semidir.lielie}). On the other hand, recall
(from Definition \ref{def.fock}) that, for every $\mu\in\mathbb{C}$, we have a
representation $F_{\mu}$ of the Lie algebra $\mathcal{A}$ on the Fock space
$F$.

Can we extend this representation $F_{\mu}$ of $\mathcal{A}$ to a
representation of the semidirect product $W\ltimes\mathcal{A}$ ?

This question splits into two questions:

\textbf{Question 1:} Can we find linear operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ for all $n\in\mathbb{Z}$ such that $\left[  L_{n},a_{m}\right]
=-ma_{n+m}$ ? (Note that there are several abuses of notation in this
question. First, we denote the sought operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ by the same letters as the elements $L_{n}$ of $W$ because our
intuition for the $L_{n}$ is as if they would form a representation of $W$,
although we do not actually require them to form a representation of $W$ in
Question 1. Second, in the equation $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$,
we use $a_{m}$ and $a_{n+m}$ as abbreviations for $a_{m}\mid_{F_{\mu}}$ and
$a_{n+m}\mid_{F_{\mu}}$, respectively (so that this equation actually means
$\left[  L_{n},a_{m}\mid_{F_{\mu}}\right]  =-ma_{n+m}\mid_{F_{\mu}}$).)

\textbf{Question 2:} Do the operators $L_{n}:F_{\mu}\rightarrow F_{\mu}$ that
answer Question 1 also satisfy $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$? (In other words, do they really form a representation of
$W$ ?)

The answers to these questions are the following:

\textbf{Answer to Question 1:} Yes, and moreover, these operators are unique
up to adding a constant (a new constant for each operator). (The uniqueness is
rather easy to prove: If we have two families $\left(  L_{n}^{\prime}\right)
_{n\in\mathbb{Z}}$ and $\left(  L_{n}^{\prime\prime}\right)  _{n\in\mathbb{Z}%
}$ of linear maps $F_{\mu}\rightarrow F_{\mu}$ satisfying $\left[
L_{n}^{\prime},a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n}^{\prime\prime
},a_{m}\right]  =-ma_{n+m}$, then every $L_{n}^{\prime}-L_{n}^{\prime\prime}$
commutes with all $a_{m}$, and thus is constant by Dixmier's lemma.)

\textbf{Answer to Question 2:} No, but almost. Our operators $L_{n}$ satisfy
$\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}$ whenever $n+m\neq
0$, but the $n+m=0$ case requires a correction term. This correction term (as
a function of $\left(  L_{n},L_{m}\right)  $) happens to be the $2$-cocycle
$\omega$ of Theorem \ref{thm.H^2(W)}. So the $\mathcal{A}$-module $F_{\mu}$
does not extend to a $W\ltimes\mathcal{A}$-module, but extends to a
$\operatorname*{Vir}\ltimes\mathcal{A}$-module, where $\operatorname*{Vir}%
\ltimes\mathcal{A}$ is defined as follows:

\begin{proposition}
\label{prop.VirtoDerA}There is a natural homomorphism $\widetilde{\eta
}:\operatorname*{Vir}\rightarrow\operatorname*{Der}\mathcal{A}$ of Lie
algebras given by
\[
\left(  \widetilde{\eta}\left(  f\partial+\lambda K\right)  \right)  \left(
g,\alpha\right)  =\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for
all }f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{, }\lambda\in\mathbb{C}\text{ and }\alpha\in
\mathbb{C}.
\]
This homomorphism $\widetilde{\eta}$ is simply the extension of the
homomorphism $\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ (defined in
Lemma \ref{lem.WtoDerA}) to $\operatorname*{Vir}$ by means of requiring that
$\widetilde{\eta}\left(  K\right)  =0$.

This homomorphism $\widetilde{\eta}$ makes $\mathcal{A}$ a
$\operatorname*{Vir}$-module, and $\operatorname*{Vir}$ acts on $\mathcal{A}$
by derivations. Therefore, a Lie algebra $\operatorname*{Vir}\ltimes
\mathcal{A}$ is defined (according to Definition \ref{def.semidir.lielie}).
\end{proposition}

The proof of Proposition \ref{prop.VirtoDerA} is straightforward and left to
the readers.

Now we are going to prove the answers to Questions 1 and 2 formulated above.
First, we must define our operators $L_{n}$. "Formally" (in the sense of "not
caring about divergence of sums"), one could try to define $L_{n}$ by
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z} \label{def.fockvir.wrong}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$), and this would "formally" make $F_{\mu}$ into a
$W\ltimes\mathcal{A}$-module (in the sense that if the sums were not
divergent, one could manipulate them to "prove" that $\left[  L_{n}%
,a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$ for all $n$ and $m$). But the problem with this "formal"
approach is that the sum $\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}$ does not
make sense for $n=0$: it is an infinite sum, and infinitely many of its terms
yield nonzero values when applied to a given vector.\footnote{In fact, assume
that this sum would make sense for $n=0$. Thus we would have $L_{0}=\dfrac
{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{m}$. Applied to the vector $1\in
F_{0}$, this would give $L_{0}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}%
a_{-m}a_{m}1$. The terms for $m>0$ will get killed (since $a_{m}1=0$ for
$m>0$), but the terms for $m\leq0$ will survive. The sum would become
\begin{align*}
L_{0}1  &  =\dfrac{1}{2}\left(  a_{0}a_{-0}1+a_{1}a_{-1}1+a_{2}a_{-2}%
1+a_{3}a_{-3}1+...\right) \\
&  =\dfrac{1}{2}\left(  \mu^{2}1+1\dfrac{\partial}{\partial x_{1}}%
x_{1}+2\dfrac{\partial}{\partial x_{2}}x_{2}+3\dfrac{\partial}{\partial x_{3}%
}x_{3}+...\right)  =\dfrac{1}{2}\left(  \mu^{2}+1+2+3+...\right)  .
\end{align*}
Unless we interpret $1+2+3+...$ as $-\dfrac{1}{12}$ (which we are going to do
in some sense: the modified formulae further below include $-\dfrac{1}{12}$
factors), this makes no sense.} So we are not allowed to make the definition
(\ref{def.fockvir.wrong}), and we cannot rescue it just by defining a more
liberal notion of convergence. Instead, we must modify this "definition".

In order to modify it, we define the so-called \textit{normal ordering}:

\begin{definition}
\label{def.fockvir.normal}For any two integers $m$ and $n$, define the
\textit{normal ordered product }$\left.  :a_{m}a_{n}:\right.  $ in the
universal enveloping algebra $U\left(  \mathcal{A}\right)  $ by\textit{ }%
\[
\left.  :a_{m}a_{n}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]


More generally, for any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$, define the
\textit{normal ordered product }$\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}%
}:\right.  $ in the universal enveloping algebra $U\left(  \mathcal{A}\right)
$ by%
\[
\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.  \ =\left(
\begin{array}
[c]{c}%
\text{the product of the elements }a_{n_{1}}\text{, }a_{n_{2}}\text{,
}...\text{, }a_{n_{k}}\text{ of }U\left(  \mathcal{A}\right)  \text{,}\\
\text{rearranged in such a way that the subscripts are in increasing order}%
\end{array}
\right)  .
\]
(More formally, this normal ordered product $\left.  :a_{n_{1}}a_{n_{2}%
}...a_{n_{k}}:\right.  $ is defined as the product $a_{m_{1}}a_{m_{2}%
}...a_{m_{k}}$, where $\left(  m_{1},m_{2},...,m_{k}\right)  $ is the
permutation of the list $\left(  n_{1},n_{2},...,n_{k}\right)  $ satisfying
$m_{1}\leq m_{2}\leq...\leq m_{k}$.)
\end{definition}

Note that we have thus defined only normal ordered products of elements of the
form $a_{n}$ for $n\in\mathbb{Z}$. Normal ordered products of basis elements
of other Lie algebras are not always defined by the same formulas (although
sometimes they are).

\begin{remark}
\label{rmk.fockvir.normal.mn}If $m$ and $n$ are integers such that $m\neq-n$,
then $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}$. (This is because $\left[
a_{m},a_{n}\right]  =0$ in $\mathcal{A}$ when $m\neq-n$.)
\end{remark}

Normal ordered products have the property of being commutative:

\begin{remark}
\label{rmk.fockvir.normal.comm}\textbf{(a)} Any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$ satisfy $\left.  :a_{m}a_{n}:\right.  =\left.  :a_{n}%
a_{m}:\right.  $.

\textbf{(b)} Any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$ and any permutation
$\pi\in S_{k}$ satisfy $\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.
=\left.  :a_{n_{\pi\left(  1\right)  }}a_{n_{\pi\left(  2\right)  }%
}...a_{n_{\pi\left(  k\right)  }}:\right.  $.
\end{remark}

The proof of this is trivial.

By Remark \ref{rmk.fockvir.normal.mn} (and by the rather straightforward
generalization of this fact to many integers), normal ordered products are
rarely different from the usual products. But even when they are different,
they don't differ much:

\begin{remark}
\label{rmk.fockvir.normal.K}Let $m$ and $n$ be integers.

\textbf{(a)} Then, $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K$. Here, when $\mathfrak{A}$ is an assertion, we
denote by $\left[  \mathfrak{A}\right]  $ the truth value of $\mathfrak{A}$
(that is, the number $\left\{
\begin{array}
[c]{c}%
1\text{, if }\mathfrak{A}\text{ is true;}\\
0\text{, if }\mathfrak{A}\text{ is false }%
\end{array}
\right.  $).

\textbf{(b)} For any $x\in U\left(  \mathcal{A}\right)  $, we have $\left[
x,\left.  :a_{m}a_{n}:\right.  \right]  =\left[  x,a_{m}a_{n}\right]  $ (where
$\left[  \cdot,\cdot\right]  $ denotes the commutator in $U\left(
\mathcal{A}\right)  $).
\end{remark}

Note that when we denote by $\left[  \cdot,\cdot\right]  $ the commutator in
$U\left(  \mathcal{A}\right)  $, we are seemingly risking a confusion with the
notation $\left[  \cdot,\cdot\right]  $ for the Lie bracket of $\mathcal{A}$
(because we embed $\mathcal{A}$ in $U\left(  \mathcal{A}\right)  $). However,
this confusion is harmless, because the very definition of $U\left(
\mathcal{A}\right)  $ ensures that the commutator of two elements of
$\mathcal{A}$, taken in $U\left(  \mathcal{A}\right)  $, equals to their Lie
bracket in $\mathcal{A}$.

\textit{Proof of Remark \ref{rmk.fockvir.normal.K}.} \textbf{(a)} We
distinguish between three cases:

\textit{Case 1:} We have $m\neq-n$.

\textit{Case 2:} We have $m=-n$ and $m>0$.

\textit{Case 3:} We have $m=-n$ and $m\leq0$.

In Case 1, we have $m\neq-n$, so that $\delta_{m,-n}=0$ and thus%
\[
a_{m}a_{n}+n\left[  m>0\right]  \underbrace{\delta_{m,-n}}_{=0}K=a_{m}%
a_{n}=\left.  :a_{m}a_{n}:\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark
\ref{rmk.fockvir.normal.mn})}\right)  .
\]
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 1.

In Case 2, we have $m=-n$ and $m>0$, so that $m>n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{n}a_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m>n\right) \\
&  =a_{m}a_{n}+\underbrace{\left[  a_{n},a_{m}\right]  }_{=n\delta
_{n,-m}K=n1\delta_{m,-n}K}=a_{m}a_{n}+n\underbrace{1}_{\substack{=\left[
m>0\right]  \\\text{(since }m>0\text{)}}}\delta_{m,-n}K=a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K.
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 2.

In Case 3, we have $m=-n$ and $m\leq0$, so that $m\leq n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{m}a_{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\leq n\right) \\
&  =a_{m}a_{n}+n\left[  m>0\right]  \delta_{m,-n}K\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{we have just added a zero term}\\
\text{(since }m\leq0\text{, so that }\left[  m>0\right]  =0\text{ and thus}\\
n\left[  m>0\right]  \delta_{m,-n}K=0\text{)}%
\end{array}
\right)  .
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 3.

Thus, we have proven Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} in all
three possible cases. This completes the proof of Remark
\ref{rmk.fockvir.normal.K} \textbf{(a)}.

\textbf{(b)} We have $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(
U\left(  \mathcal{A}\right)  \right)  $ (since the center of a Lie algebra is
contained in the center of its universal enveloping algebra). Hence, $\left[
x,K\right]  =0$ for any $x\in U\left(  \mathcal{A}\right)  $.

Since $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[  m>0\right]
\delta_{m,-n}K$, we have%
\begin{align*}
\left[  x,\left.  :a_{m}a_{n}:\right.  \right]   &  =\left[  x,a_{m}%
a_{n}+n\left[  m>0\right]  \delta_{m,-n}K\right] \\
&  =\left[  x,a_{m}a_{n}\right]  +n\left[  m>0\right]  \delta_{m,-n}%
\underbrace{\left[  x,K\right]  }_{=0}=\left[  x,a_{m}a_{n}\right]  .
\end{align*}
This proves Remark \ref{rmk.fockvir.normal.K} \textbf{(b)}.

Now, the true definition of our maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ will
be the following:

\begin{definition}
\label{def.fockvir}For every $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$, define a
linear map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ by%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
\label{def.fockvir.def}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$). This sum $\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  $ is an infinite sum, but it is well-defined in the
following sense: For any vector $v\in F_{\mu}$, applying $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ to the vector $v$ gives the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  v$, which has
only finitely many nonzero addends (because of Lemma \ref{lem.fockvir.welldef}
\textbf{(c)} below) and thus has a well-defined value.
\end{definition}

Note that we have not defined the meaning of the sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ in the universal enveloping
algebra $U\left(  \mathcal{A}\right)  $ itself, but only its meaning as an
endomorphism of $F_{\mu}$. However, if we wanted, we could also define the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ as an element
of a suitable completion of the universal enveloping algebra $U\left(
\mathcal{A}\right)  $ (although not in $U\left(  \mathcal{A}\right)  $
itself). We don't really have a reason to do so here, however.

\begin{Convention}
\label{conv.fockvir.L}During the rest of Section \ref{subsect.fockvir}, we are
going to use the labels $L_{n}$ for the maps $L_{n}:F_{\mu}\rightarrow F_{\mu
}$ introduced in Definition \ref{def.fockvir}, and \textbf{not} for the
eponymous elements of the Virasoro algebra $\operatorname*{Vir}$ or of the
Witt algebra $W$, unless we explicitly refer to "the element $L_{n}$ of
$\operatorname*{Vir}$" or "the element $L_{n}$ of $W$" or something similarly unambiguous.

(While it is correct that the maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ satisfy
the same relations as the eponymous elements $L_{n}$ of $\operatorname*{Vir}$
(but not the eponymous elements $L_{n}$ of $W$), this is a nontrivial fact
that needs to be proven, and until it is proven we must avoid any confusion
between these different meanings of $L_{n}$.)
\end{Convention}

Let us first show that Definition \ref{def.fockvir} makes sense:

\begin{lemma}
\label{lem.fockvir.welldef}Let $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$. Let
$v\in F_{\mu}$. Then:

\textbf{(a)} If $m\in\mathbb{Z}$ is sufficiently high, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(b)} If $m\in\mathbb{Z}$ is sufficiently low, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(c)} All but finitely many $m\in\mathbb{Z}$ satisfy $\left.
:a_{-m}a_{n+m}:\right.  v=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.fockvir.welldef}.} \textbf{(a)} Since $v\in
F_{\mu}\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $, the vector $v$ is
a polynomial in infinitely many variables. Since every polynomial contains
only finitely many variables, there exists an integer $N\in\mathbb{N}$ such
that no variable $x_{r}$ with $r>N$ occurs in $v$. Consider this $N$. Then,
\begin{equation}
\dfrac{\partial}{\partial x_{r}}v=0\ \ \ \ \ \ \ \ \ \ \text{for every integer
}r>N. \label{pf.fockvir.welldef.1}%
\end{equation}


Now, let $m\geq\max\left\{  -n+N+1,-\dfrac{1}{2}n\right\}  $. Then,
$m\geq-n+N+1$ and $m\geq-\dfrac{1}{2}n$.

Since $m\geq-\dfrac{1}{2}n$, we have $2m\geq-n$, so that $-m\leq n+m$.

From $m\geq-n+N+1$, we get $n+m\geq N+1$, so that $n+m>0$. Hence, $a_{n+m}%
\mid_{F_{\mu}}=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}$, so
that $a_{n+m}v=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}v$. Since
$\dfrac{\partial}{\partial x_{n+m}}v=0$ (by (\ref{pf.fockvir.welldef.1}),
applied to $r=n+m$ (since $n+m\geq N+1>N$)), we thus have $a_{n+m}v=0$.

By Definition \ref{def.fockvir.normal}, we have
\[
\left.  :a_{-m}a_{n+m}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{-m}a_{n+m},\ \ \ \ \ \ \ \ \ \ \text{if }-m\leq n+m;\\
a_{n+m}a_{-m},\ \ \ \ \ \ \ \ \ \ \text{if }-m>n+m
\end{array}
\right.  .
\]
Since $-m\leq n+m$, this rewrites as $\left.  :a_{-m}a_{n+m}:\right.
=a_{-m}a_{n+m}$. Thus, $\left.  :a_{-m}a_{n+m}:\right.  v=a_{-m}%
\underbrace{a_{n+m}v}_{=0}=0$, and Lemma \ref{lem.fockvir.welldef}
\textbf{(a)} is proven.

\textbf{(b)} Applying Lemma \ref{lem.fockvir.welldef} \textbf{(a)} to $-n-m$
instead of $m$, we see that, if $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.  v=0$.
Since%
\[
\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.
=\left.  :a_{n+m}a_{-m}:\right.  =\left.  :a_{-m}a_{n+m}:\right.
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.fockvir.normal.comm}
\textbf{(a)}}\right)  ,
\]
this rewrites as follows: If $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-m}a_{n+m}:\right.  v=0$. This proves Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\textbf{(c)} Lemma \ref{lem.fockvir.welldef} \textbf{(c)} follows immediately
by combining Lemma \ref{lem.fockvir.welldef} \textbf{(a)} and Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\begin{remark}
\label{rmk.fockvir.explicit}\textbf{(a)} If $n\neq0$, then the operator
$L_{n}$ defined in Definition \ref{def.fockvir} can be rewritten as%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}.
\]
In other words, for $n\neq0$, our old definition (\ref{def.fockvir.wrong}) of
$L_{n}$ makes sense and is equivalent to the new definition (Definition
\ref{def.fockvir}).

\textbf{(b)} But when $n=0$, the formula (\ref{def.fockvir.wrong}) is devoid
of sense, whereas Definition \ref{def.fockvir} is legit. However, we can
rewrite the definition of $L_{0}$ without using normal ordered products:
Namely, we have%
\[
L_{0}=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}}{2}=\sum\limits_{m>0}%
a_{-m}a_{m}+\dfrac{\mu^{2}}{2}.
\]


\textbf{(c)} Let us grade the space $F_{\mu}$ as in Definition
\ref{def.fock.grad}. (Recall that this is the grading which gives every
variable $x_{i}$ the degree $-i$ and makes $F_{\mu}=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ into a graded $\mathbb{C}$-algebra. This is
\textbf{not} the modified grading that we gave to the space $F_{\mu}$ in
Remark \ref{rmk.fockgrad}.) Let $d\in\mathbb{N}$. Then, every homogeneous
polynomial $f\in F_{\mu}$ of degree $d$ (with respect to this grading)
satisfies $L_{0}f=\left(  \dfrac{\mu^{2}}{2}-d\right)  f$.

\textbf{(d)} Consider the grading on $F_{\mu}$ defined in part \textbf{(c)}.
For every $n\in\mathbb{Z}$, the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ is
homogeneous of degree $n$.
\end{remark}

\textit{Proof of Remark \ref{rmk.fockvir.explicit}.} \textbf{(a)} Let $n\neq
0$. Then, every $m\in\mathbb{Z}$ satisfies $-m\neq-\left(  n+m\right)  $ and
thus $\left.  :a_{-m}a_{n+m}:\right.  =a_{-m}a_{n+m}$ (by Remark
\ref{rmk.fockvir.normal.mn}, applied to $-m$ and $n+m$ instead of $m$ and
$n$). Hence, the formula $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  $ (which is how we defined $L_{n}$) rewrites
as $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}$. This proves
Remark \ref{rmk.fockvir.explicit} \textbf{(a)}.

\textbf{(b)} By the definition of $L_{0}$ (in Definition \ref{def.fockvir}),
we have%
\begin{align*}
L_{0}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{0+m}:\right.  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{m}:\right. \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m<0}\underbrace{\left.  :a_{-m}%
a_{m}:\right.  }_{\substack{=a_{m}a_{-m}\\\text{(by the definition of }\left.
:a_{-m}a_{m}:\right.  \\\text{(since }m<0\text{ and thus }-m>m\text{))}%
}}+\underbrace{\left.  :a_{-0}a_{0}:\right.  }_{\substack{=\left.  :a_{0}%
a_{0}:\right.  =a_{0}a_{0}\\\text{(by the definition of }\left.  :a_{0}%
a_{0}:\right.  \\\text{(since }0\leq0\text{))}}}+\sum\limits_{m>0}%
\underbrace{\left.  :a_{-m}a_{m}:\right.  }_{\substack{=a_{-m}a_{m}\\\text{(by
the definition of }\left.  :a_{-m}a_{m}:\right.  \\\text{(since }m>0\text{ and
thus }-m\leq m\text{))}}}\right) \\
&  =\dfrac{1}{2}\left(  \underbrace{\sum\limits_{m<0}a_{m}a_{-m}%
}_{\substack{=\sum\limits_{m>0}a_{-m}a_{m}\\\text{(here, we substituted
}m\text{ for }-m\text{ in the sum)}}}+\underbrace{a_{0}a_{0}}_{=a_{0}^{2}%
}+\sum\limits_{m>0}a_{-m}a_{m}\right) \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m>0}a_{-m}a_{m}+a_{0}^{2}+\sum
\limits_{m>0}a_{-m}a_{m}\right)  =\dfrac{1}{2}\left(  2\sum\limits_{m>0}%
a_{-m}a_{m}+a_{0}^{2}\right)  =\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}%
}{2}\\
&  =\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{\mu^{2}}{2}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }a_{0}\text{ acts as multiplication with }\mu\text{ on }F_{\mu
}\right)
\end{align*}
on $F_{\mu}$. This proves Remark \ref{rmk.fockvir.explicit} \textbf{(b)}.

\textbf{(c)} We must prove the equation $L_{0}f=\left(  \dfrac{\mu^{2}}%
{2}-d\right)  f$ for every homogeneous polynomial $f\in F_{\mu}$ of degree
$d$. Since this equation is linear in $f$, it is clearly enough to prove this
for the case of $f$ being a monomial\footnote{Here, "monomial" means "monomial
without coefficient".} of degree $d$. So let $f$ be a monomial of degree $d$.
Then, $f$ can be written in the form $f=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}%
}x_{3}^{\alpha_{3}}...$ for a sequence $\left(  \alpha_{1},\alpha_{2}%
,\alpha_{3},...\right)  $ of nonnegative integers such that $\sum
\limits_{m>0}\left(  -m\right)  \alpha_{m}=d$ (the $-m$ coefficient comes from
$\deg\left(  x_{m}\right)  =-m$) and such that all but finitely many
$i\in\left\{  1,2,3,...\right\}  $ satisfy $\alpha_{i}=0$. Consider this
sequence. Clearly, $\sum\limits_{m>0}\left(  -m\right)  \alpha_{m}=d$ yields
$\sum\limits_{m>0}m\alpha_{m}=-d$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(b)}, we have $L_{0}%
=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{\mu^{2}}{2}$. Since $a_{m}=m\dfrac
{\partial}{\partial x_{m}}$ and $a_{-m}=x_{m}$ for every integer $m>0$ (by the
definition of the action of $a_{m}$ on $F_{\mu}$), this rewrites as
$L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}}+\dfrac{\mu^{2}%
}{2}$. Now, since $f=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}^{\alpha_{3}%
}...$, every $m>0$ satisfies%
\begin{align*}
x_{m}m\dfrac{\partial}{\partial x_{m}}f  &  =x_{m}m\underbrace{\dfrac
{\partial}{\partial x_{m}}\left(  x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
x_{3}^{\alpha_{3}}...\right)  }_{\substack{=\alpha_{m}x_{1}^{\alpha_{1}}%
x_{2}^{\alpha_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}%
^{\alpha_{m+1}}x_{m+2}^{\alpha_{m+2}}...\\\text{(this term should be
understood as }0\text{ if }\alpha_{m}=0\text{)}}}\\
&  =x_{m}m\alpha_{m}x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}...x_{m-1}%
^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}}x_{m+2}^{\alpha
_{m+2}}...\\
&  =m\alpha_{m}\cdot\underbrace{x_{m}\cdot x_{1}^{\alpha_{1}}x_{2}^{\alpha
_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}%
}x_{m+2}^{\alpha_{m+2}}...}_{=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}}x_{m+1}^{\alpha_{m+1}}%
x_{m+2}^{\alpha_{m+2}}...=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}%
^{\alpha_{3}}...=f}=m\alpha_{m}f.
\end{align*}
Hence,
\begin{align*}
L_{0}f  &  =\sum\limits_{m>0}\underbrace{x_{m}m\dfrac{\partial}{\partial
x_{m}}f}_{=m\alpha_{m}f}+\dfrac{\mu^{2}}{2}f\ \ \ \ \ \ \ \ \ \ \left(
\text{since }L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}%
}+\dfrac{\mu^{2}}{2}\right) \\
&  =\underbrace{\sum\limits_{m>0}m\alpha_{m}}_{=-d}f+\dfrac{\mu^{2}}%
{2}f=-df+\dfrac{\mu^{2}}{2}f=\left(  \dfrac{\mu^{2}}{2}-d\right)  f.
\end{align*}
We thus have proven the equation $L_{0}f=\left(  \dfrac{\mu^{2}}{2}-d\right)
f$ for every monomial $f$ of degree $d$. As we said above, this completes the
proof of Remark \ref{rmk.fockvir.explicit} \textbf{(c)}.

\textbf{(d)} For every $m\in\mathbb{Z}$,%
\begin{equation}
\text{the map }a_{m}:F_{\mu}\rightarrow F_{\mu}\text{ is homogeneous of degree
}m\text{.} \label{pf.fockvir.explicit.5}%
\end{equation}
(In fact, this is easily seen from the definition of how $a_{m}$ acts on
$F_{\mu}$.)

Thus, for every $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$, the map $\left.
:a_{u}a_{v}:\right.  $ is homogeneous of degree $u+v$%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$.
By (\ref{pf.fockvir.explicit.5}) (applied to $m=u$), the map $a_{u}$ is
homogeneous of degree $u$. Similarly, the map $a_{v}$ is homogeneous of degree
$v$. Thus, the map $a_{u}a_{v}$ is homogeneous of degree $u+v$. Similarly, the
map $a_{v}a_{u}$ is homogeneous of degree $v+u=u+v$.
\par
Since $\left.  :a_{u}a_{v}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{u}a_{v},\ \ \ \ \ \ \ \ \ \ \text{if }u\leq v;\\
a_{v}a_{u},\ \ \ \ \ \ \ \ \ \ \text{if }u>v
\end{array}
\right.  $ (by the definition of normal ordered products), the map $\left.
:a_{u}a_{v}:\right.  $ equals one of the maps $a_{u}a_{v}$ and $a_{v}a_{u}$.
Since both of these maps $a_{u}a_{v}$ and $a_{v}a_{u}$ are homogeneous of
degree $u+v$, this yields that $\left.  :a_{u}a_{v}:\right.  $ is homogeneous
of degree $u+v$, qed.}. Applied to $u=-m$ and $v=n+m$, this yields: For every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the map $\left.  :a_{-m}a_{n+m}%
:\right.  $ is homogeneous of degree $\left(  -m\right)  +\left(  n+m\right)
=n$. Now, the map%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\underbrace{\left.
:a_{-m}a_{n+m}:\right.  }_{\text{this map is homogeneous of degree }n}%
\]
must be homogeneous of degree $n$. This proves Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}.

Now it turns out that the operators $L_{n}$ that we have defined give a
positive answer to question \textbf{1)}:

\begin{proposition}
\label{prop.fockvir.answer1}Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and $\mu
\in\mathbb{C}$. Then, $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$ (where $L_{n}$
is defined as in Definition \ref{def.fockvir}, and $a_{\ell}$ is shorthand
notation for $a_{\ell}\mid_{F_{\mu}}$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.fockvir.answer1}.} Since%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
=\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.
\]
we have%
\begin{align}
\left[  L_{n},a_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{j\in
\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.  ,a_{m}\right]  =\dfrac{1}{2}%
\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-j}a_{n+j}%
:\right.  ,a_{m}\right]  }_{=-\left[  a_{m},\left.  :a_{-j}a_{n+j}:\right.
\right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  a_{m},\left.
:a_{-j}a_{n+j}:\right.  \right]  }_{\substack{=\left[  a_{m},a_{-j}%
a_{n+j}\right]  \\\text{(by Remark \ref{rmk.fockvir.normal.K} \textbf{(b)},
applied}\\\text{to }a_{m}\text{, }-j\text{ and }n+j\text{ instead of }x\text{,
}m\text{ and }n\text{)}}}=-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}%
}\underbrace{\left[  a_{m},a_{-j}a_{n+j}\right]  }_{=\left[  a_{m}%
,a_{-j}\right]  a_{n+j}+a_{-j}\left[  a_{m},a_{n+j}\right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  \underbrace{\left[
a_{m},a_{-j}\right]  }_{=m\delta_{m,-\left(  -j\right)  }K}a_{n+j}%
+a_{-j}\underbrace{\left[  a_{m},a_{n+j}\right]  }_{=m\delta_{m,-\left(
n+j\right)  }K}\right) \nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\underbrace{\delta
_{m,-\left(  -j\right)  }}_{=\delta_{m,j}}Ka_{n+j}+a_{-j}m\underbrace{\delta
_{m,-\left(  n+j\right)  }}_{=\delta_{-m,n+j}=\delta_{-m-n,j}}K\right)
\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\delta_{m,j}%
Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  . \label{pf.fockvir.answer1.2}%
\end{align}


But each of the two sums $\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}$
and $\sum\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K$ is
convergent\footnote{In fact, due to the factors $\delta_{m,j}$ and
$\delta_{-m-n,j}$ in the addends, it is clear that in each of these two sums,
only at most one addend can be nonzero. Concretely:%
\[
\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}=mKa_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K=a_{-\left(  -m-n\right)  }mK.
\]
}. Hence, we can split the sum $\sum\limits_{j\in\mathbb{Z}}\left(
m\delta_{m,j}Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  $ into $\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}+\sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K$. Thus, (\ref{pf.fockvir.answer1.2}) becomes%
\begin{align*}
\left[  L_{n},a_{m}\right]   &  =-\dfrac{1}{2}\left(  \underbrace{\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}}_{=mKa_{n+m}}+\underbrace{\sum
\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K}_{=a_{-\left(  -m-n\right)
}mK}\right)  =-\dfrac{1}{2}\left(  mKa_{n+m}+a_{-\left(  -m-n\right)
}mK\right) \\
&  =-\dfrac{1}{2}\left(  ma_{n+m}+a_{-\left(  -m-n\right)  }m\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }\operatorname*{id}%
\text{ on }F_{\mu}\right) \\
&  =-\dfrac{1}{2}m\left(  a_{n+m}+\underbrace{a_{-\left(  -m-n\right)  }%
}_{=a_{m+n}}\right)  =-\dfrac{1}{2}m\left(  a_{n+m}+a_{n+m}\right)  =ma_{n+m}.
\end{align*}
This proves Proposition \ref{prop.fockvir.answer1}.

Now let us check whether our operators $L_{n}$ answer Question \textbf{2)}, or
at least try to do so. We are going to make some "dirty" arguments; cleaner
ones can be found in the proof of Proposition \ref{prop.fockvir.answer2} that
we give below.

First, it is easy to see that any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
satisfy%
\[
\left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m},a_{k}\right]
=0\ \ \ \ \ \ \ \ \ \ \text{for any }k\in\mathbb{Z}%
\]
\footnote{\textit{Proof.} Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and
$k\in\mathbb{Z}$. Then,%
\begin{align*}
&  \left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}%
,a_{k}\right] \\
&  =\underbrace{\left[  \left[  L_{n},L_{m}\right]  ,a_{k}\right]
}_{\substack{=\left[  \left[  L_{n},a_{k}\right]  ,L_{m}\right]  +\left[
L_{n},\left[  L_{m},a_{k}\right]  \right]  \\\text{(by the Leibniz identity
for commutators)}}}-\left(  n-m\right)  \left[  L_{n+m},a_{k}\right] \\
&  =\left[  \underbrace{\left[  L_{n},a_{k}\right]  }_{\substack{=-ka_{n+k}%
\\\text{(by Proposition \ref{prop.fockvir.answer1},}\\\text{applied to
}k\text{ instead of }m\text{)}}},L_{m}\right]  +\left[  L_{n}%
,\underbrace{\left[  L_{m},a_{k}\right]  }_{\substack{=-ka_{m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }m\text{ and
}k\\\text{instead of }n\text{ and }m\text{)}}}\right]  -\left(  n-m\right)
\underbrace{\left[  L_{n+m},a_{k}\right]  }_{\substack{=-ka_{n+m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }n+m\text{ and
}k\\\text{instead of }n\text{ and }m\text{)}}}\\
&  =-k\underbrace{\left[  a_{n+k},L_{m}\right]  }_{=-\left[  L_{m}%
,a_{n+k}\right]  }-k\left[  L_{n},a_{m+k}\right]  +\left(  n-m\right)
ka_{n+m+k}\\
&  =k\underbrace{\left[  L_{m},a_{n+k}\right]  }_{\substack{=-\left(
n+k\right)  a_{m+n+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m\text{ and }n+k\text{ instead of }n\text{ and
}m\text{)}}}-k\underbrace{\left[  L_{n},a_{m+k}\right]  }_{\substack{=-\left(
m+k\right)  a_{n+m+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m+k\text{ instead of }m\text{)}}}+\left(  n-m\right)
ka_{n+m+k}\\
&  =-k\left(  n+k\right)  \underbrace{a_{m+n+k}}_{=a_{n+m+k}}+k\left(
m+k\right)  a_{n+m+k}+\left(  n-m\right)  ka_{n+m+k}\\
&  =-k\left(  n+k\right)  a_{n+m+k}+k\left(  m+k\right)  a_{n+m+k}+\left(
n-m\right)  ka_{n+m+k}\\
&  =\underbrace{\left(  -k\left(  n+k\right)  +k\left(  m+k\right)  +\left(
n-m\right)  k\right)  }_{=0}a_{n+m+k}=0.
\end{align*}
Qed.}. Hence, for any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the endomorphism
$\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ of $F_{\mu}$ is an
$\mathcal{A}$-module homomorphism (since $\left[  \left[  L_{n},L_{m}\right]
-\left(  n-m\right)  L_{n+m},K\right]  =0$ also holds, for obvious reasons).
Since $F_{\mu}$ is an irreducible $\mathcal{A}$-module of countable dimension,
this yields (by Lemma \ref{lem.dix}) that, for any $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, the map $\left[  L_{n},L_{m}\right]  -\left(  n-m\right)
L_{n+m}:F_{\mu}\rightarrow F_{\mu}$ is a scalar multiple of the identity. But
since this map $\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ must
also be homogeneous of degree $n+m$ (by an application of Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}), this yields that $\left[
L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=0$ whenever $n+m\neq0$
(because any homogeneous map of degree $\neq0$ which is, at the same time, a
scalar multiple of the identity, must be the $0$ map). Thus, for every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we can write%
\begin{equation}
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\gamma_{n}%
\delta_{n,-m}\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for some }\gamma
_{n}\in\mathbb{C}\text{ depending on }n\text{.} \label{pf.fockvir.answer2.1}%
\end{equation}
We can get some more information about these $\gamma_{n}$ if we consider the
Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}\cup\left(
\operatorname*{id}\right)  $\ \ \ \ \footnote{This is well-defined because (as
the reader can easily check) the family $\left(  L_{n}\right)  _{n\in
\mathbb{Z}}\cup\left(  \operatorname*{id}\right)  $ of operators on $F_{\mu}$
is linearly independent.}. (Note that, according to Convention
\ref{conv.fockvir.L}, these $L_{n}$ still denote maps from $F_{\mu}$ to
$F_{\mu}$, rather than elements of $\operatorname*{Vir}$ or $W$. Of course,
this Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}%
\cup\left(  \operatorname*{id}\right)  $ \textbf{will} turn out to be
isomorphic to $\operatorname*{Vir}$, but we have not yet proven this.) This
Lie algebra, due to the formula (\ref{pf.fockvir.answer2.1}) and to the fact
that $\operatorname*{id}$ commutes with everything, must be a $1$-dimensional
central extension of the Witt algebra. Hence, the map
\[
W\times W\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \left(  L_{n},L_{m}\right)
\mapsto\gamma_{n}\delta_{n,-m}%
\]
(where $L_{n}$ and $L_{m}$ really mean the elements $L_{n}$ and $L_{m}$ of $W$
this time) must be a $2$-cocycle on $W$. But since we know (from Theorem
\ref{thm.H^2(W)}) that every $2$-cocycle on $W$ is a scalar multiple of the
$2$-cocycle $\omega$ defined in Theorem \ref{thm.H^2(W)} modulo the
$2$-coboundaries, this yields that this $2$-cocycle is a scalar multiple of
$\omega$ modulo the $2$-coboundaries. In other words, there exist
$c\in\mathbb{C}$ and $\xi\in W^{\ast}$ such that%
\[
\gamma_{n}\delta_{n,-m}=c\omega\left(  L_{n},L_{m}\right)  +\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Since $\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}$,
this rewrites as%
\[
\gamma_{n}\delta_{n,-m}=c\dfrac{n^{3}-n}{6}\delta_{n,-m}+\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Applied to $m=-n$, this yields%
\begin{equation}
\gamma_{n}=c\dfrac{n^{3}-n}{6}+\xi\left(  \underbrace{\left[  L_{n}%
,L_{-n}\right]  }_{=2nL_{0}}\right)  =c\dfrac{n^{3}-n}{6}+2n\xi\left(
L_{0}\right)  . \label{pf.fockvir.answer2.2}%
\end{equation}


All that remains now, in order to get the values of $\left[  L_{n}%
,L_{m}\right]  -\left(  n-m\right)  L_{n+m}$, is to compute the scalars $c$
and $\xi\left(  L_{0}\right)  $. For this, we only need to compute $\gamma
_{1}$ and $\gamma_{2}$ (because this will give $2$ linear equations for $c$
and $L_{0}$). In order to do this, we will evaluate the endomorphisms $\left[
L_{1},L_{-1}\right]  -2L_{0}$ and $\left[  L_{2},L_{-2}\right]  -4L_{0}$ at
the element $1$ of $F_{\mu}$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(c)} (applied to $d=0$ and
$f=1$), we get $L_{0}1=\left(  \dfrac{\mu^{2}}{2}-0\right)  1=\dfrac{\mu^{2}%
}{2}$.

Since $L_{1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{1+m}:\right.  $, we have $L_{1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{1+m}:\right.  1=0$ (because, as it is easily seen, $\left.
:a_{-m}a_{1+m}:\right.  1=0$ for every $m\in\mathbb{Z}$). Similarly,
$L_{2}1=0$.

Since $L_{-1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{-1+m}:\right.  $, we have $L_{-1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{-1+m}:\right.  1$. It is easy to see that the only
$m\in\mathbb{Z}$ for which $\left.  :a_{-m}a_{-1+m}:\right.  1$ is nonzero are
$m=0$ and $m=1$. Hence,
\[
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-1+m}:\right.
1=\underbrace{\left.  :a_{-0}a_{-1+0}:\right.  1}_{=\left.  :a_{0}%
a_{-1}:\right.  1=a_{-1}a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}+\underbrace{\left.
:a_{-1}a_{-1+1}:\right.  1}_{=\left.  :a_{-1}a_{0}:\right.  1=a_{-1}%
a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}=\mu x_{1}+\mu x_{1}=2\mu x_{1},
\]
so that $L_{-1}1=\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-1+m}:\right.  1}_{=2\mu x_{1}}=\mu x_{1}$. Thus,%
\begin{align*}
L_{1}L_{-1}1  &  =L_{1}\mu x_{1}=\mu\underbrace{L_{1}}_{=\dfrac{1}{2}%
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{1+m}:\right.  }x_{1}=\mu
\cdot\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{1+m}:\right.  x_{1}}_{\substack{=\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}+\left.  :a_{-0}a_{1+0}:\right.
x_{1}\\\text{(in fact, it is easy to see that the only}\\m\in\mathbb{Z}\text{
for which }\left.  :a_{-m}a_{1+m}:\right.  x_{1}\neq0\text{ are }m=-1\text{
and }m=0\text{)}}}\\
&  =\mu\cdot\dfrac{1}{2}\left(  \underbrace{\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}}_{=\left.  :a_{1}a_{0}:\right.
x_{1}=a_{0}a_{1}x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}+\underbrace{\left.  :a_{-0}a_{1+0}:\right.  x_{1}}_{=\left.  :a_{0}%
a_{1}:\right.  x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}\right) \\
&  =\mu\cdot\dfrac{1}{2}\left(  \mu+\mu\right)  =\mu^{2}.
\end{align*}


A similar (but messier) computation works for $L_{2}L_{-2}1$: Since
$L_{-2}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-2+m}%
:\right.  $, we have $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-2+m}:\right.  1$. It is easy to see that the only $m\in\mathbb{Z}$
for which $\left.  :a_{-m}a_{-2+m}:\right.  1$ is nonzero are $m=0$, $m=1$ and
$m=2$. This allows us to simplify $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{-2+m}:\right.  1$ to $L_{-2}1=\mu x_{2}+\dfrac
{1}{2}x_{1}^{2}$ (the details are left to the reader). Thus,%
\[
L_{2}L_{-2}1=L_{2}\left(  \mu x_{2}+\dfrac{1}{2}x_{1}^{2}\right)  =\mu
L_{2}x_{2}+\dfrac{1}{2}L_{2}x_{1}^{2}.
\]
Straightforward computations, which I omit, show that $L_{2}x_{2}=2\mu$ and
$L_{2}x_{1}^{2}=1$. Hence,%
\[
L_{2}L_{-2}1=\mu\underbrace{L_{2}x_{2}}_{=2\mu}+\dfrac{1}{2}\underbrace{L_{2}%
x_{1}^{2}}_{=1}=2\mu^{2}+\dfrac{1}{2}.
\]


Now,%
\[
\left(  \left[  L_{1},L_{-1}\right]  -2L_{0}\right)  1=\underbrace{L_{1}%
L_{-1}1}_{=\mu^{2}}-L_{-1}\underbrace{L_{1}1}_{=0}-2\underbrace{L_{0}%
1}_{=\dfrac{\mu^{2}}{2}}=\mu^{2}-0-2\cdot\dfrac{\mu^{2}}{2}=0.
\]
Since%
\begin{align*}
\left[  L_{1},L_{-1}\right]  -2L_{0}  &  =\gamma_{1}\underbrace{\delta
_{1,-\left(  -1\right)  }}_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.fockvir.answer2.1}), applied to }n=1\text{ and }m=-1\right) \\
&  =\gamma_{1}=c\underbrace{\dfrac{1^{3}-1}{6}}_{=0}+2\cdot1\cdot\xi\left(
L_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.2}%
), applied to }n=1\right) \\
&  =0+2\cdot1\cdot\xi\left(  L_{0}\right)  =2\xi\left(  L_{0}\right)  ,
\end{align*}
this rewrites as $2\xi\left(  L_{0}\right)  \cdot1=0$, so that $\xi\left(
L_{0}\right)  =0$.

On the other hand,%
\[
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1=\underbrace{L_{2}%
L_{-2}1}_{=2\mu^{2}+\dfrac{1}{2}}-L_{-2}\underbrace{L_{2}1}_{=0}%
-4\underbrace{L_{0}1}_{=\dfrac{\mu^{2}}{2}}=\left(  2\mu^{2}+\dfrac{1}%
{2}\right)  -0-4\cdot\dfrac{\mu^{2}}{2}=\dfrac{1}{2}.
\]
Since%
\begin{align*}
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1  &  =\gamma
_{2}\underbrace{\delta_{2,-\left(  -2\right)  }}_{=1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.1}), applied to
}n=2\text{ and }m=-2\right) \\
&  =\gamma_{2}=c\underbrace{\dfrac{2^{3}-2}{6}}_{=1}+2\cdot2\cdot
\underbrace{\xi\left(  L_{0}\right)  }_{=0}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.fockvir.answer2.2}), applied to }n=2\right) \\
&  =c+0=c,
\end{align*}
this rewrites as $c=\dfrac{1}{2}$.

Due to $\xi\left(  L_{0}\right)  =0$ and $c=\dfrac{1}{2}$, we can rewrite
(\ref{pf.fockvir.answer2.2}) as
\[
\gamma_{n}=\dfrac{1}{2}\cdot\dfrac{n^{3}-n}{6}+2n0=\dfrac{n^{3}-n}{12}.
\]
Hence, (\ref{pf.fockvir.answer2.1}) becomes%
\[
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\dfrac{n^{3}-n}%
{12}\delta_{n,-m}\operatorname*{id}.
\]
We have thus proven:

\begin{proposition}
\label{prop.fockvir.answer2}For any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we
have%
\begin{equation}
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}-n}%
{12}\delta_{n,-m}\operatorname*{id} \label{prop.fockvir.answer2.form}%
\end{equation}
(where $L_{n}$ and $L_{m}$ are maps $F_{\mu}\rightarrow F_{\mu}$ as explained
in Convention \ref{conv.fockvir.L}). Thus, we can make $F_{\mu}$ a
representation of $\operatorname*{Vir}$ by letting the element $L_{n}$ of
$\operatorname*{Vir}$ act as the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$, and letting the element $C$ of $\operatorname*{Vir}$
act as $\operatorname*{id}$.
\end{proposition}

Due to Proposition \ref{prop.fockvir.answer1}, this $\operatorname*{Vir}%
$-action harmonizes with the $\mathcal{A}$-action on $F_{\mu}$:

\begin{proposition}
The $\mathcal{A}$-action on $F_{\mu}$ extends (essentially uniquely) to an
action of $\operatorname*{Vir}\ltimes\mathcal{A}$ on $F_{\mu}$ with $C$ acting
as $1$.
\end{proposition}

This is the reason why the construction of the Virasoro algebra involved the
$2$-cocycle $\dfrac{1}{2}\omega$ rather than $\omega$ (or, actually, rather
than simpler-looking $2$-cocycles like $\left(  L_{n},L_{m}\right)  \mapsto
n^{3}\delta_{n,-m}$).

Our proof of Proposition \ref{prop.fockvir.answer2} above was rather insidious
and nonconstructive: We used the Dixmier theorem to prove (what boils down to)
an algebraic identity, and later we used Theorem \ref{thm.H^2(W)} (which is
constructive but was applied in a rather unexpected way) to reduce our
computations to two concrete cases. We will now show a different, more direct
proof of Proposition \ref{prop.fockvir.answer2}:\footnote{The following proof
is a slight variation of the proof given in the Kac-Raina book (where our
Proposition \ref{prop.fockvir.answer2} is Proposition 2.3).}

\textit{Second proof of Proposition \ref{prop.fockvir.answer2}.} Let
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. By (\ref{def.fockvir.def}) (with the
index $m$ renamed as $\ell$), we have $L_{n}=\dfrac{1}{2}\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{n+\ell}:\right.  $. Hence,%
\begin{align}
\left[  L_{n},L_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{n+\ell}:\right.  ,L_{m}\right]  =\dfrac
{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-\ell
}a_{n+\ell}:\right.  ,L_{m}\right]  }_{=-\left[  a_{m},\left.  :a_{-\ell
}a_{n+\ell}:\right.  \right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  . \label{pf.fockvir.answer2.pf0}%
\end{align}


Now, let $\ell\in\mathbb{Z}$. Then, we obtain $\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  =\left[  L_{m},a_{-\ell}a_{n+\ell
}\right]  $ (more or less by applying Remark \ref{rmk.fockvir.normal.K}
\textbf{(b)} to $L_{m}$, $-\ell$ and $n+\ell$ instead of $x$, $m$ and
$n$\ \ \ \ \footnote{I am saying "more or less" because this is not completely
correct: We cannot apply Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to
$L_{m}$, $-\ell$ and $n+\ell$ instead of $x$, $m$ and $n$ (since $L_{m}$ does
not lie in $U\left(  \mathcal{A}\right)  $). However, there are two ways to
get around this obstruction:
\par
One way is to generalize Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to a
suitable completion of $U\left(  \mathcal{A}\right)  $. We will not do this
here.
\par
Another way is to notice that we can replace $U\left(  \mathcal{A}\right)  $
by $\operatorname*{End}\left(  F_{\mu}\right)  $ throughout Remark
\ref{rmk.fockvir.normal.K}. (This, of course, means that $a_{n}$ and $a_{m}$
have to be reinterpreted as endomorphisms of $F_{\mu}$ rather than elements of
$\mathcal{A}$; but since the action of $\mathcal{A}$ on $F_{\mu}$ is a Lie
algebra representation, all equalities that hold in $U\left(  \mathcal{A}%
\right)  $ remain valid in $\operatorname*{End}\left(  F_{\mu}\right)  $.) The
proof of Remark \ref{rmk.fockvir.normal.K} still works after this replacement
(except that $\left[  x,K\right]  =0$ should no longer be proven using the
argument $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(  U\left(
\mathcal{A}\right)  \right)  $, but simply follows from the fact that $K$ acts
as the identity on $F_{\mu}$). Now, after this replacement, we \textbf{can}
apply Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to $L_{m}$, $-\ell$ and
$n+\ell$ instead of $x$, $m$ and $n$, and we obtain $\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  =\left[  L_{m},a_{-\ell}a_{n+\ell
}\right]  $.}), so that%
\begin{align*}
\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right]   &  =\left[
L_{m},a_{-\ell}a_{n+\ell}\right] \\
&  =\underbrace{\left[  L_{m},a_{-\ell}\right]  }_{\substack{=-\left(
-\ell\right)  a_{m+\left(  -\ell\right)  }\\\text{(by Proposition
\ref{prop.fockvir.answer1}}\\\text{(applied to }m\text{ and }-\ell\text{
instead of }n\text{ and }m\text{))}}}a_{n+\ell}+a_{-\ell}\underbrace{\left[
L_{m},a_{n+\ell}\right]  }_{\substack{=-\left(  n+\ell\right)  a_{m+\left(
n+\ell\right)  }\\\text{(by Proposition \ref{prop.fockvir.answer1}%
}\\\text{(applied to }m\text{ and }n+\ell\text{ instead of }n\text{ and
}m\text{))}}}\\
&  =\underbrace{-\left(  -\ell\right)  }_{=\ell}\underbrace{a_{m+\left(
-\ell\right)  }}_{=a_{m-\ell}}a_{n+\ell}+\underbrace{a_{-\ell}\left(  -\left(
n+\ell\right)  a_{m+\left(  n+\ell\right)  }\right)  }_{=-\left(
n+\ell\right)  a_{-\ell}a_{m+n+\ell}}\\
&  =\ell a_{m-\ell}a_{n+\ell}-\left(  n+\ell\right)  a_{-\ell}a_{m+n+\ell}.
\end{align*}
Since $a_{m-\ell}a_{n+\ell}=\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(
n+\ell\right)  \left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}%
$\ \ \ \ \footnote{because Remark \ref{rmk.fockvir.normal.K} \textbf{(a)}
(applied to $m-\ell$ and $n+\ell$ instead of $m$ and $n$) yields
\begin{align*}
\left.  :a_{m-\ell}a_{n+\ell}:\right.   &  =a_{m-\ell}a_{n+\ell}+\left(
n+\ell\right)  \underbrace{\left[  m-\ell>0\right]  }_{=\left[  \ell<m\right]
}\underbrace{\delta_{m-\ell,-\left(  n+\ell\right)  }}_{=\delta_{m-\ell
,-n-\ell}=\delta_{m,-n}}\underbrace{K}_{\substack{=\operatorname*{id}%
\\\text{(since }K\text{ acts as }\operatorname*{id}\text{ on }F_{\mu}\text{)}%
}}\\
&  =a_{m-\ell}a_{n+\ell}+\left(  n+\ell\right)  \left[  \ell<m\right]
\delta_{m,-n}\operatorname*{id}%
\end{align*}
} and $a_{-\ell}a_{m+n+\ell}=\left.  :a_{-\ell}a_{m+n+\ell}:\right.
-\ell\left[  \ell<0\right]  \delta_{m,-n}\operatorname*{id}$%
\ \ \ \ \footnote{because Remark \ref{rmk.fockvir.normal.K} \textbf{(a)}
(applied to $\ell$ and $n+m+\ell$ instead of $m$ and $n$) yields
\begin{align*}
\left.  :a_{-\ell}a_{m+n+\ell}:\right.   &  =a_{-\ell}a_{m+n+\ell}+\left(
m+n+\ell\right)  \underbrace{\left[  -\ell>0\right]  }_{=\left[
\ell<0\right]  }\underbrace{\delta_{-\ell,-\left(  m+n+\ell\right)  }%
}_{=\delta_{-\ell,-m-n-\ell}=\delta_{m,-n}}\underbrace{K}%
_{\substack{=\operatorname*{id}\\\text{(since }K\text{ acts as }%
\operatorname*{id}\text{ on }F_{\mu}\text{)}}}\\
&  =a_{-\ell}a_{m+n+\ell}+\underbrace{\left(  m+n+\ell\right)  \left[
\ell<0\right]  }_{=\left[  \ell<0\right]  \left(  m+n+\ell\right)  }%
\delta_{m,-n}\operatorname*{id}\\
&  =a_{-\ell}a_{m+n+\ell}+\left[  \ell<0\right]  \underbrace{\left(
m+n+\ell\right)  \delta_{m,-n}}_{\substack{=\ell\delta_{m,-n}\\\text{(this can
be easily proven by treating}\\\text{the cases of }m=-n\text{ and of }%
m\neq-n\text{ separately)}}}\operatorname*{id}\\
&  =a_{-\ell}a_{m+n+\ell}+\underbrace{\left[  \ell<0\right]  \ell}%
_{=\ell\left[  \ell<0\right]  }\delta_{m,-n}\operatorname*{id}=a_{-\ell
}a_{m+n+\ell}+\ell\left[  \ell<0\right]  \delta_{m,-n}\operatorname*{id}%
\end{align*}
}, this equation rewrites as%
\begin{align}
&  \left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right] \nonumber\\
&  =\ell\underbrace{a_{m-\ell}a_{n+\ell}}_{=\left.  :a_{m-\ell}a_{n+\ell
}:\right.  -\left(  n+\ell\right)  \left[  \ell<m\right]  \delta
_{m,-n}\operatorname*{id}}-\left(  n+\ell\right)  \underbrace{a_{-\ell
}a_{m+n+\ell}}_{=\left.  :a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}}\nonumber\\
&  =\ell\left(  \left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}\right)  -\left(
n+\ell\right)  \left(  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}\right) \nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\ell\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}-\left(  n+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right.  +\left(  n+\ell\right)  \ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\underbrace{\left(
n+\ell\right)  }_{=\left(  n-m\right)  +\left(  m+\ell\right)  }\left.
:a_{-\ell}a_{m+n+\ell}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  n+\ell\right)  \ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}-\ell\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}}_{=\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\underbrace{\left(  \left(
n-m\right)  +\left(  m+\ell\right)  \right)  \left.  :a_{-\ell}a_{m+n+\ell
}:\right.  }_{=\left(  n-m\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.
+\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n-m\right)  \left.
:a_{-\ell}a_{m+n+\ell}:\right.  -\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id} \label{pf.fockvir.answer2.pf2}%
\end{align}


Now forget that we fixed $\ell$. We want to use the equality
(\ref{pf.fockvir.answer2.pf2}) in order to split the infinite sum
$\sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell
}:\right.  \right]  $ on the right hand side of (\ref{pf.fockvir.answer2.pf0})
into
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  -\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right. \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}.
\end{align*}
But before we can do this, we must check that this splitting is allowed (since
infinite sums cannot always be split: e. g., the sum $\sum\limits_{\ell
\in\mathbb{Z}}\left(  1-1\right)  $ is well-defined (and has value $0$), but
splitting it into $\sum\limits_{\ell\in\mathbb{Z}}1-\sum\limits_{\ell
\in\mathbb{Z}}1$ is not allowed). Clearly, in order to check this, it is
enough to check that the four infinite sums $\sum\limits_{\ell\in\mathbb{Z}%
}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $, $\sum\limits_{\ell\in
\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $, $\sum\limits_{\ell
\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  $
and $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}$ converge.

Before we do this, let us formalize what we mean by "converge": We consider
the product topology on the set $\left(  F_{\mu}\right)  ^{F_{\mu}}$ (the set
of all maps $F_{\mu}\rightarrow F_{\mu}$) by viewing this set as
$\prod\limits_{v\in F_{\mu}}F_{\mu}$, where each $F_{\mu}$ is endowed with the
discrete topology. With respect to this topology, a net $\left(  f_{i}\right)
_{i\in I}$ of maps $f_{i}:F_{\mu}\rightarrow F_{\mu}$ converges to a map
$f:F_{\mu}\rightarrow F_{\mu}$ if and only if%
\[
\left(
\begin{array}
[c]{c}%
\text{for every }v\in F_{\mu}\text{, the net of values }\left(  f_{i}\left(
v\right)  \right)  _{i\in I}\text{ converges to }f\left(  v\right)  \in
F_{\mu}\\
\text{with respect to the discrete topology on }F_{\mu}%
\end{array}
\right)  .
\]
Hence, with respect to this topology, an infinite sum $\sum\limits_{\ell
\in\mathbb{Z}}f_{\ell}$ of maps $f_{\ell}:F_{\mu}\rightarrow F_{\mu}$
converges if and only if%
\[
\left(  \text{for every }v\in F_{\mu}\text{, all but finitely many }\ell
\in\mathbb{Z}\text{ satisfy }f_{\ell}\left(  v\right)  =0\right)  .
\]
Hence, this is exactly the notion of convergence which we used in Definition
\ref{def.fockvir} to make sense of the infinite sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $.

Now, we are going to show that the infinite sums $\sum\limits_{\ell
\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $, $\sum
\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $,
$\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ and $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}$ converge with respect to this topology.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{m+n+\ell}:\right.  $\textit{:} For every $v\in F_{\mu}$, all but
finitely many $\ell\in\mathbb{Z}$ satisfy $\left.  :a_{-\ell}a_{m+n+\ell
}:\right.  v=0$ (by Lemma \ref{lem.fockvir.welldef} \textbf{(c)}, applied to
$m+n$ and $\ell$ instead of $n$ and $m$). Hence, the sum $\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}\left(
m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  $\textit{:} For every
$v\in F_{\mu}$, all but finitely many $\ell\in\mathbb{Z}$ satisfy $\left.
:a_{-\ell}a_{m+n+\ell}:\right.  v=0$ (by Lemma \ref{lem.fockvir.welldef}
\textbf{(c)}, applied to $m+n$ and $\ell$ instead of $n$ and $m$). Hence, for
every $v\in F_{\mu}$, all but finitely many $\ell\in\mathbb{Z}$ satisfy
$\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  =0$. Thus, the
sum $\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}%
\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $\textit{:} We know that the sum
$\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ converges. Thus, we have%
\begin{align}
\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.   &  =\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left(
m+\left(  \ell-m\right)  \right)  }_{=\ell}\left.  :\underbrace{a_{-\left(
\ell-m\right)  }}_{=a_{m-\ell}}\underbrace{a_{m+n+\left(  \ell-m\right)  }%
}_{=a_{n+\ell}}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\ell-m\text{ for
}\ell\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
. \label{pf.fockvir.answer2.pf5}%
\end{align}
Hence, the sum $\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell
}a_{n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}%
\ell\left(  n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[
\ell<m\right]  \right)  \delta_{m,-n}\operatorname*{id}$\textit{:} It is easy
to see that:

\begin{itemize}
\item Every sufficiently small $\ell\in\mathbb{Z}$ satisfies $\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}=0$.\ \ \ \ \footnote{\textit{Proof.} Every
sufficiently small $\ell\in\mathbb{Z}$ satisfies $\ell<0$ and $\ell<m$ and
thus%
\[
\ell\left(  n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]
}_{=1\text{ (since }\ell<0\text{)}}-\underbrace{\left[  \ell<m\right]
}_{=1\text{ (since }\ell<m\text{)}}\right)  \delta_{m,-n}\operatorname*{id}%
=\ell\left(  n+\ell\right)  \underbrace{\left(  1-1\right)  }_{=0}%
\delta_{m,-n}\operatorname*{id}=0.
\]
}

\item Every sufficiently high $\ell\in\mathbb{Z}$ satisfies $\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}=0$.\ \ \ \ \footnote{\textit{Proof.} Every
sufficiently high $\ell\in\mathbb{Z}$ satisfies $\ell\geq0$ and $\ell\geq m$
and thus%
\[
\ell\left(  n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]
}_{=0\text{ (since }\ell\geq0\text{)}}-\underbrace{\left[  \ell<m\right]
}_{=0\text{ (since }\ell\geq m\text{)}}\right)  \delta_{m,-n}%
\operatorname*{id}=\ell\left(  n+\ell\right)  \underbrace{\left(  0-0\right)
}_{=0}\delta_{m,-n}\operatorname*{id}=0.
\]
}
\end{itemize}

Combining these two results, we conclude that all but finitely many $\ell
\in\mathbb{Z}$ satisfy $\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}$. The sum $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}$ therefore converges.

We now know that all four sums that we care about converge, and that two of
them have the same value (by (\ref{pf.fockvir.answer2.pf5})). Let us compute
the other two of the sums:

First of all, by (\ref{def.fockvir.def}) (with the index $m$ renamed as $\ell
$), we have $L_{n}=\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{n+\ell}:\right.  $. Applying this to $m+n$ instead of $n$, we get%
\begin{equation}
L_{m+n}=\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  . \label{pf.fockvir.answer2.pf7}%
\end{equation}
This gives us the value of one of the sums we need.

It is a completely elementary computation exercise to show that
\begin{equation}
\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}=-\dfrac{n^{3}-n}{6}\delta_{m,-n}\operatorname*{id}.
\label{pf.fockvir.answer2.pf6}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.fockvir.answer2.pf6}).} We must be in one
of the following three cases:
\par
\textit{Case 1:} We have $m\neq-n$.
\par
\textit{Case 2:} We have $m=-n$ and $m\geq0$.
\par
\textit{Case 3:} We have $m=-n$ and $m<0$.
\par
First, let us consider Case 1. In this case, $m\neq-n$, so that $\delta
_{m,-n}=0$. Hence, $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)
\left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\underbrace{\delta_{m,-n}}_{=0}\operatorname*{id}=\sum\limits_{\ell
\in\mathbb{Z}}0=0$ and $-\dfrac{n^{3}-n}{6}\underbrace{\delta_{m,-n}}%
_{=0}\operatorname*{id}=0$. This shows that $\sum\limits_{\ell\in\mathbb{Z}%
}\ell\left(  n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[
\ell<m\right]  \right)  \delta_{m,-n}\operatorname*{id}=-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}$. Thus, (\ref{pf.fockvir.answer2.pf6}) is
proven in Case 1.
\par
Next, let us consider Case 2. In this case, $m=-n$ and $m\geq0$. Since $m=-n$,
we have $\delta_{m,-n}=1$. Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \underbrace{\delta_{m,-n}%
}_{=1}\operatorname*{id}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \operatorname*{id}\\
&  =\sum\limits_{\ell=-\infty}^{-1}\ell\left(  n+\ell\right)  \left(
\underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }\ell<0\leq m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=0}^{m-1}\ell\left(  n+\ell\right)
\left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since }\ell
\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }%
\ell<m\text{)}}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=m}^{\infty}\ell\left(
n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since
}\ell\geq m\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=0\text{
(since }\ell\geq m\text{)}}\right)  \operatorname*{id}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\geq0\right) \\
&  =\sum\limits_{\ell=-\infty}^{-1}\ell\left(  n+\ell\right)
\underbrace{\left(  1-1\right)  }_{=0}\operatorname*{id}+\sum\limits_{\ell
=0}^{m-1}\ell\left(  \underbrace{n}_{\substack{=-m\\\text{(since }%
m=-n\text{)}}}+\ell\right)  \underbrace{\left(  0-1\right)  }_{=-1}%
\operatorname*{id}+\sum\limits_{\ell=m}^{\infty}\ell\left(  n+\ell\right)
\underbrace{\left(  0-0\right)  }_{=0}\operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=-\infty}^{-1}0}_{=0}+\sum\limits_{\ell
=0}^{m-1}\underbrace{\ell\left(  -m+\ell\right)  \left(  -1\right)  }%
_{=m\ell-\ell^{2}}\operatorname*{id}+\underbrace{\sum\limits_{\ell=m}^{\infty
}0}_{=0}=\underbrace{\sum\limits_{\ell=0}^{m-1}\left(  m\ell-\ell^{2}\right)
}_{=m\sum\limits_{\ell=0}^{m-1}\ell-\sum\limits_{\ell=0}^{m-1}\ell^{2}%
}\operatorname*{id}\\
&  =\left(  m\underbrace{\sum\limits_{\ell=0}^{m-1}\ell}_{\substack{=\dfrac
{\left(  m-1\right)  m}{2}\\\text{(by standard formulas)}}}-\underbrace{\sum
\limits_{\ell=0}^{m-1}\ell^{2}}_{\substack{=\dfrac{\left(  m-1\right)
m\left(  2m-1\right)  }{6}\\\text{(by standard formulas)}}}\right)
\operatorname*{id}=\underbrace{\left(  m\cdot\dfrac{\left(  m-1\right)  m}%
{2}-\dfrac{\left(  m-1\right)  m\left(  2m-1\right)  }{6}\right)
}_{\substack{=\dfrac{m^{3}-m}{6}=\dfrac{\left(  -n\right)  ^{3}-\left(
-n\right)  }{6}\\\text{(since }m=-n\text{)}}}\operatorname*{id}\\
&  =\underbrace{\dfrac{\left(  -n\right)  ^{3}-\left(  -n\right)  }{6}%
}_{=-\dfrac{n^{3}-n}{6}\cdot1}\operatorname*{id}=-\dfrac{n^{3}-n}{6}%
\cdot\underbrace{1}_{=\delta_{m,-n}}\operatorname*{id}=-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}.
\end{align*}
\par
Thus, (\ref{pf.fockvir.answer2.pf6}) is proven in Case 2.
\par
Finally, let us consider Case 3. In this case, $m=-n$ and $m<0$. Since $m=-n$,
we have $\delta_{m,-n}=1$ and $-m=n$. Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \underbrace{\delta_{m,-n}%
}_{=1}\operatorname*{id}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \operatorname*{id}\\
&  =\sum\limits_{\ell=-\infty}^{m-1}\ell\left(  n+\ell\right)  \left(
\underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<m<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }\ell<m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=m}^{-1}\ell\left(  n+\ell\right)
\left(  \underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=0\text{ (since }\ell\geq m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=0}^{\infty}\ell\left(
n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since
}\ell\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=0\text{ (since
}\ell\geq0>m\text{)}}\right)  \operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }m<0\right) \\
&  =\sum\limits_{\ell=-\infty}^{m-1}\ell\left(  n+\ell\right)
\underbrace{\left(  1-1\right)  }_{=0}\operatorname*{id}+\sum\limits_{\ell
=m}^{-1}\ell\left(  n+\ell\right)  \underbrace{\left(  1-0\right)  }%
_{=1}\operatorname*{id}+\sum\limits_{\ell=0}^{\infty}\ell\left(
n+\ell\right)  \underbrace{\left(  0-0\right)  }_{=0}\operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=-\infty}^{m-1}0}_{=0}+\sum\limits_{\ell
=m}^{-1}\ell\left(  n+\ell\right)  1\operatorname*{id}+\underbrace{\sum
\limits_{\ell=0}^{\infty}0}_{=0}=\sum\limits_{\ell=m}^{-1}\ell\left(
n+\ell\right)  1\operatorname*{id}=\sum\limits_{\ell=m}^{-1}\ell\left(
n+\ell\right)  \operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=1}^{-m}}_{\substack{=\sum\limits_{\ell
=1}^{n}\\\text{(since }-m=n\text{)}}}\underbrace{\left(  -\ell\right)  \left(
n+\left(  -\ell\right)  \right)  }_{=\ell^{2}-n\ell}\operatorname*{id}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\ell\text{ for }%
-\ell\text{ in the sum}\right) \\
&  =\underbrace{\sum\limits_{\ell=1}^{n}\left(  \ell^{2}-n\ell\right)
}_{=\sum\limits_{\ell=1}^{n}\ell^{2}-n\sum\limits_{\ell=1}^{n}\ell
}\operatorname*{id}=\left(  \underbrace{\sum\limits_{\ell=1}^{n}\ell^{2}%
}_{\substack{=\dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }{6}\\\text{(by
standard formulas)}}}-n\underbrace{\sum\limits_{\ell=1}^{n}\ell}%
_{\substack{=\dfrac{n\left(  n+1\right)  }{2}\\\text{(by standard formulas)}%
}}\right)  \operatorname*{id}\\
&  =\underbrace{\left(  \dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }%
{6}-n\cdot\dfrac{n\left(  n+1\right)  }{2}\right)  }_{=-\dfrac{n^{3}-n}{6}%
}\underbrace{\operatorname*{id}}_{=1\operatorname*{id}}=-\dfrac{n^{3}-n}%
{6}\underbrace{1}_{=\delta_{m,-n}}\operatorname*{id}=-\dfrac{n^{3}-n}{6}%
\delta_{m,-n}\operatorname*{id}.
\end{align*}
\par
Thus, (\ref{pf.fockvir.answer2.pf6}) is proven in Case 3.
\par
We have therefore proven (\ref{pf.fockvir.answer2.pf6}) in each of the three
cases 1, 2 and 3. Since these three cases cover all possibilities, this
completes the proof of (\ref{pf.fockvir.answer2.pf6}).}

Now, since (\ref{pf.fockvir.answer2.pf2}) holds for every $\ell\in\mathbb{Z}$,
we have
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell
}:\right.  \right] \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\left(  \ell\left.  :a_{m-\ell}a_{n+\ell
}:\right.  -\left(  n-m\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.
-\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  +\ell\left(  n+\ell\right)
\left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)  \delta
_{m,-n}\operatorname*{id}\right) \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \underbrace{\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{m+n+\ell}:\right.  }_{\substack{=2L_{m+n}\\\text{(by
(\ref{pf.fockvir.answer2.pf7}))}}}-\underbrace{\sum\limits_{\ell\in\mathbb{Z}%
}\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  }%
_{\substack{=\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell
}:\right.  \\\text{(by (\ref{pf.fockvir.answer2.pf5}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}}_{\substack{=-\dfrac{n^{3}-n}{6}\delta
_{m,-n}\operatorname*{id}\\\text{(by (\ref{pf.fockvir.answer2.pf6}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split the sum; this was allowed, since the infinite
sums}\\
\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
\text{, }\sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell
}:\right.  \text{, }\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right. \\
\text{and }\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(
\left[  \ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}\text{ converge}%
\end{array}
\right) \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \cdot2L_{m+n}-\sum\limits_{\ell\in\mathbb{Z}}\ell\left.
:a_{m-\ell}a_{n+\ell}:\right.  -\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}\\
&  =-\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}.
\end{align*}
Hence, (\ref{pf.fockvir.answer2.pf0}) becomes%
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =-\dfrac{1}{2}\underbrace{\sum\limits_{\ell
\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right]
}_{=-\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}}\\
&  =-\dfrac{1}{2}\left(  -\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}\right) \\
&  =\left(  n-m\right)  \underbrace{L_{m+n}}_{=L_{n+m}}-\dfrac{n^{3}-n}%
{12}\underbrace{\delta_{m,-n}}_{=\delta_{n,-m}}\operatorname*{id}=\left(
n-m\right)  L_{n+m}-\dfrac{n^{3}-n}{12}\delta_{n,-m}\operatorname*{id}.
\end{align*}


This proves Proposition \ref{prop.fockvir.answer2}.

We can generalize our family $\left(  L_{n}\right)  _{n\in\mathbb{Z}}$ of
operators on $F_{\mu}$ as follows (the so-called \textit{Fairlie construction}):

\begin{theorem}
\label{thm.fockvir.hw2ex1}Let $\mu\in\mathbb{C}$ and $\lambda\in\mathbb{C}$.
We can define a linear map $\widetilde{L}_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$ as follows: For $n\neq0$, define the map $\widetilde{L}%
_{n}$ by%
\[
\widetilde{L}_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{m+n}:\right.  +i\lambda na_{n}%
\]
(where $i$ stands for the complex number $\sqrt{-1}$). Define the map
$\widetilde{L}_{0}$ by%
\[
\widetilde{L}_{0}=\dfrac{\mu^{2}}{2}+\dfrac{\lambda^{2}}{2}+\sum
\limits_{j>0}a_{-j}a_{j}.
\]
Then, this defines an action of $\operatorname*{Vir}$ on $F_{\mu}$ with
$c=1+12\lambda^{2}$ (by letting $L_{n}\in\operatorname*{Vir}$ act as the
operator $\widetilde{L}_{n}$, and by letting $C\in\operatorname*{Vir}$ acting
as $\left(  1+12\lambda^{2}\right)  \operatorname*{id}$). Moreover, it
satisfies $\left[  \widetilde{L}_{n},a_{m}\right]  =-ma_{n+m}+i\lambda
n^{2}\delta_{n,-m}\operatorname*{id}$ for all $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$.
\end{theorem}

Proving this proposition was exercise 1 in homework problem set 2. It is
rather easy now that we have proven Propositions \ref{prop.fockvir.answer1}
and \ref{prop.fockvir.answer2} and thus left to the reader.

\begin{proposition}
\label{prop.fockvir.unitary}Let $\mu\in\mathbb{R}$. Consider the
representation $F_{\mu}$ of $\mathcal{A}$. Let $\left\langle \cdot
,\cdot\right\rangle :F_{\mu}\times F_{\mu}\rightarrow\mathbb{C}$ be the unique
Hermitian form satisfying $\left\langle 1,1\right\rangle =1$ and%
\begin{equation}
\left\langle av,w\right\rangle =\left\langle v,a^{\dag}w\right\rangle
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathcal{A}\text{, }v\in F_{\mu}\text{
and }w\in F_{\mu} \label{pf.fockvir.unitary.1}%
\end{equation}
(this is the usual Hermitian form on $F_{\mu}$). Then, equipped with this
form, $F_{\mu}$ is a unitary representation of $\mathcal{A}$.
\end{proposition}

\textit{Proof.} We must prove that the form $\left\langle \cdot,\cdot
\right\rangle $ is positive definite.

Let $\overrightarrow{n}=\left(  n_{1},n_{2},n_{3},...\right)  $ and
$\overrightarrow{m}=\left(  m_{1},m_{2},m_{3},...\right)  $ be two sequences
of nonnegative integers, each of them containing only finitely many nonzero
entries. We are going to compute the value $\left\langle x_{1}^{n_{1}}%
x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...\right\rangle $. This will give us the matrix that represents the
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ with respect to the
monomial basis of $F_{\mu}$.

If $n_{1}+n_{2}+n_{3}+...\neq m_{1}+m_{2}+m_{3}+...$, then this value is
clearly zero, because the Hermitian form $\left\langle \cdot,\cdot
\right\rangle $ is of degree $0$ (as can be easily seen). Thus, we can WLOG
assume that $n_{1}+n_{2}+n_{3}+...=m_{1}+m_{2}+m_{3}+...$.

Let $k$ be a positive integer such that every $i>k$ satisfies $n_{i}=0$ and
$m_{i}=0$. (Such a $k$ clearly exists.) Then, $n_{1}+n_{2}+...+n_{k}%
=n_{1}+n_{2}+n_{3}+...$ and $m_{1}+m_{2}+...+m_{k}=m_{1}+m_{2}+m_{3}+...$.
Hence, the equality $n_{1}+n_{2}+n_{3}+...=m_{1}+m_{2}+m_{3}+...$ (which we
know to hold) rewrites as $n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}+...+m_{k}$. Now,
since every $i>k$ satisfies $n_{i}=0$ and $m_{i}=0$, we have%
\begin{align*}
&  \left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right\rangle \\
&  =\left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}},\underbrace{x_{1}%
^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}}_{\substack{=a_{-1}^{m_{1}}a_{-2}%
^{m_{2}}...a_{-k}^{m_{k}}1\\=\left(  a_{1}^{\dag}\right)  ^{m_{1}}\left(
a_{2}^{\dag}\right)  ^{m_{2}}...\left(  a_{k}^{\dag}\right)  ^{m_{k}}%
1}}\right\rangle =\left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}%
},\left(  a_{1}^{\dag}\right)  ^{m_{1}}\left(  a_{2}^{\dag}\right)  ^{m_{2}%
}...\left(  a_{k}^{\dag}\right)  ^{m_{k}}1\right\rangle \\
&  =\left\langle a_{k}^{m_{k}}a_{k-1}^{m_{k-1}}...a_{1}^{m_{1}}x_{1}^{n_{1}%
}x_{2}^{n_{2}}...x_{k}^{n_{k}},1\right\rangle \ \ \ \ \ \ \ \ \ \ \left(
\text{due to (\ref{pf.fockvir.unitary.1}), applied several times}\right) \\
&  =\left\langle \underbrace{\left(  k\dfrac{\partial}{\partial x_{k}}\right)
^{m_{k}}\left(  \left(  k-1\right)  \dfrac{\partial}{\partial x_{k-1}}\right)
^{m_{k-1}}...\left(  1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}%
x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}}_{\substack{\text{this is a
constant polynomial,}\\\text{since }n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}%
+...+m_{k}}},1\right\rangle \\
&  =\left(  k\dfrac{\partial}{\partial x_{k}}\right)  ^{m_{k}}\left(  \left(
k-1\right)  \dfrac{\partial}{\partial x_{k-1}}\right)  ^{m_{k-1}}...\left(
1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}x_{1}^{n_{1}}x_{2}^{n_{2}%
}...x_{k}^{n_{k}}\\
&  =\prod\limits_{j=1}^{k}j^{m_{j}}\cdot\underbrace{\left(  \dfrac{\partial
}{\partial x_{k}}\right)  ^{m_{k}}\left(  \dfrac{\partial}{\partial x_{k-1}%
}\right)  ^{m_{k-1}}...\left(  \dfrac{\partial}{\partial x_{1}}\right)
^{m_{1}}x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}}_{\substack{=\delta
_{\overrightarrow{n},\overrightarrow{m}}\cdot\prod\limits_{j=1}^{k}%
m_{j}!\\\text{(since }n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}+...+m_{k}\text{)}%
}}=\delta_{\overrightarrow{n},\overrightarrow{m}}\cdot\prod\limits_{j=1}%
^{k}j^{m_{j}}\prod\limits_{j=1}^{k}m_{j}!.
\end{align*}
This term is $0$ when $\overrightarrow{n}\neq\overrightarrow{m}$, and a
positive integer when $\overrightarrow{n}=\overrightarrow{m}$. Thus, the
matrix which represents the form $\left\langle \cdot,\cdot\right\rangle $ with
respect to the monomial basis of $F_{\mu}$ is diagonal with positive diagonal
entries. This form is therefore positive definite. Proposition
\ref{prop.fockvir.unitary} is proven.

\begin{corollary}
If $\mu,\lambda\in\mathbb{R}$, then the $\operatorname*{Vir}$-representation
on $F_{\mu}$ given by $\widetilde{L}_{n}$ is unitary.
\end{corollary}

\textit{Proof.} For $n\neq0$, we have%
\begin{align*}
\widetilde{L}_{n}^{\dag}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  ^{\dag}+\left(  i\lambda na_{n}\right)  ^{\dag}\\
&  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{-n-m}:\right.
-i\lambda na_{-n}=\widetilde{L}_{-n}.
\end{align*}


\begin{corollary}
The $\operatorname*{Vir}$-representation $F_{\mu}$ is completely reducible for
$\mu\in\mathbb{R}$.
\end{corollary}

Now, $L_{0}1=\dfrac{\mu^{2}+\lambda^{2}}{2}1$ and $C1=\left(  1+12\lambda
^{2}\right)  1$. Thus, the Verma module $M_{h,c}:=M_{h,c}^{+}$ of the Virasoro
algebra $\operatorname*{Vir}$ for $h=\dfrac{\mu^{2}+\lambda^{2}}{2}$ and
$c=1+12\lambda^{2}$ maps to $F_{\mu}$ with $v_{h,c}\mapsto1$.

\begin{proposition}
For Weil generic $\mu$ and $\lambda$, this is an isomorphism.
\end{proposition}

\textit{Proof.} The dimension of the degree-$n$ part of both modules is
$p\left(  n\right)  $. The map has degree $0$. Hence, if it is injective, it
is surjective. But for Weil generic $\mu$ and $\lambda$, the
$\operatorname*{Vir}$-module $M_{h,c}$ is irreducible, so the map is injective.

\begin{corollary}
For Weil generic $\mu$ and $\lambda$ in $\mathbb{R}$, the representation
$M_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

For any $\mu$ and $\lambda$ in $\mathbb{R}$, the representation $L_{\dfrac
{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

In other words, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq\dfrac{c-1}{24}$.
\end{corollary}

\subsection{Quantum fields}

For us, when we study Lie algebras, we are mainly concerned with their
elements, usually basis elements (e. g., the $a_{n}$ in $\mathcal{A}$). For
physicists, instead, certain generating functions built of these objects are
objects of primary concern, since they are closer to what they observe. They
are called \textit{quantum fields}.

Before we introduce these quantum fields, let us recall some definitions and
give a warning.

\begin{definition}
\label{def.qf.powerseries}For every vector space $B$ and symbol $z$, we make
the following definitions:

\textbf{(a)} We denote by $B\left[  z\right]  $ the vector space of all
sequences $\left(  b_{n}\right)  _{n\in\mathbb{N}}$ such that only finitely
many $n\in\mathbb{N}$ satisfy $b_{n}\neq0$. Such a sequence $\left(
b_{n}\right)  _{n\in\mathbb{N}}$ is denoted by $\sum\limits_{n\in\mathbb{N}%
}b_{n}z^{n}$. The elements of $B\left[  z\right]  $ are called
\textit{polynomials in the indeterminate }$z$ \textit{over }$B$ (even when $B$
is not a ring).

\textbf{(b)} We denote by $B\left[  \left[  z\right]  \right]  $ the vector
space of all sequences $\left(  b_{n}\right)  _{n\in\mathbb{N}}$. Such a
sequence $\left(  b_{n}\right)  _{n\in\mathbb{N}}$ is denoted by
$\sum\limits_{n\in\mathbb{N}}b_{n}z^{n}$. The elements of $B\left[  z\right]
$ are called \textit{formal power series in the indeterminate }$z$
\textit{over }$B$ (even when $B$ is not a ring).

\textbf{(c)} We denote by $B\left(  \left(  z\right)  \right)  $ the vector
space of all two-sided sequences $\left(  b_{n}\right)  _{n\in\mathbb{Z}}$
such that only finitely many negative $n\in\mathbb{Z}$ satisfy $b_{n}\neq0$.
(A \textit{two-sided sequence} means a sequence indexed by integers, not just
nonnegative integers.) Such a sequence $\left(  b_{n}\right)  _{n\in
\mathbb{Z}}$ is denoted by $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$.
Sometimes, $B\left(  \left(  z\right)  \right)  $ is also denoted by $B\left[
\left[  z,z^{-1}\right.  \right]  $. The elements of $B\left(  \left(
z\right)  \right)  $ are called \textit{formal Laurent series in the
indeterminate }$z$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(d)} We denote by $B\left[  \left[  z,z^{-1}\right]  \right]  $ the
vector space of all two-sided sequences $\left(  b_{n}\right)  _{n\in
\mathbb{Z}}$. Such a sequence $\left(  b_{n}\right)  _{n\in\mathbb{Z}}$ is
denoted by $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$.

All four of these spaces $B\left[  z\right]  $, $B\left[  \left[  z\right]
\right]  $, $B\left(  \left(  z\right)  \right)  $ and $B\left[  \left[
z,z^{-1}\right]  \right]  $ are $\mathbb{C}\left[  z\right]  $-modules in an
obvious way. Besides, $B\left[  \left[  z\right]  \right]  $ and $B\left(
\left(  z\right)  \right)  $ are $\mathbb{C}\left[  \left[  z\right]  \right]
$-modules. Also, $B\left(  \left(  z\right)  \right)  $ is a $\mathbb{C}%
\left(  \left(  z\right)  \right)  $-module.

Of course, if $B$ is a $\mathbb{C}$-algebra, then the above-defined spaces
$B\left[  z\right]  $, $B\left[  \left[  z\right]  \right]  $ and $B\left(
\left(  z\right)  \right)  $ are $\mathbb{C}$-algebras themselves (in the
obvious way), and in fact $B\left[  z\right]  $ is the algebra of polynomials
in the variable $z$ over $B$, and $B\left[  \left[  z\right]  \right]  $ is
the algebra of formal power series in the variable $z$ over $B$.

It should be noticed that $B\left[  z\right]  \cong B\otimes\mathbb{C}\left[
z\right]  $ canonically, but such isomorphisms do \textbf{not} hold for
$B\left[  \left[  z\right]  \right]  $, $B\left(  \left(  z\right)  \right)  $
and $B\left[  \left[  z,z^{-1}\right]  \right]  $ unless $B$ is finite-dimensional.
\end{definition}

Given this definition, one might expect $B\left[  \left[  z,z^{-1}\right]
\right]  $ to canonically become a $\mathbb{C}\left[  \left[  z,z^{-1}\right]
\right]  $-algebra. But this is not true even for $B=\mathbb{C}$ (because
there is no reasonable way to define a product of two elements of
$\mathbb{C}\left[  \left[  z,z^{-1}\right]  \right]  $\ \ \ \ \footnote{If we
would try the natural way, we would get nonsense results. For instance, if we
tried to compute the coefficient of $\left(  \sum\limits_{n\in\mathbb{Z}%
}1z^{n}\right)  \cdot\left(  \sum\limits_{n\in\mathbb{Z}}1z^{n}\right)  $
before $z^{0}$, we would get $\sum\limits_{\substack{\left(  n,m\right)
\in\mathbb{Z}^{2};\\n+m=0}}1\cdot1$, which is not a convergent series.}). This
also answers why $B\left[  \left[  z,z^{-1}\right]  \right]  $ does not become
a ring when $B$ is a $\mathbb{C}$-algebra. Nor is $B\left[  \left[
z,z^{-1}\right]  \right]  $, in general, a $B\left[  \left[  z\right]
\right]  $-module.

Now, what are quantum fields?

For example, in $\mathcal{A}$, let us set $a\left(  z\right)  =\sum
\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$, where $z$ is a formal variable. This
sum $\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is a formal sum which is
infinite in both directions, so it is not an element of any of the rings
$U\left(  \mathcal{A}\right)  \left[  \left[  z\right]  \right]  $ or
$U\left(  \mathcal{A}\right)  \left[  \left[  z,z^{-1}\right.  \right]  $, but
only an element of $U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  $.

As we said, the vector space $U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  $ is \textbf{not} a ring (even though $U\left(
\mathcal{A}\right)  $ is a $\mathbb{C}$-algebra), so we cannot multiply two
"sums" like $a\left(  z\right)  $ in general. \textbf{However}, in the
following, we are going to learn about several things that we \textbf{can} do
with such "sums". One first thing that we notice about our concrete "sum"
$a\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is that if we
apply $a\left(  z\right)  $ to some vector $v$ in $F_{\mu}$ (by evaluating the
term $\left(  a\left(  z\right)  \right)  v$ componentwise\footnote{By
"evaluating" a term like $\left(  a\left(  z\right)  \right)  v$ at a vector
$v$ "componentwise", we mean evaluating $\sum\limits_{n\in\mathbb{Z}}\left(
a_{n}z^{-n-1}\right)  \left(  v\right)  $. Here, the variable $z$ is decreed
to commute with everything else, so that $\left(  a_{n}z^{-n-1}\right)
\left(  v\right)  $ means $z^{-n-1}a_{n}v$.}), then we get a sum
$\sum\limits_{n\in\mathbb{Z}}z^{-n-1}a_{n}v$ which is infinite in one
direction only (namely, in the direction $n\rightarrow-\infty$). So this sum
$\sum\limits_{n\in\mathbb{Z}}z^{-n-1}a_{n}v$ evaluates to an element of
$F_{\mu}\left[  \left[  z,z^{-1}\right.  \right]  $. As a consequence,
$a\left(  z\right)  $ "acts" on $F_{\mu}$. I am saying "acts" in quotation
marks, since this "action" is not a map $F_{\mu}\rightarrow F_{\mu}$ but a map
$F_{\mu}\rightarrow F_{\mu}\left[  \left[  z,z^{-1}\right.  \right]  $, and
since $a\left(  z\right)  $ does not lie in a ring (as I said, $U\left(
\mathcal{A}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $ is
\textbf{not} a ring).

Physicists call $a\left(  z\right)  $ a \textit{quantum field} (more
precisely, a free bosonic field).

While we cannot take the square $\left(  a\left(  z\right)  \right)  ^{2}$ of
our "sum" $a\left(  z\right)  $ (since $U\left(  \mathcal{A}\right)  \left[
\left[  z,z^{-1}\right]  \right]  $ is not a ring), we can multiply two sums
"with different variables"; e. g., we can multiply $a\left(  z\right)  $ and
$a\left(  w\right)  $, where $z$ and $w$ are two distinct formal variables.
The product $a\left(  z\right)  a\left(  w\right)  $ is defined as the formal
sum $\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}a_{n}a_{m}%
z^{-n-1}z^{-m-1}\in U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  $. Note
that elements of $U\left(  \mathcal{A}\right)  \left[  \left[  z,z^{-1}%
\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  $ are two-sided
sequences of two-sided sequences of elements of $U\left(  \mathcal{A}\right)
$; of course, we can interpret them as maps $\mathbb{Z}^{2}\rightarrow
U\left(  \mathcal{A}\right)  $.

It is easy to see that $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$. This identity, in the
first place, holds on the level of formal sums (where $\sum\limits_{n\in
\mathbb{Z}}nz^{-n-1}w^{n-1}$ is a shorthand notation for a particular sequence
of sequences: namely, the one whose $j$-th element is the sequence whose
$i$-th element is $\delta_{i+j+2,0}\left(  j+1\right)  $), but if we evaluate
it on an element $v$ of $F_{\mu}$, then we get an identity $\left[  a\left(
z\right)  ,a\left(  w\right)  \right]  v=\sum\limits_{n\in\mathbb{Z}}%
nz^{-n-1}w^{n-1}v$ which holds in the space $F_{\mu}\left(  \left(  z\right)
\right)  \left(  \left(  w\right)  \right)  $.

We can obtain the "series" $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$ by differentiating a
more basic "series":%
\[
\delta\left(  w-z\right)  :=\sum_{n\in\mathbb{Z}}z^{-n-1}w^{n}.
\]
This, again, is a formal series infinite in both directions. Why do we call it
$\delta\left(  w-z\right)  $ ? Because in analysis, the delta-"function"
(actually a distribution) satisfies the formula $\int\delta\left(  x-y\right)
f\left(  y\right)  dy=f\left(  x\right)  $ for every function $f$, whereas our
series $\delta\left(  w-z\right)  $ satisfies a remarkably similar
property\footnote{Namely, if we define the "formal residue" $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}q\left(  z\right)  dz$ of an
element $q\left(  z\right)  \in B\left(  \left(  z\right)  \right)  $ (for $B$
being some vector space) to be the coefficient of $q\left(  z\right)  $ before
$z^{-1}$, then every $f=\sum\limits_{n\in\mathbb{Z}}f_{n}z^{n}$ (with
$f_{n}\in B$) satisfies $\dfrac{1}{2\pi i}\oint\limits_{\left\vert
z\right\vert =1}z^{-n-1}f\left(  z\right)  dz=f_{n}$, and thus $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}\delta\left(  w-z\right)  f\left(
z\right)  dz=f\left(  w\right)  $.}. And now, $\left[  a\left(  z\right)
,a\left(  w\right)  \right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$
becomes $\left[  a\left(  z\right)  ,a\left(  w\right)  \right]  =\partial
_{w}\delta\left(  w-z\right)  =:\delta^{\prime}\left(  w-z\right)  $.

Something more interesting comes out for the Witt algebra: Set $T\left(
z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$\textbf{ in the Witt
algebra}. Then, we have%
\begin{align*}
&  \left[  T\left(  z\right)  ,T\left(  w\right)  \right] \\
&  =\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left(  n-m\right)
L_{n+m}z^{-n-2}w^{-m-2}=\sum\limits_{\left(  k,m\right)  \in\mathbb{Z}^{2}%
}L_{k}\underbrace{\left(  k-2m\right)  }_{=\left(  k+2\right)  +2\left(
-m-1\right)  }z^{m-k-2}w^{-m-2}\\
&  =\underbrace{\left(  \sum\limits_{k\in\mathbb{Z}}L_{k}\left(  k+2\right)
z^{-k-3}\right)  }_{=-T^{\prime}\left(  z\right)  }\underbrace{\left(
\sum\limits_{m\in\mathbb{Z}}z^{m+1}w^{-m-2}\right)  }_{=\delta\left(
w-z\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +2\underbrace{\left(  \sum\limits_{k\in\mathbb{Z}}%
L_{k}z^{-k-2}\right)  }_{=T\left(  z\right)  }\underbrace{\left(
\sum\limits_{m\in\mathbb{Z}}\left(  -m-1\right)  z^{m}w^{-m-2}\right)
}_{=\delta^{\prime}\left(  w-z\right)  }\\
&  =-T^{\prime}\left(  z\right)  \delta\left(  w-z\right)  +2T\left(
z\right)  \delta^{\prime}\left(  w-z\right)  .
\end{align*}
Note that this formula uniquely determines the Lie bracket of the Witt
algebra. This is how physicists would define the Witt algebra.

Now, let us set $T\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}%
z^{-n-2}$\textbf{ in the Virasoro algebra}. (This power series $T$ looks
exactly like the one before, but note that the $L_{n}$ now mean elements of
the Virasoro algebra rather than the Witt algebra.) Then, our previous
computation of $\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  $ must
be modified by adding a term of $\sum\limits_{n\in\mathbb{Z}}\dfrac{n^{3}%
-n}{12}Cz^{-n-2}w^{n-2}=\dfrac{C}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  $. So we get%
\[
\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  =-T^{\prime}\left(
z\right)  \delta\left(  w-z\right)  +2T\left(  z\right)  \delta^{\prime
}\left(  w-z\right)  +\dfrac{C}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  .
\]


\textbf{Exercise:} Check that, if we interpret $L_{n}$ and $a_{m}$ as the
actions of $L_{n}\in\operatorname*{Vir}$ and $a_{m}\in\mathcal{A}$ on the
$\operatorname*{Vir}\ltimes\mathcal{A}$-module $F_{\mu}$, then the following
identity between maps $F_{\mu}\rightarrow F_{\mu}\left(  \left(  z\right)
\right)  \left(  \left(  w\right)  \right)  $ holds:
\[
\left[  T\left(  z\right)  ,a\left(  w\right)  \right]  =a\left(  z\right)
\delta^{\prime}\left(  w-z\right)  .
\]


Recall%
\[
:a_{m}a_{n}:\ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
So we can reasonably define the "normal ordered" product $\left.  :a\left(
z\right)  a\left(  w\right)  :\right.  $ to be
\[
\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left.  :a_{n}%
a_{m}:\right.  z^{-n-1}w^{-m-1}\in U\left(  \mathcal{A}\right)  \left[
\left[  z,z^{-1}\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  .
\]
This definition of $\left.  :a\left(  z\right)  a\left(  w\right)  :\right.  $
is equivalent to the definition given in Problem 2 of Problem Set 3.

That $\left.  :a\left(  z\right)  a\left(  w\right)  :\right.  $ is
well-defined is not a surprise: the variables $z$ and $w$ are distinct, so
there are no terms to collect in the sum $\sum\limits_{\left(  n,m\right)
\in\mathbb{Z}^{2}}\left.  :a_{n}a_{m}:\right.  z^{-n-1}w^{-m-1}$, and thus
there is no danger of obtaining an infinite sum which makes no sense (like
what we would get if we would try to define $a\left(  z\right)  ^{2}%
$).\ \ \ \ \footnote{For the same reason, the product $a\left(  z\right)
a\left(  w\right)  $ (without normal ordering) is well-defined.} But it is
more interesting that (although we cannot define $a\left(  z\right)  ^{2}$) we
can define a "normal ordered" square $\left.  :a\left(  z\right)
^{2}:\right.  $ (or, what is the same, $\left.  :a\left(  z\right)  a\left(
z\right)  :\right.  $), although it will not be an element of $U\left(
\mathcal{A}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $ but rather of
a suitable completion. We are not going to do elaborate on how to choose this
completion here; but for us it will be enough to notice that, if we
reinterpret the $a_{n}$ as endomorphisms of $F_{\mu}$ (using the action of
$\mathcal{A}$ on $F_{\mu}$) rather than elements of $U\left(  \mathcal{A}%
\right)  $, then the "normal ordered" square $\left.  :a\left(  z\right)
^{2}:\right.  $ is a well-defined element of $\left(  \operatorname*{End}%
F_{\mu}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $. Namely:%
\begin{align*}
&  \left.  :a\left(  z\right)  ^{2}:\right. \\
&  =\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left.  :a_{n}%
a_{m}:\right.  z^{-n-1}z^{-m-1}=\sum\limits_{k\in\mathbb{Z}}\left(
\sum\limits_{\substack{\left(  n,m\right)  \in\mathbb{Z}^{2};\\n+m=k}}\left.
:a_{n}a_{m}:\right.  \right)  z^{-k-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{this is how power series are always multiplied; but we don't yet}\\
\text{know that the sum }\sum\limits_{\substack{\left(  n,m\right)
\in\mathbb{Z}^{2};\\n+m=k}}\left.  :a_{n}a_{m}:\right.  \text{ makes sense for
all }k\\
\text{(although we will see in a few lines that it does)}%
\end{array}
\right) \\
&  =\sum\limits_{k\in\mathbb{Z}}\left(  \sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{k-m}:\right.  \right)  z^{-k-2}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we substituted }\left(  m,k-m\right)  \text{ for }\left(
n,m\right)  \right) \\
&  =\sum\limits_{n\in\mathbb{Z}}\left(  \sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }k\text{ by }n\text{ in the first sum,}\\
\text{and we substituted }m\text{ by }-m\text{ in the second sum}%
\end{array}
\right)  ,
\end{align*}
and the sums $\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $
are well-defined for all $n\in\mathbb{Z}$ (by Lemma \ref{lem.fockvir.welldef}
\textbf{(c)}). We can simplify this result if we also reinterpret the
$L_{n}\in\operatorname*{Vir}$ as endomorphisms of $F_{\mu}$ (using the action
of $\operatorname*{Vir}$ on $F_{\mu}$ that was introduced in Proposition
\ref{prop.fockvir.answer2}) rather than elements of $U\left(
\operatorname*{Vir}\right)  $. In fact, the "series" $T\left(  z\right)
=\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$\textbf{ }then becomes%
\begin{align*}
T\left(  z\right)   &  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}%
=\sum\limits_{n\in\mathbb{Z}}\dfrac{1}{2}\left(  \sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{def.fockvir.def})}\right) \\
&  =\dfrac{1}{2}\underbrace{\sum\limits_{n\in\mathbb{Z}}\left(  \sum
\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}%
}_{=\left.  :a\left(  z\right)  ^{2}:\right.  }=\dfrac{1}{2}\left.  :a\left(
z\right)  ^{2}:\right.  .
\end{align*}


\begin{remark}
In Definition \ref{def.fockvir.normal}, we have defined the normal ordered
product $\left.  :a_{m}a_{n}:\right.  $ in the universal enveloping algebra of
the Heisenberg algebra. This is not the only situation in which we can define
a normal ordered product, but in other situations the definition can happen to
be different. For example, in Proposition \ref{prop.ramond.rep}, we will
define a normal ordered product (on a different algebra) which will not be
commutative, and not even "super-commutative". There is no general rule to
define normal ordered products; it is done on a case-by-case basis.

\textbf{However}, the definition of the normal ordered product of two
\textbf{quantum fields} given in Problem 2 of Problem Set 3 is general, i. e.,
it is defined not only for quantum fields over $U\left(  \mathcal{A}\right)  $.
\end{remark}

\textbf{Exercise 1.} For any $\beta\in\mathbb{C}$, the formula $T\left(
z\right)  =\dfrac{1}{2}:a\left(  z\right)  ^{2}:+\beta a^{\prime}\left(
z\right)  $ defines a representation of $\operatorname*{Vir}$ on $F_{\mu}$
with $c=1-12\beta^{2}$.

\textbf{Exercise 2.} For any $\beta\in\mathbb{C}$, there is a homomorphism
$\varphi_{\beta}:\operatorname*{Vir}\rightarrow\operatorname*{Vir}%
\ltimes\mathcal{A}$ (a splitting of the projection $\operatorname*{Vir}%
\ltimes\mathcal{A}\rightarrow\operatorname*{Vir}$) given by%
\begin{align*}
\varphi_{\beta}\left(  L_{n}\right)   &  =L_{n}+\beta a_{n}%
,\ \ \ \ \ \ \ \ \ \ n\neq0;\\
\varphi_{\beta}\left(  L_{0}\right)   &  =L_{0}+\beta a_{0}+\dfrac{\beta^{2}%
}{2}K.
\end{align*}


\textbf{Exercise 3.} If we twist the action of Exercise 1 by this map, we
recover the action of problem 1 of Homework 2 for $\beta=i\lambda$.

\subsection{More on unitary representations}

\textbf{Last time:} $L_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is
unitary (for $\lambda,\mu\in\mathbb{R}$), so $L_{h,c}$ is unitary if $c\geq1$
and $h\geq\dfrac{c-1}{24}$.

We can extend this as follows: $L_{0,1}^{\otimes m-1}\otimes L_{h,c}$ is
unitary and has a highest-weight vector $v_{0,1}^{\otimes m-1}\otimes v_{h,c}$
which has weight $\left(  h,c+m-1\right)  $. Hence, the representation
$L_{h,c+m-1}$ is unitary [why? use irreducibility of unitary modules and stuff].

Hence, $L_{h,c}$ is unitary if $c\geq m$ and $h\geq\dfrac{c-m}{24}$.

\begin{theorem}
In fact, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq0$.
\end{theorem}

But this is harder to show.

This is still not an only-if. For example, $L_{0,0}$ is unitary (and $1$-dimensional).

\begin{proposition}
\label{prop.Lhc.unitary.triv}If $L_{h,c}$ is unitary, then $h\geq0$ and
$c\geq0$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Lhc.unitary.triv}.} We have $\left(
L_{-n}v_{h,c},L_{-n}v_{h,c}\right)  \geq0$. But $\left(  L_{-n}v_{h,c}%
,L_{-n}v_{h,c}\right)  =\left(  \underbrace{L_{n}L_{-n}}_{=2nL_{0}%
+\dfrac{n^{3}-n}{12}C}v_{h,c},v_{h,c}\right)  =2nh+\dfrac{n^{3}-n}{12}c$. By
taking $n\rightarrow\infty$, we show $c\geq0$. By taking $n=1$, we get
$h\geq0$. This proves Proposition \ref{prop.Lhc.unitary.triv}.

\begin{definition}
Let $\delta\in\left\{  0,\dfrac{1}{2}\right\}  $. Let $C_{\delta}$ be the
$\mathbb{C}$-algebra with generators $\left\{  \psi_{j}\ \mid\ j\in
\delta+\mathbb{Z}\right\}  $ and relations%
\[
\psi_{j}\psi_{k}+\psi_{k}\psi_{j}=\delta_{k,-j}\ \ \ \ \ \ \ \ \ \ \text{for
all }j,k\in\delta+\mathbb{Z}.
\]
This $\mathbb{C}$-algebra $C_{\delta}$ is an infinite-dimensional Clifford
algebra (namely, the Clifford algebra of the free vector space with basis
$\left\{  \psi_{j}\ \mid\ j\in\delta+\mathbb{Z}\right\}  $ and bilinear form
$\left(  \psi_{j},\psi_{k}\right)  \mapsto\dfrac{1}{2}\delta_{k,-j}$). The
algebra $C_{\delta}$ is called an \textit{algebra of free fermions}. For
$\delta=0$, it is called the \textit{Ramond sector}; for $\delta=\dfrac{1}{2}$
it is called \textit{Neveu-Schwarz sector}.

Let us now construct a representation $V_{\delta}$ of $C_{\delta}$: Let
$V_{\delta}$ be the $\mathbb{C}$-algebra $\wedge\left(  \xi_{n}\ \mid
\ n\in\left(  \delta+\mathbb{Z}\right)  _{\geq0}\right)  $. For any
$i\in\delta+\mathbb{Z}$, define an operator $\dfrac{\partial}{\partial\xi_{i}%
}:V_{\delta}\rightarrow V_{\delta}$ by%
\begin{align*}
&  \dfrac{\partial}{\partial\xi_{i}}\left(  \xi_{j_{1}}\wedge\xi_{j_{2}}%
\wedge...\wedge\xi_{j_{k}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{1},j_{2},...,j_{k}\right\}
;\\
\left(  -1\right)  ^{\ell-1}\xi_{j_{1}}\wedge\xi_{j_{2}}\wedge...\wedge
\xi_{j_{\ell-1}}\wedge\xi_{j_{\ell+1}}\wedge\xi_{j_{\ell+2}}\wedge...\wedge
\xi_{j_{k}},\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{1},j_{2}%
,...,j_{k}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }j_{1}<j_{2}<...<j_{k}\text{ in
}\delta+\mathbb{Z}\right.  ,
\end{align*}
where, in the case when $i\in\left\{  j_{1},j_{2},...,j_{k}\right\}  $, we
denote by $\ell$ the element $u$ of $\left\{  1,2,...,k\right\}  $ satisfying
$j_{\ell}=u$. (Note the $\left(  -1\right)  ^{\ell-1}$ sign, which
distinguishes this "differentiation" from differentiation in the commutative
case. This is a particular case of the Koszul sign rule.)

Define an action of $V_{\delta}$ on $C_{\delta}$ by%
\begin{align*}
\psi_{-n}  &  \mapsto\xi_{n}\ \ \ \ \ \ \ \ \ \ \text{for }n<0;\\
\psi_{n}  &  \mapsto\dfrac{\partial}{\partial\xi_{n}}%
\ \ \ \ \ \ \ \ \ \ \text{for }n>0;\\
\psi_{0}  &  \mapsto\dfrac{1}{\sqrt{2}}\left(  \dfrac{\partial}{\partial
\xi_{0}}+\xi_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{this is only
relevant if }\delta=0\right)  .
\end{align*}


This indeed defines a representation of $C_{\delta}$ (exercise!). This is an
infinite-dimensional analogue of the well-known spinor representation of
Clifford algebras.
\end{definition}

From Homework Set 2 problem 2, we know:

\begin{proposition}
\label{prop.ramond.rep}Let%
\[
L_{k}=\delta_{k,0}\dfrac{1-2\delta}{16}+\dfrac{1}{2}\sum\limits_{j\in
\mathbb{Z}+\delta}j:\psi_{-j}\psi_{j+k}:,
\]
where the normal ordering is defined as follows:%
\[
:\psi_{n}\psi_{m}:\ =\ \left\{
\begin{array}
[c]{c}%
-\psi_{m}\psi_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
\psi_{n}\psi_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
Then:

\textbf{(a)} $\left[  \psi_{m},L_{k}\right]  =\left(  m+\dfrac{k}{2}\right)
\psi_{m+k}$.

\textbf{(b)} $\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}%
+\delta_{n,-m}\dfrac{m^{3}-m}{24}$ (so $c=\dfrac{1}{2}$).
\end{proposition}

Now this representation $V_{\delta}$ of $\operatorname*{Vir}$ is unitary. In
fact, consider the Hermitian form under which all monomials in $\psi_{i}$ are
orthonormal (positive definite). Then it is easy to see that $\psi_{j}^{\dag
}=\psi_{-j}$. Thus, $L_{n}^{\dag}=L_{-n}$.

But these $V_{\delta}$ are reducible, since the $L_{n}$ preserve parity:
$V_{\delta}=V_{\delta}^{+}\oplus V_{\delta}^{-}$.

\begin{theorem}
$V_{\delta}^{+}$ and $V_{\delta}^{-}$ are irreducible Virasoro modules.
\end{theorem}

We will not prove this.

What are the highest weights?

First consider the case $\delta=0$. The highest-weight vector of $V_{\delta
}^{+}$ is $1$, with weight $\left(  \dfrac{1}{16},\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{0}$, with weight $\left(  \dfrac{1}{16},\dfrac
{1}{2}\right)  $. Thus, $V_{\delta}^{+}\cong V_{\delta}^{-}$ by action of
$\psi_{0}$ (since $\psi_{0}^{2}=\dfrac{1}{2}$).

Now consider the case $\delta=\dfrac{1}{2}$. The highest-weight vector of
$V_{\delta}^{+}$ is $1$, with weight $\left(  0,\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{1/2}$, with weight $\left(  \dfrac{1}{2}%
,\dfrac{1}{2}\right)  $.

\begin{corollary}
The representation $L_{h,\dfrac{1}{2}}$ is unitary if $h=0$, $h=\dfrac{1}{16}$
or $h=\dfrac{1}{2}$. (In physics: Ising model.)
\end{corollary}

We will not prove:

\begin{proposition}
This is an only-if as well.
\end{proposition}

General answer for $c<1$: for $c=1-\dfrac{6}{\left(  m+2\right)  \left(
m+3\right)  }$ for $m\in\mathbb{N}$, there are finitely many $h$ where
$L_{h,c}$ is unitary. For other values of $c$, there are no such values.

\begin{definition}
The \textit{character }$\operatorname*{ch}\nolimits_{V}\left(  q\right)  $ of
a $\operatorname*{Vir}$-module $V$ from category $\mathcal{O}^{+}$ is
$\operatorname*{Tr}\nolimits_{V}\left(  q^{L_{0}}\right)  =\sum\left(  \dim
V_{\lambda}\right)  q^{\lambda}$ for $V_{\lambda}=$generalized eigenspace of
$L_{0}$ with eigenvalue $\lambda$.
\end{definition}

This is related to the old definition of character [how?]

What are the characters of the above modules? Since $V_{\delta}^{+}%
=\wedge\left(  \xi_{1},\xi_{2},\xi_{3},...\right)  ^{+}$, we have%
\[
\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
=q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}\right)
...=q^{1/16}\prod\limits_{n\geq1}\left(  1+q^{n}\right)
\]
(because
\begin{align*}
2\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{0}}\left(  q\right)  =q^{1/16}\left(
1+1\right)  \left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}%
\right)  ...\\
&  =2q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(
1+q^{3}\right)  ...
\end{align*}
).

Now%
\begin{align*}
\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)
+\operatorname*{ch}\nolimits_{L_{\dfrac{1}{2},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{\dfrac{1}{2}}}\left(  q\right)  =\left(
1+q^{1/2}\right)  \left(  1+q^{3/2}\right)  \left(  1+q^{5/2}\right)  ...\\
&  =\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  .
\end{align*}
Thus, $\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)  $ is
the integer part of the product $\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}%
}\left(  1+q^{n}\right)  $, and $\operatorname*{ch}\nolimits_{L_{\dfrac{1}%
{2},\dfrac{1}{2}}}\left(  q\right)  $ is the half-integer part of the product
$\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  $.

\subsection{Representations of $\mathfrak{gl}_{\infty}$}

For every $n\in\mathbb{N}$, we can define a Lie algebra $\mathfrak{gl}_{n}$ of
$n\times n$-matrices over $\mathbb{C}$. One can wonder how this can be
generalized to the "$n=\infty$ case", i. e., to infinite matrices. Obviously,
not every pair of infinite matrices has a reasonable commutator (because not
any such pair can be multiplied), but there are certain restrictions on
infinite matrices which allow us to multiply them and form their commutators.
These restrictions can be used to define various Lie algebras consisting of
infinite matrices. We will be concerned with some such Lie algebras; the first
of them is $\mathfrak{gl}_{\infty}$:

\begin{definition}
\label{def.glinf.glinf}We define $\mathfrak{gl}_{\infty}$ to be the vector
space of infinite matrices whose rows and columns are labeled by integers (not
only positive integers!) such that only finitely many entries of the matrix
are nonzero. This vector space $\mathfrak{gl}_{\infty}$ is an associative
algebra \textit{without unit} (by matrix multiplication); we can thus make
$\mathfrak{gl}_{\infty}$ into a Lie algebra by the commutator in this
associative algebra.
\end{definition}

We will study the representations of this $\mathfrak{gl}_{\infty}$. The theory
of these representations will extend the well-known (Schur-Weyl) theory of
representations of $\mathfrak{gl}_{n}$.

\begin{definition}
\label{def.glinf.V}The \textit{vector representation} $V$ of $\mathfrak{gl}%
_{\infty}$ is defined as the vector space $\mathbb{C}^{\left(  \mathbb{Z}%
\right)  }=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }%
\mid\ x_{i}\in\mathbb{C}\text{; only finitely many }x_{i}\text{ are
nonzero}\right\}  $. The Lie algebra $\mathfrak{gl}_{\infty}$ acts on the
vector representation $V$ in the obvious way: namely, for any $a\in
\mathfrak{gl}_{\infty}$ and $v\in V$, we let $a\rightharpoonup v$ be the
product of the matrix $a$ with the column vector $v$. Here, every element
$\left(  x_{i}\right)  _{i\in\mathbb{Z}}$ of $V$ is identified with the column
vector $\left(
\begin{array}
[c]{c}%
...\\
x_{-2}\\
x_{-1}\\
x_{0}\\
x_{1}\\
x_{2}\\
...
\end{array}
\right)  $.

For every $j\in\mathbb{Z}$, let $v_{j}$ be the vector $\left(  \delta
_{i,j}\right)  _{i\in\mathbb{Z}}\in V$. Then, $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ is a basis of the vector space $V$.
\end{definition}

\begin{Convention}
When we draw infinite matrices whose rows and columns are labeled by integers,
the index of the rows is supposed to increase as we go from left to right, and
the index of the columns is supposed to increase as we go from top to bottom.
\end{Convention}

\begin{remark}
In Definition \ref{def.glinf.V}, we used the following (very simple) fact: For
every $a\in\mathfrak{gl}_{\infty}$ and every $v\in V$, the product $av$ of the
matrix $a$ with the column vector $v$ is a well-defined element of $V$. This
fact can be generalized: If $a$ is an infinite matrix (whose rows and columns
are labeled by integers) such that every column of $a$ has only finitely many
nonzero entries, and $v$ is an element of $V$, then the product $av$ is a
well-defined element of $V$. However, this does \textbf{no longer} hold if we
drop the condition that every column of $a$ have only finitely many nonzero
entries. (For example, if $a$ would be the matrix whose all entries equal $1$,
then the product $av_{0}$ would \textbf{not} be an element of $V$, but rather
the element $\left(
\begin{array}
[c]{c}%
...\\
1\\
1\\
1\\
1\\
1\\
...
\end{array}
\right)  $ of the \textbf{larger} vector space $\mathbb{C}^{\mathbb{Z}%
}=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }\mid\ x_{i}%
\in\mathbb{C}\right\}  $. Besides, the product $a\left(
\begin{array}
[c]{c}%
...\\
1\\
1\\
1\\
1\\
1\\
...
\end{array}
\right)  $ would not make any sense at all, not even in $\mathbb{C}%
^{\mathbb{Z}}$.)
\end{remark}

We can consider the representation $\wedge^{i}V$ of $\mathfrak{gl}_{\infty}$
for every $i\in\mathbb{N}$. More generally, we have the so-called
\textit{Schur modules}:

\begin{definition}
If $\pi\in\operatorname*{Irr}S_{n}$, then we can define a representation
$S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ by $S_{\pi}\left(
V\right)  =\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes
n}\right)  $. This $S_{\pi}\left(  V\right)  $ is called the $\pi$\textit{-th
Schur module} of $V$.
\end{definition}

This definition mimics the well-known definition (or, more precisely, one of
the definitions) of the Schur modules of a finite-dimensional vector space.

\begin{proposition}
\label{prop.glinf.schur.irred}For every $\pi\in\operatorname*{Irr}S_{n}$, the
representation $S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ is irreducible.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.schur.irred}.} The following is
not a self-contained proof; it is just a way to reduce Proposition
\ref{prop.glinf.schur.irred} to the similar fact about finite-dimensional
vector spaces (which is a well-known fact in the representation theory of
$\mathfrak{gl}_{m}$).

For every vector subspace $W\subseteq V$, we can canonically identify $S_{\pi
}\left(  W\right)  $ with a vector subspace of $S_{\pi}\left(  V\right)  $.

For every subset $I$ of $\mathbb{Z}$, let $W_{I}$ be the subset of $V$
generated by all $v_{i}$ with $i\in I$. Clearly, whenever two subsets $I$ and
$J$ of $\mathbb{Z}$ satisfy $I\subseteq J$, we have $W_{I}\subseteq W_{J}$.
Also, whenever $I$ is a finite subset of $\mathbb{Z}$, the vector space
$W_{I}$ is finite-dimensional.

For every tensor $u\in V^{\otimes n}$, there exists a finite subset $I$ of
$\mathbb{Z}$ such that $u\in\left(  W_{I}\right)  ^{\otimes n}$%
.\ \ \ \ \footnote{\textit{Proof.} The family $\left(  v_{i_{1}}\otimes
v_{i_{2}}\otimes...\otimes v_{i_{n}}\right)  _{\left(  i_{1},i_{2}%
,...,i_{n}\right)  \in\mathbb{Z}^{n}}$ is a basis of $V^{\otimes n}$ (since
$\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ is a basis of $V$). Thus, we can
write the tensor $u\in V^{\otimes n}$ as a $\mathbb{C}$-linear combination of
finitely many tensors of the form $v_{i_{1}}\otimes v_{i_{2}}\otimes...\otimes
v_{i_{n}}$ with $\left(  i_{1},i_{2},...,i_{n}\right)  \in\mathbb{Z}^{n}$. Let
$I$ be the union of the sets $\left\{  i_{1},i_{2},...,i_{n}\right\}  $ over
all the tensors which appear in this linear combination. Since only finitely
many tensors appear in this linear combination, the set $I$ is finite. Every
tensor $v_{i_{1}}\otimes v_{i_{2}}\otimes...\otimes v_{i_{n}}$ which appears
in this linear combination satisfies $\left\{  i_{1},i_{2},...,i_{n}\right\}
\subseteq I$ (by the construction of $I$) and thus $v_{i_{1}}\otimes v_{i_{2}%
}\otimes...\otimes v_{i_{n}}\in\left(  W_{I}\right)  ^{\otimes n}$. Thus, $u$
must lie in $\left(  W_{I}\right)  ^{\otimes n}$, too (because $u$ is the
value of this linear combination). Hence, we have found a finite subset $I$ of
$\mathbb{Z}$ such that $u\in\left(  W_{I}\right)  ^{\otimes n}$. Qed.} Denote
this subset $I$ by $I\left(  u\right)  $. Thus, $u\in\left(  W_{I\left(
u\right)  }\right)  ^{\otimes n}$ for every $u\in V^{\otimes n}$.

For every $w\in S_{\pi}\left(  V\right)  $, there exists some finite subset
$I$ of $\mathbb{Z}$ such that $w\in S_{\pi}\left(  W_{I}\right)
$.\ \ \ \ \footnote{\textit{Proof.} Let $w\in S_{\pi}\left(  V\right)  $.
Then, $w\in S_{\pi}\left(  V\right)  =\operatorname*{Hom}\nolimits_{S_{n}%
}\left(  \pi,V^{\otimes n}\right)  $. But since $\pi$ is a finite-dimensional
vector space, the image $w\left(  \pi\right)  $ must be finite-dimensional.
Hence, $w\left(  \pi\right)  $ is a finite-dimensional vector subspace of
$V^{\otimes n}$. Thus, $w\left(  \pi\right)  $ is generated by some elements
$u_{1},u_{2},...,u_{k}\in V^{\otimes n}$. Let $I$ be the union $\bigcup
\limits_{j=1}^{k}I\left(  u_{j}\right)  $. Then, $I$ is finite (because for
every $j\in\left\{  1,2,...,k\right\}  $, the set $I\left(  u_{j}\right)  $ is
finite) and satisfies $I\left(  u_{j}\right)  \subseteq I$ for every
$j\in\left\{  1,2,...,k\right\}  $.
\par
Recall that every $u\in V^{\otimes n}$ satisfies $u\in\left(  W_{I\left(
u\right)  }\right)  ^{\otimes n}$. Thus, every $j\in\left\{
1,2,...,k\right\}  $ satisfies $u_{j}\in\left(  W_{I\left(  u_{j}\right)
}\right)  ^{\otimes n}\subseteq\left(  W_{I}\right)  ^{\otimes n}$ (since
$I\left(  u_{j}\right)  \subseteq I$ and thus $W_{I\left(  u_{j}\right)
}\subseteq W_{I}$). In other words, all $k$ elements $u_{1},u_{2},...,u_{k}$
lie in the vector space $\left(  W_{I}\right)  ^{\otimes n}$. Since the
elements $u_{1},u_{2},...,u_{k}$ generate the subspace $w\left(  \pi\right)
$, this yields that $w\left(  \pi\right)  \subseteq\left(  W_{I}\right)
^{\otimes n}$. Hence, the map $w:\pi\rightarrow V^{\otimes n}$ factors through
a map $\pi\rightarrow\left(  W_{I}\right)  ^{\otimes n}$. In other words,
$w\in\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes n}\right)  $
is contained in $\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,\left(
W_{I}\right)  ^{\otimes n}\right)  =S_{\pi}\left(  W_{I}\right)  $, qed.}
Denote this subset $I$ by $I\left(  w\right)  $. Thus, $w\in S_{\pi}\left(
W_{I\left(  w\right)  }\right)  $ for every $w\in S_{\pi}\left(  V\right)  $.

Let $w$ and $w^{\prime}$ be two vectors in $S_{\pi}\left(  V\right)  $ such
that $w\neq0$. We are going to prove that $w^{\prime}\in U\left(
\mathfrak{gl}_{\infty}\right)  w$. Once this is proven, it will be obvious
that $S_{\pi}\left(  V\right)  $ is irreducible, and we will be done.

There exists a finite subset $I$ of $\mathbb{Z}$ such that $w\in S_{\pi
}\left(  W_{I}\right)  $ and $w^{\prime}\in S_{\pi}\left(  W_{I}\right)
$.\ \ \ \ \footnote{\textit{Proof.} Let $I=I\left(  w\right)  \cup I\left(
w^{\prime}\right)  $. Then, $I$ is a finite subset of $\mathbb{Z}$ (since
$I\left(  w\right)  $ and $I\left(  w^{\prime}\right)  $ are finite subsets of
$\mathbb{Z}$), and $I\left(  w\right)  \subseteq I$ and $I\left(  w^{\prime
}\right)  \subseteq I$. We have $w\in S_{\pi}\left(  W_{I\left(  w\right)
}\right)  \subseteq S_{\pi}\left(  W_{I}\right)  $ (since $I\left(  w\right)
\subseteq I$ and thus $W_{I\left(  w\right)  }\subseteq W_{I}$) and similarly
$w^{\prime}\in S_{\pi}\left(  W_{I}\right)  $. Thus, there exists a finite
subset $I$ of $\mathbb{Z}$ such that $w\in S_{\pi}\left(  W_{I}\right)  $ and
$w^{\prime}\in S_{\pi}\left(  W_{I}\right)  $, qed.} Consider this $I$.

Since $I$ is finite, the vector space $W_{I}$ is finite-dimensional. Thus, by
the analogue of Proposition \ref{prop.glinf.schur.irred} for representations
of $\mathfrak{gl}_{m}$, the representation $S_{\pi}\left(  W_{I}\right)  $ of
the Lie algebra $\mathfrak{gl}\left(  W_{I}\right)  $ is irreducible. Hence,
$w^{\prime}\in U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w$.

Now, we have a canonical injective Lie algebra homomorphism $\mathfrak{gl}%
\left(  W_{I}\right)  \rightarrow\mathfrak{gl}_{\infty}$\ \ \ \ \footnote{Here
is how it is defined: For every linear map $A\in\mathfrak{gl}\left(
W_{I}\right)  $, we define a linear map $A^{\prime}\in\mathfrak{gl}\left(
V\right)  $ by setting%
\[
A^{\prime}v_{i}=\left\{
\begin{array}
[c]{c}%
Av_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\in I;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin I
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\mathbb{Z}.
\]
This linear map $A^{\prime}$ is represented (with respect to the basis
$\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ of $V$) by an infinite matrix whose
rows and columns are labeled by integers. This matrix lies in $\mathfrak{gl}%
_{\infty}$.
\par
Thus, we have assigned to every $A\in\mathfrak{gl}\left(  W_{I}\right)  $ a
matrix in $\mathfrak{gl}_{\infty}$. This defines an injective Lie algebra
homomorphism $\mathfrak{gl}\left(  W_{I}\right)  \rightarrow\mathfrak{gl}%
_{\infty}$.}. Thus, we can view $\mathfrak{gl}\left(  W_{I}\right)  $ as a Lie
subalgebra of $\mathfrak{gl}_{\infty}$ in a canonical way. Moreover, the
classical action $\mathfrak{gl}\left(  W_{I}\right)  \times S_{\pi}\left(
W_{I}\right)  \rightarrow S_{\pi}\left(  W_{I}\right)  $ of the Lie algebra
$\mathfrak{gl}\left(  W_{I}\right)  $ on the Schur module $S_{\pi}\left(
W_{I}\right)  $ can be viewed as the restriction of the action $\mathfrak{gl}%
_{\infty}\times S_{\pi}\left(  V\right)  \rightarrow S_{\pi}\left(  V\right)
$ to $\mathfrak{gl}\left(  W_{I}\right)  \times S_{\pi}\left(  W_{I}\right)
$. Hence, $U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w\subseteq
U\left(  \mathfrak{gl}_{\infty}\right)  w$. Since we know that $w^{\prime}\in
U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w$, we thus conclude
$w^{\prime}\in U\left(  \mathfrak{gl}_{\infty}\right)  w$. This completes the
proof of Proposition \ref{prop.glinf.schur.irred}.

On the other hand, we can define so-called \textit{highest-weight
representations}. Before we do so, let us make $\mathfrak{gl}_{\infty}$ into a
graded Lie algebra:

\begin{definition}
\label{def.glinf.grade}For every $i\in\mathbb{Z}$, let $\mathfrak{gl}_{\infty
}^{i}$ be the subspace of $\mathfrak{gl}_{\infty}$ which consists of matrices
which have nonzero entries only on the $i$-th diagonal. (The $i$\textit{-th
diagonal} consists of the entries in the $\left(  \alpha,\beta\right)  $-th
places with $\beta-\alpha=i$.)

Then, $\mathfrak{gl}_{\infty}=\bigoplus\limits_{i\in\mathbb{Z}}\mathfrak{gl}%
_{\infty}^{i}$, and this makes $\mathfrak{gl}_{\infty}$ into a $\mathbb{Z}%
$-graded Lie algebra. Note that $\mathfrak{gl}_{\infty}^{0}$ is abelian. Let
$\mathfrak{gl}_{\infty}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\mathfrak{gl}_{\infty}$,
so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}\mathfrak{gl}%
_{\infty}^{i}$ is the space of all strictly lower-triangular matrices in
$\mathfrak{gl}_{\infty}$, the subspace $\mathfrak{h}=\mathfrak{gl}_{\infty
}^{0}$ is the space of all diagonal matrices in $\mathfrak{gl}_{\infty}$, and
the subspace $\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\mathfrak{gl}_{\infty
}^{i}$ is the space of all strictly upper-triangular matrices in
$\mathfrak{gl}_{\infty}$.
\end{definition}

\begin{definition}
For every $i,j\in\mathbb{Z}$, let $E_{i,j}$ be the matrix (with rows and
columns labeled by integers) whose $\left(  i,j\right)  $-th entry is $1$ and
whose all other entries are $0$. Then, $\left(  E_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$ is a basis of $\mathfrak{gl}_{\infty}$.
\end{definition}

\begin{definition}
For every $\lambda\in\mathfrak{h}^{\ast}$, let $M_{\lambda}$ be the
highest-weight Verma module $M_{\lambda}^{+}$ (as defined in Definition
\ref{def.verma}). Let $J_{\lambda}=\operatorname*{Ker}\left(  \cdot
,\cdot\right)  \subseteq M_{\lambda}$ be the maximal proper graded submodule.
Let $L_{\lambda}$ be the quotient module $M_{\lambda}\diagup J_{\lambda
}=M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$; then, $L_{\lambda}$
is irreducible (as we know).
\end{definition}

\begin{definition}
We can define an antilinear antiinvolution $\dag:\mathfrak{gl}_{\infty
}\rightarrow\mathfrak{gl}_{\infty}$ on $\mathfrak{gl}_{\infty}$ by setting%
\[
E_{i,j}^{\dag}=E_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  i,j\right)
\in\mathbb{Z}^{2}.
\]
(Thus, $\dag:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}_{\infty}$ is the
operator which transposes a matrix and then applies complex conjugation to
each of its entries.) Thus we can speak of Hermitian and unitary
$\mathfrak{gl}_{\infty}$-modules.
\end{definition}

A very important remark:

For the Lie algebra $\mathfrak{gl}_{n}$, the highest-weight modules are the
Schur modules up to tensoring with a power of the determinant module. (More
precisely: For $\mathfrak{gl}_{n}$, every finite-dimensional irreducible
representation and any unitary irreducible representation is of the form
$S_{\pi}\left(  V_{n}\right)  \otimes\left(  \wedge^{n}\left(  V_{n}^{\ast
}\right)  \right)  ^{\otimes j}$ for some partition $\pi$ and some
$j\in\mathbb{N}$, where $V_{n}$ is the $\mathfrak{gl}_{n}$-module
$\mathbb{C}^{n}$.)

Nothing like this is true for $\mathfrak{gl}_{\infty}$. Instead, exterior
powers of $V$ and highest-weight representations live "in different worlds".
This is because $V$ is composed of infinite-dimensional vectors which have "no
top or bottom"; $V$ has no highest or lowest weight and does not lie in
category $\mathcal{O}^{+}$ or $\mathcal{O}^{-}$.

This is important, because many beautiful properties of representations of
$\mathfrak{gl}_{n}$ come from the equality of the highest-weight and Schur
module representations.

A way to marry these two worlds is by considering so-called
\textit{semiinfinite wedges}.

\subsubsection{Semiinfinite wedges}

Let us first give an informal definition of semiinfinite wedges and the
semiinfinite wedge space $\wedge^{\dfrac{\infty}{2}}V$ (we will later define
these things formally):

An \textit{elementary semiinfinite wedge} will mean a formal infinite "wedge
product" $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with $\left(
i_{0},i_{1},i_{2},...\right)  $ being a sequence of integers satisfying
$i_{0}>i_{1}>i_{2}>...$ and $i_{k+1}=i_{k}-1$ for all sufficiently large $k$.
(At the moment, we consider this wedge product $v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...$ just as a fancy symbol for the sequence $\left(
i_{0},i_{1},i_{2},...\right)  $.)

The \textit{semiinfinite wedge space} $\wedge^{\dfrac{\infty}{2}}V$ is defined
as the free vector space with basis given by elementary semiinfinite wedges.

Note that, despite the notation $\wedge^{\dfrac{\infty}{2}}V$, the
semiinfinite wedge space is not a functor in the vector space $V$. We could
replace our definition of $\wedge^{\dfrac{\infty}{2}}V$ by a somewhat more
functorial one, which doesn't use the basis $\left(  v_{i}\right)
_{i\in\mathbb{Z}}$ of $V$ anymore. But it would still need a topology on $V$
(which makes $V$ locally linearly compact), and some working with formal
Laurent series. It proceeds through the semiinfinite Grassmannian, and will
not be done in these lectures.\footnote{Some pointers to the more functorial
definition:
\par
Consider the field $\mathbb{C}\left(  \left(  t\right)  \right)  $ of formal
Laurent series over $\mathbb{C}$ as a $\mathbb{C}$-vector space.
\par
Let $\operatorname*{Gr}=\left\{  U\text{ vector subspace of }\mathbb{C}\left(
\left(  t\right)  \right)  \ \mid\ \left(
\begin{array}
[c]{c}%
U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ and}\\
\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[  \left[  t\right]  \right]
\right)  \right)  <\infty
\end{array}
\right)  \text{ for some sufficiently high }n\right\}  $.
\par
For every $U\in\operatorname*{Gr}$, define an integer $\operatorname*{sdim}U$
by $\operatorname*{sdim}U=\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[
\left[  t\right]  \right]  \right)  \right)  -n$ for any $n\in\mathbb{Z}$
satisfying $U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]  $.
Note that this integer does not depend on $n$ as long as $n$ is sufficiently
high to satisfy $U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]
$.
\par
This Grassmannian $\operatorname*{Gr}$ is the disjoint union $\coprod
\operatorname*{Gr}\nolimits_{n}$.
\par
There is something called a determinant line bundle on $\operatorname*{Gr}$.
The space of semiinfinite wedges is then defined as the space of regular
sections of this line bundle (in the sense of algebraic geometry).
\par
See the book by Pressley and Segal about loop groups for explanations of these
matters.} For us, the definition using the basis will be enough.

The space $\wedge^{\dfrac{\infty}{2}}V$ is countably dimensional. More
precisely, we can write $\wedge^{\dfrac{\infty}{2}}V$ as
\begin{align*}
\wedge^{\dfrac{\infty}{2}}V  &  =\bigoplus\limits_{m\in\mathbb{Z}}%
\wedge^{\dfrac{\infty}{2},m}V,\ \ \ \ \ \ \ \ \ \ \text{where}\\
\wedge^{\dfrac{\infty}{2},m}V  &  =\operatorname*{span}\left\{  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for
sufficiently large }k\right\}  .
\end{align*}
The space $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for sufficiently
large }k\right\}  $, which is easily seen to be countable. We will see later
that this basis can be naturally labeled by partitions (of all integers, not
just of $m$).

\subsubsection{The action of $\mathfrak{gl}_{\infty}$ on $\wedge
^{\dfrac{\infty}{2}}V$}

For every $m\in\mathbb{Z}$, we want to define an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the space $\wedge^{\dfrac{\infty}{2},m}V$ which is
given "by the usual Leibniz rule", i. e., satisfies the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$. Of course, it is not
immediately clear how to interpret the infinite wedge products $v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...$ on the right
hand side of this equation, since they are (in general) not elementary
semiinfinite wedges anymore. We must find a reasonable definition for such
wedge products. What properties should a wedge product (infinite as it is)
satisfy? It should be multilinear\footnote{i. e., it should satisfy
\begin{align*}
&  a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge\left(  \lambda
a+\lambda^{\prime}a^{\prime}\right)  \wedge a_{k+1}\wedge a_{k+2}\wedge...\\
&  =\lambda a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge a\wedge
a_{k+1}\wedge a_{k+2}\wedge...+\lambda^{\prime}a_{0}\wedge a_{1}%
\wedge...\wedge a_{k-1}\wedge a^{\prime}\wedge a_{k+1}\wedge a_{k+2}\wedge...
\end{align*}
for all $k\in\mathbb{N}$, $a_{0},a_{1},a_{2},...\in V$ and $\lambda
,\lambda^{\prime}\in\mathbb{C}$ for which the right hand side is well-defined}
and antisymmetric\footnote{i. e., a well-defined wedge product $a_{0}\wedge
a_{1}\wedge a_{2}\wedge...$ should be $0$ whenever two of the $a_{k}$ are
equal}. These properties make it possible to compute any wedge product of the
form $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ with $a_{0},a_{1},a_{2},...$
being vectors in $V$ which satisfy%
\[
a_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
In fact, whenever we are given such vectors $a_{0},a_{1},a_{2},...$, we can
compute the wedge product $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ by the
following procedure:

\begin{itemize}
\item Find an integer $M\in\mathbb{N}$ such that every $i\geq M$ satisfies
$a_{i}=v_{m-i}$. (This $M$ exists by the condition that $a_{i}=v_{m-i}$ for
sufficiently large $i$.)

\item Expand each of the vectors $a_{0},a_{1},...,a_{M-1}$ as a $\mathbb{C}%
$-linear combination of the basis vectors $v_{\ell}$.

\item Using these expansions and the multilinearity of the wedge product,
reduce the computation of $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ to the
computation of finitely many wedge products of basis vectors.

\item Each wedge product of basis vectors can now be computed as follows: If
two of the basis vectors are equal, then it must be $0$ (by antisymmetry of
the wedge product). If not, reorder the basis vectors in such a way that their
indices decrease (this is possible, because "most" of these basis vectors are
already in order, and only the first few must be reordered). Due to the
antisymmetry of the wedge product, the wedge product of the basis vectors
before reordering must be $\left(  -1\right)  ^{\pi}$ times the wedge product
of the basis vectors after reordering, where $\pi$ is the permutation which
corresponds to our reordering. But the wedge product of the basis vectors
after reordering is an elementary semiinfinite wedge, and thus we know how to
compute it.
\end{itemize}

This procedure is not exactly a formal definition, and it is not immediately
clear that the value of $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ that it
computes is independent of, e. g., the choice of $M$. In the following
subsection (Subsection \ref{subsect.degress}), we will give a formal version
of this definition.

\subsubsection{\label{subsect.degress}The $\mathfrak{gl}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2}}V$: a formal definition}

Before we formally define the value of $a_{0}\wedge a_{1}\wedge a_{2}%
\wedge...$, let us start at the beginning and repeat the definitions of
$\wedge^{\dfrac{\infty}{2}}V$ and $\wedge^{\dfrac{\infty}{2},m}V$ in a cleaner
fashion than how we defined them above.

\textbf{Warning:} Parts of the following nomenclature (particularly, the
notions of "$m$-degression" and "straying $m$-degression") are mine (=Darij's).

First, we introduce the notion of $m$\textit{-degressions} and formalize the
definitions of $\wedge^{\dfrac{\infty}{2}}V$ and $\wedge^{\dfrac{\infty}{2}%
,m}V$.

\begin{definition}
\label{def.glinf.m-deg}Let $m\in\mathbb{Z}$. An $m$\textit{-degression} will
mean a strictly decreasing sequence $\left(  i_{0},i_{1},i_{2},...\right)  $
of integers such that every sufficiently high $k\in\mathbb{N}$ satisfies
$i_{k}+k=m$. It is clear that any $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ automatically satisfies $i_{k}-i_{k-1}=1$ for all
sufficiently high $k$.

For any $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, we introduce
a new symbol $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$. This symbol
is, for the time being, devoid of any meaning. The symbol $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ will be called an \textit{elementary
semiinfinite wedge}.
\end{definition}

\begin{definition}
\textbf{(a)} Let $\wedge^{\dfrac{\infty}{2}}V$ denote the free $\mathbb{C}%
$-vector space with basis $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  _{m\in\mathbb{Z};\ \left(  i_{0},i_{1},i_{2},...\right)
\text{ is an }m\text{-degression}}$. We will refer to $\wedge^{\dfrac{\infty
}{2}}V$ as the \textit{semiinfinite wedge space}.

\textbf{(b)} For every $m\in\mathbb{Z}$, define a $\mathbb{C}$-vector subspace
$\wedge^{\dfrac{\infty}{2},m}V$ of $\wedge^{\dfrac{\infty}{2}}V$ by%
\[
\wedge^{\dfrac{\infty}{2},m}V=\operatorname*{span}\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ \left(  i_{0},i_{1},i_{2}%
,...\right)  \text{ is an }m\text{-degression}\right\}  .
\]
Clearly, $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$.
\end{definition}

Obviously, $\wedge^{\dfrac{\infty}{2}}V=\bigoplus\limits_{m\in\mathbb{Z}%
}\wedge^{\dfrac{\infty}{2},m}V$.

Now, let us introduce the (more flexible) notion of \textit{straying }%
$m$\textit{-degressions}. This notion is obtained from the notion of
$m$-degressions by dropping the "strictly decreasing" condition:

\begin{definition}
\label{def.glinf.straym-deg}Let $m\in\mathbb{Z}$. A \textit{straying }%
$m$\textit{-degression} will mean a sequence $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ of integers such that every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=m$.
\end{definition}

As a consequence, a straying $m$-degression is strictly decreasing from some
point onwards, but needs not be strictly decreasing from the beginning (it can
"stray", whence the name). A strictly decreasing straying $m$-degression is
exactly the same as an $m$-degression.

\begin{definition}
Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a
straying $m$-degression. If no two elements of this sequence $\left(
i_{0},i_{1},i_{2},...\right)  $ are equal, then there exists a unique
permutation $\pi$ of $\mathbb{N}$ such that $\left(  i_{\pi^{-1}\left(
0\right)  },i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)
},...\right)  $ is an $m$-degression. This permutation $\pi$ is called the
\textit{straightening permutation} of $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{definition}

Let us recall here that a \textit{permutation} of an infinite set is defined
as a bijection from the set to itself which fixes all but finitely many of its elements.

\begin{definition}
Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a
straying $m$-degression. We define the meaning of the term $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ as follows:

- If two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ are
equal, then $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ is defined to
mean the element $0$ of $\wedge^{\dfrac{\infty}{2},m}V$.

- If no two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $
are equal, then $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ is
defined to mean the element $\left(  -1\right)  ^{\pi}v_{i_{\pi^{-1}\left(
0\right)  }}\wedge v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi
^{-1}\left(  2\right)  }}\wedge...$ of $\wedge^{\dfrac{\infty}{2},m}V$, where
$\pi$ is the straightening permutation of $\left(  i_{0},i_{1},i_{2}%
,...\right)  $.
\end{definition}

\begin{definition}
Let $m\in\mathbb{Z}$. Let $a_{0},a_{1},a_{2},...$ be vectors in $V$ which
satisfy%
\[
a_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
Then, let us define the wedge product $a_{0}\wedge a_{1}\wedge a_{2}%
\wedge...\in\wedge^{\dfrac{\infty}{2},m}V$ as follows:

Find an integer $M\in\mathbb{N}$ such that every $i\geq M$ satisfies
$a_{i}=v_{m-i}$. (This $M$ exists by the condition that $a_{i}=v_{m-i}$ for
sufficiently large $i$.)

For every $i\in\left\{  0,1,...,M-1\right\}  $, write the vector $a_{i}$ as a
$\mathbb{C}$-linear combination $\sum\limits_{j\in\mathbb{Z}}\lambda
_{i,j}v_{j}$ (with $\lambda_{i,j}\in\mathbb{C}$ for all $j$).

Now, define $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ to be%
\[
\sum\limits_{\left(  j_{0},j_{1},...,j_{M-1}\right)  \in\mathbb{Z}^{M}}%
\lambda_{0,j_{0}}\lambda_{1,j_{1}}...\lambda_{M-1,j_{M-1}}v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{M-1}}\wedge v_{m-M}\wedge v_{m-M-1}\wedge
v_{m-M-2}\wedge....
\]
Here, $v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{M-1}}\wedge
v_{m-M}\wedge v_{m-M-1}\wedge v_{m-M-2}\wedge...$ is well-defined, since
$\left(  j_{0},j_{1},...,j_{M-1},m-M,m-M-1,m-M-2,...\right)  $ is a straying
$m$-degression.
\end{definition}

This wedge product is easily seen to be well-defined (i. e., the choice of $M$
does not affect the value of $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$) and
indeed antisymmetric and multilinear.

Now, we can define the action of $\mathfrak{gl}_{\infty}$ on $\wedge
^{\dfrac{\infty}{2},m}V$ just as we wanted to:

\begin{definition}
\label{def.glinf.semiinfwedge}Let $m\in\mathbb{Z}$. Define an action of the
Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge
^{\dfrac{\infty}{2},m}V$ by the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ (and by linear extension).
\end{definition}

Of course, this definition is only justified after showing that this indeed is
an action. But this is rather easy. Let us state this as a proposition:

\begin{proposition}
Let $m\in\mathbb{Z}$. Then, Definition \ref{def.glinf.semiinfwedge} really
defines a representation of the Lie algebra $\mathfrak{gl}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$.
\end{proposition}

Also, due to the multilinearity of our wedge product, we easily see that:

\begin{proposition}
\label{prop.glinf.glinfact}Let $m\in\mathbb{Z}$. Let $a_{0},a_{1},a_{2},...$
be vectors in $V$ which satisfy%
\[
a_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]
Let $a\in\mathfrak{gl}_{\infty}$. Then,%
\[
a\rightharpoonup\left(  a_{0}\wedge a_{1}\wedge a_{2}\wedge...\right)
=\sum\limits_{k\geq0}a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge\left(
a\rightharpoonup a_{k}\right)  \wedge a_{k+1}\wedge a_{k+2}\wedge....
\]

\end{proposition}

We can also explicitly describe the action of $\mathfrak{gl}_{\infty}$ on
$\wedge^{\dfrac{\infty}{2},m}V$:

\begin{proposition}
Let $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$. Let $v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...$ be an elementary semiinfinite wedge.

\textbf{(a)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
$E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =0$.

\textbf{(b)} If $j\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
$E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ is the wedge product which is obtained from $v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ by replacing the $v_{j}$ factor
(which indeed appears in $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$
because $j\in\left\{  i_{0},i_{1},i_{2},...\right\}  $) by $v_{i}$.
\end{proposition}

Since we have given $\wedge^{\dfrac{\infty}{2},m}V$ a $\mathfrak{gl}_{\infty}%
$-module structure for every $m\in\mathbb{Z}$, it is clear that $\wedge
^{\dfrac{\infty}{2}}V=\bigoplus\limits_{m\in\mathbb{Z}}\wedge^{\dfrac{\infty
}{2},m}V$ also becomes a $\mathfrak{gl}_{\infty}$-module.

\subsubsection{Properties of $\wedge^{\dfrac{\infty}{2},m}V$}

There is an easy way to define a grading on $\wedge^{\dfrac{\infty}{2},m}V$.
To do it, we notice that:

\begin{proposition}
\label{prop.glinf.wedge.grading}For every $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $, the sequence $\left(  i_{k}+k-m\right)  _{k\geq0}$
is a partition (i. e., a nonincreasing sequence of nonnegative integers such
that all but finitely many of its elements are $0$). In particular, every
integer $k\geq0$ satisfies $i_{k}+k-m\geq0$, and only finitely many integers
$k\geq0$ satisfy $i_{k}+k-m\neq0$. Hence, the sum $\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  $ is well-defined and equals a nonnegative integer.
\end{proposition}

The proof of this is very easy and left to the reader. As a consequence of
this proposition, we have:

\begin{definition}
\label{def.glinf.wedge.grading}Let $m\in\mathbb{Z}$. We define a grading on
the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}V$ by setting%
\begin{align*}
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  d\right]   &
=\left\langle v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid
\ \left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression
}\phantom{\sum\limits_{k}}\right. \\
&
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.
\text{satisfying }\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)
=-d\right\rangle \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for every }d\in\mathbb{Z}\right.  .
\end{align*}
In other words, we define a grading on the $\mathbb{C}$-vector space
$\wedge^{\dfrac{\infty}{2},m}V$ by setting%
\[
\deg\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=-\sum\limits_{k}\left(  i_{k}+k-m\right)
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.

This grading satisfies $\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\leq
0}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  d\right]  $ (since
Proposition \ref{prop.glinf.wedge.grading} yields that $\sum\limits_{k\geq
0}\left(  i_{k}+k-m\right)  $ is nonnegative for every for every
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $). In other words,
$\wedge^{\dfrac{\infty}{2},m}V$ is nonpositively graded.
\end{definition}

Note that, for every given $m\in\mathbb{Z}$, the $m$-degressions are in a
1-to-1 correspondence with the partitions. This correspondence maps any
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ to the sequence
$\left(  i_{k}+k-m\right)  _{k\geq0}$ (this sequence is a partition due to
Proposition \ref{prop.glinf.wedge.grading}). The degree $\deg\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ of the semiinfinite wedge
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ equals minus the sum of
the parts of this partition.

It is easy to check that:

\begin{proposition}
Let $m\in\mathbb{Z}$. With the grading defined in Definition
\ref{def.glinf.wedge.grading}, the $\mathfrak{gl}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2},m}V$ is graded (where the grading on $\mathfrak{gl}%
_{\infty}$ is the one from Definition \ref{def.glinf.grade}).
\end{proposition}

Let us say more about this module:

\begin{proposition}
\label{prop.Lomegam}Let $m\in\mathbb{Z}$. The graded $\mathfrak{gl}_{\infty}%
$-module $\wedge^{\dfrac{\infty}{2},m}V$ is the irreducible highest-weight
representation $L_{\omega_{m}}$ of $\mathfrak{gl}_{\infty}$ with highest
weight $\omega_{m}=\left(  ...,1,1,0,0,...\right)  $, where the last $1$ is on
place $m$ and the first $0$ is on place $m+1$. Moreover, $L_{\omega_{m}}$ is unitary.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Lomegam}.} Define a $w_{m}\in
\wedge^{\dfrac{\infty}{2},m}V$ by $w_{m}=v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...$. Then, $\mathfrak{n}_{+}\cdot w_{m}=0$. (In fact, if
$E_{i,j}\in\mathfrak{n}_{+}$ then $i<j$ and thus indices are replaced by
smaller indices...) Moreover, every $h\in\mathfrak{h}$ satisfies
$hw_{m}=\omega_{m}\left(  h\right)  w_{m}$ (in fact, test at $h=E_{i,i}$).
Also, $w_{m}$ generates the $\mathfrak{gl}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$. Thus, $\wedge^{\dfrac{\infty}{2},m}V$ is a
highest-weight representation with highest weight $\omega_{m}$.

Next let us prove that it is unitary. This will yield that it is
irreducible.\footnote{We could also show the irreducibility more directly, by
showing that every sum of wedges can be used to get back $w_{m}$.}

The unitarity is because the form in which the wedges are orthonormal is
$\dag$-invariant. Thus, irreducible. (We used Lemma \ref{lem.unitrick}.)
Proposition \ref{prop.Lomegam} is proven.

\begin{corollary}
\label{cor.lomegam.unit}For every finite sum $\sum\limits_{i\in\mathbb{Z}%
}k_{i}\omega_{i}$ with $k_{i}\in\mathbb{N}$, the representation $L_{\sum
\limits_{i\in\mathbb{Z}}k_{i}\omega_{i}}$ is unitary.
\end{corollary}

\textit{Proof.} Take the module $\bigotimes\limits_{i}L_{\omega_{i}}^{\otimes
k_{i}}$, and let $v$ be the tensor product of their respective highest-weight
vectors. Let $L$ be the submodule generated by $v$. Then, $L$ is a
highest-weight module, and is unitary since it is a submodule of a unitary
module. Hence it is irreducible, and thus $L\cong L_{\sum\limits_{i}%
k_{i}\omega_{i}}$, qed.

\subsection{$\overline{\mathfrak{a}_{\infty}}$}

The Lie algebra $\mathfrak{gl}_{\infty}$ is fairly small (it doesn't even
contain the identity matrix) - too small for several applications. Here is a
larger Lie algebra with roughly similar properties:

\begin{definition}
We define $\overline{\mathfrak{a}_{\infty}}$ to be the vector space of
infinite matrices with rows and columns labeled by integers (not only positive
integers) such that only finitely many \textbf{diagonals} are nonzero. This is
an associative algebra with $1$ (due to Remark \ref{rmk.ainf.mult}
\textbf{(a)} below), and thus, by the commutator, a Lie algebra.
\end{definition}

We can think of the elements of $\overline{\mathfrak{a}_{\infty}}$ as
difference operators:

Consider $V$ as the space of sequences\footnote{In the following, "sequences"
means "sequences labeled by integers".} with finitely many nonzero entries.
One very important endomorphism of $V$ is defined as follows:

\begin{definition}
\label{def.shiftoperator}Let $T:V\rightarrow V$ be the linear map given by
\[
\left(  Tx\right)  _{n}=x_{n+1}\ \ \ \ \ \ \ \ \ \ \text{for all }x\in V\text{
and }n\in\mathbb{Z}.
\]
This map $T$ is called the \textit{shift operator}. It satisfies
$Tv_{i+1}=v_{i}$ for every $i\in\mathbb{Z}$.

We can also write $T$ in the form $T=\sum\limits_{i\in\mathbb{Z}}E_{i,i+1}$,
where the sum is infinite but makes sense entrywise (i. e., for every $\left(
a,b\right)  \in\mathbb{Z}^{2}$, there are only finitely many $i\in\mathbb{Z}$
for which the matrix $E_{i,i+1}$ has nonzero $\left(  a,b\right)  $-th entry).
\end{definition}

Note that:

\begin{proposition}
\label{prop.shiftoperator.Tj}The shift operator $T$ is invertible. Every
$j\in\mathbb{Z}$ satisfies $T^{j}=\sum\limits_{i\in\mathbb{Z}}E_{i,i+j}$.
\end{proposition}

A \textit{difference operator} is an operator of the form $A=\sum
\limits_{i=p}^{q}a_{i}\left(  n\right)  T^{i}$, where $p$ and $q$ are some
integers, and $a_{i}:\mathbb{Z}\rightarrow\mathbb{C}$ are some
functions.\footnote{The sum $\sum\limits_{i=p}^{q}a_{i}\left(  n\right)
T^{i}$ has to be understood as the linear map $X:V\rightarrow V$ given by%
\[
\left(  Xx\right)  _{n}=\sum\limits_{i=p}^{q}a_{i}\left(  n\right)
x_{n+i}\ \ \ \ \ \ \ \ \ \ \text{for all }x\in V\text{ and }n\in\mathbb{Z}.
\]
} Then, $\overline{\mathfrak{a}_{\infty}}$ is the algebra of all such
operators. (These operators also act on the space of \textit{all} sequences,
not only on the space of sequences with finitely many nonzero entries.)

Note that $\overline{\mathfrak{a}_{\infty}}$ is no longer countably
dimensional. The family $\left(  E_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}$ is no longer a vector space basis, but it is a topological
basis in an appropriately defined topology.

Let us make a remark on multiplication of infinite matrices:

\begin{remark}
\label{rmk.ainf.mult}\textbf{(a)} For every $A\in\overline{\mathfrak{a}%
_{\infty}}$ and $B\in\overline{\mathfrak{a}_{\infty}}$, the matrix $AB$ is
well-defined and lies in $\overline{\mathfrak{a}_{\infty}}$.

\textbf{(b)} For every $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\mathfrak{gl}_{\infty}$, the matrix $AB$ is well-defined and lies in
$\mathfrak{gl}_{\infty}$.
\end{remark}

\textit{Proof of Remark \ref{rmk.ainf.mult}.} \textbf{(a)} Let $A\in
\overline{\mathfrak{a}_{\infty}}$ and $B\in\overline{\mathfrak{a}_{\infty}}$.
Write the matrix $A$ in the form $\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$, and write the matrix $B$ in the form $\left(
b_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$.

Since $A\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of
$A$ are nonzero. Hence, there exists a finite subset $\mathfrak{A}$ of
$\mathbb{Z}$ such that%
\begin{equation}
\text{for every }u\in\mathbb{Z}\diagdown\mathfrak{A}\text{, the }u\text{-th
diagonal of }A\text{ is zero.} \label{pf.ainf.mult.1}%
\end{equation}
Consider this $\mathfrak{A}$.

Since $B\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of
$B$ are nonzero. Hence, there exists a finite subset $\mathfrak{B}$ of
$\mathbb{Z}$ such that%
\begin{equation}
\text{for every }v\in\mathbb{Z}\diagdown\mathfrak{B}\text{, the }v\text{-th
diagonal of }B\text{ is zero.} \label{pf.ainf.mult.2}%
\end{equation}
Consider this $\mathfrak{B}$.

For every $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$, the infinite sum
$\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$ has a well-defined value, because
all but finitely many addends of this sum are zero\footnote{\textit{Proof.}
Every $k\in\mathbb{Z}$ such that $k-i\notin\mathfrak{A}$ satisfies $a_{i,k}=0$
(because $k-i\notin\mathfrak{A}$, so that $k-i\in\mathbb{Z}\diagdown
\mathfrak{A}$, and thus (\ref{pf.ainf.mult.1}) (applied to $u=k-i$) yields
that the $\left(  k-i\right)  $-th diagonal of $A$ is zero, and thus $a_{i,k}$
(being an entry in this diagonal) must be $=0$). Hence, every $k\in\mathbb{Z}$
such that $k-i\notin\mathfrak{A}$ satisfies $a_{i,k}b_{k,j}=0b_{k,j}=0$. Since
$\mathfrak{A}$ is a finite set, all but finitely many $k\in\mathbb{Z}$ satisfy
$k-i\notin\mathfrak{A}$, and thus all but finitely many $k\in\mathbb{Z}$
satisfy $a_{i,k}b_{k,j}=0$ (because every $k\in\mathbb{Z}$ such that
$k-i\notin\mathfrak{A}$ satisfies $a_{i,k}b_{k,j}=0$). In other words, all but
finitely many addends of the sum $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$
are zero, qed.}. Hence, the matrix $AB$ is well-defined (because the matrix
$AB$ is defined as the matrix whose $\left(  i,j\right)  $-th entry is
$\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$ for all $\left(  i,j\right)
\in\mathbb{Z}^{2}$), and satisfies%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}%
\]
for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$.

Now we must show that $AB\in\overline{\mathfrak{a}_{\infty}}$.

Let $\mathfrak{A}+\mathfrak{B}$ denote the set $\left\{  a+b\ \mid\ \left(
a,b\right)  \in\mathfrak{A}\times\mathfrak{B}\right\}  $. Clearly,
$\mathfrak{A}+\mathfrak{B}$ is a finite set (since $\mathfrak{A}$ and
$\mathfrak{B}$ are finite). Now, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $j-i\notin\mathfrak{A}+\mathfrak{B}$, every $k\in\mathbb{Z}$
satisfies $a_{i,k}b_{k,j}=0$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $j-i\notin\mathfrak{A}%
+\mathfrak{B}$, and let $k\in\mathbb{Z}$. Assume that $a_{i,k}b_{k,j}\neq0$.
Then, $a_{i,k}\neq0$ and $b_{k,j}\neq0$.
\par
Since $a_{i,k}$ is an entry of the $\left(  k-i\right)  $-th diagonal of $A$,
we see that some entry of the $\left(  k-i\right)  $-th diagonal of $A$ is
nonzero (since $a_{i,k}\neq0$). Hence, the $\left(  k-i\right)  $-th diagonal
of $A$ is nonzero. Thus, $k-i\notin\mathbb{Z}\diagdown\mathfrak{A}$ (because
otherwise, we would have $k-i\in\mathbb{Z}\diagdown\mathfrak{A}$, so that
(\ref{pf.ainf.mult.1}) (applied to $u=k-i$) would yield that the $\left(
k-i\right)  $-th diagonal of $A$ is zero, contradicting the fact that it is
nonzero), so that $k-i\in\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $\left(  j-k\right)  $-th diagonal of $B$,
we see that some entry of the $\left(  j-k\right)  $-th diagonal of $B$ is
nonzero (since $b_{k,j}\neq0$). Hence, the $\left(  j-k\right)  $-th diagonal
of $B$ is nonzero. Thus, $j-k\notin\mathbb{Z}\diagdown\mathfrak{B}$ (because
otherwise, we would have $j-k\in\mathbb{Z}\diagdown\mathfrak{B}$, so that
(\ref{pf.ainf.mult.2}) (applied to $v=j-k$) would yield that the $\left(
j-k\right)  $-th diagonal of $B$ is zero, contradicting the fact that it is
nonzero), so that $j-k\in\mathfrak{B}$.
\par
Now, $j-i=\underbrace{\left(  k-i\right)  }_{\in\mathfrak{A}}%
+\underbrace{\left(  j-k\right)  }_{\in\mathfrak{B}}\in\mathfrak{A}%
+\mathfrak{B}$. This contradicts $j-i\notin\mathfrak{A}+\mathfrak{B}$. Thus,
our assumption that $a_{i,k}b_{k,j}\neq0$ must have been wrong. Hence,
$a_{i,k}b_{k,j}=0$, qed.}. Thus, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $j-i\notin\mathfrak{A}+\mathfrak{B}$, we have%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}b_{k,j}}%
_{\substack{=0\\\text{(since }j-i\notin\mathfrak{A}+\mathfrak{B}\text{)}%
}}=\sum\limits_{k\in\mathbb{Z}}0=0.
\]
Thus, for every integer $w\notin\mathfrak{A}+\mathfrak{B}$, and any
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfying $j-i=w$, we have $\left(
\left(  i,j\right)  \text{-th entry of the matrix }AB\right)  =0$ (since
$j-i=w\notin\mathfrak{A}+\mathfrak{B}$). In other words, for every integer
$w\notin\mathfrak{A}+\mathfrak{B}$, the $w$-th diagonal of $AB$ is zero. Since
$\mathfrak{A}+\mathfrak{B}$ is a finite set, this yields that all but finitely
many diagonals of $AB$ are zero. In other words, only finitely many diagonals
of $AB$ are nonzero. In other words, $AB\in\overline{\mathfrak{a}_{\infty}}$.
This proves Remark \ref{rmk.ainf.mult} \textbf{(a)}.

\textbf{(b)} We know from Remark \ref{rmk.ainf.mult} \textbf{(a)} that the
matrix $AB$ is well-defined (since $B\in\mathfrak{gl}_{\infty}\subseteq
\overline{\mathfrak{a}_{\infty}}$).

The matrix $B$ lies in $\mathfrak{gl}_{\infty}$ and thus has only finitely
many nonzero entries. Hence, $B$ has only finitely many nonzero rows. In other
words, there exists a finite subset $\mathfrak{R}$ of $\mathbb{Z}$ such that%
\begin{equation}
\text{for every }x\in\mathbb{Z}\diagdown\mathfrak{R}\text{, the }x\text{-th
row of }B\text{ is zero.} \label{pf.ainf.mult.3}%
\end{equation}


Also, $B$ has only finitely many nonzero entries, and thus only finitely many
nonzero columns. In other words, there exists a finite subset $\mathfrak{C}$
of $\mathbb{Z}$ such that%
\begin{equation}
\text{for every }y\in\mathbb{Z}\diagdown\mathfrak{C}\text{, the }y\text{-th
column of }B\text{ is zero.} \label{pf.ainf.mult.4}%
\end{equation}


Define $\mathfrak{A}$ as in the proof of Remark \ref{rmk.ainf.mult}
\textbf{(a)}. Let $\mathfrak{R}-\mathfrak{A}$ denote the set $\left\{
r-a\ \mid\ \left(  r,a\right)  \in\mathfrak{R}\times\mathfrak{A}\right\}  $.
Clearly, $\mathfrak{R}-\mathfrak{A}$ is a finite set (since $\mathfrak{A}$ and
$\mathfrak{R}$ are finite), and thus $\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$ is a finite set (since $\mathfrak{C}$, too, is
finite). Now, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfying
$\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}\right)
\times\mathfrak{C}$, we have $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}%
=0$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
be such that $\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$. Assume that $\sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}\neq0$. Then, there exists some $k\in\mathbb{Z}$ such that
$a_{i,k}b_{k,j}\neq0$. Consider this $k$.
\par
Since $a_{i,k}b_{k,j}\neq0$, we have $a_{i,k}\neq0$ and $b_{k,j}\neq0$.
\par
Since $a_{i,k}$ is an entry of the $\left(  k-i\right)  $-th diagonal of $A$,
we see that some entry of the $\left(  k-i\right)  $-th diagonal of $A$ is
nonzero (since $a_{i,k}\neq0$). Hence, the $\left(  k-i\right)  $-th diagonal
of $A$ is nonzero. Thus, $k-i\notin\mathbb{Z}\diagdown\mathfrak{A}$ (because
otherwise, we would have $k-i\in\mathbb{Z}\diagdown\mathfrak{A}$, so that
(\ref{pf.ainf.mult.1}) (applied to $u=k-i$) would yield that the $\left(
k-i\right)  $-th diagonal of $A$ is zero, contradicting the fact that it is
nonzero), so that $k-i\in\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $k$-th row of $B$, we see that some entry
of the $k$-th row of $B$ is nonzero (since $b_{k,j}\neq0$). Hence, the $k$-th
row of $B$ is nonzero. Thus, $k\notin\mathbb{Z}\diagdown\mathfrak{R}$ (because
otherwise, we would have $k\in\mathbb{Z}\diagdown\mathfrak{R}$, so that
(\ref{pf.ainf.mult.3}) (applied to $x=k$) would yield that the $k$-th row of
$B$ is zero, contradicting the fact that it is nonzero), so that
$k\in\mathfrak{R}$.
\par
Thus, $i=\underbrace{k}_{\in\mathfrak{R}}-\underbrace{\left(  k-i\right)
}_{\in\mathfrak{A}}\in\mathfrak{R}-\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $j$-th column of $B$, we see that some
entry of the $j$-th column of $B$ is nonzero (since $b_{k,j}\neq0$). Hence,
the $j$-th column of $B$ is nonzero. Thus, $j\notin\mathbb{Z}\diagdown
\mathfrak{C}$ (because otherwise, we would have $j\in\mathbb{Z}\diagdown
\mathfrak{C}$, so that (\ref{pf.ainf.mult.4}) (applied to $y=j$) would yield
that the $j$-th column of $B$ is zero, contradicting the fact that it is
nonzero), so that $j\in\mathfrak{C}$. Combined with $i\in\mathfrak{R}%
-\mathfrak{A}$, this yields $\left(  i,j\right)  \in\left(  \mathfrak{R}%
-\mathfrak{A}\right)  \times\mathfrak{C}$, contradicting $\left(  i,j\right)
\notin\left(  \mathfrak{R}-\mathfrak{A}\right)  \times\mathfrak{C}$. Hence,
the assumption that $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}\neq0$ must
have been wrong. In other words, $\sum\limits_{k\in\mathbb{Z}}a_{i,k}%
b_{k,j}=0$, qed.}. Hence, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$, we have%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}=0.
\]
Since $\left(  \mathfrak{R}-\mathfrak{A}\right)  \times\mathfrak{C}$ is a
finite set, this yields that all but finitely many entries of the matrix $AB$
are zero. In other words, $AB$ has only finitely many nonzero entries. Thus,
$AB\in\mathfrak{gl}_{\infty}$. Remark \ref{rmk.ainf.mult} \textbf{(b)} is proven.

Let us make $\overline{\mathfrak{a}_{\infty}}$ into a graded Lie algebra:

\begin{definition}
\label{def.ainf.grade}For every $i\in\mathbb{Z}$, let $\overline
{\mathfrak{a}_{\infty}^{i}}$ be the subspace of $\overline{\mathfrak{a}%
_{\infty}}$ which consists of matrices which have nonzero entries only on the
$i$-th diagonal. (The $i$\textit{-th diagonal} consists of the entries in the
$\left(  \alpha,\beta\right)  $-th places with $\beta-\alpha=i$.)

Then, $\overline{\mathfrak{a}_{\infty}}=\bigoplus\limits_{i\in\mathbb{Z}%
}\overline{\mathfrak{a}_{\infty}^{i}}$, and this makes $\overline
{\mathfrak{a}_{\infty}}$ into a $\mathbb{Z}$-graded Lie algebra. Note that
$\overline{\mathfrak{a}_{\infty}^{0}}$ is abelian. Let $\overline
{\mathfrak{a}_{\infty}}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\overline{\mathfrak{a}%
_{\infty}}$, so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}%
\overline{\mathfrak{a}_{\infty}^{i}}$ is the space of all strictly
lower-triangular matrices in $\overline{\mathfrak{a}_{\infty}}$, the subspace
$\mathfrak{h}=\overline{\mathfrak{a}_{\infty}^{0}}$ is the space of all
diagonal matrices in $\overline{\mathfrak{a}_{\infty}}$, and the subspace
$\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\overline{\mathfrak{a}_{\infty}^{i}}$
is the space of all strictly upper-triangular matrices in $\overline
{\mathfrak{a}_{\infty}}$.
\end{definition}

Note that this was completely analogous to Definition \ref{def.glinf.grade}.

\subsection{The action of $\mathfrak{a}_{\infty}$ on $\wedge^{\dfrac{\infty
}{2},m}V$}

\begin{definition}
Let $m\in\mathbb{Z}$. Let $\rho:\mathfrak{gl}_{\infty}\rightarrow
\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ be the
representation of $\mathfrak{gl}_{\infty}$ on $\wedge^{\dfrac{\infty}{2},m}V$
defined in Definition \ref{def.glinf.semiinfwedge}.
\end{definition}

The following question poses itself naturally now: Can we extend this
representation $\rho$ to a representation of $\overline{\mathfrak{a}_{\infty}%
}$ in a reasonable way?

This question depends on what we means by "reasonable". One way to concretize
this is by noticing that $\overline{\mathfrak{a}_{\infty}}=\bigoplus
\limits_{i\in\mathbb{Z}}\overline{\mathfrak{a}_{\infty}^{i}}$, where
$\overline{\mathfrak{a}_{\infty}^{i}}$ is the space of all matrices with
nonzero entries only on the $i$-th diagonal. For each $i\in\mathbb{Z}$, the
vector space $\overline{\mathfrak{a}_{\infty}^{i}}$ can be given the product
topology (i. e., the topology in which a net $\left(  a_{z}\right)  _{z\in Z}$
of matrices converges to a matrix $a$ if and only if for any $\left(
m,n\right)  \in\mathbb{Z}^{2}$ satisfying $n-m=i$, the net of the $\left(
m,n\right)  $-th entries of the matrices $a_{z}$ converge to the $\left(
m,n\right)  $-th entry of $a$ in the discrete topology). Then, $\mathfrak{gl}%
_{\infty}^{i}$ in dense in $\overline{\mathfrak{a}_{\infty}^{i}}$ for every
$i\in\mathbb{Z}$. We can also make $\wedge^{\dfrac{\infty}{2},m}V$ into a
topological space by using the discrete topology. Our question can now be
stated as follows: Can we extend $\rho$ by continuity to a representation of
$\overline{\mathfrak{a}_{\infty}}$ (where "continuous" means "continuous on
each $\overline{\mathfrak{a}_{\infty}^{i}}$", since we have not defined a
topology on the whole space $\overline{\mathfrak{a}_{\infty}}$) ?

Answer: Almost, but not precisely. We cannot make $\overline{\mathfrak{a}%
_{\infty}}$ act on $\wedge^{\dfrac{\infty}{2},m}V$ in such a way that its
action extends $\rho$ continuously, but we can make a central extension of
$\overline{\mathfrak{a}_{\infty}}$ act on $\wedge^{\dfrac{\infty}{2},m}V$ in a
way that only slightly differs from $\rho$.

Let us first see what goes wrong if we try to find an extension of $\rho$ to
$\overline{\mathfrak{a}_{\infty}}$ by continuity:

For $i\neq0$, a typical element $X\in\overline{\mathfrak{a}_{\infty}^{i}}$ is
of the form $X=\sum\limits_{j\in\mathbb{Z}}a_{j}E_{j,j+i}$ with $a_{j}%
\in\mathbb{C}$. Now we can define $\rho\left(  X\right)  v=\sum\limits_{j\in
\mathbb{Z}}a_{j}\rho\left(  E_{j,j+i}\right)  v$ for every $v\in\wedge
^{\dfrac{\infty}{2},m}V$; this sum has only finitely many nonzero
addends\footnote{\textit{Proof.} We must prove that, for every $v\in
\wedge^{\dfrac{\infty}{2},m}V$, the sum $\sum\limits_{j\in\mathbb{Z}}a_{j}%
\rho\left(  E_{j,j+i}\right)  v$ has only finitely many nonzero addends. It is
clearly enough to prove this in the case when $v$ is an elementary
semiinfinite wedge. So let us WLOG assume that $v$ is an elementary
semiinfinite wedge. In other words, WLOG assume that $v=v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ for some $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $. Consider this $m$-degression. By the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satsfies $i_{k}+k=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every integer $k\geq K$ satisfies $i_{k}+k=m$. Consider this $K$. Then,
every integer $j\leq i_{K}$ appears in the $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $.
\par
Now, we have the following two observations:
\par
\begin{itemize}
\item Every integer $j>i_{0}-i$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$
(because for every integer $j>i_{0}-i$, we have $j+i>i_{0}$, so that the
integer $j+i$ does not appear in the $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $).
\par
\item Every integer $j\leq i_{K}$ satisfies $\rho\left(  E_{j,j+i}\right)
v=0$ (because every integer $j\leq i_{K}$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, and because $i\neq0$).
\end{itemize}
\par
Combining these two observations, we conclude that every sufficiently large
integer $j$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$ and that every
sufficiently small integer $j$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$.
Hence, only finitely many integers $v$ satisfy $\rho\left(  E_{j,j+i}\right)
v\neq0$. Thus, the sum $\sum\limits_{j\in\mathbb{Z}}a_{j}\rho\left(
E_{j,j+i}\right)  v$ has only finitely many nonzero addends, qed.} and thus
makes sense.

But when $i=0$, we run into a problem with this approach: $\rho\left(
\sum\limits_{j\in\mathbb{Z}}a_{j}E_{j,j}\right)  v=\sum\limits_{j\in
\mathbb{Z}}a_{j}\rho\left(  E_{j,j}\right)  v$ is an infinite sum which may
very well have infinitely many nonzero addends, and thus makes no sense.

To fix this problem, we define a map $\widehat{\rho}$ which will be a "small"
modification of $\rho$:

\begin{definition}
\label{def.glinf.rhohat.abar}Define a linear map $\widehat{\rho}%
:\overline{\mathfrak{a}_{\infty}}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ by%
\begin{align}
\widehat{\rho}\left(  \left(  a_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\right)   &  =\sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right. \label{def.glinf.rhohat.generalcase}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for every }\left(  a_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\overline{\mathfrak{a}_{\infty}%
}\right. \nonumber
\end{align}
(where $1$ means the endomorphism $\operatorname*{id}$ of $\wedge
^{\dfrac{\infty}{2},m}V$). Here, the infinite sum $\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  $ is well-defined as an endomorphism of $\wedge^{\dfrac{\infty}{2}%
,m}V$, because for every $v\in\wedge^{\dfrac{\infty}{2},m}V$, the sum
$\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v$ has only finitely many nonzero addends (as Proposition
\ref{prop.glinf.rhohat.welldef} shows).
\end{definition}

The map $\widehat{\rho}$ just defined does not extend the map $\rho$, but is
the unique continuous (in the sense explained above) extension of the map
$\widehat{\rho}\mid_{\mathfrak{gl}_{\infty}}$ to $\overline{\mathfrak{a}%
_{\infty}}$ as a linear map. The map $\widehat{\rho}\mid_{\mathfrak{gl}%
_{\infty}}$ is, in a certain sense, a "very close approximation to $\rho$", as
can be seen from the following remark:

\begin{remark}
\label{rmk.glinf.rhohat.abar}From Definition \ref{def.glinf.rhohat.abar}, it
follows that%
\begin{equation}
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)
\in\mathbb{Z}^{2}. \label{def.glinf.rhohat}%
\end{equation}

\end{remark}

We are not done yet: This map $\widehat{\rho}$ is not a representation of
$\overline{\mathfrak{a}_{\infty}}$. We will circumvent this by defining a
central extension $\mathfrak{a}_{\infty}$ of $\overline{\mathfrak{a}_{\infty}%
}$ for which the map $\widehat{\rho}$ (once suitably extended) will be a
representation. But first, let us show a lemma that we owe for the definition
of $\widehat{\rho}$:

\begin{proposition}
\label{prop.glinf.rhohat.welldef}Let $\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\in\overline{\mathfrak{a}_{\infty}}$ and
$v\in\wedge^{\dfrac{\infty}{2},m}V$. Then, the sum%
\[
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v
\]
has only finitely many nonzero addends.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.rhohat.welldef}.} We know that
$v$ is an element of $\wedge^{\dfrac{\infty}{2},m}V$. Hence, $v$ is a
$\mathbb{C}$-linear combination of elements of the form $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ being an $m$-degression (since $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$ is a basis of
$\wedge^{\dfrac{\infty}{2},m}V$). Hence, we can WLOG assume that $v$ is an
element of the form $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with
$\left(  i_{0},i_{1},i_{2},...\right)  $ being an $m$-degression (because the
claim of Proposition \ref{prop.glinf.rhohat.welldef} is clearly linear in
$v$). Assume this. Then, $v=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ for some $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
Consider this $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. By the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every integer $k\geq K$ satisfies $i_{k}+k=m$. Consider this $K$. Then,
every integer which is less or equal to $i_{K}$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $.

For every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, let $r_{i,j}$ be the map
$\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \in\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)
$. Then, the sum%
\[
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v
\]
clearly rewrites as $\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}a_{i,j}r_{i,j}v$. Hence, in order to prove Proposition
\ref{prop.glinf.rhohat.welldef}, we only need to prove that the sum
$\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}r_{i,j}v$ has only
finitely many nonzero addends.

Since $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of the
matrix $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ are
nonzero. In other words, there exists an $M\in\mathbb{N}$ such that%
\begin{equation}
\left(  \text{the }m\text{-th diagonal of the matrix }\left(  a_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\text{ is zero for every }%
m\in\mathbb{Z}\text{ such that }\left\vert m\right\vert \geq M\right)  .
\label{pf.glinf.rhohat.welldef.M}%
\end{equation}
Consider this $M$.

Now, we have the following three observations:

\begin{itemize}
\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $j>\max\left\{
i_{0},0\right\}  $ satisfies $r_{i,j}v=0\ \ \ \ $\footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ be such that $j>\max\left\{
i_{0},0\right\}  $. Then, $j>i_{0}$ and $j>0$. Since $j>0$, we cannot have
$i=j$ and $i\leq0$. Now, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  $ (since we cannot have $i=j$ and
$i\leq0$), so that $r_{i,j}v=\rho\left(  E_{i,j}\right)  v=0$ (because
$j>i_{0}$, so that the integer $j$ does not appear in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $), qed.} and thus $a_{i,j}%
\underbrace{r_{i,j}v}_{=0}=0$.

\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $i\leq
\min\left\{  i_{K},0\right\}  $ satisfies $r_{i,j}v=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\mathbb{Z}^{2}$
be such that $i\leq\min\left\{  i_{K},0\right\}  $. Then, $i\leq i_{K}$ and
$i\leq0$.
\par
Since $i\leq i_{K}$, the integer $i$ appears in the $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $ (because every integer which is less or equal
to $i_{K}$ appears in the $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $). We now must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq j$.
\par
\textit{Case 2:} We have $i=j$.
\par
Let us first consider Case 1. In this case, $i\neq j$. Thus, $\rho\left(
E_{i,j}\right)  v=0$ (because the integer $i$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, so that after applying $\rho\left(
E_{i,j}\right)  $ to $v=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$,
we obtain a wedge in which $v_{i}$ appears twice). On the other hand, $i\neq
j$, so that we cannot have $i=j$ and $i\leq0$. Now, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  $ (since we cannot have $i=j$ and
$i\leq0$), and thus $r_{i,j}v=\rho\left(  E_{i,j}\right)  v=0$.
\par
Now, let us consider Case 2. In this case, $i=j$. Thus, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  -1$ (since $i=j$ and $i\leq0$). Since
$E_{i,j}=E_{i,i}$ (because $j=i$), this rewrites as $r_{i,j}=\rho\left(
E_{i,i}\right)  -1$. On the other hand, the integer $i$ appears in the
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, so that $\rho\left(
E_{i,i}\right)  v=v$. Hence, from $r_{i,j}=\rho\left(  E_{i,i}\right)  -1$, we
get $r_{i,j}v=\left(  \rho\left(  E_{i,i}\right)  -1\right)
v=\underbrace{\rho\left(  E_{i,i}\right)  v}_{=v}-v=v-v=0$.
\par
Thus, in each of the cases 1 and 2, we have proven that $r_{i,j}v=0$. Hence,
$r_{i,j}v=0$ always holds, qed.} and thus $a_{i,j}\underbrace{r_{i,j}v}%
_{=0}=0$.

\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert
i-j\right\vert \geq M$ satisfies $a_{i,j}=0$\ \ \ \ \footnote{\textit{Proof.}
Let $\left(  u,v\right)  \in\mathbb{Z}^{2}$ be such that $\left\vert
u-v\right\vert \geq M$. Then, since $\left\vert v-u\right\vert =\left\vert
u-v\right\vert \geq M$, the $\left(  v-u\right)  $-th diagonal of the matrix
$\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is zero (by
(\ref{pf.glinf.rhohat.welldef.M}), applied to $m=v-u$), and thus $a_{u,v}=0$
(since $a_{u,v}$ is an entry on the $\left(  v-u\right)  $-th diagonal of the
matrix $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$). We
thus have shown that every $\left(  u,v\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert u-v\right\vert \geq M$ satisfies $a_{u,v}=0$. Renaming $\left(
u,v\right)  $ as $\left(  i,j\right)  $ in this fact, we obtain: Every
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert i-j\right\vert
\geq M$ satisfies $a_{i,j}=0$, qed.} and thus $\underbrace{a_{i,j}}%
_{=0}r_{i,j}v=0$.
\end{itemize}

Now, for any $\alpha\in\mathbb{Z}$ and $\beta\in\mathbb{Z}$, let $\left[
\alpha,\beta\right]  _{\mathbb{Z}}$ denote the set $\left\{  x\in
\mathbb{Z}\ \mid\ \alpha\leq x\leq\beta\right\}  $ (this set is finite). It is
easy to see that%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that }%
a_{i,j}r_{i,j}v\neq0\text{ satisfies}\\
\left(  i,j\right)  \in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{
i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}}\times\left[  \min\left\{
i_{K},0\right\}  -M+2,\max\left\{  i_{0},0\right\}  \right]  _{\mathbb{Z}}%
\end{array}
\right)  \label{pf.gli.rhohat.welldef.bnd}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gli.rhohat.welldef.bnd}):} Let $\left(
i,j\right)  \in\mathbb{Z}^{2}$ be such that $a_{i,j}r_{i,j}v\neq0$. Then, we
cannot have $j>\max\left\{  i_{0},0\right\}  $ (since every $\left(
i,j\right)  \in\mathbb{Z}^{2}$ such that $j>\max\left\{  i_{0},0\right\}  $
satisfies $a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq0$). In
other words, $j\leq\max\left\{  i_{0},0\right\}  $. Also, we cannot have
$i\leq\min\left\{  i_{K},0\right\}  $ (since every $\left(  i,j\right)
\in\mathbb{Z}^{2}$ such that $i\leq\min\left\{  i_{K},0\right\}  $ satisfies
$a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq0$). Thus, we have
$i>\min\left\{  i_{K},0\right\}  $, so that $i\geq\min\left\{  i_{K}%
,0\right\}  +1$ (since $i$ and $\min\left\{  i_{K},0\right\}  $ are integers).
Finally, we cannot have $\left\vert i-j\right\vert \geq M$ (since every
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert i-j\right\vert
\geq M$ satisfies $a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq
0$). Thus, we have $\left\vert i-j\right\vert <M$, so that $\left\vert
i-j\right\vert \leq M-1$ (since $\left\vert i-j\right\vert $ and $M$ are
integers). Thus, $i-j\leq\left\vert i-j\right\vert \leq M-1$. Hence,
$i\leq\underbrace{j}_{\leq\max\left\{  i_{0},0\right\}  }+M-1\leq\max\left\{
i_{0},0\right\}  +M-1$. Combined with $i\geq\min\left\{  i_{K},0\right\}  +1$,
this yields $i\in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{
i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}}$. From $i-j\leq M-1$, we also
obtain $j\geq\underbrace{i}_{\geq\min\left\{  i_{K},0\right\}  +1}-\left(
M-1\right)  \geq\min\left\{  i_{K},0\right\}  +1-\left(  M-1\right)
=\min\left\{  i_{K},0\right\}  -M+2$. Combined with $j\leq\max\left\{
i_{0},0\right\}  $, this yields $j\in\left[  \min\left\{  i_{K},0\right\}
-M+2,\max\left\{  i_{0},0\right\}  \right]  _{\mathbb{Z}}$. Combined with
$i\in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}
+M-1\right]  _{\mathbb{Z}}$, this yields $\left(  i,j\right)  \in\left[
\min\left\{  i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}  +M-1\right]
_{\mathbb{Z}}\times\left[  \min\left\{  i_{K},0\right\}  -M+2,\max\left\{
i_{0},0\right\}  \right]  _{\mathbb{Z}}$. This proves
(\ref{pf.gli.rhohat.welldef.bnd}).}. Since $\left[  \min\left\{
i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}%
}\times\left[  \min\left\{  i_{K},0\right\}  -M+2,\max\left\{  i_{0}%
,0\right\}  \right]  _{\mathbb{Z}}$ is a finite set, this shows that only
finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy $a_{i,j}%
r_{i,j}v\neq0$. In other words, the sum $\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}a_{i,j}r_{i,j}v$ has only finitely many nonzero addends.
This proves Proposition \ref{prop.glinf.rhohat.welldef}.

Our definition of $\widehat{\rho}$ is somewhat unwieldy, since computing
$\widehat{\rho}\left(  a\right)  v$ for a matrix $a\in\overline{\mathfrak{a}%
_{\infty}}$ and a $v\in\wedge^{\dfrac{\infty}{2},m}V$ using it requires
writing $v$ as a linear combination of elementary semiinfinite wedges.
However, since our $\widehat{\rho}$ only slightly differs from $\rho$, there
are many matrices $a$ for which $\widehat{\rho}\left(  a\right)  $ behaves
exactly as $\rho\left(  a\right)  $ would if we could extend $\rho$ to
$\overline{\mathfrak{a}_{\infty}}$:

\begin{proposition}
\label{prop.glinf.ainfact}Let $m\in\mathbb{Z}$. Let $a_{0},a_{1},a_{2},...$ be
vectors in $V$ which satisfy%
\[
a_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
Let $a\in\overline{\mathfrak{a}_{\infty}}$. Assume that, for every integer
$i\leq0$, the $\left(  i,i\right)  $-th entry of $a$ is $0$. Then,%
\[
\left(  \widehat{\rho}\left(  a\right)  \right)  \left(  a_{0}\wedge
a_{1}\wedge a_{2}\wedge...\right)  =\sum\limits_{k\geq0}a_{0}\wedge
a_{1}\wedge...\wedge a_{k-1}\wedge\left(  a\rightharpoonup a_{k}\right)
\wedge a_{k+1}\wedge a_{k+2}\wedge....
\]
In particular, the infinite sum $\sum\limits_{k\geq0}a_{0}\wedge a_{1}%
\wedge...\wedge a_{k-1}\wedge\left(  a\rightharpoonup a_{k}\right)  \wedge
a_{k+1}\wedge a_{k+2}\wedge...$ is well-defined (i. e., all but finitely many
integers $k\geq0$ satisfy $a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}%
\wedge\left(  a\rightharpoonup a_{k}\right)  \wedge a_{k+1}\wedge
a_{k+2}\wedge...=0$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.ainfact}.} For every $\left(
i,j\right)  \in\mathbb{Z}^{2}$, let $a_{i,j}$ be the $\left(  i,j\right)  $-th
entry of the matrix $a$. Then, $a=\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}=\sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}a_{i,j}E_{i,j}$. But every $\left(  i,j\right)  \in
\mathbb{Z}^{2}$ such that $i=j$ and $i\leq0$ satisfies $a_{i,j}=a_{i,i}=0$
(because we assumed that, for every integer $i\leq0$, the $\left(  i,i\right)
$-th entry of $a$ is $0$). Thus, $\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\i=j\text{ and }i\leq0}}\underbrace{a_{i,j}}_{=0}%
E_{i,j}=\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0E_{i,j}=0$, so that%
\[
a=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}E_{i,j}%
=\underbrace{\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0E_{i,j}}_{=0}+\sum\limits_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }%
i\leq0\right)  }}a_{i,j}E_{i,j}=\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}E_{i,j}.
\]


But from $a=\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$,
we have%
\begin{align*}
\widehat{\rho}\left(  a\right)   &  =\widehat{\rho}\left(  \left(
a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}\right)
=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{def.glinf.rhohat.generalcase})}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\i=j\text{
and }i\leq0}}\underbrace{a_{i,j}}_{=0}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  }_{\substack{=\rho\left(  E_{i,j}\right)  \\\text{(since we do not
have }\left(  i=j\text{ and }i\leq0\right)  \text{)}}}\\
&  =\underbrace{\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  }_{=0}+\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\rho\left(
E_{i,j}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\rho\left(  E_{i,j}\right)  ,
\end{align*}
so that%
\begin{align*}
&  \left(  \widehat{\rho}\left(  a\right)  \right)  \left(  a_{0}\wedge
a_{1}\wedge a_{2}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\underbrace{\rho\left(
E_{i,j}\right)  \left(  a_{0}\wedge a_{1}\wedge a_{2}\wedge...\right)
}_{\substack{=E_{i,j}\rightharpoonup\left(  a_{0}\wedge a_{1}\wedge
a_{2}\wedge...\right)  \\=\sum\limits_{k\geq0}a_{0}\wedge a_{1}\wedge...\wedge
a_{k-1}\wedge\left(  E_{i,j}\rightharpoonup a_{k}\right)  \wedge a_{k+1}\wedge
a_{k+2}\wedge...\\\text{(by Definition \ref{def.glinf.semiinfwedge})}}}\\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\sum\limits_{k\geq0}%
a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge\left(  E_{i,j}\rightharpoonup
a_{k}\right)  \wedge a_{k+1}\wedge a_{k+2}\wedge...\\
&  =\sum\limits_{k\geq0}a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}%
\wedge\underbrace{\left(  \sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}\left(  E_{i,j}\rightharpoonup a_{k}\right)  \right)  }%
_{\substack{=\left(  \sum\limits_{\substack{\left(  i,j\right)  \in
\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}E_{i,j}\right)  \rightharpoonup a_{k}=a\rightharpoonup a_{k}%
\\\text{(since }\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}%
E_{i,j}=a\text{)}}}\wedge a_{k+1}\wedge a_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we interchanged the summation signs; this is allowed because (as
the reader}\\
\text{can check) all but finitely many }\left(  \left(  i,j\right)  ,k\right)
\in\mathbb{Z}^{2}\times\mathbb{Z}\text{ satisfying }k\geq0\text{ and not}\\
\left(  i=j\text{ and }i\leq0\right)  \text{ satisfy }a_{i,j}\cdot a_{0}\wedge
a_{1}\wedge...\wedge a_{k-1}\wedge\left(  E_{i,j}\rightharpoonup a_{k}\right)
\wedge a_{k+1}\wedge a_{k+2}\wedge...=0
\end{array}
\right) \\
&  =\sum\limits_{k\geq0}a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge\left(
a\rightharpoonup a_{k}\right)  \wedge a_{k+1}\wedge a_{k+2}\wedge...
\end{align*}
(and en passant, this argument has shown that the infinite sum $\sum
\limits_{k\geq0}a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge\left(
a\rightharpoonup a_{k}\right)  \wedge a_{k+1}\wedge a_{k+2}\wedge...$ is
well-defined). This proves Proposition \ref{prop.glinf.ainfact}.

The issue that remains is that $\widehat{\rho}$ is not a representation of
$\overline{\mathfrak{a}_{\infty}}$. To mitigate this, we will define a central
extension of $\overline{\mathfrak{a}_{\infty}}$ by the so-called
\textit{Japanese cocycle}. Let us define this cocycle first:

\begin{theorem}
\label{thm.japan}For any $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\overline{\mathfrak{a}_{\infty}}$, we have $\widehat{\rho}\left(  \left[
A,B\right]  \right)  -\left[  \widehat{\rho}\left(  A\right)  ,\widehat{\rho
}\left(  B\right)  \right]  =\alpha\left(  A,B\right)  $ where $\alpha\left(
A,B\right)  $ is a scalar depending on $A$ and $B$ (thus a $2$-cocycle). This
$\alpha\left(  A,B\right)  $ can be computed as follows: Write $A$ and $B$ as
block matrices $A=\left(
\begin{array}
[c]{cc}%
A_{11} & A_{12}\\
A_{21} & A_{22}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
B_{11} & B_{12}\\
B_{21} & B_{22}%
\end{array}
\right)  $, where the blocks are separated as follows:

- The left blocks contain the $j$-th columns for all $j\leq0$; the right
blocks contain the $j$-th columns for all $j>0$.

- The upper blocks contain the $i$-th rows for all $i\leq0$; the lower blocks
contain the $i$-th rows for all $i>0$.

Then, $\alpha\left(  A,B\right)  =\operatorname*{Tr}\left(  -B_{12}%
A_{21}+A_{12}B_{21}\right)  $. (This trace makes sense because the matrices
$A_{12}$, $B_{21}$, $A_{21}$, $B_{12}$ have only finitely many nonzero entries.)
\end{theorem}

\begin{definition}
\label{def.japan}This $2$-cocycle $\alpha$ is called the \textit{Japanese
cocycle}.
\end{definition}

The proof of Theorem \ref{thm.japan} is a homework problem.

We are going to prove soon (Proposition \ref{prop.japan.nontr} and Corollary
\ref{cor.japan.triv}) that $\alpha$ is a nontrivial $2$-cocycle, but its
restriction to $\mathfrak{gl}_{\infty}$ is trivial. This is a strange
situation (given that $\mathfrak{gl}_{\infty}$ is a dense Lie subalgebra of
$\overline{\mathfrak{a}_{\infty}}$ with respect to a reasonably defined
topology), but we will later see the reason for this behavior.

\begin{corollary}
The $\alpha$ defined in Theorem \ref{thm.japan} is a $2$-cocycle on
$\overline{\mathfrak{a}_{\infty}}$.

We define $\mathfrak{a}_{\infty}$ as the $1$-dimensional central extension
$\widehat{\overline{\mathfrak{a}_{\infty}}}_{\alpha}$ of $\overline
{\mathfrak{a}_{\infty}}$ by $\mathbb{C}$ using this cocycle $\alpha$ (see
Definition \ref{def.centex} for what this means).
\end{corollary}

\begin{theorem}
Let us extend the linear map $\widehat{\rho}:\overline{\mathfrak{a}_{\infty}%
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
(introduced in Definition \ref{def.glinf.rhohat.abar}) to a linear map
$\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ by setting $\widehat{\rho}\left(
K\right)  =\operatorname*{id}$. (This makes sense since $\mathfrak{a}_{\infty
}=\overline{\mathfrak{a}_{\infty}}\oplus\mathbb{C}K$ as vector spaces.) Then,
this map $\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}%
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ is a representation of
$\mathfrak{a}_{\infty}$.

Thus, $\wedge^{\dfrac{\infty}{2},m}V$ becomes an $\mathfrak{a}_{\infty}$-module.
\end{theorem}

\begin{definition}
\label{def.ainf.grade2}Since $\mathfrak{a}_{\infty}=\overline{\mathfrak{a}%
_{\infty}}\oplus\mathbb{C}K$ as vector space, we can define a grading on
$\mathfrak{a}_{\infty}$ as the direct sum of the grading on $\overline
{\mathfrak{a}_{\infty}}$ (which was defined in Definition \ref{def.ainf.grade}%
) and the trivial grading on $\mathbb{C}K$ (that is the grading which puts $K$
in degree $0$). This is easily seen to make $\mathfrak{a}_{\infty}$ a
$\mathbb{Z}$-graded Lie algebra.
\end{definition}

\begin{proposition}
Let $m\in\mathbb{Z}$. With the grading defined in Definition
\ref{def.ainf.grade2}, the $\mathfrak{a}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$ is graded.
\end{proposition}

\begin{corollary}
\label{cor.japan.triv}The restriction of $\alpha$ to $\mathfrak{gl}_{\infty
}\times\mathfrak{gl}_{\infty}$ is a $2$-coboundary.
\end{corollary}

\textit{Proof of Corollary \ref{cor.japan.triv}.} Let $J$ be the block matrix
$\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & -I_{\infty}%
\end{array}
\right)  \in\overline{\mathfrak{a}_{\infty}}$, where the blocks are separated
in the same way as in Theorem \ref{thm.japan}. Define a linear map
$f:\mathfrak{gl}_{\infty}\rightarrow\mathbb{C}$ by
\[
\left(  f\left(  A\right)  =\operatorname*{Tr}\left(  JA\right)
\ \ \ \ \ \ \ \ \ \ \text{for any }A\in\mathfrak{gl}_{\infty}\right)
\]
\footnote{Note that $\operatorname*{Tr}\left(  JA\right)  $ is well-defined
for every $A\in\mathfrak{gl}_{\infty}$, since Remark \ref{rmk.ainf.mult}
\textbf{(b)} (applied to $J$ and $A$ instead of $A$ and $B$) yields that
$JA\in\mathfrak{gl}_{\infty}$.}. Then, any $A\in\mathfrak{gl}_{\infty}$ and
$B\in\mathfrak{gl}_{\infty}$ satisfy $\alpha\left(  A,B\right)  =f\left(
\left[  A,B\right]  \right)  $. This is because (for any $A\in\mathfrak{gl}%
_{\infty}$ and $B\in\mathfrak{gl}_{\infty}$) we can write the matrix $\left[
A,B\right]  $ in the form $\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
\ast & \ast\\
\ast & \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}A_{12}%
\end{array}
\right)  $ (where asterisks mean blocks which we don't care about), so that
$J\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
0 & 0\\
\ast & -\left(  \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}%
A_{12}\right)
\end{array}
\right)  $ and thus
\begin{align*}
&  \operatorname*{Tr}\left(  J\left[  A,B\right]  \right) \\
&  =-\operatorname*{Tr}\left(  \left[  A_{22},B_{22}\right]  +A_{21}%
B_{12}-B_{21}A_{12}\right)  =-\underbrace{\operatorname*{Tr}\left[
A_{22},B_{22}\right]  }_{=0}-\underbrace{\operatorname*{Tr}\left(
A_{21}B_{12}\right)  }_{=\operatorname*{Tr}\left(  B_{12}A_{21}\right)
}+\underbrace{\operatorname*{Tr}\left(  B_{21}A_{12}\right)  }%
_{=\operatorname*{Tr}\left(  A_{12}B_{21}\right)  }\\
&  =-\operatorname*{Tr}\left(  B_{12}A_{21}\right)  +\operatorname*{Tr}\left(
A_{12}B_{21}\right)  =\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}%
B_{21}\right)  =\alpha\left(  A,B\right)  .
\end{align*}
The proof of Corollary \ref{cor.japan.triv} is thus finished.

But note that this proof does not extend to $\overline{\mathfrak{a}_{\infty}}%
$, because $f$ does not continuously extend to $\overline{\mathfrak{a}%
_{\infty}}$ (for any reasonable notion of continuity).

\begin{proposition}
\label{prop.japan.nontr}The $2$-cocycle $\alpha$ itself is not a $2$-coboundary.
\end{proposition}

\textit{Proof of Proposition \ref{prop.japan.nontr}.} Let $T$ be the shift
operator defined above. The span $\left\langle T^{j}\ \mid\ j\in
\mathbb{Z}\right\rangle $ is an abelian Lie subalgebra of $\overline
{\mathfrak{a}_{\infty}}$ (isomorphic to the abelian Lie algebra $\mathbb{C}%
\left[  t,t^{-1}\right]  $, and to the quotient $\overline{\mathcal{A}}$ of
the Heisenberg algebra $\mathcal{A}$ by its central subalgebra $\left\langle
K\right\rangle $). Any $2$-coboundary must become zero when restricted onto an
abelian Lie subalgebra. But the $2$-cocycle $\alpha$, restricted onto the span
$\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $, does not become $0$,
since%
\[
\alpha\left(  T^{i},T^{j}\right)  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq-j;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }i=-j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\mathbb{Z}.
\]
Proposition \ref{prop.japan.nontr} is thus proven.

In this proof, we have constructed an embedding $\overline{\mathcal{A}%
}\rightarrow\overline{\mathfrak{a}_{\infty}}$ which sends $\overline{a_{j}}$
to $T^{j}$ for every $j\in\mathbb{Z}$. This embedding is crucial to what we
are going to do, so let us give it a formal definition:

\begin{definition}
\label{def.ainf.A}The map%
\[
\overline{\mathcal{A}}\rightarrow\overline{\mathfrak{a}_{\infty}%
},\ \ \ \ \ \ \ \ \ \ a_{j}\mapsto T^{j}%
\]
(where $\overline{\mathcal{A}}$ is the quotient of the Heisenberg algebra
$\mathcal{A}$ by its central subalgebra $\left\langle K\right\rangle $) is an
embedding of Lie algebras. We will regard this embedding as an inclusion, and
thus we will regard $\overline{\mathcal{A}}$ as a Lie subalgebra of
$\overline{\mathfrak{a}_{\infty}}$.

This embedding is easily seen to give rise to an embedding $\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}$ of Lie algebras which sends $K$ to $K$ and
sends $a_{j}$ to $T^{j}$ for every $j\in\mathbb{Z}$. This embedding will also
be regarded as an inclusion, so that $\mathcal{A}$ will be considered as a Lie
subalgebra of $\mathfrak{a}_{\infty}$.
\end{definition}

It is now easy to see:

\begin{proposition}
Extend our map $\widehat{\rho}:\overline{\mathfrak{a}_{\infty}}\rightarrow
\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ to a map
$\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  $, also denoted by $\widehat{\rho}$, by
setting $\widehat{\rho}\left(  K\right)  =\operatorname*{id}$. Then, this map
$\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ is a Lie algebra homomorphism, i. e.,
it makes $\wedge^{\dfrac{\infty}{2},m}V$ into an $\mathfrak{a}_{\infty}%
$-module. The element $K$ of $\mathfrak{a}_{\infty}$ acts as
$\operatorname*{id}$ on this module.

By means of the embedding $\mathcal{A}\rightarrow\mathfrak{a}_{\infty}$, this
$\mathfrak{a}_{\infty}$-module gives rise to an $\mathcal{A}$-module
$\wedge^{\dfrac{\infty}{2},m}V$, on which $K$ acts as $\operatorname*{id}$.
\end{proposition}

In Proposition \ref{prop.Lomegam}, we identified $\wedge^{\dfrac{\infty}{2}%
,m}V$ as an irreducible highest-weight $\mathfrak{gl}_{\infty}$-module;
similarly, we can identify it as an irreducible highest-weight $\mathfrak{a}%
_{\infty}$-module:

\begin{proposition}
\label{prop.Lomegam.a}Let $m\in\mathbb{Z}$. Let $\overline{\omega}_{m}$ be the
$\mathbb{C}$-linear map $\mathfrak{a}_{\infty}\left[  0\right]  \rightarrow
\mathbb{C}$ which sends every infinite diagonal matrix $\operatorname*{diag}%
\left(  ...,d_{-2},d_{-1},d_{0},d_{1},d_{2},...\right)  \in\overline
{\mathfrak{a}_{\infty}}$ to $\left\{
\begin{array}
[c]{c}%
\sum\limits_{j=1}^{m}d_{j},\ \ \ \ \ \ \ \ \ \ \text{if }m\geq0;\\
-\sum\limits_{j=m+1}^{0}d_{j},\ \ \ \ \ \ \ \ \ \ \text{if }m<0
\end{array}
\right.  $, and sends $K$ to $1$. Then, the graded $\mathfrak{a}_{\infty}%
$-module $\wedge^{\dfrac{\infty}{2},m}V$ is the irreducible highest-weight
representation $L_{\overline{\omega}_{m}}$ of $\mathfrak{a}_{\infty}$ with
highest weight $L_{\overline{\omega}_{m}}$. Moreover, $L_{\overline{\omega
}_{m}}$ is unitary.
\end{proposition}

\begin{remark}
Note the analogy between the weight $\overline{\omega}_{m}$ in Proposition
\ref{prop.Lomegam.a} and the weight $\omega_{m}$ in Proposition
\ref{prop.Lomegam}: The weight $\omega_{m}$ in Proposition \ref{prop.Lomegam}
sends every diagonal matrix $\operatorname*{diag}\left(  ...,d_{-2}%
,d_{-1},d_{0},d_{1},d_{2},...\right)  \in\mathfrak{gl}_{\infty}$ to
$\sum\limits_{j=-\infty}^{m}d_{j}$. Note that this sum $\sum\limits_{j=-\infty
}^{m}d_{j}$ is well-defined (because for a diagonal matrix
$\operatorname*{diag}\left(  ...,d_{-2},d_{-1},d_{0},d_{1},d_{2},...\right)  $
to lie in $\mathfrak{gl}_{\infty}$, it has to satisfy $d_{j}=0$ for all but
finitely many $j\in\mathbb{Z}$).
\end{remark}

In analogy to Corollary \ref{cor.lomegam.unit}, we can also show:

\begin{corollary}
\label{cor.lomegam.unit.a}For every finite sum $\sum\limits_{i\in\mathbb{Z}%
}k_{i}\overline{\omega}_{i}$ with $k_{i}\in\mathbb{N}$, the representation
$L_{\sum\limits_{i\in\mathbb{Z}}k_{i}\overline{\omega}_{i}}$ of $\mathfrak{a}%
_{\infty}$ is unitary.
\end{corollary}

\subsection{Virasoro actions on $\wedge^{\dfrac{\infty}{2},m}V$}

We can also embed the Virasoro algebra $\operatorname*{Vir}$ into
$\mathfrak{a}_{\infty}$, and not just in one way, but in infinitely many ways
depending on two parameters:

\begin{proposition}
Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Let the
$\operatorname*{Vir}$-module $V_{\alpha,\beta}$ be defined as in Proposition
\ref{prop.Vab.1}.

For every $k\in\mathbb{Z}$, let $v_{k}=z^{-k+\alpha}\left(  dz\right)
^{\beta}\in V_{\alpha,\beta}$. Here, for any $\ell\in\mathbb{Z}$, the term
$z^{\ell+\alpha}\left(  dz\right)  ^{\beta}$ denotes $z^{\ell}z^{\alpha
}\left(  dz\right)  ^{\beta}$.

According to Proposition \ref{prop.Vab.1} \textbf{(b)}, every $m\in\mathbb{Z}$
satisfies%
\[
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\]
Thus, if we write $L_{m}$ as a matrix with respect to the basis $\left(
v_{k}\right)  _{k\in\mathbb{Z}}$ of $V_{\alpha,\beta}$, then this matrix lies
in $\overline{\mathfrak{a}_{\infty}}$ (in fact, its only nonzero diagonal is
the $m$-th one).

This defines an injective map $\overline{\varphi_{\alpha,\beta}}%
:W\rightarrow\overline{\mathfrak{a}_{\infty}}$, which sends every $L_{m}\in W$
to the matrix representing the action of $L_{m}$ on $V_{\alpha,\beta}$. This
map $\overline{\varphi_{\alpha,\beta}}$ lifts to an injective map
$\widehat{W}\rightarrow\mathfrak{a}_{\infty}$, where $\widehat{W}$ is defined
as follows: Let $\widetilde{\alpha}:\overline{\mathfrak{a}_{\infty}}%
\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ be the Japanese
cocycle (this cocycle has been called $\alpha$ in Definition \ref{def.japan},
but here we use the letter $\alpha$ for something different), and let
$\widetilde{\alpha}^{\prime}:W\times W\rightarrow\mathbb{C}$ be the
restriction of this Japanese cocycle $\widetilde{\alpha}:\overline
{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow
\mathbb{C}$ to $W\times W$ via the map $\overline{\varphi_{\alpha,\beta}%
}\times\overline{\varphi_{\alpha,\beta}}:W\times W\rightarrow\overline
{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}}$. Then,
$\widehat{W}$ denotes the central extension of $W$ defined by the $2$-cocycle
$\widetilde{\alpha}^{\prime}$.

But let us now compute $\widetilde{\alpha}^{\prime}$ and $\widehat{W}$. In
fact, from a straightforward calculation (Homework Set 4 exercise 3) it
follows that%
\[
\widetilde{\alpha}^{\prime}\left(  L_{m},L_{n}\right)  =\delta_{n,-m}\left(
\dfrac{n^{3}-n}{12}c_{\beta}+2nh_{\alpha,\beta}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z},
\]
where
\[
c_{\beta}=-12\beta^{2}+12\beta-2\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ h_{\alpha,\beta}=\dfrac{1}{2}\alpha\left(  \alpha
+2\beta-1\right)  .
\]
Thus, the $2$-cocycle $\widetilde{\alpha}^{\prime}$ differs from the
$2$-cocycle $\omega$ (defined in Theorem \ref{thm.H^2(W)}) merely by a
multiplicative factor ($\dfrac{c_{\beta}}{2}$) and a $2$-coboundary (which
sends every $\left(  L_{m},L_{n}\right)  $ to $\delta_{n,-m}\cdot
2nh_{\alpha,\beta}$). Thus, the central extension $\widehat{W}$ of $W$ defined
by the $2$-cocycle $\widetilde{\alpha}^{\prime}$ is isomorphic (as a Lie
algebra) to the central extension of $W$ defined by the $2$-cocycle $\omega$,
that is, to the Virasoro algebra $\operatorname*{Vir}$. This turns the Lie
algebra homomorphism $\widehat{W}\rightarrow\mathfrak{a}_{\infty}$ into a
homomorphism $\operatorname*{Vir}\rightarrow\mathfrak{a}_{\infty}$. Let us
describe this homomorphism explicitly:

Let $\widehat{L_{0}}$ be the element $\overline{\varphi_{\alpha,\beta}}\left(
L_{0}\right)  +h_{\alpha,\beta}K\in\mathfrak{a}_{\infty}$. Then, the linear
map%
\begin{align*}
\operatorname*{Vir}  &  \rightarrow\mathfrak{a}_{\infty},\\
L_{n}  &  \mapsto\overline{\varphi_{\alpha,\beta}}\left(  L_{n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for }n\neq0,\\
L_{0}  &  \mapsto\widehat{L_{0}},\\
C  &  \mapsto c_{\beta}K
\end{align*}
is a Lie algebra homomorphism. Denote this map by $\varphi_{\alpha,\beta}$. By
means of this homomorphism, we can restrict the $\mathfrak{a}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2},m}V$ to a $\operatorname*{Vir}$-module. Denote this
$\operatorname*{Vir}$-module by $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}%
$. Note that $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is a Virasoro
module with central charge $c=c_{\beta}$. This $\wedge^{\dfrac{\infty}{2}%
,m}V_{\alpha,\beta}$ is called the \textit{module of semiinfinite forms}. The
vector $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ has highest
degree (namely, $0$).

We have $L_{i}\psi_{m}=0$ for $i>0$, and we have $L_{0}\psi_{m}=\dfrac{1}%
{2}\left(  \alpha-m\right)  \left(  \alpha+2\beta-1-m\right)  \psi_{m}$.
(Proof: Homework exercise.)
\end{proposition}

\begin{corollary}
Let $\alpha,\beta\in\mathbb{C}$. We have a homomorphism%
\begin{align*}
M_{\lambda}  &  \rightarrow\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta},\\
v_{\lambda}  &  \mapsto\psi_{m}%
\end{align*}
of Virasoro modules, where%
\[
\lambda=\left(  \dfrac{1}{2}\left(  \alpha-m\right)  \left(  \alpha
+2\beta-1-m\right)  ,-12\beta^{2}+12\beta-2\right)  .
\]

\end{corollary}

We will see that this is an isomorphism for generic $\lambda$. For concrete
$\lambda$ it is not always one, and can have a rather complicated kernel.

\subsection{The dimensions of the graded components of $\wedge^{\dfrac{\infty
}{2},m}V$}

Fix $m\in\mathbb{Z}$. We already know from Definition
\ref{def.glinf.wedge.grading} that $\wedge^{\dfrac{\infty}{2},m}V$ is a graded
$\mathbb{C}$-vector space. More concretely,%
\[
\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\geq0}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  ,
\]
where every $d\geq0$ satisfies%
\[
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  =\left\langle
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ \sum\limits_{k\geq
0}\left(  i_{k}+k-m\right)  =d\right\rangle .
\]


We also know that the $m$-degressions are in a 1-to-1 correspondence with the
partitions. This correspondence maps any $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ to the partition $\left(  i_{k}+k-m\right)
_{k\geq0}$; this is a partition of the integer $\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  $. As a consequence, for every integer $d\geq0$, the
$m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfying
$\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)  =d$ are in 1-to-1
correspondence with the partitions of $d$. Hence, for every integer $d\geq0$,
the number of all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfying $\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)  =d$ equals the
number of the partitions of $d$. Thus, for every integer $d\geq0$, we have%
\begin{align*}
&  \dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right) \\
&  =\left(  \text{the number of }m\text{-degressions }\left(  i_{0}%
,i_{1},i_{2},...\right)  \text{ satisfying}\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  =d\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression satisfying }\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)
=d}\\
\text{ is a basis of }\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]
\end{array}
\right) \\
&  =\left(  \text{the number of partitions of }d\right)  =p\left(  d\right)  ,
\end{align*}
where $p$ is the partition function. Hence:

\begin{proposition}
\label{prop.wedge.genfun}Let $m\in\mathbb{Z}$. Every integer $d\geq0$
satisfies $\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right)  =p\left(  d\right)  $, where $d$ is the partition
function. As a consequence, in the ring of formal power series $\mathbb{C}%
\left[  \left[  q\right]  \right]  $, we have%
\[
\sum\limits_{d\geq0}\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)
\left[  -d\right]  \right)  q^{d}=\sum\limits_{d\geq0}p\left(  d\right)
q^{d}=\dfrac{1}{\left(  1-q\right)  \left(  1-q^{2}\right)  \left(
1-q^{3}\right)  \cdots}.
\]

\end{proposition}

\subsection{The Boson-Fermion correspondence}

\begin{proposition}
\label{prop.wedge.fock}Let $m\in\mathbb{Z}$.

\textbf{(a)} As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}{2},m}V$ is
isomorphic to the Fock module $F_{m}$. More precisely, there exists a graded
$\mathcal{A}$-module isomorphism $\widetilde{\sigma}_{m}:F_{m}\rightarrow
\wedge^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such that
$\widetilde{\sigma}_{m}\left(  1\right)  =\psi_{m}$.

\textbf{(b)} As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}{2},m}V$ is
isomorphic to the Fock module $\widetilde{F}_{m}$. More precisely, there
exists a graded $\mathcal{A}$-module isomorphism $\sigma_{m}:\widetilde{F}%
_{m}\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such
that $\sigma_{m}\left(  1\right)  =\psi_{m}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.wedge.fock}.} \textbf{(a)} Let us first
notice that in the ring $\mathbb{C}\left[  \left[  q\right]  \right]  $, we
have
\begin{align*}
\sum\limits_{d\geq0}\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)
\left[  -d\right]  \right)  q^{d}  &  =\dfrac{1}{\left(  1-q\right)  \left(
1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}\ \ \ \ \ \ \ \ \ \ \left(
\text{by Proposition \ref{prop.wedge.genfun}}\right) \\
&  =\sum\limits_{n\geq0}\dim\left(  \underbrace{F}_{\substack{=F_{m}%
\\\text{(as vector spaces)}}}\left[  -n\right]  \right)  q^{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Definition \ref{def.fock.grad}}\right) \\
&  =\sum\limits_{n\geq0}\dim\left(  F_{m}\left[  -n\right]  \right)
q^{n}=\sum\limits_{d\geq0}\dim\left(  F_{m}\left[  -d\right]  \right)  q^{d}.
\end{align*}
By comparing coefficients, this yields that every integer $d\geq0$ satisfies
\begin{equation}
\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]
\right)  =\dim\left(  F_{m}\left[  -d\right]  \right)  .
\label{pf.wedge.fock.dim}%
\end{equation}


We have $a_{i}\psi_{m}=0$ for all $i>0$ (by degree considerations), and we
also have $K\psi_{m}=\psi_{m}$. Besides, it is easy to see that $a_{0}\psi
_{m}=m\psi_{m}$\ \ \ \ \footnote{\textit{Proof.} The embedding $\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}$ sends $a_{0}$ to $T^{0}=\mathbf{1}$, where
$\mathbf{1}$ denotes the identity matrix in $\mathfrak{a}_{\infty}$. Thus,
$a_{0}\psi_{m}=\mathbf{1}\psi_{m}$. (Note that $\mathbf{1}\psi_{m}$ needs not
equal $\psi_{m}$ in general, since the action of $\mathfrak{a}_{\infty}$ on
$\wedge^{\dfrac{\infty}{2},m}V$ is not an associative algebra action, but just
a Lie algebra action.) Recall that $\wedge^{\dfrac{\infty}{2},m}V$ became an
$\mathfrak{a}_{\infty}$-module via the map $\widehat{\rho}$, so that
$U\psi_{m}=\widehat{\rho}\left(  U\right)  \psi_{m}$ for every $U\in
\mathfrak{a}_{\infty}$. Now,%
\begin{align*}
a_{0}\psi_{m}  &  =\mathbf{1}\psi_{m}=\sum\limits_{i\in\mathbb{Z}}E_{i,i}%
\psi_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{1}=\sum
\limits_{i\in\mathbb{Z}}E_{i,i}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\underbrace{\widehat{\rho}\left(
E_{i,i}\right)  }_{\substack{=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  \\\text{(by the definition of }\widehat{\rho}\text{)}}%
}\underbrace{\psi_{m}}_{=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge
v_{1}\wedge v_{0}\wedge v_{-1}\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U\psi_{m}=\widehat{\rho}\left(
U\right)  \psi_{m}\text{ for every }U\in\mathfrak{a}_{\infty}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge
v_{0}\wedge v_{-1}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  }_{=\rho\left(  E_{i,i}\right)  }\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  }_{=\rho\left(  E_{i,i}\right)  -1}\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\rho\left(  E_{i,i}\right)
\cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge
v_{-1}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}\wedge....
\end{align*}
\par
Now, we distinguish between two cases:
\par
\textit{Case 1:} We have $m\geq0$.
\par
\textit{Case 2:} We have $m<0$.
\par
In Case 1, we have%
\begin{align*}
a_{0}\psi_{m}  &  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\rho\left(
E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge
v_{1}\wedge v_{0}\wedge v_{-1}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}%
\wedge....\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}\underbrace{\rho\left(
E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge
v_{1}\wedge v_{0}\wedge v_{-1}\wedge...}_{\substack{=0\\\text{(since }i\text{
does not appear in the }m\text{-degression }\left(  m,m-1,m-2,...\right)
\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq
m}}\underbrace{\rho\left(  E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}\wedge...}%
_{\substack{=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge
v_{0}\wedge v_{-1}\wedge...\\\text{(since }i\text{ appears in the
}m\text{-degression }\left(  m,m-1,m-2,...\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\underbrace{\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot
v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge
v_{-1}\wedge...}_{\substack{=0\\\text{(since }i\text{ appears in the
}m\text{-degression }\left(  m,m-1,m-2,...\right)  \\\text{and thus we have
}\rho\left(  E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}%
\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}\wedge...\\=v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge v_{-1}%
\wedge...\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since we are in Case 1, so that }%
m\geq0\right) \\
&  =\underbrace{\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}0}%
_{=0}+\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}\underbrace{v_{m}%
\wedge v_{m-1}\wedge v_{m-2}\wedge...\wedge v_{1}\wedge v_{0}\wedge
v_{-1}\wedge...}_{=\psi_{m}}+\underbrace{\sum\limits_{\substack{i\in
\mathbb{Z};\\i\leq0}}0}_{=0}\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}\psi_{m}=m\psi_{m}.
\end{align*}
Hence, $a_{0}\psi_{m}=m\psi_{m}$ is proven in Case 1. In Case 2, the proof of
$a_{0}\psi_{m}=m\psi_{m}$ is similar (but instead of splitting the
$\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}$ sum into a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}$ and a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}$ sum, we must now split
the $\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}$ sum into a
$\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0;\ i>m}}$ and a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0;\ i\leq m}}$ sum). Thus, $a_{0}%
\psi_{m}=m\psi_{m}$ holds in both cases 1 and 2. In other words, the proof of
$a_{0}\psi_{m}=m\psi_{m}$ is complete.}.

Hence, Lemma \ref{lem.V=F.A} (applied to $m$ and $\wedge^{\dfrac{\infty}{2}%
,m}V$ instead of $\mu$ and $V$) yields that there exists a homomorphism
$\widetilde{\sigma}_{m}:F_{m}\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ of
$\mathcal{A}$-modules such that $\widetilde{\sigma}_{m}\left(  1\right)
=\psi_{m}$. (An alternative way to prove the existence of this
$\widetilde{\sigma}_{m}$ would be to apply Lemma \ref{lem.singvec}, making use
of the fact (Proposition \ref{prop.fockverma.A}) that $F_{m}$ is a Verma
module for $\mathcal{A}$.)

This $\widetilde{\sigma}_{m}$ is injective (since $F_{m}$ is irreducible) and
preserves grading. Hence, for every integer $d\geq0$, it induces a
homomorphism from $F_{m}\left[  -d\right]  $ to $\left(  \wedge^{\dfrac
{\infty}{2},m}V\right)  \left[  -d\right]  $. This induced homomorphism must
be injective (since $\widetilde{\sigma}_{m}$ was injective), and thus is an
isomorphism (since the vector spaces $F_{m}\left[  -d\right]  $ and $\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  $ have the same
dimension (by (\ref{pf.wedge.fock.dim})) and are both finite-dimensional).
Since this holds for every integer $d\geq0$, this yields that
$\widetilde{\sigma}_{m}$ itself must be an isomorphism. This proves
Proposition \ref{prop.wedge.fock} \textbf{(a)}.

Proposition \ref{prop.wedge.fock} \textbf{(b)} follows from Proposition
\ref{prop.wedge.fock} \textbf{(a)} due to Proposition \ref{prop.resc}
\textbf{(b)}.

Note that Proposition \ref{prop.wedge.fock} is surprising: It gives an
isomorphism between a space of polynomials (the Fock space $F_{m}$, also
called a \textit{bosonic space}) and a space of wedge products (the space
$\wedge^{\dfrac{\infty}{2},m}V$, also called a \textit{fermionic space});
isomorphisms like this are unheard of in finite-dimensional contexts.

\begin{definition}
We write $\mathcal{B}^{\left(  m\right)  }$ for the $\mathcal{A}$-module
$\widetilde{F}_{m}$. We write $\mathcal{B}$ for the $\mathcal{A}$-module
$\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }=\bigoplus\limits_{m}%
\widetilde{F}_{m}$. We write $\mathcal{F}^{\left(  m\right)  }$ for the
$\mathcal{A}$-module $\wedge^{\dfrac{\infty}{2},m}V$. We write $\mathcal{F}$
for the $\mathcal{A}$-module $\bigoplus\limits_{m}\mathcal{F}^{\left(
m\right)  }$.

The isomorphism $\sigma_{m}$ (constructed in Proposition \ref{prop.wedge.fock}
\textbf{(b)}) is thus an isomorphism $\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{F}^{\left(  m\right)  }$. We write $\sigma$ for the
$\mathcal{A}$-module isomorphism $\bigoplus\limits_{m}\sigma_{m}%
:\mathcal{B}\rightarrow\mathcal{F}$. This $\sigma$ is called the
\textit{Boson-Fermion Correspondence}.
\end{definition}

Note that we can do the same for the Virasoro algebra: If $M_{\lambda}$ is
irreducible, then the homomorphism $M_{\lambda}\rightarrow\wedge
^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is an isomorphism. And we know that
$\operatorname*{Vir}$ is nondegenerate, so $M_{\lambda}$ is irreducible for
Weil-generic $\lambda$.

\begin{corollary}
For generic $\alpha$ and $\beta$, the $\operatorname*{Vir}$-module
$\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is irreducible.
\end{corollary}

But now, back to the Boson-Fermion Correspondence:

Both $\mathcal{B}$ and $\mathcal{F}$ are $\mathcal{A}$-modules, and
Proposition \ref{prop.wedge.fock} \textbf{(b)} showed us that they are
isomorphic as such through the isomorphism $\sigma:\mathcal{B}\rightarrow
\mathcal{F}$. However, $\mathcal{F}$ is also an $\mathfrak{a}_{\infty}%
$-module, whereas $\mathcal{B}$ is not. But of course, with the isomorphism
$\sigma$ being given, we can transfer the $\mathfrak{a}_{\infty}$-module
structure from $\mathcal{F}$ to $\mathcal{B}$. The same can be done with the
$\mathfrak{gl}_{\infty}$-module structure. Let us explicitly define these:

\begin{definition}
\textbf{(a)} We make $\mathcal{B}$ into an $\mathfrak{a}_{\infty}$-module by
transferring the $\mathfrak{a}_{\infty}$-module structure on $\mathcal{F}$
(given by the map $\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow
\operatorname*{End}\mathcal{F}$) to $\mathcal{B}$ via the isomorphism
$\sigma:\mathcal{B}\rightarrow\mathcal{F}$. Note that the $\mathcal{A}$-module
$\mathcal{B}$ is a restriction of the $\mathfrak{a}_{\infty}$-module
$\mathcal{B}$ (since the $\mathcal{A}$-module $\mathcal{F}$ is the restriction
of the $\mathfrak{a}_{\infty}$-module $\mathcal{F}$.) We denote the
$\mathfrak{a}_{\infty}$-module structure on $\mathcal{B}$ by $\widehat{\rho
}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\mathcal{B}$.

\textbf{(b)} We make $\mathcal{B}$ into a $\mathfrak{gl}_{\infty}$-module by
transferring the $\mathfrak{gl}_{\infty}$-module structure on $\mathcal{F}$
(given by the map $\rho:\mathfrak{gl}_{\infty}\rightarrow\operatorname*{End}%
\mathcal{F}$) to $\mathcal{B}$ via the isomorphism $\sigma:\mathcal{B}%
\rightarrow\mathcal{F}$. We denote the $\mathfrak{gl}_{\infty}$-module
structure on $\mathcal{B}$ by $\rho:\mathfrak{gl}_{\infty}\rightarrow
\operatorname*{End}\mathcal{B}$.
\end{definition}

How do we describe these module structures on $\mathcal{B}$ explicitly (i. e.,
in formulas?) This question is answered using the so-called \textit{vertex
operator construction}.

But first, some easier things:

\begin{definition}
\label{def.createdestroy}Let $m\in\mathbb{Z}$. Let $i\in\mathbb{Z}$.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}}:\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}%
^{\left(  m+1\right)  }$ by%
\[
\widehat{v_{i}}\cdot\psi=v_{i}\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all
}\psi\in\mathcal{F}^{\left(  m\right)  }.
\]
Here, $v_{i}\wedge\psi$ is formally defined as follows: Write $\psi$ as a
$\mathbb{C}$-linear combination of (well-defined) semiinfinite wedge products
$a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ (for instance, elementary
semiinfinite wedges); then, $v_{i}\wedge\psi$ is obtained by replacing each
such product $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ by $v_{i}\wedge
a_{0}\wedge a_{1}\wedge a_{2}\wedge...$.

\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}}:\mathcal{F}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m-1\right)  }$ as follows:

For every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, we let
$\overset{\vee}{v_{i}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\wedge v_{i_{j-1}}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},i_{2},...\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $i_{k}=i$. Thus, the map $\overset{\vee
}{v_{i}}$ is defined on all elementary semiinfinite wedges; we extend this to
a map $\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(
m-1\right)  }$ by linearity.
\end{definition}

Note that the somewhat unwieldy definition of $\overset{\vee}{v_{i}}$ can be
slightly improved: While it only gave a formula for $m$-degressions, it is
easy to see that the same formula holds for straying $m$-degressions:

\begin{proposition}
Let $m\in\mathbb{Z}$ and $i\in\mathbb{Z}$. Let $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ be a straying $m$-degression which has no two equal
elements. Then,%
\begin{align*}
&  \overset{\vee}{v_{i}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\wedge v_{i_{j-1}}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},i_{2},...\right\}
\end{array}
\right.  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $i_{k}=i$.
\end{proposition}

These operators satisfy the relations%
\begin{align*}
\widehat{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\widehat{v_{i}}  &
=0,\ \ \ \ \ \ \ \ \ \ \overset{\vee}{v_{i}}\overset{\vee}{v_{j}%
}+\overset{\vee}{v_{j}}\overset{\vee}{v_{i}}=0,\\
\overset{\vee}{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\overset{\vee}{v_{i}}  &
=\delta_{i,j}%
\end{align*}
for all $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$.

\begin{definition}
For every $i\in\mathbb{Z}$, define $\xi_{i}=\widehat{v_{i}}$ and $\xi
_{i}^{\ast}=\overset{\vee}{v_{i}}$.
\end{definition}

Then, all $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $\rho\left(
E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}$ and
\[
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  .
\]


The $\xi_{i}$ and $\xi_{i}^{\ast}$ are called \textit{fermionic operators}.

So what are the $\xi_{i}$ in terms of $a_{j}$ ?

\subsection{The vertex operator construction}

We identify the space $\mathbb{C}\left[  z,z^{-1},x_{1},x_{2},...\right]
=\bigoplus\limits_{m}z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ with
$\mathcal{B}=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }$ by means of
identifying $z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ with
$\mathcal{B}^{\left(  m\right)  }$ for every $m\in\mathbb{Z}$ (the
identification being made through the map%
\begin{align*}
\mathcal{B}^{\left(  m\right)  }  &  \rightarrow z^{m}\mathbb{C}\left[
x_{1},x_{2},...\right]  ,\\
p  &  \mapsto z^{m}\cdot p
\end{align*}
).

Note also that $z$ (that is, multiplication by $z$) is an isomorphism of
$\mathcal{A}_{0}$-modules, but not of $\mathcal{A}$-modules.

The Boson-Fermion correspondence goes like this:%
\[
\mathcal{F}=\bigoplus\limits_{m}\mathcal{F}^{\left(  m\right)  }%
\overset{\sigma=\bigoplus\limits_{m}\sigma_{m}}{\leftarrow}\mathcal{B}%
=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }.
\]
On $\mathcal{F}$ there are operators $\widehat{v_{i}}=\xi_{i}$, $\overset{\vee
}{v_{i}}=\xi_{i}^{\ast}$, $\rho\left(  E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}%
$, \newline$\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  $. By conjugating with the Boson-Fermion correspondence $\sigma$,
these operators give rise to operators on $\mathcal{B}$. How do the latter
operators look like?

\begin{definition}
\label{def.euler.XGamma}Introduce the quantum fields%
\begin{align*}
X\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}\in\left(
\operatorname*{End}\mathcal{F}\right)  \left[  \left[  u^{-1},u\right]
\right]  ,\\
X^{\ast}\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}^{\ast
}u^{-n}\in\left(  \operatorname*{End}\mathcal{F}\right)  \left[  \left[
u^{-1},u\right]  \right]  ,\\
\Gamma\left(  u\right)   &  =\sigma^{-1}\circ X\left(  u\right)  \circ
\sigma\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u^{-1},u\right]  \right]  ,\\
\Gamma^{\ast}\left(  u\right)   &  =\sigma^{-1}\circ X^{\ast}\left(  u\right)
\circ\sigma\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u^{-1},u\right]  \right]  .
\end{align*}
Note that $\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$ is to be read as
"conjugate every term of the power series $X\left(  u\right)  $ by $\sigma$";
in other words, $\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$ means
$\sum\limits_{n\in\mathbb{Z}}\left(  \sigma^{-1}\circ\xi_{n}\circ
\sigma\right)  u^{n}$.
\end{definition}

Recall that $\xi_{n}=\widehat{v_{n}}$ sends $\mathcal{F}^{\left(  m\right)  }$
to $\mathcal{F}^{\left(  m+1\right)  }$ for any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$. Thus, every term of the power series $X\left(  u\right)
=\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}$ sends $\mathcal{F}^{\left(
m\right)  }$ to $\mathcal{F}^{\left(  m+1\right)  }$ for any $m\in\mathbb{Z}$.
Abusing notation, we will abbreviate this fact by saying that $X\left(
u\right)  :\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(
m+1\right)  }$ for any $m\in\mathbb{Z}$. Similarly, $X^{\ast}\left(  u\right)
:\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$ (since $\xi_{n}^{\ast}=\overset{\vee}{v_{n}}$
sends $\mathcal{F}^{\left(  m\right)  }$ to $\mathcal{F}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$ and $n\in\mathbb{Z}$). As a consequence,
$\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{B}^{\left(  m+1\right)  }$ and $\Gamma^{\ast}\left(  u\right)
:\mathcal{B}^{\left(  m\right)  }\rightarrow\mathcal{B}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$.

Now, here is how we can describe $\Gamma\left(  u\right)  $ and $\Gamma^{\ast
}\left(  u\right)  $ (and therefore the operators $\sigma^{-1}\circ\xi
_{n}\circ\sigma$ and $\sigma^{-1}\circ\xi_{n}^{\ast}\circ\sigma$) in terms of
$\mathcal{B}$:

\begin{theorem}
\label{thm.euler}Let $m\in\mathbb{Z}$. On $\mathcal{B}^{\left(  m\right)  }$,
we have%
\begin{align*}
\Gamma\left(  u\right)   &  =u^{m+1}z\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  ;\\
\Gamma^{\ast}\left(  u\right)   &  =u^{-m}z^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
Here, $\exp A$ means $1+A+\dfrac{A^{2}}{2!}+\dfrac{A^{3}}{3!}+...$ for any $A$
for which this series makes any sense.
\end{theorem}

Let us explain what we mean by the products $\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  $ and $\exp\left(  -\sum\limits_{j>0}\dfrac{a_{-j}%
}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  $ in Theorem \ref{thm.euler}. Why do these products (which are
products of exponentials of infinite sums) make any sense? This is easily answered:

\begin{itemize}
\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)  $ is
well-defined and is valued in $\mathcal{B}^{\left(  m\right)  }\left[
u^{-1}\right]  $. (In fact, if we blindly expand%
\begin{align*}
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)   &
=\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  ^{\ell}\\
&  =\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\left(  -1\right)  ^{\ell
}\sum\limits_{j_{1},j_{2},...,j_{\ell}\text{ positive integers}}%
\dfrac{a_{j_{1}}a_{j_{2}}...a_{j_{\ell}}}{j_{1}j_{2}...j_{\ell}}u^{-\left(
j_{1}+j_{2}+...+j_{\ell}\right)  },
\end{align*}
and apply every term of the resulting power series to $v$, then (for fixed
$v$) only finitely many of these terms yield a nonzero result, since $v$ is a
polynomial and thus has finite degree, whereas each $a_{j}$ lowers degree by
$j$.)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  v$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (In fact, we have just shown that $\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)  \in\mathcal{B}^{\left(
m\right)  }\left[  u^{-1}\right]  $; therefore, applying $\exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \in\left(  \operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u\right]  \right]  $ to this gives a well-defined power series in
$\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $ (because
if $\mathfrak{A}$ is an algebra and $\mathfrak{M}$ is an $\mathfrak{A}%
$-module, then the application of a power series in $\mathfrak{A}\left[
\left[  u\right]  \right]  $ to an element of $\mathfrak{M}\left[
u^{-1}\right]  $ gives a well-defined element of $\mathfrak{M}\left(  \left(
u\right)  \right)  $).)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
-\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  v$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (This is proven similarly.)
\end{itemize}

Thus, the formulas of Theorem \ref{thm.euler} make sense.

\begin{remark}
Let us get some intuition for Theorem \ref{thm.euler}. [Remark: Not my
intuition; Etingof's intuition.]

Morally speaking, Theorem \ref{thm.euler} says that $\Gamma\left(  u\right)  $
is $uz\left.  :\exp\left(  \int a\left(  u\right)  du\right)  :\right.  $, and
$\Gamma^{\ast}\left(  u\right)  $ is $z^{-1}\left.  :\exp\left(  -\int
a\left(  u\right)  du\right)  :\right.  $. What does this mean? $a\left(
u\right)  =\sum\limits_{m}a_{m}u^{-m-1}$, thus $\int a\left(  u\right)
du=\sum\limits_{m\neq0}\dfrac{a_{m}}{m}u^{-m}+a_{0}\log u$. Exponentiating
this \textbf{in the normal ordering}, we get%
\begin{align*}
&  \underbrace{\exp\left(  a_{0}\log u\right)  }_{\substack{=u^{a_{0}}%
=u^{m}\\\text{(since }a_{0}\text{ acts by }m\text{ on }\mathcal{B}^{\left(
m\right)  }\text{)}}}\cdot\exp\left(  \sum\limits_{m<0}\dfrac{a_{m}}{m}%
u^{-m}\right)  \cdot\exp\left(  \sum\limits_{m>0}\dfrac{a_{m}}{m}u^{-m}\right)
\\
&  =u^{m}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
This is reminiscent of Euler's formula $y=c\exp\left(  \int a\left(  u\right)
du\right)  $ for the solution $y$ of the differential equation $y^{\prime}=ay$.
\end{remark}

Before we can show Theorem \ref{thm.euler}, we state a lemma about the action
of $\mathcal{A}$ on $\mathcal{B}$:

\begin{lemma}
\label{lem.euler.aGamma}For every $j\in\mathbb{Z}$, we have $\left[
a_{j},\Gamma\left(  u\right)  \right]  =u^{j}\Gamma\left(  u\right)  $ and
$\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]  =-u^{j}\Gamma^{\ast
}\left(  u\right)  $.
\end{lemma}

\textit{Proof.} Let us prove the first formula. Let $j\in\mathbb{Z}$.

On the fermionic space $\mathcal{F}$, the element $a_{j}\in\mathcal{A}$ acts
as%
\[
\widehat{\rho}\left(  T^{j}\right)  =\sum\limits_{i}\widehat{\rho}\left(
E_{i,i+j}\right)  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.
\]
(since $\widehat{\rho}\left(  E_{i,i+j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.  $ for every $i\in\mathbb{Z}$). Hence, on $\mathcal{F}$, we have%
\begin{align*}
\left[  a_{j},X\left(  u\right)  \right]   &  =\left[  \sum\limits_{i}\left\{
%
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.  ,X\left(  u\right)  \right] \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\left[  \xi_{i}\xi_{i+j}^{\ast}-1,X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }i\leq0,\\
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and }i\leq0
\end{array}
\right. \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }i\leq0,\\
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and }i\leq0
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  \xi_{i}\xi_{i+j}^{\ast
}-1,X\left(  u\right)  \right]  =\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(
u\right)  \right]  \right) \\
&  =\sum\limits_{i}\left[  \xi_{i}\xi_{i+j}^{\ast},\sum\limits_{m}\xi_{m}%
u^{m}\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }X\left(  u\right)
=\sum\limits_{m}\xi_{m}u^{m}\right) \\
&  =\sum\limits_{i}\sum\limits_{m}\underbrace{\left[  \xi_{i}\xi_{i+j}^{\ast
},\xi_{m}\right]  }_{\substack{=\delta_{m,i+j}\xi_{i}\\\text{(this is easy to
check)}}}u^{m}=\sum\limits_{i}\sum\limits_{m}\delta_{m,i+j}\xi_{i}u^{m}\\
&  =\sum\limits_{m}\xi_{m-j}u^{m}=u^{j}\underbrace{\sum\limits_{m}\xi
_{m-j}u^{m-j}}_{=X\left(  u\right)  }=u^{j}X\left(  u\right)  .
\end{align*}
Conjugating this equation by $\sigma$, we obtain $\left[  a_{j},\Gamma\left(
u\right)  \right]  =u^{j}\Gamma\left(  u\right)  $. Similarly, we can prove
$\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]  =-u^{j}\Gamma^{\ast
}\left(  u\right)  $. Lemma \ref{lem.euler.aGamma} is proven.

\textit{Proof of Theorem \ref{thm.euler}.} Define $\Gamma_{+}\left(  u\right)
=\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $. Note that
$\Gamma_{+}\left(  u\right)  $ lies in $\left(  \operatorname*{End}%
\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $, which is a
$\mathbb{C}$-algebra. We have%
\begin{align}
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\label{pf.euler.1}\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &  =u^{i}\Gamma_{+}\left(
u\right)  \ \ \ \ \ \ \ \ \ \ \text{if }i<0. \label{pf.euler.2}%
\end{align}
In fact, (\ref{pf.euler.1}) is trivial (because when $i\geq0$, the element
$a_{i}$ commutes with $a_{j}$ for every $j>0$, and thus also commutes with
$\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $). To prove
(\ref{pf.euler.2}), it is enough to show that $\left[  a_{i},\exp\left(
-\dfrac{a_{-i}}{-i}u^{i}\right)  \right]  =u^{i}\exp\left(  -\dfrac{a_{-i}%
}{-i}u^{i}\right)  $ (since we can write $\Gamma_{+}\left(  u\right)  $ in the
form
\[
\Gamma_{+}\left(  u\right)  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right)  =\prod\limits_{j>0}\exp\left(  -\dfrac{a_{j}}{j}%
u^{-j}\right)  ,
\]
and it is clear that $a_{i}$ commutes with all terms $-\dfrac{a_{j}}{j}u^{-j}$
for $j\neq-i$). But this is easily checked using the fact that $\left[
a_{i},a_{-i}\right]  =i$ and Lemma \ref{lem.powerseries1} (applied to
$K=\mathbb{Q}$, $R=\left(  \operatorname*{End}\mathcal{B}\right)  \left[
\left[  u^{-1}\right]  \right]  $, $\alpha=a_{i}$, $\beta=a_{-i}$ and
$P=\exp\left(  -\dfrac{X}{-i}u^{i}\right)  $). This completes the proof of
(\ref{pf.euler.2}).

Since $\Gamma_{+}\left(  u\right)  $ is an invertible power series in $\left(
\operatorname*{End}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]
$ (because the constant term of $\Gamma_{+}\left(  u\right)  $ is $1$), it
makes sense to speak of the power series $\Gamma_{+}\left(  u\right)  ^{-1}%
\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u^{-1}\right]  \right]  $. From (\ref{pf.euler.1}) and (\ref{pf.euler.2}), we
can derive the formulas%
\begin{align}
\left[  a_{i},\Gamma_{+}\left(  u\right)  ^{-1}\right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\label{pf.euler.1inv}\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  ^{-1}\right]   &  =-u^{i}\Gamma
_{+}\left(  u\right)  ^{-1}\ \ \ \ \ \ \ \ \ \ \text{if }i<0
\label{pf.euler.2inv}%
\end{align}
(using the standard fact that $\left[  \alpha,\beta^{-1}\right]  =-\beta
^{-1}\left[  \alpha,\beta\right]  \beta^{-1}$ for any two elements $\alpha$
and $\beta$ of a ring such that $\beta$ is invertible).

Now define a map $\Delta\left(  u\right)  :\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$ by $\Delta\left(  u\right)  =\Gamma\left(  u\right)  \Gamma_{+}\left(
u\right)  ^{-1}z^{-1}$. Let us check why this definition makes sense:

\begin{itemize}
\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, we have $z^{-1}%
v\in\mathcal{B}^{\left(  m-1\right)  }$, and the term $\Gamma_{+}\left(
u\right)  ^{-1}z^{-1}v$ is well-defined and is valued in $\mathcal{B}^{\left(
m-1\right)  }\left[  u^{-1}\right]  $. (In fact, if we blindly expand%
\begin{align*}
\Gamma_{+}\left(  u\right)  ^{-1}  &  =\left(  \exp\left(  -\sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)  ^{-1}=\exp\left(
\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Corollary \ref{cor.exp(-w)} (applied to }R=\left(
\operatorname*{End}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]
\text{,}\\
I=\left(  \text{the ideal of }R\text{ consisting of all power series with
constant term }1\right)  \text{,}\\
\text{and }\gamma=-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\text{) yields}\\
\left(  \exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)
\cdot\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\right)  =1
\end{array}
\right) \\
&  =\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\sum\limits_{j_{1}%
,j_{2},...,j_{\ell}\text{ positive integers}}\dfrac{a_{j_{1}}a_{j_{2}%
}...a_{j_{\ell}}}{j_{1}j_{2}...j_{\ell}}u^{-\left(  j_{1}+j_{2}+...+j_{\ell
}\right)  },
\end{align*}
and apply every term of the resulting power series to $z^{-1}v$, then (for
fixed $v$) only finitely many of these terms yield a nonzero result, since
$z^{-1}v$ is a polynomial and thus has finite degree, whereas each $a_{j}$
lowers degree by $j$.)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\Gamma\left(
u\right)  \Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (In fact, we have just shown that $\Gamma_{+}\left(  u\right)  ^{-1}%
z^{-1}v\in\mathcal{B}^{\left(  m-1\right)  }\left[  u^{-1}\right]  $;
therefore, applying $\Gamma\left(  u\right)  \in\left(  \operatorname*{End}%
\mathcal{B}\right)  \left[  \left[  u^{-1},u\right]  \right]  $ to this gives
a well-defined power series in $\mathcal{B}\left(  \left(  u\right)  \right)
$ (because if $\mathfrak{A}$ is an algebra and $\mathfrak{M}$ is an
$\mathfrak{A}$-module, then the application of a power series in
$\mathfrak{A}\left[  \left[  u\right]  \right]  $ to an element of
$\mathfrak{M}\left[  u^{-1}\right]  $ gives a well-defined element of
$\mathfrak{M}\left(  \left(  u\right)  \right)  $), and this power series
actually lies in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)
\right)  $ (since $\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m-1\right)
}\rightarrow\mathcal{B}^{\left(  m\right)  }$).)
\end{itemize}

Since $\left[  a_{0},z\right]  =z$ and $\left[  a_{i},z\right]  =0$ for all
$i\neq0$, we have%
\begin{equation}
\left[  a_{i},\Delta\left(  u\right)  \right]  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\leq0;\\
u^{i}\Delta\left(  u\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }i>0
\end{array}
\right.  \label{pf.euler.Del}%
\end{equation}
(due to (\ref{pf.euler.1inv}), (\ref{pf.euler.2inv}) and Lemma
\ref{lem.euler.aGamma}). In particular, $\left[  a_{i},\Delta\left(  u\right)
\right]  =0$ if $i\leq0$. Thus, $\Delta\left(  u\right)  $ is a homomorphism
of $\mathcal{A}_{-}$-modules, where $\mathcal{A}_{-}$ is the Lie subalgebra
$\left\langle a_{-1},a_{-2},a_{-3},...\right\rangle $ of $\mathcal{A}$. (Of
course, this formulation means that every term of the formal power series
$\Delta\left(  u\right)  $ is a homomorphism of $\mathcal{A}_{-}$-modules.)

Consider now the element $z^{m}$ of $z^{m}\mathbb{C}\left[  x_{1}%
,x_{2},...\right]  =\mathcal{B}^{\left(  m\right)  }=\widetilde{F}_{m}$. Also,
consider the element $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ of
$\wedge^{\dfrac{\infty}{2},m}V=\mathcal{F}^{\left(  m\right)  }$. By the
definition of $\sigma_{m}$, we have $\sigma_{m}\left(  z^{m}\right)  =\psi
_{m}$. (In fact, $z^{m}$ is what was denoted by $1$ in Proposition
\ref{prop.wedge.fock}.)

From Lemma \ref{lem.F.P1=P}, it is clear that the Fock module $F$ is generated
by $1$ as an $\mathcal{A}_{-}$-module (since $\mathcal{A}_{-}=\left\langle
a_{-1},a_{-2},a_{-3},...\right\rangle $). Since there exists an $\mathcal{A}%
_{-}$-module isomorphism $F\rightarrow\widetilde{F}$ which sends $1$ to $1$
(in fact, the map $\operatorname*{resc}$ of Proposition \ref{prop.resc} is
such an isomorphism), this yields that $\widetilde{F}$ is generated by $1$ as
an $\mathcal{A}_{-}$-module. Since there exists an $\mathcal{A}_{-}$-module
isomorphism $\widetilde{F}\rightarrow\widetilde{F}_{m}$ which sends $1$ to
$z^{m}$ (in fact, multiplication by $z^{m}$ is such an isomorphism), this
yields that $\widetilde{F}_{m}$ is generated by $z^{m}$ as an $\mathcal{A}%
_{-}$-module. Consequently, the $m$-th term of the power series $\Delta\left(
u\right)  $ is completely determined by $\left(  \Delta\left(  u\right)
\right)  \left(  z^{m}\right)  $ (because we know that $\Delta\left(
u\right)  $ is a homomorphism of $\mathcal{A}_{-}$-modules). So let us compute
$\left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right)  $. Since
$\Delta\left(  u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $, we know
that $\left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right)  $ is an
element of $\underbrace{\mathcal{B}^{\left(  m\right)  }}_{=z^{m}%
\widetilde{F}}\left(  \left(  u\right)  \right)  =z^{m}\widetilde{F}\left(
\left(  u\right)  \right)  $. In other words, $\left(  \Delta\left(  u\right)
\right)  \left(  z^{m}\right)  $ is $z^{m}$ times a Laurent series in $u$
whose coefficients are polynomials in $x_{1},x_{2},x_{3},...$. Denote this
Laurent series by $Q$. Thus, $\left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  =z^{m}Q$.

For every $i>0$, we have
\[
a_{i}\Delta\left(  u\right)  =\Delta\left(  u\right)  a_{i}%
+\underbrace{\left[  a_{i},\Delta\left(  u\right)  \right]  }%
_{\substack{=u^{i}\Delta\left(  u\right)  \\\text{(by (\ref{pf.euler.Del}))}%
}}=\Delta\left(  u\right)  a_{i}+u^{i}\Delta\left(  u\right)  ,
\]
so that%
\begin{align*}
\left(  a_{i}\Delta\left(  u\right)  \right)  \left(  z^{m}\right)   &
=\left(  \Delta\left(  u\right)  a_{i}+u^{i}\Delta\left(  u\right)  \right)
\left(  z^{m}\right)  =\Delta\left(  u\right)  \underbrace{a_{i}z^{m}%
}_{\substack{=0\\\text{(since }a_{i}=\dfrac{\partial}{\partial x_{i}}\text{)}%
}}+u^{i}\underbrace{\left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  }_{=z^{m}Q}\\
&  =u^{i}z^{m}Q=z^{m}u^{i}Q.
\end{align*}
Since $\left(  a_{i}\Delta\left(  u\right)  \right)  \left(  z^{m}\right)
=a_{i}\underbrace{\left(  \left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  \right)  }_{=z^{m}Q}=z^{m}\underbrace{a_{i}}_{=\dfrac{\partial
}{\partial x_{i}}}Q=z^{m}\dfrac{\partial Q}{\partial x_{i}}$, this rewrites as
$z^{m}\dfrac{\partial Q}{\partial x_{i}}=z^{m}u^{i}Q$. Hence, for every $i>0$,
we have $\dfrac{\partial Q}{\partial x_{i}}=u^{i}Q$. Thus, we can write the
formal Laurent series $Q$ in the form $Q=f\left(  u\right)  \exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  $ for some Laurent series $f\left(
u\right)  \in\mathbb{C}\left(  \left(  u\right)  \right)  \ \ \ \ $%
.\footnote{Here, we have used the following fact:
\par
\textbf{Fact:} If $R$ is a $\mathbb{Q}$-algebra, $U$ is an $R$-module,
$\left(  \alpha_{1},\alpha_{2},\alpha_{3},...\right)  $ is a sequence of
elements of $R$, and $P\in U\left[  \left[  x_{1},x_{2},x_{3},...\right]
\right]  $ is a formal power series with coefficients in $U$ such that every
$i>0$ satisfies $\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P$, then there
exists some $f\in U$ such that $P=f\cdot\exp\left(  \sum\limits_{j>0}%
x_{j}\alpha_{j}\right)  $.
\par
Proving this fact is easy (just let $f$ be the constant term of the power
series $P$, and prove by induction that every monomial of $P$ equals the
corresponding monomial of $f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha
_{j}\right)  $). We have applied this fact to $R=\mathbb{C}\left[  u\right]
$, $U=\mathbb{C}\left(  \left(  u\right)  \right)  $, $\left(  \alpha
_{1},\alpha_{2},\alpha_{3},...\right)  =\left(  u^{1},u^{2},u^{3},...\right)
$ and $P=Q$.} Thus,
\begin{align*}
&  \left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right) \\
&  =z^{m}Q=z^{m}f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}%
u^{j}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }Q=f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \left(  z^{m}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
each }\dfrac{a_{-j}}{j}\text{ acts as multiplication by }x_{j}\text{ on
}\widetilde{F}\right)  .
\end{align*}
In other words, the two maps $\Delta\left(  u\right)  $ and $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ are
equal on $z^{m}$. Since each of these two maps is an $\mathcal{A}_{-}$-module
homomorphism\footnote{In fact, we know that $\Delta\left(  u\right)  $ is an
$\mathcal{A}_{-}$-module homomorphism, and it is clear that $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ is an
$\mathcal{A}_{-}$-module homomorphism because $\mathcal{A}_{-}$ is an abelian
Lie algebra.}, this yields that these two maps must be identical (because
$\widetilde{F}_{m}$ is generated by $z^{m}$ as an $\mathcal{A}_{-}$-module).
In other words, $\Delta\left(  u\right)  =f\left(  u\right)  \exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $. Since $\Delta\left(
u\right)  =\Gamma\left(  u\right)  \Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$,
this becomes $\Gamma\left(  u\right)  \Gamma_{+}\left(  u\right)  ^{-1}%
z^{-1}=f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  $, so that%
\begin{align}
\Gamma\left(  u\right)   &  =f\left(  u\right)  \exp\left(  \sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\Gamma_{+}\left(  u\right)
\cdot z=f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \cdot z\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Gamma_{+}\left(  u\right)
=\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)
\nonumber\\
&  =f\left(  u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  . \label{pf.euler.Gamma-through-f}%
\end{align}
It remains to show that $f\left(  u\right)  =u^{m+1}$.

In order to do this, we recall that
\begin{align*}
\left(  \Gamma\left(  u\right)  \right)  \left(  z^{m}\right)   &  =f\left(
u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\underbrace{\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\left(  z^{m}\right)  }_{\substack{=z^{m}\\\text{(because }a_{j}\left(
z^{m}\right)  =0\text{ for every }j>0\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.euler.Gamma-through-f})}\right) \\
&  =f\left(  u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \left(  z^{m}\right)  =f\left(  u\right)  z\exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  z^{m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since each }\dfrac{a_{-j}}{j}\text{ acts
as multiplication by }x_{j}\text{ on }\widetilde{F}\right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)
z^{m+1}.
\end{align*}
On the other hand, back on the fermionic side, for the vector $\psi_{m}%
=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$, we have%
\begin{align*}
\left(  X\left(  u\right)  \right)  \psi_{m}  &  =\sum\limits_{n\in\mathbb{Z}%
}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }X\left(  u\right)  =\sum\limits_{n\in\mathbb{Z}}\underbrace{\xi
_{n}}_{=\widehat{v_{n}}}u^{n}=\sum\limits_{n\in\mathbb{Z}}\widehat{v_{n}}%
u^{n}\right) \\
&  =\sum\limits_{\substack{n\in\mathbb{Z};\\n\leq m}%
}\underbrace{\widehat{v_{n}}\left(  \psi_{m}\right)  }%
_{\substack{=0\\\text{(since }n\leq m\text{, so that }v_{n}\\\text{appears in
}v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...=\psi_{m}\text{)}}}u^{n}%
+\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(
\psi_{m}\right)  u^{n}=\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}.
\end{align*}
Thus, $\sigma^{-1}\left(  \left(  X\left(  u\right)  \right)  \psi_{m}\right)
=\sigma^{-1}\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)  $. Compared with%
\begin{align*}
\sigma^{-1}\left(  \left(  X\left(  u\right)  \right)  \underbrace{\psi_{m}%
}_{=\sigma\left(  z^{m}\right)  }\right)   &  =\sigma^{-1}\left(  \left(
X\left(  u\right)  \right)  \left(  \sigma\left(  z^{m}\right)  \right)
\right)  =\underbrace{\left(  \sigma^{-1}\circ X\left(  u\right)  \circ
\sigma\right)  }_{=\Gamma\left(  u\right)  }\left(  z^{m}\right)  =\left(
\Gamma\left(  u\right)  \right)  \left(  z^{m}\right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)
z^{m+1},
\end{align*}
this yields $\sigma^{-1}\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)  =f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}$, so that%
\begin{equation}
\sigma\left(  f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}%
u^{j}\right)  z^{m+1}\right)  =\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}. \label{pf.euler.compare}%
\end{equation}
We want to find $f\left(  u\right)  $ by comparing the sides of this equation.
In order to do this, we recall that each space $\mathcal{B}^{\left(  i\right)
}$ is graded; hence, $\mathcal{B}$ (being the direct sum of the $\mathcal{B}%
^{\left(  i\right)  }$) is also graded (by taking the direct sum of all the
gradings). Also, each space $\mathcal{F}^{\left(  i\right)  }$ is graded;
hence, $\mathcal{F}$ (being the direct sum of the $\mathcal{F}^{\left(
i\right)  }$) is also graded (by taking the direct sum of all the gradings).
Since each $\sigma_{m}$ is a graded map, the direct sum $\sigma=\bigoplus
\limits_{m\in\mathbb{Z}}\sigma_{m}$ is also graded. Therefore,%
\begin{align}
&  \sigma\left(  0\text{-th homogeneous component of }f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right) \nonumber\\
&  =\left(  0\text{-th homogeneous component of }\sigma\left(  f\left(
u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right)
\right) \nonumber\\
&  =\left(  0\text{-th homogeneous component of }\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)
\label{pf.euler.compare2}%
\end{align}
(by (\ref{pf.euler.compare})). Now, for every $n\in\mathbb{Z}$ satisfying
$n\geq m+1$, the element $\widehat{v_{n}}\left(  \psi_{m}\right)  $ equals
$v_{n}\wedge v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$, and thus has degree
$-\left(  n-m-1\right)  $. Hence, for every nonpositive $i\in\mathbb{Z}$, the
$i$-th homogeneous component of the sum $\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}%
\in\mathcal{F}$ is $\widehat{v_{m+1-i}}\left(  \psi_{m}\right)  u^{m+1-i}$. In
particular, the $0$-th homogeneous component of $\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}$ is
$\widehat{v_{m+1}}\left(  \psi_{m}\right)  u^{m+1}=\psi_{m+1}u^{m+1}$ (since
$\widehat{v_{m+1}}\left(  \psi_{m}\right)  =v_{m+1}\wedge v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...=\psi_{m+1}$). Therefore,
(\ref{pf.euler.compare2}) becomes%
\begin{equation}
\sigma\left(  0\text{-th homogeneous component of }f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right)  =\psi
_{m+1}u^{m+1}. \label{pf.euler.compare3}%
\end{equation}
On the other hand, the $0$-th homogeneous component of the element $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}%
\in\mathcal{B}$ is clearly $f\left(  u\right)  z^{m+1}$ (because $\exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  =1+\left(  \text{terms involving at least
one }x_{j}\right)  $, and every $x_{j}$ lowers the degree). Thus,
(\ref{pf.euler.compare3}) becomes $\sigma\left(  f\left(  u\right)
z^{m+1}\right)  =\psi_{m+1}u^{m+1}$. Since $\sigma\left(  f\left(  u\right)
z^{m+1}\right)  =f\left(  u\right)  \underbrace{\sigma\left(  z^{m+1}\right)
}_{=\psi_{m+1}}=f\left(  u\right)  \psi_{m+1}$, this rewrites as $f\left(
u\right)  \psi_{m+1}=\psi_{m+1}u^{m+1}$, so that $f\left(  u\right)  =u^{m+1}%
$. Hence, (\ref{pf.euler.Gamma-through-f}) becomes
\begin{align*}
\Gamma\left(  u\right)   &  =\underbrace{f\left(  u\right)  }_{=u^{m+1}}%
z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right) \\
&  =u^{m+1}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
This proves one of the equalities of Theorem \ref{thm.euler}. The other is
proven similarly.

Theorem \ref{thm.euler} is proven.

\begin{corollary}
\label{cor.euler}Let $m\in\mathbb{Z}$. On $\mathcal{B}^{\left(  m\right)  }$,
we have%
\[
\rho\left(  \sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)  =\sum\limits_{i,j}%
u^{i}v^{-j}\xi_{i}\xi_{j}^{\ast}=X\left(  u\right)  X^{\ast}\left(  v\right)
,
\]
thus%
\begin{align*}
&  \sigma^{-1}\circ\rho\left(  \sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)
\circ\sigma\\
&  =\sigma^{-1}\circ X\left(  u\right)  X^{\ast}\left(  v\right)  \circ
\sigma=\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}

\end{corollary}

The importance of Corollary \ref{cor.euler} lies in the fact that it gives an
easy way to compute the $\rho$-action of $\mathfrak{gl}_{\infty}$ on
$\mathcal{B}^{\left(  m\right)  }$: In fact, for any $p\in\mathbb{Z}$ and
$q\in\mathbb{Z}$, the coefficient of $\sigma^{-1}\circ\rho\left(
\sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\in\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)
\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]  $ before $u^{p}v^{-q}$ is
$\sigma^{-1}\circ\rho\left(  E_{p,q}\right)  \circ\sigma$, and this is exactly
the action of $E_{p,q}$ on $\mathcal{B}^{\left(  m\right)  }$ obtained by
transferring the action $\rho$ of $\mathfrak{gl}_{\infty}$ on $\mathcal{F}%
^{\left(  m\right)  }$ to $\mathcal{B}^{\left(  m\right)  }$.

\textit{Proof of Corollary \ref{cor.euler}.} By Theorem \ref{thm.euler}
(applied to $m-1$ instead of $m$), we have%
\[
\Gamma\left(  u\right)  =u^{m}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m-1\right)
}%
\]
By Theorem \ref{thm.euler}, we have%
\[
\Gamma^{\ast}\left(  v\right)  =v^{-m}z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}v^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}v^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
m\right)  }.
\]
Multiplying these two equalities, we obtain%
\begin{align*}
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)   &  =u^{m}v^{-m}%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}%
{j}a_{-j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
\ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m\right)  }.
\end{align*}
We need to reorder the second and the third exponential on the right hand side
of this equation. To do so, we notice that each of $-\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}$ and $-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}$ lies in
the ring $\left(  \operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \right)  \left[  \left[  u^{-1},v\right]  \right]  $%
\ \ \ \ \footnote{This is the ring of formal power series in the
indeterminates $u^{-1}$ and $v$ over the ring $\operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  $. Note that $\operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  $ is non-commutative, but the
ring of formal power series is still defined in the same way as over
commutative rings. The indeterminates $u^{-1}$ and $v$ themselves commute with
each other and with each element of $\operatorname*{End}\left(  \mathcal{B}%
^{\left(  m\right)  }\right)  $.}. Let $I$ be the ideal of the ring $\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)
\left[  \left[  u^{-1},v\right]  \right]  $ containing all power series with
constant term $0$. This ring $\left(  \operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u^{-1},v\right]  \right]  $ is a $\mathbb{Q}$-algebra and is complete and
Hausdorff with respect to the $I$-adic topology. Let $\alpha=-\sum
\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}$ and $\beta=-\sum\limits_{j>0}\dfrac
{v^{j}}{j}a_{-j}$. Clearly, both $\alpha$ and $\beta$ lie in $I$. Also,%
\begin{align*}
\left[  \alpha,\beta\right]   &  =\left[  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j},-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right]  =\sum\limits_{j>0}%
\sum\limits_{k>0}\dfrac{u^{-j}v^{k}}{jk}\underbrace{\left[  a_{j}%
,a_{-k}\right]  }_{=\delta_{j,k}j}\\
&  =\sum\limits_{j>0}\sum\limits_{k>0}\dfrac{u^{-j}v^{k}}{jk}\delta
_{j,k}j=\sum\limits_{j>0}\dfrac{u^{-j}v^{j}}{jj}j=\sum\limits_{j>0}\dfrac
{1}{j}\left(  \dfrac{v}{u}\right)  ^{j}=-\log\left(  1-\dfrac{v}{u}\right)
\end{align*}
is a power series with coefficients in $\mathbb{Q}$, and thus lies in the
center of $\left(  \operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \right)  \left[  \left[  u^{-1},v\right]  \right]  $, and hence
commutes with each of $\alpha$ and $\beta$. Thus, we can apply Lemma
\ref{lem.powerseries3} to $K=\mathbb{Q}$ and $R=\left(  \operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u^{-1},v\right]  \right]  $, and obtain $\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  =\left(  \exp\beta\right)  \cdot\left(
\exp\alpha\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $.
Hence,%
\begin{align*}
&  \Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =u^{m}v^{-m}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}%
a_{-j}\right)  \exp\underbrace{\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  }_{=\alpha}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\underbrace{\left(  -\sum\limits_{j>0}%
\dfrac{v^{j}}{j}a_{-j}\right)  }_{=\beta}\exp\left(  \sum\limits_{j>0}%
\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =u^{m}v^{-m}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}%
a_{-j}\right)  \cdot\underbrace{\left(  \exp\alpha\right)  \cdot\left(
\exp\beta\right)  }_{=\left(  \exp\beta\right)  \cdot\left(  \exp
\alpha\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =\underbrace{u^{m}v^{-m}}_{=\left(  \dfrac{u}{v}\right)  ^{m}}\cdot
\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \cdot
\exp\underbrace{\beta}_{=-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\underbrace{\alpha}_{=-\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}}\cdot\underbrace{\exp\left[  \alpha,\beta\right]
}_{\substack{=\dfrac{1}{1-\dfrac{v}{u}}\\\text{(since }\left[  \alpha
,\beta\right]  =-\log\left(  1-\dfrac{v}{u}\right)  \text{)}}}\cdot\exp\left(
\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =\left(  \dfrac{u}{v}\right)  ^{m}\cdot\exp\left(  \sum\limits_{j>0}%
\dfrac{u^{j}}{j}a_{-j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{v^{j}}{j}a_{-j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  \cdot\dfrac{1}{1-\dfrac{v}{u}}\cdot\exp\left(  \sum
\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
\end{align*}%
\begin{align*}
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}%
\cdot\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}%
a_{-j}\right)  \\\text{(by Theorem \ref{thm.exp(u+v)}, applied to
}R=\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \left[
\left[  u,v\right]  \right]  \text{,}\\I=\left(  \text{the ideal of }R\text{
consisting of all power series with constant term }0\right)  \text{,}%
\\\alpha=\sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\text{ and }\beta
=-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac
{v^{-j}}{j}a_{j}\right)  }_{\substack{=\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  \\\text{(by Theorem \ref{thm.exp(u+v)},
applied to }R=\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \left[  \left[  u^{-1},v^{-1}\right]  \right]  \text{,}\\I=\left(
\text{the ideal of }R\text{ consisting of all power series with constant term
}0\right)  \text{,}\\\alpha=-\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\text{ and
}\beta=\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\text{)}}}\\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}
This proves Corollary \ref{cor.euler}.

\subsection{Expliciting $\sigma^{-1}$ using Schur polynomials}

Next we are going to give an explicit (in as far as one can do) formula for
$\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
$ for an elementary semiinfinite wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$. Before we do so, we need to introduce the notion of
\textit{Schur polynomials}. We first define \textit{elementary Schur
polynomials}:

\subsubsection{Schur polynomials}

\begin{Convention}
\label{conv.schur.x}In the following, we let $x$ denote the countable family
of indeterminates $\left(  x_{1},x_{2},x_{3},...\right)  $. Thus, for any
polynomial $P$ in countably many indeterminates, we write $P\left(  x\right)
$ for $P\left(  x_{1},x_{2},x_{3},...\right)  $.
\end{Convention}

\begin{definition}
\label{def.schur.Sk}For every $k\in\mathbb{N}$, let $S_{k}\in\mathbb{Q}\left[
x_{1},x_{2},x_{3},...\right]  $ be the coefficient of the power series
$\exp\left(  \sum\limits_{i\geq1}x_{i}z^{i}\right)  \in\mathbb{Q}\left[
x_{1},x_{2},x_{3},...\right]  \left[  \left[  z\right]  \right]  $ before
$z^{k}$. Then, obviously,%
\begin{equation}
\sum\limits_{k\geq0}S_{k}\left(  x\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}x_{i}z^{i}\right)  . \label{def.schur.sk.genfun}%
\end{equation}

\end{definition}

For example, $S_{0}\left(  x\right)  =1$, $S_{1}\left(  x\right)  =x_{1}$,
$S_{2}\left(  x\right)  =\dfrac{x_{1}^{2}}{2}+x_{2}$, $S_{3}\left(  x\right)
=\dfrac{x_{1}^{3}}{6}+x_{1}x_{2}+x_{3}$.

Note that the polynomials $S_{k}$ that we just defined are \textbf{not}
symmetric polynomials. Instead, they "represent" the complete symmetric
functions in terms of the $\dfrac{p_{i}}{i}$ (where $p_{i}$ are the power
sums). Here is what exactly we mean by this:

\begin{definition}
\label{def.schur.y}Let $N\in\mathbb{N}$, and let $y$ denote a family of $N$
indeterminates $\left(  y_{1},y_{2},...,y_{N}\right)  $. Thus, for any
polynomial $P$ in $N$ indeterminates, we write $P\left(  y\right)  $ for
$P\left(  y_{1},y_{2},...,y_{N}\right)  $.
\end{definition}

\begin{definition}
\label{def.schur.hk}For every $k\in\mathbb{N}$, define the $k$\textit{-th
complete symmetric function} $h_{k}$\textit{ }in the variables $y_{1}%
,y_{2},...,y_{N}$ by $h_{k}\left(  y_{1},y_{2},...,y_{N}\right)
=\sum\limits_{\substack{p_{1},p_{2},...,p_{N}\in\mathbb{N};\\p_{1}%
+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}$.
\end{definition}

\begin{proposition}
\label{prop.schur.hk}In the ring $\mathbb{Q}\left[  y_{1},y_{2},...,y_{N}%
\right]  \left[  \left[  z\right]  \right]  $, we have%
\[
\sum\limits_{k\geq0}z^{k}h_{k}\left(  y\right)  =\prod\limits_{j=1}^{N}%
\dfrac{1}{1-zy_{j}}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.hk}.} For every $j\in\left\{
1,2,...,N\right\}  $, the sum formula for the geometric series yields
$\dfrac{1}{1-zy_{j}}=\sum\limits_{p\in\mathbb{N}}\left(  zy_{j}\right)
^{p}=\sum\limits_{p\in\mathbb{N}}y_{j}^{p}z^{p}$. Hence,%
\begin{align*}
\prod\limits_{j=1}^{N}\dfrac{1}{1-zy_{j}}  &  =\prod\limits_{j=1}^{N}\left(
\sum\limits_{p\in\mathbb{N}}y_{j}^{p}z^{p}\right)  =\sum\limits_{p_{1}%
,p_{2},...,p_{N}\in\mathbb{N}}\underbrace{\left(  y_{1}^{p_{1}}z^{p_{1}%
}\right)  \left(  y_{2}^{p_{2}}z^{p_{2}}\right)  ...\left(  y_{N}^{p_{N}%
}z^{p_{n}}\right)  }_{=y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}%
z^{p_{1}+p_{2}+...+p_{N}}}\\
&  =\sum\limits_{p_{1},p_{2},...,p_{N}\in\mathbb{N}}y_{1}^{p_{1}}y_{2}^{p_{2}%
}...y_{N}^{p_{N}}z^{p_{1}+p_{2}+...+p_{N}}=\sum\limits_{k\geq0}%
\underbrace{\sum\limits_{\substack{p_{1},p_{2},...,p_{N}\in\mathbb{N}%
;\\p_{1}+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}%
}_{=h_{k}\left(  y_{1},y_{2},...,y_{N}\right)  =h_{k}\left(  y\right)  }%
z^{k}\\
&  =\sum\limits_{k\geq0}h_{k}\left(  y\right)  z^{k}=\sum\limits_{k\geq0}%
z^{k}h_{k}\left(  y\right)  .
\end{align*}
This proves Proposition \ref{prop.schur.hk}.

\begin{definition}
Let $N\in\mathbb{N}$. We define a map $\operatorname*{PSE}\nolimits_{N}%
:\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \rightarrow\mathbb{C}\left[
y_{1},y_{2},...,y_{N}\right]  $ as follows: For every polynomial $P\in\left[
x_{1},x_{2},x_{3},...\right]  $, let $\operatorname*{PSE}\nolimits_{N}\left(
P\right)  $ be the result of substituting $x_{j}=\dfrac{y_{1}^{j}+y_{2}%
^{j}+...+y_{N}^{j}}{j}$ for all positive integers $j$ into the polynomial $P$.

Clearly, this map $\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra homomorphism.
\end{definition}

(The notation $\operatorname*{PSE}\nolimits_{N}$ is mine and has been chosen
as an abbreviation for "Power Sum Evaluation in $N$ variables".)

\begin{proposition}
\label{prop.schur.h_k.as.schur}For every $N\in\mathbb{N}$, we have
$h_{k}\left(  y\right)  =\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  $ for each $k\in\mathbb{N}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.h_k.as.schur}.} Fix
$N\in\mathbb{N}$. We know that $\sum\limits_{k\geq0}S_{k}\left(  x\right)
z^{k}=\exp\left(  \sum\limits_{i\geq1}x_{i}z^{i}\right)  $. Since
$\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra homomorphism,
this yields%
\begin{align*}
\sum\limits_{k\geq0}\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  z^{k}  &  =\exp\left(  \sum\limits_{i\geq1}%
\operatorname*{PSE}\nolimits_{N}\left(  x_{i}\right)  z^{i}\right)
=\exp\left(  \sum\limits_{i\geq1}\sum\limits_{j=1}^{N}\dfrac{y_{j}^{i}}%
{i}z^{i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\left(  x_{i}\right)  =\dfrac{y_{1}^{i}+y_{2}^{i}+...+y_{N}^{i}}{i}%
=\sum\limits_{j=1}^{N}\dfrac{y_{j}^{i}}{i}\right) \\
&  =\exp\left(  \sum\limits_{j=1}^{N}\sum\limits_{i\geq1}\dfrac{y_{j}^{i}}%
{i}z^{i}\right)  =\prod\limits_{j=1}^{N}\exp\left(  \sum\limits_{i\geq1}%
\dfrac{y_{j}^{i}}{i}z^{i}\right) \\
&  =\prod\limits_{j=1}^{N}\exp\underbrace{\left(  \sum\limits_{i\geq1}%
\dfrac{y_{j}^{i}z^{i}}{i}\right)  }_{=-\log\left(  1-y_{j}z\right)  }%
=\prod\limits_{j=1}^{N}\underbrace{\exp\left(  -\log\left(  1-y_{j}z\right)
\right)  }_{=\dfrac{1}{1-y_{j}z}=\dfrac{1}{1-zy_{j}}}\\
&  =\prod\limits_{j=1}^{N}\dfrac{1}{1-zy_{j}}=\sum\limits_{k\geq0}z^{k}%
h_{k}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.schur.hk}}\right)  .
\end{align*}
By comparing coefficients in this equality, we conclude that
$\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)
=h_{k}\left(  y\right)  $ for each $k\in\mathbb{N}$. Proposition
\ref{prop.schur.h_k.as.schur} is proven.

\begin{definition}
\label{def.schur.Slambda}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition, so that $\lambda_{1}\geq
\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers.

We define $S_{\lambda}\left(  x\right)  \in\mathbb{Q}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to be the polynomial%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
S_{\lambda_{1}}\left(  x\right)  & S_{\lambda_{1}+1}\left(  x\right)  &
S_{\lambda_{1}+2}\left(  x\right)  & ... & S_{\lambda_{1}+m-1}\left(  x\right)
\\
S_{\lambda_{2}-1}\left(  x\right)  & S_{\lambda_{2}}\left(  x\right)  &
S_{\lambda_{2}+1}\left(  x\right)  & ... & S_{\lambda_{2}+m-2}\left(  x\right)
\\
S_{\lambda_{3}-2}\left(  x\right)  & S_{\lambda_{3}-1}\left(  x\right)  &
S_{\lambda_{3}}\left(  x\right)  & ... & S_{\lambda_{3}+m-3}\left(  x\right)
\\
... & ... & ... & ... & ...\\
S_{\lambda_{m}-m+1}\left(  x\right)  & S_{\lambda_{m}-m+2}\left(  x\right)  &
S_{\lambda_{m}-m+3}\left(  x\right)  & ... & S_{\lambda_{m}}\left(  x\right)
\end{array}
\right) \\
&  =\det\left(  \left(  S_{\lambda_{i}+j-i}\left(  x\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  ,
\end{align*}
where $S_{j}$ denotes $0$ if $j<0$. (Note that this does not depend on
trailing zeroes in the partition; in other words, $S_{\left(  \lambda
_{1},\lambda_{2},...,\lambda_{m}\right)  }\left(  x\right)  =S_{\left(
\lambda_{1},\lambda_{2},...,\lambda_{m},0,0,...,0\right)  }\left(  x\right)  $
for any number of zeroes. This is because any nonnegative integers $m$ and
$\ell$, any $m\times m$ matrix $A$ and any $m\times\ell$ matrix $B$ satisfy
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0 & I_{\ell}%
\end{array}
\right)  =\det A$.)
\end{definition}

To a reader acquainted with the Schur polynomials of combinatorics (and
representation theory of symmetric groups), this definition may sound
familiar, but it should be reminded that our $S_{\lambda}$ is \textbf{not a
symmetric function per se}; instead, it can be made into a symmetric function
- and this will, indeed, be the $\lambda$-Schur polynomial known from
combinatorics - by substituting for each $x_{j}$ the term $\dfrac{\left(
j\text{-th power sum symmetric function}\right)  }{j}$. We will prove this in
Proposition \ref{prop.schur.Schur=schur} (albeit only for finitely many
variables). Let us first formulate one of the many definitions of Schur
polynomials from combinatorics:

\begin{definition}
\label{def.schur.schurpoly}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition, so that $\lambda_{1}\geq
\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers. We define $\lambda
_{\ell}$ to mean $0$ for all integers $\ell>m$; thus, we obtain a
nonincreasing sequence $\left(  \lambda_{1},\lambda_{2},\lambda_{3}%
,...\right)  $ of nonnegative integers.

Let $N\in\mathbb{N}$.

The so-called $\lambda$\textit{-Schur module} $V_{\lambda}$ \textit{over
}$\operatorname*{GL}\left(  N\right)  $ is defined to be the
$\operatorname*{GL}\left(  N\right)  $-module $\operatorname*{Hom}%
\nolimits_{S_{n}}\left(  S^{\lambda},\left(  \mathbb{C}^{N}\right)  ^{\otimes
n}\right)  $, where $n$ denotes the number $\lambda_{1}+\lambda_{2}%
+...+\lambda_{m}$ and $S^{\lambda}$ denotes the Specht module over the
symmetric group $S_{n}$ corresponding to the partition $\lambda$. (The
$\operatorname*{GL}\left(  N\right)  $-module structure on
$\operatorname*{Hom}\nolimits_{S_{n}}\left(  S^{\lambda},\left(
\mathbb{C}^{N}\right)  ^{\otimes n}\right)  $ is obtained from the
$\operatorname*{GL}\left(  N\right)  $-module structure on $\mathbb{C}^{N}$.)
This $\lambda$-Schur module $V_{\lambda}$ is not only a $\operatorname*{GL}%
\left(  N\right)  $-module, but also a $\mathfrak{gl}\left(  N\right)
$-module. If $\lambda_{N+1}=0$, then $V_{\lambda}$ is irreducible both as a
representation of $\operatorname*{GL}\left(  N\right)  $ and as a
representation of $\mathfrak{gl}\left(  N\right)  $. If $\lambda_{N+1}\neq0$,
then $V_{\lambda}=0$.

It is known that there exists a unique polynomial $\chi_{\lambda}\in
\mathbb{Q}\left[  y_{1},y_{2},...,y_{N}\right]  $ (depending both on $\lambda$
and on $N$) such that every diagonal matrix $A=\operatorname*{diag}\left(
a_{1},a_{2},...,a_{N}\right)  \in\operatorname*{GL}\left(  N\right)  $
satisfies $\chi_{\lambda}\left(  a_{1},a_{2},...,a_{N}\right)  =\left(
\operatorname*{Tr}\mid_{V_{\lambda}}\right)  \left(  A\right)  $ (where
$\left(  \operatorname*{Tr}\mid_{V_{\lambda}}\right)  \left(  A\right)  $
means the trace of the action of $A\in\operatorname*{GL}\left(  N\right)  $ on
$V_{\lambda}$ by means of the $\operatorname*{GL}\left(  N\right)  $-module
structure on $V_{\lambda}$). In the language of representation theory,
$\chi_{\lambda}$ is thus the character of the $\operatorname*{GL}\left(
N\right)  $-module $V_{\lambda}$. This polynomial $\chi_{\lambda}$ is called
the $\lambda$\textit{-th Schur polynomial in }$N$ \textit{variables}.
\end{definition}

Now, the relation between the $S_{\lambda}$ and the Schur polynomials looks
like this:

\begin{proposition}
\label{prop.schur.Schur=schur}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition. Then, $\chi_{\lambda}\left(
y_{1},y_{2},...,y_{N}\right)  =\operatorname*{PSE}\nolimits_{N}\left(
S_{\lambda}\left(  x\right)  \right)  $.
\end{proposition}

This generalizes Proposition \ref{prop.schur.h_k.as.schur} (in fact, set
$\lambda=\left(  k\right)  $ and notice that $V_{\lambda}=S^{k}\mathbb{C}^{N}$).

\textit{Proof of Proposition \ref{prop.schur.Schur=schur}.} Define $h_{k}$ to
mean $0$ for every $k<0$.

Proposition \ref{prop.schur.h_k.as.schur} yields $h_{k}\left(  y\right)
=\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)  $ for
each $k\in\mathbb{N}$. Since $h_{k}\left(  y\right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)  $ also holds for every
nonnegative integer $k$ (since every nonnegative integer $k$ satisfies
$h_{k}=0$ and $S_{k}=0$), we thus conclude that%
\begin{equation}
h_{k}\left(  y\right)  =\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\label{pf.schur.Schur=schur.1}%
\end{equation}


We know that $\chi_{\lambda}$ is the $\lambda$-th Schur polynomial in $N$
variables. By the first Giambelli formula, this yields that%
\begin{align*}
&  \chi_{\lambda}\left(  y_{1},y_{2},...,y_{N}\right) \\
&  =\det\underbrace{\left(
\begin{array}
[c]{ccccc}%
h_{\lambda_{1}}\left(  y\right)  & h_{\lambda_{1}+1}\left(  y\right)  &
h_{\lambda_{1}+2}\left(  y\right)  & ... & h_{\lambda_{1}+m-1}\left(  y\right)
\\
h_{\lambda_{2}-1}\left(  y\right)  & h_{\lambda_{2}}\left(  y\right)  &
h_{\lambda_{2}+1}\left(  y\right)  & ... & h_{\lambda_{2}+m-2}\left(  y\right)
\\
h_{\lambda_{3}-2}\left(  y\right)  & h_{\lambda_{3}-1}\left(  y\right)  &
h_{\lambda_{3}}\left(  y\right)  & ... & h_{\lambda_{3}+m-3}\left(  y\right)
\\
... & ... & ... & ... & ...\\
h_{\lambda_{m}-m+1}\left(  y\right)  & h_{\lambda_{m}-m+2}\left(  y\right)  &
h_{\lambda_{m}-m+3}\left(  y\right)  & ... & h_{\lambda_{m}}\left(  y\right)
\end{array}
\right)  }_{=\left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}}\\
&  =\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  =\det\left(  \left(  \operatorname*{PSE}%
\nolimits_{N}\left(  S_{\lambda_{i}+j-i}\left(  x\right)  \right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.Schur=schur.1})}\right)
\\
&  =\operatorname*{PSE}\nolimits_{N}\underbrace{\left(  \det\left(  \left(
S_{\lambda_{i}+j-i}\left(  x\right)  \right)  _{1\leq i\leq m,\ 1\leq j\leq
m}\right)  \right)  }_{=S_{\lambda}\left(  x\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{PSE}\nolimits_{N}\text{ is a }\mathbb{C}%
\text{-algebra homomorphism, whereas }\det\text{ is a polynomial}\\
\text{(and any }\mathbb{C}\text{-algebra homomorphism commutes with any
polynomial)}%
\end{array}
\right) \\
&  =\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)
\right)  .
\end{align*}
Proposition \ref{prop.schur.Schur=schur} is proven.

\subsubsection{The statement of the fact}

\begin{theorem}
\label{thm.schur}Whenever $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
$0$-degression (see Definition \ref{def.glinf.m-deg} for what this means), we
have $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =S_{\lambda}\left(  x\right)  $ where $\lambda=\left(
i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $. (Note that this $\lambda$ is indeed a
partition since $\left(  i_{0},i_{1},i_{2},...\right)  $ is a $0$-degression.)
\end{theorem}

We are going to give two proofs of this theorem. The first proof will be
covered in Subsubsections \ref{subsubsect.powersums} to
\ref{subsubsect.schur1}, whereas the second proof will encompass
Subsubsections \ref{subsubsect.newton} to \ref{subsubsect.schur2}.

\subsubsection{\label{subsubsect.powersums}The power sums are algebraically
independent}

Our first proof of Theorem \ref{thm.schur} will require some lemmata from
algebraic combinatorics. First of all:

\begin{lemma}
\label{lem.schur.algind}Let $N\in\mathbb{N}$. For every positive integer $j$,
let $p_{j}$ denote the polynomial $y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}%
\in\mathbb{C}\left[  y_{1},y_{2},...,y_{N}\right]  $. Then, the polynomials
$p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically independent.
\end{lemma}

In order to prove this fact, we need the following known facts (which we won't prove):

\begin{lemma}
\label{lem.schur.elsym}Let $N\in\mathbb{N}$. For every $j\in\mathbb{N}$, let
$e_{j}$ denote the $j$-th elementary symmetric polynomial $\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{j}\leq N}y_{i_{1}}y_{i_{2}}...y_{i_{j}}$ in $\mathbb{C}%
\left[  y_{1},y_{2},...,y_{N}\right]  $. Then, the elements $e_{1}$, $e_{2}$,
$...$, $e_{N}$ are algebraically independent.
\end{lemma}

Lemma \ref{lem.schur.elsym} is one half of a known theorem. The other half
says that the elements $e_{1}$, $e_{2}$, $...$, $e_{N}$ generate the
$\mathbb{C}$-algebra of symmetric polynomials in $\mathbb{C}\left[
y_{1},y_{2},...,y_{N}\right]  $. We will prove neither of these halves; they
are both classical and well-known (under the name "fundamental theorem of
symmetric polynomials", which is usually formulated in a more general setting
when $\mathbb{C}$ is replaced by any commutative ring).

\begin{lemma}
\label{lem.schur.newtonid}Let $N\in\mathbb{N}$. For every positive integer
$j$, define $p_{j}$ as in Lemma \ref{lem.schur.algind}. For every
$j\in\mathbb{N}$, define $e_{j}$ as in Lemma \ref{lem.schur.elsym}. Then,
every $k\in\mathbb{N}$ satisfies $ke_{k}=\sum\limits_{i=1}^{k}\left(
-1\right)  ^{i-1}e_{k-i}p_{i}$.
\end{lemma}

This lemma is known as the \textit{Newton identity} (or identities), and won't
be proven due to being well-known. But we will use it to derive the following corollary:

\begin{corollary}
\label{cor.schur.newton}Let $N\in\mathbb{N}$. For every positive integer $j$,
define $p_{j}$ as in Lemma \ref{lem.schur.algind}. For every $j\in\mathbb{N}$,
define $e_{j}$ as in Lemma \ref{lem.schur.elsym}. Then, for every positive
$k\in\mathbb{N}$, there exists a polynomial $P_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k}\right]  $ such that $p_{k}=P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  $ and $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $. (Here, of course,
$\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $ is identified with a
subalgebra of $\mathbb{Q}\left[  T_{1},T_{2},...,T_{k}\right]  $.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.newton}.} We will prove Corollary
\ref{cor.schur.newton} by strong induction over $k$:

\textit{Induction step:} Let $\ell$ be a positive integer. Assume that
Corollary \ref{cor.schur.newton} holds for every positive integer $k<\ell$. We
must then prove that Corollary \ref{cor.schur.newton} holds for $k=\ell$.

Corollary \ref{cor.schur.newton} holds for every positive integer $k<\ell$ (by
the induction hypothesis). In other words, for every $k<\ell$, there exists a
polynomial $P_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k}\right]  $ such
that $p_{k}=P_{k}\left(  e_{1},e_{2},...,e_{k}\right)  $ and $P_{k}-\left(
-1\right)  ^{k-1}kT_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $.
Consider these polynomials $P_{1}$, $P_{2}$, $...$, $P_{\ell-1}$.

Applying Lemma \ref{lem.schur.newtonid} to $k=\ell$, we obtain%
\begin{align*}
\ell e_{\ell}  &  =\sum\limits_{i=1}^{\ell}\left(  -1\right)  ^{i-1}e_{\ell
-i}p_{i}=\sum\limits_{k=1}^{\ell}\left(  -1\right)  ^{k-1}e_{\ell-k}%
p_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed }i\text{ as }k\text{
in the sum}\right) \\
&  =\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}+\left(
-1\right)  ^{\ell-1}\underbrace{e_{\ell-\ell}}_{=e_{0}=1}p_{\ell}%
=\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}+\left(
-1\right)  ^{\ell-1}p_{\ell},
\end{align*}
so that $\left(  -1\right)  ^{\ell-1}p_{\ell}=\ell e_{\ell}-\sum
\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}$ and thus%
\[
p_{\ell}=\left(  -1\right)  ^{\ell-1}\left(  \ell e_{\ell}-\sum\limits_{k=1}%
^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}\right)  =\left(  -1\right)
^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell
-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}.
\]


Now, define a polynomial $P_{\ell}\in\mathbb{Q}\left[  T_{1},T_{2}%
,...,T_{\ell}\right]  $ by%
\[
P_{\ell}=\left(  -1\right)  ^{\ell-1}\ell T_{\ell}-\left(  -1\right)
^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}T_{\ell-k}%
P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  .
\]
Then,%
\[
P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}=-\left(  -1\right)
^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}%
\underbrace{T_{\ell-k}}_{\substack{\in\mathbb{Q}\left[  T_{1},T_{2}%
,...,T_{\ell-1}\right]  \\\text{(since }\ell-k\leq\ell-1\text{)}%
}}\underbrace{P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  }_{\substack{\in
\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  \\\text{(since }k\leq
\ell-1\text{)}}}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  .
\]
Moreover, $P_{\ell}=\left(  -1\right)  ^{\ell-1}\ell T_{\ell}-\left(
-1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}%
T_{\ell-k}P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  $ yields%
\begin{align*}
P_{\ell}\left(  e_{1},e_{2},...,e_{\ell}\right)   &  =\left(  -1\right)
^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell
-1}\left(  -1\right)  ^{k-1}e_{\ell-k}\underbrace{P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  }_{\substack{=p_{k}\\\text{(by the definition of
}P_{k}\text{)}}}\\
&  =\left(  -1\right)  ^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}%
\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}=p_{\ell}.
\end{align*}


We thus have shown that $p_{\ell}=P_{\ell}\left(  e_{1},e_{2},...,e_{\ell
}\right)  $ and $P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  $. Thus, there exists
a polynomial $P_{\ell}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell}\right]  $
such that $p_{\ell}=P_{\ell}\left(  e_{1},e_{2},...,e_{\ell}\right)  $ and
$P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{\ell-1}\right]  $. In other words, Corollary
\ref{cor.schur.newton} holds for $k=\ell$. This completes the induction step.
The induction proof of Corollary \ref{cor.schur.newton} is thus complete.

\textit{Proof of Lemma \ref{lem.schur.algind}.} Assume the contrary. Thus, the
polynomials $p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically dependent.
Hence, there exists a nonzero polynomial $Q\in\mathbb{C}\left[  U_{1}%
,U_{2},...,U_{N}\right]  $ such that $Q\left(  p_{1},p_{2},...,p_{N}\right)
=0$. Consider this $Q$.

Consider the lexicographic order on the monomials in $\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $ given by $T_{1}<T_{2}<...<T_{N}$.

For every $j\in\mathbb{N}$, define $e_{j}$ as in Lemma \ref{lem.schur.elsym}.
For every positive $k\in\mathbb{N}$, Corollary \ref{cor.schur.newton}
guarantees the existence of a polynomial $P_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k}\right]  $ such that $p_{k}=P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  $ and $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $. Consider such a
polynomial $P_{k}$.

For every $k\in\left\{  1,2,...,N\right\}  $, there exists a polynomial
$Q_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $ such that
$P_{k}-\left(  -1\right)  ^{k-1}kT_{k}=Q_{k}\left(  T_{1},T_{2},...,T_{k-1}%
\right)  $ (since $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k-1}\right]  $). Consider such a polynomial $Q_{k}$.

For every $k\in\left\{  1,2,...,N\right\}  $, let $\widetilde{P}_{k}$ be the
polynomial $P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  \in\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $. (This is the same polynomial as $P_{k}$, but
now considered as a polynomial in $N$ variables over $\mathbb{C}$ rather than
in $k$ variables over $\mathbb{Q}$.)

Then, for every $k\in\left\{  1,2,...,N\right\}  $, we have%
\begin{align*}
\widetilde{P}_{k}\left(  e_{1},e_{2},...,e_{N}\right)   &  =P_{k}\left(
e_{1},e_{2},...,e_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\widetilde{P}_{k}=P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  \right) \\
&  =p_{k}.
\end{align*}


Also, for every $k\in\left\{  1,2,...,N\right\}  $, the leading
monomial\footnote{Here, "monomial" means "monomial without coefficient", and
the "leading monomial" of a polynomial means the highest monomial (with
nonzero coefficient) of the polynomial.} of $\widetilde{P}_{k}$ (with respect
to the lexicographic order defined above) is $T_{k}$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,...,N\right\}  $.
Then,
\begin{align*}
\underbrace{\widetilde{P}_{k}}_{=P_{k}\left(  T_{1},T_{2},...,T_{k}\right)
}-\left(  -1\right)  ^{k-1}kT_{k}  &  =P_{k}\left(  T_{1},T_{2},...,T_{k}%
\right)  -\left(  -1\right)  ^{k-1}kT_{k}\\
&  =\underbrace{\left(  P_{k}-\left(  -1\right)  ^{k-1}kT_{k}\right)
}_{=Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  }\left(  T_{1},T_{2}%
,...,T_{k}\right) \\
&  =\left(  Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  \right)  \left(
T_{1},T_{2},...,T_{k}\right)  =Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  ,
\end{align*}
so that $\widetilde{P}_{k}=\left(  -1\right)  ^{k-1}kT_{k}+Q_{k}\left(
T_{1},T_{2},...,T_{k-1}\right)  $. Hence, the only monomials which occur with
nonzero coefficient in the polynomial $\widetilde{P}_{k}$ are the monomial
$T_{k}$ (occuring with coefficient $\left(  -1\right)  ^{k-1}k$) and the
monomials of the polynomial $Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  $.
But the latter monomials don't contain any variable other than $T_{1}$,
$T_{2}$, $...$, $T_{k-1}$ (because they are monomials of the polynomial
$Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  $), and thus are smaller than
the monomial $T_{k}$ (because any monomial which doesn't contain any variable
other than $T_{1}$, $T_{2}$, $...$, $T_{k-1}$ is smaller than any monomial
which contains $T_{k}$ (since we have a lexicographic order given by
$T_{1}<T_{2}<...<T_{N}$)). Hence, the leading monomial of $\widetilde{P}_{k}$
must be $T_{k}$, qed.}. Since the leading monomial of a product of polynomials
equals the product of their leading monomials, this yields that for every
$\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$,%
\begin{equation}
\text{the leading monomial of }\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\text{ is }T_{1}^{\alpha
_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}. \label{pf.schur.algind.3}%
\end{equation}


Since every $k\in\left\{  1,2,...,N\right\}  $ satisfies $p_{k}=\widetilde{P}%
_{k}\left(  e_{1},e_{2},...,e_{N}\right)  $, we have%
\begin{align*}
Q\left(  p_{1},p_{2},...,p_{N}\right)   &  =Q\left(  \widetilde{P}_{1}\left(
e_{1},e_{2},...,e_{N}\right)  ,\widetilde{P}_{2}\left(  e_{1},e_{2}%
,...,e_{N}\right)  ,...,\widetilde{P}_{N}\left(  e_{1},e_{2},...,e_{N}\right)
\right) \\
&  =\left(  Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}%
_{N}\right)  \right)  \left(  e_{1},e_{2},...,e_{N}\right)  .
\end{align*}
Hence, $Q\left(  p_{1},p_{2},...,p_{N}\right)  =0$ rewrites as $\left(
Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}_{N}\right)
\right)  \left(  e_{1},e_{2},...,e_{N}\right)  =0$. Since $e_{1}$, $e_{2}$,
$...$, $e_{N}$ are algebraically independent (by Lemma \ref{lem.schur.elsym}),
this yields $Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}%
_{N}\right)  =0$. Since $Q\neq0$, this shows that the elements $\widetilde{P}%
_{1}$, $\widetilde{P}_{2}$, $...$, $\widetilde{P}_{N}$ are algebraically
dependent. In other words, the family $\left(  \widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\right)
_{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}}$ is
linearly dependent. Thus, there exists a family $\left(  \lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\right)  _{\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}}$ of elements of $\mathbb{C}$
such that:

\begin{itemize}
\item all but finitely many $\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$;

\item not all $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}=0$;

\item we have $\sum\limits_{\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}=0$.
\end{itemize}

Consider this family. By identifying every $N$-tuple $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ with the monomial
$T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}\in\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $, we obtain a lexicographic order on the
$N$-tuples $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  $ (from the
lexicographic order on the monomials in $\mathbb{C}\left[  T_{1}%
,T_{2},...,T_{N}\right]  $).

Since all but finitely many $\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$, but not all $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1}%
,\alpha_{2},...,\alpha_{N}}=0$, there exists a highest (with respect to the
above-defined order) $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}$ satisfying $\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}\neq0$. Let this $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  $ be
called $\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $. Then, $\left(
\beta_{1},\beta_{2},...,\beta_{N}\right)  $ is the highest $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfying
$\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\neq0$. Thus, $\lambda
_{\beta_{1},\beta_{2},...,\beta_{N}}\neq0$, but
\begin{equation}
\text{every }\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}\text{ higher than }\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  \text{ satisfies }\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}=0. \label{pf.schur.algind.4}%
\end{equation}


Now it is easy to see that for every $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfying $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $, the term%
\begin{equation}
\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\text{ is a
}\mathbb{C}\text{-linear combination of monomials smaller than }T_{1}%
^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}. \label{pf.schur.algind.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.algind.5}).} Let $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\left(
\alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $. Since the lexicographic order is a total order,
we must be in one of the following two cases:
\par
\textit{Case 1:} We have $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\geq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $.
\par
\textit{Case 2:} We have $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
<\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $.
\par
First, consider Case 1. In this case, $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \geq\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  $, so that $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
>\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $ (since $\left(
\alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $). Thus, $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ is higher than $\left(  \beta
_{1},\beta_{2},...,\beta_{N}\right)  $. Hence, $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$ (by (\ref{pf.schur.algind.4})), so that $\lambda
_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}=0$ is clearly
a $\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\alpha
_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}$. Thus, (\ref{pf.schur.algind.5})
holds in Case 1.
\par
Now, let us consider Case 2. In this case, $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  <\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)
$, so that $T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}%
<T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$ (because the order on
$N$-tuples is obtained from the order on monomials by identifying every
$N$-tuple $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in
\mathbb{N}^{N}$ with the monomial $T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}%
}...T_{N}^{\alpha_{N}}\in\mathbb{C}\left[  T_{1},T_{2},...,T_{N}\right]  $).
\par
Due to (\ref{pf.schur.algind.3}), every monomial which occurs with nonzero
coefficient in $\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}%
}...\widetilde{P}_{N}^{\alpha_{N}}$ is smaller or equal to $T_{1}^{\alpha_{1}%
}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}$. Combined with $T_{1}^{\alpha_{1}%
}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}<T_{1}^{\beta_{1}}T_{2}^{\beta_{2}%
}...T_{N}^{\beta_{N}}$, this yields that every monomial which occurs with
nonzero coefficient in $\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}$ is smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Hence,
$\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}$ is a $\mathbb{C}$-linear combination of monomials smaller
than $T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Thus,
$\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. We have thus proven that
(\ref{pf.schur.algind.5}) holds in Case 2.
\par
Hence, (\ref{pf.schur.algind.5}) holds in each of cases 1 and 2. Since no
other cases are possible, this yields that (\ref{pf.schur.algind.5}) always
holds.} As a consequence,%
\[
\sum\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}%
\]
is a sum of $\mathbb{C}$-linear combinations of monomials smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$, and thus itself a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$.

Now,%
\begin{align*}
0  &  =\sum\limits_{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}%
\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}\\
&  =\lambda_{\beta_{1},\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta_{1}%
}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}+\sum
\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}},
\end{align*}
so that%
\[
\sum\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}=-\lambda_{\beta_{1}%
,\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}_{2}%
^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}.
\]
Since we know that $\sum\limits_{\substack{\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  }}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}%
_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}%
^{\alpha_{N}}$ is a $\mathbb{C}$-linear combination of monomials smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$, we thus conclude
that $-\lambda_{\beta_{1},\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta
_{1}}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Since $-\lambda_{\beta_{1},\beta
_{2},...,\beta_{N}}$ is invertible (because $\lambda_{\beta_{1},\beta
_{2},...,\beta_{N}}\neq0$), this yields that $\widetilde{P}_{1}^{\beta_{1}%
}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. In other words, every monomial which
occurs with nonzero coefficient in $\widetilde{P}_{1}^{\beta_{1}}%
\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is less than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. In particular, the
leading monomial of $\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}_{2}^{\beta
_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is less than $T_{1}^{\beta_{1}}%
T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. But this contradicts the fact that
(due to (\ref{pf.schur.algind.3}), applied to $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  =\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)
$) the leading monomial of $\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}%
_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is $T_{1}^{\beta_{1}}%
T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$.

This contradiction shows that our assumption was wrong. Hence, Lemma
\ref{lem.schur.algind} is proven.

(I have learned the above proof from:

Julia Pevtsova, 504A Fall 2009 Homework Set 3,\newline%
\texttt{http://www.math.washington.edu/\symbol{126}%
julia/teaching/504\_Fall2009/HW7\_sol.pdf} .)

We will apply Lemma \ref{lem.schur.algind} not directly, but through the
following corollary:

\begin{corollary}
\label{cor.schur.PSEinj}Let $P$ and $Q$ be polynomials in $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $. Assume that $\operatorname*{PSE}%
\nolimits_{N}\left(  P\right)  =\operatorname*{PSE}\nolimits_{N}\left(
Q\right)  $ for every sufficiently high $N\in\mathbb{N}$. Then, $P=Q$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.PSEinj}.} A polynomial (even if it
is a polynomial in infinitely many indeterminates) can have only finitely many
indeterminates actually appear in it. Hence, only finitely many indeterminates
appear in $P-Q$. Thus, there exists an $M\in\mathbb{N}$ such that no
indeterminates other than $x_{1}$, $x_{2}$, $...$, $x_{M}$ appear in $P-Q$.
Consider this $M$.

Recall that $\operatorname*{PSE}\nolimits_{N}\left(  P\right)
=\operatorname*{PSE}\nolimits_{N}\left(  Q\right)  $ for every sufficiently
high $N\in\mathbb{N}$. Thus, there exists an $N\in\mathbb{N}$ such that $N\geq
M$ and $\operatorname*{PSE}\nolimits_{N}\left(  P\right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  Q\right)  $. Pick such an $N$.

No indeterminates other than $x_{1}$, $x_{2}$, $...$, $x_{M}$ appear in $P-Q$.
Since $N\geq M$, this clearly yields that no indeterminates other than $x_{1}%
$, $x_{2}$, $...$, $x_{N}$ appear in $P-Q$. Then, there exists a polynomial
$R\in\mathbb{C}\left[  x_{1},x_{2},...,x_{N}\right]  $ such that $P-Q=R\left(
x_{1},x_{2},...,x_{N}\right)  $. Consider this $R$.

Now, let us use the notations of Lemma \ref{lem.schur.algind}.

We defined $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  $ as the
result of substituting $x_{j}=\dfrac{y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}}{j}$
for all positive integers $j$ into the polynomial $P-Q$. Since $y_{1}%
^{j}+y_{2}^{j}+...+y_{N}^{j}=p_{j}$ for all positive integers $j$, this
rewrites as follows: $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  $ is
the result of substituting $x_{j}=\dfrac{p_{j}}{j}$ for all positive integers
$j$ into the polynomial $P-Q$. In other words,
\begin{align*}
\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)   &  =\underbrace{\left(
P-Q\right)  }_{=R\left(  x_{1},x_{2},...,x_{N}\right)  }\left(  \dfrac{p_{1}%
}{1},\dfrac{p_{2}}{2},\dfrac{p_{3}}{3},...\right)  =\left(  R\left(
x_{1},x_{2},...,x_{N}\right)  \right)  \left(  \dfrac{p_{1}}{1},\dfrac{p_{2}%
}{2},\dfrac{p_{3}}{3},...\right) \\
&  =R\left(  \dfrac{p_{1}}{1},\dfrac{p_{2}}{2},...,\dfrac{p_{N}}{N}\right)  .
\end{align*}
But since $\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra
homomorphism, we have $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)
=\operatorname*{PSE}\nolimits_{N}\left(  P\right)  -\operatorname*{PSE}%
\nolimits_{N}\left(  Q\right)  =0$ (since $\operatorname*{PSE}\nolimits_{N}%
\left(  P\right)  =\operatorname*{PSE}\nolimits_{N}\left(  Q\right)  $). Thus,%
\[
R\left(  \dfrac{p_{1}}{1},\dfrac{p_{2}}{2},...,\dfrac{p_{N}}{N}\right)
=\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  =0.
\]
Since $\dfrac{p_{1}}{1}$, $\dfrac{p_{2}}{2}$, $...$, $\dfrac{p_{N}}{N}$ are
algebraically independent (because Lemma \ref{lem.schur.algind} yields that
$p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically independent), this yields
$R=0$, so that $P-Q=\underbrace{R}_{=0}\left(  x_{1},x_{2},...,x_{N}\right)
=0$, thus $P=Q$. Corollary \ref{cor.schur.PSEinj} is proven.

Corollary \ref{cor.schur.PSEinj} allows us to prove equality of polynomials in
$\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ by means of evaluating them
at power sums. Now, let us show what such evaluations look like for the Schur functions:

\subsubsection{\label{subsubsect.schur1}First proof of Theorem \ref{thm.schur}%
}

\begin{theorem}
\label{thm.schur.altern}Let $\lambda=\left(  \lambda_{1},\lambda_{2}%
,\lambda_{3},...\right)  $ be a partition, so that $\lambda_{1}\geq\lambda
_{2}\geq...$ are nonnegative integers.

Let $N$ be an integer such that $\lambda_{N}=0$. Then,%
\[
\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)  \right)
=\dfrac{\det\left(  \left(  y_{i}^{\lambda_{j-1}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }.
\]

\end{theorem}

\textit{Proof of Theorem \ref{thm.schur.altern}.} We will not really prove
this theorem; we will just reduce it to a known fact about Schur functions.

In fact, let $m$ be an integer such that $\lambda_{m+1}=0$ (such an integer
clearly exists). Then, the partition $\lambda$ can also be written in the form
$\left(  \lambda_{1},\lambda_{2},...,\lambda_{m}\right)  $. Hence, by the
first Giambelli formula, the $\lambda$-th Schur polynomial evaluated at
$\left(  y_{1},y_{2},...,y_{N}\right)  $ equals%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
h_{\lambda_{1}}\left(  y\right)  & h_{\lambda_{1}+1}\left(  y\right)  &
h_{\lambda_{1}+2}\left(  y\right)  & ... & h_{\lambda_{1}+m-1}\left(  y\right)
\\
h_{\lambda_{2}-1}\left(  y\right)  & h_{\lambda_{2}}\left(  y\right)  &
h_{\lambda_{2}+1}\left(  y\right)  & ... & h_{\lambda_{2}+m-2}\left(  y\right)
\\
h_{\lambda_{3}-2}\left(  y\right)  & h_{\lambda_{3}-1}\left(  y\right)  &
h_{\lambda_{3}}\left(  y\right)  & ... & h_{\lambda_{3}+m-3}\left(  y\right)
\\
... & ... & ... & ... & ...\\
h_{\lambda_{m}-m+1}\left(  y\right)  & h_{\lambda_{m}-m+2}\left(  y\right)  &
h_{\lambda_{m}-m+3}\left(  y\right)  & ... & h_{\lambda_{m}}\left(  y\right)
\end{array}
\right) \\
&  =\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  .
\end{align*}
But since the $\lambda$-th Schur polynomial evaluated at $\left(  y_{1}%
,y_{2},...,y_{N}\right)  $ also equals \newline$\dfrac{\det\left(  \left(
y_{i}^{\lambda_{j-1}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }$ (by the "Vandermonde-determinant" definition of Schur
polynomials), this yields that
\[
\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  =\dfrac{\det\left(  \left(  y_{i}%
^{\lambda_{j-1}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }.
\]
Comparing this with the equality $\det\left(  \left(  h_{\lambda_{i}%
+j-i}\left(  y\right)  \right)  _{1\leq i\leq m,\ 1\leq j\leq m}\right)
=\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)  \right)
$ (which was verified during the proof of Proposition
\ref{prop.schur.Schur=schur}), we obtain%
\[
\dfrac{\det\left(  \left(  y_{i}^{\lambda_{j-1}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }=\operatorname*{PSE}\nolimits_{N}\left(
S_{\lambda}\left(  x\right)  \right)  .
\]
Theorem \ref{thm.schur.altern} is thus proven.

We will now use a harmless-looking result about determinants:

\begin{proposition}
\label{prop.schur.det}Let $N\in\mathbb{N}$. Let $\left(  a_{i,j}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}$ be an $N\times N$-matrix of elements of a
commutative ring $R$. Let $b_{1}$, $b_{2}$, $...$, $b_{N}$ be $N$ elements of
$R$. Then,%
\begin{equation}
\sum\limits_{k=1}^{N}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  =\left(  b_{1}+b_{2}+...+b_{N}%
\right)  \det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  . \label{prop.schur.det.1}%
\end{equation}
Equivalently (in more reader-friendly terms):%
\begin{align}
&  \det\left(
\begin{array}
[c]{cccc}%
b_{1}a_{1,1} & a_{1,2} & ... & a_{1,N}\\
b_{2}a_{2,1} & a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
b_{N}a_{N,1} & a_{N,2} & ... & a_{N,N}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & b_{1}a_{1,2} & ... & a_{1,N}\\
a_{2,1} & b_{2}a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & b_{N}a_{N,2} & ... & a_{N,N}%
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +...+\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & ... & b_{1}a_{1,N}\\
a_{2,1} & a_{2,2} & ... & b_{2}a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & a_{N,2} & ... & b_{N}a_{N,N}%
\end{array}
\right) \nonumber\\
&  =\left(  b_{1}+b_{2}+...+b_{N}\right)  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & ... & a_{1,N}\\
a_{2,1} & a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & a_{N,2} & ... & a_{N,N}%
\end{array}
\right)  . \label{prop.schur.det.2}%
\end{align}

\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.det}.} Recall the explicit
formula for a determinant of a matrix as a sum over permutations: For every
$N\times N$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$,
we have%
\begin{equation}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}c_{\sigma\left(  j\right)  ,j}. \label{pf.schur.det.1}%
\end{equation}
Applied to $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}=\left(
a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$, this yields%
\begin{equation}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}a_{\sigma\left(  j\right)  ,j}. \label{pf.schur.det.2}%
\end{equation}


For every $k\in\left\{  1,2,...,N\right\}  $, we can apply
(\ref{pf.schur.det.1}) to $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}=\left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}$, and obtain%
\begin{align*}
\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)   &  =\sum\limits_{\sigma\in S_{N}}\left(
-1\right)  ^{\sigma}\underbrace{\prod\limits_{j=1}^{N}\left(  a_{i,j}%
b_{\sigma\left(  j\right)  }^{\delta_{j,k}}\right)  }_{=\prod\limits_{j=1}%
^{N}a_{i,j}\prod\limits_{j=1}^{N}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}%
}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{i,j}\underbrace{\prod\limits_{j=1}^{N}b_{\sigma\left(
j\right)  }^{\delta_{j,k}}}_{=b_{\sigma\left(  k\right)  }^{\delta_{k,k}}%
\prod\limits_{\substack{j\in\left\{  1,2,...,N\right\}  ;\\j\neq k}%
}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{i,j}\underbrace{b_{\sigma\left(  k\right)  }^{\delta
_{k,k}}}_{\substack{=b_{\sigma\left(  k\right)  }\\\text{(since }\delta
_{k,k}=1\text{)}}}\prod\limits_{\substack{j\in\left\{  1,2,...,N\right\}
;\\j\neq k}}\underbrace{b_{\sigma\left(  j\right)  }^{\delta_{j,k}}%
}_{\substack{=1\\\text{(since }j\neq k\text{ and thus }\delta_{j,k}=0\text{)}%
}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{i,j}b_{\sigma\left(  k\right)  }\underbrace{\prod
\limits_{\substack{j\in\left\{  1,2,...,N\right\}  ;\\j\neq k}}1}_{=1}%
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}a_{i,j}b_{\sigma\left(  k\right)  }.
\end{align*}
Hence,%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\sum\limits_{k=1}^{N}\sum\limits_{\sigma\in S_{N}}\left(  -1\right)
^{\sigma}\prod\limits_{j=1}^{N}a_{i,j}b_{\sigma\left(  k\right)  }%
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}a_{i,j}\underbrace{\sum\limits_{k=1}^{N}b_{\sigma\left(  k\right)  }%
}_{\substack{=\sum\limits_{k=1}^{N}b_{k}\\\text{(since }\sigma\text{ is a
permutation)}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{i,j}\sum\limits_{k=1}^{N}b_{k}=\underbrace{\left(
\sum\limits_{k=1}^{N}b_{k}\right)  }_{=b_{1}+b_{2}+...+b_{N}}\underbrace{\sum
\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}a_{i,j}}_{\substack{=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  \\\text{(by (\ref{pf.schur.det.2}))}}}\\
&  =\left(  b_{1}+b_{2}+...+b_{N}\right)  \det\left(  \left(  a_{i,j}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
This proves Proposition \ref{prop.schur.det}.

\begin{corollary}
\label{cor.schur.det}Let $N\in\mathbb{N}$. Let $\left(  i_{0},i_{1}%
,...,i_{N-1}\right)  \in\mathbb{Z}^{N}$ be such that $i_{j-1}+N>0$ for every
$j\in\left\{  1,2,...,N\right\}  $. Let $m\in\mathbb{N}$. Then,%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j,k}%
m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}

\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.det}.} Applying Proposition
\ref{prop.schur.det} to $R=\mathbb{C}\left[  y_{1},y_{2},...,y_{N}\right]  $,
$\left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}=\left(
y_{i}^{i_{j-1}+N}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ and $b_{i}%
=y_{i}^{m}$, we obtain%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\left(
y_{i}^{m}\right)  ^{\delta_{j,k}}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
Since any $i\in\left\{  1,2,...,N\right\}  $, $j\in\left\{  1,2,...,N\right\}
$ and $k\in\left\{  1,2,...,N\right\}  $ satisfy $y_{i}^{i_{j-1}+N-1}\left(
y_{i}^{m}\right)  ^{\delta_{j,k}}=y_{i}^{i_{j-1}+N+\delta_{j,k}m}%
=y_{i}^{i_{j-1}+\delta_{j,k}m+N-1}$, this rewrites as%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j,k}%
m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
Corollary \ref{cor.schur.det} is proven.

Now, to the main proof.

\textit{Proof of Theorem \ref{thm.schur}.} Define a $\mathbb{C}$-linear map
$\tau:\mathcal{F}^{\left(  0\right)  }\rightarrow\mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  $ by%
\[
\tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }0\text{-degression }\left(  i_{0}%
,i_{1},i_{2},...\right)  .
\]
(This definition makes sense, because we know that $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is a }0\text{-degression}}$ is a basis of
$\wedge^{\dfrac{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$.)

Our aim is to prove that $\tau=\sigma^{-1}$.

\textit{1st step:} First of all, the definition of $\tau$ (applied to the
$0$-degression $\left(  0,-1,-2,...\right)  $) yields%
\[
\tau\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  =S_{\left(
0+0,-1+1,-2+2,...\right)  }\left(  x\right)  =S_{\left(  0,0,0,...\right)
}\left(  x\right)  =1.
\]


\textit{2nd step:} If $N\in\mathbb{N}$, and $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $0$-degression, then we say that $\left(
i_{0},i_{1},i_{2},...\right)  $ is $N$\textit{-finished} if the following two
conditions (\ref{pf.schur.step2.fin1}) and (\ref{pf.schur.step2.fin2}) hold:%
\begin{align}
&  \left(  \text{every integer }k\geq N\text{ satisfies }i_{k}+k=0\right)
;\label{pf.schur.step2.fin1}\\
&  \left(  \text{each of the integers }i_{0}\text{, }i_{1}\text{, }...\text{,
}i_{N-1}\text{ is }>-N\right)  . \label{pf.schur.step2.fin2}%
\end{align}


Now, we claim the following:

For any $N\in\mathbb{N}$, and any $N$-finished straying $0$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, we have%
\begin{equation}
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\dfrac{\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step2}%
\end{equation}


\textit{Proof of (\ref{pf.schur.step2}):} Let $N\in\mathbb{N}$, and let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be an $N$-finished straying $0$-degression.

Since $\left(  i_{0},i_{1},i_{2},...\right)  $ is $N$-finished, we conclude
(by the definition of "$N$-finished") that it satisfies the conditions
(\ref{pf.schur.step2.fin1}) and (\ref{pf.schur.step2.fin2}).

If two of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are equal, then
(\ref{pf.schur.step2}) is true.\footnote{\textit{Proof.} Assume that two of
the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are equal. Then, two elements
of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ are equal, so that
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=0$ (by the definition of
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$) and thus
$\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  0\right)  =0$. Thus, the left hand side of
(\ref{pf.schur.step2}) is $0$. On the other hand, the matrix $\left(
y_{i}^{i_{j-1}+N}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ has two equal
columns (since two of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are
equal) and thus its determinant vanishes, i. e., we have $\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  =0$, so
that the right hand side of (\ref{pf.schur.step2}) is $0$.
\par
Thus, both the left hand side and the right hand side of (\ref{pf.schur.step2}%
) are $0$. Hence, (\ref{pf.schur.step2}) is true, qed.} Hence, for the rest of
this proof, we assume that no two of the integers $i_{0}$, $i_{1}$, $...$,
$i_{N-1}$ are equal. Then, there exists a permutation $\phi$ of the set
$\left\{  0,1,...,N-1\right\}  $ such that $i_{\phi^{-1}\left(  0\right)
}>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }$.
Consider this $\phi$.

Since $\phi^{-1}\left(  N-1\right)  \in\left\{  0,1,...,N-1\right\}  $, the
integer $i_{\phi^{-1}\left(  N-1\right)  }$ is one of the integers $i_{0}$,
$i_{1}$, $...$, $i_{N-1}$, and therefore $>-N$ (due to
(\ref{pf.schur.step2.fin2})). We thus have shown $i_{\phi^{-1}\left(
N-1\right)  }>-N$. Combining this with $i_{\phi^{-1}\left(  0\right)
}>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }$, we
get $i_{\phi^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(  1\right)
}>...>i_{\phi^{-1}\left(  N-1\right)  }>-N$.

Let $\pi$ be the permutation of $\mathbb{N}$ which sends every $k\in
\mathbb{N}$ to $\left\{
\begin{array}
[c]{l}%
\phi\left(  k\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k\in\left\{
0,1,...,N-1\right\}  ;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k\notin\left\{  0,1,...,N-1\right\}
\end{array}
\right.  $. Then, $\left(  -1\right)  ^{\pi}=\left(  -1\right)  ^{\phi}$;
moreover, every $k\in\mathbb{N}$ satisfies%
\begin{equation}
\pi^{-1}\left(  k\right)  =\left\{
\begin{array}
[c]{l}%
\phi^{-1}\left(  k\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k\in\left\{
0,1,...,N-1\right\}  ;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k\notin\left\{  0,1,...,N-1\right\}
\end{array}
\right.  . \label{pf.schur.step2.pf.2}%
\end{equation}
In particular, every integer $k\geq N$ satisfies $\pi^{-1}\left(  k\right)
=k$.

From (\ref{pf.schur.step2.pf.2}), it is clear that%
\begin{equation}
\text{every }k\in\left\{  0,1,...,N-1\right\}  \text{ satisfies }\pi
^{-1}\left(  k\right)  =\phi^{-1}\left(  k\right)  .
\label{pf.schur.step2.pf.4}%
\end{equation}
Hence, $i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)
}>...>i_{\pi^{-1}\left(  N-1\right)  }>-N$ (since $i_{\phi^{-1}\left(
0\right)  }>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(
N-1\right)  }>-N$).

Now, every integer $k\geq N$ satisfies $\pi^{-1}\left(  k\right)  =k$, thus
$i_{\pi^{-1}\left(  k\right)  }=i_{k}=-k$ (since (\ref{pf.schur.step2.fin1})
yields $i_{k}+k=0$). Hence, $-N=i_{\pi^{-1}\left(  N\right)  }>i_{\pi
^{-1}\left(  N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>...$ (because
$-N=-N>-\left(  N+1\right)  >-\left(  N+2\right)  >...$). Combined with
$i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>-N$, this becomes%
\[
i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>-N=i_{\pi^{-1}\left(  N\right)  }>i_{\pi
^{-1}\left(  N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>....
\]
Thus,%
\[
i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>i_{\pi^{-1}\left(  N\right)  }>i_{\pi^{-1}\left(
N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>....
\]
In other words, the sequence $\left(  i_{\pi^{-1}\left(  0\right)  }%
,i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)  },...\right)  $
is strictly decreasing. Since every sufficiently high $k\in\mathbb{N}$
satisfies $i_{\pi^{-1}\left(  k\right)  }+k=0$ (in fact, every $k\geq N$
satisfies $i_{\pi^{-1}\left(  k\right)  }=-k$ and thus $i_{\pi^{-1}\left(
k\right)  }+k=0$), this sequence $\left(  i_{\pi^{-1}\left(  0\right)
},i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)  },...\right)  $
must thus be a $0$-degression. Hence, by the definition of $\tau$, we have%
\[
\tau\left(  v_{i_{\pi^{-1}\left(  0\right)  }}\wedge v_{i_{\pi^{-1}\left(
1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }}\wedge...\right)
=S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  .
\]
Since $\pi$ is a permutation of $\mathbb{N}$ such that $\left(  i_{\pi
^{-1}\left(  0\right)  },i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(
2\right)  },...\right)  $ is a $0$-degression, $\pi$ is the straightening
permutation of $\left(  i_{0},i_{1},i_{2},...\right)  $. Thus, by the
definition of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, we have%
\begin{align*}
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...  &  =\underbrace{\left(
-1\right)  ^{\pi}}_{=\left(  -1\right)  ^{\phi}}v_{i_{\pi^{-1}\left(
0\right)  }}\wedge v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi
^{-1}\left(  2\right)  }}\wedge...\\
&  =\left(  -1\right)  ^{\phi}v_{i_{\pi^{-1}\left(  0\right)  }}\wedge
v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }%
}\wedge...,
\end{align*}
so that%
\begin{align}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \nonumber\\
&  =\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  \left(  -1\right)
^{\phi}v_{i_{\pi^{-1}\left(  0\right)  }}\wedge v_{i_{\pi^{-1}\left(
1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }}\wedge...\right)  \right)
\nonumber\\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}%
\underbrace{\left(  \tau\left(  v_{i_{\pi^{-1}\left(  0\right)  }}\wedge
v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }%
}\wedge...\right)  \right)  }_{=S_{\left(  i_{\pi^{-1}\left(  0\right)
}+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi^{-1}\left(  2\right)
}+2,...\right)  }\left(  x\right)  }\nonumber\\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(
S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  \right)  .
\label{pf.schur.step2.pf.4a}%
\end{align}


Let $\mu$ be the partition $\left(  i_{\pi^{-1}\left(  0\right)  }%
+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi^{-1}\left(  2\right)
}+2,...\right)  $. Then, every $j\in\left\{  1,2,...,N\right\}  $ satisfies%
\begin{align*}
\mu_{j-1}  &  =i_{\pi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mu\right) \\
&  =i_{\phi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since (\ref{pf.schur.step2.pf.4}) (applied
to }k=j-1\text{) yields }\pi^{-1}\left(  j-1\right)  =\phi^{-1}\left(
j-1\right)  \right)  ,
\end{align*}
so that $\mu_{j-1}+N-j=i_{\phi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
+N-j=i_{\phi^{-1}\left(  j-1\right)  }+N-1$. Hence,%
\begin{equation}
\det\left(  \left(  y_{i}^{\mu_{j-1}+N-j}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  =\det\left(  \left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)
}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\label{pf.schur.step2.pf.5}%
\end{equation}
But the matrix $\left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }+N-1}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}$ is obtained from the matrix $\left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ by permuting the
columns using the permutation $\phi$. Hence,%
\[
\det\left(  \left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }+N-1}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  =\left(  -1\right)  ^{\phi}%
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)
\]
(since permuting the columns of a matrix changes the determinant by the sign
of the permutation). Combining this with (\ref{pf.schur.step2.pf.5}), we
obtain%
\begin{equation}
\det\left(  \left(  y_{i}^{\mu_{j-1}+N-j}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  =\left(  -1\right)  ^{\phi}\det\left(  \left(  y_{i}%
^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\label{pf.schur.step2.pf.7}%
\end{equation}


Also, by the definition of $\mu$, we have $\mu_{N}=i_{\pi^{-1}\left(
N\right)  }+N=0$ (because $-N=i_{\pi^{-1}\left(  N\right)  }$), and thus we
can apply Theorem \ref{thm.schur.altern} to $\mu$ instead of $\lambda$. This
results in
\begin{align}
\operatorname*{PSE}\nolimits_{N}\left(  S_{\mu}\left(  x\right)  \right)   &
=\dfrac{\det\left(  \left(  y_{i}^{\mu_{j-1}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }\nonumber\\
&  =\dfrac{\left(  -1\right)  ^{\phi}\det\left(  \left(  y_{i}^{i_{j-1}%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(
y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }
\label{pf.schur.step2.pf.8}%
\end{align}
(by (\ref{pf.schur.step2.pf.7})). But (\ref{pf.schur.step2.pf.4a}) becomes%
\begin{align*}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(
S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  \right) \\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(  S_{\mu
}\left(  x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi
^{-1}\left(  2\right)  }+2,...\right)  =\mu\right) \\
&  =\left(  -1\right)  ^{\phi}\dfrac{\left(  -1\right)  ^{\phi}\det\left(
\left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.step2.pf.8}%
)}\right) \\
&  =\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }.
\end{align*}
This proves (\ref{pf.schur.step2}). The proof of the 2nd step is thus complete.

\textit{3rd step:} Consider the action of the Heisenberg algebra $\mathcal{A}$
on $\widetilde{F}=\mathcal{B}^{\left(  0\right)  }$ and $\wedge^{\dfrac
{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$. We will now prove that the
map $\tau:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ satisfies%
\begin{equation}
\tau\circ a_{-m}=a_{-m}\circ\tau\ \ \ \ \ \ \ \ \ \ \text{for every positive
integer }m. \label{pf.schur.step3}%
\end{equation}


\textit{Proof of (\ref{pf.schur.step3}):} Let $m$ be a positive integer.

Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. By the
definition of a $0$-degression, $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
strictly decreasing sequence of integers such that every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=0$. In other words, there exists an
$\ell\in\mathbb{N}$ such that every integer $k\geq\ell$ satisfies $i_{k}+k=0$.
Consider this $\ell$.

Let $N$ be any integer satisfying $N\geq\ell+m$. Then, it is easy to see that,
for every integer $k\geq N$, we have $i_{k}+m=i_{k-m}$.

By the definition of the $\mathcal{A}$-module structure on $\wedge
^{\dfrac{\infty}{2},0}V$, the action of $a_{-m}$ on $\wedge^{\dfrac{\infty}%
{2},0}V$ is $\widehat{\rho}\left(  T^{-m}\right)  $, where $T$ is the shift
operator. Thus,%
\begin{equation}
a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\left(  \widehat{\rho}\left(  T^{-m}\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  . \label{pf.schur.step3.2}%
\end{equation}


Since $m\neq0$, the matrix $T^{-m}$ has the property that, for every integer
$i$, the $\left(  i,i\right)  $-th entry of $T^{-m}$ is $0$. Hence,
Proposition \ref{prop.glinf.ainfact} (applied to $m=0$, $a=T^{-m}$ and
$a_{k}=v_{i_{k}}$) yields%
\begin{align*}
&  \left(  \widehat{\rho}\left(  T^{-m}\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{-m}\rightharpoonup v_{i_{k}}\right)
}_{=v_{i_{k}+m}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\underbrace{\sum\limits_{\substack{k\geq0;\\k<N}}}_{=\sum\limits_{k=0}%
^{N-1}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{\substack{=v_{i_{0}%
+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge...\wedge v_{i_{k-1}%
+\delta_{k-1,k}m}\wedge v_{i_{k}+\delta_{k,k}m}\wedge v_{i_{k+1}%
+\delta_{k+1,k}m}\wedge v_{i_{k+2}+\delta_{k+2,k}m}\wedge...\\\text{(here we
are simply making use of the fact that every }j\in\mathbb{N}\text{ such that
}j\neq k\text{ satisfies}\\i_{j}=i_{j}+\delta_{j,k}m\text{ (since }%
\delta_{j,k}=0\text{), whereas }i_{k}+m=i_{k}+\delta_{k,k}m\text{ (since
}\delta_{k,k}=1\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq N}\underbrace{v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...}_{\substack{=0\text{ (because the sequence }\left(
i_{0},i_{1},...,i_{k-1},i_{k}+m,i_{k+1},i_{k+2},...\right)  \\\text{has two
equal elements (since }i_{k}+m=i_{k-m}\text{))}}}\\
&  =\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta
_{1,k}m}\wedge...\wedge v_{i_{k-1}+\delta_{k-1,k}m}\wedge v_{i_{k}%
+\delta_{k,k}m}\wedge v_{i_{k+1}+\delta_{k+1,k}m}\wedge v_{i_{k+2}%
+\delta_{k+2,k}m}\wedge...\\
&  =\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta
_{1,k}m}\wedge v_{i_{2}+\delta_{2,k}m}\wedge....
\end{align*}
Combined with (\ref{pf.schur.step3.2}), this yields%
\[
a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}%
m}\wedge v_{i_{2}+\delta_{2,k}m}\wedge...,
\]
so that%
\begin{align}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
\nonumber\\
&  =\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  \sum\limits_{k=0}%
^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge
v_{i_{2}+\delta_{2,k}m}\wedge...\right)  \right) \nonumber\\
&  =\sum\limits_{k=0}^{N-1}\underbrace{\operatorname*{PSE}\nolimits_{N}\left(
\tau\left(  v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge
v_{i_{2}+\delta_{2,k}m}\wedge...\right)  \right)  }_{\substack{=\dfrac
{\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j-1,k}m+N-1}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }\\\text{(by (\ref{pf.schur.step2}), applied
to }\left(  i_{0}+\delta_{0,k}m,i_{1}+\delta_{1,k}m,i_{2}+\delta
_{2,k}m,...\right)  \\\text{instead of }\left(  i_{0},i_{1},i_{2},...\right)
\text{ (since }\left(  i_{0}+\delta_{0,k}m,i_{1}+\delta_{1,k}m,i_{2}%
+\delta_{2,k}m,...\right)  \text{ is easily seen to}\\\text{ be an
}N\text{-finished straying }0\text{-degression))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\text{ and }\tau\text{ are both linear}\right) \nonumber\\
&  =\sum\limits_{k=0}^{N-1}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j-1,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\nonumber\\
&  =\sum\limits_{k=1}^{N}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j-1,k-1}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k-1\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{k=1}^{N}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{j-1,k-1}%
=\delta_{j,k}\text{ for all }j\text{ and }k\right) \nonumber\\
&  =\dfrac{1}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }\underbrace{\sum\limits_{k=1}^{N}\det\left(  \left(
y_{i}^{i_{j-1}+\delta_{j,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }_{\substack{=\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  \\\text{(by Corollary \ref{cor.schur.det})}}}\nonumber\\
&  =\dfrac{1}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right) \nonumber\\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \cdot\dfrac{\det\left(
\left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step3.8}%
\end{align}
On the other hand, since $\left(  i_{0},i_{1},i_{2},...\right)  $ is strictly
decreasing, $\left(  i_{0},i_{1},i_{2},...\right)  $ is $N$-finished. Thus,
(\ref{pf.schur.step2}) yields%
\begin{equation}
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\dfrac{\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step3.9}%
\end{equation}
Now, (\ref{pf.schur.step3.8}) becomes%
\begin{align*}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right) \\
&  =\underbrace{\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
}_{\substack{=m\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)
\\\text{(since the definition of }\operatorname*{PSE}\nolimits_{N}\text{
yields}\\\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)  =\dfrac
{y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}}{m}\text{)}}}\cdot\underbrace{\dfrac
{\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }}_{\substack{=\operatorname*{PSE}\nolimits_{N}\left(
\tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\right)  \\\text{(by (\ref{pf.schur.step3.9}))}}}\\
&  =m\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)  \cdot
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\operatorname*{PSE}\nolimits_{N}\underbrace{\left(  mx_{m}\cdot\tau\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)
}_{\substack{=a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  \\\text{(since }a_{-m}\text{ acts on
}\widetilde{F}\text{ as multiplication by }mx_{m}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\text{ is a }\mathbb{C}\text{-algebra homomorphism}\right) \\
&  =\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)  .
\end{align*}
Now forget that we fixed $N$. We thus have shown that every integer $N\geq
\ell+m$ satisfies%
\[
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
=\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)  .
\]
Hence,%
\[
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
=\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
\]
for every sufficiently high $N\in\mathbb{N}$. Thus, Corollary
\ref{cor.schur.PSEinj} (applied to $P=\tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  $ and
$Q=a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  $) yields that
\[
\tau\left(  a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  =a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  .
\]
In other words,%
\[
\left(  \tau\circ a_{-m}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  =\left(  a_{-m}\circ\tau\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\]
Now forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We have
thus shown that $\left(  \tau\circ a_{-m}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(  a_{-m}\circ\tau\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ for every
$0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, the maps
$\tau\circ a_{-m}$ and $a_{-m}\circ\tau$ are equal to each other on a basis of
$\wedge^{\dfrac{\infty}{2},0}V$ (namely, on the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is a }0\text{-degression}}$). Since these two maps
are linear, this yields that these two maps must be identical, i. e., we have
$\tau\circ a_{-m}=a_{-m}\circ\tau$. This proves (\ref{pf.schur.step3}). The
proof of the 3rd step is thus complete.

\textit{4th step:} We can now easily conclude Theorem \ref{thm.schur}.

Let $\mathcal{A}_{-}$ be the Lie subalgebra $\left\langle a_{-1},a_{-2}%
,a_{-3},...\right\rangle $ of $\mathcal{A}$. Then, $\tau$ is an $\mathcal{A}%
_{-}$-module homomorphism $\wedge^{\dfrac{\infty}{2},0}V\rightarrow
\widetilde{F}$ (according to (\ref{pf.schur.step3})).

Consider the element $\psi_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$ of
$\wedge^{\dfrac{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$. By the
definition of $\sigma_{0}$, we have $\sigma_{0}\left(  1\right)  =\psi_{0}$,
so that $\sigma_{0}^{-1}\left(  \psi_{0}\right)  =1$. Compared with%
\begin{align*}
\tau\left(  \psi_{0}\right)   &  =\tau\left(  v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi
_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right) \\
&  =1,
\end{align*}
this yields $\tau\left(  \psi_{0}\right)  =\sigma_{0}^{-1}\left(  \psi
_{0}\right)  $.

From Lemma \ref{lem.F.P1=P}, it is clear that the Fock module $F$ is generated
by $1$ as an $\mathcal{A}_{-}$-module (since $\mathcal{A}_{-}=\left\langle
a_{-1},a_{-2},a_{-3},...\right\rangle $). Since there exists an $\mathcal{A}%
_{-}$-module isomorphism $F\rightarrow\widetilde{F}$ which sends $1$ to $1$
(in fact, the map $\operatorname*{resc}$ of Proposition \ref{prop.resc} is
such an isomorphism), this yields that $\widetilde{F}$ is generated by $1$ as
an $\mathcal{A}_{-}$-module. Since there exists an $\mathcal{A}_{-}$-module
isomorphism $\widetilde{F}\rightarrow\wedge^{\dfrac{\infty}{2},0}V$ which
sends $1$ to $\psi_{0}$ (in fact, the map $\sigma_{0}$ is such an isomorphism,
since $\sigma_{0}\left(  1\right)  =\psi_{0}$), this yields that
$\wedge^{\dfrac{\infty}{2},0}V$ is generated by $\psi_{0}$ as an
$\mathcal{A}_{-}$-module. Hence, if two $\mathcal{A}_{-}$-module homomorphisms
from $\wedge^{\dfrac{\infty}{2},0}V$ to another $\mathcal{A}_{-}$-module are
equal to each other on $\psi_{0}$, then they must be identical. We can apply
this observation to the two $\mathcal{A}_{-}$-module homomorphisms
$\tau:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ and $\sigma
_{0}^{-1}:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ (which are
equal to each other on $\psi_{0}$, since $\tau\left(  \psi_{0}\right)
=\sigma_{0}^{-1}\left(  \psi_{0}\right)  $), and conclude that these
homomorphisms are identical, i. e., we have $\tau=\sigma_{0}^{-1}$. Now, every
$0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =\underbrace{\sigma_{0}^{-1}}_{=\tau}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\right) \\
&  =S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)  ,
\end{align*}
where $\lambda=\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $. This proves
Theorem \ref{thm.schur}.

\subsubsection{\label{subsubsect.newton}The multivariate Newton formula}

Before we step to the second proof of Theorem \ref{thm.schur}, we show a lemma
about polynomials over $\mathbb{Q}$-algebras:

\begin{lemma}
\label{lem.hirota.newton}Let $K$ be a commutative $\mathbb{Q}$-algebra, let
$\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of elements of $K$, and
let $\left(  z_{1},z_{2},z_{3},...\right)  $ be a sequence of new symbols.
Denote the sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ by $y$. Denote
the sequence $\left(  z_{1},z_{2},z_{3},...\right)  $ by $z$. Then, every
$P\in K\left[  z_{1},z_{2},z_{3},...\right]  $ satisfies%
\[
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
P\left(  z\right)  =P\left(  y+z\right)  .
\]
Here, $y+z$ means the componentwise sum of the sequences $y$ and $z$ (so that
$y+z=\left(  y_{1}+z_{1},y_{2}+z_{2},y_{3}+z_{3},...\right)  $).
\end{lemma}

If you think about Lemma \ref{lem.hirota.newton}, you will notice that it is a
multivariate version of the famous Newton formula%
\[
\exp\left(  \alpha\dfrac{\partial}{\partial\xi}\right)  P\left(  \xi\right)
=P\left(  \alpha+\xi\right)
\]
which holds for any polynomial $P\in K\left[  \xi\right]  $ and any $\alpha\in
K$.

\textit{Proof of Lemma \ref{lem.hirota.newton}.} Let $A$ be the map%
\[
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
:K\left[  z_{1},z_{2},z_{3},...\right]  \rightarrow K\left[  z_{1},z_{2}%
,z_{3},...\right]
\]
(this is easily seen to be well-defined). Let $B$ be the map%
\[
K\left[  z_{1},z_{2},z_{3},...\right]  \rightarrow K\left[  z_{1},z_{2}%
,z_{3},...\right]  ,\ \ \ \ \ \ \ \ \ \ P\mapsto P\left(  y+z\right)  .
\]


We have $A=\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  $, so that $A$ is the exponential of a derivation (since
$\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}$ is a derivation).
Thus, $A$ is a $K$-algebra homomorphism (since there is a known fact that the
exponential of a derivation is a $K$-algebra homomorphism). Combined with the
fact that $B$ is a $K$-algebra homomorphism (in fact, $B$ is an evaluation
homomorphism), this yields that both $A$ and $B$ are $K$-algebra homomorphisms.

Now, let $k$ be a positive integer. We will prove that $Az_{k}=Bz_{k}$.

We have
\[
\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
z_{k}=\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}z_{k}%
=y_{k}\underbrace{\dfrac{\partial}{\partial z_{k}}z_{k}}_{=1}+\sum
\limits_{\substack{s>0;\\s\neq k}}y_{s}\underbrace{\dfrac{\partial}{\partial
z_{s}}z_{k}}_{\substack{=0\\\text{(since }s\neq k\text{)}}}=y_{k}%
+\underbrace{\sum\limits_{\substack{s>0;\\s\neq k}}y_{s}0}_{=0}=y_{k},
\]
so that%
\[
\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
^{2}z_{k}=\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  \underbrace{\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
z_{s}}\right)  z_{k}}_{=y_{k}}=\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial
}{\partial z_{s}}\right)  y_{k}=\sum\limits_{s>0}y_{s}\underbrace{\dfrac
{\partial}{\partial z_{s}}y_{k}}_{=0}=0.
\]
As a consequence,
\begin{equation}
\text{every integer }i\geq2\text{ satisfies }\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}z_{k}=0.
\label{pf.hirota.newton.1}%
\end{equation}
Now, since $A=\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
z_{s}}\right)  =\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}\left(
\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}$, we have%
\begin{align*}
Az_{k}  &  =\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}\left(  \sum
\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}z_{k}\\
&  =\underbrace{\dfrac{1}{0!}}_{=1}\underbrace{\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{0}}_{=\operatorname*{id}}%
z_{k}+\underbrace{\dfrac{1}{1!}}_{=1}\underbrace{\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{1}}_{=\sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}}z_{k}+\sum\limits_{i\geq2}\dfrac{1}%
{i!}\underbrace{\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  ^{i}z_{k}}_{\substack{=0\\\text{(by (\ref{pf.hirota.newton.1}))}}}\\
&  =\underbrace{\operatorname*{id}z_{k}}_{=z_{k}}+\underbrace{\left(
\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  z_{k}}_{=y_{k}%
}+\underbrace{\sum\limits_{i\geq2}\dfrac{1}{i!}0}_{=0}=z_{k}+y_{k}=y_{k}%
+z_{k}.
\end{align*}
Compared to
\begin{align*}
Bz_{k}  &  =z_{k}\left(  y+z\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }B\right) \\
&  =y_{k}+z_{k},
\end{align*}
this yields $Az_{k}=Bz_{k}$.

Now, forget that we fixed $k$. We thus have shown that $Az_{k}=Bz_{k}$ for
every positive integer $k$. In other words, the maps $A$ and $B$ coincide on
the set $\left\{  z_{1},z_{2},z_{3},...\right\}  $. Since the set $\left\{
z_{1},z_{2},z_{3},...\right\}  $ generates $K\left[  z_{1},z_{2}%
,z_{3},...\right]  $ as a $K$-algebra, this yields that the maps $A$ and $B$
coincide on a generating set of the $K$-algebra $K\left[  z_{1},z_{2}%
,z_{3},...\right]  $. Since $A$ and $B$ are $K$-algebra homomorphisms, this
yields that $A=B$ (because if two $K$-algebra homomorphisms coincide on a
$K$-algebra generating set of their domain, then they must be equal). Hence,
every $P\in K\left[  z_{1},z_{2},z_{3},...\right]  $ satisfies%
\[
\underbrace{\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  }_{=A=B}\underbrace{P\left(  z\right)  }_{=P}=BP=P\left(
y+z\right)
\]
(by the definition of $B$). This proves Lemma \ref{lem.hirota.newton}.

\subsubsection{\label{subsubsect.GLinfty}$\operatorname*{GL}\left(
\infty\right)  $ and $\operatorname*{M}\left(  \infty\right)  $}

We now introduce the groups $\operatorname*{GL}\left(  \infty\right)  $ and
$\operatorname*{M}\left(  \infty\right)  $ and their actions on $\wedge
^{\dfrac{\infty}{2},m}V$. On the one hand, this will prepare us to the second
proof of Theorem \ref{thm.schur}; on the other hand, these group actions are
of autonomous interest, and we will meet them again in Subsubsection
\ref{subsubsect.infgrass}.

\begin{definition}
\label{def.Minf}We let $\operatorname*{M}\left(  \infty\right)  $ denote the
set $\operatorname*{id}+\mathfrak{gl}_{\infty}$. In other words, we let
$\operatorname*{M}\left(  \infty\right)  $ denote the set of all infinite
matrices (infinite in both directions) which are equal to the infinite
identity matrix $\operatorname*{id}$ in all but finitely many entries.
\end{definition}

Clearly, $\operatorname*{M}\left(  \infty\right)  \subseteq\overline
{\mathfrak{a}_{\infty}}$ as sets. We notice that:

\begin{proposition}
\label{prop.Minf.monoid}\textbf{(a)} For every $A\in\operatorname*{M}\left(
\infty\right)  $ and $B\in\operatorname*{M}\left(  \infty\right)  $, the
matrix $AB$ is well-defined and lies in $\operatorname*{M}\left(
\infty\right)  $.

\textbf{(b)} We have $\operatorname*{id}\in\operatorname*{M}\left(
\infty\right)  $ (where $\operatorname*{id}$ denotes the infinite identity matrix).

\textbf{(c)} The set $\operatorname*{M}\left(  \infty\right)  $ becomes a
monoid under multiplication of matrices.

\textbf{(d)} If a matrix $A\in\operatorname*{M}\left(  \infty\right)  $ is
invertible, then its inverse also lies in $\operatorname*{M}\left(
\infty\right)  $.

\textbf{(e)} Denote by $\operatorname*{GL}\left(  \infty\right)  $ the subset
$\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ A\text{ is
invertible}\right\}  $ of $\operatorname*{M}\left(  \infty\right)  $. Then,
$\operatorname*{GL}\left(  \infty\right)  $ becomes a group under
multiplication of matrices.
\end{proposition}

\begin{remark}
\label{rmk.GLinf}In Proposition \ref{prop.Minf.monoid}, a matrix
$A\in\operatorname*{M}\left(  \infty\right)  $ is said to be
\textit{invertible} if there exists an infinite matrix $B$ (with rows and
columns indexed by integers) satisfying $AB=BA=\operatorname*{id}$. The matrix
$B$ is then called the \textit{inverse} of $A$. Note that we don't a-priori
require that $B$ lie in $\operatorname*{M}\left(  \infty\right)  $, or any
other "finiteness conditions" for $B$; Proposition \ref{prop.Minf.monoid}
\textbf{(d)} shows that these conditions are automatically satisfied.
\end{remark}

\begin{definition}
\label{def.GLinf}Let $\operatorname*{GL}\left(  \infty\right)  $ denote the
group $\operatorname*{GL}\left(  \infty\right)  $ defined in Proposition
\ref{prop.Minf.monoid} \textbf{(e)}.
\end{definition}

\textit{Proof of Proposition \ref{prop.Minf.monoid}.} \textbf{(a)} Let
$A\in\operatorname*{M}\left(  \infty\right)  $ and $B\in\operatorname*{M}%
\left(  \infty\right)  $. Since $A\in\operatorname*{M}\left(  \infty\right)
=\operatorname*{id}+\mathfrak{gl}_{\infty}$, there exists an $a\in
\mathfrak{gl}_{\infty}$ such that $A=\operatorname*{id}+a$. Consider this $a$.

Since $B\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists a $b\in\mathfrak{gl}_{\infty}$ such
that $B=\operatorname*{id}+b$. Consider this $b$.

Since $A=\operatorname*{id}+a$ and $B=\operatorname*{id}+b$, we have
$AB=\left(  \operatorname*{id}+a\right)  \left(  \operatorname*{id}+b\right)
=\operatorname*{id}+a+b+ab$, which is clearly well-defined (because
$a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ lead to $ab$
being well-defined) and lies in $\operatorname*{M}\left(  \infty\right)  $
(since $\underbrace{a}_{\in\mathfrak{gl}_{\infty}}+\underbrace{b}%
_{\in\mathfrak{gl}_{\infty}}+\underbrace{ab}_{\substack{\in\mathfrak{gl}%
_{\infty}\\\text{(since }a\in\mathfrak{gl}_{\infty}\text{ and }b\in
\mathfrak{gl}_{\infty}\text{)}}}\in\mathfrak{gl}_{\infty}+\mathfrak{gl}%
_{\infty}+\mathfrak{gl}_{\infty}\subseteq\mathfrak{gl}_{\infty}$ and thus
$\operatorname*{id}+a+b+ab\in\operatorname*{id}+\mathfrak{gl}_{\infty
}=\operatorname*{M}\left(  \infty\right)  $). This proves Proposition
\ref{prop.Minf.monoid} \textbf{(a)}.

\textbf{(b)} Trivial.

\textbf{(c)} Follows from \textbf{(a)} and \textbf{(b)}.

\textbf{(d)} Let $A\in\operatorname*{M}\left(  \infty\right)  $ be invertible.

Since $A\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists an $a\in\mathfrak{gl}_{\infty}$ such
that $A=\operatorname*{id}+a$. Consider this $a$.

Since $A$ is invertible, there exists an infinite matrix $B$ (with rows and
columns indexed by integers) satisfying $AB=BA=\operatorname*{id}$ (according
to how we defined "invertible" in Remark \ref{rmk.GLinf}). Consider this $B$.
This $B$ is the inverse of $A$. Let $b=B-\operatorname*{id}$. Then,
$B=\operatorname*{id}+b$. Since $A=\operatorname*{id}+a$ and
$B=\operatorname*{id}+b$, we have $AB=\left(  \operatorname*{id}+a\right)
\left(  \operatorname*{id}+b\right)  =\operatorname*{id}+a+b+ab$, which is
clearly well-defined (because $a\in\mathfrak{gl}_{\infty}$ leads to $ab$ being
well-defined). Since $\operatorname*{id}=AB=\operatorname*{id}+ab+a+b$, we
have $0=ab+a+b$.

Let us introduce two notations that we will use during this proof:

\begin{itemize}
\item For any infinite matrix $M$ and any pair $\left(  i,j\right)  $ of
integers, let us denote by $M_{i,j}$ the $\left(  i,j\right)  $-th entry of
the matrix $M$. (In particular, for any pair $\left(  i,j\right)  $ of
integers, we denote by $a_{i,j}$ the $\left(  i,j\right)  $-th entry of the
matrix $a$ (not of the matrix $A$ !), and we denote by $b_{i,j}$ the $\left(
i,j\right)  $-th entry of the matrix $b$ (not of the matrix $B$ !).)

\item For any assertion $\mathcal{A}$, let $\left[  \mathcal{A}\right]  $
denote the integer $\left\{
\begin{array}
[c]{l}%
1,\text{ if }\mathcal{A}\text{ is true;}\\
0,\text{ if }\mathcal{A}\text{ is wrong}%
\end{array}
\right.  $.
\end{itemize}

Since $a\in\mathfrak{gl}_{\infty}$, only finitely many entries of the matrix
$a$ are nonzero. In particular, this yields that only finitely many columns of
the matrix $a$ are nonzero. Hence, there exists a nonnegative integer $N$ such
that
\begin{equation}
\left(  \text{for every integer }j\text{ with }\left\vert j\right\vert
>N\text{, the }j\text{-th column of }a\text{ is zero}\right)  .
\label{pf.Minf.monoid.1}%
\end{equation}
Consider this $N$. Clearly,%
\begin{equation}
\left(  \text{for every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that
}\left\vert j\right\vert >N\text{, we have }a_{i,j}=0\right)
\label{pf.Minf.monoid.1a}%
\end{equation}
(because for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert j\right\vert >N$, the $j$-th column of $a$ is zero (by
(\ref{pf.Minf.monoid.1})), so that every entry on the $j$-th column of $a$ is
zero, so that $a_{i,j}$ is zero (because the element $a_{i,j}$ is the $\left(
i,j\right)  $-th entry of $a$, hence an entry on the $j$-th column of $a$)).

Recall that only finitely many entries of the matrix $a$ are nonzero. In
particular, this yields that only finitely many rows of the matrix $a$ are
nonzero. Hence, there exists a nonnegative integer $M$ such that
\begin{equation}
\left(  \text{for every integer }i\text{ with }\left\vert i\right\vert
>M\text{, the }i\text{-th row of }a\text{ is zero}\right)  .
\label{pf.Minf.monoid.2}%
\end{equation}
Consider this $M$. Clearly,%
\begin{equation}
\left(  \text{for every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that
}\left\vert i\right\vert >M\text{, we have }a_{i,j}=0\right)
\label{pf.Minf.monoid.2a}%
\end{equation}
(because for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert i\right\vert >M$, the $i$-th row of $a$ is zero (by
(\ref{pf.Minf.monoid.2})), so that every entry on the $i$-th row of $a$ is
zero, so that $a_{i,j}$ is zero (because the element $a_{i,j}$ is the $\left(
i,j\right)  $-th entry of $a$, hence an entry on the $i$-th row of $a$)).

Let $P=\max\left\{  M,N\right\}  $. Clearly, $P\geq M$ and $P\geq N$. It is
now easy to see that
\begin{equation}
\text{any }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfies }%
a_{i,j}=\left[  \left\vert i\right\vert \leq P\right]  \cdot a_{i,j}.
\label{pf.Minf.monoid.a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.Minf.monoid.a}):} Let $\left(  i,j\right)
\in\mathbb{Z}^{2}$. Then, we must be in one of the following three cases:
\par
\textit{Case 1:} We don't have $\left\vert i\right\vert \leq P$.
\par
\textit{Case 2:} We have $\left\vert i\right\vert \leq P$.
\par
Let us consider Case 1 first. In this case, we don't have $\left\vert
i\right\vert \leq P$. Thus, $\left[  \left\vert i\right\vert \leq P\right]
=0$ and $\left\vert i\right\vert >P$. From $\left\vert i\right\vert >P\geq M$,
we conclude that $a_{i,j}=0$ (by (\ref{pf.Minf.monoid.2a})). Compared with
$\underbrace{\left[  \left\vert i\right\vert \leq P\right]  }_{=0}\cdot
a_{i,j}=0$, this yields $a_{i,j}=\left[  \left\vert i\right\vert \leq
P\right]  \cdot a_{i,j}$. Hence, (\ref{pf.Minf.monoid.a}) is proven in Case 1.
\par
Finally, let us consider Case 2. In this case, we have $\left\vert
i\right\vert \leq P$. Hence, $\left[  \left\vert i\right\vert \leq P\right]
=1$. Thus, $\underbrace{\left[  \left\vert i\right\vert \leq P\right]  }%
_{=1}\cdot a_{i,j}=a_{i,j}$. Hence, (\ref{pf.Minf.monoid.a}) is proven in Case
2.
\par
Altogether, we have thus proven (\ref{pf.Minf.monoid.a}) in each of the two
cases 1 and 2. Since these two cases cover all possibilities, this shows that
(\ref{pf.Minf.monoid.a}) always holds. Thus, (\ref{pf.Minf.monoid.a}) is
proven.} Similarly,%
\begin{equation}
\text{any }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfies }%
a_{i,j}=\left[  \left\vert j\right\vert \leq P\right]  \cdot a_{i,j}.
\label{pf.Minf.monoid.a'}%
\end{equation}


Let $b^{\prime}$ be the infinite matrix (with rows and columns indexed by
integers) defined by%
\begin{equation}
\left(  b_{i,j}^{\prime}=\left[  \left\vert i\right\vert \leq P\right]
\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{i,j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  i,j\right)  \in\mathbb{Z}%
^{2}\right)  . \label{pf.Minf.monoid.bprime}%
\end{equation}
It is clear that only finitely many entries of $b^{\prime}$ are
nonzero\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\mathbb{Z}^{2}$
such that $b_{i,j}^{\prime}\neq0$. Then, $\left\vert i\right\vert \leq P$
(because otherwise, we would have $\left[  \left\vert i\right\vert \leq
P\right]  =0$, so that $b_{i,j}^{\prime}=\underbrace{\left[  \left\vert
i\right\vert \leq P\right]  }_{=0}\cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}=0$, contradicting to $b_{i,j}^{\prime}\neq0$), so that
$i\in\left\{  -P,-P+1,...,P\right\}  $, and similarly $j\in\left\{
-P,-P+1,...,P\right\}  $. Hence, $\left(  i,j\right)  \in\left\{
-P,-P+1,...,P\right\}  ^{2}$ (since $i\in\left\{  -P,-P+1,...,P\right\}  $ and
$j\in\left\{  -P,-P+1,...,P\right\}  $).
\par
Now forget that we fixed $\left(  i,j\right)  $. We thus have showed that
every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $b_{i,j}^{\prime}%
\neq0$ satisfies $\left(  i,j\right)  \in\left\{  -P,-P+1,...,P\right\}  ^{2}%
$. Since there are only finitely many $\left(  i,j\right)  \in\left\{
-P,-P+1,...,P\right\}  ^{2}$, this yields that there are only finitely many
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $b_{i,j}^{\prime}\neq0$. In
other words, there are only finitely many $\left(  i,j\right)  \in
\mathbb{Z}^{2}$ such that the $\left(  i,j\right)  $-th entry of $b^{\prime}$
is nonzero. In other words, only finitely many entries of $b^{\prime}$ are
nonzero, qed.}. In other words, $b^{\prime}\in\mathfrak{gl}_{\infty}$, so that
$\operatorname*{id}+b^{\prime}\in\operatorname*{id}+\mathfrak{gl}_{\infty
}=\operatorname*{M}\left(  \infty\right)  $.

We will now prove that $A\left(  \operatorname*{id}+b^{\prime}\right)
=\operatorname*{id}$.

For every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, we have%
\begin{align*}
&  \left(  ab^{\prime}+a+b^{\prime}\right)  _{i,j}\\
&  =\underbrace{\left(  ab^{\prime}\right)  _{i,j}}_{\substack{=\sum
\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}^{\prime}\\\text{(by the definition of
the}\\\text{product of two matrices)}}}+a_{i,j}+\underbrace{b_{i,j}^{\prime}%
}_{\substack{=\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\\text{(by
(\ref{pf.Minf.monoid.bprime}))}}}\\
&  =\sum\limits_{k\in\mathbb{Z}}a_{i,k}\underbrace{b_{k,j}^{\prime}%
}_{\substack{=\left[  \left\vert k\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{k,j}\\\text{(by
(\ref{pf.Minf.monoid.bprime}), applied to}\\k\text{ instead of }i\text{)}%
}}+a_{i,j}+\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}\left[  \left\vert
k\right\vert \leq P\right]  }_{\substack{=\left[  \left\vert k\right\vert \leq
P\right]  \cdot a_{i,k}=a_{i,k}\\\text{(since (\ref{pf.Minf.monoid.a'})
(applied to}\\k\text{ instead of }j\text{) yields }a_{i,k}=\left[  \left\vert
k\right\vert \leq P\right]  \cdot a_{i,k}\text{)}}}\cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot b_{k,j}+\underbrace{a_{i,j}}%
_{\substack{=\left[  \left\vert i\right\vert \leq P\right]  \cdot
a_{i,j}\\\text{(by (\ref{pf.Minf.monoid.a}))}}}+\left[  \left\vert
i\right\vert \leq P\right]  \cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}}_{\substack{=\left[
\left\vert i\right\vert \leq P\right]  \cdot a_{i,k}\\\text{(by
(\ref{pf.Minf.monoid.a}), applied to}\\k\text{ instead of }j\text{)}}%
}\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{k,j}+\left[
\left\vert i\right\vert \leq P\right]  \cdot\underbrace{a_{i,j}}%
_{\substack{=\left[  \left\vert j\right\vert \leq P\right]  \cdot
a_{i,j}\\\text{(by (\ref{pf.Minf.monoid.a'}))}}}+\left[  \left\vert
i\right\vert \leq P\right]  \cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\left[  \left\vert i\right\vert \leq P\right]
\cdot a_{i,k}\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot
b_{k,j}+\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot a_{i,j}+\left[  \left\vert i\right\vert \leq
P\right]  \cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\
&  =\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}+a_{i,j}+b_{i,j}\right)  =\left[  \left\vert i\right\vert \leq
P\right]  \cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot
\underbrace{\left(  \left(  ab\right)  _{i,j}+a_{i,j}+b_{i,j}\right)
}_{\substack{=\left(  ab+a+b\right)  _{i,j}=0\\\text{(since }ab+a+b=0\text{)}%
}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  ab\right)  _{i,j}=\sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}\text{ (by the definition of the product of two matrices),}\\
\text{so that }\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}=\left(  ab\right)
_{i,j}%
\end{array}
\right) \\
&  =0.
\end{align*}
Thus, $ab^{\prime}+a+b^{\prime}=0$. Since $A=\operatorname*{id}+a$, we have
$A\left(  \operatorname*{id}+b^{\prime}\right)  =\left(  \operatorname*{id}%
+a\right)  \left(  \operatorname*{id}+b^{\prime}\right)  =\operatorname*{id}%
+\underbrace{ab^{\prime}+a+b^{\prime}}_{=0}=\operatorname*{id}$.

We thus have shown that $A\left(  \operatorname*{id}+b^{\prime}\right)
=\operatorname*{id}$.

Now, it is easy to see that the products $B\left(  A\left(  \operatorname*{id}%
+b^{\prime}\right)  \right)  $ and $\left(  BA\right)  \left(
\operatorname*{id}+b^{\prime}\right)  $ are well-defined and satisfy
associativity, i. e., we have $B\left(  A\left(  \operatorname*{id}+b^{\prime
}\right)  \right)  =\left(  BA\right)  \left(  \operatorname*{id}+b^{\prime
}\right)  $. Now,%
\[
B=B\cdot\underbrace{\operatorname*{id}}_{=A\left(  \operatorname*{id}%
+b^{\prime}\right)  }=B\left(  A\left(  \operatorname*{id}+b^{\prime}\right)
\right)  =\underbrace{\left(  BA\right)  }_{=\operatorname*{id}}\left(
\operatorname*{id}+b^{\prime}\right)  =\operatorname*{id}+b^{\prime}%
\in\operatorname*{M}\left(  \infty\right)  .
\]
Since $B$ is the inverse of $A$, this yields that the inverse of $A$ lies in
$\operatorname*{M}\left(  \infty\right)  $. This proves Proposition
\ref{prop.Minf.monoid} \textbf{(d)}.

\textbf{(e)} Follows from \textbf{(c)} and \textbf{(d)}.

The proof of Proposition \ref{prop.Minf.monoid} is complete.

We now construct a group action of $\operatorname*{GL}\left(  \infty\right)  $
on $\mathcal{F}^{\left(  m\right)  }$ that is related to the Lie algebra
action $\rho$ of $\mathfrak{gl}_{\infty}$ on $\mathcal{F}^{\left(  m\right)
}$ in the same way as the action of a Lie group on a representation is usually
related to its "derivative" action of the corresponding Lie algebra:

\begin{definition}
\label{def.GLinf.act}Let $m\in\mathbb{Z}$. We define an action $\varrho
:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ of the monoid $\operatorname*{M}%
\left(  \infty\right)  $ on the vector space $\mathcal{F}^{\left(  m\right)
}=\wedge^{\dfrac{\infty}{2},m}V$ as follows: For every $A\in\operatorname*{M}%
\left(  \infty\right)  $ and every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we set%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge
Av_{i_{2}}\wedge....
\]
(This is then extended to the whole $\mathcal{F}^{\left(  m\right)  }$ by
linearity.) It is very easy to see that this is well-defined (because
$Av_{k}=v_{k}$ for all sufficiently small $k$) and indeed gives a monoid action.

The restriction $\varrho\mid_{\operatorname*{GL}\left(  \infty\right)
}:\operatorname*{GL}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}^{\left(  m\right)  }\right)  $ to $\operatorname*{GL}%
\left(  \infty\right)  $ is thus a group action of $\operatorname*{GL}\left(
\infty\right)  $ on $\mathcal{F}^{\left(  m\right)  }$.

Since we have defined an action of $\operatorname*{M}\left(  \infty\right)  $
on $\mathcal{F}^{\left(  m\right)  }$ for every $m\in\mathbb{Z}$, we thus
obtain an action of $\operatorname*{M}\left(  \infty\right)  $ on
$\mathcal{F}=\bigoplus\limits_{m\in\mathbb{Z}}\mathcal{F}^{\left(  m\right)
}$ (namely, the direct sum of the previous actions). This latter action will
also be denoted by $\varrho$.
\end{definition}

Note that the letter $\varrho$ is a capital rho, as opposed to $\rho$ which is
the lowercase rho.

To get an explicit formula for the action $\varrho$, we first need a definition:

\begin{definition}
Let $B$ be a matrix over $\mathbb{C}$ whose rows are indexed by nonnegative
integers, and whose columns also are indexed by nonnegative integers. Assume
that the matrix $B$ differs from the identity matrix in only finitely many entries.

\textbf{(a)} Then, we can write the matrix $B$ in the form $\left(
\begin{array}
[c]{cc}%
C & 0\\
0 & I_{\infty}%
\end{array}
\right)  $ for some $n\in\mathbb{N}$ and some $n\times n$-matrix $C$. Such a
matrix $C$ will be called a \textit{faithful block-diagonal truncation} of $B$.

\textbf{(b)} We define the \textit{determinant} $\det B$ of the matrix $B$ to
be $\det C$, where $C$ is a faithful block-diagonal truncation of $B$. This is
well-defined, because a faithful block-diagonal truncation of $B$ exists and
because the determinant $\det C$ does not depend on the choice of the faithful
block-diagonal truncation $C$. (The latter assertion follows from the fact
that $\det\left(
\begin{array}
[c]{cc}%
D & 0\\
0 & I_{k}%
\end{array}
\right)  =\det D$ for any $n\in\mathbb{N}$, any $k\in\mathbb{N}$ and any
$n\times n$-matrix $D$.)
\end{definition}

Now, it is easy to see:

\begin{remark}
\label{rmk.GLinf.det}Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an
$m$-degression. Let $A\in\operatorname*{M}\left(  \infty\right)  $. For any
$m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, let $A_{i_{0}%
,i_{1},i_{2},...}^{j_{0},j_{1},j_{2},...}$ denote the matrix which is obtained
from $A$ by removing all rows except for the $i_{0}$-th, the $i_{1}$-th, the
$i_{2}$-th, etc. ones and removing all columns except for the $j_{0}$-th, the
$j_{1}$-th, the $j_{2}$-th, etc. ones. Then,
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is an }m\text{-degression}}\det\left(
A_{i_{0},i_{1},i_{2},...}^{j_{0},j_{1},j_{2},...}\right)  v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge....
\]
Here, the determinant $\det\left(  A_{i_{0},i_{1},i_{2},...}^{j_{0}%
,j_{1},j_{2},...}\right)  $ makes sense because the matrix $A_{i_{0}%
,i_{1},i_{2},...}^{j_{0},j_{1},j_{2},...}$ differs from the identity matrix in
only finitely many entries.
\end{remark}

\textit{Proof of Remark \ref{rmk.GLinf.det}.} Let $C$ be a faithful
block-diagonal truncation of $A$.

[...]

\subsubsection{Semiinfinite vectors and an action of $\mathfrak{u}_{\infty}$}

The actions of $\mathfrak{gl}_{\infty}$, $\mathfrak{a}_{\infty}$,
$\operatorname*{M}\left(  \infty\right)  $ and $\operatorname*{GL}\left(
\infty\right)  $ on $\wedge^{\dfrac{\infty}{2},m}V$ have many good properties,
but for what we want to do with them, they are in some sense "too small" (even
$\mathfrak{a}_{\infty}$). Of course, we cannot let the space of \textbf{all}
infinite matrices act on $\wedge^{\dfrac{\infty}{2},m}V$ (this space is not
even a Lie algebra), but it turns out that if we restrict ourself to strictly
upper-triangular infinite matrices, then we can get away with this. First, let
us define a kind of completed version of $V$:

\begin{definition}
\label{def.uinf.Vhat}\textbf{(a)} A family $\left(  x_{i}\right)
_{i\in\mathbb{Z}}$ of elements of some additive group indexed by integers is
said to be \textit{semiinfinite} if every sufficiently high $i\in\mathbb{Z}$
satisfies $x_{i}=0$.

\textbf{(b)} Let $\widehat{V}$ be the vector subspace $\left\{  v\in
\mathbb{C}^{\mathbb{Z}}\text{\ }\mid\ v\text{ is semiinfinite}\right\}  $ of
$\mathbb{C}^{\mathbb{Z}}$. Let $\mathfrak{u}_{\infty}$ denote the Lie algebra
of all \textbf{strictly} upper-triangular infinite matrices (with rows and
columns indexed by integers). It is easy to see that the Lie algebra
$\mathfrak{u}_{\infty}$ acts on the vector space $\widehat{V}$ in the obvious
way: namely, for any $a\in\mathfrak{u}_{\infty}$ and $v\in\widehat{V}$, we let
$a\rightharpoonup v$ be the product of the matrix $a$ with the column vector
$v$. Here, every element $\left(  x_{i}\right)  _{i\in\mathbb{Z}}$ of
$\widehat{V}$ is identified with the column vector $\left(
\begin{array}
[c]{c}%
...\\
x_{-2}\\
x_{-1}\\
x_{0}\\
x_{1}\\
x_{2}\\
...
\end{array}
\right)  $.

The vector space $V$ defined in Definition \ref{def.glinf.V} clearly is a
subspace of $\widehat{V}$. Restricting the $\mathfrak{u}_{\infty}$-action on
$\widehat{V}$ to an $\left(  \mathfrak{u}_{\infty}\cap\mathfrak{gl}_{\infty
}\right)  $-action on $V$ yields the same $\left(  \mathfrak{u}_{\infty}%
\cap\mathfrak{gl}_{\infty}\right)  $-module as restricting the $\mathfrak{gl}%
_{\infty}$-action on $V$ to an $\left(  \mathfrak{u}_{\infty}\cap
\mathfrak{gl}_{\infty}\right)  $-action on $V$.
\end{definition}

We thus have obtained an $\mathfrak{u}_{\infty}$-module $\widehat{V}$, which
is a kind of completion of $V$. One could now hope that this allows us to
construct an $\mathfrak{u}_{\infty}$-module structure on some kind of
completion of $\wedge^{\dfrac{\infty}{2},m}V$. A quick observation shows that
this works better than one would expect, because we don't have to take any
completion of $\wedge^{\dfrac{\infty}{2},m}V$ (although we can if we want to).
We can make $\wedge^{\dfrac{\infty}{2},m}V$ itself an $\mathfrak{u}_{\infty}$-module:

\begin{definition}
\label{def.uinf.Vhatproj}Let $\ell\in\mathbb{Z}$. Let $\pi_{\ell}%
:\widehat{V}\rightarrow V$ be the linear map which sends every $\left(
x_{i}\right)  _{i\in\mathbb{Z}}\in\widehat{V}$ to $\left(  \left\{
\begin{array}
[c]{c}%
x_{i}\text{, if }i>\ell;\\
0\text{, if }i\leq\ell
\end{array}
\right.  \right)  _{i\in\mathbb{Z}}\in V$. (It is very easy to see that this
map $\widehat{V}$ is well-defined.)
\end{definition}

\begin{definition}
\label{def.uinf.Vhatwedge}Let $m\in\mathbb{Z}$. Let $a_{0},a_{1},a_{2},...$ be
vectors in $V$ which satisfy%
\[
a_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]
Define an element $a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ of $\wedge
^{\dfrac{\infty}{2},m}V$ as follows: Pick some $N\in\mathbb{N}$ such that
every $i\geq N$ satisfies $a_{i}=v_{m-i}$. (Such an $N$ exists, since we know
that $a_{i}=v_{m-i}$ for all sufficiently large $i$.) Then, we define
$a_{0}\wedge a_{1}\wedge a_{2}\wedge...$ to be the element
\[
\pi_{m-N}\left(  a_{0}\right)  \wedge\pi_{m-N}\left(  a_{1}\right)
\wedge...\wedge\pi_{m-N}\left(  a_{i-1}\right)  \wedge v_{m-i}\wedge
v_{m-i-1}\wedge v_{m-i-2}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V.
\]
This element does not depend on the choice of $N$ (according to Proposition
\ref{prop.uinf.Vhatwedge.welldef} \textbf{(a)} below). Hence, $a_{0}\wedge
a_{1}\wedge a_{2}\wedge...$ is well-defined.
\end{definition}

\begin{proposition}
\label{prop.uinf.Vhatwedge.welldef}Let $m\in\mathbb{Z}$. Let $a_{0}%
,a_{1},a_{2},...$ be vectors in $V$ which satisfy%
\[
a_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]


\textbf{(a)} If we pick some $N\in\mathbb{N}$ such that every $i\geq N$
satisfies $a_{i}=v_{m-i}$, then the element%
\[
\pi_{m-N}\left(  a_{0}\right)  \wedge\pi_{m-N}\left(  a_{1}\right)
\wedge...\wedge\pi_{m-N}\left(  a_{i-1}\right)  \wedge v_{m-i}\wedge
v_{m-i-1}\wedge v_{m-i-2}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V
\]
does not depend on the choice of $N$.

\textbf{(b)} [multilin]
\end{proposition}

\begin{definition}
Let $a\in\mathfrak{gl}_{\infty}$. Then,%
\[
a\rightharpoonup\left(  a_{0}\wedge a_{1}\wedge a_{2}\wedge...\right)
=\sum\limits_{k\geq0}a_{0}\wedge a_{1}\wedge...\wedge a_{k-1}\wedge\left(
a\rightharpoonup a_{k}\right)  \wedge a_{k+1}\wedge a_{k+2}\wedge....
\]

\end{definition}

Let $m\in\mathbb{Z}$. Define an action of the Lie algebra $\mathfrak{gl}%
_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ by the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ (and by linear extension).

[...]

\subsubsection{\label{subsubsect.schur2}Second proof of Theorem
\ref{thm.schur}}

[...] [The following argument is shaky and partly incomplete.]

Now we will sketch a second proof of Theorem \ref{thm.schur}, which will use
less algebraic combinatorics but instead will use the group action
$\varrho\mid_{\operatorname*{GL}\left(  \infty\right)  }:\operatorname*{GL}%
\left(  \infty\right)  \rightarrow\operatorname*{End}\left(  \mathcal{F}%
^{\left(  m\right)  }\right)  $ of Definition \ref{def.GLinf.act}.

\textit{Second proof of Theorem \ref{thm.schur}.} Let us denote by $1$ the
unity of $\mathcal{B}^{\left(  0\right)  }=\mathbb{C}\left[  x_{1},x_{2}%
,x_{3},...\right]  $.

From now on, we let $y$ denote another countable family of indeterminates
$\left(  y_{1},y_{2},y_{3},...\right)  $ (rather than a finite family like the
$\left(  y_{1},y_{2},...,y_{N}\right)  $ of Definition \ref{def.schur.y}).
Thus, whenever $P$ is a polynomial in countably many indeterminates, $P\left(
y\right)  $ will mean $P\left(  y_{1},y_{2},y_{3},...\right)  $.

Moreover, we let $x+y$ denote the family $\left(  x_{1}+y_{1},x_{2}%
+y_{2},x_{3}+y_{3},...\right)  $.

Fix a $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $; then,
$i_{0}>i_{1}>i_{2}>...$ and $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\mathcal{F}^{\left(  0\right)  }$. Denote $\sigma^{-1}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ by $P\left(
x\right)  $, where $x$ still denotes the whole collection of variables
$\left(  x_{1},x_{2},x_{3},...\right)  $. We need to show that $P\left(
x\right)  =S_{\lambda}\left(  x\right)  $.

In the following, we will work over the base ring $\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  $ instead of $\mathbb{C}$. This means that we
will consider the Heisenberg algebra $\mathcal{A}_{\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  }$ defined over $\mathbb{C}\left[  y_{1}%
,y_{2},y_{3},...\right]  $ instead of the Heisenberg algebra $\mathcal{A}$
defined over $\mathbb{C}$\ \ \ \ \footnote{Note that $\mathcal{A}%
_{\mathbb{C}\left[  y_{1},y_{2},y_{3},...\right]  }\cong\mathcal{A}%
\otimes_{\mathbb{C}}\mathbb{C}\left[  y_{1},y_{2},y_{3},...\right]  $, but we
will not use this.}. Similarly, we will consider the Lie algebra
$\mathfrak{gl}_{\infty\mathbb{C}\left[  y_{1},y_{2},y_{3},...\right]  }$
defined over $\mathbb{C}\left[  y_{1},y_{2},y_{3},...\right]  $ instead of the
Lie algebra $\mathfrak{gl}_{\infty}$ defined over $\mathbb{C}$, and likewise
for $V$ and $\mathcal{B}$ and all our other objects.

Now, let us be sloppy and denote the new objects $\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  $, $\mathcal{A}_{\mathbb{C}\left[  y_{1}%
,y_{2},y_{3},...\right]  }$, $\mathfrak{gl}_{\infty\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  }$, $...$ again by $\mathbb{C}$, $\mathcal{A}$,
$\mathfrak{gl}_{\infty}$, $...$, respectively. In other words, let us suppress
from our notation the fact that we are working over $\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  $ instead of working over $\mathbb{C}$.

Even if neither the infinite sum $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ nor
its exponential $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $
are well-defined in $U\left(  \mathcal{A}\right)  $, the term $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \left(  v\right)  $ has a
well-defined meaning for every $v\in\mathcal{B}^{\left(  0\right)  }$. This is
because every $v\in\mathcal{B}^{\left(  0\right)  }$ is a polynomial of finite
degree in finitely many of the variables $y_{i}$, and thus is annihilated by
all but finitely many monomials of the form $a_{1}^{n_{1}}a_{2}^{n_{2}}%
a_{3}^{n_{3}}...$.

Let $\left(  \cdot,\cdot\right)  $ be the form $\left(  \cdot,\cdot\right)
:\widetilde{F}\times\widetilde{F}\rightarrow\mathbb{C}$ defined in Proposition
\ref{prop.A.contravariantform}. Every polynomial $R\in\widetilde{F}$
satisfies:%
\begin{equation}
R\left(  1\right)  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  R\right)  \text{ with respect to the
basis}\\
\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  _{\left(
j_{0},j_{1},j_{2},...\right)  \text{ a }0\text{-degression}}\text{ of
}\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right)  . \label{pf.schur.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.1}).} Let $p_{0,\mathcal{B}}$ be the
canonical projection of the graded space $\mathcal{B}^{\left(  0\right)  }$
onto its $0$-th graded component $\mathcal{B}^{\left(  0\right)  }\left[
0\right]  =\mathbb{C}\cdot1$, and let $p_{0,\mathcal{F}}$ be the canonical
projection of the graded space $\mathcal{F}^{\left(  0\right)  }$ onto its
$0$-th graded component $\mathcal{F}^{\left(  0\right)  }\left[  0\right]
=\mathbb{C}\psi_{0}$. Since $\sigma_{0}:\mathcal{B}^{\left(  0\right)
}\rightarrow\mathcal{F}^{\left(  0\right)  }$ is a graded homomorphism,
$\sigma_{0}$ commutes with the projections on the $0$-th graded components; in
other words, $\sigma_{0}\circ p_{0,\mathcal{B}}=p_{0,\mathcal{F}}\circ
\sigma_{0}$. Now, we know that $p_{0,\mathcal{B}}\left(  R\right)  =R\left(
1\right)  \cdot1$ (since $\mathcal{B}=\widetilde{F}=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $), and thus $\left(  \sigma_{0}\circ
p_{0,\mathcal{B}}\right)  \left(  R\right)  =\sigma_{0}\left(
\underbrace{p_{0,\mathcal{B}}\left(  R\right)  }_{=R\left(  1\right)  \cdot
1}\right)  =\sigma_{0}\left(  R\left(  1\right)  \cdot1\right)  =R\left(
1\right)  \cdot\underbrace{\sigma_{0}\left(  1\right)  }_{=\psi_{0}}=R\left(
1\right)  \psi_{0}$.
\par
On the other hand, let $\kappa$ denote the $\left(  v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right)  $-coordinate of $\sigma\left(  R\right)  $ with
respect to the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}^{\left(  0\right)  }$. Then, the
projection of $\sigma\left(  R\right)  $ onto the $0$-th graded component
$\mathcal{F}^{\left(  0\right)  }\left[  0\right]  $ of $\mathcal{F}^{\left(
0\right)  }$ is $\kappa\cdot v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$
(because the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}^{\left(  0\right)  }$ is a graded
basis, and the $0$-th graded component $\mathcal{F}^{\left(  0\right)
}\left[  0\right]  $ of $\mathcal{F}^{\left(  0\right)  }$ is spanned by
$\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  $). In other words,
$p_{0,\mathcal{F}}\left(  \sigma\left(  R\right)  \right)  =\kappa
\cdot\underbrace{v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...}_{=\psi_{0}}%
=\kappa\psi_{0}$. Hence,%
\[
R\left(  1\right)  \psi_{0}=\underbrace{\left(  \sigma_{0}\circ
p_{0,\mathcal{B}}\right)  }_{=p_{0,\mathcal{F}}\circ\sigma_{0}}\left(
R\right)  =\left(  p_{0,\mathcal{F}}\circ\sigma_{0}\right)  \left(  R\right)
=p_{0,\mathcal{F}}\left(  \sigma\left(  R\right)  \right)  =\kappa\psi_{0}.
\]
Since $\psi_{0}\neq0$, this yields%
\[
R\left(  1\right)  =\kappa=\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  R\right)  \text{ with respect to the
basis}\\
\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  _{\left(
j_{0},j_{1},j_{2},...\right)  \text{ a }0\text{-degression}}\text{ of
}\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right)  .
\]
This proves (\ref{pf.schur.1}).}

We must show that $P\left(  x\right)  =S_{\lambda}\left(  x\right)  $. In
order to prove this, it suffices to show that
\[
\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(
x\right)  \right)  =\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  S_{\lambda}\left(  x\right)  \right)  .
\]
In fact, this will show that $\left(  1,a_{1}^{n_{1}}a_{2}^{n_{2}}...P\left(
x\right)  \right)  =\left(  1,a_{1}^{n_{1}}a_{2}^{n_{2}}...S_{\lambda}\left(
x\right)  \right)  $, which will rewrite as $\left(  a_{-1}^{n_{1}}%
a_{-2}^{n_{2}}...1,P\left(  x\right)  \right)  =\left(  a_{-1}^{n_{1}}%
a_{-2}^{n_{2}}...1,S_{\lambda}\left(  x\right)  \right)  $, and $\left(
a_{-1}^{n_{1}}a_{-2}^{n_{2}}...1\right)  _{\left(  n_{1},n_{2},...\right)  }$
is a basis and the form $\left(  \cdot,\cdot\right)  $ is nondegenerate (by
Proposition \ref{prop.A.contravariantform} \textbf{(a)}).

Let us now compute $\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  P\left(  x\right)  \right)  $. In fact, every polynomial $R$
satisfies%
\begin{align*}
&  \left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  R\left(
x\right)  \right) \\
&  =\left(  1,\exp\left(  y_{1}\dfrac{\partial}{\partial x_{1}}+y_{2}%
\dfrac{\partial}{\partial x_{2}}+y_{3}\dfrac{\partial}{\partial x_{3}%
}+...\right)  R\left(  x\right)  \right) \\
&  =\left(  1,R\left(  x+y\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\exp\left(  y_{1}\dfrac{\partial}{\partial x_{1}}+y_{2}%
\dfrac{\partial}{\partial x_{2}}+y_{3}\dfrac{\partial}{\partial x_{3}%
}+...\right)  R\left(  x\right)  =\exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial x_{s}}\right)  R\left(  x\right)  =R\left(
x+y\right) \\
\text{by Lemma \ref{lem.hirota.newton} (applied to }\left(  x_{1},x_{2}%
,x_{3},...\right)  \text{ and}\\
\mathbb{C}\left[  y_{1},y_{2},y_{3},...\right]  \text{ instead of }\left(
z_{1},z_{2},z_{3},...\right)  \text{ and }K\text{)}%
\end{array}
\right) \\
&  =R\left(  x+y\right)  \mid_{x=0}=R\left(  y\right)  .
\end{align*}
Thus, $\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
S_{\lambda}\left(  x\right)  \right)  =S_{\lambda}\left(  y\right)  $.

So our job is to show that $\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  P\left(  x\right)  \right)  =S_{\lambda}\left(
y\right)  $.

Now consider the matrix $T$ which has $1$'s on the diagonal right above the
main one, and $0$'s everywhere else. Then, $a_{i}=T^{i}$ (this is how we
embedded the Heisenberg algebra in $\mathfrak{a}_{\infty}$).

Thus,%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right) \\
&  =\exp\left(  y_{1}T^{1}+y_{2}T^{2}+y_{3}T^{3}+...\right)  =\sum
\limits_{k\in\mathbb{N}}S_{k}\left(  y\right)  T^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\exp\left(  y_{1}w^{1}+y_{2}w^{2}+y_{3}w^{3}+...\right)
=\sum\limits_{k\in\mathbb{N}}S_{k}\left(  y\right)  w^{k}\\
\text{as a generating function identity in }\mathbb{C}\left[  y_{1}%
,y_{2},y_{3},...\right]  \left[  \left[  w\right]  \right]
\end{array}
\right) \\
&  =\left(  S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}=:A.
\end{align*}
Modulo some cheating, we can view $A$ as an element of $\operatorname*{GL}%
\left(  \infty\right)  $ (actually, not really, but of some completion of
$\operatorname*{GL}\left(  \infty\right)  $ over the ring $\mathbb{C}\left[
y_{1},y_{2},y_{3},...\right]  $).

Now we go to $\mathcal{F}$ (by means of $\sigma$), and find%
\begin{align*}
&  \left(  1,\underbrace{\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  }_{=A}P\left(  x\right)  \right) \\
&  =\left(  1,AP\left(  x\right)  \right)  =\left(  AP\left(  x\right)
\right)  \left(  1\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  AP\left(  x\right)  \right)  \text{ with
respect to the basis}\\
\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  _{\left(
j_{0},j_{1},j_{2},...\right)  \text{ a }0\text{-degression}}\text{ of
}\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.1})}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }A\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \text{ with respect to the basis}\\
\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  _{\left(
j_{0},j_{1},j_{2},...\right)  \text{ a }0\text{-degression}}\text{ of
}\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\sigma\left(  AP\left(  x\right)
\right)  =A\sigma\left(  P\left(  x\right)  \right)  =A\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the minor of }A\text{ which is exactly the same determinant}\\
\text{that was used to define }S_{\lambda}\left(  y\right)
\end{array}
\right)  =S_{\lambda}\left(  y\right)  .
\end{align*}
Theorem \ref{thm.schur} is proven.

\subsection{Applications to integrable systems}

Let us show how these things can be applied to partial differential equations.

\begin{Convention}
If $v$ is a function in several variables $x_{1}$, $x_{2}$, $...$, $x_{k}$,
then, for every $i\in\left\{  1,2,...,k\right\}  $, the derivative of $v$ by
the variable $x_{i}$ will be denoted by $\partial_{x_{i}}v$ and by $v_{x_{i}}%
$. In other words, $\partial_{x_{i}}v=v_{x_{i}}=\dfrac{\partial}{\partial
x_{i}}v$. (For example, if $v$ is a function in two variables $x$ and $t$,
then $v_{t}$ will mean the derivative of $v$ by $t$.)
\end{Convention}

The PDE (partial differential equation) we will be concerned with is the
\textbf{Korteweg-de Vries equation} (abbreviated as \textbf{KdV equation}%
)\textbf{:} This is the equation $u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}%
u_{xxx}$ for a function $u\left(  t,x\right)  $.\ \ \ \ \footnote{There seems
to be no consistent definition of the KdV equation across literature. We
defined the KdV equation as $u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}$
because this is the form most suited to our approach. Some other authors,
instead, define the KdV equation as $v_{t}=v_{xxx}+6vv_{x}$ for a function
$v\left(  t,x\right)  $. Others define it as $w_{t}+ww_{x}+w_{xxx}=0$ for a
function $w\left(  t,x\right)  $. Yet others define it as $q_{t}%
+q_{xxx}+6qq_{x}=0$ for a function $q\left(  t,x\right)  $. These equations
are not literally equivalent, but can be transformed into each other by very
simple substitutions. In fact, for a function $u\left(  t,x\right)  $, we have
the following equivalence of assertions:%
\begin{align*}
&  \ \left(  \text{the function }u\left(  t,x\right)  \text{ satisfies the
equation }u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }v\left(  t,x\right)
:=u\left(  4t,x\right)  \text{ satisfies the equation }v_{t}=v_{xxx}%
+6vv_{x}\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }w\left(  t,x\right)
:=6u\left(  -4t,x\right)  \text{ satisfies the equation }w_{t}+ww_{x}%
+w_{xxx}=0\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }q\left(  t,x\right)
:=u\left(  -4t,x\right)  \text{ satisfies the equation }q_{t}+q_{xxx}%
+6qq_{x}=0\right)  .
\end{align*}
}

We will discuss several interesting solutions of this equation. Here is the
most basic family of solutions:%
\[
u\left(  t\right)  =\dfrac{2a^{2}}{\cosh^{2}\left(  a\left(  x+a^{2}t\right)
\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{for }a\text{ being arbitrary but
fixed}\right)  .
\]
These are so-called "traveling wave solutions". It is a peculiar kind of wave:
it has only one bump; it is therefore called a \textit{soliton} (or
\textit{solitary wave}). Such waves never occur in linear systems. Note that
when we speak of "wave", we are imagining a time-dependent 2-dimensional graph
with the x-axis showing $t$, the y-axis showing $u\left(  t\right)  $, and the
time parameter being $x$. So when we speak of "traveling wave", we mean that
it is a wave for any fixed time $x$ and "travels" when $x$ moves.

The first to study this kind of waves was J. S. Russell in 1834, describing
the motion of water in a shallow canal (tsunami waves are similar). The first
models for these waves were found by Korteweg-de Vries in 1895.

The term $\dfrac{1}{4}u_{xxx}$ in the Korteweg-de Vries equation $u_{t}%
=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}$ is called the \textit{dispersion
term}.

\textbf{Exercise:} Solve the equation $u_{t}=\dfrac{3}{2}uu_{x}$. (Note that
these waves develop shocks, in contrast to Korteweg-de Vries equation.)

The Korteweg-de Vries equation is famous for having lots of explicit solutions
(unexpectedly for a nonlinear partial differential equation). We will
construct some of them using infinite-dimensional Lie algebras. (There are
many other ways to construct solutions. In some sense, all of mathematics is
related to its solutions.)

\textbf{Generalization:} The \textbf{Kadomtsev-Petviashvili equation}
(abbreviated as \textbf{KP equation}) $u_{yy}=\left(  u_{t}-\dfrac{3}{2}%
uu_{x}-\dfrac{1}{4}u_{xxx}\right)  _{x}$ (or, after some rescaling, $\dfrac
{3}{4}\partial_{y}^{2}u=\partial_{x}\left(  \partial_{t}u-\dfrac{3}%
{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)  $) on a function
$u\left(  t,x,y\right)  $.

We will also find some solutions to this equation.

We are going to use the \textit{infinite Grassmannian} for this. First, recall
what the \textit{finite Grassmannian} is:

\subsubsection{The finite Grassmannian}

\begin{definition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. Let $V$ be the
$\mathbb{C}$-vector space $\mathbb{C}^{n}$. Let $\left(  v_{1},v_{2}%
,...,v_{n}\right)  $ be the standard basis of $\mathbb{C}^{n}$. Recall that
$\wedge^{k}V$ is a representation of $\operatorname*{GL}\left(  V\right)  $
with a highest-weight vector $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Denote
by $\Omega$ the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $.
\end{definition}

\begin{proposition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. We have $\Omega
=\left\{  x\in\wedge^{k}V\text{ nonzero}\ \mid\ x=x_{1}\wedge x_{2}%
\wedge...\wedge x_{k}\text{ for some }x_{i}\in V\right\}  $. Also,
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\neq0$ if and only if $x_{1}$, $x_{2}$,
$...$, $x_{k}$ are linearly independent.
\end{proposition}

\textit{Proof.} Very easy.

\begin{definition}
The \textit{Pl\"{u}cker embedding} is defined as the map%
\begin{align*}
\operatorname*{Pl}:\operatorname*{Gr}\left(  k,V\right)   &  \rightarrow
\mathbb{P}\left(  \wedge^{k}V\right)  ,\\
\left(
\begin{array}
[c]{c}%
k\text{-dimensional subspace of }V\\
\text{with basis }x_{1},x_{2},...,x_{k}%
\end{array}
\right)   &  \mapsto\left(
\begin{array}
[c]{c}%
\text{projection of}\\
x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge^{k}V\diagdown\left\{
0\right\} \\
\text{on }\mathbb{P}\left(  \wedge^{k}V\right)
\end{array}
\right)  .
\end{align*}
It is easy to see that this is well-defined (i. e., that the projection of
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge^{k}V\diagdown\left\{
0\right\}  $ on $\mathbb{P}\left(  \wedge^{k}V\right)  $ does not depend on
the choice of basis $x_{1},x_{2},...,x_{k}$). The image of this map is
$\operatorname*{Im}\operatorname*{Pl}=\Omega\diagup\left(  \text{scalars}%
\right)  $.
\end{definition}

\begin{proposition}
\label{prop.plu.injective}This map $\operatorname*{Pl}$ is injective.
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.injective}.} Proving Proposition
\ref{prop.plu.injective} boils down to showing that if $\lambda$ is a complex
number and $v_{1}$, $v_{2}$, $...$, $v_{k}$, $w_{1}$, $w_{2}$, $...$, $w_{k}$
are any vectors in a vector space $U$ satisfying $v_{1}\wedge v_{2}%
\wedge...\wedge v_{k}=\lambda\cdot w_{1}\wedge w_{2}\wedge...\wedge w_{k}%
\neq0$, then the vector subspace of $U$ spanned by the vectors $v_{1}$,
$v_{2}$, $...$, $v_{k}$ is identical with the vector subspace of $U$ spanned
by the vectors $w_{1}$, $w_{2}$, $...$, $w_{k}$. This is a well-known fact.
The details are left to the reader.

Thus, $\operatorname*{Gr}\left(  k,V\right)  \cong\Omega\diagup\left(
\text{scalars}\right)  $. (For algebraic geometers: $\Omega$ is the total
space of the determinant bundle on $\operatorname*{Gr}\left(  k,V\right)  $
(but only the nonzero elements).)

We are now going to describe the image $\operatorname*{Im}\operatorname*{Pl}$
by algebraic equations. These equations go under the name \textit{Pl\"{u}cker
relations}.

First, we define (in analogy to Definition \ref{def.createdestroy}) "wedging"
and "contraction" operators on the exterior algebra of $V$:

\begin{definition}
\label{def.createdestroy.fin}Let $n\in\mathbb{N}$. Let $k\in\mathbb{Z}$. Let
$V$ be the vector space $\mathbb{C}^{n}$. Let $\left(  v_{1},v_{2}%
,...,v_{n}\right)  $ be the standard basis of $V$. Let $i\in\left\{
1,2,...,n\right\}  $.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ by%
\[
\widehat{v_{i}}\cdot\psi=v_{i}\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all
}\psi\in\wedge^{k}V.
\]


\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k-1}V$ as follows:

For every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$, we let $\overset{\vee}{v_{i}%
}\left(  v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{k}}\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}
;\\
\left(  -1\right)  ^{j-1}v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{j-1}%
}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{1},i_{2},...,i_{k}\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell}=i$. Thus, the map
$\overset{\vee}{v_{i}}$ is defined on a basis of the vector space $\wedge
^{k}V$; we extend this to a map $\wedge^{k}V\rightarrow\wedge^{k-1}V$ by linearity.

Note that, for every negative $\ell\in\mathbb{Z}$, we understand $\wedge
^{\ell}V$ to mean the zero space.
\end{definition}

Now we can formulate the Pl\"{u}cker relations as follows:

\begin{theorem}
\label{thm.plu}Let $n\in\mathbb{N}$. Let $k\in\mathbb{Z}$. We consider the
vector space $V=\mathbb{C}^{n}$ with its standard basis $\left(  v_{1}%
,v_{2},...,v_{n}\right)  $. Let $S=\sum\limits_{i=1}^{n}\widehat{v_{i}}%
\otimes\overset{\vee}{v_{i}}:\wedge^{k}V\otimes\wedge^{k}V\rightarrow
\wedge^{k+1}V\otimes\wedge^{k-1}V$.

\textbf{(a)} This map $S$ does not depend on the choice of the basis and is
$\operatorname*{GL}\left(  V\right)  $-invariant. In other words, for
\textbf{any} basis $\left(  w_{1},w_{2},...,w_{n}\right)  $ of $V$, we have
$S=\sum\limits_{i=1}^{n}\widehat{w_{i}}\otimes\overset{\vee}{w_{i}}$ (where
the maps $\widehat{w_{i}}$ and $\overset{\vee}{w_{i}}$ are defined just as
$\widehat{v_{i}}$ and $\overset{\vee}{v_{i}}$, but with respect to the basis
$\left(  w_{1},w_{2},...,w_{n}\right)  $).

\textbf{(b)} Let $k\in\left\{  1,2,...,n\right\}  $. A nonzero element
$\tau\in\wedge^{k}V$ belongs to $\Omega$ if and only if $S\left(  \tau
\otimes\tau\right)  =0$.

\textbf{(c)} The map $S$ is $\operatorname*{M}\left(  V\right)  $-invariant.
(Here, $\operatorname*{M}\left(  V\right)  $ denotes the multiplicative monoid
of all endomorphisms of $V$.)
\end{theorem}

Part \textbf{(b)} of this theorem is what is actually called the Pl\"{u}cker
relations, although it is not how these relations are usually formulated in
literature. For a more classical formulation, see Theorem \ref{thm.plu.coo}.
Of course, Theorem \ref{thm.plu} \textbf{(b)} not only shows when an element
of $\wedge^{k}V$ belongs to $\Omega$, but also shows when an element of
$\mathbb{P}\left(  \wedge^{k}V\right)  $ lies in $\operatorname*{Im}%
\operatorname*{Pl}$ (because an element of $\mathbb{P}\left(  \wedge
^{k}V\right)  $ is an equivalence class of elements of $\wedge^{k}%
V\diagdown\left\{  0\right\}  $, and lies in $\operatorname*{Im}%
\operatorname*{Pl}$ if and only if its representatives lie in $\Omega$).

\textit{Proof of Theorem \ref{thm.plu}.} Before we start proving the theorem,
let us introduce some notations.

First of all, for every basis $\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V$,
let $\left(  e_{1}^{\ast},e_{2}^{\ast},...,e_{n}^{\ast}\right)  $ denote its
dual basis (this is a basis of $V^{\ast}$).

Next, for any element $v\in V$ we define the so called $v$\textit{-wedging
operator} $\widehat{v}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ by%
\[
\widehat{v}\cdot\psi=v\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all }\psi
\in\wedge^{k}V.
\]
Of course, this definition does not conflict with Definition
\ref{def.createdestroy.fin} \textbf{(a)}. (In fact, for every $i\in\left\{
1,2,...,n\right\}  $, the $v_{i}$-wedging operator that we just defined is
exactly identical with the $i$-th wedging operator defined in Definition
\ref{def.createdestroy.fin} \textbf{(a)}, and hence there is no harm from
denoting both of them by $\widehat{v_{i}}$.)

Further, for any $f\in V^{\ast}$, we define the so called $f$%
\textit{-contraction operator} $\overset{\vee}{f}:\wedge^{k}V\rightarrow
\wedge^{k-1}V$ by%
\begin{align*}
\overset{\vee}{f}\cdot\left(  u_{1}\wedge u_{2}\wedge...\wedge u_{k}\right)
&  =\sum\limits_{i=1}^{k}\left(  -1\right)  ^{i-1}f\left(  u_{i}\right)  \cdot
u_{1}\wedge u_{2}\wedge...\wedge u_{i-1}\wedge u_{i+1}\wedge u_{i+2}%
\wedge...\wedge u_{k}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{1},u_{2},...,u_{k}\in V.
\end{align*}
\footnote{In order to prove that this is well-defined, we need to check that
the term $\sum\limits_{i=1}^{k}\left(  -1\right)  ^{i-1}f\left(  u_{i}\right)
\cdot u_{1}\wedge u_{2}\wedge...\wedge u_{i-1}\wedge u_{i+1}\wedge
u_{i+2}\wedge...\wedge u_{k}$ depends multilinearly and antisymmetrically on
$u_{1},u_{2},...,u_{k}$. This is easy and left to the reader.} These
contraction operators are connected to the contraction operators defined in
Definition \ref{def.createdestroy.fin} \textbf{(b)}: Namely, $\overset{\vee
}{v_{i}}=\overset{\vee}{v_{i}^{\ast}}$ for every $i\in\left\{
1,2,...,n\right\}  $. More generally, $\overset{\vee}{e_{i}}=\overset{\vee
}{e_{i}^{\ast}}$ for every basis $\left(  e_{1},e_{2},...,e_{n}\right)  $ of
$V$ (where the maps $\widehat{e_{i}}$ and $\overset{\vee}{e_{i}}$ are defined
just as $\widehat{v_{i}}$ and $\overset{\vee}{v_{i}}$, but with respect to the
basis $\left(  e_{1},e_{2},...,e_{n}\right)  $).

The $f$-contraction operators, however, have a major advantage against the
contraction operators defined in Definition \ref{def.createdestroy.fin}
\textbf{(b)}: In fact, the former are canonical (i. e., they can be defined in
the same way for every vector space instead of $V$, and then they are
canonical maps that don't depend on any choice of basis), while the latter
have the basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ "hard-coded" into them.

Note that many sources denote the $f$-contraction operator by $i_{f}$ and call
it the \textit{interior product operator} with $f$.

It is easy to see that%
\begin{equation}
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}=f\left(  v\right)
\cdot\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{
and }v\in V \label{pf.plu.fv+vf}%
\end{equation}
(where, in the case $k=0$, we interpret $\widehat{v}\overset{\vee}{f}$ as $0$).

\textbf{(a)} We will give a basis-free definition of $S$. This will prove the
basis independence.

There is a unique vector space isomorphism $\Phi:V^{\ast}\otimes
V\rightarrow\operatorname*{End}V$ which satisfies%
\[
\Phi\left(  f\otimes v\right)  =\left(  \text{the map }V\rightarrow V\text{
sending each }w\text{ to }f\left(  w\right)  v\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{ and }v\in V.
\]
This $\Phi$ and its inverse isomorphism $\Phi^{-1}$ are actually basis-independent.

Now, define a map
\[
T:V^{\ast}\otimes V\otimes\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge
^{k+1}V\otimes\wedge^{k-1}V
\]
by%
\[
T\left(  f\otimes v\otimes\psi\otimes\phi\right)  =\left(  \widehat{v}%
\cdot\psi\right)  \otimes\left(  \overset{\vee}{f}\cdot\phi\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{, }v\in V\text{, }%
\psi\in\wedge^{k}V\text{ and }\phi\in\wedge^{k}V.
\]
This map $T$ is clearly well-defined (because $\widehat{v}\cdot\psi$ depends
bilinearly on $v$ and $\psi$, and because $\overset{\vee}{f}\cdot\phi$ depends
bilinearly on $f$ and $\phi$).

It is now easy to show that $S$ is the map $\wedge^{k}V\otimes\wedge
^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$ which sends $\psi
\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}%
\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in\wedge^{k}V$ and
$\phi\in\wedge^{k}V$.\ \ \ \ \footnote{\textit{Proof.} Consider the map
$\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$
which sends $\psi\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}%
\nolimits_{V}\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in
\wedge^{k}V$ and $\phi\in\wedge^{k}V$. This map is clearly well-defined. Now,
since $\Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}\right)  =\sum
\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}$ (because every $w\in V$ satisfies
\begin{align*}
\left(  \Phi\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}\right)
\right)  \left(  w\right)   &  =\sum\limits_{i=1}^{n}\underbrace{\left(
\Phi\left(  v_{i}^{\ast}\otimes v_{i}\right)  \right)  \left(  w\right)
}_{\substack{=v_{i}^{\ast}\left(  w\right)  v_{i}\\\text{(by the definition of
}\Phi\text{)}}}=\sum\limits_{i=1}^{n}v_{i}^{\ast}\left(  w\right)  v_{i}=w\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  v_{1}^{\ast},v_{2}^{\ast
},...,v_{n}^{\ast}\right)  \text{ is the dual basis of }\left(  v_{1}%
,v_{2},...,v_{n}\right)  \right) \\
&  =\operatorname*{id}\nolimits_{V}\left(  w\right)  ,
\end{align*}
so that $\Phi\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}\right)
=\operatorname*{id}\nolimits_{V}$), this map sends $\psi\otimes\phi$ to%
\begin{align*}
T\left(  \underbrace{\Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}\right)
}_{=\sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}}\otimes\psi\otimes
\phi\right)   &  =T\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes
v_{i}\otimes\psi\otimes\phi\right)  =\sum\limits_{i=1}^{n}\underbrace{T\left(
v_{i}^{\ast}\otimes v_{i}\otimes\psi\otimes\phi\right)  }_{\substack{=\left(
\widehat{v_{i}}\cdot\psi\right)  \otimes\left(  \overset{\vee}{v_{i}^{\ast}%
}\cdot\phi\right)  \\\text{(by the definition of }T\text{)}}}\\
&  =\sum\limits_{i=1}^{n}\left(  \widehat{v_{i}}\cdot\psi\right)
\otimes\left(  \underbrace{\overset{\vee}{v_{i}^{\ast}}}_{=\overset{\vee
}{v_{i}}}\cdot\phi\right)  =\sum\limits_{i=1}^{n}\left(  \widehat{v_{i}}%
\cdot\psi\right)  \otimes\left(  \overset{\vee}{v_{i}}\cdot\phi\right)
\end{align*}
for all $\psi\in\wedge^{k}V$ and $\phi\in\wedge^{k}V$. In other words, this
map is the map $\sum\limits_{i=1}^{n}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}=S$. So we have shown that $S$ is the map $\wedge^{k}V\otimes
\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$ which sends
$\psi\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}%
\nolimits_{V}\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in
\wedge^{k}V$ and $\phi\in\wedge^{k}V$, qed.} This shows immediately that $S$
is basis-independent (since $T$ and $\Phi^{-1}$ are basis-independent).

Since $S$ is basis-independent, it is clear that $S$ is $\operatorname*{GL}%
\left(  V\right)  $-invariant (because the action of $\operatorname*{GL}%
\left(  V\right)  $ transforms $S$ into the same operator $S$ but constructed
for a different basis; but since $S$ is basis-independent, this other $S$ must
be the $S$ that we started with). This proves Theorem \ref{thm.plu}
\textbf{(a)}.

\textbf{(b)} Let $\tau\in\Omega$ be nonzero.

\textbf{1)} First let us show that if $\tau\in\Omega$, then $S\left(
\tau\otimes\tau\right)  =0$.

In order to show this, it is enough to prove that $S\left(  \tau\otimes
\tau\right)  =0$ holds in the case $\tau=v_{1}\wedge v_{2}\wedge...\wedge
v_{k}$ (since $S$ is $\operatorname*{GL}\left(  V\right)  $-invariant, and
every wedge product of the form $v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge
v_{i_{k}}$ is in the $\operatorname*{GL}\left(  V\right)  $-orbit of
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$).

But this is obvious, because for every $i\in\left\{  1,2,...,n\right\}  $,
either $\widehat{v_{i}}$ or $\overset{\vee}{v_{i}}$ annihilates $\tau$.

\textbf{2)} Let us now (conversely) prove that if $S\left(  \tau\otimes
\tau\right)  =0$, then $\tau\in\Omega$.

(There is a combinatorial proof of this in the infinite setting in the
Kac-Raina book, but we will make a different proof here.)

Define $E\subseteq V$ to be the set $\left\{  v\in V\ \mid\ \widehat{v}%
\tau=0\right\}  $. Define $E^{\prime}\subseteq V^{\ast}$ to be the set
$\left\{  f\in V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  $. Clearly,
$E$ is a subspace of $V$, and $E^{\prime}$ is a subspace of $V^{\ast}$.

We know that all $v\in E$ and $f\in E^{\prime}$ satisfy $\left(
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}\right)  \tau=0$
(since the definition of $E$ yields $\widehat{v}\tau=0$, and the definition of
$E^{\prime}$ yields $\overset{\vee}{f}\tau=0$). But $\underbrace{\left(
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}\right)
}_{\substack{=f\left(  v\right)  \operatorname*{id}\\\text{(by
(\ref{pf.plu.fv+vf}))}}}\tau=f\left(  v\right)  \tau$, so this yields
$f\left(  v\right)  \tau=0$, and thus $f\left(  v\right)  =0$ (since $\tau
\neq0$). Thus, $E\subseteq E^{\prime\perp}$.

Let $m=\dim E$ and $r=\dim\left(  E^{\prime\perp}\right)  $. Pick a basis
$\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V$ such that $\left(  e_{1}%
,e_{2},...,e_{m}\right)  $ is a basis of $E$ and such that $\left(
e_{1},e_{2},...,e_{r}\right)  $ is a basis of $E^{\prime\perp}$. (Such a basis
clearly exists.)

Clearly, for every $i\in\left\{  1,2,...,m\right\}  $, we have $e_{i}\in E$
and thus $\widehat{e_{i}}\tau=0$ (by the definition of $E$).

Also, for every $i\in\left\{  r+1,r+2,...,n\right\}  $, we have $\overset{\vee
}{e_{i}^{\ast}}\tau=0$ (because $i>r$, so that $e_{i}^{\ast}\left(
e_{j}\right)  =0$ for all $j\in\left\{  1,2,...,r\right\}  $, so that
$e_{i}^{\ast}\left(  E^{\prime}\right)  =0$ (since $\left(  e_{1}%
,e_{2},...,e_{r}\right)  $ is a basis of $E^{\prime\perp}$), so that
$e_{i}^{\ast}\in\left(  E^{\prime\perp}\right)  ^{\perp}=E^{\prime}$).

The vectors $\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,n\right\}  $
are linearly independent (because if some linear combination of them was zero,
then some linear combination of the $e_{i}$ with $i\in\left\{
m+1,m+2,...,n\right\}  $ would lie in $\left\{  v\in V\ \mid\ \widehat{v}%
\tau=0\right\}  =E$, but this contradicts the fact that $\left(  e_{1}%
,e_{2},...,e_{m}\right)  $ is a basis of $E$). Hence, the vectors
$\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,r\right\}  $ are linearly independent.

We defined $S$ using the basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ of $V$
by the formula $S=\sum\limits_{i=1}^{n}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$. Since $S$ did not depend on the basis, we get the same $S$ if we
define it using the basis $\left(  e_{1},e_{2},...,e_{n}\right)  $. Thus, we
have $S=\sum\limits_{i=1}^{n}\widehat{e_{i}}\otimes\overset{\vee}{e_{i}}$.
Hence,%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =\sum\limits_{i=1}^{m}%
\underbrace{\widehat{e_{i}}\tau}_{\substack{=0\\\text{(since }i\in\left\{
1,2,...,m\right\}  \text{)}}}\otimes\overset{\vee}{e_{i}^{\ast}}\tau
+\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}%
}\tau+\sum\limits_{i=r+1}^{n}\widehat{e_{i}}\tau\otimes
\underbrace{\overset{\vee}{e_{i}^{\ast}}\tau}_{\substack{=0\\\text{(since
}i\in\left\{  r+1,r+2,...,n\right\}  \text{)}}}\\
&  =\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee
}{e_{i}^{\ast}}\tau.
\end{align*}
Thus, $S\left(  \tau\otimes\tau\right)  =0$ rewrites as $\sum\limits_{i=m+1}%
^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}}\tau=0$. But since
the vectors $\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,r\right\}  $
are linearly independent, this yields that $\overset{\vee}{e_{i}^{\ast}}%
\tau=0$ for any $i\in\left\{  m+1,m+2,...,r\right\}  $. Thus, for every
$i\in\left\{  m+1,m+2,...,r\right\}  $, we have $e_{i}^{\ast}\in\left\{  f\in
V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  =E^{\prime}$, so that
$e_{i}^{\ast}\left(  E^{\prime\perp}\right)  =0$. But on the other hand, for
every $i\in\left\{  m+1,m+2,...,r\right\}  $, we have $e_{i}\in E^{\prime
\perp}$ (since $\left(  e_{1},e_{2},...,e_{r}\right)  $ is a basis of
$E^{\prime\perp}$, and since $i\leq r$). Thus, for every $i\in\left\{
m+1,m+2,...,r\right\}  $, we have $1=e_{i}^{\ast}\left(  \underbrace{e_{i}%
}_{\in E^{\prime\perp}}\right)  \in e_{i}^{\ast}\left(  E^{\prime\perp
}\right)  =0$. This is a contradiction unless there are no $i\in\left\{
m+1,m+2,...,r\right\}  $ at all.

So we conclude that there are no $i\in\left\{  m+1,m+2,...,r\right\}  $ at
all. In other words, $m=r$. Thus, $\dim E=m=r=\dim\left(  E^{\prime\perp
}\right)  $. Combined with $E\subseteq E^{\prime\perp}$, this yields
$E=E^{\prime\perp}$.

Now, recall that $\left(  e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}%
}\right)  _{1\leq i_{1}<i_{2}<...<i_{k}\leq n}$ is a basis of $\wedge^{k}V$.
Hence, we can write $\tau$ in the form $\tau=\sum\limits_{1\leq i_{1}%
<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}$ for some scalars $\lambda_{i_{1}%
,i_{2},...,i_{k}}\in\mathbb{C}$.

Now, we will prove:

\textit{Observation 1:} For every $k$-tuple $\left(  j_{1},j_{2}%
,...,j_{k}\right)  $ of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq
n$ and $\left\{  1,2,...,m\right\}  \not \subseteq \left\{  j_{1}%
,j_{2},...,j_{k}\right\}  $, we have $\lambda_{j_{1},j_{2},...,j_{k}}=0$.

\textit{Proof of Observation 1:} Let $\left(  j_{1},j_{2},...,j_{k}\right)  $
be a $k$-tuple of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ and
$\left\{  1,2,...,m\right\}  \not \subseteq \left\{  j_{1},j_{2}%
,...,j_{k}\right\}  $. Then, there exists an $i\in\left\{  1,2,...,m\right\}
$ such that $i\notin\left\{  j_{1},j_{2},...,j_{k}\right\}  $. Consider this
$i$. As we saw above, this yields $\widehat{e_{i}}\tau=0$. Thus,%
\begin{align*}
0  &  =\widehat{e_{i}}\tau=e_{i}\wedge\tau=\sum\limits_{1\leq i_{1}%
<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i}\wedge e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau=\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  =\sum\limits_{\substack{1\leq i_{1}<i_{2}<...<i_{k}\leq n;\\i\notin\left\{
i_{1},i_{2},...,i_{k}\right\}  }}\lambda_{i_{1},i_{2},...,i_{k}}e_{i}\wedge
e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since all terms of the sum with }%
i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  \text{ are }0\right)  .
\end{align*}
Thus, for every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$ and $i\notin\left\{
i_{1},i_{2},...,i_{k}\right\}  $, we must have $\lambda_{i_{1},i_{2}%
,...,i_{k}}=0$ (because the wedge products $e_{i}\wedge e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}$ for all such $k$-tuples are linearly
independent elements of $\wedge^{k+1}V$). Applied to $\left(  i_{1}%
,i_{2},...,i_{k}\right)  =\left(  j_{1},j_{2},...,j_{k}\right)  $, this yields
that $\lambda_{j_{1},j_{2},...,j_{k}}=0$. Observation 1 is proven.

\textit{Observation 2:} For every $k$-tuple $\left(  j_{1},j_{2}%
,...,j_{k}\right)  $ of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq
n$ and $\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $, we have $\lambda_{j_{1},j_{2},...,j_{k}}=0$.

\textit{Proof of Observation 2:} Let $\left(  j_{1},j_{2},...,j_{k}\right)  $
be a $k$-tuple of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ and
$\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $. Then, there exists an $i\in\left\{  j_{1},j_{2}%
,...,j_{k}\right\}  $ such that $i\notin\left\{  1,2,...,m\right\}  $.
Consider this $i$. Then, $i\notin\left\{  1,2,...,m\right\}  $, so that
$i>m=r$, so that $i\in\left\{  r+1,r+2,...,n\right\}  $. As we saw above, this
yields $\overset{\vee}{e_{i}^{\ast}}\tau=0$. Thus,
\begin{align*}
0  &  =\underbrace{\overset{\vee}{e_{i}^{\ast}}}_{=\overset{\vee}{e_{i}}}%
\tau=\overset{\vee}{e_{i}}\tau=\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq
n}\lambda_{i_{1},i_{2},...,i_{k}}\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau=\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  =\sum\limits_{\substack{1\leq i_{1}<i_{2}<...<i_{k}\leq n;\\i\in\left\{
i_{1},i_{2},...,i_{k}\right\}  }}\lambda_{i_{1},i_{2},...,i_{k}}%
\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge
e_{i_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since all terms of the sum with }%
i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}  \text{ are }0\right)  .
\end{align*}
Thus, for every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$ and $i\in\left\{  i_{1}%
,i_{2},...,i_{k}\right\}  $, we must have $\lambda_{i_{1},i_{2},...,i_{k}}=0$
(because the wedge products $\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right)  $ for all such $k$-tuples are
linearly independent elements of $\wedge^{k-1}V$\ \ \ \ \footnote{To check
this, it is enough to recall how $\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\right)  $ was defined: It was
defined to be $\left(  -1\right)  ^{j-1}e_{i_{1}}\wedge e_{i_{2}}%
\wedge...\wedge e_{i_{j-1}}\wedge e_{i_{j+1}}\wedge e_{i_{j+2}}\wedge...\wedge
e_{i_{k}}$, where $j$ is the integer $\ell$ satisfying $i_{\ell}=i$.}).
Applied to $\left(  i_{1},i_{2},...,i_{k}\right)  =\left(  j_{1}%
,j_{2},...,j_{k}\right)  $, this yields that $\lambda_{j_{1},j_{2},...,j_{k}%
}=0$. Observation 2 is proven.

Now, every $k$-tuple $\left(  j_{1},j_{2},...,j_{k}\right)  $ of integers
satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ must satisfy either $\left\{
1,2,...,m\right\}  \not \subseteq \left\{  j_{1},j_{2},...,j_{k}\right\}  $,
or $\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $, or $\left(  1,2,...,m\right)  =\left(  j_{1}%
,j_{2},...,j_{k}\right)  $. In the first of these three cases, we have
$\lambda_{j_{1},j_{2},...,j_{k}}=0$ by Observation 1; in the second case, we
have $\lambda_{j_{1},j_{2},...,j_{k}}=0$ by Observation 2. Hence, the only
case where $\lambda_{j_{1},j_{2},...,j_{k}}$ can be nonzero is the third case,
i. e., the case when $\left(  1,2,...,m\right)  =\left(  j_{1},j_{2}%
,...,j_{k}\right)  $. Hence, the only nonzero addend that the sum
$\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2}%
,...,i_{k}}e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}$ can have is the
addend for $\left(  i_{1},i_{2},...,i_{k}\right)  =\left(  1,2,...,m\right)
$. Thus, all other addends of this sum can be removed, and therefore
$\tau=\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1}%
,i_{2},...,i_{k}}e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}$ rewrites
as $\tau=\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}$. Since
$\tau\neq0$, we thus have $\lambda_{1,2,...,m}\neq0$. Hence, $m=k$ (because
$\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}=\tau\in\wedge^{k}%
V$). Hence,%
\[
\tau=\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}=\lambda
_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{k}=\left(  \lambda
_{1,2,...,m}e_{1}\right)  \wedge e_{2}\wedge e_{3}\wedge...\wedge e_{k}.
\]


Now, since $\lambda_{1,2,...,m}\neq0$, the $n$-tuple $\left(  \lambda
_{1,2,...,m}e_{1},e_{2},e_{3},...,e_{n}\right)  $ is a basis of $V$. Thus,
there exists an element of $\operatorname*{GL}\left(  V\right)  $ which sends
$\left(  v_{1},v_{2},...,v_{n}\right)  $ to $\left(  \lambda_{1,2,...,m}%
e_{1},e_{2},e_{3},...,e_{n}\right)  $. This element therefore sends
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ to $\left(  \lambda_{1,2,...,m}%
e_{1}\right)  \wedge e_{2}\wedge e_{3}\wedge...\wedge e_{k}=\tau$. Hence,
$\tau$ lies in the $\operatorname*{GL}\left(  V\right)  $-orbit of
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Since this orbit was called $\Omega
$, this becomes $\tau\in\Omega$.

We thus have shown that if $S\left(  \tau\otimes\tau\right)  =0$, then
$\tau\in\Omega$. This completes the proof of Theorem \ref{thm.plu}
\textbf{(b)}.

\textbf{(c)} We know from Theorem \ref{thm.plu} \textbf{(a)} that $S$ is
$\operatorname*{GL}\left(  V\right)  $-invariant. Since $\operatorname*{GL}%
\left(  V\right)  $ is Zariski-dense in $\operatorname*{M}\left(  V\right)  $,
this yields that $S$ is $\operatorname*{M}\left(  V\right)  $-invariant
(because the $\operatorname*{M}\left(  V\right)  $-invariance of $S$ can be
written as a collection of polynomial identities). This proves Theorem
\ref{thm.plu} \textbf{(c)}.

We can rewrite Theorem \ref{thm.plu} \textbf{(b)} in coordinates:

\begin{theorem}
\label{thm.plu.coo}Let $n\in\mathbb{N}$. Let $k\in\left\{  1,2,...,n\right\}
$. We consider the vector space $V=\mathbb{C}^{n}$ with its standard basis
$\left(  v_{1},v_{2},...,v_{n}\right)  $.

Let $\tau\in\wedge^{k}V$.

For every subset $K$ of $\left\{  1,2,...,n\right\}  $, let $v_{K}$ denote the
element of $\wedge^{\left\vert K\right\vert }V$ defined by $v_{K}=v_{k_{1}%
}\wedge v_{k_{2}}\wedge...\wedge v_{k_{\ell}}$ where $k_{1}$, $k_{2}$, $...$,
$k_{\ell}$ are the elements of $K$ in increasing order. We know that $\left(
v_{K}\right)  _{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert
K\right\vert =k}$ is a basis of the vector space $\wedge^{k}V$. For every
subset $K$ of $\left\{  1,2,...,n\right\}  $ satisfying $\left\vert
K\right\vert =k$, let $P_{K}$ be the $K$-coordinate of $\tau$ with respect to
this basis.

Then, $\tau\in\Omega$ if and only if
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{for all }I\subseteq\left\{  1,2,...,n\right\}  \text{ with }\left\vert
I\right\vert =k-1\text{ and all }J\subseteq\left\{  1,2,...,n\right\} \\
\text{with }\left\vert J\right\vert =k+1\text{, we have }\sum\limits_{j\in
J;\ j\notin I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)
^{\nu\left(  j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{
j\right\}  }=0,
\end{array}
\right)  \label{thm.plu.coo.plu}%
\end{equation}
where $\nu\left(  j\right)  $ is the integer $\ell$ for which $j$ is the
$\ell$-th smallest element of the set $J$, and where $\mu\left(  j\right)  $
is the number of elements of the set $I$ which are smaller than $j$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.plu.coo} (sketched).} We know that $\left(
v_{K}\right)  _{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert
K\right\vert =k+1}$ is a basis of $\wedge^{k+1}V$, and $\left(  v_{K}\right)
_{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert K\right\vert =k-1}$ is a
basis of $\wedge^{k-1}V$. Hence, $\left(  v_{K}\otimes v_{L}\right)
_{\substack{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert K\right\vert
=k+1,\\L\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert L\right\vert
=k-1}}$ is a basis of $\wedge^{k+1}V\otimes\wedge^{k-1}V$. It is not hard to
check that the $v_{J}\otimes v_{I}$-coordinate (with respect to this basis) of
$S\left(  \tau\otimes\tau\right)  $ is precisely $\sum\limits_{j\in
J;\ j\notin I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)
^{\nu\left(  j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{
j\right\}  }$ for all $I\subseteq\left\{  1,2,...,n\right\}  $ with
$\left\vert I\right\vert =k-1$ and all $J\subseteq\left\{  1,2,...,n\right\}
$ with $\left\vert J\right\vert =k+1$. Hence, (\ref{thm.plu.coo.plu}) holds if
and only if every coordinate of $S\left(  \tau\otimes\tau\right)  $ is zero,
i. e., if $S\left(  \tau\otimes\tau\right)  =0$, but the latter condition is
equivalent to $\tau\in\Omega$ (because of Theorem \ref{thm.plu} \textbf{(b)}).
This proves Theorem \ref{thm.plu.coo}.

Note that the $\Longrightarrow$ direction of Theorem \ref{thm.plu.coo} can be
formulated as a determinantal identity:

\begin{corollary}
\label{cor.plu.matrix}Let $n\in\mathbb{N}$. Let $k\in\left\{
1,2,...,n\right\}  $. Let $\left(
\begin{array}
[c]{cccc}%
x_{11} & x_{12} & ... & x_{1k}\\
x_{21} & x_{22} & ... & x_{2k}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & ... & x_{nk}%
\end{array}
\right)  $ be any matrix with $n$ rows and $k$ columns.

For every $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k$, let $P_{I}$ be the minor of this matrix obtained by only
keeping the rows whose indices lie in $I$ (and throwing all other rows away).

Then, for all $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k-1$ and all $J\subseteq\left\{  1,2,...,n\right\}  $ with
$\left\vert J\right\vert =k+1$, we have $\sum\limits_{j\in J;\ j\notin
I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)  ^{\nu\left(
j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{  j\right\}
}=0$.
\end{corollary}

\textit{Example:} If $n=4$ and $k=2$, then the claim of Corollary
\ref{cor.plu.matrix} is easily simplified to the single equation $P_{12}%
P_{34}+P_{14}P_{23}-P_{13}P_{24}=0$ (where we abbreviate two-element sets
$\left\{  i,j\right\}  $ by $ij$).

\textit{Proof of Corollary \ref{cor.plu.matrix} (sketched).} WLOG assume
$k\leq n$ (else, everything is vacuously true).

For every $i\in\left\{  1,2,...,k\right\}  $, let $x_{i}\in V$ be the vector
$\left(
\begin{array}
[c]{c}%
x_{1i}\\
x_{2i}\\
\vdots\\
x_{ni}%
\end{array}
\right)  $, where $V$ is as in Theorem \ref{thm.plu.coo}. Since Corollary
\ref{cor.plu.matrix} is a collection of polynomial identities, we can WLOG
assume that the vectors $x_{1}$, $x_{2}$, $...$, $x_{k}$ are linearly
independent (since the set of linearly independent $k$-tuples $\left(
x_{1},x_{2},...,x_{k}\right)  $ of vectors in $V$ is Zariski-dense in $V^{k}%
$). Then, there exists an element of $\operatorname*{GL}\left(  V\right)  $
which maps $v_{1}$, $v_{2}$, $...$, $v_{k}$ to $x_{1}$, $x_{2}$, $...$,
$x_{k}$. Thus, $x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\Omega$ (since
$\Omega$ is the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $). Now, apply Theorem \ref{thm.plu.coo}
to $\tau=x_{1}\wedge x_{2}\wedge...\wedge x_{k}$, and Corollary
\ref{cor.plu.matrix} follows.

Of course, this was not the easiest way to prove Corollary
\ref{cor.plu.matrix}. We could just as well have derived Corollary
\ref{cor.plu.matrix} from the Cauchy-Binet identity, and thus given a new
proof for the $\Longrightarrow$ direction of Theorem \ref{thm.plu.coo}; but
the $\Longleftarrow$ direction is not that easy.

\subsubsection{\label{subsubsect.infgrass}The semiinfinite Grassmannian:
preliminary work}

Now we prepare for the semiinfinite Grassmannian:

Let $\psi_{0}$ denote the elementary semiinfinite wedge $v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\in\mathcal{F}^{\left(  0\right)  }$. We recall
the action $\varrho:\operatorname*{M}\left(  \infty\right)  \rightarrow
\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $ of the
monoid $\operatorname*{M}\left(  \infty\right)  $ on $\mathcal{F}^{\left(
m\right)  }$ for every $m\in\mathbb{Z}$. This action was defined in Definition
\ref{def.GLinf.act}.

\begin{definition}
From now on, $\Omega$ denotes the set $\operatorname*{GL}\left(
\infty\right)  \cdot\psi_{0}$. (Here and in the following, we abbreviate
$\left(  \varrho\left(  A\right)  \right)  v$ by $Av$ for every $A\in
\operatorname*{M}\left(  \infty\right)  $ and $v\in\mathcal{F}^{\left(
m\right)  }$ and every $m\in\mathbb{Z}$. In particular, $\operatorname*{GL}%
\left(  \infty\right)  \psi_{0}$ means $\left(  \varrho\left(
\operatorname*{GL}\left(  \infty\right)  \right)  \right)  \psi_{0}$.)
\end{definition}

\begin{proposition}
\label{prop.plu.inf.pure}For all $0$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we have $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\Omega$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.inf.pure}.} Let $\left(
i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. Then, there exists a
permutation $\sigma:\mathbb{Z}\rightarrow\mathbb{Z}$ which fixes all but
finitely many numbers, and satisfies $i_{k}=\sigma\left(  -k\right)  $ for
every $k\in\mathbb{N}$. Since $\sigma$ fixes all but finitely many numbers, we
can represent $\sigma$ by a matrix in $\operatorname*{GL}\left(
\infty\right)  $. Let us (by abuse of notation) denote this matrix by $\sigma$
again. Then, every $k\in\mathbb{N}$ satisfies $v_{i_{k}}=v_{\sigma\left(
-k\right)  }=\sigma v_{-k}$. Thus,
\[
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=\sigma v_{0}\wedge\sigma
v_{-1}\wedge\sigma v_{-2}\wedge...=\sigma\underbrace{\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\right)  }_{=\psi_{0}}=\sigma\psi_{0}%
\in\operatorname*{GL}\left(  \infty\right)  \psi_{0}=\Omega.
\]
This proves Proposition \ref{prop.plu.inf.pure}.

Next, an "infinite" analogue of Theorem \ref{thm.plu}:

\begin{theorem}
\label{thm.plu.inf}For every $m\in\mathbb{Z}$, define a map $S:\mathcal{F}%
^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m+1\right)  }\otimes\mathcal{F}^{\left(  m-1\right)  }$
by $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}}%
$. (Note that the map $S$ is well-defined because, for every $T\in
\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }$, only
finitely many terms of the infinite sum $\sum\limits_{i\in\mathbb{Z}}\left(
\widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)  \left(  T\right)  $ are nonzero.)

\textbf{(a)} For every $m\in\mathbb{Z}$, this map $S$ is $\operatorname*{GL}%
\left(  \infty\right)  $-invariant.

\textbf{(b)} Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be nonzero. Then,
$\tau\in\Omega$ if and only if $S\left(  \tau\otimes\tau\right)  =0$.

\textbf{(c)} For every $m\in\mathbb{Z}$, the map $S$ is $\operatorname*{M}%
\left(  \infty\right)  $-invariant.
\end{theorem}

We are going to prove this theorem by reducing it to its "finite-dimensional
version" (i. e., Theorem \ref{thm.plu}). This reduction requires us to link
the set $\Omega$ with its finite-dimensional analoga. To do this, we set up
some definitions:

\subsubsection{Proof of Theorem \ref{thm.plu.inf}}

While the following definitions and results are, superficially seen, auxiliary
to the proof of Theorem \ref{thm.plu.inf}, their use is not confined to this
proof. They can be used to derive various results about semiinfinite wedges
(elements of $\mathcal{F}^{\left(  m\right)  }$ for integer $m$) from similar
statement about finite wedges (elements of $\wedge^{k}W$ for integer $k$ and
finite-dimensional $W$). Our proof of Theorem \ref{thm.plu.inf} below will be
just one example of such a derivation.

Note that most of the proofs in this subsection are straightforward and boring
and are easier to do by the reader than to understand from these notes.

\begin{definition}
\label{def.plu.inf.VN}Let $V$ be the vector space $\mathbb{C}^{\left(
\mathbb{Z}\right)  }=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}%
\text{\ }\mid\ x_{i}\in\mathbb{C}\text{; only finitely many }x_{i}\text{ are
nonzero}\right\}  $ as defined in Definition \ref{def.glinf.V}. Let $\left(
v_{j}\right)  _{j\in\mathbb{Z}}$ be the basis of $V$ introduced in Definition
\ref{def.glinf.V}.

For every $N\in\mathbb{N}$, let $V_{N}$ denote the $\left(  2N+1\right)
$-dimensional vector subspace $\left\langle v_{-N},v_{-N+1},...,v_{N}%
\right\rangle $ of $V$. It is clear that $V_{0}\subseteq V_{1}\subseteq
V_{2}\subseteq...$ and $V=\bigcup\limits_{N\in\mathbb{N}}V_{N}$.
\end{definition}

\begin{definition}
\label{def.plu.inf.iN}Let $N\in\mathbb{N}$. Let $\operatorname*{M}\left(
V_{N}\right)  $ denote the set of all $\left(  2N+1\right)  \times\left(
2N+1\right)  $-matrices over $\mathbb{C}$ whose rows are indexed by elements
of $\left\{  -N,-N+1,...,N\right\}  $ and whose columns are also indexed by
elements of $\left\{  -N,-N+1,...,N\right\}  $. Define a map $i_{N}%
:\operatorname*{M}\left(  V_{N}\right)  \rightarrow\operatorname*{M}\left(
\infty\right)  $ as follows: For every matrix $A\in\operatorname*{M}\left(
V_{N}\right)  $, let $i_{N}\left(  A\right)  $ be the infinite matrix (with
rows and columns indexed by integers) such that%
\[
\left(
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }i_{N}\left(
A\right)  \right) \\
=\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{for
every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\end{array}
\right)  .
\]
It is easy to see that this map $i_{N}$ is well-defined (i. e., for every
$A\in\operatorname*{M}\left(  V_{N}\right)  $, the matrix $i_{N}\left(
A\right)  $ that we just defined really lies in $\operatorname*{M}\left(
\infty\right)  $), injective and a monoid homomorphism.

The vector space $V_{N}$ has a basis $\left(  v_{-N},v_{-N+1},...,v_{N}%
\right)  $ which is indexed by the set $\left\{  -N,-N+1,...,N\right\}  $.
Thus, we can identify matrices in $\operatorname*{M}\left(  V_{N}\right)  $
with endomorphisms of the vector space $V_{N}$ in the obvious way. Hence, the
invertible elements of $\operatorname*{M}\left(  V_{N}\right)  $ are
identified with the invertible endomorphisms of the vector space $V_{N}$, i.
e., with the elements of $\operatorname*{GL}\left(  V_{N}\right)  $. The
injective map $i_{N}:\operatorname*{M}\left(  V_{N}\right)  \rightarrow
\operatorname*{M}\left(  \infty\right)  $ restricts to an injective map
$i_{N}\mid_{\operatorname*{GL}\left(  V_{N}\right)  }:\operatorname*{GL}%
\left(  V_{N}\right)  \rightarrow\operatorname*{GL}\left(  \infty\right)  $.
\end{definition}

\begin{remark}
Here is a more lucid way to describe the map $i_{N}$ we just defined:

Let $I_{-\infty}$ be the infinite identity matrix whose rows are indexed by
all negative integers, and whose columns are indexed by all negative integers.

Let $I_{\infty}$ be the infinite identity matrix whose rows are indexed by all
positive integers, and whose columns are indexed by all positive integers.

For any matrix $A\in\operatorname*{M}\left(  V_{N}\right)  $, we define
$i_{N}\left(  A\right)  $ to be the block-diagonal matrix $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & A & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ whose diagonal blocks are $I_{-\infty}$, $A$ and $I_{\infty}$,
where the first block covers the rows with indices smaller than $-N$ (and
therefore also the columns with indices smaller than $-N$), the second block
covers the rows with indices in $\left\{  -N,-N+1,...,N\right\}  $ (and
therefore also the columns with indices in $\left\{  -N,-N+1,...,N\right\}
$), and the third block covers the rows with indices larger than $N$ (and
therefore also the columns with indices larger than $N$). From this
definition, it becomes clear why $i_{N}$ is a monoid homomorphism. (In fact,
it is clear that the block-diagonal matrix $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & I_{2N+1} & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ is the identity matrix, and using the rules for computing with
block matrices it is also easy to see that $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & A & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & B & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & AB & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ for all $A\in\operatorname*{M}\left(  V_{N}\right)  $ and
$B\in\operatorname*{M}\left(  V_{N}\right)  $.)
\end{remark}

\begin{remark}
\label{rmk.plu.inf.iN}\textbf{(a)} Every $N\in\mathbb{N}$ satisfies%
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  =\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  .
\]


\textbf{(b)} We have $i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)
\right)  \subseteq i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)
\right)  \subseteq i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)
\right)  \subseteq...$.

\textbf{(c)} We have $\operatorname*{M}\left(  \infty\right)  =\bigcup
\limits_{N\in\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right)  $.
\end{remark}

\textit{Proof of Remark \ref{rmk.plu.inf.iN}.} \textbf{(a)} Let $N\in
\mathbb{N}$. Then,%
\[
\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  \subseteq i_{N}\left(  \operatorname*{M}\left(
V_{N}\right)  \right)
\]
\footnote{\textit{Proof.} To prove this, it is clearly enough to show that
every matrix $A\in\operatorname*{M}\left(  \infty\right)  $ which satisfies
\begin{equation}
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}\right)  \label{pf.plu.inf.iN.1}%
\end{equation}
lies in $i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $. So
let $A\in\operatorname*{M}\left(  \infty\right)  $ be a matrix which satisfies
(\ref{pf.plu.inf.iN.1}). We must prove that $A\in i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $.
\par
Indeed, let $B\in\operatorname*{M}\left(  V_{N}\right)  $ be the matrix
defined by%
\begin{equation}
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)  \text{
for every }\left(  i,j\right)  \in\left\{  -N,-N+1,...,N\right\}  ^{2}\right)
. \label{pf.plu.inf.iN.2}%
\end{equation}
Then, $i_{N}\left(  B\right)  =A$ (because for every $\left(  i,j\right)
\in\mathbb{Z}^{2}$, we have%
\begin{align*}
&  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }i_{N}\left(
B\right)  \right) \\
&  =\left\{  \left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right.  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{N}\left(
B\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.plu.inf.iN.2})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\mathbb{Z}%
^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{i,j}=\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }A\right)  \text{ for every }\left(
i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}\text{ (by (\ref{pf.plu.inf.iN.1}))}\right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\end{align*}
). Thus, $A=i_{N}\left(  B\right)  \in i_{N}\left(  \operatorname*{M}\left(
V_{N}\right)  \right)  $ (since $B\in\operatorname*{M}\left(  V_{N}\right)
$), qed.} and%
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  \subseteq\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}
\]
(by the definition of $i_{N}$). Combining these two relations, we obtain
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  =\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  .
\]
This proves Remark \ref{rmk.plu.inf.iN} \textbf{(a)}.

\textbf{(b)} By Remark \ref{rmk.plu.inf.iN} \textbf{(a)}, for any
$N\in\mathbb{N}$, the set $i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right)  $ is the set of all matrices $A\in\operatorname*{M}\left(
\infty\right)  $ satisfying the condition%
\[
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}\right)  .
\]
If this condition is satisfied for some $N$, then it is (all the more)
satisfied for $N+1$ instead of $N$. Hence, $i_{N}\left(  \operatorname*{M}%
\left(  V_{N}\right)  \right)  \subseteq i_{N+1}\left(  \operatorname*{M}%
\left(  V_{N+1}\right)  \right)  $ for any $N\in\mathbb{N}$. Thus,
$i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)  \right)  \subseteq
i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)  \right)  \subseteq
i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)  \right)  \subseteq...$.
This proves Remark \ref{rmk.plu.inf.iN} \textbf{(b)}.

\textbf{(c)} Let $B\in\operatorname*{M}\left(  \infty\right)  $ be arbitrary.
We will now construct an $N\in\mathbb{N}$ such that $B\in i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $.

Since $B\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists a $b\in\mathfrak{gl}_{\infty}$ such
that $B=\operatorname*{id}+b$. Consider this $b$.

For any $\left(  i,j\right)  \in\mathbb{Z}^{2}$, let $b_{i,j}$ denote the
$\left(  i,j\right)  $-th entry of the matrix $b$.

Since $b\in\mathfrak{gl}_{\infty}$, only finitely many entries of the matrix
$b$ are nonzero. In other words, only finitely many $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfy $\left(  \left(  u,v\right)  \text{-th entry of
}b\right)  \neq0$. In other words, only finitely many $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfy $b_{u,v}\neq0$ (since $\left(  \left(  u,v\right)
\text{-th entry of }b\right)  =b_{u,v}$).

Let
\[
N=\max\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  .
\]
This $N$ is a well-defined nonnegative integer (since only finitely many
$\left(  u,v\right)  \in\mathbb{Z}^{2}$ satisfy $b_{u,v}\neq0$, and thus the
set $\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  $ is finite).

Let $\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{
-N,-N+1,...,N\right\}  ^{2}$. Then, $\left(  i,j\right)  \notin\left\{
-N,-N+1,...,N\right\}  ^{2}$. We are now going to show that $b_{i,j}=0$.

In fact, assume (for the sake of contradiction) that $b_{i,j}\neq0$. Thus,
$\left(  i,j\right)  \in\left\{  \left(  u,v\right)  \in\mathbb{Z}^{2}%
\ \mid\ b_{u,v}\neq0\right\}  $. Hence,%
\[
\max\left\{  \left\vert i\right\vert ,\left\vert j\right\vert \right\}
\in\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  .
\]
Since any element of a finite set is less or equal to the maximum of the set,
this yields%
\[
\max\left\{  \left\vert i\right\vert ,\left\vert j\right\vert \right\}
\leq\max\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  =N.
\]
Thus, $\left\vert i\right\vert \leq\max\left\{  \left\vert i\right\vert
,\left\vert j\right\vert \right\}  \leq N$, so that $i\in\left\{
-N,-N+1,...,N\right\}  $ and similarly $j\in\left\{  -N,-N+1,...,N\right\}  $.
Hence, $\left(  i,j\right)  \in\left\{  -N,-N+1,...,N\right\}  ^{2}$ (because
$i\in\left\{  -N,-N+1,...,N\right\}  $ and $j\in\left\{
-N,-N+1,...,N\right\}  $), which contradicts $\left(  i,j\right)
\notin\left\{  -N,-N+1,...,N\right\}  ^{2}$. This contradiction shows that our
assumption (that $b_{i,j}\neq0$) was wrong. We thus have $b_{i,j}=0$.

Since $B=\operatorname*{id}+b$, we have:%
\[
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\underbrace{\left(  \text{the }\left(  i,j\right)  \text{-th entry of
}\operatorname*{id}\right)  }_{=\delta_{i,j}}+\underbrace{\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }b\right)  }_{=b_{i,j}=0}=\delta
_{i,j}.
\]


Now, forget that we fixed $\left(  i,j\right)  $. We thus have shown that
$\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\delta_{i,j}$ for every $\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown
\left\{  -N,-N+1,...,N\right\}  ^{2}$. In other words,%
\begin{align*}
B  &  \in\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  =i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.plu.inf.iN}
\textbf{(a)}}\right) \\
&  \subseteq\bigcup\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}%
\left(  V_{P}\right)  \right)  .
\end{align*}
Now forget that we fixed $B$. We thus have proven that every $B\in
\operatorname*{M}\left(  \infty\right)  $ satisfies $B\in\bigcup
\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}\left(  V_{P}\right)
\right)  $. In other words, $\operatorname*{M}\left(  \infty\right)
\subseteq\bigcup\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}\left(
V_{P}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $ (here, we renamed the index
$P$ as $N$). Combined with the obvious inclusion $\bigcup\limits_{N\in
\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)
\subseteq\operatorname*{M}\left(  \infty\right)  $, this yields
$\operatorname*{M}\left(  \infty\right)  =\bigcup\limits_{N\in\mathbb{N}}%
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $. Remark
\ref{rmk.plu.inf.iN} \textbf{(c)} is therefore proven.

\begin{definition}
\label{def.plu.inf.jmN}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. We define a
linear map $j_{N}^{\left(  m\right)  }:\wedge^{N+m+1}\left(  V_{N}\right)
\rightarrow\mathcal{F}^{\left(  m\right)  }$ by setting%
\[
\left(
\begin{array}
[c]{r}%
j_{N}^{\left(  m\right)  }\left(  a_{0}\wedge a_{1}\wedge...\wedge
a_{N+m}\right)  =a_{0}\wedge a_{1}\wedge...\wedge a_{N+m}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\\
\text{for any }a_{0},a_{1},...,a_{N+m}\in V_{N}%
\end{array}
\right)  .
\]
This map $j_{N}^{\left(  m\right)  }$ is well-defined (because $a_{0}\wedge
a_{1}\wedge...\wedge a_{N+m}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...$ is easily seen to lie in $\mathcal{F}^{\left(  m\right)  }$
and depend multilinearly and antisymmetrically on $a_{0},a_{1},...,a_{N+m}$)
and injective (because the elements of the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}%
>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $ are sent by
$j_{N}^{\left(  m\right)  }$ to pairwise distinct elements of the basis
$\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(
i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ of
$\mathcal{F}^{\left(  m\right)  }$).
\end{definition}

These definitions satisfy reasonable compatibilities:

\begin{proposition}
\label{prop.plu.inf.iNjmN}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. For any
$u\in\wedge^{N+m+1}\left(  V_{N}\right)  $ and $A\in\operatorname*{M}\left(
V_{N}\right)  $, we have%
\[
i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)  }\left(  u\right)
=j_{N}^{\left(  m\right)  }\left(  Au\right)  .
\]
(Here, of course, $i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)
}\left(  u\right)  $ stands for $\left(  \varrho\left(  i_{N}\left(  A\right)
\right)  \right)  \left(  j_{N}^{\left(  m\right)  }\left(  u\right)  \right)
$.)
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.inf.iNjmN}.} Let $A\in
\operatorname*{M}\left(  V_{N}\right)  $ and $u\in\wedge^{N+m+1}\left(
V_{N}\right)  $. We must prove the equality $i_{N}\left(  A\right)  \cdot
j_{N}^{\left(  m\right)  }\left(  u\right)  =j_{N}^{\left(  m\right)  }\left(
Au\right)  $. Since this equality is linear in $u$, we can WLOG assume that
$u$ is an element of the basis $\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of
$\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume this. Then, there exists an
$N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $ of integers such that
$N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

By the definition of $i_{N}\left(  A\right)  $, we have
\begin{equation}
\left(  i_{N}\left(  A\right)  \cdot v_{k}=Av_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }k\in\left\{  -N,-N+1,...,N\right\}  \right)
\label{pf.plu.inf.iNjmN.in}%
\end{equation}
and
\begin{equation}
\left(  i_{N}\left(  A\right)  \cdot v_{k}=v_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }k\in\mathbb{Z}\diagdown\left\{  -N,-N+1,...,N\right\}  \right)  .
\label{pf.plu.inf.iNjmN.out}%
\end{equation}


Note that every $\ell\in\left\{  0,1,...,N+m\right\}  $ satisfies $i_{\ell}%
\in\left\{  -N,-N+1,...,N\right\}  $ (since $N\geq i_{0}>i_{1}>...>i_{N+m}%
\geq-N$ and thus $N\geq i_{\ell}\geq-N$) and thus
\begin{equation}
i_{N}\left(  A\right)  \cdot v_{i_{\ell}}=Av_{i_{\ell}}
\label{pf.plu.inf.iNjmN.in2}%
\end{equation}
(by (\ref{pf.plu.inf.iNjmN.in}), applied to $k=i_{\ell}$). Also, every
positive integer $r$ satisfies $-N-r\in\mathbb{Z}\diagdown\left\{
-N,-N+1,...,N\right\}  $ and thus%
\begin{equation}
i_{N}\left(  A\right)  \cdot v_{-N-r}=v_{-N-r} \label{pf.plu.inf.iNjmN.out2}%
\end{equation}
(by (\ref{pf.plu.inf.iNjmN.out}), applied to $k=-N-r$).

Now, since $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we have%
\begin{align*}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...
\end{align*}
(by the definition of $j_{N}^{\left(  m\right)  }$), so that%
\begin{align}
&  i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)  }\left(  u\right)
\nonumber\\
&  =i_{N}\left(  A\right)  \cdot\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}%
\wedge...\right) \nonumber\\
&  =\underbrace{i_{N}\left(  A\right)  \cdot v_{i_{0}}\wedge i_{N}\left(
A\right)  \cdot v_{i_{1}}\wedge...\wedge i_{N}\left(  A\right)  \cdot
v_{i_{N+m}}}_{\substack{=Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge
Av_{i_{N+m}}\\\text{(because every }\ell\in\left\{  0,1,...,N+m\right\}
\text{ satisfies }i_{N}\left(  A\right)  \cdot v_{i_{\ell}}=Av_{i_{\ell}%
}\text{ (by (\ref{pf.plu.inf.iNjmN.in2})))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \wedge\underbrace{i_{N}\left(  A\right)  \cdot
v_{-N-1}\wedge i_{N}\left(  A\right)  \cdot v_{-N-2}\wedge i_{N}\left(
A\right)  \cdot v_{-N-3}\wedge...}_{\substack{=v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(because every positive integer }r\text{ satisfies
}i_{N}\left(  A\right)  \cdot v_{-N-r}=v_{-N-r}\text{ (by
(\ref{pf.plu.inf.iNjmN.out2})))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the action }%
\varrho:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}^{\left(  m\right)  }\right)  \right) \nonumber\\
&  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge.... \label{pf.plu.inf.iNjmN.left}%
\end{align}
On the other hand, since $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}$, we have $Au=Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge
Av_{i_{N+m}}$, so that%
\begin{align*}
j_{N}^{\left(  m\right)  }\left(  Au\right)   &  =j_{N}^{\left(  m\right)
}\left(  Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\right) \\
&  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...
\end{align*}
(by the definition of $j_{N}^{\left(  m\right)  }$). Compared with
(\ref{pf.plu.inf.iNjmN.left}), this yields $i_{N}\left(  A\right)  \cdot
j_{N}^{\left(  m\right)  }\left(  u\right)  =j_{N}^{\left(  m\right)  }\left(
Au\right)  $. This proves Proposition \ref{prop.plu.inf.iNjmN}.

An important property of the maps $j_{N}^{\left(  m\right)  }$ is that their
images (for fixed $m$ and varying $N$) cover (not just span, but actually
cover) all of $\mathcal{F}^{\left(  m\right)  }$:

\begin{proposition}
\label{prop.plu.inf.cover}Let $m\in\mathbb{Z}$.

\textbf{(a)} We have $j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(
V_{0}\right)  \right)  \subseteq j_{1}^{\left(  m\right)  }\left(
\wedge^{1+m+1}\left(  V_{1}\right)  \right)  \subseteq j_{2}^{\left(
m\right)  }\left(  \wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq...$.

\textbf{(b)} For every $Q\in\mathbb{N}$, we have $\mathcal{F}^{\left(
m\right)  }=\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  $.
\end{proposition}

Actually, the "$N\geq Q$" in Proposition \ref{prop.plu.inf.cover} \textbf{(b)}
doesn't have much effect since Proposition \ref{prop.plu.inf.cover}
\textbf{(a)} yields $\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}%
}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)
\right)  =\bigcup\limits_{N\in\mathbb{N}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $; but we prefer to put it in
because it is needed in our application.

\textit{Proof of Proposition \ref{prop.plu.inf.cover}.} \textbf{(a)} Let
$N\in\mathbb{N}$. From the definitions of $j_{N}$ and $j_{N+1}$, it is easy to
see that%
\[
j_{N}^{\left(  m\right)  }\left(  a_{0}\wedge a_{1}\wedge...\wedge
a_{N+m}\right)  =j_{N+1}^{\left(  m\right)  }\left(  a_{0}\wedge a_{1}%
\wedge...\wedge a_{N+m}\wedge v_{-N-1}\right)
\]
for any $a_{0},a_{1},...,a_{N+m}\in V_{N}$. Due to linearity, this yields that
$j_{N}^{\left(  m\right)  }\left(  a\right)  =j_{N+1}^{\left(  m\right)
}\left(  a\wedge v_{-N-1}\right)  $ for any $a\in\wedge^{N+m+1}\left(
V_{N}\right)  $. Hence, $j_{N}^{\left(  m\right)  }\left(  a\right)
=j_{N+1}^{\left(  m\right)  }\left(  a\wedge v_{-N-1}\right)  \in
j_{N+1}^{\left(  m\right)  }\left(  \wedge^{\left(  N+1\right)  +m+1}\left(
V_{N+1}\right)  \right)  $ for any $a\in\wedge^{N+m+1}\left(  V_{N}\right)  $.
In other words, $j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  \subseteq j_{N+1}^{\left(  m\right)  }\left(
\wedge^{\left(  N+1\right)  +m+1}\left(  V_{N+1}\right)  \right)  $.

We thus have proven that every $N\in\mathbb{N}$ satisfies $j_{N}^{\left(
m\right)  }\left(  V_{N}\right)  \subseteq j_{N+1}^{\left(  m\right)  }\left(
\wedge^{\left(  N+1\right)  +m+1}\left(  V_{N+1}\right)  \right)  $. In other
words, $j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)
\right)  \subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq...$. Proposition
\ref{prop.plu.inf.cover} \textbf{(a)} is proven.

\textbf{(b)} We need three notations:

\begin{itemize}
\item For any $m$-degression $\mathbf{i}$, define a nonnegative integer
$\operatorname*{exting}\left(  \mathbf{i}\right)  $ as the largest
$k\in\mathbb{N}$ satisfying $i_{k}+k\neq m$\ \ \ \ \footnote{If no such $k$
exists, then we set $\operatorname*{exting}\left(  \mathbf{i}\right)  $ to be
$0$.}, where $\mathbf{i}$ is written in the form $\left(  i_{0},i_{1}%
,i_{2},...\right)  $. (Such a largest $k$ indeed exists, because (by the
definition of an $m$-degression) every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$.)

\item For any $m$-degression $\mathbf{i}$, define an integer
$\operatorname*{head}\left(  \mathbf{i}\right)  $ by $\operatorname*{head}%
\left(  \mathbf{i}\right)  =i_{0}$, where $\mathbf{i}$ is written in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $.

\item For any $m$-degression $\mathbf{i}$, define an element $v_{\mathbf{i}}$
of $\mathcal{F}^{\left(  m\right)  }$ by $v_{\mathbf{i}}=v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$, where $\mathbf{i}$ is written in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{itemize}

Thus, $\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }%
m\text{-degression}}=\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression}}$. Since \newline$\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is
an }m\text{-degression}}$ is a basis of the vector space $\mathcal{F}^{\left(
m\right)  }$, we thus conclude that $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\text{ is an }m\text{-degression}}$ is a basis of the vector space
$\mathcal{F}^{\left(  m\right)  }$.

Now we prove a simple fact:%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{If }\mathbf{i}\text{ is an }m\text{-degression, and }P\text{ is an
integer such that }\\
P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \text{, then
}v_{\mathbf{i}}\in j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)
\end{array}
\right)  . \label{pf.plu.inf.cover.1}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.cover.1}):} Let $\mathbf{i}$ be an
$m$-degression, and $P$ be an integer such that $P\geq\max\left\{
0,\operatorname*{exting}\left(  \mathbf{i}\right)  -m,\operatorname*{head}%
\left(  \mathbf{i}\right)  \right\}  $. Write $\mathbf{i}$ in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $. Then, $\operatorname*{exting}\left(
\mathbf{i}\right)  $ is the largest $k\in\mathbb{N}$ satisfying $i_{k}+k\neq
m$ (by the definition of $\operatorname*{exting}\left(  \mathbf{i}\right)  $).
Hence,%
\begin{equation}
\text{every }k\in\mathbb{N}\text{ such that }k>\operatorname*{exting}\left(
\mathbf{i}\right)  \text{ satisfies }i_{k}+k=m. \label{pf.plu.inf.cover.2}%
\end{equation}


Since $P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \geq0$, the map
$j_{P}^{\left(  m\right)  }$ and the space $V_{P}$ are well-defined.

Since $P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \geq
\operatorname*{exting}\left(  \mathbf{i}\right)  -m$, we have $P+m\geq
\operatorname*{exting}\left(  \mathbf{i}\right)  \geq0$. Now,%
\begin{equation}
\text{every positive integer }\ell\text{ satisfies }i_{P+m+\ell}%
=-P-\ell\label{pf.plu.inf.cover.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.cover.3}):} Let $\ell\in
\mathbb{N}$ be a positive integer. Then, $P+m+\underbrace{\ell}_{>0}%
>P+m\geq\operatorname*{exting}\left(  \mathbf{i}\right)  $. Hence,
(\ref{pf.plu.inf.cover.2}) (applied to $k=P+m+\ell$) yields $i_{P+m+\ell
}+P+m+\ell=m$. In other words, $i_{P+m+\ell}=-P-\ell$. This proves
(\ref{pf.plu.inf.cover.3}).}. Applied to $\ell=1$, this yields $i_{P+m+1}%
=-P-1$.

Notice also that $P\geq\max\left\{  0,\operatorname*{exting}\left(
\mathbf{i}\right)  -m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}
\geq\operatorname*{head}\left(  \mathbf{i}\right)  =i_{0}$ (by the definition
of $\operatorname*{head}\left(  \mathbf{i}\right)  $). Now it is easy to see
that%
\begin{equation}
\text{every }k\in\mathbb{N}\text{ such that }k\leq P+m\text{ satisfies
}v_{i_{k}}\in V_{P}. \label{pf.plu.inf.cover.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.cover.4}):} Let $k\in\mathbb{N}$
be such that $k\leq P+m$. Thus, $k<P+m+1$.
\par
Since $\left(  i_{0},i_{1},i_{2},...\right)  =\mathbf{i}$ is an $m$%
-degression, the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ is strictly
decreasing, i. e., we have $i_{0}>i_{1}>i_{2}>...$. As a consequence,
$i_{0}\geq i_{k}$ (since $0\leq k$) and $i_{k}>i_{P+m+1}$ (since $k<P+m+1$).
Since $i_{k}>i_{P+m+1}=-P-1$, we have $i_{k}\geq-P$ (since both $i_{k}$ and
$-P$ are integers). Combining $P\geq i_{0}\geq i_{k}$ with $i_{k}\geq-P$, we
obtain $P\geq i_{k}\geq-P$. Hence, $v_{i_{k}}\in\left\langle v_{-P}%
,v_{-P+1},...,v_{P}\right\rangle =V_{P}$ (because $V_{P}$ is defined as
$\left\langle v_{-P},v_{-P+1},...,v_{P}\right\rangle $). This proves
(\ref{pf.plu.inf.cover.4}).} Hence, $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{P+m}}\in\wedge^{P+m+1}\left(  V_{P}\right)  $. Now, by the definition of
$j_{P}^{\left(  m\right)  }$, we have%
\begin{align*}
&  j_{P}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{P+m}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\wedge
\underbrace{v_{-P-1}\wedge v_{-P-2}\wedge v_{-P-3}\wedge...}%
_{\substack{=v_{i_{P+m+1}}\wedge v_{i_{P+m+2}}\wedge v_{i_{P+m+3}}%
\wedge...\\\text{(because every positive integer }\ell\\\text{satisfies
}-P-\ell=i_{P+m+\ell}\text{ (by (\ref{pf.plu.inf.cover.3})))}}}\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\wedge v_{i_{P+m+1}%
}\wedge v_{i_{P+m+2}}\wedge v_{i_{P+m+3}}\wedge...=v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...=v_{\mathbf{i}}%
\end{align*}
(since $v_{\mathbf{i}}$ was defined as $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$). Thus, $v_{\mathbf{i}}=j_{P}^{\left(  m\right)  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\right)  \in
j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(  V_{P}\right)  \right)
$. This proves (\ref{pf.plu.inf.cover.1}).

Now, fix an arbitrary $Q\in\mathbb{N}$.

Let $w$ be any element of $\mathcal{F}^{\left(  m\right)  }$. Since $\left(
v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }m\text{-degression}}$ is a
basis of $\mathcal{F}^{\left(  m\right)  }$, we can write $w$ as a linear
combination of elements of the family $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\text{ is an }m\text{-degression}}$. Since every linear
combination contains only finitely many vectors, this yields that we can write
$w$ as a linear combination of \textbf{finitely many} elements of the family
$\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }m\text{-degression}%
}$. In other words, there exists a finite set $S$ of $m$-degressions such that
$w$ is a linear combination of the family $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\in S}$. Consider this $S$. Since $w$ is a linear combination of
the family $\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\in S}$, we can find a
scalar $\lambda_{\mathbf{i}}\in\mathbb{C}$ for every $\mathbf{i}\in S$ such
that $w=\sum\limits_{\mathbf{i}\in S}\lambda_{\mathbf{i}}v_{\mathbf{i}}$.
Consider these scalars $\lambda_{\mathbf{i}}$. Let
\[
P=\max\left\{  Q,\max\left\{  \max\left\{  0,\operatorname*{exting}\left(
\mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}
\ \mid\ \mathbf{j}\in S\right\}  \right\}
\]
(where the maximum of the empty set is to be understood as $0$). Then, first
of all, $P\geq Q$. Second, every $\mathbf{i}\in S$ satisfies%
\begin{align*}
P  &  =\max\left\{  Q,\max\left\{  \max\left\{  0,\operatorname*{exting}%
\left(  \mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)
\right\}  \ \mid\ \mathbf{j}\in S\right\}  \right\} \\
&  \geq\max\left\{  \max\left\{  0,\operatorname*{exting}\left(
\mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}
\ \mid\ \mathbf{j}\in S\right\} \\
&  \geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \text{ is an
element of the set}\\
\left\{  \max\left\{  0,\operatorname*{exting}\left(  \mathbf{j}\right)
-m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}  \ \mid
\ \mathbf{j}\in S\right\}  \text{ (because }\mathbf{i}\in S\text{),}\\
\text{and the maximum of a set is }\geq\text{ to any element of this set}%
\end{array}
\right)
\end{align*}
and thus $v_{\mathbf{i}}\in j_{P}^{\left(  m\right)  }\left(  \wedge
^{P+m+1}\left(  V_{P}\right)  \right)  $ (by (\ref{pf.plu.inf.cover.1})).
Hence,%
\begin{align*}
w  &  =\sum\limits_{\mathbf{i}\in S}\lambda_{\mathbf{i}}%
\underbrace{v_{\mathbf{i}}}_{\in j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  }\in\sum\limits_{\mathbf{i}\in
S}\lambda_{\mathbf{i}}j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  \subseteq j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  \text{ is a vector space}\right)
\\
&  \subseteq\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\geq Q\right)  .
\end{align*}


Now, forget that we fixed $w$. We thus have proven that every $w\in
\mathcal{F}^{\left(  m\right)  }$ satisfies $w\in\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $. Thus, $\mathcal{F}^{\left(
m\right)  }\subseteq\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}%
}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)
\right)  $. Combined with the obvious inclusion $\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  \subseteq\mathcal{F}^{\left(
m\right)  }$, this yields $\mathcal{F}^{\left(  m\right)  }=\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $. Proposition
\ref{prop.plu.inf.cover} \textbf{(b)} is thus proven.

\begin{verlong}
And a corollary of Proposition \ref{prop.plu.inf.cover} (that we won't need):

\begin{corollary}
\label{cor.plu.inf.cover.tensor}Let $m\in\mathbb{Z}$. We have $\mathcal{F}%
^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }=\bigcup
\limits_{N\in\mathbb{N}}\left(  j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  \otimes j_{N}^{\left(  m\right)
}\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  \right)  $.
\end{corollary}

To prove this, we need the following lemma:

\begin{lemma}
\label{lem.plu.inf.cover.tensor}Let $W$ be a vector space, and $\left(
W_{n}\right)  _{n\in\mathbb{N}}$ a family of vector subspaces of $W$ such that
$W_{0}\subseteq W_{1}\subseteq W_{2}\subseteq...$ and $W=\bigcup
\limits_{n\in\mathbb{N}}W_{n}$. Let $U$ be a vector space, and $\left(
U_{n}\right)  _{n\in\mathbb{N}}$ a family of vector subspaces of $U$ such that
$U_{0}\subseteq U_{1}\subseteq U_{2}\subseteq...$ and $U=\bigcup
\limits_{n\in\mathbb{N}}U_{n}$. Then, $U\otimes W=\bigcup\limits_{n\in
\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.cover.tensor}.} Let $t\in U\otimes W$
be arbitrary. Since $t$ is a tensor, we can write $t$ in the form
$t=\sum\limits_{i=1}^{m}u_{i}\otimes w_{i}$ for some $m\in\mathbb{N}$, some
elements $u_{1}$, $u_{2}$, $...$, $u_{m}$ of $U$, and some elements $w_{1}$,
$w_{2}$, $...$, $w_{m}$ of $W$. Consider this $u$, these $u_{1}$, $u_{2}$,
$...$, $u_{m}$ and these $w_{1}$, $w_{2}$, $...$, $w_{m}$.

For every $i\in\left\{  1,2,...,m\right\}  $, there exists some $\alpha_{i}%
\in\mathbb{N}$ such that $u_{i}\in U_{\alpha_{i}}$ (since $u_{i}\in
U=\bigcup\limits_{n\in\mathbb{N}}U_{n}$). Consider this $\alpha_{i}$.

For every $i\in\left\{  1,2,...,m\right\}  $, there exists some $\beta_{i}%
\in\mathbb{N}$ such that $w_{i}\in W_{\beta_{i}}$ (since $w_{i}\in
W=\bigcup\limits_{n\in\mathbb{N}}W_{n}$). Consider this $\beta_{i}$.

Let $N=\max\left(  \left\{  \alpha_{1},\alpha_{2},...,\alpha_{m}\right\}
\cup\left\{  \beta_{1},\beta_{2},...,\beta_{m}\right\}  \right)  $. Then,
every $i\in\left\{  1,2,...,m\right\}  $ satisfies $\alpha_{i}\in\left\{
\alpha_{1},\alpha_{2},...,\alpha_{m}\right\}  \subseteq\left\{  \alpha
_{1},\alpha_{2},...,\alpha_{m}\right\}  \cup\left\{  \beta_{1},\beta
_{2},...,\beta_{m}\right\}  $, so that%
\begin{align*}
\alpha_{i}  &  \leq\max\left(  \left\{  \alpha_{1},\alpha_{2},...,\alpha
_{m}\right\}  \cup\left\{  \beta_{1},\beta_{2},...,\beta_{m}\right\}  \right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since any element of a finite set is
}\leq\text{ to the maximum of this set}\right) \\
&  =N
\end{align*}
and thus $U_{\alpha_{i}}\subseteq U_{N}$ (since $U_{0}\subseteq U_{1}\subseteq
U_{2}\subseteq...$), so that $u_{i}\in U_{\alpha_{i}}\subseteq U_{N}$.
Similarly, every $i\in\left\{  1,2,...,m\right\}  $ satisfies $w_{i}\in W_{N}%
$. Thus,%
\begin{align*}
t  &  =\sum\limits_{i=1}^{m}\underbrace{u_{i}}_{\in U_{N}}\otimes
\underbrace{w_{i}}_{\in W_{N}}\in\sum\limits_{i=1}^{m}U_{N}\otimes
W_{N}\subseteq U_{N}\otimes W_{N}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U_{N}\otimes W_{N}\text{ is a
}k\text{-vector space}\right) \\
&  \subseteq\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)
.
\end{align*}


Now, forget that we fixed $t$. We thus have proven that every $t\in U\otimes
W$ satisfies $t\in\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes
W_{n}\right)  $. In other words, $U\otimes W\subseteq\bigcup\limits_{n\in
\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)  $. Combined with the obvious
inclusion $\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)
\subseteq U\otimes W$, this yields $U\otimes W=\bigcup\limits_{n\in\mathbb{N}%
}\left(  U_{n}\otimes W_{n}\right)  $, so that Lemma
\ref{lem.plu.inf.cover.tensor} is proven.

\textit{Proof of Corollary \ref{cor.plu.inf.cover.tensor}.} Proposition
\ref{prop.plu.inf.cover} \textbf{(b)} (applied to $Q=0$) yields
\[
\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in\mathbb{N}%
;\\N\geq0}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  .
\]
Proposition \ref{prop.plu.inf.cover} \textbf{(a)} yields $j_{0}^{\left(
m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)  \subseteq
j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(  V_{1}\right)  \right)
\subseteq j_{2}^{\left(  m\right)  }\left(  \wedge^{2+m+1}\left(
V_{2}\right)  \right)  \subseteq...$. Thus, Lemma
\ref{lem.plu.inf.cover.tensor} (applied to $W=\mathcal{F}^{\left(  m\right)
}$, $W_{i}=j_{i}^{\left(  m\right)  }\left(  \wedge^{i+m+1}\left(
V_{1}\right)  \right)  $, $U=\mathcal{F}^{\left(  m\right)  }$ and
$U_{i}=j_{i}^{\left(  m\right)  }\left(  \wedge^{i+m+1}\left(  V_{1}\right)
\right)  $) yields $\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}%
^{\left(  m\right)  }=\bigcup\limits_{N\in\mathbb{N}}\left(  j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  \otimes
j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\right)  $. This proves Corollary \ref{cor.plu.inf.cover.tensor}.
\end{verlong}

What comes next is almost a carbon copy of Definition
\ref{def.createdestroy.fin}:

\begin{definition}
\label{def.plu.inf.createdestroy}Let $N\in\mathbb{N}$. Let $k\in\mathbb{Z}$.
Let $i\in\left\{  -N,-N+1,...,N\right\}  $.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}^{\left(  N\right)  }}:\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k+1}\left(  V_{N}\right)  $ by%
\[
\widehat{v_{i}^{\left(  N\right)  }}\cdot\psi=v_{i}\wedge\psi
\ \ \ \ \ \ \ \ \ \ \text{for all }\psi\in\wedge^{k}\left(  V_{N}\right)  .
\]


\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}^{\left(  N\right)  }}:\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k-1}\left(  V_{N}\right)  $ as follows:

For every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $N\geq i_{1}>i_{2}>...>i_{k}\geq-N$, we let $\overset{\vee
}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge
v_{i_{k}}\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}
;\\
\left(  -1\right)  ^{j-1}v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{j-1}%
}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{1},i_{2},...,i_{k}\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell}=i$. Thus, the map
$\overset{\vee}{v_{i}^{\left(  N\right)  }}$ is defined on a basis of the
vector space $\wedge^{k}\left(  V_{N}\right)  $; we extend this to a map
$\wedge^{k}\left(  V_{N}\right)  \rightarrow\wedge^{k-1}\left(  V_{N}\right)
$ by linearity.

Note that, for every negative $\ell\in\mathbb{Z}$, we understand $\wedge
^{\ell}\left(  V_{N}\right)  $ to mean the zero space.
\end{definition}

Also:

\begin{definition}
For every $N\in\mathbb{N}$ and $k\in\left\{  1,2,...,2N+1\right\}  $, let
$\Omega_{N}^{\left(  k\right)  }$ denote the orbit of $v_{N}\wedge
v_{N-1}\wedge...\wedge v_{N-k+1}$ under the action of $\operatorname*{GL}%
\left(  V_{N}\right)  $.
\end{definition}

The following lemma, then, is an easy corollary of Theorem \ref{thm.plu}:

\begin{lemma}
\label{lem.plu.inf.plu}Let $N\in\mathbb{N}$ and $k\in\mathbb{Z}$. Let
$S_{N}^{\left(  k\right)  }=\sum\limits_{i=-N}^{N}\widehat{v_{i}^{\left(
N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}:\wedge
^{k}\left(  V_{N}\right)  \otimes\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k+1}\left(  V_{N}\right)  \otimes\wedge^{k-1}\left(
V_{N}\right)  $.

\textbf{(a)} This map $S_{N}^{\left(  k\right)  }$ does not depend on the
choice of the basis of $V_{N}$, and is $\operatorname*{GL}\left(
V_{N}\right)  $-invariant. In other words, for \textbf{any} basis $\left(
w_{N},w_{N-1},...,w_{-N}\right)  $ of $V_{N}$, we have $S_{N}^{\left(
k\right)  }=\sum\limits_{i=-N}^{N}\widehat{w_{i}^{\left(  N\right)  }}%
\otimes\overset{\vee}{w_{i}^{\left(  N\right)  }}$ (where the maps
$\widehat{w_{i}^{\left(  N\right)  }}$ and $\overset{\vee}{w_{i}^{\left(
N\right)  }}$ are defined just as $\widehat{v_{i}^{\left(  N\right)  }}$ and
$\overset{\vee}{v_{i}^{\left(  N\right)  }}$, but with respect to the basis
$\left(  w_{N},w_{N-1},...,w_{-N}\right)  $).

\textbf{(b)} Let $k\in\left\{  1,2,...,2N+1\right\}  $. A nonzero element
$\tau\in\wedge^{k}\left(  V_{N}\right)  $ belongs to $\Omega_{N}^{\left(
k\right)  }$ if and only if $S_{N}^{\left(  k\right)  }\left(  \tau\otimes
\tau\right)  =0$.

\textbf{(c)} The map $S_{N}^{\left(  k\right)  }$ is $\operatorname*{M}\left(
V_{N}\right)  $-invariant.
\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.plu}.} If we set $n=2N+1$ in Theorem
\ref{thm.plu}, and do the following renaming operations:

\begin{itemize}
\item rename the standard basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ as
$\left(  v_{N},v_{N-1},...,v_{-N}\right)  $;

\item rename the vector space $V$ as $V_{N}$;

\item rename the map $S$ as $S_{N}^{\left(  k\right)  }$;

\item rename the basis $\left(  w_{1},w_{2},...,w_{n}\right)  $ as $\left(
w_{N},w_{N-1},...,w_{-N}\right)  $;

\item rename the maps $\widehat{v_{i}}$ as $\widehat{v_{i}^{\left(  N\right)
}}$;

\item rename the maps $\overset{\vee}{v_{i}}$ as $\overset{\vee}{v_{i}%
^{\left(  N\right)  }}$;

\item rename the maps $\widehat{w_{i}}$ as $\widehat{w_{i}^{\left(  N\right)
}}$;

\item rename the maps $\overset{\vee}{w_{i}}$ as $\overset{\vee}{w_{i}%
^{\left(  N\right)  }}$;

\item rename the set $\Omega$ as $\Omega_{N}^{\left(  k\right)  }$;
\end{itemize}

then what we obtain is exactly the statement of Lemma \ref{lem.plu.inf.plu}.
Thus, Lemma \ref{lem.plu.inf.plu} is proven.

The maps $S_{N}^{\left(  k\right)  }$ have their own compatibility relation
with the $j_{N}^{\left(  m\right)  }$:

\begin{lemma}
\label{lem.plu.inf.S.comp}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. Define
the notation $S_{N}^{\left(  N+m+1\right)  }$ as in Lemma
\ref{lem.plu.inf.plu}. Then,%
\[
\left(  j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)
}\right)  \circ S_{N}^{\left(  N+m+1\right)  }=S\circ\left(  j_{N}^{\left(
m\right)  }\otimes j_{N}^{\left(  m\right)  }\right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.S.comp}.} Define the maps
$\widehat{v_{i}^{\left(  N\right)  }}$ and $\overset{\vee}{v_{i}^{\left(
N\right)  }}$ (for all $i\in\left\{  -N,-N+1,...,N\right\}  $) as in
Definition \ref{def.plu.inf.createdestroy}. Define the maps $\widehat{v_{i}}$
and $\overset{\vee}{v_{i}}$ (for all $i\in\mathbb{Z}$) as in Definition
\ref{def.createdestroy}.

\textbf{a)} Let us first show that%
\begin{equation}
j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }%
}=\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left\{  N,N-1,...,-N\right\}  . \label{pf.plu.inf.S.comp.a}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.a}):} Let $i\in\left\{
N,N-1,...,-N\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.a}), it is
clearly enough to show that $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\widehat{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $
for every $u\in\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}%
^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $. Since this equality is
linear in $u$, we can WLOG assume that $u$ is an element of the basis $\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq
i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $.
Assume this. Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1}%
,...,i_{N+m}\right)  $ of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}%
\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider
this $N+m+1$-tuple.

Comparing%
\begin{align*}
\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }%
}\right)  \left(  u\right)   &  =j_{N}^{\left(  m+1\right)  }%
\underbrace{\left(  \widehat{v_{i}^{\left(  N\right)  }}\left(  u\right)
\right)  }_{\substack{=v_{i}\wedge u\\\text{(by the definition of
}\widehat{v_{i}^{\left(  N\right)  }}\text{)}}}=j_{N}^{\left(  m+1\right)
}\left(  v_{i}\wedge\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}}\right) \\
&  =j_{N}^{\left(  m+1\right)  }\left(  v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m+1\right)  }\right)
\end{align*}
with%
\begin{align*}
\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\widehat{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =v_{i}\wedge j_{N}^{\left(  m\right)  }\left(
\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{v_{i}}\right)
\\
&  =v_{i}\wedge\underbrace{j_{N}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  }_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(by the definition of }j_{N}^{\left(  m\right)
}\text{)}}}\\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...,
\end{align*}
we obtain $\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right)  =\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $. This is exactly what
we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.a}). The proof of (\ref{pf.plu.inf.S.comp.a}) is thus finished.

\textbf{b)} Let us next show that%
\begin{equation}
j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(  N\right)  }%
}=\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  N,N-1,...,-N\right\}  .
\label{pf.plu.inf.S.comp.b}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.b}):} Let $i\in\left\{
N,N-1,...,-N\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.b}), it is
clearly enough to show that $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)
=\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $ for every $u\in\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $. Since this equality is linear in $u$, we can WLOG assume that $u$
is an element of the basis $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge
^{N+m+1}\left(  V_{N}\right)  $. Assume this. Then, there exists an
$N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $ of integers such that
$N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be the sequence $\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  $. From $u=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m\right)  }\right) \nonumber\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\label{pf.plu.inf.S.comp.b.0}%
\end{align}


We distinguish between two cases:

\textit{Case 1:} We have $i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $.

\textit{Case 2:} We have $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $.

Let us first consider Case 1. In this case, from $u=v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align*}
&  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right) \\
&  =\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},...,i_{N+m}%
\right\}  ;\\
\left(  -1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\left(  j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge
v_{i_{\left(  j-1\right)  +2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$.\ \ \ \ \footnote{If you
are wondering where the $-1$ (for example, in $i_{\ell-1}$ and in $i_{\left(
j-1\right)  -1}$) comes from: It comes from the fact that the indexing of our
$N+m+1$-tuple $\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{N+m}}\right)  $ begins
with $0$, and not with $1$ as in Definition \ref{def.createdestroy.fin}.}
Since $i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $ (because we are in
Case 1), this simplifies to%
\[
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)  =0.
\]


On the other hand, combining $i\notin\left\{  -N-1,-N-2,-N-3,...\right\}  $
(which is because $i\in\left\{  N,N-1,...,-N\right\}  $) with $i\notin\left\{
i_{0},i_{1},...,i_{N+m}\right\}  $ (which is because we are in Case 1), we
obtain%
\begin{align*}
i  &  \notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  \cup\left\{
-N-1,-N-2,-N-3,...\right\} \\
&  =\left\{  i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right\}  =\left\{
j_{0},j_{1},j_{2},...\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\end{align*}
Now,%
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.b.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\notin\left\{
j_{0},j_{1},j_{2},...\right\}  $, this simplifies to%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =0.
\]
Compared with%
\[
\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right)  =j_{N}^{\left(  m+1\right)
}\underbrace{\left(  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(
u\right)  \right)  }_{=0}=0,
\]
this yields $\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $. We have thus proven $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)
=\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $ in Case 1.

Next, let us consider Case 2. In this case, $i\in\left\{  i_{0},i_{1}%
,...,i_{N+m}\right\}  $, so there exists an $\ell\in\left\{
0,1,...,N+m\right\}  $ such that $i_{\ell}=i$. Denote this $\ell$ by $\kappa$.
Then, $i_{\kappa}=i$. Clearly,
\begin{align}
&  \left(  i_{0},i_{1},...,i_{\kappa-1},i_{\kappa+1},i_{\kappa+2}%
,...,i_{N+m},-N-1,-N-2,-N-3,...\right) \nonumber\\
&  =\left(  \phantom{\underbrace{I}_{I}}\text{result of removing the }%
\kappa+1\text{-th term from the sequence}\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  \underbrace{\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  }_{=\left(  j_{0}%
,j_{1},j_{2},...\right)  }\right) \nonumber\\
&  =\left(  \text{result of removing the }\kappa+1\text{-th term from the
sequence }\left(  j_{0},j_{1},j_{2},...\right)  \right) \nonumber\\
&  =\left(  j_{0},j_{1},...,j_{\kappa-1},j_{\kappa+1},j_{\kappa+2},...\right)
. \label{pf.plu.inf.S.comp.b.2.triv}%
\end{align}


From $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align*}
&  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right) \\
&  =\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},...,i_{N+m}%
\right\}  ;\\
\left(  -1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\left(  j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge
v_{i_{\left(  j-1\right)  +2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$.\ \ \ \ \footnote{If you
are wondering where the $-1$ (for example, in $i_{\ell-1}$ and in $i_{\left(
j-1\right)  -1}$) comes from: It comes from the fact that the indexing of our
$N+m+1$-tuple $\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{N+m}}\right)  $ begins
with $0$, and not with $1$ as in Definition \ref{def.createdestroy.fin}.}
Since $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, and since the integer
$\ell$ satisfying $i_{\ell-1}=i$ is $\kappa+1$ (because $i_{\left(
\kappa+1\right)  -1}=i_{\kappa}=i$), this simplifies to%
\begin{align*}
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)   &  =\left(
-1\right)  ^{\left(  \kappa+1\right)  -1}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{\left(  \left(  \kappa+1\right)  -1\right)  -1}}\wedge
v_{i_{\left(  \left(  \kappa+1\right)  -1\right)  +1}}\wedge v_{i_{\left(
\left(  \kappa+1\right)  -1\right)  +2}}\wedge...\wedge v_{i_{k}}\\
&  =\left(  -1\right)  ^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge
v_{i_{k}}%
\end{align*}
(since $\left(  \kappa+1\right)  -1=\kappa$). Thus,%
\begin{align}
&  \left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right) \nonumber\\
&  =j_{N}^{\left(  m+1\right)  }\left(  \underbrace{\overset{\vee
}{v_{i}^{\left(  N\right)  }}\left(  u\right)  }_{=\left(  -1\right)
^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\kappa-1}}\wedge
v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge v_{i_{k}}}\right)
\nonumber\\
&  =j_{N}^{\left(  m+1\right)  }\left(  \left(  -1\right)  ^{\kappa}v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge
v_{i_{\kappa+2}}\wedge...\wedge v_{i_{k}}\right) \nonumber\\
&  =\left(  -1\right)  ^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge
v_{i_{k}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m+1\right)  }\right) \nonumber\\
&  =\left(  -1\right)  ^{\kappa}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\wedge v_{j_{\kappa-1}}\wedge v_{j_{\kappa+1}}\wedge v_{j_{\kappa
+2}}\wedge...\label{pf.plu.inf.S.comp.b.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{pf.plu.inf.S.comp.b.2.triv}) yields}\\
\left(  i_{0},i_{1},...,i_{\kappa-1},i_{\kappa+1},i_{\kappa+2},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right) \\
=\left(  j_{0},j_{1},...,j_{\kappa-1},j_{\kappa+1},j_{\kappa+2},...\right)
\end{array}
\right)  .\nonumber
\end{align}


On the other hand,
\begin{align*}
i  &  \in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  \subseteq\left\{
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right\}  =\left\{  j_{0}%
,j_{1},j_{2},...\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\end{align*}
Moreover, the integer $k$ satisfying $j_{k}=i$ is $\kappa$%
\ \ \ \ \footnote{because%
\begin{align*}
j_{\kappa}  &  =i_{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,...,N\right)  =\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ and }\kappa\in\left\{  0,1,...,N+m\right\}  \right)
\\
&  =i
\end{align*}
}. Now,
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.b.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\in\left\{  j_{0}%
,j_{1},j_{2},...\right\}  $, and since the integer $k$ satisfying $j_{k}=i$ is
$\kappa$, this simplifies to%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =\left(  -1\right)  ^{\kappa}v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\wedge v_{j_{\kappa-1}}\wedge v_{j_{\kappa+1}}\wedge
v_{j_{\kappa+2}}\wedge....
\]
Compared with (\ref{pf.plu.inf.S.comp.b.2}), this yields $\left(
j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }}\right)
\left(  u\right)  =\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  \left(  u\right)  $. This is exactly what we needed to prove in
order to complete the proof of (\ref{pf.plu.inf.S.comp.b}). The proof of
(\ref{pf.plu.inf.S.comp.b}) is thus finished.

\textbf{c)} Let us next show that%
\begin{equation}
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }=0\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left\{  -N-1,-N-2,-N-3,...\right\}  . \label{pf.plu.inf.S.comp.c}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.c}):} Let $i\in\left\{
-N-1,-N-2,-N-3,...\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.c}),
it is clearly enough to show that $\left(  \widehat{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \left(  u\right)  =0$ for every $u\in\wedge^{N+m+1}\left(
V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  \left(  u\right)  =0$. Since this equality is linear in $u$, we can
WLOG assume that $u$ is an element of the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}%
>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume this.
Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $
of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

The vector $v_{i}$ occurs twice in the semiinfinite wedge $v_{i}\wedge
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ (namely, it occurs once in the very
beginning of this wedge, and then it occurs again in the $v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ part (because $i\in\left\{
-N-1,-N-2,-N-3,...\right\}  $)). Hence, the semiinfinite wedge $v_{i}\wedge
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ equals $0$ (since a semiinfinite wedge in
which a vector occurs more than once must always be equal to $0$).

Now,%
\begin{align*}
\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\widehat{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =v_{i}\wedge j_{N}^{\left(  m\right)  }\left(
\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{v_{i}}\right)
\\
&  =v_{i}\wedge\underbrace{j_{N}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  }_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(by the definition of }j_{N}^{\left(  m\right)
}\text{)}}}\\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{as we proved above}\right)  .
\end{align*}
This is exactly what we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.c}). The proof of (\ref{pf.plu.inf.S.comp.c}) is thus finished.

\textbf{d)} Let us now show that
\begin{equation}
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }%
=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  N+1,N+2,N+3,...\right\}  .
\label{pf.plu.inf.S.comp.d}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.d}):} Let $i\in\left\{
N+1,N+2,N+3,...\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.b}), it
is clearly enough to show that $\left(  \overset{\vee}{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  =0$ for every $u\in
\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \left(  u\right)  =0$. Since this equality is linear in
$u$, we can WLOG assume that $u$ is an element of the basis $\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}%
>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume
this. Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}%
\right)  $ of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and
$u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider this
$N+m+1$-tuple.

Notice that $i\in\left\{  N+1,N+2,N+3,...\right\}  $, so that $i\notin\left\{
N,N-1,...,-N\right\}  $ and $i\notin\left\{  N,N-1,N-2,...\right\}  $.

Since $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$, we have $\left\{  i_{0}%
,i_{1},...,i_{N+m}\right\}  \subseteq\left\{  N,N-1,...,-N\right\}  $ and thus
$i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $ (because $i\notin\left\{
N,N-1,...,-N\right\}  $).

Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be the sequence $\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  $. Then,%
\begin{align*}
\left\{  j_{0},j_{1},j_{2},...\right\}   &  =\left\{  i_{0},i_{1}%
,...,i_{N+m},-N-1,-N-2,-N-3,...\right\} \\
&  =\underbrace{\left\{  i_{0},i_{1},...,i_{N+m}\right\}  }_{\subseteq\left\{
N,N-1,...,-N\right\}  }\cup\left\{  -N-1,-N-2,-N-3,...\right\} \\
&  \subseteq\left\{  N,N-1,...,-N\right\}  \cup\left\{
-N-1,-N-2,-N-3,...\right\}  =\left\{  N,N-1,N-2,...\right\}  .
\end{align*}
Thus, $i\notin\left\{  j_{0},j_{1},j_{2},...\right\}  $ (since $i\notin%
\left\{  N,N-1,N-2,...\right\}  $).

From $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m\right)  }\right) \nonumber\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  ,
\label{pf.plu.inf.S.comp.d.0}%
\end{align}
so that%
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.d.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\notin\left\{
j_{0},j_{1},j_{2},...\right\}  $, this simplifies to $\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  =0$.

This is exactly what we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.d}). The proof of (\ref{pf.plu.inf.S.comp.d}) is thus finished.

\textbf{e)} Now it is the time to draw conclusions.

We have $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$ (by the definition of $S$). Thus,%
\begin{align*}
&  S\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  =\left(  \sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes
\overset{\vee}{v_{i}}\right)  \circ\left(  j_{N}^{\left(  m\right)  }\otimes
j_{N}^{\left(  m\right)  }\right)  =\sum\limits_{i\in\mathbb{Z}}%
\underbrace{\left(  \widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)
\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  }_{=\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
\otimes\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
}\\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \widehat{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \otimes\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(
m\right)  }\right) \\
&  =\sum\limits_{i=-\infty}^{-N-1}\underbrace{\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  }_{\substack{=0\\\text{(by
(\ref{pf.plu.inf.S.comp.c}))}}}\otimes\left(  \overset{\vee}{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  +\sum\limits_{i=-N}^{N}\underbrace{\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{\substack{=j_{N}%
^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }}\\\text{(by
(\ref{pf.plu.inf.S.comp.a}))}}}\otimes\underbrace{\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{\substack{=j_{N}^{\left(
m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\\\text{(by
(\ref{pf.plu.inf.S.comp.b}))}}}+\sum\limits_{i=N+1}^{\infty}\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \otimes
\underbrace{\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  }_{\substack{=0\\\text{(by (\ref{pf.plu.inf.S.comp.d}))}}}\\
&  =\underbrace{\sum\limits_{i=-\infty}^{-N-1}0\otimes\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{=0}+\sum\limits_{i=-N}%
^{N}\underbrace{\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}%
^{\left(  N\right)  }}\right)  \otimes\left(  j_{N}^{\left(  m+1\right)
}\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  }_{=\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)
\circ\left(  \widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  }+\underbrace{\sum\limits_{i=N+1}%
^{\infty}\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
\otimes0}_{=0}\\
&  =\sum\limits_{i=-N}^{N}\left(  j_{N}^{\left(  m+1\right)  }\otimes
j_{N}^{\left(  m-1\right)  }\right)  \circ\left(  \widehat{v_{i}^{\left(
N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  =\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)
\circ\left(  \sum\limits_{i=-N}^{N}\widehat{v_{i}^{\left(  N\right)  }}%
\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  .
\end{align*}
But since $S_{N}^{\left(  N+m+1\right)  }=\sum\limits_{i=-N}^{N}%
\widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(
N\right)  }}$ (by the definition of $S_{N}^{\left(  N+m+1\right)  }$), this
rewrites as%
\[
S\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  =\left(  j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(
m-1\right)  }\right)  \circ\underbrace{\left(  \sum\limits_{i=-N}%
^{N}\widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  }_{=S_{N}^{\left(  N+m+1\right)  }}=\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)  \circ
S_{N}^{\left(  N+m+1\right)  }.
\]
This proves Lemma \ref{lem.plu.inf.S.comp}.

Now we can finally come to proving Theorem \ref{thm.plu.inf}:

\textit{Proof of Theorem \ref{thm.plu.inf}.} Let $\varrho^{\prime
}:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}\otimes\mathcal{F}\right)  $ be the action of the monoid
$\operatorname*{M}\left(  \infty\right)  $ on the tensor product of the
$\operatorname*{M}\left(  \infty\right)  $-module $\mathcal{F}$ with itself.
Clearly,%
\[
\varrho^{\prime}\left(  M\right)  =\varrho\left(  M\right)  \otimes
\varrho\left(  M\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }M\in
\operatorname*{M}\left(  \infty\right)
\]
(because this is how one defines the tensor product of two modules over a monoid).

\textbf{(c)} Let $m\in\mathbb{Z}$. Let $M\in\operatorname*{M}\left(
\infty\right)  $. Let $v\in\mathcal{F}^{\left(  m\right)  }$ and
$w\in\mathcal{F}^{\left(  m\right)  }$. We are going to prove that $\left(
S\circ\varrho^{\prime}\left(  M\right)  \right)  \left(  v\otimes w\right)
=\left(  \varrho^{\prime}\left(  M\right)  \circ S\right)  \left(  v\otimes
w\right)  $.

Since $M\in\operatorname*{M}\left(  \infty\right)  =\bigcup\limits_{N\in
\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $ (by
Remark \ref{rmk.plu.inf.iN} \textbf{(c)}), there exists an $R\in\mathbb{N}$
such that $M\in i_{R}\left(  \operatorname*{M}\left(  V_{R}\right)  \right)
$. Consider this $R$.

Since $v\in\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq R}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $Q=R$), there exists some $T\in\mathbb{N}$ such that
$T\geq R$ and $v\in j_{T}^{\left(  m\right)  }\left(  \wedge^{T+m+1}\left(
V_{T}\right)  \right)  $. Consider this $T$.

Since $w\in\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq T}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $Q=T$), there exists some $P\in\mathbb{N}$ such that
$P\geq T$ and $w\in j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  $. Consider this $P$. There exists a $w^{\prime}%
\in\wedge^{P+m+1}\left(  V_{P}\right)  $ such that $w=j_{P}^{\left(  m\right)
}\left(  w^{\prime}\right)  $ (because $w\in j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  $). Consider this $w^{\prime}$.

Applying Proposition \ref{prop.plu.inf.cover} \textbf{(a)}, we get
$j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)
\right)  \subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq...$. Thus,
$j_{T}^{\left(  m\right)  }\left(  \wedge^{T+m+1}\left(  V_{T}\right)
\right)  \subseteq j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  $ (since $T\leq P$), so that $v\in j_{T}^{\left(
m\right)  }\left(  \wedge^{T+m+1}\left(  V_{T}\right)  \right)  \subseteq
j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(  V_{P}\right)  \right)
$. Hence, there exists a $v^{\prime}\in\wedge^{P+m+1}\left(  V_{P}\right)  $
such that $v=j_{P}^{\left(  m\right)  }\left(  v^{\prime}\right)  $. Consider
this $v^{\prime}$. Since $v=j_{P}^{\left(  m\right)  }\left(  v^{\prime
}\right)  $ and $w=j_{P}^{\left(  m\right)  }\left(  w^{\prime}\right)  $, we
have%
\begin{equation}
v\otimes w=j_{P}^{\left(  m\right)  }\left(  v^{\prime}\right)  \otimes
j_{P}^{\left(  m\right)  }\left(  w^{\prime}\right)  =\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  \left(  v^{\prime
}\otimes w^{\prime}\right)  . \label{pf.plu.inf.-20}%
\end{equation}


Since $R\leq T\leq P$, we have $i_{R}\left(  \operatorname*{M}\left(
V_{R}\right)  \right)  \subseteq i_{P}\left(  \operatorname*{M}\left(
V_{P}\right)  \right)  $ (since Remark \ref{rmk.plu.inf.iN} \textbf{(b)}
yields $i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)  \right)  \subseteq
i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)  \right)  \subseteq
i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)  \right)  \subseteq...$).
Thus, $M\in i_{R}\left(  \operatorname*{M}\left(  V_{R}\right)  \right)
\subseteq i_{P}\left(  \operatorname*{M}\left(  V_{P}\right)  \right)  $. In
other words, there exists an $A\in\operatorname*{M}\left(  V_{P}\right)  $
such that $M=i_{P}\left(  A\right)  $. Consider this $A$.

In the following, we will write the action of $\operatorname*{M}\left(
\infty\right)  $ on $\mathcal{F}$ as a left action. In other words, we will
abbreviate $\left(  \varrho\left(  N\right)  \right)  u$ by $Nu$, wherever
$N\in\operatorname*{M}\left(  \infty\right)  $ and $u\in\mathcal{F}$.
Similarly, we will write the action of $\operatorname*{M}\left(
\infty\right)  $ on $\mathcal{F}\otimes\mathcal{F}$ (this action is obtained
by tensoring the $\operatorname*{M}\left(  \infty\right)  $-module
$\mathcal{F}$ with itself); this action satisfies $\varrho^{\prime}\left(
A\right)  =\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  $.

Let us also denote by $\varrho$ the action of the monoid $\operatorname*{M}%
\left(  V_{N}\right)  $ on $\wedge\left(  V_{N}\right)  $. Moreover, let us
denote by $\varrho^{\prime}$ the action of the monoid $\operatorname*{M}%
\left(  V_{N}\right)  $ on $\wedge\left(  V_{N}\right)  \otimes\wedge\left(
V_{N}\right)  $ (this action is obtained by tensoring the $\operatorname*{M}%
\left(  V_{N}\right)  $-module $\wedge\left(  V_{N}\right)  $ with itself).

We notice that every $\ell\in\mathbb{Z}$ satisfies%
\begin{equation}
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  \ell\right)
}=j_{P}^{\left(  \ell\right)  }\circ\left(  \varrho\left(  A\right)  \right)
. \label{pf.plu.inf.-10}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.-10}):} Let $\ell\in\mathbb{Z}$.
Every $u\in\mathcal{F}^{\left(  \ell\right)  }$ satisfies%
\begin{align*}
\left(  \left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(
\ell\right)  }\right)  \left(  u\right)   &  =\left(  \varrho\left(  M\right)
\right)  \left(  j_{P}^{\left(  \ell\right)  }\left(  u\right)  \right)
=\underbrace{M}_{=i_{P}\left(  A\right)  }\cdot j_{P}^{\left(  \ell\right)
}\left(  u\right)  =i_{P}\left(  A\right)  \cdot j_{P}^{\left(  \ell\right)
}u=j_{P}^{\left(  \ell\right)  }\underbrace{\left(  Au\right)  }_{=\left(
\varrho\left(  A\right)  \right)  u}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.plu.inf.iNjmN},
applied to }P\text{ and }\ell\text{ instead of }N\text{ and }m\right) \\
&  =j_{P}^{\left(  \ell\right)  }\left(  \left(  \varrho\left(  A\right)
\right)  u\right)  =\left(  j_{P}^{\left(  \ell\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \left(  u\right)  .
\end{align*}
Thus, $\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(
\ell\right)  }=j_{P}^{\left(  \ell\right)  }\circ\left(  \varrho\left(
A\right)  \right)  $, so that (\ref{pf.plu.inf.-10}) is proven.}

Applying Lemma \ref{lem.plu.inf.S.comp} to $N=P$, we obtain
\begin{equation}
\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ S_{P}^{\left(  P+m+1\right)  }=S\circ\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  . \label{pf.plu.inf.1}%
\end{equation}


On the other hand, the map $S_{P}^{\left(  P+m+1\right)  }$ is
$\operatorname*{M}\left(  \infty\right)  $-invariant (by Lemma
\ref{lem.plu.inf.plu} \textbf{(c)}, applied to $N=P$ and $k=P+m+1$), so that%
\[
S_{P}^{\left(  P+m+1\right)  }\circ\left(  \varrho^{\prime}\left(  A\right)
\right)  =\left(  \varrho^{\prime}\left(  A\right)  \right)  \circ
S_{P}^{\left(  P+m+1\right)  }.
\]
Since $\varrho^{\prime}\left(  A\right)  =\varrho\left(  A\right)
\otimes\varrho\left(  A\right)  $, this rewrites as
\begin{equation}
S_{P}^{\left(  P+m+1\right)  }\circ\left(  \varrho\left(  A\right)
\otimes\varrho\left(  A\right)  \right)  =\left(  \varrho\left(  A\right)
\otimes\varrho\left(  A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  }.
\label{pf.plu.inf.2}%
\end{equation}


Comparing%
\begin{align*}
&  S\circ\underbrace{\left(  \varrho^{\prime}\left(  M\right)  \right)
}_{=\varrho\left(  M\right)  \otimes\varrho\left(  M\right)  }\circ\left(
j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)  }\right) \\
&  =S\circ\underbrace{\left(  \varrho\left(  M\right)  \otimes\varrho\left(
M\right)  \right)  \circ\left(  j_{P}^{\left(  m\right)  }\otimes
j_{P}^{\left(  m\right)  }\right)  }_{=\left(  \left(  \varrho\left(
M\right)  \right)  \circ j_{P}^{\left(  m\right)  }\right)  \otimes\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m\right)
}\right)  }\\
&  =S\circ\left(  \underbrace{\left(  \left(  \varrho\left(  M\right)
\right)  \circ j_{P}^{\left(  m\right)  }\right)  }_{\substack{=j_{P}^{\left(
m\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m\text{)}}}\otimes\underbrace{\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m\right)
}\right)  }_{\substack{=j_{P}^{\left(  m\right)  }\circ\left(  \varrho\left(
A\right)  \right)  \\\text{(by (\ref{pf.plu.inf.-10}), applied to }%
\ell=m\text{)}}}\right) \\
&  =S\circ\underbrace{\left(  \left(  j_{P}^{\left(  m\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \otimes\left(  j_{P}^{\left(
m\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \right)  \right)
}_{=\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  }=\underbrace{S\circ\left(  j_{P}^{\left(  m\right)
}\otimes j_{P}^{\left(  m\right)  }\right)  }_{\substack{=\left(
j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)  }\right)  \circ
S_{P}^{\left(  P+m+1\right)  }\\\text{(by (\ref{pf.plu.inf.1}))}}}\circ\left(
\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  \right) \\
&  =\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\underbrace{S_{P}^{\left(  P+m+1\right)  }\circ\left(
\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  \right)
}_{\substack{=\left(  \varrho\left(  A\right)  \otimes\varrho\left(  A\right)
\right)  \circ S_{P}^{\left(  P+m+1\right)  }\\\text{(by (\ref{pf.plu.inf.2}%
))}}}=\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  }%
\end{align*}
with%
\begin{align*}
&  \underbrace{\left(  \varrho^{\prime}\left(  M\right)  \right)  }%
_{=\varrho\left(  M\right)  \otimes\varrho\left(  M\right)  }\circ
\underbrace{S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  }_{\substack{=\left(  j_{P}^{\left(  m+1\right)  }\otimes
j_{P}^{\left(  m-1\right)  }\right)  \circ S_{P}^{\left(  P+m+1\right)
}\\\text{(by (\ref{pf.plu.inf.1}))}}}\\
&  =\underbrace{\left(  \varrho\left(  M\right)  \otimes\varrho\left(
M\right)  \right)  \circ\left(  j_{P}^{\left(  m+1\right)  }\otimes
j_{P}^{\left(  m-1\right)  }\right)  }_{=\left(  \left(  \varrho\left(
M\right)  \right)  \circ j_{P}^{\left(  m+1\right)  }\right)  \otimes\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m-1\right)
}\right)  }\circ S_{P}^{\left(  P+m+1\right)  }\\
&  =\left(  \underbrace{\left(  \left(  \varrho\left(  M\right)  \right)
\circ j_{P}^{\left(  m+1\right)  }\right)  }_{\substack{=j_{P}^{\left(
m+1\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m+1\text{)}}}\otimes
\underbrace{\left(  \left(  \varrho\left(  M\right)  \right)  \circ
j_{P}^{\left(  m-1\right)  }\right)  }_{\substack{=j_{P}^{\left(  m-1\right)
}\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m-1\text{)}}}\right)  \circ
S_{P}^{\left(  P+m+1\right)  }\\
&  =\underbrace{\left(  \left(  j_{P}^{\left(  m+1\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \otimes\left(  j_{P}^{\left(
m-1\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \right)  \right)
}_{=\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  }\circ S_{P}^{\left(  P+m+1\right)  }\\
&  =\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  },
\end{align*}
we obtain%
\begin{equation}
S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \circ\left(
j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  =\left(
\varrho^{\prime}\left(  M\right)  \right)  \circ S\circ\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  . \label{pf.plu.inf.14}%
\end{equation}


Now,
\begin{align*}
&  \left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \right)
\underbrace{\left(  v\otimes w\right)  }_{\substack{=\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  \left(  v^{\prime
}\otimes w^{\prime}\right)  \\\text{(by (\ref{pf.plu.inf.-20}))}}}\\
&  =\left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \right)
\left(  \left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \left(  v^{\prime}\otimes w^{\prime}\right)  \right)
=\underbrace{\left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)
\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \right)  }_{\substack{=\left(  \varrho^{\prime}\left(  M\right)
\right)  \circ S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  \\\text{(by (\ref{pf.plu.inf.14}))}}}\left(  v^{\prime
}\otimes w^{\prime}\right) \\
&  =\left(  \left(  \varrho^{\prime}\left(  M\right)  \right)  \circ
S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \right)  \left(  v^{\prime}\otimes w^{\prime}\right)  =\left(
\left(  \varrho^{\prime}\left(  M\right)  \right)  \circ S\right)
\underbrace{\left(  \left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  \left(  v^{\prime}\otimes w^{\prime}\right)  \right)
}_{\substack{=v\otimes w\\\text{(by (\ref{pf.plu.inf.-20}))}}}\\
&  =\left(  \left(  \varrho^{\prime}\left(  M\right)  \right)  \circ S\right)
\left(  v\otimes w\right)  .
\end{align*}


Now forget that we fixed $v$ and $w$. We thus have proven that $\left(
S\circ\varrho^{\prime}\left(  M\right)  \right)  \left(  v\otimes w\right)
=\left(  \varrho^{\prime}\left(  M\right)  \circ S\right)  \left(  v\otimes
w\right)  $ for every $v\in\mathcal{F}^{\left(  m\right)  }$ and
$w\in\mathcal{F}^{\left(  m\right)  }$. In other words, the two maps
$S\circ\varrho^{\prime}\left(  M\right)  $ and $\varrho^{\prime}\left(
M\right)  \circ S$ are equal to each other on every pure tensor in
$\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }$.
Thus, these two maps must be identical (on $\mathcal{F}^{\left(  m\right)
}\otimes\mathcal{F}^{\left(  m\right)  }$). In other words, $S\circ
\varrho^{\prime}\left(  M\right)  =\varrho^{\prime}\left(  M\right)  \circ S$.

Now forget that we fixed $M$. We have proven that $S\circ\varrho^{\prime
}\left(  M\right)  =\varrho^{\prime}\left(  M\right)  \circ S$ for every
$M\in\operatorname*{M}\left(  \infty\right)  $. In other words, $S$ is
$\operatorname*{M}\left(  \infty\right)  $-invariant. This proves Theorem
\ref{thm.plu.inf} \textbf{(c)}.

\textbf{(a)} Theorem \ref{thm.plu.inf} \textbf{(a)} follows from Theorem
\ref{thm.plu.inf} \textbf{(c)} since $\operatorname*{GL}\left(  \infty\right)
\subseteq\operatorname*{M}\left(  \infty\right)  $.

\textbf{(b)} $\Longrightarrow:$ Assume that $\tau\in\Omega$. We want to prove
that $S\left(  \tau\otimes\tau\right)  =0$.

Since $\Omega=\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}$, we have
$\tau\in\Omega=\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}$. In
other words, there exists $A\in\operatorname*{GL}\left(  \infty\right)  $ such
that $\tau=A\psi_{0}$. Consider this $A$.

It is easy to see that%
\begin{equation}
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for
every integer }i>0. \label{pf.plu.inf.b1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.b1}):} Let $i>0$ be an integer.
Then,%
\begin{align*}
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)   &  =\overset{\vee}{v_{i}%
}\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi_{0}=v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  0,-1,-2,...\right\}  ;\\
\left(  -1\right)  ^{j}v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\wedge
v_{-\left(  j-1\right)  }\wedge v_{-\left(  j+1\right)  }\wedge v_{-\left(
j+2\right)  }\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{
0,-1,-2,...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  0,-1,-2,...\right\}  $, we denote by $j$ the
integer $k$ satisfying $-k=i$. Since $i\notin\left\{  0,-1,-2,...\right\}  $
(because $i>0$), this simplifies to $\overset{\vee}{v_{i}}\left(  \psi
_{0}\right)  =0$. This proves (\ref{pf.plu.inf.b1}).} Also,%
\begin{equation}
\widehat{v_{i}}\left(  \psi_{0}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for every
integer }i\leq0. \label{pf.plu.inf.b2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.b2}):} Let $i\leq0$ be an integer.
Since $\psi_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$, we have%
\[
\widehat{v_{i}}\left(  \psi_{0}\right)  =\widehat{v_{i}}\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\right)  =v_{i}\wedge v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...
\]
(by the definition of $\widehat{v_{i}}$). But the semiinfinite wedge
$v_{i}\wedge v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$ contains the vector
$v_{i}$ twice (in fact, it contains the vector $v_{i}$ once in its very
beginning, and once again in its $v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$
part (since $i\leq0$)), and thus must equal $0$ (since any semiinfinite wedge
which contains a vector more than once must equal $0$). We thus have
\[
\widehat{v_{i}}\left(  \psi_{0}\right)  =v_{i}\wedge v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...=0.
\]
This proves (\ref{pf.plu.inf.b2}).}

Since $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$, we have%
\begin{align*}
S\left(  \psi_{0}\otimes\psi_{0}\right)   &  =\sum\limits_{i\in\mathbb{Z}%
}\underbrace{\left(  \widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)
\left(  \psi_{0}\otimes\psi_{0}\right)  }_{=\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  }%
=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\left(  \psi_{0}\right)
\otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}\underbrace{\widehat{v_{i}%
}\left(  \psi_{0}\right)  }_{\substack{=0\\\text{(by (\ref{pf.plu.inf.b2}))}%
}}\otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  +\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0}}\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes\underbrace{\overset{\vee}{v_{i}}\left(  \psi_{0}\right)
}_{\substack{=0\\\text{(by (\ref{pf.plu.inf.b1}))}}}\\
&  =\underbrace{\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}0\otimes
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  }_{=0}+\underbrace{\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0}}\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes0}_{=0}=0.
\end{align*}


Now, since $\tau=A\psi_{0}$, we have $\tau\otimes\tau=A\psi_{0}\otimes
A\psi_{0}=A\left(  \psi_{0}\otimes\psi_{0}\right)  $, so that%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =S\left(  A\left(  \psi_{0}\otimes
\psi_{0}\right)  \right) \\
&  =A\cdot\underbrace{S\left(  \psi_{0}\otimes\psi_{0}\right)  }%
_{=0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }S\text{ is }\operatorname*{M}%
\left(  \infty\right)  \text{-linear (by Theorem \ref{thm.plu.inf}
\textbf{(c)})}\right) \\
&  =A\cdot0=0.
\end{align*}
This proves the $\Longrightarrow$ direction of Theorem \ref{thm.plu.inf}
\textbf{(b)}.

$\Longleftarrow:$ Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be such that
$S\left(  \tau\otimes\tau\right)  =0$. We want to prove that $\tau\in\Omega$.

Since $\tau\in\mathcal{F}^{\left(  0\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq0}}j_{N}^{\left(  0\right)  }\left(  \wedge^{N+0+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $m=0$ and $Q=0$), there exists some $N\in\mathbb{N}$
such that $N\geq0$ and $\tau\in j_{N}^{\left(  0\right)  }\left(
\wedge^{N+0+1}\left(  V_{N}\right)  \right)  $. Consider this $N$.

Lemma \ref{lem.plu.inf.S.comp} (applied to $m=0$) yields
\begin{equation}
\left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(  -1\right)  }\right)
\circ S_{N}^{\left(  N+1\right)  }=S\circ\left(  j_{N}^{\left(  0\right)
}\otimes j_{N}^{\left(  0\right)  }\right)  . \label{pf.plu.inf.b5}%
\end{equation}


Recall that the map $j_{N}^{\left(  m\right)  }$ is injective for every
$m\in\mathbb{Z}$. In particular, the maps $j_{N}^{\left(  1\right)  }$ and
$j_{N}^{\left(  -1\right)  }$ are injective, so that the map $j_{N}^{\left(
1\right)  }\otimes j_{N}^{\left(  -1\right)  }$ is also injective.

But $\tau\in j_{N}^{\left(  0\right)  }\left(  \wedge^{N+0+1}\left(
V_{N}\right)  \right)  =j_{N}^{\left(  0\right)  }\left(  \wedge^{N+1}\left(
V_{N}\right)  \right)  $. In other words, there exists some $\tau^{\prime}%
\in\wedge^{N+1}\left(  V_{N}\right)  $ such that $\tau=j_{N}^{\left(
0\right)  }\left(  \tau^{\prime}\right)  $. Consider this $\tau^{\prime}$.

Since $\tau=j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  $, we have
$\tau\otimes\tau=j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)
\otimes j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  =\left(
j_{N}^{\left(  0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \left(
\tau^{\prime}\otimes\tau^{\prime}\right)  $, so that%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =S\left(  \left(  j_{N}^{\left(
0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \left(  \tau^{\prime
}\otimes\tau^{\prime}\right)  \right)  =\underbrace{\left(  S\circ\left(
j_{N}^{\left(  0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \right)
}_{\substack{=\left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }\right)  \circ S_{N}^{\left(  N+1\right)  }\\\text{(by
(\ref{pf.plu.inf.b5}))}}}\left(  \tau^{\prime}\otimes\tau^{\prime}\right) \\
&  =\left(  \left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }\right)  \circ S_{N}^{\left(  N+1\right)  }\right)  \left(
\tau^{\prime}\otimes\tau^{\prime}\right)  =\left(  j_{N}^{\left(  1\right)
}\otimes j_{N}^{\left(  -1\right)  }\right)  \left(  S_{N}^{\left(
N+1\right)  }\left(  \tau^{\prime}\otimes\tau^{\prime}\right)  \right)  .
\end{align*}
Compared with $S\left(  \tau\otimes\tau\right)  =0$, this yields $\left(
j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(  -1\right)  }\right)  \left(
S_{N}^{\left(  N+1\right)  }\left(  \tau^{\prime}\otimes\tau^{\prime}\right)
\right)  =0$. Since $j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }$ is injective, this yields $S_{N}^{\left(  N+1\right)  }\left(
\tau^{\prime}\otimes\tau^{\prime}\right)  =0$. But Lemma \ref{lem.plu.inf.plu}
\textbf{(b)} (applied to $N+1$ and $\tau^{\prime}$ instead of $k$ and $\tau$)
yields that $\tau^{\prime}$ belongs to $\Omega_{N}^{\left(  N+1\right)  }$ if
and only if $S_{N}^{\left(  N+1\right)  }\left(  \tau^{\prime}\otimes
\tau^{\prime}\right)  =0$. Since we know that $S_{N}^{\left(  N+1\right)
}\left(  \tau^{\prime}\otimes\tau^{\prime}\right)  =0$, we can thus conclude
that $\tau^{\prime}$ belongs to $\Omega_{N}^{\left(  N+1\right)  }$. Since
$\Omega_{N}^{\left(  N+1\right)  }$ is the orbit of $v_{N}\wedge v_{N-1}%
\wedge...\wedge v_{N-\left(  N+1\right)  +1}$ under the action of
$\operatorname*{GL}\left(  V_{N}\right)  $ (this is how $\Omega_{N}^{\left(
N+1\right)  }$ was defined), this yields that $\tau^{\prime}$ belongs to the
orbit of $v_{N}\wedge v_{N-1}\wedge...\wedge v_{N-\left(  N+1\right)  +1}$
under the action of $\operatorname*{GL}\left(  V_{N}\right)  $. In other
words, there exists some $A\in\operatorname*{GL}\left(  V_{N}\right)  $ such
that $\tau^{\prime}=A\cdot\left(  v_{N}\wedge v_{N-1}\wedge...\wedge
v_{N-\left(  N+1\right)  +1}\right)  $. Consider this $A$.

We have $\tau^{\prime}=A\cdot\left(  v_{N}\wedge v_{N-1}\wedge...\wedge
\underbrace{v_{N-\left(  N+1\right)  +1}}_{=v_{0}}\right)  =A\cdot\left(
v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}\right)  $.

There clearly exists an invertible linear map $B\in\operatorname*{GL}\left(
V_{N}\right)  $ which sends $v_{N}$, $v_{N-1}$, $...$, $v_{0}$ to $v_{0}$,
$v_{-1}$, $...$, $v_{-N}$, respectively\footnote{\textit{Proof.} Since
$\left(  v_{N},v_{N-1},...,v_{-N}\right)  $ is a basis of $V_{N}$, there
exists a linear map $B\in\operatorname*{End}\left(  V_{N}\right)  $ which
sends $v_{i}$ to $\left\{
\begin{array}
[c]{c}%
v_{i-N},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\\
v_{-i},\ \ \ \ \ \ \ \ \ \ \text{if }i<0
\end{array}
\right.  $ for every $i\in\left\{  N,N-1,...,-N\right\}  $. This linear map
$B$ is invertible (since it permutes the elements of the basis $\left(
v_{N},v_{N-1},...,v_{-N}\right)  $ of $V_{N}$), and thus lies in
$\operatorname*{GL}\left(  V_{N}\right)  $, and it clearly sends $v_{N}$,
$v_{N-1}$, $...$, $v_{0}$ to $v_{0}$, $v_{-1}$, $...$, $v_{-N}$, respectively.
Qed.}. Pick such a $B$. Then, $B\cdot\left(  v_{N}\wedge v_{N-1}%
\wedge...\wedge v_{0}\right)  =v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}$
(since $B$ sends $v_{N}$, $v_{N-1}$, $...$, $v_{0}$ to $v_{0}$, $v_{-1}$,
$...$, $v_{-N}$, respectively), so that $B^{-1}\cdot\left(  v_{0}\wedge
v_{-1}\wedge...\wedge v_{-N}\right)  =v_{N}\wedge v_{N-1}\wedge...\wedge
v_{0}$ and thus%
\[
A\underbrace{B^{-1}\cdot\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }_{=v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}}=A\cdot\left(
v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}\right)  =\tau^{\prime}.
\]


Let $M=i_{N}\left(  AB^{-1}\right)  $. Then, $M=i_{N}\underbrace{\left(
AB^{-1}\right)  }_{\in\operatorname*{GL}\left(  V_{N}\right)  }\in
i_{N}\left(  \operatorname*{GL}\left(  V_{N}\right)  \right)  \subseteq
\operatorname*{GL}\left(  \infty\right)  $. Also,%
\begin{align*}
j_{N}^{\left(  0\right)  }\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)   &  =v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
0\right)  }\right) \\
&  =v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...=\psi_{0}.
\end{align*}
Now,%
\begin{align*}
&  \underbrace{M}_{=i_{N}\left(  AB^{-1}\right)  }\cdot\underbrace{\psi_{0}%
}_{=j_{N}^{\left(  0\right)  }\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }\\
&  =i_{N}\left(  AB^{-1}\right)  \cdot j_{N}^{\left(  0\right)  }\left(
v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}\right)  =j_{N}^{\left(  0\right)
}\left(  \underbrace{AB^{-1}\cdot\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }_{=\tau^{\prime}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.plu.inf.iNjmN},
applied to }0\text{, }AB^{-1}\text{ and }v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\text{ instead of }m\text{, }A\text{ and }u\right) \\
&  =j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  =\tau.
\end{align*}
Thus, $\tau=\underbrace{M}_{\in\operatorname*{GL}\left(  \infty\right)  }%
\cdot\psi_{0}\in\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}=\Omega
$. This proves the $\Longleftarrow$ direction of Theorem \ref{thm.plu.inf}
\textbf{(b)}.

\subsubsection{The semiinfinite Grassmannian}

Denote $\Omega\diagup\mathbb{C}^{\times}$ by $\operatorname*{Gr}$; this is
called the \textit{semiinfinite Grassmannian}.

Think of the space $V$ as $\mathbb{C}\left[  t,t^{-1}\right]  $ (by
identifying $v_{i}$ with $t^{-i}$). Then, $\left\langle v_{0},v_{-1}%
,v_{-2},...\right\rangle =\mathbb{C}\left[  t\right]  $.

\textbf{Exercise:} Then, $\operatorname*{Gr}$ is the set%
\[
\left\{  E\subseteq V\ \text{subspace\ }\mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  t\right]  \text{ for sufficiently large
}N\text{, and}\\
\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N\text{ for
sufficiently large }N
\end{array}
\right)  \right\}  .
\]
\footnote{Here, "subspace" means "$\mathbb{C}$-vector subspace".} (Note that
when the relations $E\supseteq t^{N}\mathbb{C}\left[  t\right]  $ and
$\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N$ hold for
\textit{some} $N$, it is easy to see that they also hold for all greater $N$.)

We can also replace $\mathbb{C}\left[  t,t^{-1}\right]  $ with $\mathbb{C}%
\left(  \left(  t\right)  \right)  $ (the formal Laurent series), and then
\[
\operatorname*{Gr}=\left\{  E\subseteq V\text{ subspace}\ \mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ for
sufficiently large }N\text{, and}\\
\dim\left(  E\diagup t^{N}\mathbb{C}\left[  \left[  t\right]  \right]
\right)  =N\text{ for sufficiently large }N
\end{array}
\right)  \right\}  .
\]


For any $E\in\operatorname*{Gr}$, there exists some $N\in\mathbb{N}$ such that
$t^{N}\mathbb{C}\left[  t\right]  \subseteq E\subseteq t^{-N}\mathbb{C}\left[
t\right]  $, so that the quotient $E\diagup t^{N}\mathbb{C}\left[  t\right]
\subseteq t^{-N}\mathbb{C}\left[  t\right]  \diagup t^{N}\mathbb{C}\left[
t\right]  \cong\mathbb{C}^{2N}$.

Thus, $\operatorname*{Gr}=\bigcup\limits_{N\geq1}\operatorname*{Gr}\left(
N,2N\right)  $ (a nested union). (By a variation of this construction,
$\operatorname*{Gr}=\bigcup\limits_{N\geq1}\bigcup\limits_{M\geq
1}\operatorname*{Gr}\left(  N,N+M\right)  $.)

\subsubsection{The preimage of the Grassmannian under the Boson-Fermion
correspondence: the Hirota bilinear relations}

Now, how do we actually use these things to find solutions to the
Kadomtsev-Petviashvili equations and other integrable systems?

By Theorem \ref{thm.plu.inf} \textbf{(b)}, the elements of $\Omega$ are
exactly the nonzero elements $\tau$ of $\mathcal{F}^{\left(  0\right)  }$
satisfying $S\left(  \tau\otimes\tau\right)  =0$. We might wonder what happens
to these elements under the Boson-Fermion correspondence $\sigma$: how can
their preimages under $\sigma$ be described? In other words, can we find a
necessary and sufficient condition for a polynomial $\tau\in\mathcal{B}%
^{\left(  0\right)  }$ to satisfy $\sigma\left(  \tau\right)  \in\Omega$
(without using $\sigma$ in this very condition)?

Recall the power series $X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}%
\xi_{i}u^{i}$ and $X^{\ast}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}%
\xi_{i}^{\ast}u^{-i}$ defined in Definition \ref{def.euler.XGamma}. These
power series "act" on the fermionic space $\mathcal{F}$. The word "act" has
been put in inverted commas here because it is not the power series but their
coefficients which really act on $\mathcal{F}$, whereas the power series
themselves only map elements of $\mathcal{F}$ to elements of $\mathcal{F}%
\left(  \left(  u\right)  \right)  $. This, actually, is an important
observation:%
\begin{equation}
\text{every }\omega\in\mathcal{F}\text{ satisfies }X\left(  u\right)
\omega\in\mathcal{F}\left(  \left(  u\right)  \right)  \text{ and }X^{\ast
}\left(  u\right)  \omega\in\mathcal{F}\left(  \left(  u\right)  \right)  .
\label{KdV.F((u))}%
\end{equation}
\footnote{\textit{Proof of (\ref{KdV.F((u))}):} Let $\omega\in\mathcal{F}$.
Since $X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}$, we have
$X\left(  u\right)  \omega=\sum\limits_{i\in\mathbb{Z}}\xi_{i}\left(
\omega\right)  u^{i}\in\mathcal{F}\left(  \left(  u\right)  \right)  $,
because every sufficiently small $i\in\mathbb{Z}$ satisfies $\xi_{i}\left(
\omega\right)  =0$ (this is easy to see). On the other hand, since $X^{\ast
}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}u^{-i}$, we have
$X^{\ast}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}\left(
\omega\right)  u^{-i}\in\mathcal{F}\left(  \left(  u\right)  \right)  $, since
every sufficiently high $i\in\mathbb{Z}$ satisfies $\xi_{i}^{\ast}\left(
\omega\right)  =0$ (this, again, is easy to see). This proves
(\ref{KdV.F((u))}).}

Let $\tau\in\mathcal{B}^{\left(  0\right)  }$ be arbitrary. We want to find an
equivalent form for the equation $S\left(  \sigma\left(  \tau\right)
\otimes\sigma\left(  \tau\right)  \right)  =0$ which does not refer to
$\sigma$.

Let us give two definitions first:

\begin{definition}
\label{def.OMEGA}Let $A$ and $B$ be two $\mathbb{C}$-vector spaces, and let
$u$ be a symbol. Then, the map%
\begin{align*}
A\left(  \left(  u\right)  \right)  \times B\left(  \left(  u\right)  \right)
&  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)  \right)
,\\
\left(  \sum\limits_{i\in\mathbb{Z}}a_{i}u^{i},\sum\limits_{i\in\mathbb{Z}%
}b_{i}u^{i}\right)   &  \mapsto\sum\limits_{i\in\mathbb{Z}}\left(
\sum\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}\right)  u^{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{i}\text{ lie in }A\text{
and all }b_{i}\text{ lie in }B\right)
\end{align*}
is well-defined (in fact, it is easy to see that for any Laurent series
$\sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\in A\left(  \left(  u\right)  \right)
$ with all $a_{i}$ lying in $A$, any Laurent series $\sum\limits_{i\in
\mathbb{Z}}b_{i}u^{i}\in B\left(  \left(  u\right)  \right)  $ with all
$b_{i}$ lying in $B$, and any integer $i\in\mathbb{Z}$, the sum $\sum
\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}$ has only finitely many addends
and vanishes if $i$ is small enough) and $\mathbb{C}$-bilinear. Hence, it
induces a $\mathbb{C}$-linear map%
\begin{align*}
A\left(  \left(  u\right)  \right)  \otimes B\left(  \left(  u\right)
\right)   &  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)
\right)  ,\\
\left(  \sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\right)  \otimes\left(
\sum\limits_{i\in\mathbb{Z}}b_{i}u^{i}\right)   &  \mapsto\sum\limits_{i\in
\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}\right)
u^{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{i}\text{ lie in }A\text{
and all }b_{i}\text{ lie in }B\right)  .
\end{align*}
This map will be denoted by $\Omega_{A,B,u}$.
\end{definition}

More can be said about the map $\Omega_{A,B,u}$: It factors as a composition
of the canonical projection $A\left(  \left(  u\right)  \right)  \otimes
B\left(  \left(  u\right)  \right)  \rightarrow A\left(  \left(  u\right)
\right)  \otimes_{\mathbb{C}\left(  \left(  u\right)  \right)  }B\left(
\left(  u\right)  \right)  $ with a $\mathbb{C}\left(  \left(  u\right)
\right)  $-linear map $A\left(  \left(  u\right)  \right)  \otimes
_{\mathbb{C}\left(  \left(  u\right)  \right)  }B\left(  \left(  u\right)
\right)  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)
\right)  $. We won't need this in the following. What we will need is the
following observation:

\begin{remark}
\label{rmk.OMEGA.linear}Let $A$ and $B$ be two $\mathbb{C}$-algebras, and let
$u$ be a symbol. Then, the map $\Omega_{A,B,u}$ is $A\otimes B$-linear.
\end{remark}

\begin{definition}
Let $A$ be a $\mathbb{C}$-vector space, and let $u$ be a symbol. Then,
$\operatorname*{CT}\nolimits_{u}:A\left(  \left(  u\right)  \right)
\rightarrow A$ will denote the map which sends every Laurent series
$\sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\in A\left(  \left(  u\right)  \right)
$ (where all $a_{i}$ lie in $A$) to $a_{0}\in A$. The image of a Laurent
series $\alpha$ under $\operatorname*{CT}\nolimits_{u}$ will be called the
\textbf{constant term} of $\alpha$. The map $\operatorname*{CT}\nolimits_{u}$
is clearly $A$-linear.
\end{definition}

This notion of "constant term" we have thus defined for Laurent series is, of
course, completely analogous to the one used for polynomials and formal power
series. The label $\operatorname*{CT}\nolimits_{u}$ is an abbreviation for
"constant term with respect to the variable $u$".

Now, for every $\omega\in\mathcal{F}^{\left(  0\right)  }$ and $\rho
\in\mathcal{F}^{\left(  0\right)  }$, we have%
\begin{equation}
S\left(  \omega\otimes\rho\right)  =\operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{F},\mathcal{F},u}\left(  X\left(  u\right)  \omega\otimes
X^{\ast}\left(  u\right)  \rho\right)  \right)  . \label{KdV.S=CT}%
\end{equation}
\footnote{\textit{Proof of (\ref{KdV.S=CT}):} Let $\omega\in\mathcal{F}%
^{\left(  0\right)  }$ and $\rho\in\mathcal{F}^{\left(  0\right)  }$. Since
$X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}$ and $X^{\ast
}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}u^{-i}%
=\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}u^{i}$ (here, we substituted $-i$
for $i$ in the sum), we have%
\[
X\left(  u\right)  \omega\otimes X^{\ast}\left(  u\right)  \rho=\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}\right)  \omega\otimes\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}u^{i}\right)  \rho=\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{i}\left(  \omega\right)  u^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}\left(  \rho\right)
u^{i}\right)  ,
\]
so that%
\begin{align*}
&  \Omega_{\mathcal{F},\mathcal{F},u}\left(  X\left(  u\right)  \omega\otimes
X^{\ast}\left(  u\right)  \rho\right) \\
&  =\Omega_{\mathcal{F},\mathcal{F},u}\left(  \left(  \sum\limits_{i\in
\mathbb{Z}}\xi_{i}\left(  \omega\right)  u^{i}\right)  \otimes\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}\left(  \rho\right)  u^{i}\right)
\right)  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
\xi_{j}\left(  \omega\right)  \otimes\xi_{-\left(  i-j\right)  }^{\ast}\left(
\rho\right)  \right)  u^{i}%
\end{align*}
(by the definition of $\Omega_{\mathcal{F},\mathcal{F},u}$). Thus (by the
definition of $\operatorname*{CT}\nolimits_{u}$) we have%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\left(  X\left(  u\right)  \omega\otimes X^{\ast}\left(  u\right)
\rho\right)  \right) \\
&  =\sum\limits_{j\in\mathbb{Z}}\xi_{j}\left(  \omega\right)  \otimes
\xi_{-\left(  0-j\right)  }^{\ast}\left(  \rho\right)  =\sum\limits_{j\in
\mathbb{Z}}\xi_{j}\left(  \omega\right)  \otimes\xi_{j}^{\ast}\left(
\rho\right)  =\sum\limits_{i\in\mathbb{Z}}\underbrace{\xi_{i}}%
_{=\widehat{v_{i}}}\left(  \omega\right)  \otimes\underbrace{\xi_{i}^{\ast}%
}_{=\overset{\vee}{v_{i}}}\left(  \rho\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}j\text{ in the sum}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\left(  \omega\right)
\otimes\overset{\vee}{v_{i}}\left(  \rho\right)  =\underbrace{\left(
\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}%
}\right)  }_{\substack{=S\\\text{(because this is how }S\text{ was defined)}%
}}\left(  \omega\otimes\rho\right)  =S\left(  \omega\otimes\rho\right)  ,
\end{align*}
so that (\ref{KdV.S=CT}) is proven.}

Now, let $\tau\in\mathcal{B}^{\left(  0\right)  }$. Due to (\ref{KdV.F((u))})
(applied to $\omega=\sigma\left(  \tau\right)  $), we have $X\left(  u\right)
\sigma\left(  \tau\right)  \in\mathcal{F}\left(  \left(  u\right)  \right)  $
and $X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  \in\mathcal{F}%
\left(  \left(  u\right)  \right)  $.

Now, let us abuse notation and denote by $\sigma$ the map from $\mathcal{B}%
\left(  \left(  u\right)  \right)  $ to $\mathcal{F}\left(  \left(  u\right)
\right)  $ which is canonically induced by the Boson-Fermion correspondence
$\sigma:\mathcal{B}\rightarrow\mathcal{F}$. Then, of course, this new map
$\sigma:\mathcal{B}\left(  \left(  u\right)  \right)  \rightarrow
\mathcal{F}\left(  \left(  u\right)  \right)  $ is also an isomorphism. Then,
the equalities $\Gamma\left(  u\right)  =\sigma^{-1}\circ X\left(  u\right)
\circ\sigma$ and $\Gamma^{\ast}\left(  u\right)  =\sigma^{-1}\circ X^{\ast
}\left(  u\right)  \circ\sigma$ (from Definition \ref{def.euler.XGamma}) are
not just abbreviations for termwise equalities (as we explained them back in
Definition \ref{def.euler.XGamma}), but also hold literally (if we interpret
$\sigma$ to mean our isomorphism $\sigma:\mathcal{B}\left(  \left(  u\right)
\right)  \rightarrow\mathcal{F}\left(  \left(  u\right)  \right)  $ rather
than the original Boson-Fermion correspondence $\sigma:\mathcal{B}%
\rightarrow\mathcal{F}$). As a consequence, $\sigma\circ\Gamma\left(
u\right)  =X\left(  u\right)  \circ\sigma$ and $\sigma\circ\Gamma^{\ast
}\left(  u\right)  =X^{\ast}\left(  u\right)  \circ\sigma$. Thus,%
\[
\sigma\left(  \Gamma\left(  u\right)  \tau\right)  =\underbrace{\left(
\sigma\circ\Gamma\left(  u\right)  \right)  }_{=X\left(  u\right)  \circ
\sigma}\tau=\left(  X\left(  u\right)  \circ\sigma\right)  \tau=X\left(
u\right)  \sigma\left(  \tau\right)
\]
and%
\[
\sigma\left(  \Gamma^{\ast}\left(  u\right)  \tau\right)  =\underbrace{\left(
\sigma\circ\Gamma^{\ast}\left(  u\right)  \right)  }_{=X^{\ast}\left(
u\right)  \circ\sigma}\tau=\left(  X^{\ast}\left(  u\right)  \circ
\sigma\right)  \tau=X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  ,
\]
so that%
\[
\underbrace{X\left(  u\right)  \sigma\left(  \tau\right)  }_{=\sigma\left(
\Gamma\left(  u\right)  \tau\right)  }\otimes\underbrace{X^{\ast}\left(
u\right)  \sigma\left(  \tau\right)  }_{=\sigma\left(  \Gamma^{\ast}\left(
u\right)  \tau\right)  }=\left(  \sigma\left(  \Gamma\left(  u\right)
\tau\right)  \otimes\sigma\left(  \Gamma^{\ast}\left(  u\right)  \tau\right)
\right)  =\left(  \sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  .
\]
Now,
\begin{align*}
S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)  \right)
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\underbrace{\left(  X\left(  u\right)  \sigma\left(  \tau\right)  \otimes
X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  \right)  }_{=\left(
\sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)  \tau\otimes
\Gamma^{\ast}\left(  u\right)  \tau\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{KdV.S=CT}), applied to }%
\omega=\sigma\left(  \tau\right)  \text{ and }\rho=\sigma\left(  \tau\right)
\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\left(  \left(  \sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  \right) \\
&  =\underbrace{\left(  \operatorname*{CT}\nolimits_{u}\circ\Omega
_{\mathcal{F},\mathcal{F},u}\circ\left(  \sigma\otimes\sigma\right)  \right)
}_{\substack{=\sigma\circ\operatorname*{CT}\nolimits_{u}\circ\Omega
_{\mathcal{B},\mathcal{B},u}\\\text{(since }\operatorname*{CT}\nolimits_{u}%
\text{ and }\Omega_{A,B,u}\text{ are functorial)}}}\left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right) \\
&  =\left(  \sigma\circ\operatorname*{CT}\nolimits_{u}\circ\Omega
_{\mathcal{B},\mathcal{B},u}\right)  \left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right) \\
&  =\sigma\left(  \operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B}%
,\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  \right)  .
\end{align*}
Therefore, the equation $S\left(  \sigma\left(  \tau\right)  \otimes
\sigma\left(  \tau\right)  \right)  =0$ is equivalent to $\sigma\left(
\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)
\tau\right)  \right)  \right)  =0$. This latter equation, in turn, is
equivalent to $\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B}%
,\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  =0$ (since $\sigma$ is an isomorphism). This,
in turn, is equivalent to $\left(  z^{-1}\otimes z\right)  \cdot
\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)
\tau\right)  \right)  =0$ (because $z^{-1}\otimes z$ is an invertible element
of $\mathcal{B}\otimes\mathcal{B}$). Since%
\begin{align*}
&  \left(  z^{-1}\otimes z\right)  \cdot\operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau
\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \underbrace{\left(  z^{-1}\otimes
z\right)  \cdot\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)
}_{\substack{=\Omega_{\mathcal{B},\mathcal{B},u}\left(  \left(  z^{-1}\otimes
z\right)  \left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  \\\text{(since }\Omega_{\mathcal{B}%
,\mathcal{B},u}\text{ is }\mathcal{B}\otimes\mathcal{B}\text{-linear)}%
}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{CT}\nolimits_{u}%
\text{ is }\mathcal{B}\otimes\mathcal{B}\text{-linear (by Remark
\ref{rmk.OMEGA.linear})}\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\underbrace{\left(  \left(  z^{-1}\otimes z\right)  \left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)
}_{=z^{-1}\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(  u\right)
\tau}\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  ,
\end{align*}
this is equivalent to $\operatorname*{CT}\nolimits_{u}\left(  \Omega
_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes
z\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  =0$. Let us combine what
we have proven: We have proven the equivalence of assertions
\begin{equation}
\left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0\right)  \ \Longleftrightarrow\ \left(  \operatorname*{CT}%
\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}%
\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(  u\right)
\tau\right)  \right)  =0\right)  . \label{pf.hirota.firstrewriting}%
\end{equation}


Now, let us simplify $\operatorname*{CT}\nolimits_{u}\left(  \Omega
_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes
z\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  $.

For this, we recall that $\mathcal{B}^{\left(  0\right)  }=\widetilde{F}%
=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $. Thus, the elements of
$\mathcal{B}^{\left(  0\right)  }$ are polynomials in the countably many
indeterminates $x_{1}$, $x_{2}$, $x_{3}$, $...$. We are going to interpret the
elements of $\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }$ as polynomials in "twice as many" indeterminates; by this we mean
the following:

\begin{Convention}
\label{conv.hirota.x'x''}Let $\left(  x_{1}^{\prime},x_{2}^{\prime}%
,x_{3}^{\prime},...\right)  $ and $\left(  x_{1}^{\prime\prime},x_{2}%
^{\prime\prime},x_{3}^{\prime\prime},...\right)  $ be two countable families
of new symbols. We denote the family $\left(  x_{1}^{\prime},x_{2}^{\prime
},x_{3}^{\prime},...\right)  $ by $x^{\prime}$, and we denote the family
$\left(  x_{1}^{\prime\prime},x_{2}^{\prime\prime},x_{3}^{\prime\prime
},...\right)  $ by $x^{\prime\prime}$. Thus, if $P\in\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $, we will denote by $P\left(  x^{\prime
}\right)  $ the polynomial $P\left(  x_{1}^{\prime},x_{2}^{\prime}%
,x_{3}^{\prime},...\right)  $, and we will denote by $P\left(  x^{\prime
\prime}\right)  $ the polynomial $P\left(  x_{1}^{\prime\prime},x_{2}%
^{\prime\prime},x_{3}^{\prime\prime},...\right)  $.

The $\mathbb{C}$-linear map%
\begin{align*}
\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }  &
\rightarrow\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}%
^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]
,\\
P\otimes Q  &  \mapsto P\left(  x^{\prime}\right)  Q\left(  x^{\prime\prime
}\right)
\end{align*}
is a $\mathbb{C}$-algebra isomorphism. By means of this isomorphism, we are
going to identify $\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }$ with $\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime}%
,x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime
},...\right]  $.
\end{Convention}

Another convention:

\begin{Convention}
\label{conv.hirota.P(y)}For any $P\in\mathcal{B}\left(  \left(  u\right)
\right)  $ and any family $\left(  y_{1},y_{2},y_{3},...\right)  $ of elements
of a $\mathbb{C}\left(  \left(  u\right)  \right)  $-algebra $A$, we define an
element $P\left(  y_{1},y_{2},y_{3},...\right)  $ of $A$ as follows: Write $P$
in the form $P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$ for some
$P_{i}\in\mathcal{B}$, and set $P\left(  y_{1},y_{2},y_{3},...\right)
=\sum\limits_{i\in\mathbb{Z}}P_{i}\left(  y_{1},y_{2},y_{3},...\right)  \cdot
u^{i}$. (In words, $P\left(  y_{1},y_{2},y_{3},...\right)  $ is defined by
substituting $y_{1},y_{2},y_{3},...$ for the variables $x_{1},x_{2},x_{3},...$
in $P$ while keeping the variable $u$ unchanged).
\end{Convention}

Now, let us notice that:

\begin{lemma}
\label{lem.hirota.PQ}For any $P\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $ and $Q\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $, we have%
\[
\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)  =P\left(
x^{\prime}\right)  \cdot Q\left(  x^{\prime\prime}\right)
\]
(where $P\left(  x^{\prime}\right)  $ and $Q\left(  x^{\prime\prime}\right)  $
are to be understood according to Convention \ref{conv.hirota.P(y)} and
Convention \ref{conv.hirota.x'x''}, and where $\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }$ is identified with $\mathbb{C}%
\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime
\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  $ according to
Convention \ref{conv.hirota.x'x''}).
\end{lemma}

\textit{Proof of Lemma \ref{lem.hirota.PQ}.} Let $P\in\mathcal{B}^{\left(
0\right)  }\left(  \left(  u\right)  \right)  $ and $Q\in\mathcal{B}^{\left(
0\right)  }\left(  \left(  u\right)  \right)  $. Write $P$ in the form
$P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$ for some $P_{i}\in
\mathcal{B}^{\left(  0\right)  }$. Write $Q$ in the form $Q=\sum
\limits_{i\in\mathbb{Z}}Q_{i}\cdot u^{i}$ for some $Q_{i}\in\mathcal{B}%
^{\left(  0\right)  }$. Since $P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$
and $Q=\sum\limits_{i\in\mathbb{Z}}Q_{i}\cdot u^{i}$, we have
\begin{align*}
\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)   &
=\Omega_{\mathcal{B},\mathcal{B},u}\left(  \left(  \sum\limits_{i\in
\mathbb{Z}}P_{i}\cdot u^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{Z}}Q_{i}\cdot u^{i}\right)  \right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}%
}\underbrace{P_{j}\otimes Q_{i-j}}_{\substack{=P_{j}\left(  x^{\prime}\right)
\cdot Q_{i-j}\left(  x^{\prime\prime}\right)  \\\text{(due to our
identification of}\\\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}%
^{\left(  0\right)  }\text{ with}\\\mathbb{C}\left[  x_{1}^{\prime}%
,x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime}%
,x_{3}^{\prime\prime},...\right]  \text{)}}}\right)  u^{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Omega_{\mathcal{B}%
,\mathcal{B},u}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)
\right)  u^{i}%
\end{align*}
and%
\begin{align*}
P\left(  x^{\prime}\right)  \cdot Q\left(  x^{\prime\prime}\right)   &
=\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}\right)
\left(  x^{\prime}\right)  }_{\substack{=\sum\limits_{i\in\mathbb{Z}}%
P_{i}\left(  x^{\prime}\right)  \cdot u^{i}=\sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot u^{j}\\\text{(here, we renamed }i\text{
as }j\text{)}}}\cdot\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}Q_{i}\cdot
u^{i}\right)  \left(  x^{\prime\prime}\right)  }_{=\sum\limits_{i\in
\mathbb{Z}}Q_{i}\left(  x^{\prime\prime}\right)  \cdot u^{i}}\\
&  =\left(  \sum\limits_{j\in\mathbb{Z}}P_{j}\left(  x^{\prime}\right)  \cdot
u^{j}\right)  \cdot\left(  \sum\limits_{i\in\mathbb{Z}}Q_{i}\left(
x^{\prime\prime}\right)  \cdot u^{i}\right)  =\sum\limits_{j\in\mathbb{Z}}%
\sum\limits_{i\in\mathbb{Z}}P_{j}\left(  x^{\prime}\right)  \cdot u^{j}\cdot
Q_{i}\left(  x^{\prime\prime}\right)  \cdot u^{i}\\
&  =\sum\limits_{j\in\mathbb{Z}}\sum\limits_{i\in\mathbb{Z}}P_{j}\left(
x^{\prime}\right)  \cdot\underbrace{u^{j}\cdot Q_{i-j}\left(  x^{\prime\prime
}\right)  \cdot u^{i-j}}_{=Q_{i-j}\left(  x^{\prime\prime}\right)  \cdot
u^{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i-j\text{ for
}i\text{ in the second sum}\right) \\
&  =\sum\limits_{j\in\mathbb{Z}}\sum\limits_{i\in\mathbb{Z}}P_{j}\left(
x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)  \cdot
u^{i}=\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)
\right)  u^{i}=\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)  .
\end{align*}
This proves Lemma \ref{lem.hirota.PQ}.

Now, Theorem \ref{thm.euler} (applied to $m=0$) yields%
\begin{align}
\Gamma\left(  u\right)   &  =uz\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\label{pf.hirota.2}\\
\Gamma^{\ast}\left(  u\right)   &  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  \label{pf.hirota.3}%
\end{align}
on $\mathcal{B}^{\left(  0\right)  }$. Thus,%
\begin{align*}
z^{-1}\Gamma\left(  u\right)  \tau &  =z^{-1}uz\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  \tau\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.hirota.2})}\right) \\
&  =u\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\\
&  =u\exp\left(  \sum\limits_{j>0}\dfrac{jx_{j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{\dfrac{\partial}{\partial x_{j}}}%
{j}u^{-j}\right)  \tau\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{j}\text{ acts as }\dfrac{\partial}{\partial x_{j}}\text{ on
}\widetilde{F}\text{ for every }j>0\text{,}\\
\text{ and since }a_{-j}\text{ acts as }jx_{j}\text{ on }\widetilde{F}\text{
for every }j>0
\end{array}
\right) \\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau,
\end{align*}
so that%
\begin{align}
\left(  z^{-1}\Gamma\left(  u\right)  \tau\right)  \left(  x^{\prime}\right)
&  =\left(  u\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau\right)  \left(  x^{\prime}\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)  .
\label{pf.hirota.5}%
\end{align}
Also,%
\begin{align*}
z\Gamma^{\ast}\left(  u\right)  \tau &  =zz^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.hirota.3})}\right) \\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{jx_{j}}{j}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{\dfrac{\partial}{\partial x_{j}}}{j}%
u^{-j}\right)  \tau\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{j}\text{ acts as }\dfrac{\partial}{\partial x_{j}}\text{ on
}\widetilde{F}\text{ for every }j>0\text{, and }\\
\text{since }a_{-j}\text{ acts as }jx_{j}\text{ on }\widetilde{F}\text{ for
every }j>0
\end{array}
\right) \\
&  =\exp\left(  -\sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau,
\end{align*}
so that%
\begin{align}
\left(  z\Gamma^{\ast}\left(  u\right)  \tau\right)  \left(  x^{\prime\prime
}\right)   &  =\left(  \exp\left(  -\sum\limits_{j>0}x_{j}u^{j}\right)
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}%
}u^{-j}\right)  \tau\right)  \left(  x^{\prime\prime}\right) \nonumber\\
&  =\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  . \label{pf.hirota.6}%
\end{align}


Now,%
\begin{align}
&  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)
\tau\otimes z\Gamma^{\ast}\left(  u\right)  \tau\right) \nonumber\\
&  =\left(  z^{-1}\Gamma\left(  u\right)  \tau\right)  \left(  x^{\prime
}\right)  \cdot\left(  z\Gamma^{\ast}\left(  u\right)  \tau\right)  \left(
x^{\prime\prime}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.hirota.PQ}, applied to
}P=z^{-1}\Gamma\left(  u\right)  \tau\text{ and }Q=z\Gamma^{\ast}\left(
u\right)  \tau\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime
}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(
x^{\prime\prime}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.5}) and
(\ref{pf.hirota.6})}\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}%
\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(
x^{\prime}\right)  \right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime\prime}\right)  \right)  . \label{pf.hirota.7}%
\end{align}


We are going to rewrite the right hand side of this equality. First of all,
notice that Theorem \ref{thm.exp(u+v)} (applied to $R=\left(  \mathcal{B}%
^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(
\left(  u\right)  \right)  $, \newline$I=\left(  \text{closure of the ideal of
}R\text{ generated by }x_{j}^{\prime}\text{ and }x_{j}^{\prime\prime}\text{
with }j\text{ ranging over all positive integers}\right)  $, $\alpha
=\sum\limits_{j>0}x_{j}^{\prime}u^{j}$ and $\beta=-\sum\limits_{j>0}%
x_{j}^{\prime\prime}u^{j}$) yields%
\[
\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}+\left(  -\sum\limits_{j>0}%
x_{j}^{\prime\prime}u^{j}\right)  \right)  =\exp\left(  \sum\limits_{j>0}%
x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}x_{j}%
^{\prime\prime}u^{j}\right)  .
\]
Thus,%
\begin{align}
\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)   &  =\exp
\underbrace{\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}+\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)  \right)  }_{=\sum
\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}^{\prime\prime}\right)
}\nonumber\\
&  =\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  . \label{pf.hirota.8}%
\end{align}


Now, let us recall a very easy fact: If $\phi$ is an endomorphism of a vector
space $V$, and $v$ is a vector in $V$ such that $\phi v=0$, then $\left(
\exp\phi\right)  v$ is well-defined (in the sense that the power series
$\sum\limits_{n\geq0}\dfrac{1}{n!}\phi^{n}v$ converges) and satisfies $\left(
\exp\phi\right)  v=v$. Applying this fact to $V=\left(  \mathcal{B}^{\left(
0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(  \left(
u\right)  \right)  $, $\phi=\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}$ and $v=\tau\left(  x^{\prime}\right)
$, we see that $\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime
}\right)  \right)  $ is well-defined and satisfies
\begin{equation}
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)
\right)  =\tau\left(  x^{\prime}\right)  \label{pf.hirota.9}%
\end{equation}
(since $\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)
\right)  =\sum\limits_{j>0}\dfrac{1}{j}\underbrace{\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\left(  \tau\left(  x^{\prime}\right)  \right)  }%
_{=0}u^{-j}=0$). The same argument (with $x_{j}^{\prime}$ and $x_{j}%
^{\prime\prime}$ switching places) shows that $\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime\prime}\right)  \right)  $ is well-defined and satisfies%
\begin{equation}
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime}\right)
\right)  =\tau\left(  x^{\prime\prime}\right)  . \label{pf.hirota.10}%
\end{equation}


Now,%
\begin{align}
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)   &  =\exp\left(  \left(  -\sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  +\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}%
u^{-j}\right) \nonumber\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\label{pf.hirota.11}%
\end{align}
\footnote{Here, the last equality sign follows from Theorem \ref{thm.exp(u+v)}%
, applied to
\begin{align*}
R  &  =\left(
\begin{array}
[c]{c}%
\text{closure of the }\mathbb{C}\left(  \left(  u\right)  \right)
\text{-subalgebra of }\operatorname*{End}\nolimits_{\mathbb{C}\left(  \left(
u\right)  \right)  }\left(  \left(  \mathcal{B}^{\left(  0\right)  }%
\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(  \left(  u\right)
\right)  \right) \\
\text{generated by }\dfrac{\partial}{\partial x_{j}^{\prime}}\text{ and
}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\text{ with }j\text{ ranging
over all positive integers}%
\end{array}
\right)  ,\\
I  &  =\left(
\begin{array}
[c]{c}%
\text{closure of the ideal of }R\text{ generated by }\dfrac{\partial}{\partial
x_{j}^{\prime}}\text{ and }\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\text{ with}\\
j\text{ ranging over all positive integers}%
\end{array}
\right)  ,\\
\alpha &  =-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j},\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \beta
=\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}u^{-j}.
\end{align*}
} and similarly%
\begin{equation}
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)  =\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ\exp\left(  -\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  . \label{pf.hirota.12}%
\end{equation}


But since $-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  $ is a derivation, $\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  $ is a $\mathbb{C}$-algebra
homomorphism (since exponentials of derivations are $\mathbb{C}$-algebra
homomorphisms), so that%
\begin{align*}
&  \exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)  \left(  \tau\left(  x^{\prime}\right)  \tau\left(
x^{\prime\prime}\right)  \right) \\
&  =\underbrace{\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  }_{\substack{=\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \\\text{(by (\ref{pf.hirota.11}%
))}}}\left(  \tau\left(  x^{\prime}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}%
-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)  \right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ\exp\left(  -\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \\\text{(by (\ref{pf.hirota.12}))}}}\left(  \tau\left(
x^{\prime\prime}\right)  \right) \\
&  =\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime}}u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\right)  \left(  \tau\left(  x^{\prime}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\left(  \exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ
\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right) \\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \underbrace{\left(  \exp\left(  \sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)  \right)
}_{\substack{=\tau\left(  x^{\prime}\right)  \\\text{(by (\ref{pf.hirota.9}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}%
\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\underbrace{\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  \right)  }_{\substack{=\tau\left(  x^{\prime\prime}\right)
\\\text{(by (\ref{pf.hirota.10}))}}}\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  .
\end{align*}
Hence, (\ref{pf.hirota.7}) becomes%
\begin{align}
&  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)
\tau\otimes z\Gamma^{\ast}\left(  u\right)  \tau\right) \nonumber\\
&  =u\underbrace{\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}%
-x_{j}^{\prime\prime}\right)  \right)  \\\text{(by (\ref{pf.hirota.8}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime}\right)  \right)  \cdot\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\left(  \tau\left(  x^{\prime\prime}\right)  \right)  }_{=\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial
x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)
\right)  \left(  \tau\left(  x^{\prime}\right)  \tau\left(  x^{\prime\prime
}\right)  \right)  }\nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  \cdot\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}%
-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)  \right)  \left(
\tau\left(  x^{\prime}\right)  \tau\left(  x^{\prime\prime}\right)  \right)  .
\label{pf.hirota.15}%
\end{align}
Thus, (\ref{pf.hirota.firstrewriting}) rewrites as%
\begin{align}
&  \left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(
\tau\right)  \right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \operatorname*{CT}\nolimits_{u}\left(
u\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}^{\prime\prime
}\right)  \right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  \left(  \tau\left(  x^{\prime}\right)
\tau\left(  x^{\prime\prime}\right)  \right)  \right)  =0\right)  .
\label{pf.hirota.secondrewriting}%
\end{align}
This already gives a criterion for a $\tau\in\mathcal{B}^{\left(  0\right)  }$
to satisfy $\sigma\left(  \tau\right)  \in\Omega$, but it is yet a rather
messy one. We are going to simplify it in the following. First, we do a
substitution of variables:

\begin{Convention}
Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of new symbols. We
identify the $\mathbb{C}$-algebra $\mathbb{C}\left[  x_{1},y_{1},x_{2}%
,y_{2},x_{3},y_{3},...\right]  $ with the $\mathbb{C}$-algebra $\mathbb{C}%
\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime
\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  =\mathcal{B}^{\left(
0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }$ by the following
substitution:%
\begin{align*}
x_{j}^{\prime}  &  =x_{j}-y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j>0;\\
x_{j}^{\prime\prime}  &  =x_{j}+y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j>0.
\end{align*}


If we define the sum and the difference of two sequences by componentwise
addition resp. subtraction, then this rewrites as follows:%
\begin{align*}
x^{\prime}  &  =x-y;\\
x^{\prime\prime}  &  =x+y.
\end{align*}

\end{Convention}

It is now easy to see that%
\[
x_{j}^{\prime}-x_{j}^{\prime\prime}=-2y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every
}j>0,
\]
and%
\[
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}=-\dfrac{\partial}{\partial y_{j}}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j>0
\]
(where $\dfrac{\partial}{\partial x_{j}^{\prime}}$ and $\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}$ mean differentiation over the variables
$x_{j}^{\prime}$ and $x_{j}^{\prime\prime}$ in the polynomial ring
$\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime}%
,x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  $,
whereas $\dfrac{\partial}{\partial y_{j}}$ means differentiation over the
variable $y_{j}$ in the polynomial ring $\mathbb{C}\left[  x_{1},y_{1}%
,x_{2},y_{2},x_{3},y_{3},...\right]  $). As a consequence,%
\begin{align*}
&  u\exp\left(  \sum\limits_{j>0}u^{j}\underbrace{\left(  x_{j}^{\prime}%
-x_{j}^{\prime\prime}\right)  }_{=-2y_{j}}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\underbrace{\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  }_{=-\dfrac{\partial}{\partial y_{j}}}\right)  \left(  \tau\left(
\underbrace{x^{\prime}}_{=x-y}\right)  \tau\left(  \underbrace{x^{\prime
\prime}}_{=x+y}\right)  \right) \\
&  =u\exp\left(  -2\sum\limits_{j>0}u^{j}y_{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial}{\partial y_{j}}\right)
\left(  \tau\left(  x-y\right)  \tau\left(  x+y\right)  \right)  .
\end{align*}
Hence, (\ref{pf.hirota.secondrewriting}) rewrites as%
\begin{align}
&  \left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(
\tau\right)  \right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \operatorname*{CT}\nolimits_{u}\left(
u\exp\left(  -2\sum\limits_{j>0}u^{j}y_{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial}{\partial y_{j}}\right)
\left(  \tau\left(  x-y\right)  \tau\left(  x+y\right)  \right)  \right)
=0\right)  . \label{pf.hirota.thirdrewriting}%
\end{align}


To simplify this even further, a new notation is needed:

\begin{definition}
\label{def.hirota.A(P,f,g)}Let $K$ be a commutative ring. Let $\left(
x_{1},x_{2},x_{3},...\right)  $, $\left(  z_{1},z_{2},z_{3},...\right)  $, and
$\left(  w_{1},w_{2},w_{3},...\right)  $ be three disjoint families of
indeterminates. Denote by $x$ the family $\left(  x_{1},x_{2},x_{3}%
,...\right)  $, and denote by $z$ the family $\left(  z_{1},z_{2}%
,z_{3},...\right)  $.

\textbf{(a)} For any polynomial $r\in K\left[  x_{1},x_{2},x_{3}%
,...,z_{1},z_{2},z_{3},...\right]  $, let $r\mid_{z=0}$ denote the polynomial
in $K\left[  x_{1},x_{2},x_{3},...\right]  $ obtained by substituting $\left(
0,0,0,...\right)  $ for $\left(  z_{1},z_{2},z_{3},...\right)  $ in $P$.

\textbf{(b)} Consider the differential operators $\dfrac{\partial}{\partial
z_{1}},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}},...$
on $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $. For any
power series $P\in K\left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]  $,
let $P\left(  \partial_{z}\right)  $ mean the value of $P$ when applied to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $ (that is, the result of
substituting $\dfrac{\partial}{\partial z_{j}}$ for each $w_{j}$ in $P$). This
value is a well-defined differential operator on $K\left[  x_{1},x_{2}%
,x_{3},...,z_{1},z_{2},z_{3},...\right]  $ (due to Remark
\ref{rmk.hirota.welldef} below).

\textbf{(c)} For any power series $P\in K\left[  \left[  w_{1},w_{2}%
,w_{3},...\right]  \right]  $ and any two polynomials $f\in K\left[
x_{1},x_{2},x_{3},...\right]  $ and $g\in K\left[  x_{1},x_{2},x_{3}%
,...\right]  $, define a polynomial $A\left(  P,f,g\right)  \in K\left[
x_{1},x_{2},x_{3},...\right]  $ by
\[
A\left(  P,f,g\right)  =\left(  P\left(  \partial_{z}\right)  \left(  f\left(
x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}.
\]

\end{definition}

\begin{remark}
\label{rmk.hirota.welldef}Let $K$ be a commutative ring. Let $\left(
x_{1},x_{2},x_{3},...\right)  $, $\left(  z_{1},z_{2},z_{3},...\right)  $, and
$\left(  w_{1},w_{2},w_{3},...\right)  $ be three disjoint families of
indeterminates. Let $P\in K\left[  \left[  w_{1},w_{2},w_{3},...\right]
\right]  $ be a power series. Then, if we apply the power series $P$ to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $, we obtain a
well-defined endomorphism of $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2}%
,z_{3},...\right]  $.
\end{remark}

\textit{Proof of Remark \ref{rmk.hirota.welldef}.} Let $\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ be defined as in
Convention \ref{conv.fin}. Write the power series $P$ in the form
\[
P=\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }w_{1}^{i_{1}}w_{2}^{i_{2}}w_{3}^{i_{3}}...
\]
for $\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }\in K$. Then, if we apply
the power series $P$ to the family $\left(  \dfrac{\partial}{\partial z_{1}%
},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}%
},...\right)  $, we obtain%
\[
\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\left(  \dfrac{\partial}{\partial z_{1}%
}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}}\right)  ^{i_{2}%
}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}}....
\]
In order to prove that this is a well-defined endomorphism of $K\left[
x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $, we must prove that for
every $r\in K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $,
the sum%
\[
\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\left(  \left(  \dfrac{\partial}{\partial
z_{1}}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}}\right)
^{i_{2}}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}}...\right)
r
\]
is well-defined, i. e., has only finitely many nonzero addends. But this is
clear, because only finitely many $\left(  i_{1},i_{2},i_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfy
$\left(  \left(  \dfrac{\partial}{\partial z_{1}}\right)  ^{i_{1}}\left(
\dfrac{\partial}{\partial z_{2}}\right)  ^{i_{2}}\left(  \dfrac{\partial
}{\partial z_{3}}\right)  ^{i_{3}}...\right)  r\neq0$ \ \ \ \ \footnote{This
is because $r$ is a polynomial, so that only finitely many variables occur in
$r$, and the degrees of the monomials of $r$ are bounded from above.}. Hence,
we have proven that the sum $\sum\limits_{\left(  i_{1},i_{2},i_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}}\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }\left(  \dfrac{\partial
}{\partial z_{1}}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}%
}\right)  ^{i_{2}}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}%
}...$ is a well-defined endomorphism of $K\left[  x_{1},x_{2},x_{3}%
,...,z_{1},z_{2},z_{3},...\right]  $. Since this sum is the result of applying
the power series $P$ to the family $\left(  \dfrac{\partial}{\partial z_{1}%
},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}%
},...\right)  $, we thus conclude that applying the power series $P$ to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $ yields a well-defined
endomorphism of $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]
$. Remark \ref{rmk.hirota.welldef} is proven.

\textbf{Example:} If $P\left(  w\right)  =w_{1}$ (the first variable), then%
\[
A\left(  P,f,g\right)  =\left(  \dfrac{\partial}{\partial z_{1}}\left(
f\left(  x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}%
=-\dfrac{\partial f}{\partial x_{1}}g+\dfrac{\partial g}{\partial x_{1}}f.
\]


\begin{lemma}
For any three polynomials $P,f,g$, we have $A\left(  P,f,g\right)  =A\left(
P_{-},g,f\right)  $, where $P_{-}\left(  w\right)  =P\left(  -w\right)  $.
\end{lemma}

\begin{corollary}
\label{cor.hirota.odd}For any two polynomials $P$ and $f$, we have $A\left(
P,f,f\right)  =0$ if $P$ is odd.
\end{corollary}

This is clear from the definition.

We now state the so-called \textit{Hirota bilinear relations}, which are a
simplified version of (\ref{pf.hirota.thirdrewriting}):

\begin{theorem}
[Hirota bilinear relations]\label{thm.hirota}Let $\tau\in\mathcal{B}^{\left(
0\right)  }$ be a nonzero vector. Let $\left(  y_{1},y_{2},y_{3},...\right)  $
and $\left(  w_{1},w_{2},w_{3},...\right)  $ be two families of new symbols.
Let $\widetilde{w}$ denote the sequence $\left(  \dfrac{w_{1}}{1},\dfrac
{w_{2}}{2},\dfrac{w_{3}}{3},...\right)  $. Define the elementary Schur
polynomials $S_{k}$ as in Definition \ref{def.schur.Sk}.

Then, $\sigma\left(  \tau\right)  \in\Omega$ if and only if%
\begin{equation}
A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right)  =0, \label{thm.hirota.eqn}%
\end{equation}
where the term $A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  $ is to be interpreted by applying Definition
\ref{def.hirota.A(P,f,g)} \textbf{(c)} to $K=\mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  $ (since $\sum\limits_{j=0}^{\infty
}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(
\sum\limits_{s>0}y_{s}w_{s}\right)  \in\left(  \mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  \right)  \left[  \left[  w_{1}%
,w_{2},w_{3},...\right]  \right]  $ and $\tau\in\mathcal{B}^{\left(  0\right)
}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \subseteq\left(
\mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  \right)
\left[  x_{1},x_{2},x_{3},...\right]  $).
\end{theorem}

Before we prove this, we need a simple lemma about polynomials:

\begin{lemma}
\label{lem.hirota.y+z}Let $K$ be a commutative $\mathbb{Q}$-algebra. Let
$\left(  y_{1},y_{2},y_{3},...\right)  $ and $\left(  z_{1},z_{2}%
,z_{3},...\right)  $ be two sequence of new symbols. Denote the sequence
$\left(  y_{1},y_{2},y_{3},...\right)  $ by $y$. Denote the sequence $\left(
z_{1},z_{2},z_{3},...\right)  $ by $z$. Denote by $\widetilde{\partial_{y}}$
the sequence $\left(  \dfrac{1}{1}\dfrac{\partial}{\partial y_{1}},\dfrac
{1}{2}\dfrac{\partial}{\partial y_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial
y_{3}},...\right)  $ of endomorphisms of $\left(  K\left[  \left[  y_{1}%
,y_{2},y_{3},...\right]  \right]  \right)  \left[  z_{1},z_{2},z_{3}%
,...\right]  $. Denote by $\widetilde{\partial_{z}}$ the sequence $\left(
\dfrac{1}{1}\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial
}{\partial z_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $
of $\left(  K\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  \right)
\left[  z_{1},z_{2},z_{3},...\right]  $. Let $P$ and $Q$ be two elements of
$K\left[  w_{1},w_{2},w_{3},...\right]  $ (where $\left(  w_{1},w_{2}%
,w_{3},...\right)  $ is a further sequence of new symbols). Then,%
\[
Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(  y+z\right)
\right)  =Q\left(  \widetilde{\partial_{z}}\right)  \left(  P\left(
y+z\right)  \right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.hirota.y+z}.} Let $D$ be the $K$-subalgebra of
$\operatorname*{End}\left(  \left(  K\left[  \left[  y_{1},y_{2}%
,y_{3},...\right]  \right]  \right)  \left[  z_{1},z_{2},z_{3},...\right]
\right)  $ generated by $\dfrac{\partial}{\partial y_{1}},\dfrac{\partial
}{\partial y_{2}},\dfrac{\partial}{\partial y_{3}},...,\dfrac{\partial
}{\partial z_{1}},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial
z_{3}},...$. Then, clearly, $D$ is a commutative $K$-algebra (since its
generators commute), and all elements of the sequences $\widetilde{\partial
_{y}}$ and $\widetilde{\partial_{z}}$ lie in $D$ (since $\widetilde{\partial
_{y}}=\left(  \dfrac{1}{1}\dfrac{\partial}{\partial y_{1}},\dfrac{1}{2}%
\dfrac{\partial}{\partial y_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial y_{3}%
},...\right)  $ and $\widetilde{\partial_{z}}=\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial z_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $).

Let $I$ be the ideal of $D$ generated by $\dfrac{\partial}{\partial y_{i}%
}-\dfrac{\partial}{\partial z_{i}}$ with $i$ ranging over the positive
integers. Then, $\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial
z_{i}}\in I$ for every positive integer $i$. Hence, every positive integer $i$
satisfies $\dfrac{1}{i}\dfrac{\partial}{\partial y_{i}}\equiv\dfrac{1}%
{i}\dfrac{\partial}{\partial z_{i}}\operatorname{mod}I$ (since $\dfrac{1}%
{i}\dfrac{\partial}{\partial y_{i}}-\dfrac{1}{i}\dfrac{\partial}{\partial
z_{i}}=\dfrac{1}{i}\underbrace{\left(  \dfrac{\partial}{\partial y_{i}}%
-\dfrac{\partial}{\partial z_{i}}\right)  }_{\in I}\in I$). In other words,
for every positive integer $i$, the $i$-th element of the sequence
$\widetilde{\partial_{y}}$ is congruent to the $i$-th element of the sequence
$\widetilde{\partial_{z}}$ modulo $I$ (since the $i$-th element of the
sequence $\widetilde{\partial_{y}}$ is $\dfrac{1}{i}\dfrac{\partial}{\partial
y_{i}}$, while the $i$-th element of the sequence $\widetilde{\partial_{z}}$
is $\dfrac{1}{i}\dfrac{\partial}{\partial z_{i}}$). Thus, each element of the
sequence $\widetilde{\partial_{y}}$ is congruent to the corresponding element
of the sequence $\widetilde{\partial_{z}}$ modulo $I$. Hence, $Q\left(
\widetilde{\partial_{y}}\right)  \equiv Q\left(  \widetilde{\partial_{z}%
}\right)  \operatorname{mod}I$ (since $Q$ is a polynomial, and $I$ is an
ideal). Hence,
\begin{align*}
&  Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(  \widetilde{\partial
_{z}}\right)  \in I\\
&  =\left(  \text{ideal of }D\text{ generated by }\dfrac{\partial}{\partial
y_{i}}-\dfrac{\partial}{\partial z_{i}}\text{ with }i\text{ ranging over the
positive integers}\right)  .
\end{align*}
In other words, $Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  $ is a $D$-linear combinations of terms of
the form $\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}$
with $i$ ranging over the positive integers. Thus, we can write $Q\left(
\widetilde{\partial_{y}}\right)  -Q\left(  \widetilde{\partial_{z}}\right)  $
in the form $Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  =\sum\limits_{i>0}d_{i}\circ\left(
\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  $,
where each $d_{i}$ is an element of $D$, and all but finitely many $i>0$
satisfy $d_{i}=0$. Consider these $d_{i}$.

But it is easy to see that%
\begin{equation}
\text{every positive integer }i\text{ satisfies }\left(  \dfrac{\partial
}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  \left(  P\left(
y+z\right)  \right)  =0. \label{pf.hirota.y+z.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.hirota.y+z.1}):} Let $i$ be a positive
integer. Let us identify $\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $
with $\left(  \mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2}%
,...\right]  \right)  \left[  w_{i}\right]  $. Then, $P\in\mathbb{C}\left[
w_{1},w_{2},w_{3},...\right]  =\left(  \mathbb{C}\left[  w_{1},w_{2}%
,...,w_{i-1},w_{i+1},w_{i+2},...\right]  \right)  \left[  w_{i}\right]  $, so
that we can write $P$ as a polynomial in the variable $w_{i}$ over the ring
$\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $. In
other words, we can write $P$ in the form $P=\sum\limits_{n\in\mathbb{N}}%
p_{n}w_{i}^{n}$, where every $n\in\mathbb{N}$ satisfies $p_{n}\in
\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $ and
all but finitely many $n\in\mathbb{N}$ satisfy $p_{n}=0$. Consider these
$p_{n}$.
\par
Let $n\in\mathbb{N}$ be arbitrary. Consider $p_{n}\in\mathbb{C}\left[
w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $ as an element of
$\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $ (by means of the canonical
embedding $\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2}%
,...\right]  \subseteq\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $).
Then, $p_{n}$ is a polynomial in which the variable $w_{i}$ does not occur.
Hence, $p_{n}\left(  y+z\right)  $ is a polynomial in which neither of the
variables $y_{i}$ and $z_{i}$ occur. Thus, $\dfrac{\partial}{\partial y_{i}%
}\left(  p_{n}\left(  y+z\right)  \right)  =0$ and $\dfrac{\partial}{\partial
z_{i}}\left(  p_{n}\left(  y+z\right)  \right)  =0$.
\par
On the other hand, it is very easy to check that $\dfrac{\partial}{\partial
y_{i}}\left(  y_{i}+z_{i}\right)  ^{n}=\dfrac{\partial}{\partial z_{i}}\left(
y_{i}+z_{i}\right)  ^{n}$ (in fact, this is obvious in the case when $n=0$,
and in every other case follows from $\dfrac{\partial}{\partial y_{i}}\left(
y_{i}+z_{i}\right)  ^{n}=n\left(  y_{i}+z_{i}\right)  ^{n-1}$ and
$\dfrac{\partial}{\partial y_{i}}\left(  y_{i}+z_{i}\right)  ^{n}=n\left(
y_{i}+z_{i}\right)  ^{n-1}$). Now, by the Leibniz rule,%
\begin{align*}
\dfrac{\partial}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)   &  =\underbrace{\left(  \dfrac{\partial
}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \right)  \right)
}_{=0=\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)
\right)  }\cdot\left(  y_{i}+z_{i}\right)  ^{n}+p_{n}\left(  y+z\right)
\cdot\underbrace{\dfrac{\partial}{\partial y_{i}}\left(  y_{i}+z_{i}\right)
^{n}}_{=\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}}\\
&  =\left(  \dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)
\right)  \right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}+p_{n}\left(
y+z\right)  \cdot\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)
^{n}.
\end{align*}
Compared with%
\[
\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)  =\left(  \dfrac{\partial}{\partial z_{i}%
}\left(  p_{n}\left(  y+z\right)  \right)  \right)  \cdot\left(  y_{i}%
+z_{i}\right)  ^{n}+p_{n}\left(  y+z\right)  \cdot\dfrac{\partial}{\partial
z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}%
\]
(this follows from the Leibniz rule), this yields%
\begin{equation}
\dfrac{\partial}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)  =\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)  .
\label{pf.hirota.y+z.2}%
\end{equation}
\par
Now, forget that we fixed $n\in\mathbb{N}$. We have shown that every
$n\in\mathbb{N}$ satisfies (\ref{pf.hirota.y+z.2}). Now, since $P=\sum
\limits_{n\in\mathbb{N}}p_{n}w_{i}^{n}$, we have $P\left(  y+z\right)
=\sum\limits_{n\in\mathbb{N}}p_{n}\left(  y+z\right)  \cdot\left(  y_{i}%
+z_{i}\right)  ^{n}$, so that%
\begin{align*}
&  \left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  \left(  P\left(  y+z\right)  \right) \\
&  =\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  \left(  \sum\limits_{n\in\mathbb{N}}p_{n}\left(  y+z\right)
\cdot\left(  y_{i}+z_{i}\right)  ^{n}\right) \\
&  =\sum\limits_{n\in\mathbb{N}}\underbrace{\dfrac{\partial}{\partial y_{i}%
}\left(  p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)
^{n}\right)  }_{\substack{=\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)
\\\text{(by (\ref{pf.hirota.y+z.2}))}}}-\sum\limits_{n\in\mathbb{N}}%
\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right) \\
&  =\sum\limits_{n\in\mathbb{N}}\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)
-\sum\limits_{n\in\mathbb{N}}\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)  =0.
\end{align*}
This proves (\ref{pf.hirota.y+z.1}).} Thus,%
\begin{align*}
&  Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(  y+z\right)
\right)  -Q\left(  \widetilde{\partial_{z}}\right)  \left(  P\left(
y+z\right)  \right) \\
&  =\underbrace{\left(  Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  \right)  }_{=\sum\limits_{i>0}d_{i}%
\circ\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  }\left(  P\left(  y+z\right)  \right)  =\sum\limits_{i>0}\left(
d_{i}\circ\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial
z_{i}}\right)  \right)  \left(  P\left(  y+z\right)  \right) \\
&  =\sum\limits_{i>0}d_{i}\underbrace{\left(  \left(  \dfrac{\partial
}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  \left(  P\left(
y+z\right)  \right)  \right)  }_{\substack{=0\\\text{(by
(\ref{pf.hirota.y+z.1}))}}}=\sum\limits_{i>0}\underbrace{d_{i}\left(
0\right)  }_{=0}=0.
\end{align*}
In other words, $Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(
y+z\right)  \right)  =Q\left(  \widetilde{\partial_{z}}\right)  \left(
P\left(  y+z\right)  \right)  $. This proves Lemma \ref{lem.hirota.y+z}.

\textit{Proof of Theorem \ref{thm.hirota}.} We introduce a new family of
indeterminates $\left(  z_{1},z_{2},z_{3},...\right)  $. Denote this family by
$z$. (This $z$ has nothing to do with the element $z$ of $\mathcal{B}$. It is
best to forget about $\mathcal{B}$ here, and only think about $\mathcal{B}%
^{\left(  0\right)  }=\mathbb{C}\left[  z_{1},z_{2},z_{3},...\right]  $.)
Denote by $\widetilde{\partial_{z}}$ the sequence $\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial z_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $.

Denote by $\widetilde{\partial_{y}}$ the sequence $\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial y_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial y_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial y_{3}},...\right)  $. Also, let $-2y$
be the sequence $\left(  -2y_{1},-2y_{2},-2y_{3},...\right)  $. Then,%
\begin{align}
\sum\limits_{k=0}^{\infty}S_{k}\left(  -2y\right)  u^{k}  &  =\sum
\limits_{k\geq0}S_{k}\left(  -2y\right)  u^{k}=\exp\left(  \sum\limits_{i\geq
1}-2y_{i}u^{i}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.schur.sk.genfun}), with
}-2y\text{ substituted for }x\text{ and }u\text{ substituted for }z\right)
\nonumber\\
&  =\exp\left(  \sum\limits_{j\geq1}-2y_{j}u^{j}\right)  =\exp\left(
-2\sum\limits_{j>0}u^{j}y_{j}\right)  \label{pf.hirota.29}%
\end{align}
and%
\begin{align}
\sum\limits_{k=0}^{\infty}S_{k}\left(  \widetilde{\partial_{y}}\right)
u^{-k}  &  =\sum\limits_{k\geq0}S_{k}\left(  \widetilde{\partial_{y}}\right)
u^{-k}=\exp\left(  \sum\limits_{i\geq1}\dfrac{1}{i}\dfrac{\partial}{\partial
y_{i}}u^{-i}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.schur.sk.genfun}), with
}\widetilde{\partial_{y}}\text{ substituted for }x\text{ and }u^{-1}\text{
substituted for }z\right) \nonumber\\
&  =\exp\left(  \sum\limits_{j\geq1}\dfrac{1}{j}\dfrac{\partial}{\partial
y_{j}}u^{-j}\right)  =\exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  . \label{pf.hirota.30}%
\end{align}


Applying Lemma \ref{lem.hirota.newton} to $K=\left(  \mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  \right)  \left[  x_{1},x_{2}%
,x_{3},...\right]  $ and $P=\tau\left(  x+z\right)  \tau\left(  x-z\right)  $,
we obtain%
\begin{equation}
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
\left(  \tau\left(  x+z\right)  \tau\left(  x-z\right)  \right)  =\tau\left(
x+y+z\right)  \tau\left(  x-y-z\right)  . \label{pf.hirota.31}%
\end{equation}


Now,%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  u\exp\left(  -2\sum\limits_{j>0}%
u^{j}y_{j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  \left(  \tau\left(  x-y\right)
\tau\left(  x+y\right)  \right)  \right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  u\underbrace{\exp\left(
-2\sum\limits_{j>0}u^{j}y_{j}\right)  }_{\substack{=\sum\limits_{k=0}^{\infty
}S_{k}\left(  -2y\right)  u^{k}\\\text{(by (\ref{pf.hirota.29}))}%
}}\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial
}{\partial y_{j}}\right)  }_{\substack{=\sum\limits_{k=0}^{\infty}S_{k}\left(
\widetilde{\partial_{y}}\right)  u^{-k}\\\text{(by (\ref{pf.hirota.30}))}%
}}\left(  \tau\left(  x+y+z\right)  \tau\left(  x-y-z\right)  \right)
\right)  \mid_{z=0}\\
&  =\operatorname*{CT}\nolimits_{u}\left(  u\left(  \sum\limits_{k=0}^{\infty
}S_{k}\left(  -2y\right)  u^{k}\right)  \left(  \sum\limits_{k=0}^{\infty
}S_{k}\left(  \widetilde{\partial_{y}}\right)  u^{-k}\right)  \left(
\tau\left(  x+y+z\right)  \tau\left(  x-y-z\right)  \right)  \right)
\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  \underbrace{S_{j+1}%
\left(  \widetilde{\partial_{y}}\right)  \left(  \tau\left(  x+y+z\right)
\tau\left(  x-y-z\right)  \right)  }_{\substack{=S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \left(  \tau\left(  x+y+z\right)  \tau\left(
x-y-z\right)  \right)  \\\text{(by Lemma \ref{lem.hirota.y+z}, applied
to}\\K=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \text{, }P=\tau\left(
x+w\right)  \tau\left(  x-w\right)  \text{ and }Q=S_{j+1}\left(  w\right)
\text{)}}}\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \underbrace{\left(  \tau\left(  x+y+z\right)
\tau\left(  x-y-z\right)  \right)  }_{\substack{=\exp\left(  \sum
\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(
x+z\right)  \tau\left(  x-z\right)  \right)  \\\text{(by (\ref{pf.hirota.31}%
))}}}\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(  x+z\right)
\tau\left(  x-z\right)  \right)  \mid_{z=0}.
\end{align*}
Compared with the fact that (by the definition of $A\left(  \sum
\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}%
\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  ,\tau,\tau\right)  $)
we have%
\begin{align*}
&  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right) \\
&  =\underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  \right)  \left(  \partial_{z}\right)  }_{=\sum\limits_{j=0}%
^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{\partial_{z}%
}\right)  \exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  }\left(  \tau\left(  x+z\right)  \tau\left(  x-z\right)  \right)
\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(  x+z\right)
\tau\left(  x-z\right)  \right)  \mid_{z=0},
\end{align*}
this yields%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  u\exp\left(  -2\sum\limits_{j>0}%
u^{j}y_{j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  \left(  \tau\left(  x-y\right)
\tau\left(  x+y\right)  \right)  \right) \\
&  =A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right)  .
\end{align*}
Hence, (\ref{pf.hirota.thirdrewriting}) rewrites as follows:%
\[
\left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0\right)  \ \Longleftrightarrow\ \left(  A\left(  \sum
\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}%
\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  ,\tau,\tau\right)
=0\right)  .
\]
Since $S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0$ is equivalent to $\sigma\left(  \tau\right)  \in\Omega$ (by
Theorem \ref{thm.plu.inf} \textbf{(b)}, applied to $\sigma\left(  \tau\right)
$ instead of $\tau$), this rewrites as follows:%
\[
\left(  \sigma\left(  \tau\right)  \in\Omega\right)  \ \Longleftrightarrow
\ \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  =0\right)  .
\]
This proves Theorem \ref{thm.hirota}.

Theorem \ref{thm.hirota} tells us that a nonzero $\tau\in\mathcal{B}^{\left(
0\right)  }$ satisfies $\sigma\left(  \tau\right)  \in\Omega$ if and only if
it satisfies the equation (\ref{thm.hirota.eqn}). The left hand side of this
equation is a power series with respect to the variables $y_{1},y_{2}%
,y_{3},...$. A power series is $0$ if and only if each of its coefficients is
$0$. Hence, the equation (\ref{thm.hirota.eqn}) holds if and only if for each
monomial in $y_{1},y_{2},y_{3},...$, the coefficient of the left hand side of
(\ref{thm.hirota.eqn}) in front of this monomial is $0$. Thus, the equation
(\ref{thm.hirota.eqn}) is equivalent to \textbf{a system of infinitely many
equations}, one for each monomial in $y_{1},y_{2},y_{3},...$. We don't know of
a good way to describe these equations (without using the variables
$y_{1},y_{2},y_{3},...$), but we can describe the equations corresponding to
the simplest among our monomials: the monomials of degree $0$ and those of
degree $1$.

In the following, we consider $\left(  \mathbb{C}\left[  \left[  y_{1}%
,y_{2},y_{3},...\right]  \right]  \right)  \left[  x_{1},x_{2},x_{3}%
,...\right]  $ as a subring of \newline$\left(  \mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  \right)  \left[  \left[  y_{1},y_{2},y_{3}%
,...\right]  \right]  $. For every commutative ring $K$, every element $T$ of
$K\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  $ and any
monomial\footnote{When we say "monomial", we mean a monomial without
coefficient.} $\mathfrak{m}$ in the variables $y_{1},y_{2},y_{3},...$, we
denote by $T\left[  \mathfrak{m}\right]  $ the coefficient of the monomial
$\mathfrak{m}$ in the power series $T$. (For example, $\left(  \exp\left(
x_{2}y_{2}\right)  \right)  \left[  y_{2}^{3}\right]  =\dfrac{x_{2}^{3}}{6}$;
note that $K=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ in this
example, so that $x_{2}$ counts as a constant!)

For every $P\in\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]  $ and
every monomial $\mathfrak{m}$ in the variables $y_{1},y_{2},y_{3},...$, we
have
\begin{equation}
\left(  A\left(  P,\tau,\tau\right)  \right)  \left[  \mathfrak{m}\right]
=A\left(  P\left[  \mathfrak{m}\right]  ,\tau,\tau\right)  .
\label{pf.hirota.66}%
\end{equation}
\footnote{\textit{Proof.} We have $P=\sum\limits_{\substack{\mathfrak{n}\text{
is a monomial}\\\text{in }y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]
\cdot\mathfrak{n}$. Since the map%
\begin{align*}
\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]
\right)  \left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]   &
\rightarrow\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  x_{1},x_{2},x_{3},...\right]  ,\\
Q  &  \mapsto A\left(  Q,\tau,\tau\right)
\end{align*}
is $\mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  $-linear,
we have
\[
A\left(  \sum\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in
}y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]  \cdot\mathfrak{n}%
,\tau,\tau\right)  =\sum\limits_{\substack{\mathfrak{n}\text{ is a
monomial}\\\text{in }y_{1},y_{2},y_{3},...}}A\left(  P\left[  \mathfrak{n}%
\right]  ,\tau,\tau\right)  \cdot\mathfrak{n}.
\]
Thus,
\[
A\left(  P,\tau,\tau\right)  =A\left(  \sum\limits_{\substack{\mathfrak{n}%
\text{ is a monomial}\\\text{in }y_{1},y_{2},y_{3},...}}P\left[
\mathfrak{n}\right]  \cdot\mathfrak{n},\tau,\tau\right)  =\sum
\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in }y_{1}%
,y_{2},y_{3},...}}A\left(  P\left[  \mathfrak{n}\right]  ,\tau,\tau\right)
\cdot\mathfrak{n},
\]
so that the coefficient of $A\left(  P,\tau,\tau\right)  $ before
$\mathfrak{m}$ equals $A\left(  P\left[  \mathfrak{m}\right]  ,\tau
,\tau\right)  $. Since we denoted the coefficient of $A\left(  P,\tau
,\tau\right)  $ before $\mathfrak{m}$ by $\left(  A\left(  P,\tau,\tau\right)
\right)  \left[  \mathfrak{m}\right]  $, this rewrites as $\left(  A\left(
P,\tau,\tau\right)  \right)  \left[  \mathfrak{m}\right]  =A\left(  P\left[
\mathfrak{m}\right]  ,\tau,\tau\right)  $, qed.}

Now, let us describe the equations that are obtained from
(\ref{thm.hirota.eqn}) by taking coefficients before monomials of degree $0$
and $1$:

\textbf{Monomials of degree }$0$\textbf{:} The only monomial of degree $0$ in
$y_{1},y_{2},y_{3},...$ is $1$. We have%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  1\right] \\
&  =A\left(  \underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum
\limits_{s>0}y_{s}w_{s}\right)  \right)  \left[  1\right]  }_{=S_{1}\left(
\widetilde{w}\right)  =w_{1}},\tau,\tau\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.66}), applied to
}P=\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  \text{
and }\mathfrak{m}=1\right) \\
&  =A\left(  w_{1},\tau,\tau\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
Corollary \ref{cor.hirota.odd}, since }w_{1}\text{ is odd}\right)  .
\end{align*}
Therefore, if we take coefficients with respect to the monomial $1$ in the
equation (\ref{pf.hirota.66}), we obtain a tautology.

\textbf{Monomials of degree }$1$\textbf{:} This will be more interesting. The
monomials of degree $1$ in $y_{1},y_{2},y_{3},...$ are $y_{1},y_{2},y_{3}%
,...$. Let $r$ be a positive integer. We have%
\begin{align}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{r}\right] \nonumber\\
&  =A\left(  \underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum
\limits_{s>0}y_{s}w_{s}\right)  \right)  \left[  y_{r}\right]  }%
_{\substack{=-2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r}\\\text{(by
easy computations)}}},\tau,\tau\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.66}), applied to
}P=\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  \text{
and }\mathfrak{m}=y_{r}\right) \nonumber\\
&  =A\left(  -2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r},\tau
,\tau\right)  . \label{pf.hirota.69}%
\end{align}
Denote the polynomial $-2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r}$ by
$T_{r}\left(  w\right)  $. Then, (\ref{pf.hirota.69}) rewrites as%
\begin{equation}
\left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{r}\right]  =A\left(
T_{r}\left(  w\right)  ,\tau,\tau\right)  . \label{pf.hirota.70}%
\end{equation}
We have $T_{1}\left(  w\right)  =w_{2}$, $T_{2}\left(  w\right)
=-\dfrac{w_{1}^{3}}{3}-\dfrac{2w_{3}}{3}$ and $T_{3}\left(  w\right)
=\dfrac{w_{1}w_{3}}{3}-\dfrac{w_{4}}{2}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}%
}{12}-\dfrac{w_{1}^{2}w_{2}}{2}$. Since $T_{1}\left(  w\right)  $ and
$T_{2}\left(  w\right)  $ are odd, we have $A\left(  T_{1}\left(  w\right)
,\tau,\tau\right)  =0$ and $A\left(  T_{2}\left(  w\right)  ,\tau,\tau\right)
=0$ (by Corollary \ref{cor.hirota.odd}). Therefore, taking coefficients with
respect to the monomials $y_{1}$ and $y_{2}$ in the equation
(\ref{pf.hirota.66}) yields tautologies. However, $T_{3}\left(  w\right)  $ is
\textbf{not odd}. Applying (\ref{pf.hirota.70}) to $r=3$, we obtain%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{3}\right] \\
&  =A\left(  T_{3}\left(  w\right)  ,\tau,\tau\right)  =A\left(  \dfrac
{w_{1}w_{3}}{3}-\dfrac{w_{4}}{2}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12}-\dfrac{w_{1}^{2}w_{2}}{2},\tau,\tau\right) \\
&  =A\left(  \dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12},\tau,\tau\right)  +\underbrace{A\left(  -\dfrac{w_{4}}{2}-\dfrac
{w_{1}^{2}w_{2}}{2},\tau,\tau\right)  }_{\substack{=0\\\text{(by Corollary
\ref{cor.hirota.odd}, since}\\-\dfrac{w_{4}}{2}-\dfrac{w_{1}^{2}w_{2}}%
{2}\text{ is odd)}}}\\
&  =A\left(  \dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12},\tau,\tau\right)  =\left(  \left(  \dfrac{\dfrac{\partial}{\partial
z_{1}}\dfrac{\partial}{\partial z_{3}}}{3}-\dfrac{\left(  \dfrac{\partial
}{\partial z_{2}}\right)  ^{2}}{4}-\dfrac{\left(  \dfrac{\partial}{\partial
z_{1}}\right)  ^{4}}{12}\right)  \left(  \tau\left(  x-z\right)  \tau\left(
x+z\right)  \right)  \right)  \mid_{z=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }A\left(
\dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}{12},\tau
,\tau\right)  \right) \\
&  =\dfrac{1}{12}\left(  \left(  4\dfrac{\partial}{\partial z_{1}}%
\dfrac{\partial}{\partial z_{3}}-3\left(  \dfrac{\partial}{\partial z_{2}%
}\right)  ^{2}-\left(  \dfrac{\partial}{\partial z_{1}}\right)  ^{4}\right)
\left(  \tau\left(  x-z\right)  \tau\left(  x+z\right)  \right)  \right)
\mid_{z=0}\\
&  =\dfrac{1}{12}\left(  \left(  4\dfrac{\partial}{\partial w_{1}}%
\dfrac{\partial}{\partial w_{3}}-3\left(  \dfrac{\partial}{\partial w_{2}%
}\right)  ^{2}-\left(  \dfrac{\partial}{\partial w_{1}}\right)  ^{4}\right)
\left(  \tau\left(  x-w\right)  \tau\left(  x+w\right)  \right)  \right)
\mid_{w=0}.
\end{align*}
Since $\dfrac{\partial}{\partial w_{j}}=\partial_{w_{j}}$ for every $j$, we
rewrite this as%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{3}\right] \\
&  =\dfrac{1}{12}\left(  \left(  4\partial_{w_{1}}\partial_{w_{3}}%
-3\partial_{w_{2}}^{2}-\partial_{w_{1}}^{4}\right)  \left(  \tau\left(
x-w\right)  \tau\left(  x+w\right)  \right)  \right)  \mid_{w=0}.
\end{align*}
Hence, taking coefficients with respect to the monomial $y_{3}$ in the
equation (\ref{thm.hirota.eqn}), we obtain%
\[
\dfrac{1}{12}\left(  \left(  4\partial_{w_{1}}\partial_{w_{3}}-3\partial
_{w_{2}}^{2}-\partial_{w_{1}}^{4}\right)  \left(  \tau\left(  x-w\right)
\tau\left(  x+w\right)  \right)  \right)  \mid_{w=0}=0.
\]
In other words,
\begin{equation}
\left(  \partial_{w_{1}}^{4}+3\partial_{w_{2}}^{2}-4\partial_{w_{1}}%
\partial_{w_{3}}\right)  \left(  \tau\left(  x-w\right)  \tau\left(
x+w\right)  \right)  \mid_{w=0}=0. \label{KdV.star}%
\end{equation}
This does not yet look like a PDE in any usual form. We will now transform it
into one.

We make the substitution $x_{1}=x$, $x_{2}=y$, $x_{3}=t$, $x_{m}=c_{m}$ for
$m\geq4$. Here, $x$, $y$, $t$ and $c_{m}$ (for $m\geq4$) are new symbols (in
particularly, $x$ and $y$ no longer denote the sequences $\left(  x_{1}%
,x_{2},x_{3},...\right)  $ and $\left(  y_{1},y_{2},y_{3},...\right)  $). Let
$u=2\partial_{x}^{2}\log\tau$.

\begin{proposition}
\label{prop.KdV.computation}The polynomial $\tau\left(  x,y,t,c_{4}%
,c_{5},..\right)  $ satisfies (\ref{KdV.star}) if and only if the function $u$
satisfies the KP equation
\[
\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(  \partial_{t}u-\dfrac{3}%
{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)
\]
(where $c_{4}$, $c_{5}$, $c_{6}$, $...$ are considered as constants).
\end{proposition}

\textit{Proof of Proposition \ref{prop.KdV.computation}.} Optional homework exercise.

Thus, we know that any element $\tau$ of $\Omega$ gives rise to a solution of
the KP equation (namely, the solution is $2\partial_{x}^{2}\log\left(
\sigma^{-1}\left(  \tau\right)  \right)  $). Two elements of $\tau$ differing
from each other by a scalar factor yield one and the same solution of the KP
equation. Hence, any element of $\operatorname*{Gr}$ gives rise to a solution
of the KP equation. Since we know how to produce elements of
$\operatorname*{Gr}$, we thus know how to produce solutions of the KP equation!

This does not give \textbf{all} solutions, and in fact we cannot even hope to
find all solutions explicitly (since they depend on boundary conditions, and
these can be arbitrarily nonexplicit), but we will use this to find a dense
subset of them (in an appropriate sense).

The KP equation is not the KdV (Korteweg-de Vries) equation; but if we have a
solution of the KP equation which does not depend on $y$, then this solution
satisfies $\partial_{t}u-\dfrac{3}{2}u\partial_{x}u-\dfrac{1}{4}\partial
_{x}^{3}u=\operatorname*{const}$, and with some work it gives rise to a
solution of the KdV equation (under appropriate decay-at-infinity conditions).

The equations corresponding to the coefficients of the monomials $y_{4}$,
$y_{5}$, $...$ in (\ref{pf.hirota.66}) correspond to the \textit{KP hierarchy}
of higher-order PDEs. There is no point in writing them up explicitly; they
become more and more complicated.

\begin{corollary}
\label{cor.KdV.schursols}Let $\lambda$ be a partition. Then, $2\partial
_{x}^{2}\log\left(  S_{\lambda}\left(  x,y,t,c_{4},c_{5},...\right)  \right)
$ is a solution of the KP equation (and of the whole KP hierarchy), where
$c_{4}$, $c_{5}$, $c_{6}$, $...$ are considered as constants.
\end{corollary}

\textit{Proof of Corollary \ref{cor.KdV.schursols}.} Write $\lambda$ in the
form $\lambda=\left(  \lambda_{0},\lambda_{1},\lambda_{2},...\right)  $. Let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be the sequence defined by
$i_{k}=\lambda_{k}-k$ for every $k\in\mathbb{N}$. Then, $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ is a $0$-degression, and we know that the
elementary semiinfinite wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ is in $\Omega$. But Theorem \ref{thm.schur} yields $\sigma
^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=S_{\lambda}\left(  x\right)  $ (since $\lambda=\left(  i_{0}+0,i_{1}%
+1,i_{2}+2,...\right)  $), so that $\sigma\left(  S_{\lambda}\left(  x\right)
\right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\in\Omega$. Hence,
the function $2\partial_{x}^{2}\log\left(  S_{\lambda}\left(  x,y,t,c_{4}%
,c_{5},...\right)  \right)  $ satisfies the KP equation (and the whole KP
hierarchy). This proves Corollary \ref{cor.KdV.schursols}.

\subsubsection{$n$-soliton solutions of KdV}

Now we will construct other solutions of KdV (which are called multisoliton solutions).

Define a quantum field $\Gamma\left(  u,v\right)  \in\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  0\right)  }\right)  \right)
\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]  $ by%
\begin{align*}
\Gamma\left(  u,v\right)   &  =u\left.  :\Gamma\left(  u\right)  \Gamma^{\ast
}\left(  v\right)  :\right. \\
&  =\exp\left(  \sum\limits_{j\geq1}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)
\exp\left(  -\sum\limits_{j\geq1}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}
[...] [Explain the semantics of $::$ here.]

\begin{proposition}
\label{prop.KdV.grassm}If $\tau\in\Omega$ and $a\in\mathbb{C}$, then%
\[
\left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\in\Omega_{u,v},
\]
where%
\[
\Omega_{u,v}=\left\{  \tau\in\mathcal{B}^{\left(  0\right)  }\left(  \left(
u,v\right)  \right)  \ \mid\ S\left(  \tau\otimes\tau\right)  =0\right\}  .
\]

\end{proposition}

\begin{corollary}
We have
\begin{align*}
&  \left(  1+a_{1}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(
1+a_{2}\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a_{n}%
\Gamma\left(  u_{n},v_{n}\right)  \right)  \mathbf{1}\\
&  \in\Omega
\end{align*}
(in fact, in an appropriate $\Omega_{u_{1},v_{1},u_{2},v_{2},...}$ rather than
in $\Omega$ itself).
\end{corollary}

\textit{Idea of proof of Proposition.} We will prove $\Gamma\left(
u,v\right)  ^{2}=0$, but we will have to make sense of a term like
$\Gamma\left(  u,v\right)  ^{2}$ in order to define this. Thus, $1+a\Gamma
\left(  u,v\right)  $ will become $\exp\left(  a\Gamma\left(  u,v\right)
\right)  $.

We will formalize this proof later.

But first, here is the punchline of this:

\begin{proposition}
If $\tau=\left(  1+a_{1}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(
1+a_{2}\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a_{n}%
\Gamma\left(  u_{n},v_{n}\right)  \right)  \mathbf{1}$, then $2\partial
_{x}^{2}\log\tau$ is given by a convergent series and defines a solution of KP
depending on the parameters $a_{i}$, $u_{i}$ and $v_{i}$.
\end{proposition}

This solution is called an $n$\textit{-soliton solution}.

For $n=1$, we have%
\[
\tau=\left(  1+a\Gamma\left(  u,v\right)  \right)  \mathbf{1}=1+a\exp\left(
\left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)  y+\left(  u^{3}%
-v^{3}\right)  t+\left(  u^{4}-v^{4}\right)  c_{4}+...\right)  .
\]
Absorb the $c_{i}$ parameters into a single constant $c$, which can be
absorbed into $a$. So we get%
\[
\tau=1+a\exp\left(  \left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)
y+\left(  u^{3}-v^{3}\right)  t\right)  .
\]
This $\tau$ satisfies
\[
2\partial_{x}^{2}\log\tau=\dfrac{\left(  u-v\right)  ^{2}}{2}\dfrac{1}%
{\cosh^{2}\left(  \dfrac{1}{2}\left(  \left(  u-v\right)  x+\left(
u^{2}-v^{2}\right)  y+\left(  u^{3}-v^{3}\right)  \tau\right)  \right)  }.
\]
Call this function $U$. To make it independent of $y$ (so we get a solution of
KdV equation), we set $v=-u$, and this becomes%
\[
U=\dfrac{2u^{2}}{\cosh^{2}\left(  ux+u^{3}t\right)  }.
\]
This is exactly the soliton solution of KdV.

But let us now give the promised proof of Proposition \ref{prop.KdV.grassm}.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that
$\Gamma\left(  u,v\right)  =u\left.  :\Gamma\left(  u\right)  \Gamma^{\ast
}\left(  v\right)  :\right.  $. We can show:

\begin{lemma}
\label{lem.KdV.GG}We have%
\[
\Gamma\left(  u\right)  \Gamma\left(  v\right)  =\left(  u-v\right)
\cdot\left.  :\Gamma\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\left(
u-v\right)  \cdot\left.  :\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(
v\right)  :\right.  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.KdV.GG}.} We have%
\[
...\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \exp\left(
\sum\limits_{k>0}\dfrac{a_{-k}}{k}v^{k}\right)  ...
\]
and we have to switch these two terms. We get something like%
\[
\exp\left(  -\log\left(  1-\dfrac{v}{u}\right)  \right)  =\dfrac{1}%
{1-\dfrac{v}{u}}=\dfrac{u}{u-v}.
\]
Etc.

We can generalize this: If $\varepsilon=1$ or $\varepsilon=-1$, we can define
$\Gamma_{\varepsilon}$ by $\Gamma_{+1}=\Gamma$ and $\Gamma_{-1}=\Gamma^{\ast}%
$. Then,

\begin{proposition}
We have%
\[
\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}}\left(
u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)  =\prod
\limits_{i<j}\left(  u_{i}-u_{j}\right)  ^{\varepsilon_{i}\varepsilon_{j}%
}\left.  :\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon
_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
:\right.  .
\]
Here, series are being expanded in the region where $\left\vert u_{1}%
\right\vert >\left\vert u_{2}\right\vert >...>\left\vert u_{n}\right\vert $.
\end{proposition}

\begin{corollary}
The matrix elements of $\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)
\Gamma_{\varepsilon_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}%
}\left(  u_{n}\right)  $ (this means expressions of the form $\left(  w^{\ast
},\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}%
}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
w\right)  $ with $w\in\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}%
\in\mathcal{B}^{\left(  0\right)  \ast}$ (where $^{\ast}$ means restricted
dual); a priori, these are only series) are series which converge to rational
functions of the form%
\[
P\left(  u\right)  \cdot\prod\limits_{i<j}\left(  u_{i}-u_{j}\right)
^{\varepsilon_{i}\varepsilon_{j}},\ \ \ \ \ \ \ \ \ \ \text{where }%
P\in\mathbb{C}\left[  u_{1}^{\pm1},u_{2}^{\pm1},...,u_{n}^{\pm1}\right]  .
\]

\end{corollary}

This follows from the Proposition since matrix elements of normal ordered
products are Laurent polynomials.

\begin{corollary}
We have $\Gamma\left(  u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)
=\dfrac{\left(  u^{\prime}-u\right)  \left(  v^{\prime}-v\right)  }{\left(
v^{\prime}-u\right)  \left(  u^{\prime}-v\right)  }\left.  :\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  :\right.  $.
\end{corollary}

Here, we cancelled $u-v$ and $u^{\prime}-v^{\prime}$ which is okay because our
rational functions lie in an integral domain.

As a corollary of this corollary, we have:

\begin{corollary}
If $u\neq v$, then $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  =0$. By which we mean that for any $w\in
\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}\in\mathcal{B}^{\left(
0\right)  \ast}$, we have $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\left(  w^{\ast},\Gamma\left(  u^{\prime
},v^{\prime}\right)  \Gamma\left(  u,v\right)  w\right)  =0$ as a rational function.
\end{corollary}

Informally, this can be written $\left(  \Gamma\left(  u,v\right)  \right)
^{2}=0$. But this does not really make sense in a formal sense since we are
not supposed to take squares of such power series.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that our idea was
to use $1+a\Gamma=\exp\left(  a\Gamma\right)  $ since $\Gamma^{2}=0$. But this
is not rigorous since we cannot speak of $\Gamma^{2}$. So here is the actual proof:

We have (abbreviating $\Gamma\left(  u,v\right)  $ by $\Gamma$ occasionally)%
\begin{align*}
&  S\left(  \left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\otimes\left(
1+a\Gamma\left(  u,v\right)  \right)  \tau\right) \\
&  =\underbrace{S\left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}\tau\in\Omega\text{)}}}+a\underbrace{S\left(  \Gamma\otimes1+1\otimes
\Gamma\right)  \left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}S\text{ commutes with }\mathfrak{gl}_{\infty}\text{,}\\\text{and coefficients
of }\Gamma\text{ are in }\mathfrak{gl}_{\infty}\text{,}\\\text{and }S\left(
\tau\otimes\tau\right)  =0\text{)}}}+a^{2}S\left(  \Gamma\otimes\Gamma\right)
\left(  \tau\otimes\tau\right) \\
&  =a^{2}S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right)
.
\end{align*}
Remains to prove that $S\left(  \Gamma\otimes\Gamma\right)  \left(
\tau\otimes\tau\right)  =0$.

We have
\begin{align*}
&  S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right) \\
&  =\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime}\rightarrow
v}}\dfrac{1}{2}S\left(  \Gamma\left(  u,v\right)  \tau\otimes\Gamma\left(
u^{\prime},v^{\prime}\right)  \tau+\Gamma\left(  u^{\prime},v^{\prime}\right)
\tau\otimes\Gamma\left(  u,v\right)  \tau\right) \\
&  =\dfrac{1}{2}\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime
}\rightarrow v}}\underbrace{S\left(  \Gamma\left(  u^{\prime},v^{\prime
}\right)  \otimes1+1\otimes\Gamma\left(  u^{\prime},v^{\prime}\right)
\right)  \left(  \Gamma\left(  u,v\right)  \otimes1+1\otimes\Gamma\left(
u,v\right)  \right)  \left(  \tau\otimes\tau\right)  }%
_{\substack{=0\\\text{(since }S\text{ commutes with these things)}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2}\lim\limits_{\substack{u^{\prime
}\rightarrow u;\\v^{\prime}\rightarrow v}}S\left(  \underbrace{\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  }_{\rightarrow
0}\otimes1+1\otimes\underbrace{\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  }_{\rightarrow0}\right)  \left(  \tau\otimes
\tau\right) \\
&  =0.
\end{align*}
This proves Proposition \ref{prop.KdV.grassm}.

\subsection{Representations of $\operatorname*{Vir}$ revisited}

We now come back to the representation theory of the Virasoro algebra
$\operatorname*{Vir}$.

Recall that to every pair $\lambda=\left(  c,h\right)  $, we can attach a
Verma module $M_{\lambda}^{+}=M_{c,h}^{+}$ over $\operatorname*{Vir}$. We will
denote this module by $M_{\lambda}=M_{c,h}$, and its $v_{\lambda}^{+}$ by
$v_{\lambda}$.

This module $M_{\lambda}$ has a symmetric bilinear form $\left(  \cdot
,\cdot\right)  :M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$
such that $\left(  v_{\lambda},v_{\lambda}\right)  =1$ and $\left(
L_{n}v,w\right)  =\left(  v,L_{-n}w\right)  $ for all $n\in\mathbb{Z}$, $v\in
M_{\lambda}$ and $w\in M_{\lambda}$. This form is called the
\textit{Shapovalov form}, and is obtained from the invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ by means of the
involution on $\operatorname*{Vir}$.

Also, if $\lambda\in\mathbb{R}^{2}$, the module $M_{\lambda}^{+}$ has a
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ satisfying the same conditions.

We recall that $M_{\lambda}$ has a unique irreducible quotient $L_{\lambda}$.
We have asked questions about when it is unitary, etc.. We will try to answer
some of these questions today.

\begin{Convention}
Let us change the grading of the Virasoro algebra $\operatorname*{Vir}$ to
$\deg L_{i}=-i$. Correspondingly, $M_{\lambda}$ becomes $M_{\lambda}%
=\bigoplus\limits_{n\geq0}M_{\lambda}\left[  n\right]  $.
\end{Convention}

For any $n\geq0$, we have the polynomial $\det\nolimits_{n}\left(  c,h\right)
$ which is the determinant of the contravariant form $\left(  \cdot
,\cdot\right)  $ in degree $n$. This polynomial is defined up to a constant
scalar. Let us recall how it is defined:

Let $\left(  a_{j}\right)  $ be a basis of $U\left(  \operatorname*{Vir}%
\nolimits_{-}\right)  \left[  n\right]  $ (where $\operatorname*{Vir}%
\nolimits_{-}$ is $\left\langle L_{-1},L_{-2},L_{-3},...\right\rangle $; this
is now the \textbf{positive} part of $\operatorname*{Vir}$). Then,
$\det\nolimits_{n}\left(  c,h\right)  =\det\left(  \left(  a_{I}v_{\lambda
},a_{J}v_{\lambda}\right)  _{I,J}\right)  $. If we change the basis by a
matrix $S$, the determinant multiplies by $\left(  \det S\right)  ^{2}$.

For a Hermitian form, we can do the same when $\left(  c,h\right)  $ is real,
but then $\det\nolimits_{n}\left(  c,h\right)  $ is defined up to a
\textbf{positive} scalar, because now the determinant multiplies by
$\left\vert \det S\right\vert ^{2}$. Hence it makes sense to say that
$\det\nolimits_{n}\left(  c,h\right)  >0$.

\begin{proposition}
We have $\det\nolimits_{n}\left(  c,h\right)  =0$ if and only if there exists
a singular vector $w\neq0$ in $M_{c,h}$ of degree $\leq n$ and $>0$.

In particular, if $\det\nolimits_{n}\left(  c,h\right)  =0$, then
$\det\nolimits_{n+1}\left(  c,h\right)  =0$.
\end{proposition}

In fact, we will see that $\det\nolimits_{n+1}$ is divisible by $\det
\nolimits_{n}$.

\textit{Proof of Proposition.} Apparently this is supposed to follow from
something we did.

We recall examples:%
\begin{align*}
\det\nolimits_{1}  &  =2h,\\
\det\nolimits_{2}  &  =2h\left(  16h^{2}+2hc-10h+c\right)  .
\end{align*}
Also recall that $M_{c,h}$ is irreducible if and only if every positive $n$
satisfies $\det\nolimits_{n}\left(  c,h\right)  \neq0$.

\begin{proposition}
Let $\left(  c,h\right)  \in\mathbb{R}^{2}$. If $M_{c,h}$ is unitary, then
$\det\nolimits_{n}\left(  c,h\right)  >0$ for all positive $n$.

More generally, if $L_{c,h}\left[  n\right]  \cong M_{c,h}\left[  n\right]  $
for some positive $n$, and $L_{c,h}$ is unitary, then $\det\nolimits_{n}%
\left(  c,h\right)  >0$.
\end{proposition}

\textit{Proof of Proposition.} A positive-definite Hermitian matrix has
positive determinant.

\begin{theorem}
\label{thm.kac.leader}Fix $c$. Regard $\det\nolimits_{m}\left(  c,h\right)  $
as a polynomial in $h$. Then,%
\[
\det\nolimits_{m}\left(  c,h\right)  =K\cdot h^{\sum\limits_{\substack{r,s\geq
1;\\rs\leq m}}p\left(  m-rs\right)  }+\left(  \text{lower terms}\right)
\]
for some nonzero constant $K$ (which depends on the choice of the basis).
\end{theorem}

\textit{Proof.} We computed before the leading term of $\det\nolimits_{m}$ for
any graded Lie algebra.

$\left\langle L_{-k}^{m_{k}}...L_{-1}^{m_{1}}v_{\lambda},L_{-k}^{n_{k}%
}...L_{-1}^{n_{1}}v_{\lambda}\right\rangle $: the main contribution to the
leading term comes from diagonal.

What degree in $h$ do we get?

If $\mu$ is a partition of $m$, we can write $m=1k_{1}\left(  \mu\right)
+2k_{2}\left(  \mu\right)  +...$, where $k_{i}\left(  \mu\right)  $ is the
number of times $i$ occurs in $\mu$.

$\left(  L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v,L_{-\ell}^{k_{\ell}}%
...L_{-1}^{k_{1}}v\right)  =\left(  v,L_{1}^{k_{1}}...L_{\ell}^{k_{\ell}%
}L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v\right)  $.

So $\mu$ contributes $k_{1}+...+k_{\ell}$ to the exponent of $h$.

So we conclude that the total exponent of $h$ is $\sum\limits_{\mu\vdash
m}\sum\limits_{i}k_{i}\left(  \mu\right)  $.

The rest is easy combinatorics:

Let $m\left(  r,s\right)  $ denote the number of partitions of $m$ in which
$r$ occurs exactly $s$ times. Then, $m\left(  r,s\right)  =p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  $. Thus, with $m$ and
$r$ fixed,%
\begin{align*}
\sum\limits_{s}sm\left(  r,s\right)   &  =\sum\limits_{s}s\left(  p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  \right) \\
&  =\sum\limits_{s}sp\left(  m-rs\right)  -\sum\limits_{s}sp\left(  m-r\left(
s+1\right)  \right) \\
&  =\sum\limits_{s}sp\left(  m-rs\right)  -\sum\limits_{s}\left(  s-1\right)
p\left(  m-rs\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }s-1\text{ for
}s\text{ in the second sum}\right) \\
&  =\sum\limits_{s}\underbrace{\left(  s-\left(  s-1\right)  \right)  }%
_{=1}p\left(  m-rs\right)  =\sum\limits_{s}p\left(  m-rs\right)  .
\end{align*}


So our job is to show that $\sum\limits_{\mu\vdash m}\sum\limits_{i}%
k_{i}\left(  \mu\right)  =\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}sm\left(  r,s\right)  $. But $\sum\limits_{\substack{s\geq1;\\s\leq
m}}sm\left(  r,s\right)  $ is the total number of occurrences of $r$ in all
partitions of $m$. Summed over $r$, it yields the total number of parts of all
partitions of $m$. But this is also $\sum\limits_{\mu\vdash m}\sum
\limits_{i}k_{i}\left(  \mu\right)  $, qed.

We now quote a theorem which was proved independently by Kac and Feigin-Fuchs:

\begin{theorem}
Suppose $rs\leq m$. Then, if%
\[
h=h_{r,s}\left(  c\right)  :=\dfrac{1}{48}\left(  \left(  13-c\right)  \left(
r^{2}+s^{2}\right)  +\sqrt{\left(  c-1\right)  \left(  c-25\right)  }\left(
r^{2}-s^{2}\right)  -24rs-2+2c\right)  ,
\]
then $\det\nolimits_{m}\left(  c,h\right)  =0$. (This is true for any of the
branches of the square root.)
\end{theorem}

\begin{theorem}
\label{thm.kac.thm1}If $h=h_{r,s}\left(  c\right)  $, then $M_{c,h}$ has a
nonzero singular vector in degree $1\leq d\leq rs$.
\end{theorem}

\begin{theorem}
[Kac, also proved by Feigin-Fuchs]\label{thm.kac.thm2}We have%
\[
\det\nolimits_{m}\left(  c,h\right)  =K_{m}\cdot\prod
\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(  c\right)
\right)  ^{p\left(  m-rs\right)  },
\]
where $K_{m}$ is some constant. Note that we should choose the same branch of
the square root in $\sqrt{\left(  c-1\right)  \left(  c-25\right)  }$ for
$h_{r,s}$ and $h_{s,r}$. The square roots "cancel out" and give way to a
polynomial in $h$ and $c$.
\end{theorem}

To prove these, we will use the following lemma:

\begin{lemma}
\label{lem.kac.linalg}Let $A\left(  t\right)  $ be a polynomial in one
variable $t$ with values in $\operatorname*{End}V$, where $V$ is a
finite-dimensional vector space. Suppose that $\dim\operatorname*{Ker}\left(
A\left(  0\right)  \right)  \geq n$. Then, $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.kac.linalg}.} Pick a basis $e_{1}%
,e_{2},...,e_{m}$ of $V$ such that the first $n$ vectors $e_{1},e_{2}%
,...,e_{n}$ are in $\operatorname*{Ker}\left(  A\left(  0\right)  \right)  $.
Then, the matrix of $A\left(  t\right)  $ in this basis has first $n$ columns
divisible by $t$, so that its determinant $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.

\textit{Proof of Theorem \ref{thm.kac.thm2}.} Let $A=A\left(  h\right)  $ be
the matrix of the contravariant form in degree $m$, considered as a polynomial
in $h$. If $h=h_{r,s}\left(  c\right)  $, we have a singular vector $w$ in
degree $1\leq d\leq rs$ (by Theorem \ref{thm.kac.thm1}), which generates a
Verma submodule $M_{c,h^{\prime}}\subseteq M_{c,h}$ (by Homework Set 3 problem
1) (the $c$ is the same since $c$ is central and thus acts by the same number
on all vectors).

So $M_{c,h}\left[  m\right]  \supseteq M_{c,h^{\prime}}\left[  m-d\right]  $.
We also have $\dim\left(  M_{c,h^{\prime}}\left[  m-d\right]  \right)
=p\left(  m-d\right)  \geq p\left(  m-rs\right)  $ (since $d\leq rs$) and
$M_{c,h^{\prime}}\left[  m-d\right]  \subseteq\operatorname*{Ker}\left(
\cdot,\cdot\right)  $ (when $h=h_{r,s}$). Hence, $\dim\left(
\operatorname*{Ker}\left(  \cdot,\cdot\right)  \right)  \geq p\left(
m-rs\right)  $. By Lemma \ref{lem.kac.linalg}, this yields that $\det
\nolimits_{m}\left(  c,h\right)  $ is divisible by $\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$.

But it is easy to see that for Weil-generic $c$, the $h-h_{r,s}\left(
c\right)  $ are different, so that $\det\nolimits_{m}\left(  c,h\right)  $ is
divisible by $\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(
h-h_{r,s}\left(  c\right)  \right)  ^{p\left(  m-rs\right)  }$. But by Theorem
\ref{thm.kac.leader}, the leading term of $\det\nolimits_{m}\left(
c,h\right)  $ is $K\cdot h^{\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}p\left(  m-rs\right)  }$, which has exactly the same degree. So
$\det\nolimits_{m}\left(  c,h\right)  $ is a constant multiple of
$\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$. Theorem \ref{thm.kac.thm2} is proven.

We will not prove Theorem \ref{thm.kac.thm1}, since we do not have the tools
for that.

\begin{corollary}
The module $M_{c,h}$ is irreducible if and only if $\left(  c,h\right)  $ does
not lie on the lines
\[
h-h_{r,r}\left(  c\right)  =0\ \Longleftrightarrow\ h+\left(  r^{2}-1\right)
\left(  c-1\right)  /24=0
\]
and quadrics (in fact, hyperbolas if we are over $\mathbb{R}$)%
\begin{align*}
&  \ \left(  h-h_{r,s}\left(  c\right)  \right)  \left(  h-h_{s,r}\left(
c\right)  \right)  =0\\
&  \Longleftrightarrow\ \left(  h-\dfrac{\left(  r-s\right)  ^{2}}{4}\right)
^{2}+\dfrac{h}{24}\left(  c-1\right)  \left(  r^{2}+s^{2}-2\right)  +\dfrac
{1}{576}\left(  r^{2}-1\right)  \left(  s^{2}-1\right)  \left(  c-1\right)
^{2}\\
&  \ \ \ \ \ \ \ \ \ \ +\dfrac{1}{48}\left(  c-1\right)  \left(  r-s\right)
^{2}\left(  rs+1\right)  =0.
\end{align*}

\end{corollary}

\begin{corollary}
\label{cor.kac.irred}\textbf{(1)} Let $h\geq0$ and $c\geq1$. Then, $L_{c,h}$
is unitary.

\textbf{(2)} Let $h>0$ and $c>1$. Then, $M_{c,h}\cong L_{c,h}$, so that
$M_{c,h}$ is irreducible.
\end{corollary}

\textit{Proof of Corollary \ref{cor.kac.irred}.} \textbf{(2)} Lines and
hyperbolas do not pass through the region.

For part \textbf{(1)} we need a lemma:

\begin{lemma}
If $\mathfrak{g}$ is a graded Lie algebra (with $\dim\mathfrak{g}_{i}%
\neq\infty$) with a real structure $\dag$, and $U\subseteq\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ is the set of $\lambda$ such that $L_{\lambda}$ is
unitary, then $U$ is closed in the usual metric.
\end{lemma}

\textit{Proof of Lemma.} It follows from the fact that if $\left(
A_{n}\right)  $ is a sequence of positive definite Hermitian matrices, and
$\lim\limits_{n\rightarrow\infty}A_{n}=A_{\infty}$, then $A_{\infty}$ is
nonnegative definite.

Okay, sorry, we are not going to use this lemma; we will derive the special
case we need.

Now I claim that if $h>0$ and $c>1$, then $L_{c,h}=M_{c,h}$ is unitary. We
know this is true for some points of this region (namely, the ones "above the
zigzag line"). Then everything follows from the fact that if $A\left(
t\right)  $ is a continuous family of nondegenerate Hermitian matrices
parametrized by $t\in\left[  0,1\right]  $ such that $A\left(  0\right)  >0$,
then $A\left(  t\right)  >0$ for every $t$. (This fact is because the
signature of a nondegenerate Hermitian matrix is a continuous map to a
discrete set, and thus constant on connected components.)

e. g., consider $M_{1,h}$ as a limit of $M_{1+\dfrac{1}{n},h}$ (this is
irreducible for large $n$).

So the matrix of the form in $M_{1,h}\left[  m\right]  $ is a limit of the
matrices for $M_{1+\dfrac{1}{n},h}\left[  m\right]  $. So the matrix for
$M_{1,h}\left[  m\right]  $ is $\geq0$. But kernel lies in $J_{1,h}\left[
m\right]  $, so the form on $L_{1,h}\left[  m\right]  =\left(  M_{1,h}\diagup
J_{1,h}\right)  \left[  m\right]  $ is strictly positive.

By analyzing the Kac curves, we can show (although \textit{we} will
\textit{not} show) that in the region $0\leq c<1$, there are only countably
many points where we possibly can have unitarity:

$c\left(  m\right)  =1-\dfrac{6}{\left(  m+2\right)  \left(  m+3\right)  };$

$h_{r,s}\left(  m\right)  =\dfrac{\left(  \left(  m+3\right)  r-\left(
m+2\right)  s\right)  ^{2}-1}{4\left(  m+2\right)  \left(  m+3\right)  }$ with
$1\leq r\leq s\leq m+1$.

for $m\geq0$.

In fact we will show that at these points we indeed have unitary representations.

\begin{proposition}
\textbf{(1)} If $c\geq0$ and $L_{c,h}$ is unitary, then $h=0$.

\textbf{(2)} We have $L_{0,h}=M_{0,h}$ if and only if $h\neq\dfrac{m^{2}%
-1}{24}$ for all $m\geq0$.

\textbf{(3)} We have $L_{1,h}=M_{1,h}$ if and only if $h\neq\dfrac{m^{2}}{24}$
for all $m\geq0$.
\end{proposition}

\textit{Proof.} \textbf{(2)} and \textbf{(3)} follow immediately from the Kac
determinant formula. For \textbf{(1)}, just compute $\det\left(
\begin{array}
[c]{cc}%
\left(  L_{-N}^{2}v,L_{-N}^{2}v\right)  & \left(  L_{-N}^{2}v,L_{-2N}v\right)
\\
\left(  L_{-2N}v,L_{-N}^{2}v\right)  & \left(  L_{-2N}v,L_{-2N}v\right)
\end{array}
\right)  =4N^{3}h^{2}\left(  8h-5N\right)  $ (this is $<0$ for high enough $N$
as long as $h\neq0$), so that the only possibility for unitarity is $h=0$.

\section{Affine Lie algebras}

\subsection{Introducing $\protect\widehat{\mathfrak{gl}_{n}}$}

\begin{definition}
\label{def.glnhat}Let $V$ denote the vector representation of $\mathfrak{gl}%
_{\infty}$ defined in Definition \ref{def.glinf.V}.

Let $n$ be a positive integer. Consider $L\mathfrak{gl}_{n}=\mathfrak{gl}%
_{n}\left[  t,t^{-1}\right]  $; this is the loop algebra of the Lie algebra
$\mathfrak{gl}_{n}$. This loop algebra clearly acts on $\mathbb{C}^{n}\left[
t,t^{-1}\right]  $ (by $\left(  At^{i}\right)  \rightharpoonup\left(
wt^{j}\right)  =Awt^{i+j}$ for all $A\in\mathfrak{gl}_{n}$, $w\in
\mathbb{C}^{n}$, $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$). But we can identify
the vector space $\mathbb{C}^{n}\left[  t,t^{-1}\right]  $ with $V$ as
follows: Let $\left(  e_{1},e_{2},...,e_{n}\right)  $ be the standard basis of
$\mathbb{C}^{n}$. Then we identify $e_{i}t^{k}\in\mathbb{C}^{n}\left[
t,t^{-1}\right]  $ with $v_{i-kn}\in V$ for every $i\in\left\{
1,2,...,n\right\}  $ and $k\in\mathbb{Z}$. The action of $L\mathfrak{gl}_{n}$
on $\mathbb{C}^{n}\left[  t,t^{-1}\right]  $ now becomes an action of
$L\mathfrak{gl}_{n}$ on $V$. Hence, $L\mathfrak{gl}_{n}$ maps into
$\operatorname*{End}V$. More precisely, $L\mathfrak{gl}_{n}$ maps into
$\overline{\mathfrak{a}_{\infty}}\subseteq\operatorname*{End}V$. Here is a
direct way to construct this mapping:

Let $a\left(  t\right)  \in L\mathfrak{gl}_{n}$ be a "Laurent polynomial" with
coefficients in $\mathfrak{gl}_{n}$. Write $a\left(  t\right)  $ in the form
$a\left(  t\right)  =\sum\limits_{k\in\mathbb{Z}}a_{k}t^{k}$ with all $a_{k}$
lying in $\mathfrak{gl}_{n}$. Then, let $\operatorname*{Toep}\nolimits_{n}%
\left(  a\left(  t\right)  \right)  $ be the matrix%
\[
\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  \in\overline{\mathfrak{a}_{\infty}}.
\]
Formally speaking, this matrix is defined as the matrix whose $\left(
ni+\alpha,nj+\beta\right)  $-th entry equals the $\left(  \alpha,\beta\right)
$-th entry of the $n\times n$ matrix $a_{j-i}$ for all $i\in\mathbb{Z}$,
$j\in\mathbb{Z}$, $\alpha\in\left\{  1,2,...,n\right\}  $ and $\beta
\in\left\{  1,2,...,n\right\}  $. In other words, this is the block matrix
consisting of infinitely many $n\times n$-blocks such that the "$i$-th block
diagonal" is filled with $a_{i}$'s for every $i\in\mathbb{Z}$.

We thus have defined a map $\operatorname*{Toep}\nolimits_{n}:L\mathfrak{gl}%
_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$. This map
$\operatorname*{Toep}\nolimits_{n}$ is injective, and is exactly the map
$L\mathfrak{gl}_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$ we obtain from
the above action of $L\mathfrak{gl}_{n}$ of $V$. In particular, this map
$\operatorname*{Toep}\nolimits_{n}$ is a Lie algebra homomorphism.

In the following, we will often regard the injective map $\operatorname*{Toep}%
\nolimits_{n}$ as an inclusion, i. e., we will identify any $a\left(
t\right)  \in L\mathfrak{gl}_{n}$ with its image $\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \right)  \in\overline{\mathfrak{a}%
_{\infty}}$.
\end{definition}

Note that I chose the notation $\operatorname*{Toep}\nolimits_{n}$ because of
the notion of Toeplitz matrices. For any $a\left(  t\right)  \in
L\mathfrak{gl}_{n}$, the matrix $\operatorname*{Toep}\nolimits_{n}\left(
a\left(  t\right)  \right)  $ can be called an infinite "block-Toeplitz"
matrix. If $n=1$, then $\operatorname*{Toep}\nolimits_{1}\left(  a\left(
t\right)  \right)  $ is an actual infinite Toeplitz matrix.

\begin{example}
Since $\mathfrak{gl}_{1}$ is a $1$-dimensional abelian Lie algebra, we can
identify $L\mathfrak{gl}_{1}$ with the Lie algebra $\overline{\mathcal{A}}$.
The image $\operatorname*{Toep}\nolimits_{1}\left(  L\mathfrak{gl}_{1}\right)
$ is the abelian Lie subalgebra $\left\langle T^{j}\ \mid\ j\in\mathbb{Z}%
\right\rangle $ of $\overline{\mathfrak{a}_{\infty}}$ and is isomorphic to
$\overline{\mathcal{A}}$ (where $T$ is the shift operator).
\end{example}

It is easy to see that:

\begin{proposition}
\label{prop.Toep.alg}Let $n$ be a positive integer. Define an associative
algebra structure on $L\mathfrak{gl}_{n}=\mathfrak{gl}_{n}\left[
t,t^{-1}\right]  $ by%
\[
\left(  at^{i}\right)  \cdot\left(  bt^{j}\right)  =abt^{i+j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{gl}_{n}\text{, }%
b\in\mathfrak{gl}_{n}\text{, }i\in\mathbb{Z}\text{ and }j\in\mathbb{Z}.
\]
Then, $\operatorname*{Toep}\nolimits_{n}$ is not only a Lie algebra
homomorphism, but also a homomorphism of associative algebras.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Toep.alg}.} Let $a\left(  t\right)  \in
L\mathfrak{gl}_{n}$ and $b\left(  t\right)  \in L\mathfrak{gl}_{n}$. Write
$a\left(  t\right)  $ in the form $a\left(  t\right)  =\sum\limits_{k\in
\mathbb{Z}}a_{k}t^{k}$ with all $a_{k}$ lying in $\mathfrak{gl}_{n}$. Write
$b\left(  t\right)  $ in the form $b\left(  t\right)  =\sum\limits_{k\in
\mathbb{Z}}b_{k}t^{k}$ with all $b_{k}$ lying in $\mathfrak{gl}_{n}$. By the
definition of $\operatorname*{Toep}\nolimits_{n}$, we have%
\begin{align*}
\operatorname*{Toep}\nolimits_{n}\left(  a\left(  t\right)  \right)   &
=\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \\
\text{and}\ \ \ \ \ \ \ \ \ \ \operatorname*{Toep}\nolimits_{n}\left(
b\left(  t\right)  \right)   &  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  .
\end{align*}
Hence,%
\begin{align}
&  \operatorname*{Toep}\nolimits_{n}\left(  a\left(  t\right)  \right)
\cdot\operatorname*{Toep}\nolimits_{n}\left(  b\left(  t\right)  \right)
\nonumber\\
&  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \nonumber\\
&  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k-\left(  -1\right)  }b_{-1-k} &
\sum\limits_{k\in\mathbb{Z}}a_{k-\left(  -1\right)  }b_{0-k} & \sum
\limits_{k\in\mathbb{Z}}a_{k-\left(  -1\right)  }b_{1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k-0}b_{-1-k} & \sum\limits_{k\in
\mathbb{Z}}a_{k-0}b_{0-k} & \sum\limits_{k\in\mathbb{Z}}a_{k-0}b_{1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k-1}b_{-1-k} & \sum\limits_{k\in
\mathbb{Z}}a_{k-1}b_{0-k} & \sum\limits_{k\in\mathbb{Z}}a_{k-1}b_{1-k} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the rule for multiplying block
matrices}\right) \nonumber\\
&  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +\left(
-1\right)  -k} & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +0-k}
& \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{0+1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{1+1-k} & ...\\
... & ... & ... & ... & ...
\end{array}
\right) \label{pf.Toep.alg.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since any }\left(  i,j\right)
\in\mathbb{Z}^{2}\text{ satisfies }\sum\limits_{k\in\mathbb{Z}}a_{k-i}%
b_{j-k}=\sum\limits_{k\in\mathbb{Z}}a_{k}b_{i+j-k}\right)  .\nonumber
\end{align}
On the other hand, multiplying $a\left(  t\right)  =\sum\limits_{k\in
\mathbb{Z}}a_{k}t^{k}$ and $b\left(  t\right)  =\sum\limits_{k\in\mathbb{Z}%
}b_{k}t^{k}$, we obtain%
\[
a\left(  t\right)  \cdot b\left(  t\right)  =\left(  \sum\limits_{k\in
\mathbb{Z}}a_{k}t^{k}\right)  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}%
b_{k}t^{k}\right)  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{k\in
\mathbb{Z}}a_{k}b_{i+j-k}\right)  t^{i}%
\]
(by the definition of the product of two Laurent polynomials), so that%
\[
\operatorname*{Toep}\nolimits_{n}\left(  a\left(  t\right)  \cdot b\left(
t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +\left(
-1\right)  -k} & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +0-k}
& \sum\limits_{k\in\mathbb{Z}}a_{k}b_{\left(  -1\right)  +1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{0+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{0+1-k} & ...\\
... & \sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+\left(  -1\right)  -k} &
\sum\limits_{k\in\mathbb{Z}}a_{k}b_{1+0-k} & \sum\limits_{k\in\mathbb{Z}}%
a_{k}b_{1+1-k} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)
\]
(by the definition of $\operatorname*{Toep}\nolimits_{n}$). Compared with
(\ref{pf.Toep.alg.1}), this yields $\operatorname*{Toep}\nolimits_{n}\left(
a\left(  t\right)  \right)  \cdot\operatorname*{Toep}\nolimits_{n}\left(
b\left(  t\right)  \right)  =\operatorname*{Toep}\nolimits_{n}\left(  a\left(
t\right)  \cdot b\left(  t\right)  \right)  $.

Now forget that we fixed $a\left(  t\right)  $ and $b\left(  t\right)  $. We
thus have proven that every $a\left(  t\right)  \in L\mathfrak{gl}_{n}$ and
$b\left(  t\right)  \in L\mathfrak{gl}_{n}$ satisfy $\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \right)  \cdot\operatorname*{Toep}%
\nolimits_{n}\left(  b\left(  t\right)  \right)  =\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \cdot b\left(  t\right)  \right)  $.
Combined with the fact that $\operatorname*{Toep}\nolimits_{n}\left(
1\right)  =\operatorname*{id}$ (this is very easy to prove), this yields that
$\operatorname*{Toep}\nolimits_{n}$ is a homomorphism of associative algebras.
Hence, $\operatorname*{Toep}\nolimits_{n}$ is also a homomorphism of Lie
algebras. Proposition \ref{prop.Toep.alg} is proven.

Recall that the Lie algebra $\overline{\mathfrak{a}_{\infty}}$ has a central
extension $\mathfrak{a}_{\infty}$, which equals $\overline{\mathfrak{a}%
_{\infty}}\oplus\mathbb{C}K$ as a vector space but has its Lie bracket defined
using the cocycle $\alpha$.

\begin{proposition}
\label{prop.ainf.alphaomega}Let $\alpha:\overline{\mathfrak{a}_{\infty}}%
\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ be the Japanese cocycle.

Let $n\in\mathbb{N}$. Let $\omega:L\mathfrak{gl}_{n}\times L\mathfrak{gl}%
_{n}\rightarrow\mathbb{C}$ be the $2$-cocycle on $L\mathfrak{gl}_{n}$ which is
defined by%
\begin{equation}
\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  =\sum
\limits_{k\in\mathbb{Z}}k\operatorname*{Tr}\left(  a_{k}b_{-k}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }a\left(  t\right)  \in L\mathfrak{gl}%
_{n}\text{ and }b\left(  t\right)  \in L\mathfrak{gl}_{n}
\label{prop.ainf.alphaomega.form}%
\end{equation}
(where we write $a\left(  t\right)  $ in the form $a\left(  t\right)
=\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$ with $a_{i}\in\mathfrak{gl}_{n}$, and
where we write $b\left(  t\right)  $ in the form $b\left(  t\right)
=\sum\limits_{i\in\mathbb{Z}}b_{i}t^{i}$ with $b_{i}\in\mathfrak{gl}_{n}$).

Then, the restriction of the Japanese cocycle $\alpha:\overline{\mathfrak{a}%
_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ to
$L\mathfrak{gl}_{n}\times L\mathfrak{gl}_{n}$ is the $2$-cocycle $\omega$.
\end{proposition}

\begin{remark}
The $2$-cocycle $\omega$ in Proposition \ref{prop.ainf.alphaomega} coincides
with the cocycle $\omega$ defined in Definition \ref{def.loop} in the case
when $\mathfrak{g}=\mathfrak{gl}_{n}$ and $\left(  \cdot,\cdot\right)  $ is
the form $\mathfrak{gl}_{n}\times\mathfrak{gl}_{n}\rightarrow\mathbb{C}%
,\ \left(  a,b\right)  \mapsto\operatorname*{Tr}\left(  ab\right)  $. The
$1$-dimensional central extension $\widehat{\mathfrak{gl}_{n}}_{\omega}$
induced by this $2$-cocycle $\omega$ (by the procedure shown in Definition
\ref{def.loop}) will be denoted by $\widehat{\mathfrak{gl}_{n}}$ in the
following. Note that $\widehat{\mathfrak{gl}_{n}}=L\mathfrak{gl}_{n}%
\oplus\mathbb{C}K$ as a vector space.

Note that the equality (\ref{prop.ainf.alphaomega.form}) can be rewritten in
the suggestive form%
\[
\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\operatorname*{Res}\nolimits_{t=0}\operatorname*{Tr}\left(  da\left(
t\right)  b\left(  t\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}a\left(  t\right)  \in L\mathfrak{gl}_{n}\text{ and }b\left(  t\right)  \in
L\mathfrak{gl}_{n}%
\]
(as long as the "matrix-valued differential form" $da\left(  t\right)
b\left(  t\right)  $ is understood correctly).
\end{remark}

\textit{Proof of Proposition \ref{prop.ainf.alphaomega}.} We need to prove
that $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  $ for any
$a\left(  t\right)  \in L\mathfrak{gl}_{n}$ and $b\left(  t\right)  \in
L\mathfrak{gl}_{n}$ (where, of course, we consider $a\left(  t\right)  $ and
$b\left(  t\right)  $ as elements of $\overline{\mathfrak{a}_{\infty}}$ in the
term $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  $).

Write $a\left(  t\right)  $ in the form $a\left(  t\right)  =\sum
\limits_{k\in\mathbb{Z}}a_{k}t^{k}$ with all $a_{k}$ lying in $\mathfrak{gl}%
_{n}$. Write $b\left(  t\right)  $ in the form $b\left(  t\right)
=\sum\limits_{k\in\mathbb{Z}}b_{k}t^{k}$ with all $b_{k}$ lying in
$\mathfrak{gl}_{n}$.

In the following, for any integers $u$ and $v$, the $\left(  u,v\right)  $-th
\textit{block} of a matrix will mean the submatrix obtained by leaving only
the rows numbered $un+1$, $un+2$, $...$, $\left(  u+1\right)  n$ and the
columns numbered $vn+1$, $vn+2$, $...$, $\left(  v+1\right)  n$. (This, of
course, makes sense only when the matrix has such rows and such columns.)

By the definition of our embedding $\operatorname*{Toep}\nolimits_{n}\left(
a\left(  t\right)  \right)  :L\mathfrak{gl}_{n}\rightarrow\overline
{\mathfrak{a}_{\infty}}$, we have%
\begin{align*}
a\left(  t\right)   &  =\operatorname*{Toep}\nolimits_{n}\left(  a\left(
t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
b\left(  t\right)   &  =\operatorname*{Toep}\nolimits_{n}\left(  b\left(
t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  ,
\end{align*}
where the matrices $\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & b_{0} & b_{1} & b_{2} & ...\\
... & b_{-1} & b_{0} & b_{1} & ...\\
... & b_{-2} & b_{-1} & b_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  $ are understood as block matrices made of $n\times n$ blocks.

In order to compute $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)
\right)  $, let us write these two infinite matrices as block matrices
\textbf{made of infinite blocks}, where the blocks are separated as follows:

- The left blocks contain the $j$-th columns for all $j\leq0$; the right
blocks contain the $j$-th columns for all $j>0$.

- The upper blocks contain the $i$-th rows for all $i\leq0$; the lower blocks
contain the $i$-th rows for all $i>0$.

Written like this, the matrix $a\left(  t\right)  $ takes the form $\left(
\begin{array}
[c]{cc}%
A_{11} & A_{12}\\
A_{21} & A_{22}%
\end{array}
\right)  $ with%
\begin{align*}
A_{11}  &  =\left(
\begin{array}
[c]{cccc}%
... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2}\\
... & a_{-1} & a_{0} & a_{1}\\
... & a_{-2} & a_{-1} & a_{0}%
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ A_{12}=\left(
\begin{array}
[c]{cccc}%
... & ... & ... & ...\\
a_{3} & a_{4} & a_{5} & ...\\
a_{2} & a_{3} & a_{4} & ...\\
a_{1} & a_{2} & a_{3} & ...
\end{array}
\right)  ,\\
A_{21}  &  =\left(
\begin{array}
[c]{cccc}%
... & a_{-3} & a_{-2} & a_{-1}\\
... & a_{-4} & a_{-3} & a_{-2}\\
... & a_{-5} & a_{-4} & a_{-3}\\
... & ... & ... & ...
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ A_{22}=\left(
\begin{array}
[c]{cccc}%
a_{0} & a_{1} & a_{2} & ...\\
a_{-1} & a_{0} & a_{1} & ...\\
a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ...
\end{array}
\right)  ,
\end{align*}
and the matrix $b\left(  t\right)  $ takes the form $\left(
\begin{array}
[c]{cc}%
B_{11} & B_{12}\\
B_{21} & B_{22}%
\end{array}
\right)  $ with similarly-defined blocks $B_{11}$, $B_{12}$, $B_{21}$ and
$B_{22}$.

By the definition of $\alpha$ given in Theorem \ref{thm.japan}, we now have
$\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}B_{21}\right)  $. We now need
to compute $B_{12}A_{21}$ and $A_{12}B_{21}$ in order to simplify this.

Now, since $B_{12}=\left(
\begin{array}
[c]{cccc}%
... & ... & ... & ...\\
b_{3} & b_{4} & b_{5} & ...\\
b_{2} & b_{3} & b_{4} & ...\\
b_{1} & b_{2} & b_{3} & ...
\end{array}
\right)  $ and $A_{21}=\left(
\begin{array}
[c]{cccc}%
... & a_{-3} & a_{-2} & a_{-1}\\
... & a_{-4} & a_{-3} & a_{-2}\\
... & a_{-5} & a_{-4} & a_{-3}\\
... & ... & ... & ...
\end{array}
\right)  $, the matrix $B_{12}A_{21}$ is a matrix whose rows and columns are
indexed by nonpositive integers, and whose $\left(  i,j\right)  $-th block
equals $\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k-\left(  i+1\right)
}a_{-k+\left(  j+1\right)  }$ for any pair of negative integers $i$ and $j$.
Similarly, the matrix $A_{12}B_{21}$ is a matrix whose rows and columns are
indexed by nonpositive integers, and whose $\left(  i,j\right)  $-th block
equals $\sum\limits_{k\in\mathbb{Z};\ k>0}a_{k-\left(  i+1\right)
}b_{-k+\left(  j+1\right)  }$ for any pair of negative integers $i$ and $j$.
Thus, the matrix $-B_{12}A_{21}+A_{12}B_{21}$ is a matrix whose rows and
columns are indexed by nonpositive integers, and whose $\left(  i,j\right)
$-th block equals $-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k-\left(  i+1\right)
}a_{-k+\left(  j+1\right)  }+\sum\limits_{k\in\mathbb{Z};\ k>0}a_{k-\left(
i+1\right)  }b_{-k+\left(  j+1\right)  }$ for any pair of negative integers
$i$ and $j$. But since $\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}%
B_{21}\right)  $ is clearly the sum of the traces of the $\left(  i,i\right)
$-th blocks of the matrix $-B_{12}A_{21}+A_{12}B_{21}$ over all negative
integers $i$, we thus have%
\begin{align*}
\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}B_{21}\right)   &
=\sum\limits_{i\in\mathbb{Z};\ i<0}\operatorname*{Tr}\left(  -\sum
\limits_{k\in\mathbb{Z};\ k>0}b_{k-\left(  i+1\right)  }a_{-k+\left(
i+1\right)  }+\sum\limits_{k\in\mathbb{Z};\ k>0}a_{k-\left(  i+1\right)
}b_{-k+\left(  i+1\right)  }\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\leq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k-i}a_{-k+i}+\sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k-i}b_{-k+i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}i+1\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}+\sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k+i}b_{-k-i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}-i\text{ in the first sum}\right)  .
\end{align*}


We are now going to split the first sum on the right hand side and get the
$\operatorname*{Tr}$ out of it. To see that this is allowed, we notice that
each of the infinite sums $\sum\limits_{\substack{\left(  i,k\right)
\in\mathbb{Z}^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}$ and $\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}a_{k+i}b_{-k-i}$ converges with respect to the discrete
topology\footnote{\textit{Proof.} Since $\sum\limits_{k\in\mathbb{Z}}%
a_{k}t^{k}=a\left(  t\right)  \in L\mathfrak{gl}_{n}$, only finitely many
$k\in\mathbb{Z}$ satisfy $a_{k}\neq0$. Hence, there exists some $N\in
\mathbb{Z}$ such that every $\nu\in\mathbb{Z}$ satisfying $\nu<N$ satisfies
$a_{\nu}=0$. Consider this $N$. Any pair $\left(  i,k\right)  \in
\mathbb{Z}^{2}$ such that $k+i>-N$ satisfies $-k-i=-\underbrace{\left(
k+i\right)  }_{>-N}<N$ and thus $a_{-k-i}=0$ (because we know that every
$\nu\in\mathbb{Z}$ satisfying $\nu<N$ satisfies $a_{\nu}=0$) and thus
$b_{k+i}a_{-k-i}=0$. Thus, all but finitely many pairs $\left(  i,k\right)
\in\mathbb{Z}^{2}$ such that $i\geq0$ and $k>0$ satisfy $b_{k+i}a_{-k-i}=0$
(because it is clear that all but finitely many pairs $\left(  i,k\right)
\in\mathbb{Z}^{2}$ such that $i\geq0$ and $k>0$ satisfy $k+i>-N$). In other
words, the sum $\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}%
^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}$ converges with respect to the discrete
topology. A similar argument shows that the sum $\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}a_{k+i}b_{-k-i}$ converges with respect to the discrete topology.}.
Hence, we can transform these sums as we please: For example,%
\begin{align*}
&  \sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}b_{k+i}a_{-k-i}\\
&  =\sum\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}}\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0;\\k+i=\ell}}\underbrace{b_{k+i}}_{\substack{=b_{\ell}\\\text{(since
}k+i=\ell\text{)}}}\underbrace{a_{-k-i}}_{\substack{=a_{-\left(  k+i\right)
}=a_{-\ell}\\\text{(since }k+i=\ell\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }k+i>0\text{ for all }i\geq0\text{ and }k>0\right) \\
&  =\sum\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}}\underbrace{\sum
\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0;\\k+i=\ell}}b_{\ell}a_{-\ell}}_{\substack{=\ell b_{\ell}a_{-\ell
}\\\text{(since there exist exactly }\ell\text{ pairs }\left(  i,k\right)
\in\mathbb{Z}^{2}\\\text{satisfying }i\geq0\text{, }k>0\text{ and }%
k+i=\ell\text{)}}}=\sum\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}%
}\ell\underbrace{b_{\ell}a_{-\ell}}_{=a_{-\ell}b_{\ell}}=\sum
\limits_{\substack{\ell\in\mathbb{Z};\\\ell>0}}\ell a_{-\ell}b_{\ell}%
=\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{-k}b_{k}%
\end{align*}
(here, we renamed the summation index $\ell$ as $k$) and similarly
$\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2};\\i\geq
0;\ k>0}}a_{k+i}b_{-k-i}=\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}%
}ka_{k}b_{-k}$.

But
\begin{align*}
&  \omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right) \\
&  =\sum\limits_{k\in\mathbb{Z}}k\operatorname*{Tr}\left(  a_{k}b_{-k}\right)
=\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k<0}}k\operatorname*{Tr}%
\left(  a_{k}b_{-k}\right)  }_{=\operatorname*{Tr}\left(  \sum
\limits_{\substack{k\in\mathbb{Z};\\k<0}}ka_{k}b_{-k}\right)  }%
+\underbrace{0\operatorname*{Tr}\left(  a_{0}b_{-0}\right)  }_{=0}%
+\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}k\operatorname*{Tr}%
\left(  a_{k}b_{-k}\right)  }_{=\operatorname*{Tr}\left(  \sum
\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{k}b_{-k}\right)  }\\
&  =\operatorname*{Tr}\left(  \sum\limits_{\substack{k\in\mathbb{Z}%
;\\k<0}}ka_{k}b_{-k}\right)  +\operatorname*{Tr}\left(  \sum
\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{k}b_{-k}\right)
=\operatorname*{Tr}\left(  \sum\limits_{\substack{k\in\mathbb{Z}%
;\\k<0}}\left(  -k\right)  a_{-k}b_{k}\right)  +\operatorname*{Tr}\left(
\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{k}b_{-k}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k\text{ for
}-k\text{ in the first sum}\right) \\
&  =\operatorname*{Tr}\left(  -\underbrace{\sum\limits_{\substack{k\in
\mathbb{Z};\\k>0}}ka_{-k}b_{k}}_{=\sum\limits_{\substack{\left(  i,k\right)
\in\mathbb{Z}^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}}\right)  +\operatorname*{Tr}%
\left(  \underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k>0}}ka_{k}b_{-k}%
}_{=\sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2}%
;\\i\geq0;\ k>0}}a_{k+i}b_{-k-i}}\right) \\
&  =\operatorname*{Tr}\left(  -\sum\limits_{\substack{\left(  i,k\right)
\in\mathbb{Z}^{2};\\i\geq0;\ k>0}}b_{k+i}a_{-k-i}\right)  +\operatorname*{Tr}%
\left(  \sum\limits_{\substack{\left(  i,k\right)  \in\mathbb{Z}^{2}%
;\\i\geq0;\ k>0}}a_{k+i}b_{-k-i}\right) \\
&  =\operatorname*{Tr}\left(  -\sum\limits_{i\in\mathbb{Z};\ i\geq0}%
\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}\right)  +\operatorname*{Tr}%
\left(  \sum\limits_{i\in\mathbb{Z};\ i\geq0}\sum\limits_{k\in\mathbb{Z}%
;\ k>0}a_{k+i}b_{-k-i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have unfolded our single sums
into double sums}\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}\right)  +\sum
\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(  \sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k+i}b_{-k-i}\right) \\
&  =\sum\limits_{i\in\mathbb{Z};\ i\geq0}\operatorname*{Tr}\left(
-\sum\limits_{k\in\mathbb{Z};\ k>0}b_{k+i}a_{-k-i}+\sum\limits_{k\in
\mathbb{Z};\ k>0}a_{k+i}b_{-k-i}\right)  =\operatorname*{Tr}\left(
-B_{12}A_{21}+A_{12}B_{21}\right) \\
&  =\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  .
\end{align*}
Thus, $\alpha\left(  a\left(  t\right)  ,b\left(  t\right)  \right)
=\omega\left(  a\left(  t\right)  ,b\left(  t\right)  \right)  $ is proven, so
we have verified Proposition \ref{prop.ainf.alphaomega}.

Note that Proposition \ref{prop.ainf.alphaomega} gives a new proof of
Proposition \ref{prop.japan.nontr}. This proof (whose details are left to the
reader) uses two easy facts:

\begin{itemize}
\item If $\sigma:\mathfrak{g}\times\mathfrak{g}\rightarrow\mathbb{C}$ is a
$2$-coboundary on a Lie algebra $\mathfrak{g}$, and $\mathfrak{h}$ is a Lie
subalgebra of $\mathfrak{g}$, then $\sigma\mid_{\mathfrak{h}\times
\mathfrak{h}}$ must be a $2$-coboundary on $\mathfrak{h}$.

\item For any positive integer $n$, the $2$-cocycle $\omega$ of Proposition
\ref{prop.ainf.alphaomega} is not a $2$-coboundary.
\end{itemize}

But if we look closely at this argument, we see that it is not a completely
new proof; it is a direct generalization of the proof of Proposition
\ref{prop.japan.nontr} that we gave above. In fact, in the particular case
when $n=1$, our embedding of $L\mathfrak{gl}_{n}$ into $\overline
{\mathfrak{a}_{\infty}}$ becomes the canonical injection of the abelian Lie
subalgebra $\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $ into
$\overline{\mathfrak{a}_{\infty}}$ (where $T$ is as in the proof of
Proposition \ref{prop.japan.nontr}), and we see that what we just did was
generalizing that abelian Lie subalgebra.

\begin{definition}
Due to Proposition \ref{prop.ainf.alphaomega}, the restriction of the
$2$-cocycle $\alpha$ to $L\mathfrak{gl}_{n}\times L\mathfrak{gl}_{n}$ is the
$2$-cocycle $\omega$. Thus, the $1$-dimensional central extension of
$L\mathfrak{gl}_{n}$ determined by the $2$-cocycle $\omega$ canonically
injects into the $1$-dimensional central extension of $\overline
{\mathfrak{a}_{\infty}}$ determined by the $2$-cocycle $\alpha$. If we recall
that the $1$-dimensional central extension of $L\mathfrak{gl}_{n}$ by the
$2$-cocycle $\omega$ is $\widehat{\mathfrak{gl}_{n}}$ whereas the
$1$-dimensional central extension of $\overline{\mathfrak{a}_{\infty}}$
determined by the $2$-cocycle $\alpha$ is $\mathfrak{a}_{\infty}$, we can
rewrite this as follows: We have an injection $\widehat{\mathfrak{gl}_{n}%
}\rightarrow\mathfrak{a}_{\infty}$ which lifts the inclusion $L\mathfrak{gl}%
_{n}\subseteq\overline{\mathfrak{a}_{\infty}}$ and sends $K$ to $K$. We denote
this inclusion map $\widehat{\mathfrak{gl}_{n}}\rightarrow\mathfrak{a}%
_{\infty}$ by $\widehat{\operatorname*{Toep}\nolimits_{n}}$, but we will often
consider it as an inclusion.

Similarly, we can get an inclusion $\widehat{\mathfrak{sl}_{n}}\subseteq
\mathfrak{a}_{\infty}$ which lifts the inclusion $L\mathfrak{sl}_{n}%
\subseteq\overline{\mathfrak{a}_{\infty}}$.

So $\mathcal{B}^{\left(  m\right)  }\cong\mathcal{F}^{\left(  m\right)  }$ is
a module over $\widehat{\mathfrak{gl}_{n}}$ and $\widehat{\mathfrak{sl}_{n}}$
at level $1$ (this means that $K$ acts as $1$).
\end{definition}

\begin{corollary}
\label{cor.glnhat.div}There is a Lie algebra isomorphism $\widehat{\phi
}:\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{1}}$ which sends $K$ to $K$
and sends $a_{m}$ to $T^{m}\in\widehat{\mathfrak{gl}_{1}}$ for every
$m\in\mathbb{Z}$. (Here, we are considering the injection
$\widehat{\mathfrak{gl}_{1}}\rightarrow\mathfrak{a}_{\infty}$ as an inclusion,
so that $\widehat{\mathfrak{gl}_{1}}$ is identified with the image of this inclusion.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.glnhat.div}.} There is an obvious Lie
algebra isomorphism $\phi:\overline{\mathcal{A}}\rightarrow L\mathfrak{gl}%
_{1}$ which sends $a_{m}$ to $t^{m}\in L\mathfrak{gl}_{1}$ for every
$m\in\mathbb{Z}$. This isomorphism $\phi$ is easily seen to satisfy%
\begin{equation}
\omega\left(  \phi\left(  x\right)  ,\phi\left(  y\right)  \right)
=\omega^{\prime}\left(  x,y\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }%
x\in\overline{\mathcal{A}}\text{ and }y\in\overline{\mathcal{A}}\text{,}
\label{pf.glinhat.div.1}%
\end{equation}
where $\omega:L\mathfrak{gl}_{1}\times L\mathfrak{gl}_{1}\rightarrow
\mathbb{C}$ is the $2$-cocycle on $L\mathfrak{gl}_{1}$ defined in Proposition
\ref{prop.ainf.alphaomega}, and $\omega^{\prime}:\overline{\mathcal{A}}%
\times\overline{\mathcal{A}}\rightarrow\mathbb{C}$ is the $2$-cocycle on
$\overline{\mathcal{A}}$ defined by%
\[
\omega^{\prime}\left(  a_{k},a_{\ell}\right)  =\delta_{k,-\ell}%
\ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\text{ and }\ell\in
\mathbb{Z}.
\]
Thus, the Lie algebra isomorphism $\phi:\overline{\mathcal{A}}\rightarrow
L\mathfrak{gl}_{1}$ gives rise to an isomorphism $\widehat{\phi}$ from the
extension of $\overline{\mathcal{A}}$ defined by the $2$-cocycle
$\omega^{\prime}$ to the extension of $L\mathfrak{gl}_{1}$ defined by the
$2$-cocycle $\omega$. Since the extension of $\overline{\mathcal{A}}$ defined
by the $2$-cocycle $\omega^{\prime}$ is $\mathcal{A}$, while the extension of
$L\mathfrak{gl}_{1}$ defined by the $2$-cocycle $\omega$ is
$\widehat{\mathfrak{gl}_{1}}$, this rewrites as follows: The Lie algebra
isomorphism $\phi:\overline{\mathcal{A}}\rightarrow L\mathfrak{gl}_{1}$ gives
rise to an isomorphism $\widehat{\phi}:\mathcal{A}\rightarrow
\widehat{\mathfrak{gl}_{1}}$. This isomorphism $\widehat{\phi}$ sends $K$ to
$K$, and sends $a_{m}$ to $t^{m}\in\widehat{\mathfrak{gl}_{1}}$ for every
$m\in\mathbb{Z}$. Since $t^{m}$ corresponds to $T^{m}$ under our inclusion
$\widehat{\mathfrak{gl}_{1}}\rightarrow\mathfrak{a}_{\infty}$ (in fact,
$\operatorname*{Toep}\nolimits_{1}\left(  t^{m}\right)  =T^{m}$), this shows
that $\widehat{\phi}$ sends $a_{m}$ to $T^{m}\in\widehat{\mathfrak{gl}_{1}}$
for every $m\in\mathbb{Z}$. Corollary \ref{cor.glnhat.div} is thus proven.

\begin{proposition}
\label{prop.glnhat.T}Let $n$ be a positive integer. Consider the shift
operator $T$. Let us regard the injections $\overline{\mathfrak{a}_{\infty}%
}\rightarrow\mathfrak{a}_{\infty}$, $L\mathfrak{gl}_{n}\rightarrow
\overline{\mathfrak{a}_{\infty}}$ and $\widehat{\mathfrak{gl}_{n}}%
\rightarrow\mathfrak{a}_{\infty}$ as inclusions, so that $L\mathfrak{gl}_{n}$,
$\widehat{\mathfrak{gl}_{n}}$ and $\mathfrak{a}_{\infty}$ all become subspaces
of $\mathfrak{a}_{\infty}$.

\textbf{(a)} For every $m\in\mathbb{Z}$, we have $T^{m}\in L\mathfrak{gl}%
_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$.

\textbf{(b)} We have $\widehat{\mathfrak{gl}_{1}}\subseteq
\widehat{\mathfrak{gl}_{n}}$. Hence, the Lie algebra isomorphism
$\widehat{\phi}:\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{1}}$ constructed
in Corollary \ref{cor.glnhat.div} induces a Lie algebra injection
$\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{n}}$ (which sends every
$a\in\mathcal{A}$ to $\widehat{\phi}\left(  a\right)  \in
\widehat{\mathfrak{gl}_{n}}$). The restriction of the $\widehat{\mathfrak{gl}%
_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$ by means of this injection is
the $\mathcal{A}$-module $\mathcal{F}^{\left(  m\right)  }$ that we know.
\end{proposition}

\textit{First proof of Proposition \ref{prop.glnhat.T}.} \textbf{(a)} We
recall that $T=\left(
\begin{array}
[c]{cccccc}%
... & ... & ... & ... & ... & ...\\
... & 0 & 1 & 0 & 0 & ...\\
... & 0 & 0 & 1 & 0 & ...\\
... & 0 & 0 & 0 & 1 & ...\\
... & 0 & 0 & 0 & 0 & ...\\
... & ... & ... & ... & ... & ...
\end{array}
\right)  $ (this is the matrix which has $1$'s on the $1$-st diagonal and
$0$'s everywhere else). Clearly, $T\in\overline{\mathfrak{a}_{\infty}}$. We
want to prove that $T$ lies in $L\mathfrak{gl}_{n}\subseteq\overline
{\mathfrak{a}_{\infty}}$.

Let $a_{0}=\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & ... & 0\\
0 & 0 & 1 & ... & 0\\
0 & 0 & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & 0
\end{array}
\right)  $ (this is the $n\times n$ matrix which has $1$'s on the $1$-st
diagonal and $0$'s everywhere else).

Let $a_{1}=\left(
\begin{array}
[c]{ccccc}%
0 & 0 & 0 & ... & 0\\
0 & 0 & 0 & ... & 0\\
0 & 0 & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & 0\\
1 & 0 & 0 & ... & 0
\end{array}
\right)  $ (this is the $n\times n$ matrix which has a $1$ in its lowermost
leftmost corner, and $0$'s everywhere else).

Then, $T=\operatorname*{Toep}\nolimits_{n}\left(  a_{0}+ta_{1}\right)  $.
Thus, for every $m\in\mathbb{N}$, we have%
\begin{align*}
T^{m}  &  =\left(  \operatorname*{Toep}\nolimits_{n}\left(  a_{0}%
+ta_{1}\right)  \right)  ^{m}=\operatorname*{Toep}\nolimits_{n}\left(  \left(
a_{0}+ta_{1}\right)  ^{m}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{because of
Proposition \ref{prop.Toep.alg}}\right) \\
&  \in\operatorname*{Toep}\nolimits_{n}\left(  L\mathfrak{gl}_{n}\right)
=L\mathfrak{gl}_{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since we regard
}\operatorname*{Toep}\nolimits_{n}\text{ as an inclusion}\right)  .
\end{align*}
Since it is easy to see that $T^{-1}\in L\mathfrak{gl}_{n}$ as
well\footnote{This is analogous to $T\in L\mathfrak{gl}_{n}$ (because $T^{-1}$
is the matrix which has $1$'s on the $\left(  -1\right)  $-st diagonal and
$0$'s everywhere else).}, a similar argument yields that $\left(
T^{-1}\right)  ^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{N}$. In other
words, $T^{-m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{N}$. In other
words, $T^{m}\in\mathfrak{gl}_{n}$ for all nonpositive integers $m$. Combined
with the fact that $T^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{N}$,
this yields that $T^{m}\in L\mathfrak{gl}_{n}$ for all $m\in\mathbb{Z}$. Since
$L\mathfrak{gl}_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$, we thus have
$T^{m}\in L\mathfrak{gl}_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$ for all
$m\in\mathbb{Z}$. This proves Proposition \ref{prop.glnhat.T} \textbf{(a)}.

\textbf{(b)} For every $a\left(  t\right)  \in L\mathfrak{gl}_{1}$, we have
$\operatorname*{Toep}\nolimits_{1}\left(  a\left(  t\right)  \right)
\in\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $%
\ \ \ \ \footnote{\textit{Proof.} Let $a\left(  t\right)  \in L\mathfrak{gl}%
_{1}$. Write $a\left(  t\right)  $ in the form $\sum\limits_{i\in\mathbb{Z}%
}a_{i}t^{i}$ with $a_{i}\in\mathfrak{gl}_{1}$. Then, of course, the $a_{i}$
are scalars (since $\mathfrak{gl}_{1}=\mathbb{C}$). By the definition of
$\operatorname*{Toep}\nolimits_{1}$, we have%
\[
\operatorname*{Toep}\nolimits_{1}\left(  a\left(  t\right)  \right)  =\left(
\begin{array}
[c]{ccccc}%
... & ... & ... & ... & ...\\
... & a_{0} & a_{1} & a_{2} & ...\\
... & a_{-1} & a_{0} & a_{1} & ...\\
... & a_{-2} & a_{-1} & a_{0} & ...\\
... & ... & ... & ... & ...
\end{array}
\right)  =\sum\limits_{i\in\mathbb{Z}}a_{i}T^{i}\in\left\langle T^{j}%
\ \mid\ j\in\mathbb{Z}\right\rangle ,
\]
qed.}. Thus, $\operatorname*{Toep}\nolimits_{1}\left(  L\mathfrak{gl}%
_{1}\right)  \subseteq\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $.
Since we are considering $\operatorname*{Toep}\nolimits_{1}$ as an inclusion,
this becomes $L\mathfrak{gl}_{1}\subseteq\left\langle T^{j}\ \mid
\ j\in\mathbb{Z}\right\rangle $. Combined with $\left\langle T^{j}\ \mid
\ j\in\mathbb{Z}\right\rangle \subseteq L\mathfrak{gl}_{n}$ (because every
$m\in\mathbb{Z}$ satisfies $T^{m}\in L\mathfrak{gl}_{n}$ (according to
Proposition \ref{prop.glnhat.T} \textbf{(a)})), this yields $L\mathfrak{gl}%
_{1}\subseteq L\mathfrak{gl}_{n}$. Thus, $\widehat{\mathfrak{gl}_{1}}%
\subseteq\widehat{\mathfrak{gl}_{n}}$.

Hence, the Lie algebra isomorphism $\widehat{\phi}:\mathcal{A}\rightarrow
\widehat{\mathfrak{gl}_{1}}$ constructed in Corollary \ref{cor.glnhat.div}
induces a Lie algebra injection $\mathcal{A}\rightarrow\widehat{\mathfrak{gl}%
_{n}}$ (which sends every $a\in\mathcal{A}$ to $\widehat{\phi}\left(
a\right)  \in\widehat{\mathfrak{gl}_{n}}$). This injection is exactly the
embedding $\mathcal{A}\rightarrow\mathfrak{a}_{\infty}$ constructed in
Definition \ref{def.ainf.A} (apart from the fact that its target is
$\widehat{\mathfrak{gl}_{n}}$ rather than $\mathfrak{a}_{\infty}$). Hence, the
restriction of the $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(
m\right)  }$ by means of this injection is the $\mathcal{A}$-module
$\mathcal{F}^{\left(  m\right)  }$ that we know\footnote{because both the
$\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$ and
the $\mathcal{A}$-module $\mathcal{F}^{\left(  m\right)  }$ were defined as
restrictions of the $\mathfrak{a}_{\infty}$-module $\mathcal{F}^{\left(
m\right)  }$}. This proves Proposition \ref{prop.glnhat.T} \textbf{(b)}.

Our inclusions $L\mathfrak{gl}_{n}\subseteq\overline{\mathfrak{a}_{\infty}}$
and $\widehat{\mathfrak{gl}_{n}}\subseteq\mathfrak{a}_{\infty}$ can be
somewhat refined: For any positive integers $n$ and $N$ satisfying $n\mid N$,
we have $L\mathfrak{gl}_{n}\subseteq L\mathfrak{gl}_{N}$ and
$\widehat{\mathfrak{gl}_{n}}\subseteq\widehat{\mathfrak{gl}_{N}}$. Let us
formulate this more carefully without abuse of notation:

\begin{proposition}
\label{prop.glnhat.div}Let $n$ and $N$ be positive integers such that $n\mid
N$. Then, the inclusion $\operatorname*{Toep}\nolimits_{n}:L\mathfrak{gl}%
_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$ factors through the inclusion
$\operatorname*{Toep}\nolimits_{N}:L\mathfrak{gl}_{N}\rightarrow
\overline{\mathfrak{a}_{\infty}}$. More precisely:

Let $d=\dfrac{N}{n}$. Let $\operatorname*{Toep}\nolimits_{n,N}:L\mathfrak{gl}%
_{n}\rightarrow L\mathfrak{gl}_{N}$ be the map which sends every $a\left(
t\right)  \in L\mathfrak{gl}_{n}$ to%
\[
\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left(
\begin{array}
[c]{ccccc}%
a_{\left(  j-i\right)  d} & a_{\left(  j-i\right)  d+1} & a_{\left(
j-i\right)  d+2} & ... & a_{\left(  j-i\right)  d+\left(  d-1\right)  }\\
a_{\left(  j-i\right)  d-1} & a_{\left(  j-i\right)  d} & a_{\left(
j-i\right)  d+1} & ... & a_{\left(  j-i\right)  d+\left(  d-2\right)  }\\
a_{\left(  j-i\right)  d-2} & a_{\left(  j-i\right)  d-1} & a_{\left(
j-i\right)  d} & ... & a_{\left(  j-i\right)  d+\left(  d-3\right)  }\\
... & ... & ... & ... & ...\\
a_{\left(  j-i\right)  d-\left(  d-1\right)  } & a_{\left(  j-i\right)
d-\left(  d-2\right)  } & a_{\left(  j-i\right)  d-\left(  d-3\right)  } &
... & a_{\left(  j-i\right)  d}%
\end{array}
\right)  }_{\substack{\text{this is an }N\times N\text{-matrix constructed as
a }d\times d\text{-block matrix}\\\text{consisting of }n\times n\text{-blocks;
one can formally define this matrix}\\\text{as the }N\times N\text{-matrix
whose }\left(  nI+\alpha,nJ+\beta\right)  \text{-th entry equals}\\\text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a_{\left(  j-i\right)
d+J-I}\text{ for all }I\in\left\{  0,1,...,d-1\right\}  \text{,}\\J\in\left\{
0,1,...,d-1\right\}  \text{, }\alpha\in\left\{  1,2,...,n\right\}  \text{ and
}\beta\in\left\{  1,2,...,n\right\}  }}t^{\ell}\in L\mathfrak{gl}_{N}%
\]
(where we write $a\left(  t\right)  $ in the form $a\left(  t\right)
=\sum\limits_{i\in\mathbb{Z}}a_{i}t^{i}$ with $a_{i}\in\mathfrak{gl}_{n}$).

\textbf{(a)} We have $\operatorname*{Toep}\nolimits_{N}\circ
\operatorname*{Toep}\nolimits_{n,N}=\operatorname*{Toep}\nolimits_{n}$. In
other words, we can regard $\operatorname*{Toep}\nolimits_{n,N}$ as an
inclusion map $L\mathfrak{gl}_{n}\rightarrow L\mathfrak{gl}_{N}$ which forms a
commutative triangle with the inclusion maps $\operatorname*{Toep}%
\nolimits_{n}:L\mathfrak{gl}_{n}\rightarrow\overline{\mathfrak{a}_{\infty}}$
and $\operatorname*{Toep}\nolimits_{N}:L\mathfrak{gl}_{N}\rightarrow
\overline{\mathfrak{a}_{\infty}}$. In other words, if we consider
$L\mathfrak{gl}_{n}$ and $L\mathfrak{gl}_{N}$ as Lie subalgebras of
$\overline{\mathfrak{a}_{\infty}}$ (by means of the injections
$\operatorname*{Toep}\nolimits_{n}:L\mathfrak{gl}_{n}\rightarrow
\overline{\mathfrak{a}_{\infty}}$ and $\operatorname*{Toep}\nolimits_{N}%
:L\mathfrak{gl}_{N}\rightarrow\overline{\mathfrak{a}_{\infty}}$), then
$L\mathfrak{gl}_{n}\subseteq L\mathfrak{gl}_{N}$.

\textbf{(b)} If we consider $\operatorname*{Toep}\nolimits_{n,N}$ as an
inclusion map $L\mathfrak{gl}_{n}\rightarrow L\mathfrak{gl}_{N}$, then the
$2$-cocycle $\omega:L\mathfrak{gl}_{n}\times L\mathfrak{gl}_{n}\rightarrow
\mathbb{C}$ defined in Proposition \ref{prop.ainf.alphaomega} is the
restriction of the similarly-defined $2$-cocycle $\omega:L\mathfrak{gl}%
_{N}\times L\mathfrak{gl}_{N}\rightarrow\mathbb{C}$ (we also call it $\omega$
because it is constructed similarly) to $L\mathfrak{gl}_{n}\times
L\mathfrak{gl}_{n}$. As a consequence, the inclusion map $\operatorname*{Toep}%
\nolimits_{n,N}:L\mathfrak{gl}_{n}\rightarrow L\mathfrak{gl}_{N}$ induces a
Lie algebra injection $\widehat{\operatorname*{Toep}\nolimits_{n,N}%
}:\widehat{\mathfrak{gl}_{n}}\rightarrow\widehat{\mathfrak{gl}_{N}}$ which
satisfies $\widehat{\operatorname*{Toep}\nolimits_{N}}\circ
\widehat{\operatorname*{Toep}\nolimits_{n,N}}=\widehat{\operatorname*{Toep}%
\nolimits_{n}}$. Thus, this injection $\widehat{\operatorname*{Toep}%
\nolimits_{n,N}}$ forms a commutative triangle with the inclusion maps
$\widehat{\operatorname*{Toep}\nolimits_{n}}:\widehat{\mathfrak{gl}_{n}%
}\rightarrow\mathfrak{a}_{\infty}$ and $\widehat{\operatorname*{Toep}%
\nolimits_{N}}:\widehat{\mathfrak{gl}_{N}}\rightarrow\mathfrak{a}_{\infty}$.
In other words, if we consider $\widehat{\mathfrak{gl}_{n}}$ and
$\widehat{\mathfrak{gl}_{N}}$ as Lie subalgebras of $\mathfrak{a}_{\infty}$
(by means of the injections $\widehat{\operatorname*{Toep}\nolimits_{n}%
}:\widehat{\mathfrak{gl}_{n}}\rightarrow\mathfrak{a}_{\infty}$ and
$\widehat{\operatorname*{Toep}\nolimits_{N}}:\widehat{\mathfrak{gl}_{N}%
}\rightarrow\mathfrak{a}_{\infty}$), then $\widehat{\mathfrak{gl}_{n}%
}\subseteq\widehat{\mathfrak{gl}_{N}}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glnhat.div}.} \textbf{(a)} The proof of
Proposition \ref{prop.glnhat.div} \textbf{(a)} is completely straightforward.
(One has to show that the $\left(  Ni+nI+\alpha,Nj+nJ+\beta\right)  $-th entry
of $\left(  \operatorname*{Toep}\nolimits_{N}\circ\operatorname*{Toep}%
\nolimits_{n,N}\right)  \left(  a\left(  t\right)  \right)  $ equals the
$\left(  Ni+nI+\alpha,Nj+nJ+\beta\right)  $-th entry of $\operatorname*{Toep}%
\nolimits_{n}\left(  a\left(  t\right)  \right)  $ for every $a\left(
t\right)  \in L\mathfrak{gl}_{n}$, every $i\in\mathbb{Z}$, every
$j\in\mathbb{Z}$, every $I\in\left\{  0,1,...,d-1\right\}  $, $J\in\left\{
0,1,...,d-1\right\}  $, $\alpha\in\left\{  1,2,...,n\right\}  $ and $\beta
\in\left\{  1,2,...,n\right\}  $.)

\textbf{(b)} The $2$-cocycle $\omega:L\mathfrak{gl}_{n}\times L\mathfrak{gl}%
_{n}\rightarrow\mathbb{C}$ defined in Proposition \ref{prop.ainf.alphaomega}
is the restriction of the similarly-defined $2$-cocycle $\omega:L\mathfrak{gl}%
_{N}\times L\mathfrak{gl}_{N}\rightarrow\mathbb{C}$ to $L\mathfrak{gl}%
_{n}\times L\mathfrak{gl}_{n}$. (This is because both of these $2$-cocycles
are restrictions of the Japanese cocycle $\alpha:\overline{\mathfrak{a}%
_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$, as
shown in Proposition \ref{prop.ainf.alphaomega}.) This proves Proposition
\ref{prop.glnhat.div}.

Note that Proposition \ref{prop.glnhat.div} can be used to derive Proposition
\ref{prop.glnhat.T}:

\textit{Second proof of Proposition \ref{prop.glnhat.T}.} \textbf{(a)} For
every $m\in\mathbb{Z}$, we have $T^{m}\in\widehat{\mathfrak{gl}_{1}}$ (because
the Lie algebra isomorphism $\widehat{\phi}$ constructed in Corollary
\ref{cor.glnhat.div} satisfies $\phi\left(  a_{m}\right)  =T^{m}$, so that
$T^{m}\in\phi\left(  a_{m}\right)  \in\widehat{\mathfrak{gl}_{1}}$). Thus, for
every $m\in\mathbb{Z}$, we have $T^{m}\in\widehat{\mathfrak{gl}_{1}}%
\cap\overline{\mathfrak{a}_{\infty}}=L\mathfrak{gl}_{1}$.

Due to Proposition \ref{prop.glnhat.div} \textbf{(a)}, we have $L\mathfrak{gl}%
_{1}\subseteq L\mathfrak{gl}_{n}$ (since $1\mid n$). Thus, for every
$m\in\mathbb{Z}$, we have $T^{m}\in L\mathfrak{gl}_{1}\subseteq L\mathfrak{gl}%
_{n}\subseteq\widehat{\mathfrak{gl}_{n}}$. This proves Proposition
\ref{prop.glnhat.T} \textbf{(a)}.

\textbf{(b)} Due to Proposition \ref{prop.glnhat.div} \textbf{(b)}, we have
$\widehat{\mathfrak{gl}_{1}}\subseteq\widehat{\mathfrak{gl}_{n}}$ (since
$1\mid n$). Hence, the Lie algebra isomorphism $\widehat{\phi}:\mathcal{A}%
\rightarrow\widehat{\mathfrak{gl}_{1}}$ constructed in Corollary
\ref{cor.glnhat.div} induces a Lie algebra injection $\mathcal{A}%
\rightarrow\widehat{\mathfrak{gl}_{n}}$ (which sends every $a\in\mathcal{A}$
to $\widehat{\phi}\left(  a\right)  \in\widehat{\mathfrak{gl}_{n}}$). Formally
speaking, this injection is the map $\widehat{\operatorname*{Toep}%
\nolimits_{1,n}}\circ\widehat{\phi}:\mathcal{A}\rightarrow
\widehat{\mathfrak{gl}_{n}}$ (because the injection $\widehat{\mathfrak{gl}%
_{1}}\rightarrow\widehat{\mathfrak{gl}_{n}}$ is $\widehat{\operatorname*{Toep}%
\nolimits_{1,n}}$). Therefore, the restriction of the $\widehat{\mathfrak{gl}%
_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$ by means of this injection is%
\begin{align*}
&  \left(  \text{the restriction of the }\widehat{\mathfrak{gl}_{n}%
}\text{-module }\mathcal{F}^{\left(  m\right)  }\text{ by means of the
injection }\widehat{\operatorname*{Toep}\nolimits_{1,n}}\circ\widehat{\phi
}:\mathcal{A}\rightarrow\widehat{\mathfrak{gl}_{n}}\right) \\
&  =\left(  \text{the restriction of the }\mathfrak{a}_{\infty}\text{-module
}\mathcal{F}^{\left(  m\right)  }\text{ by means of the injection
}\widehat{\operatorname*{Toep}\nolimits_{n}}\circ\widehat{\operatorname*{Toep}%
\nolimits_{1,n}}\circ\widehat{\phi}:\mathcal{A}\rightarrow\mathfrak{a}%
_{\infty}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the }\widehat{\mathfrak{gl}_{n}}\text{-module }\mathcal{F}%
^{\left(  m\right)  }\text{ itself was the restriction of the }\mathfrak{a}%
_{\infty}\text{-module }\mathcal{F}^{\left(  m\right)  }\\
\text{by means of the injection }\widehat{\operatorname*{Toep}\nolimits_{n}%
}:\widehat{\mathfrak{gl}_{n}}\rightarrow\mathfrak{a}_{\infty}%
\end{array}
\right) \\
&  =\left(  \text{the restriction of the }\mathfrak{a}_{\infty}\text{-module
}\mathcal{F}^{\left(  m\right)  }\text{ by means of the injection
}\widehat{\operatorname*{Toep}\nolimits_{1}}\circ\widehat{\phi}:\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\widehat{\operatorname*{Toep}\nolimits_{n}}\circ
\widehat{\operatorname*{Toep}\nolimits_{1,n}}=\widehat{\operatorname*{Toep}%
\nolimits_{1}}\\
\text{(by Proposition \ref{prop.glnhat.div} \textbf{(b)}, applied to }n\text{
and }1\text{ instead of }N\text{ and }n\text{)}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the restriction of the }\mathfrak{a}_{\infty}\text{-module }%
\mathcal{F}^{\left(  m\right)  }\text{ by means of the}\\
\text{embedding }\mathcal{A}\rightarrow\mathfrak{a}_{\infty}\text{ constructed
in Definition \ref{def.ainf.A}}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }\widehat{\operatorname*{Toep}\nolimits_{1}}\circ\widehat{\phi
}:\mathcal{A}\rightarrow\mathfrak{a}_{\infty}\text{ is exactly the}\\
\text{embedding }\mathcal{A}\rightarrow\mathfrak{a}_{\infty}\text{ constructed
in Definition \ref{def.ainf.A}}%
\end{array}
\right) \\
&  =\left(  \text{the }\mathcal{A}\text{-module }\mathcal{F}^{\left(
m\right)  }\text{ that we know}\right)  .
\end{align*}
This proves Proposition \ref{prop.glnhat.T} \textbf{(b)}.

\subsection{The semidirect product $\protect\widetilde{\mathfrak{gl}_{n}}$ and
its representation theory}

\subsubsection{Extending affine Lie algebras by derivations}

Now we give a definition pertaining to general affine Lie algebras:

\begin{definition}
\label{def.gwave}If $\widehat{\mathfrak{g}}=L\mathfrak{g}\oplus\mathbb{C}K$ is
an affine Lie algebra (the $\oplus$ sign here only means a direct sum of
vector spaces, not a direct sum of Lie algebras), then there exists a unique
linear map $d:\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$ such
that $d\left(  a\left(  t\right)  \right)  =ta^{\prime}\left(  t\right)  $ for
every $a\left(  t\right)  \in L\mathfrak{g}$ (so that $d\left(  at^{\ell
}\right)  =\ell at^{\ell}$ for every $a\in\mathfrak{g}$ and $\ell\in
\mathbb{N}$) and $d\left(  K\right)  =0$. This linear map $d$ is a derivation
(as can be easily checked). Thus, the abelian Lie algebra $\mathbb{C}d$ (a
one-dimensional Lie algebra) acts on the Lie algebra $\widehat{\mathfrak{g}}$
by derivations (in the obvious way, with $d$ acting as $d$). Thus, a
semidirect product $\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$ is well-defined
(according to Definition \ref{def.semidir.lielie}).

Set $\widetilde{\mathfrak{g}}=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$.
Clearly, $\widetilde{\mathfrak{g}}=\mathbb{C}d\oplus\widehat{\mathfrak{g}}$ as
vector space. The Lie algebra $\widetilde{\mathfrak{g}}$ is graded by taking
the grading of $\widehat{\mathfrak{g}}$ and additionally giving $d$ the degree
$0$.
\end{definition}

One can wonder which $\widehat{\mathfrak{g}}$-modules can be extended to
$\widetilde{\mathfrak{g}}$-modules. This can't be generally answered, but here
is a partial uniqueness result:

\begin{lemma}
\label{lem.gwave.uniqueder}Let $\mathfrak{g}$ be a Lie algebra, and $d$ be the
unique derivation $\widehat{\mathfrak{g}}\rightarrow\widehat{\mathfrak{g}}$
constructed in Definition \ref{def.gwave}. Let $M$ be a $\widehat{\mathfrak{g}%
}$-module, and $v$ an element of $M$ such that $M$ is generated by $v$ as a
$\widehat{\mathfrak{g}}$-module. Then, there exists \textbf{at most one}
extension of the $\widehat{\mathfrak{g}}$-representation on $M$ to
$\widetilde{\mathfrak{g}}$ such that $dv=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.gwave.uniqueder}.} Let $\rho_{1}%
:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}M$ and $\rho
_{2}:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}M$ be two
extensions of the $\widehat{\mathfrak{g}}$-representation on $M$ to
$\widetilde{\mathfrak{g}}$ such that $\rho_{1}\left(  d\right)  v=0$ and
$\rho_{2}\left(  d\right)  v=0$. If we succeed in showing that $\rho_{1}%
=\rho_{2}$, then Lemma \ref{lem.gwave.uniqueder} will be proven.

Let $U$ be the subset $\left\{  u\in M\ \mid\ \rho_{1}\left(  d\right)
u=\rho_{2}\left(  d\right)  u\right\}  $ of $M$. Clearly, $U$ is a vector
subspace of $M$. Also, $v\in U$ (since $\rho_{1}\left(  d\right)  v=0=\rho
_{2}\left(  d\right)  v$). We will now show that $U$ is a
$\widehat{\mathfrak{g}}$-submodule of $M$.

In fact, since $\rho_{1}$ is an action of $\widetilde{\mathfrak{g}}$ on $M$,
every $m\in M$ and every $\alpha\in\widehat{\mathfrak{g}}$ satisfy%
\[
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \rho_{1}\left(
\alpha\right)  m\right)  -\left(  \rho_{1}\left(  \alpha\right)  \right)
\left(  \rho_{1}\left(  d\right)  m\right)  =\rho_{1}\left(  \left[
d,\alpha\right]  \right)  m.
\]
Since $\rho_{1}\left(  \alpha\right)  m=\alpha\rightharpoonup m$ (because the
action $\rho_{1}$ extends the $\widehat{\mathfrak{g}}$-representation on $M$)
and $\left[  d,\alpha\right]  =d\left(  \alpha\right)  $ (by the definition of
the Lie bracket on the semidirect product $\widetilde{\mathfrak{g}}%
=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$), this rewrites as follows: Every
$m\in M$ and every $\alpha\in\widehat{\mathfrak{g}}$ satisfy%
\[
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  -\left(  \rho_{1}\left(  \alpha\right)  \right)  \left(  \rho
_{1}\left(  d\right)  m\right)  =\rho_{1}\left(  d\left(  \alpha\right)
\right)  m.
\]
Since $\left(  \rho_{1}\left(  \alpha\right)  \right)  \left(  \rho_{1}\left(
d\right)  m\right)  =\alpha\rightharpoonup\left(  \rho_{1}\left(  d\right)
m\right)  $ (again because the action $\rho_{1}$ extends the
$\widehat{\mathfrak{g}}$-representation on $M$) and $\rho_{1}\left(  d\left(
\alpha\right)  \right)  m=\left(  d\left(  \alpha\right)  \right)
\rightharpoonup m$ (for the same reason), this further rewrites as follows:
Every $m\in M$ and every $\alpha\in\widehat{\mathfrak{g}}$ satisfy%
\begin{equation}
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  -\alpha\rightharpoonup\left(  \rho_{1}\left(  d\right)  m\right)
=\left(  d\left(  \alpha\right)  \right)  \rightharpoonup m.
\label{pf.gwave.uniqueder.1}%
\end{equation}


Now, let $m\in U$ and $\alpha\in\widehat{\mathfrak{g}}$ be arbitrary. Then,
$\rho_{1}\left(  d\right)  m=\rho_{2}\left(  d\right)  m$ (by the definition
of $U$, since $m\in U$), but we have%
\[
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  =\alpha\rightharpoonup\left(  \rho_{1}\left(  d\right)  m\right)
+\left(  d\left(  \alpha\right)  \right)  \rightharpoonup m
\]
(by (\ref{pf.gwave.uniqueder.1})) and%
\[
\left(  \rho_{2}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)  =\alpha\rightharpoonup\left(  \rho_{2}\left(  d\right)  m\right)
+\left(  d\left(  \alpha\right)  \right)  \rightharpoonup m
\]
(similarly). Hence,%
\begin{align*}
\left(  \rho_{1}\left(  d\right)  \right)  \left(  \alpha\rightharpoonup
m\right)   &  =\alpha\rightharpoonup\underbrace{\left(  \rho_{1}\left(
d\right)  m\right)  }_{=\rho_{2}\left(  d\right)  m}+\left(  d\left(
\alpha\right)  \right)  \rightharpoonup m\\
&  =\alpha\rightharpoonup\left(  \rho_{2}\left(  d\right)  m\right)  +\left(
d\left(  \alpha\right)  \right)  \rightharpoonup m=\left(  \rho_{2}\left(
d\right)  \right)  \left(  \alpha\rightharpoonup m\right)  ,
\end{align*}
so that $\alpha\rightharpoonup m\in U$ (by the definition of $U$).

Now forget that we fixed $m\in U$ and $\alpha\in\widehat{\mathfrak{g}}$. We
thus have showed that $\alpha\rightharpoonup m\in U$ for every $m\in U$ and
$\alpha\in\widehat{\mathfrak{g}}$. In other words, $U$ is a
$\widehat{\mathfrak{g}}$-submodule of $M$. Since $v\in U$, this yields that
$U$ is a $\widehat{\mathfrak{g}}$-submodule of $M$ containing $v$, and thus
must be the whole $M$ (since $M$ is generated by $v$ as a
$\widehat{\mathfrak{g}}$-module). Thus, $M=U=\left\{  u\in M\ \mid\ \rho
_{1}\left(  d\right)  u=\rho_{2}\left(  d\right)  u\right\}  $. Hence, every
$u\in M$ satisfies $\rho_{1}\left(  d\right)  u=\rho_{2}\left(  d\right)  u$.
Thus, $\rho_{1}\left(  d\right)  =\rho_{2}\left(  d\right)  $.

Combining $\rho_{1}\mid_{\widehat{\mathfrak{g}}}=\rho_{2}\mid
_{\widehat{\mathfrak{g}}}$ (because both $\rho_{1}$ and $\rho_{2}$ are
extensions of the $\widehat{\mathfrak{g}}$-representation on $M$, and thus
coincide on $\widehat{\mathfrak{g}}$) and $\rho_{1}\mid_{\mathbb{C}d}=\rho
_{2}\mid_{\mathbb{C}d}$ (because $\rho_{1}\left(  d\right)  =\rho_{2}\left(
d\right)  $), we obtain $\rho_{1}=\rho_{2}$ (because the vector space
$\widetilde{\mathfrak{g}}=\mathbb{C}d\ltimes\widehat{\mathfrak{g}}$ is
generated by $\mathbb{C}d$ and $\widehat{\mathfrak{g}}$, and thus two linear
maps which coincide on $\mathbb{C}d$ and on $\widehat{\mathfrak{g}}$ must be
identical). Thus, as we said above, Lemma \ref{lem.gwave.uniqueder} is proven.

\subsubsection{$\protect\widetilde{\mathfrak{gl}_{n}}$}

Applying Definition \ref{def.gwave} to $\mathfrak{g}=\mathfrak{gl}_{n}$, we
obtain a Lie algebra $\widetilde{\mathfrak{gl}_{n}}$. We want to study its
highest weight theory.

\begin{Convention}
For the sake of disambiguation, let us, in the following, use $E_{i,j}%
^{\mathfrak{gl}_{n}}$ to denote the elementary matrices of $\mathfrak{gl}_{n}$
(these are defined for $\left(  i,j\right)  \in\left\{  1,2,...,n\right\}
^{2}$), and use $E_{i,j}^{\mathfrak{gl}_{\infty}}$ to denote the elementary
matrices of $\mathfrak{gl}_{\infty}$ (these are defined for $\left(
i,j\right)  \in\mathbb{Z}^{2}$).
\end{Convention}

\begin{definition}
We can make $L\mathfrak{gl}_{n}$ into a graded Lie algebra by setting $\deg
E_{i,j}^{\mathfrak{gl}_{n}}=j-i$ (this, so far, is the standard grading on
$\mathfrak{gl}_{n}$) and $\deg t=n$. Consequently, $\widehat{\mathfrak{gl}%
_{n}}=\mathbb{C}K\oplus\mathfrak{gl}_{n}$ (this is just a direct sum of vector
spaces) becomes a graded Lie algebra with $\deg K=0$, and
$\widetilde{\mathfrak{gl}_{n}}=\mathbb{C}d\oplus\widehat{\mathfrak{gl}_{n}}$
(again, this is only a direct sum of vector spaces) becomes a graded Lie
algebra with $\deg d=0$.
\end{definition}

The triangular decomposition of $\widetilde{\mathfrak{gl}_{n}}$ is
$\widetilde{\mathfrak{gl}_{n}}=\widetilde{\mathfrak{n}_{-}}\oplus
\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}_{+}}$. Here,
$\widetilde{\mathfrak{h}}=\mathbb{C}K\oplus\mathbb{C}d\oplus\mathfrak{h}$
where $\mathfrak{h}$ is the Lie algebra of diagonal $n\times n$ matrices (in
other words, $\mathfrak{h}=\left\langle E_{1,1}^{\mathfrak{gl}_{n}}%
,E_{2,2}^{\mathfrak{gl}_{n}},...,E_{n,n}^{\mathfrak{gl}_{n}}\right\rangle $).
Further, $\widetilde{\mathfrak{n}_{+}}=\mathfrak{n}_{+}\oplus t\mathfrak{gl}%
_{n}\left[  t\right]  $ (where $\mathfrak{n}_{+}$ is the Lie algebra of
strictly upper-triangular matrices) and $\widetilde{\mathfrak{n}_{-}%
}=\mathfrak{n}_{-}\oplus t^{-1}\mathfrak{gl}_{n}\left[  t^{-1}\right]  $
(where $\mathfrak{n}_{-}$ is the Lie algebra of strictly lower-triangular matrices).

\begin{definition}
For every $m\in\mathbb{Z}$, define the weight $\widetilde{\omega}_{m}%
\in\widetilde{\mathfrak{h}}^{\ast}$ by%
\begin{align*}
\widetilde{\omega}_{m}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right)   &
=\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\leq\overline{m};\\
0,\text{ if }i>\overline{m}%
\end{array}
\right.  +\dfrac{m-\overline{m}}{n}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  ;\\
\widetilde{\omega}_{m}\left(  K\right)   &  =1;\\
\widetilde{\omega}_{m}\left(  d\right)   &  =0,
\end{align*}
where $\overline{m}$ is the remainder of $m$ modulo $n$ (that is, the element
of $\left\{  0,1,...,n-1\right\}  $ satisfying $m\equiv\overline
{m}\operatorname{mod}n$).
\end{definition}

Note that we can rewrite the definition of $\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  $ as%
\begin{align*}
&  \widetilde{\omega}_{m}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right) \\
&  =\left\{
\begin{array}
[c]{c}%
\left(  \text{the number of all }j\in\mathbb{Z}\text{ such that }j\equiv
i\operatorname{mod}n\text{ and }1\leq j\leq m\right)
,\ \ \ \ \ \ \ \ \ \ \text{if }m\geq0;\\
-\left(  \text{the number of all }j\in\mathbb{Z}\text{ such that }j\equiv
i\operatorname{mod}n\text{ and }m<j\leq0\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if
}m\leq0
\end{array}
\right.  .
\end{align*}


\subsubsection{The $\protect\widetilde{\mathfrak{gl}_{n}}$-module
$\mathcal{F}^{\left(  m\right)  }$}

A natural question to ask about representations of $\widehat{\mathfrak{g}}$ is
when and how they can be extended to representations of
$\widetilde{\mathfrak{g}}$. Here is an answer for $\mathfrak{g}%
=\widehat{\mathfrak{gl}_{n}}$ and the representation $\mathcal{F}^{\left(
m\right)  }$:

\begin{proposition}
\label{prop.glwave.F}Let $m\in\mathbb{Z}$. Let $\psi_{m}$ be the element
$v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\in\mathcal{F}^{\left(  m\right)
}$.

There exists a unique extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{F}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$. The action of $d$ in
this extension is given by%
\[
d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(
\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{proposition}

Note that the infinite sum $\sum\limits_{k\geq0}\left(  \left\lceil
\dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)
$ in Proposition \ref{prop.glwave.F} is well-defined\footnote{In fact,
$\left(  i_{0},i_{1},i_{2},...\right)  $ is an $m$-degression. Hence, every
sufficiently high $k\geq0$ satisfies $i_{k}+k=m$ and thus $m-k=i_{k}$ and thus
$\left\lceil \dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}%
{n}\right\rceil =0$. Thus, all but finitely many addends of the infinite sum
$\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  $ are zero, so that this
sum is well-defined, qed.}.

\textit{Proof of Proposition \ref{prop.glwave.F}.} \textit{Uniqueness:} Let us
prove that there exists \textbf{at most one} extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

By Proposition \ref{prop.glnhat.T} \textbf{(b)}, the $\mathcal{A}$-module
$\mathcal{F}^{\left(  m\right)  }$ is a restriction of the
$\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$. As a
consequence, $\mathcal{F}^{\left(  m\right)  }$ is generated by $\psi_{m}$ as
a $\widehat{\mathfrak{gl}_{n}}$-module (since $\mathcal{F}^{\left(  m\right)
}$ is generated by $\psi_{m}$ as an $\mathcal{A}$-module). Hence, by Lemma
\ref{lem.gwave.uniqueder} (applied to $\mathfrak{g}=\mathfrak{gl}_{n}$,
$M=\mathcal{F}^{\left(  m\right)  }$ and $v=\psi_{m}$), there exists
\textbf{at most one} extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{F}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

\textit{Existence:} Let us now show that there exists an extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

In fact, let us construct this extension. In order to do so, it is clearly
enough to define the action of $d$ on $\mathcal{F}^{\left(  m\right)  }$
(because an action of $\widehat{\mathfrak{gl}_{n}}$ on $\mathcal{F}^{\left(
m\right)  }$ is already defined), and then show that every $A\in
\widehat{\mathfrak{gl}_{n}}$ satisfies%
\begin{equation}
\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }},A\mid_{\mathcal{F}^{\left(
m\right)  }}\right]  =\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}%
\mid_{\mathcal{F}^{\left(  m\right)  }}. \label{pf.glwave.F.1}%
\end{equation}
\footnote{Here, for every $\xi\in\widetilde{\mathfrak{gl}_{n}}$, we denote by
$\xi\mid_{\mathcal{F}^{\left(  m\right)  }}$ the action of $\xi$ on
$\mathcal{F}^{\left(  m\right)  }$. Besides, $\left[  d,A\right]
_{\widetilde{\mathfrak{gl}_{n}}}$ means the Lie bracket of $d$ and $A$ in the
Lie algebra $\widetilde{\mathfrak{gl}_{n}}$.}

Let us define the action of $d$ on $\mathcal{F}^{\left(  m\right)  }$ by
stipulating that
\begin{equation}
d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(
\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge... \label{pf.glwave.F.2}%
\end{equation}
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. (This is
extended by linearity to the whole of $\mathcal{F}^{\left(  m\right)  }$,
since $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
_{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ is
a basis of $\mathcal{F}^{\left(  m\right)  }$.)

It is rather clear that (\ref{pf.glwave.F.2}) holds not only for every
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, but also for every
straying $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $%
.\ \ \ \ \footnote{In fact, if $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
straying $m$-degression with no two equal elements, and $\pi$ is its
straightening permutation, then $\sum\limits_{k\geq0}\left(  \left\lceil
\dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)
=\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{\pi^{-1}\left(  k\right)  }}{n}\right\rceil \right)  $,
and this readily yields (\ref{pf.glwave.F.2}). If $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $m$-degression with two equal elements,
then (\ref{pf.glwave.F.2}) is even more obvious (since both sides of
(\ref{pf.glwave.F.2}) are zero in this case).} Renaming $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ as $\left(  j_{0},j_{1},j_{2},...\right)  $ and
renaming the summation index $k$ as $p$, we can rewrite this as follows: We
have%
\begin{equation}
d\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  =\left(
\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  \right)  \cdot v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge... \label{pf.glwave.F.2stray}%
\end{equation}
for every straying $m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $.

We now need to prove that every $A\in\widehat{\mathfrak{gl}_{n}}$ satisfies
(\ref{pf.glwave.F.1}). Since this equation (\ref{pf.glwave.F.1}) is linear in
$A$, we need to check it only in the case when $A=K$ and in the case when
$A=at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and some $\ell\in\mathbb{Z}$
(because the vector space $\widehat{\mathfrak{gl}_{n}}$ is generated by $K$
and all elements of the form $at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and
some $\ell\in\mathbb{Z}$). But checking the equation (\ref{pf.glwave.F.1}) in
the case when $A=K$ is trivial\footnote{In fact, $K\mid_{\mathcal{F}^{\left(
m\right)  }}=\operatorname*{id}$, so that $\left[  d\mid_{\mathcal{F}^{\left(
m\right)  }},K\mid_{\mathcal{F}^{\left(  m\right)  }}\right]  =\left[
d\mid_{\mathcal{F}^{\left(  m\right)  }},\operatorname*{id}\right]  =0$, and
by the definition of a semidirect product of Lie algebras we have $\left[
d,K\right]  _{\widetilde{\mathfrak{gl}_{n}}}=d\left(  K\right)  =0$, so that
both sides of (\ref{pf.glwave.F.1}) are zero in the case $A=K$, so that
(\ref{pf.glwave.F.1}) trivially holds in the case when $A=K$.}. Hence, it only
remains to check the equation (\ref{pf.glwave.F.1}) in the case when
$A=at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and some $\ell\in\mathbb{Z}$.

So let $a\in\mathfrak{gl}_{n}$ and $\ell\in\mathbb{Z}$ be arbitrary. We can
WLOG assume that if $\ell=0$, then the diagonal entries of the matrix $a$ are
zero\footnote{Here is why this assumption is allowed:
\par
We must prove that every $a\in\mathfrak{gl}_{n}$ and $\ell\in\mathbb{Z}$
satisfy the equation (\ref{pf.glwave.F.1}) for $A=at^{\ell}$. In other words,
we must prove that every $a\in\mathfrak{gl}_{n}$ and $\ell\in\mathbb{Z}$
satisfy $\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }},\left(  at^{\ell
}\right)  \mid_{\mathcal{F}^{\left(  m\right)  }}\right]  =\left[  d,\left(
at^{\ell}\right)  \right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}%
^{\left(  m\right)  }}$. If $\ell\neq0$, then our assumption (that if $\ell
=0$, then the diagonal entries of the matrix $a$ are zero) is clearly allowed
(because it only makes a statement about the case $\ell=0$). So we only need
to consider the case $\ell=0$. In this case, the equation which we must prove
(this is the equation $\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }%
},\left(  at^{\ell}\right)  \mid_{\mathcal{F}^{\left(  m\right)  }}\right]
=\left[  d,\left(  at^{\ell}\right)  \right]  _{\widetilde{\mathfrak{gl}_{n}}%
}\mid_{\mathcal{F}^{\left(  m\right)  }}$) simplifies to $\left[
d\mid_{\mathcal{F}^{\left(  m\right)  }},a\mid_{\mathcal{F}^{\left(  m\right)
}}\right]  =\left[  d,a\right]  _{\widetilde{\mathfrak{gl}_{n}}}%
\mid_{\mathcal{F}^{\left(  m\right)  }}$. This equation is clearly linear in
$a$ for fixed $\ell$. Hence, we can WLOG\ assume that either the matrix $a$ is
diagonal, or all diagonal entries of the matrix $a$ are zero (because every
$n\times n$ matrix can be decomposed as a sum of a diagonal matrix with a
matrix all of whose entries are zero). But in the case when the matrix $a$ is
diagonal, the equation $\left[  d\mid_{\mathcal{F}^{\left(  m\right)  }}%
,a\mid_{\mathcal{F}^{\left(  m\right)  }}\right]  =\left[  d,a\right]
_{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(  m\right)  }}$ is
very easy to check (the details of this are left to the reader; of course, he
should use that $\ell=0$). Hence, it is enough to only consider the case when
the diagonal entries of the matrix $a$ are $0$.
\par
Thus, we are allowed to make the assumption that if $\ell=0$, then the
diagonal entries of the matrix $a$ are zero.}. Let us assume this. (The
purpose of this assumption is to ensure that we can apply Proposition
\ref{prop.glinf.ainfact} to $at^{\ell}$ in lieu of $a$.)

Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an $m$-degression.

We recall that, when we embedded $L\mathfrak{gl}_{n}$ into $\overline
{\mathfrak{a}_{\infty}}$, we identified the element $at^{\ell}\in
L\mathfrak{gl}_{n}$ with the matrix $\operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  $ whose $\left(  ni+\alpha,nj+\beta\right)  $-th entry
equals%
\[
\left\{
\begin{array}
[c]{l}%
\text{the }\left(  \alpha,\beta\right)  \text{-th entry of }%
a,\ \ \ \ \ \ \ \ \ \ \text{if }j-i=\ell;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }j-i\neq\ell
\end{array}
\right.
\]
for all $i\in\mathbb{Z}$, $j\in\mathbb{Z}$, $\alpha\in\left\{
1,2,...,n\right\}  $ and $\beta\in\left\{  1,2,...,n\right\}  $. Hence, for
every $j\in\mathbb{Z}$ and $\beta\in\left\{  1,2,...,n\right\}  $, we have%
\begin{align}
&  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{nj+\beta}\nonumber\\
&  =\sum\limits_{i\in\mathbb{Z}}\sum\limits_{\alpha\in\left\{
1,2,...,n\right\}  }\left\{
\begin{array}
[c]{l}%
\text{the }\left(  \alpha,\beta\right)  \text{-th entry of }%
a,\ \ \ \ \ \ \ \ \ \ \text{if }j-i=\ell;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }j-i\neq\ell
\end{array}
\right.  v_{ni+\alpha}\nonumber\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\underbrace{\sum
\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left\{
\begin{array}
[c]{l}%
\text{the }\left(  \alpha,\beta\right)  \text{-th entry of }%
a,\ \ \ \ \ \ \ \ \ \ \text{if }j-i=\ell;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }j-i\neq\ell
\end{array}
\right.  v_{ni+\alpha}}_{=\left(  \text{the }\left(  \alpha,\beta\right)
\text{-th entry of }a\right)  v_{n\left(  j-\ell\right)  +\alpha}}\nonumber\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right)  v_{n\left(
j-\ell\right)  +\alpha}. \label{pf.glwave.F.3}%
\end{align}


Note that the matrix $\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  $ has the property that, for every integer $i\leq0$, the $\left(
i,i\right)  $-th entry of $\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  $ is $0$. (This is due to our assumption that if $\ell=0$, then the
diagonal entries of the matrix $a$ are zero.) As a consequence, we can apply
Proposition \ref{prop.glinf.ainfact} to $\operatorname*{Toep}\nolimits_{n}%
\left(  at^{\ell}\right)  $ and $v_{i_{k}}$ instead of $a$ and $a_{k}$, and
obtain%
\begin{align}
&  \left(  \widehat{\rho}\left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.glwave.F.4}%
\end{align}


Now, we can check that, for every $k\geq0$, we have%
\begin{align}
&  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\right) \nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge.... \label{pf.glwave.F.4a}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.glwave.F.4a}):} Let $k\geq0$. Write the
integer $i_{k}$ in the form $nj+\beta$ for some $j\in\mathbb{Z}$ and $\beta
\in\left\{  1,2,...,n\right\}  $. Then,
\[
\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}=\left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \rightharpoonup v_{nj+\beta}=\sum\limits_{\alpha
\in\left\{  1,2,...,n\right\}  }\left(  \text{the }\left(  \alpha
,\beta\right)  \text{-th entry of }a\right)  v_{n\left(  j-\ell\right)
+\alpha}%
\]
due to (\ref{pf.glwave.F.3}). Hence,%
\begin{align}
&  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
\underbrace{\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  }_{=\sum\limits_{\alpha
\in\left\{  1,2,...,n\right\}  }\left(  \text{the }\left(  \alpha
,\beta\right)  \text{-th entry of }a\right)  v_{n\left(  j-\ell\right)
+\alpha}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(  j-\ell\right)
+\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge.... \label{pf.glwave.F.5}%
\end{align}
\par
Now, fix $\alpha\in\left\{  1,2,...,n\right\}  $. Let $\left(  j_{0}%
,j_{1},j_{2},...\right)  $ be the straying $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...,i_{k-1},n\left(  j-\ell\right)  +\alpha,i_{k+1}%
,i_{k+2},...\right)  $. Then, $j_{p}=i_{p}$ for every $p\geq0$ satisfying
$p\neq k$.
\par
Comparing $\left\lceil \dfrac{i_{k}}{n}\right\rceil =j+1$ (since
$i_{k}=nj+\beta$ with $\beta\in\left\{  1,2,...,n\right\}  $) with
$\left\lceil \dfrac{j_{k}}{n}\right\rceil =j-\ell+1$ (since $j_{k}=n\left(
j-\ell\right)  +\alpha$ with $\alpha\in\left\{  1,2,...,n\right\}  $), we get
$\left\lceil \dfrac{j_{k}}{n}\right\rceil =\left\lceil \dfrac{i_{k}}%
{n}\right\rceil -\ell$.
\par
Since $\left\lceil \dfrac{j_{p}}{n}\right\rceil =\left\lceil \dfrac{i_{p}}%
{n}\right\rceil $ for every $p\geq0$ satisfying $p\neq k$ (because every
$p\geq0$ satisfying $p\neq k$ satisfies $j_{p}=i_{p}$), the two sums
$\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  $ and $\sum\limits_{p\geq
0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{i_{p}}%
{n}\right\rceil \right)  $ differ only in their $k$-th addends. Since the
$k$-th addends differ in $\ell$ (because $\left\lceil \dfrac{j_{k}}%
{n}\right\rceil =\left\lceil \dfrac{i_{k}}{n}\right\rceil -\ell$), this yields
$\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  =\sum\limits_{p\geq
0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{i_{p}}%
{n}\right\rceil \right)  +\ell$.
\par
But since $\left(  i_{0},i_{1},i_{2},...,i_{k-1},n\left(  j-\ell\right)
+\alpha,i_{k+1},i_{k+2},...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  $,
we have
\par%
\begin{align}
&  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\right) \nonumber\\
&  =d\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
=\underbrace{\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}%
{n}\right\rceil -\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  \right)
}_{=\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell}\cdot
\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...}%
_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\\\text{(since }\left(  j_{0},j_{1},j_{2},...\right)  =\left(
i_{0},i_{1},i_{2},...,i_{k-1},n\left(  j-\ell\right)  +\alpha,i_{k+1}%
,i_{k+2},...\right)  \text{)}}}\nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \cdot
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(
j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glwave.F.6}%
\end{align}
Now forget that we fixed $\alpha$. Now, applying $d$ to the equality
(\ref{pf.glwave.F.5}), we get%
\begin{align*}
&  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\right) \\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{d\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(  j-\ell\right)  +\alpha}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right)  }_{\substack{=\left(
\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \cdot
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{n\left(
j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge
...\\\text{(by (\ref{pf.glwave.F.6}))}}}\\
&  =\sum\limits_{\alpha\in\left\{  1,2,...,n\right\}  }\left(  \text{the
}\left(  \alpha,\beta\right)  \text{-th entry of }a\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\left(  \sum\limits_{p\geq0}\left(  \left\lceil
\dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)
+\ell\right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\sum\limits_{\alpha\in\left\{
1,2,...,n\right\}  }\left(  \text{the }\left(  \alpha,\beta\right)  \text{-th
entry of }a\right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{n\left(  j-\ell\right)  +\alpha}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...}_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(
at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\\text{(by (\ref{pf.glwave.F.5}))}}}\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \cdot
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(
\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)
\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...,
\end{align*}
so that (\ref{pf.glwave.F.4a}) is proven.}

Since $A=at^{\ell}$, we have $A\mid_{\mathcal{F}^{\left(  m\right)  }}=\left(
at^{\ell}\right)  \mid_{\mathcal{F}^{\left(  m\right)  }}=\widehat{\rho
}\left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  \right)  $
(because the element $at^{\ell}\in L\mathfrak{gl}_{n}$ was identified with the
matrix $\operatorname*{Toep}\nolimits_{n}\left(  at^{\ell}\right)  $ and this
matrix acts on $\mathcal{F}^{\left(  m\right)  }$ via $\widehat{\rho}$). Thus,
we can rewrite (\ref{pf.glwave.F.4}) as%
\begin{align}
&  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.glwave.F.4b}%
\end{align}
Applying $d$ to this equality, we get%
\begin{align}
&  d\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \nonumber\\
&  =\sum\limits_{k\geq0}\underbrace{d\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}%
\nolimits_{n}\left(  at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right)  }%
_{\substack{=\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}%
{n}\right\rceil -\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)
+\ell\right)  \cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}%
\wedge\left(  \left(  \operatorname*{Toep}\nolimits_{n}\left(  at^{\ell
}\right)  \right)  \rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\\text{(by (\ref{pf.glwave.F.4a}))}}}\nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\sum\limits_{k\geq0}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  \operatorname*{Toep}%
\nolimits_{n}\left(  at^{\ell}\right)  \right)  \rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{\substack{=\left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \\\text{(by (\ref{pf.glwave.F.4b}%
))}}}\nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  +\ell\right)  \left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  \right)  \left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }%
}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.glwave.F.10}%
\end{align}
Since%
\begin{align*}
&  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\underbrace{\left(  d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  }_{\substack{=\left(  \sum\limits_{p\geq0}\left(
\left\lceil \dfrac{m-p}{n}\right\rceil -\left\lceil \dfrac{j_{p}}%
{n}\right\rceil \right)  \right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\\\text{(by (\ref{pf.glwave.F.2stray}), applied to }\left(
j_{0},j_{1},j_{2},...\right)  =\left(  i_{0},i_{1},i_{2},...\right)  \text{)}%
}}\\
&  =\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  \left(
\sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{j_{p}}{n}\right\rceil \right)  \right)  \cdot v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}{n}\right\rceil
-\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  \right)  \left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\end{align*}
and%
\begin{align*}
&  \left(  \left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid
_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left(  \ell A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since, by the definition of the Lie bracket on the semidirect product}\\
\widetilde{\mathfrak{gl}_{n}}=\mathbb{C}d\ltimes\widehat{\mathfrak{gl}_{n}%
}\text{, we have }\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}%
}=d\underbrace{\left(  A\right)  }_{=at^{\ell}}=d\left(  at^{\ell}\right)
=\ell\underbrace{at^{\ell}}_{=A}=\ell A
\end{array}
\right) \\
&  =\ell\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\end{align*}
we can rewrite (\ref{pf.glwave.F.10}) as%
\begin{align*}
&  d\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\underbrace{\left(  \sum\limits_{p\geq0}\left(  \left\lceil \dfrac{m-p}%
{n}\right\rceil -\left\lceil \dfrac{i_{p}}{n}\right\rceil \right)  \right)
\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=\left(  A\mid
_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  d\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\ell\left(  A\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  }_{=\left(  \left[  d,A\right]  _{\widetilde{\mathfrak{gl}%
_{n}}}\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }\\
&  =\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \left(  d\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  +\left(
\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  .
\end{align*}
In other words,%
\begin{align*}
&  \left(  \left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  +\left(  \left[
d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\left(  \left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  +\left[
d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  .
\end{align*}
Since this holds for every $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $, this yields that $\left(  d\mid_{\mathcal{F}^{\left(
m\right)  }}\right)  \circ\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }%
}\right)  =\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  +\left[
d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}$ (because $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression}}$ is a basis of $\mathcal{F}^{\left(  m\right)  }$). In
other words,%
\[
\left[  d,A\right]  _{\widetilde{\mathfrak{gl}_{n}}}\mid_{\mathcal{F}^{\left(
m\right)  }}=\left(  d\mid_{\mathcal{F}^{\left(  m\right)  }}\right)
\circ\left(  A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  -\left(
A\mid_{\mathcal{F}^{\left(  m\right)  }}\right)  \circ\left(  d\mid
_{\mathcal{F}^{\left(  m\right)  }}\right)  =\left[  d\mid_{\mathcal{F}%
^{\left(  m\right)  }},A\mid_{\mathcal{F}^{\left(  m\right)  }}\right]  .
\]
In other words, (\ref{pf.glwave.F.1}) holds.

We have thus checked the equation (\ref{pf.glwave.F.1}) in the case when
$A=at^{\ell}$ for some $a\in\mathfrak{gl}_{n}$ and some $\ell\in\mathbb{Z}$.
As explained above, this completes the proof of the equation
(\ref{pf.glwave.F.1}) for every $A\in\widehat{\mathfrak{gl}_{n}}$. Hence, we
have constructed an action of $d$ on $\mathcal{F}^{\left(  m\right)  }$. This
action clearly satisfies $d\psi_{m}=0$ (because $\psi_{m}=v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...$, so that%
\begin{align*}
d\psi_{m}  &  =d\left(  v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\right) \\
&  =\left(  \sum\limits_{k\geq0}\underbrace{\left(  \left\lceil \dfrac{m-k}%
{n}\right\rceil -\left\lceil \dfrac{m-k}{n}\right\rceil \right)  }%
_{=0}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glwave.F.2}), applied to
}i_{k}=m-k\right) \\
&  =0
\end{align*}
). Hence, we have proven the existence of an extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$.

Altogether, we have now proven both the uniqueness and the existence of an
extension of the $\widehat{\mathfrak{gl}_{n}}$-representation on
$\mathcal{F}^{\left(  m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such
that $d\psi_{m}=0$. Moreover, in the proof of the existence, we have showed
that the action of $d$ in this extension is given by%
\[
d\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(
\sum\limits_{k\geq0}\left(  \left\lceil \dfrac{m-k}{n}\right\rceil
-\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)  \right)  \cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ (because we
defined this extension using (\ref{pf.glwave.F.2})). This completes the proof
of Proposition \ref{prop.glwave.F}.

Next, an irreducibility result:

\begin{proposition}
\label{prop.glwave.F.irr}Let $m\in\mathbb{Z}$. Let $\psi_{m}$ be the element
$v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\in\mathcal{F}^{\left(  m\right)
}$.

\textbf{(a)} The $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(
m\right)  }$ is irreducible.

\textbf{(b)} Let $\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}%
}:\widetilde{\mathfrak{gl}_{n}}\rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ denote the unique extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{F}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}=0$. (This
is well-defined due to Proposition \ref{prop.glwave.F}.)

The $\widetilde{\mathfrak{gl}_{n}}$-module $\left(  \mathcal{F}^{\left(
m\right)  },\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}}\right)  $ is
irreducible with highest weight $\widetilde{\omega}_{m}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glwave.F.irr}.} \textbf{(a)} By
Proposition \ref{prop.F.irrep}, we know that $F$ is an irreducible
$\mathcal{A}_{0}$-module. In other words, $\mathcal{B}^{\left(  m\right)  }$
is an irreducible $\mathcal{A}_{0}$-module (since $\mathcal{B}^{\left(
m\right)  }=F_{m}=F$ as $\mathcal{A}_{0}$-modules). Hence, $\mathcal{B}%
^{\left(  m\right)  }$ is also an irreducible $\mathcal{A}$-module (since the
$\mathcal{A}_{0}$-module $\mathcal{B}^{\left(  m\right)  }$ is a restriction
of the $\mathcal{A}$-module $\mathcal{B}^{\left(  m\right)  }$).

Since the Boson-Fermion correspondence $\sigma_{m}:\mathcal{B}^{\left(
m\right)  }\rightarrow\mathcal{F}^{\left(  m\right)  }$ is an $\mathcal{A}%
$-module isomorphism, we have $\mathcal{B}^{\left(  m\right)  }\cong%
\mathcal{F}^{\left(  m\right)  }$ as $\mathcal{A}$-modules. Since
$\mathcal{B}^{\left(  m\right)  }$ is an irreducible $\mathcal{A}$-module,
this yields that $\mathcal{F}^{\left(  m\right)  }$ is an irreducible
$\mathcal{A}$-module.

By Proposition \ref{prop.glnhat.T} \textbf{(b)}, the $\mathcal{A}$-module
$\mathcal{F}^{\left(  m\right)  }$ is a restriction of the
$\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}^{\left(  m\right)  }$. Since
the $\mathcal{A}$-module $\mathcal{F}^{\left(  m\right)  }$ is irreducible,
this yields that the $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{F}%
^{\left(  m\right)  }$ is irreducible. Proposition \ref{prop.glwave.F.irr}
\textbf{(a)} is proven.

\textbf{(b)} It is easy to check that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$
and $x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)  \psi_{m}$ for every
$x\in\widetilde{\mathfrak{h}}$.

\textit{Proof.} Proving that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$ is easy,
since $\widetilde{\mathfrak{n}_{+}}$ embeds into $\mathfrak{a}_{\infty}$ as
strictly upper-triangular matrices (and $\mathcal{F}^{\left(  m\right)  }$ is
a graded $\mathfrak{a}_{\infty}$-module).

In order to prove that $x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)
\psi_{m}$ for every $x\in\widetilde{\mathfrak{h}}$, we must show that
$E_{i,i}^{\mathfrak{gl}_{n}}\psi_{m}=\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  \psi_{m}$ for every $i\in\left\{
1,2,...,n\right\}  $. (In fact, this is enough, because the relations
$K\psi_{m}=\widetilde{\omega}_{m}\left(  K\right)  \psi_{m}$ and $d\psi
_{m}=\widetilde{\omega}_{m}\left(  d\right)  \psi_{m}$ follow directly from
$\widehat{\rho}\left(  K\right)  =\operatorname*{id}$ and $d\psi_{m}=0$.)

Let $i\in\left\{  1,2,...,n\right\}  $. Use $\operatorname*{Toep}%
\nolimits_{n}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right)  =\sum\limits_{j\equiv
i\operatorname{mod}n}E_{j,j}^{\mathfrak{gl}_{\infty}}$ to conclude that
\[
\widehat{\rho}\left(  E_{i,i}^{\mathfrak{gl}_{n}}\right)  \psi_{m}%
=\underbrace{\left(  \left\{
\begin{array}
[c]{c}%
1,\text{ if }i\leq\overline{m};\\
0,\text{ if }i>\overline{m}%
\end{array}
\right.  +\dfrac{m-\overline{m}}{n}\right)  }_{=\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  }\psi_{m}=\widetilde{\omega}_{m}\left(
E_{i,i}^{\mathfrak{gl}_{n}}\right)  \psi_{m},
\]
where $\overline{m}$ is the element of $\left\{  0,1,...,n-1\right\}  $
satisfying $m\equiv\overline{m}\operatorname{mod}n$.

Thus, we have checked that $\widetilde{\mathfrak{n}_{+}}\psi_{m}=0$ and
$x\psi_{m}=\widetilde{\omega}_{m}\left(  x\right)  \psi_{m}$ for every
$x\in\widetilde{\mathfrak{h}}$. Thus, $\psi_{m}$ is a singular vector of
weight $\widetilde{\omega}_{m}$. In other words, $\psi_{m}\in
\operatorname*{Sing}\nolimits_{\widetilde{\omega}_{m}}\left(  \mathcal{F}%
^{\left(  m\right)  }\right)  $. By Lemma \ref{lem.singvec}, we thus have a
canonical isomorphism%
\begin{align*}
\operatorname*{Hom}\nolimits_{\widetilde{\mathfrak{gl}_{n}}}\left(
M_{\widetilde{\omega}_{m}}^{+},\mathcal{F}^{\left(  m\right)  }\right)   &
\rightarrow\operatorname*{Sing}\nolimits_{\widetilde{\omega}_{m}}\left(
\mathcal{F}^{\left(  m\right)  }\right)  ,\\
\phi &  \mapsto\phi\left(  v_{\widetilde{\omega}_{m}}^{+}\right)  .
\end{align*}
Thus, since $\psi_{m}\in\operatorname*{Sing}\nolimits_{\widetilde{\omega}_{m}%
}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $, there exists a
$\widetilde{\mathfrak{gl}_{n}}$-module homomorphism $\phi:M_{\widetilde{\omega
}_{m}}^{+}\rightarrow\mathcal{F}^{\left(  m\right)  }$ such that $\phi\left(
v_{\widetilde{\omega}_{m}}^{+}\right)  =\psi_{m}$. Consider this $\phi$.

Since $\mathcal{F}^{\left(  m\right)  }$ is generated by $\psi_{m}$ as a
$\widehat{\mathfrak{gl}_{n}}$-module (this was proven in the proof of
Proposition \ref{prop.glwave.F.irr}), it is clear that $\mathcal{F}^{\left(
m\right)  }$ is generated by $\psi_{m}$ as a $\widetilde{\mathfrak{gl}_{n}}%
$-module as well. Thus, $\phi$ must be surjective (because $\psi_{m}%
=\phi\left(  v_{\widetilde{\omega}_{m}}^{+}\right)  \in\phi\left(
M_{\widetilde{\omega}_{m}}^{+}\right)  $). Hence, $\mathcal{F}^{\left(
m\right)  }$ is (isomorphic to) a quotient of the $\widetilde{\mathfrak{gl}%
_{n}}$-module $M_{\widetilde{\omega}_{m}}^{+}$. In other words, $\mathcal{F}%
^{\left(  m\right)  }$ is a highest-weight module with highest weight
$\widetilde{\omega}_{m}$. Combined with the irreducibility of $\mathcal{F}%
^{\left(  m\right)  }$, this proves Proposition \ref{prop.glwave.F.irr}.

\subsubsection{The $\protect\widetilde{\mathfrak{gl}_{n}}$-module
$\mathcal{B}^{\left(  m\right)  }$}

By applying the Boson-Fermion correspondence $\sigma$ to Proposition
\ref{prop.glwave.F}, we obtain:

\begin{proposition}
\label{prop.glwave.B}Let $m\in\mathbb{Z}$. Let $\psi_{m}^{\prime}$ be the
element $\sigma^{-1}\left(  v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\right)
\in\mathcal{B}^{\left(  m\right)  }$ (the highest-weight vector of
$\mathcal{B}^{\left(  m\right)  }$).

There exists a unique extension of the $\widehat{\mathfrak{gl}_{n}}%
$-representation on $\mathcal{B}^{\left(  m\right)  }$ to
$\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}^{\prime}=0$. The action
of $d$ in this extension is given by%
\[
d\left(  \sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  =\left(  \sum\limits_{k\geq0}\left(  \left\lceil
\dfrac{m-k}{n}\right\rceil -\left\lceil \dfrac{i_{k}}{n}\right\rceil \right)
\right)  \cdot\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{proposition}

By applying the Boson-Fermion correspondence $\sigma$ to Proposition
\ref{prop.glwave.F.irr}, we obtain:

\begin{proposition}
Let $m\in\mathbb{Z}$. Let $\psi_{m}^{\prime}$ be the element $\sigma
^{-1}\left(  v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\right)  \in
\mathcal{B}^{\left(  m\right)  }$ (the highest-weight vector of $\mathcal{B}%
^{\left(  m\right)  }$).

\textbf{(a)} The $\widehat{\mathfrak{gl}_{n}}$-module $\mathcal{B}^{\left(
m\right)  }$ is irreducible.

\textbf{(b)} Let $\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}%
}:\widetilde{\mathfrak{gl}_{n}}\rightarrow\operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  $ denote the unique extension of the
$\widehat{\mathfrak{gl}_{n}}$-representation on $\mathcal{B}^{\left(
m\right)  }$ to $\widetilde{\mathfrak{gl}_{n}}$ such that $d\psi_{m}^{\prime
}=0$. (This is well-defined due to Proposition \ref{prop.glwave.B}.)

The $\widetilde{\mathfrak{gl}_{n}}$-module $\left(  \mathcal{B}^{\left(
m\right)  },\widehat{\rho}\mid_{\widetilde{\mathfrak{gl}_{n}}}\right)  $ is
irreducible with highest weight $\widetilde{\omega}_{m}$.
\end{proposition}

\subsubsection{$\protect\widetilde{\mathfrak{sl}_{n}}$ and its action
on$\mathfrak{\ }\mathcal{B}^{\left(  m\right)  }$}

We have $\left[  I_{n}t,\widehat{\mathfrak{sl}_{n}}\right]  =0$ in the Lie
algebra $\widehat{\mathfrak{gl}_{n}}$ (this is because $\left[  I_{n}%
t,L\mathfrak{sl}_{n}\right]  =0$ in the Lie algebra $L\mathfrak{gl}_{n}$, and
because $\omega\left(  I_{n}t,L\mathfrak{sl}_{n}\right)  =0$ where the
$2$-cocycle $\omega$ is the one defined in Proposition
\ref{prop.ainf.alphaomega}). Since $I_{n}t\in\widehat{\mathfrak{gl}_{n}}$ acts
on $\mathcal{F}$ by the operator $\widehat{\operatorname*{Toep}\nolimits_{n}%
}\left(  I_{n}t\right)  =T^{n}$ (more precisely, by the action of $T^{n}$ on
$\mathcal{F}$, but let us abbreviate this by $T^{n}$ here), this yields that
the action of $T^{n}$ on $\mathcal{F}$ is an $\widehat{\mathfrak{sl}_{n}}%
$-module homomorphism. Thus, the action of $T^{n}$ on $\mathcal{B}$ also is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism. As a consequence, the
restriction to $\widehat{\mathfrak{sl}_{n}}$ of the representation
$\mathcal{B}^{\left(  m\right)  }$ is not irreducible.

But $\psi_{m}^{\prime}$ is still a highest-weight vector with highest weight
$\widetilde{\omega}_{m}$. Let us look at how this representation
$\mathcal{B}^{\left(  m\right)  }$ decomposes.

\begin{definition}
Let $h_{i}=E_{i,i}^{\mathfrak{gl}_{n}}-E_{i+1,i+1}^{\mathfrak{gl}_{n}}$ for
$i\in\left\{  1,2,...,n-1\right\}  $, and let $h_{0}=K-h_{1}-h_{2}%
-...-h_{n-1}$. Then, $\left(  h_{0},h_{1},...,h_{n-1},d\right)  $ is a basis
of $\widetilde{\mathfrak{h}}\cap\widetilde{\mathfrak{sl}_{n}}$ (which is the
$0$-th graded component of $\widetilde{\mathfrak{sl}_{n}}$).
\end{definition}

\begin{definition}
For every $m\in\mathbb{Z}$, define the weight $\omega_{m}\in\left(
\widetilde{\mathfrak{h}}\cap\widetilde{\mathfrak{sl}_{n}}\right)  ^{\ast}$ to
be the restriction $\widetilde{\omega}_{m}\mid_{\widetilde{\mathfrak{h}}%
\cap\widetilde{\mathfrak{sl}_{n}}}$ of $\widetilde{\omega}_{m}$ to the $0$-th
graded component of $\widetilde{\mathfrak{sl}_{n}}$.
\end{definition}

This weight $\omega_{m}$ does not depend on $m$ but only depends on the
residue class of $m$ modulo $n$. In fact, it satisfies%
\begin{align*}
\omega_{m}\left(  h_{i}\right)   &  =\widetilde{\omega}_{m}\left(
h_{i}\right)  =\left\{
\begin{array}
[c]{c}%
1,\text{ if }i\equiv m\operatorname{mod}n;\\
0,\text{ if }i\not \equiv m\operatorname{mod}n
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  0,1,...,n-1\right\}
;\\
\omega_{m}\left(  d\right)   &  =\widetilde{\omega}_{m}\left(  d\right)  =0.
\end{align*}


\begin{definition}
Let $\mathcal{A}^{\left(  n\right)  }$ be the Lie subalgebra $\left\langle
K\right\rangle +\left\langle a_{ni}\ \mid\ i\in\mathbb{Z}\right\rangle $ of
$\mathcal{A}$.
\end{definition}

Note that the map%
\begin{align*}
\mathcal{A}  &  \rightarrow\mathcal{A}^{\left(  n\right)  },\\
a_{i}  &  \mapsto a_{ni}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{Z},\\
K  &  \mapsto nK
\end{align*}
is a Lie algebra isomorphism. But we still consider $\mathcal{A}^{\left(
n\right)  }$ as a Lie subalgebra of $\mathcal{A}$, and we won't identify it
with $\mathcal{A}$ via this isomorphism.

Since $\mathcal{A}^{\left(  n\right)  }$ is a Lie subalgebra of $\mathcal{A}$,
both $\mathcal{A}$-modules $\mathcal{F}$ and $\mathcal{B}$ become
$\mathcal{A}^{\left(  n\right)  }$-modules.

Let us consider the direct sum $\widehat{\mathfrak{sl}_{n}}\oplus
\mathcal{A}^{\left(  n\right)  }$ of Lie algebras. Let us denote by $K_{1}$
the element $\left(  K,0\right)  $ of $\widehat{\mathfrak{sl}_{n}}%
\oplus\mathcal{A}^{\left(  n\right)  }$ (where the $K$ means the element $K$
of $\widehat{\mathfrak{sl}_{n}}$), and let us denote by $K_{2}$ the element
$\left(  0,K\right)  $ of $\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}%
^{\left(  n\right)  }$ (where the $K$ means the element $K$ of $\mathcal{A}%
^{\left(  n\right)  }$). Note that both elements $K_{1}=\left(  K,0\right)  $
and $K_{2}=\left(  0,K\right)  $ lie in the center of $\widehat{\mathfrak{sl}%
_{n}}\oplus\mathcal{A}^{\left(  n\right)  }$; hence, so does their difference
$K_{1}-K_{2}=\left(  K,-K\right)  $. Thus, $\left\langle K_{1}-K_{2}%
\right\rangle $ (the $\mathbb{C}$-linear span of the set $\left\{  K_{1}%
-K_{2}\right\}  $) is an ideal of $\widehat{\mathfrak{sl}_{n}}\oplus
\mathcal{A}^{\left(  n\right)  }$. Thus, $\left(  \widehat{\mathfrak{sl}_{n}%
}\oplus\mathcal{A}^{\left(  n\right)  }\right)  \diagup\left(  K_{1}%
-K_{2}\right)  $ is a Lie algebra.

\begin{proposition}
The Lie algebras $\widehat{\mathfrak{gl}_{n}}$ and $\left(
\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)  }\right)
\diagup\left(  K_{1}-K_{2}\right)  $ are isomorphic. More precisely, the maps%
\begin{align*}
\left(  \widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)
}\right)  \diagup\left(  K_{1}-K_{2}\right)   &  \rightarrow
\widehat{\mathfrak{gl}_{n}},\\
\overline{\left(  At^{\ell},0\right)  }  &  \mapsto At^{\ell}%
\ \ \ \ \ \ \ \ \ \ \text{for every }A\in\mathfrak{sl}_{n}\text{ and }\ell
\in\mathbb{Z},\\
\overline{\left(  0,a_{n\ell}\right)  }  &  \mapsto\operatorname*{id}%
\nolimits_{n}t^{\ell}\ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\mathbb{Z},\\
\overline{K_{1}}=\overline{K_{2}}  &  \mapsto K
\end{align*}
and%
\begin{align*}
\widehat{\mathfrak{gl}_{n}}  &  \rightarrow\left(  \widehat{\mathfrak{sl}_{n}%
}\oplus\mathcal{A}^{\left(  n\right)  }\right)  \diagup\left(  K_{1}%
-K_{2}\right)  ,\\
At^{\ell}  &  \mapsto\overline{\left(  \left(  A-\dfrac{1}{n}\left(
\operatorname*{Tr}A\right)  \cdot\operatorname*{id}\nolimits_{n}\right)
t^{\ell},\left(  \dfrac{1}{n}\operatorname*{Tr}A\right)  a_{n\ell}\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }A\in\mathfrak{gl}_{n}\text{ and }\ell
\in\mathbb{Z},\\
K  &  \mapsto\overline{K_{1}}=\overline{K_{2}}.
\end{align*}
are mutually inverse isomorphisms of Lie algebras.
\end{proposition}

The proof of this proposition is left to the reader (it is completely
straightforward). This isomorphism $\widehat{\mathfrak{gl}_{n}}\cong\left(
\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)  }\right)
\diagup\left(  K_{1}-K_{2}\right)  $ allows us to consider any
$\widehat{\mathfrak{gl}_{n}}$-module as an $\left(  \widehat{\mathfrak{sl}%
_{n}}\oplus\mathcal{A}^{\left(  n\right)  }\right)  \diagup\left(  K_{1}%
-K_{2}\right)  $-module, i. e., as an $\widehat{\mathfrak{sl}_{n}}%
\oplus\mathcal{A}^{\left(  n\right)  }$-module on which $K_{1}$ and $K_{2}$
act the same way. In particular, $\mathcal{F}$ and $\mathcal{B}$ become
$\widehat{\mathfrak{sl}_{n}}\oplus\mathcal{A}^{\left(  n\right)  }$-modules.
Of course, the actions of the two addends $\widehat{\mathfrak{sl}_{n}}$ and
$\mathcal{A}^{\left(  n\right)  }$ on $\mathcal{F}$ and $\mathcal{B}$ are
exactly the actions of $\widehat{\mathfrak{sl}_{n}}$ and $\mathcal{A}^{\left(
n\right)  }$ on $\mathcal{F}$ and $\mathcal{B}$ that result from the canonical
inclusions $\widehat{\mathfrak{sl}_{n}}\subseteq\widehat{\mathfrak{gl}_{n}%
}\subseteq\mathfrak{a}_{\infty}$ and $\mathcal{A}^{\left(  n\right)
}\subseteq\mathcal{A}\cong\widehat{\mathfrak{gl}_{1}}\subseteq\mathfrak{a}%
_{\infty}$. (This is clear for the action of $\widehat{\mathfrak{sl}_{n}}$,
and is very easy to see for the action of $\mathcal{A}^{\left(  n\right)  }$.)

We checked above that the action of $T^{n}$ on $\mathcal{B}$ is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism. This easily generalizes:
For every integer $i$, the action of $T^{ni}$ on $\mathcal{B}$ is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism.\footnote{\textit{Proof.}
Let $i$ be an integer. We have $\left[  I_{n}t^{i},\widehat{\mathfrak{sl}_{n}%
}\right]  =0$ in the Lie algebra $\widehat{\mathfrak{gl}_{n}}$ (this is
because $\left[  I_{n}t^{i},L\mathfrak{sl}_{n}\right]  =0$ in the Lie algebra
$L\mathfrak{gl}_{n}$, and because $\omega\left(  I_{n}t^{i},L\mathfrak{sl}%
_{n}\right)  =0$ where the $2$-cocycle $\omega$ is the one defined in
Proposition \ref{prop.ainf.alphaomega}). Since $I_{n}t^{i}\in
\widehat{\mathfrak{gl}_{n}}$ acts on $\mathcal{F}$ by the operator
$\widehat{\operatorname*{Toep}\nolimits_{n}}\left(  I_{n}t^{i}\right)
=T^{ni}$ (more precisely, by the action of $T^{ni}$ on $\mathcal{F}$, but let
us abbreviate this by $T^{ni}$ here), this yields that the action of $T^{ni}$
on $\mathcal{F}$ is an $\widehat{\mathfrak{sl}_{n}}$-module homomorphism.
Thus, the action of $T^{ni}$ on $\mathcal{B}$ also is an
$\widehat{\mathfrak{sl}_{n}}$-module homomorphism.} Thus, the subspace
$\mathcal{B}_{0}^{\left(  m\right)  }=\left\{  v\in\mathcal{B}^{\left(
m\right)  }\ \mid\ T^{ni}v=0\text{ for all }i>0\right\}  $ of $\mathcal{B}%
^{\left(  m\right)  }$ is an $\widehat{\mathfrak{sl}_{n}}$-submodule.
Recalling that $\mathcal{B}^{\left(  m\right)  }=\mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  $, with $T^{ni}$ acting as $ni\dfrac{\partial
}{\partial x_{ni}}$, we have $\mathcal{B}_{0}^{\left(  m\right)  }%
\cong\mathbb{C}\left[  x_{j}\ \mid\ n\nmid j\right]  $.

\begin{theorem}
\label{thm.B0m}This $\mathcal{B}_{0}^{\left(  m\right)  }$ is an irreducible
$\widehat{\mathfrak{sl}_{n}}$-module (or $\widetilde{\mathfrak{sl}_{n}}%
$-module; this doesn't matter) with highest weight $\omega_{\overline{m}}$
(this means that $\mathcal{B}_{0}^{\left(  m\right)  }\cong L_{\omega
_{\overline{m}}}$) and depends only on $\overline{m}$ (the remainder of $m$
modulo $n$) rather than on $m$. Moreover, $\mathcal{B}^{\left(  m\right)
}\cong\mathcal{B}_{0}^{\left(  m\right)  }\otimes\widetilde{F}_{m}$, where
$\widetilde{F}_{m}$ is the appropriate Fock module over $\mathcal{A}^{\left(
n\right)  }$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.B0m}.} We clearly have such a decomposition
as vector spaces, $\widetilde{F}_{m}=\mathbb{C}\left[  x_{n},x_{2n}%
,x_{3n},...\right]  $. Each of the two Lie algebras acts in its own factor:
$\mathcal{A}^{\left(  n\right)  }$ acts in $\widetilde{F}_{m}$, and
$\widehat{\mathfrak{gl}_{n}}$ commutes with $\mathcal{A}^{\left(  n\right)  }%
$. Since the tensor product is irreducible, each factor is irreducible, so
that $\mathcal{B}_{0}^{\left(  m\right)  }$ is irreducible.

We can now classify unitary highest-weight representations of
$\widehat{\mathfrak{sl}_{n}}$:

\begin{proposition}
The highest-weight representation $L_{\omega_{m}}$ is unitary for each
$m\in\left\{  0,1,...,n-1\right\}  $.
\end{proposition}

\textit{Proof.} The contravariant Hermitian form on $L_{\omega_{m}}$ is the
restriction of the form on $\mathcal{B}^{\left(  m\right)  }$.

\begin{corollary}
If $k_{0},k_{1},...,k_{n-1}$ are nonnegative integers, then $L_{k_{0}%
\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}}$ is unitary (of level
$k_{0}+k_{1}+...+k_{n-1}$).
\end{corollary}

\textit{Proof.} The tensor product $L_{\omega_{0}}^{\otimes k_{0}}\otimes
L_{\omega_{1}}^{\otimes k_{1}}\otimes...\otimes L_{\omega_{n-1}}^{\otimes
k_{n-1}}$ is unitary (being a tensor product of unitary representations), and
thus is a direct sum of irreducible representations. Clearly, $L_{k_{0}%
\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}}$ is a summand of this
module, and thus also unitary, qed.

\subsubsection{Classification of unitary highest-weight
$\protect\widehat{\mathfrak{sl}_{n}}$-modules}

\begin{theorem}
\label{thm.sln.unitaries}These $L_{k_{0}\omega_{0}+k_{1}\omega_{1}%
+...+k_{n-1}\omega_{n-1}}$ (with $k_{0},k_{1},...,k_{n-1}$ being nonnegative
integers) are the only unitary highest-weight representations of
$\widehat{\mathfrak{sl}_{n}}$.
\end{theorem}

To prove this, first a lemma:

\begin{lemma}
\label{lem.sl2.unitaries}Consider the antilinear antiinvolution $\dag
:\mathfrak{sl}_{2}\rightarrow\mathfrak{sl}_{2}$ defined by $e^{\dag}=f$,
$f^{\dag}=e$ and $h^{\dag}=h$. Let $\lambda\in\mathfrak{h}^{\ast}$. We
identify the function $\lambda\in\mathfrak{h}^{\ast}$ with the value
$\lambda\left(  h\right)  \in\mathbb{C}$. Then, $L_{\lambda}$ is a unitary
representation of $\mathfrak{sl}_{2}$ if and only if $\lambda\in\mathbb{Z}%
_{+}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sl2.unitaries}.} Assume that $L_{\lambda}$ is
a unitary representation of $\mathfrak{sl}_{2}$. Let $v_{\lambda}=v_{\lambda
}^{+}$. Since $L_{\lambda}$ is unitary, the form $\left(  \cdot,\cdot\right)
$ is positive definite, so that $\left(  v_{\lambda},v_{\lambda}\right)  >0$.

Every $n\in\mathbb{N}$ satisfies
\[
\left(  f^{n}v_{\lambda},f^{n}v_{\lambda}\right)  =n!\overline{\lambda}\left(
\overline{\lambda}-1\right)  ...\left(  \overline{\lambda}-n+1\right)  \left(
v_{\lambda},v_{\lambda}\right)
\]
(the proof of this is analogous to the proof of (\ref{exa.sl2.bilinform}), but
uses $e^{\dag}=f$). Since $\left(  \cdot,\cdot\right)  $ is positive definite,
we must have $\left(  f^{n}v_{\lambda},f^{n}v_{\lambda}\right)  \geq0$ for
every $n\in\mathbb{N}$. Thus, every $n\in\mathbb{N}$ satisfies $n!\overline
{\lambda}\left(  \overline{\lambda}-1\right)  ...\left(  \overline{\lambda
}-n+1\right)  \left(  v_{\lambda},v_{\lambda}\right)  =\left(  f^{n}%
v_{\lambda},f^{n}v_{\lambda}\right)  \geq0$, so that $\overline{\lambda
}\left(  \overline{\lambda}-1\right)  ...\left(  \overline{\lambda
}-n+1\right)  \geq0$ (since $\left(  v_{\lambda},v_{\lambda}\right)  >0$).
Applied to $n=1$, this yields $\overline{\lambda}\geq0$, so that
$\overline{\lambda}\in\mathbb{R}$ and thus $\lambda\in\mathbb{R}$. Hence,
$\overline{\lambda}\geq0$ becomes $\lambda\geq0$.

Every $n\in\mathbb{N}$ satisfies $\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  =\overline{\lambda}\left(  \overline{\lambda}-1\right)
...\left(  \overline{\lambda}-n+1\right)  \geq0$. Thus, $\lambda\in
\mathbb{Z}_{+}$ (otherwise, $\lambda\left(  \lambda-1\right)  ...\left(
\lambda-n+1\right)  $ would alternate in sign for each sufficiently large $n$).

This proves one direction of Lemma \ref{lem.sl2.unitaries}. The converse
direction is classical and easy. Lemma \ref{lem.sl2.unitaries} is proven.

\begin{corollary}
Let $\lambda\in\mathbb{C}$. If $\mathfrak{g}$ is a Lie algebra with antilinear
antiinvolution $\dag$ and $\mathfrak{sl}_{2}$ is a Lie subalgebra of
$\mathfrak{g}$, and if $\dag\mid_{\mathfrak{sl}_{2}}$ sends $e,f,h$ to
$f,e,h$, and if $V$ is a unitary representation of $\mathfrak{g}$, and if some
$v\in V$ satisfies $ev=0$ and $hv=\lambda v$, then $\lambda\in\mathbb{Z}_{+}$.
\end{corollary}

\textit{Proof of Theorem \ref{thm.sln.unitaries}.} For every $i\in\left\{
0,1,...,n-1\right\}  $, we have an $\mathfrak{sl}_{2}$-subalgebra:%
\begin{align*}
h_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i,i}-E_{i+1,i+1},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
K+E_{n,n}-E_{1,1},\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.  ,\\
e_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i,i+1},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
E_{n,1}t,\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.  ;\\
f_{i}  &  =\left\{
\begin{array}
[c]{c}%
E_{i+1,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq0;\\
E_{1,n}t^{-1},\ \ \ \ \ \ \ \ \ \ \text{if }i=0
\end{array}
\right.
\end{align*}
\footnote{Here, $E_{i,j}$ means $E_{i,j}^{\mathfrak{gl}_{n}}$.} (these form an
$\mathfrak{sl}_{2}$-triple, as can be easily checked). These satisfy
$e_{i}^{\dag}=f_{i}$, $f_{i}^{\dag}=e_{i}$ and $h_{i}^{\dag}=h_{i}$. Thus, if
$L_{\lambda}$ is a unitary representation of $\widehat{\mathfrak{sl}_{n}}$,
then $\lambda\left(  h_{i}\right)  \in\mathbb{Z}_{+}$. But $\omega_{i}$ are a
basis for the weights, and namely the dual basis to the basis of the $h_{i}$.
Thus, $\lambda=\sum\limits_{i=0}^{n-1}\lambda\left(  h_{i}\right)  \omega_{i}%
$. Hence, $\lambda=\sum\limits_{i=0}^{n-1}k_{i}\omega_{i}$ with $k_{i}%
\in\mathbb{Z}_{+}$. Qed.

\begin{remark}
Relation between $\widehat{\mathfrak{sl}_{n}}$-modules and $\mathfrak{sl}_{n}$-modules:

Let $L_{\lambda}$ be a unitary $\widehat{\mathfrak{sl}_{n}}$-module, with
$\lambda=k_{0}\omega_{0}+k_{1}\omega_{1}+...+k_{n-1}\omega_{n-1}$.

Then, $U\left(  \mathfrak{sl}_{n}\right)  v_{\lambda}=L_{\overline{\lambda}}$
where $\overline{\lambda}=k_{1}\omega_{1}+k_{2}\omega_{2}+...+k_{n-1}%
\omega_{n-1}$ is a weight for $\mathfrak{sl}_{n}$. And if the level of
$L_{\lambda}$ was $k$, then we must have $k_{1}+k_{2}+...+k_{n-1}\leq k$.
\end{remark}

\subsection{The Sugawara construction}

We will now study the Sugawara construction. It constructs a
$\operatorname*{Vir}$ action on a $\widehat{\mathfrak{g}}$-module (under some
conditions), and it generalizes the action of $\operatorname*{Vir}$ on the
$\mu$-Fock representation $F_{\mu}$ (that was constructed in Proposition
\ref{prop.fockvir.answer2}).

\begin{definition}
\label{def.sugawara}Let $\mathfrak{g}$ be a finite-dimensional $\mathbb{C}%
$-Lie algebra with an invariant symmetric bilinear form $\left(  \cdot
,\cdot\right)  $. (This form needs not be nondegenerate; it is even allowed to
be $0$.)

Consider the affine Lie algebra $\widehat{\mathfrak{g}}=\mathfrak{g}\left[
t,t^{-1}\right]  \oplus\mathbb{C}K$ defined through the $2$-cocycle
$\omega:\mathfrak{g}\left[  t,t^{-1}\right]  \times\mathfrak{g}\left[
t,t^{-1}\right]  \rightarrow\mathbb{C}$ defined by%
\[
\omega\left(  a,b\right)  =\operatorname*{Res}\nolimits_{t=0}\left(
a^{\prime},b\right)  dt\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathfrak{g}%
\left[  t,t^{-1}\right]  \text{ and }b\in\mathfrak{g}\left[  t,t^{-1}\right]
.
\]
(This is the $2$-cocycle $\omega$ in Definition \ref{def.loop}. We just
slightly rewrote the definition.)

Let $\operatorname*{Kil}$ denote the Killing form on $\mathfrak{g}$, defined
by
\[
\operatorname*{Kil}\left(  a,b\right)  =\operatorname*{Tr}\left(
\operatorname*{ad}\left(  a\right)  \cdot\operatorname*{ad}\left(  b\right)
\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }a,b\in\mathfrak{g}.
\]


An element $k\in\mathbb{C}$ is said to be \textit{non-critical for }$\left(
\mathfrak{g},\left(  \cdot,\cdot\right)  \right)  $ if and only if the form
$k\cdot\left(  \cdot,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$ is nondegenerate.
\end{definition}

\begin{definition}
\label{def.sugawara.M}Let $M$ be a $\widehat{\mathfrak{g}}$-module.

We say that $M$ is \textit{admissible} if for every $v\in M$, there exists
some $N\in\mathbb{N}$ such that every integer $n\geq N$ and every
$a\in\mathfrak{g}$ satisfy $at^{n}\cdot v=0$.

If $k\in\mathbb{C}$, then we say that $M$ is \textit{of level }$k$ if
$K\mid_{M}=k\cdot\operatorname*{id}$.
\end{definition}

\begin{proposition}
\label{prop.WtoDerg}Let $\mathfrak{g}$ be a finite-dimensional $\mathbb{C}%
$-Lie algebra with an invariant symmetric bilinear form $\left(  \cdot
,\cdot\right)  $. Consider the affine Lie algebra $\widehat{\mathfrak{g}}$
defined as in Definition \ref{def.sugawara}.

\textbf{(a)} Then, there is a natural homomorphism $\eta
_{\widehat{\mathfrak{g}}}:W\rightarrow\operatorname*{Der}\widehat{\mathfrak{g}%
}$ of Lie algebras given by
\[
\left(  \eta_{\widehat{\mathfrak{g}}}\left(  f\partial\right)  \right)
\left(  g,\alpha\right)  =\left(  fg^{\prime},0\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[  t,t^{-1}\right]
\text{, }g\in\mathfrak{g}\left[  t,t^{-1}\right]  \text{ and }\alpha
\in\mathbb{C}.
\]


\textbf{(b)} There also is a natural homomorphism $\widetilde{\eta
}_{\widehat{\mathfrak{g}}}:\operatorname*{Vir}\rightarrow\operatorname*{Der}%
\widehat{\mathfrak{g}}$ of Lie algebras given by
\[
\left(  \widetilde{\eta}_{\widehat{\mathfrak{g}}}\left(  f\partial+\lambda
K\right)  \right)  \left(  g,\alpha\right)  =\left(  fg^{\prime},0\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in\mathbb{C}\left[  t,t^{-1}\right]
\text{, }g\in\mathfrak{g}\left[  t,t^{-1}\right]  \text{, }\lambda
\in\mathbb{C}\text{ and }\alpha\in\mathbb{C}.
\]
This homomorphism $\widetilde{\eta}_{\widehat{\mathfrak{g}}}$ is simply the
extension of the homomorphism $\eta_{\widehat{\mathfrak{g}}}:W\rightarrow
\operatorname*{Der}\widehat{\mathfrak{g}}$ to $\operatorname*{Vir}$ by means
of requiring that $\widetilde{\eta}_{\widehat{\mathfrak{g}}}\left(  K\right)
=0$.

This homomorphism $\widetilde{\eta}_{\widehat{\mathfrak{g}}}$ makes
$\widehat{\mathfrak{g}}$ a $\operatorname*{Vir}$-module on which
$\operatorname*{Vir}$ acts by derivations. Therefore, a Lie algebra
$\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$ is defined (according to
Definition \ref{def.semidir.lielie}).
\end{proposition}

The proof of Proposition \ref{prop.WtoDerg} is left to the reader. (A proof of
Proposition \ref{prop.WtoDerg} \textbf{(a)} can be obtained by carefully
generalizing the proof of Lemma \ref{lem.WtoDerA}. Actually, Proposition
\ref{prop.WtoDerg} \textbf{(a)} generalizes Lemma \ref{lem.WtoDerA}, since (as
we will see in Remark \ref{rmk.sugawara.fockvir}) the Lie algebra
$\widehat{\mathfrak{g}}$ generalizes $\mathcal{A}$.)

The following theorem is one of the most important facts about affine Lie algebras:

\begin{theorem}
[Sugawara construction]\label{thm.sugawara}Let us work in the situation of
Definition \ref{def.sugawara}.

Let $k\in\mathbb{C}$ be non-critical for $\left(  \mathfrak{g},\left(
\cdot,\cdot\right)  \right)  $. Let $M$ be an admissible
$\widehat{\mathfrak{g}}$-module of level $k$. Let $B\subseteq\mathfrak{g}$ be
a basis orthonormal with respect to the form $k\left(  \cdot,\cdot\right)
+\dfrac{1}{2}\operatorname*{Kil}$.

For every $x\in\mathfrak{g}$ and $n\in\mathbb{Z}$, let us denote by $x_{n}$
the element $xt^{n}\in\widehat{\mathfrak{g}}$.

For every $x\in\mathfrak{g}$, every $m\in\mathbb{Z}$ and $\ell\in\mathbb{Z}$,
define the "normal ordered product" $\left.  :x_{m}x_{\ell}:\right.  $ in
$U\left(  \widehat{\mathfrak{g}}\right)  $ by%
\[
\left.  :x_{m}x_{\ell}:\right.  =\left\{
\begin{array}
[c]{c}%
x_{m}x_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq\ell;\\
x_{\ell}x_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>\ell
\end{array}
\right.  .
\]


For every $n\in\mathbb{Z}$, define an endomorphism $L_{n}$ of $M$ by%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.  .
\]


\textbf{(a)} This endomorphism $L_{n}$ is indeed well-defined. In other words,
for every $n\in\mathbb{Z}$, every $a\in B$ and every $v\in M$, the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  v$ converges in
the discrete topology (i. e., has only finitely many nonzero addends).

\textbf{(b)} For every $n\in\mathbb{Z}$, the endomorphism $L_{n}$ does not
depend on the choice of the orthonormal basis $B$.

\textbf{(c)} The endomorphisms $L_{n}$ for $n\in\mathbb{Z}$ give rise to a
$\operatorname*{Vir}$-representation on $M$ with central charge%
\[
c=k\cdot\sum\limits_{a\in B}\left(  a,a\right)  .
\]


\textbf{(d)} These formulas (for $L_{n}$ and $c$) extend the action of
$\widehat{\mathfrak{g}}$ on $M$ to an action of $\operatorname*{Vir}%
\ltimes\widehat{\mathfrak{g}}$, so they satisfy $\left[  L_{n},a_{m}\right]
=-ma_{n+m}$ and $\left[  L_{n},K\right]  =0$.

\textbf{(e)} We have $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$ for any
$a\in\mathfrak{g}$ and any integers $n$ and $m$.
\end{theorem}

\begin{remark}
\label{rmk.sugawara.fockvir}We have already encountered an example of this
construction: namely, the example where $\mathfrak{g}$ is the trivial Lie
algebra $\mathbb{C}$, where $\left(  \cdot,\cdot\right)  :\mathfrak{g}%
\times\mathfrak{g}\rightarrow\mathbb{C}$ is the bilinear form $\left(
x,y\right)  \mapsto xy$, where $k=1$, and where $M$ is the
$\widehat{\mathfrak{g}}$-module $F_{\mu}$. (To make sense of this, notice that
when $\mathfrak{g}$ is the trivial Lie algebra $\mathbb{C}$, the affine Lie
algebra $\widehat{\mathfrak{g}}$ is canonically isomorphic to the Heisenberg
algebra $\mathcal{A}$, through an isomorphism $\widehat{\mathfrak{g}%
}\rightarrow\mathcal{A}$ which takes $t^{n}$ to $a_{n}$ and $K$ to $K$.) In
this example, the operators $L_{n}$ defined in Theorem \ref{thm.sugawara} are
exactly the operators $L_{n}$ defined in Definition \ref{def.fockvir}.
\end{remark}

Before we prove Theorem \ref{thm.sugawara}, we formulate a number of lemmas.
First, an elementary lemma on Killing forms of finite-dimensional Lie algebras:

\begin{lemma}
\label{lem.sugawara.Kil}Let $\mathfrak{g}$ be a finite-dimensional Lie
algebra. Denote by $\operatorname*{Kil}$ the Killing form of $\mathfrak{g}$.
Let $n\in\mathbb{N}$ and $p_{1},p_{2},...,p_{n}\in\mathfrak{g}$ and
$q_{1},q_{2},...,q_{n}\in\mathfrak{g}$ be such that the tensor $\sum
\limits_{i=1}^{n}p_{i}\otimes q_{i}\in\mathfrak{g}\otimes\mathfrak{g}$ is
$\mathfrak{g}$-invariant. Then, $\sum\limits_{i=1}^{n}\left[  \left[
b,p_{i}\right]  ,q_{i}\right]  =\sum\limits_{i=1}^{n}\operatorname*{Kil}%
\left(  b,p_{i}\right)  q_{i}$ for every $b\in\mathfrak{g}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sugawara.Kil}.} Let $\left(  c_{1}%
,c_{2},...,c_{m}\right)  $ be a basis of the vector space $\mathfrak{g}$, and
let $\left(  c_{1}^{\ast},c_{2}^{\ast},...,c_{m}^{\ast}\right)  $ be the dual
basis of $\mathfrak{g}^{\ast}$. Then, every $i\in\left\{  1,2,...,n\right\}  $
satisfies
\[
\operatorname*{Kil}\left(  b,p_{i}\right)  =\operatorname*{Tr}\left(  \left(
\operatorname*{ad}b\right)  \circ\left(  \operatorname*{ad}p_{i}\right)
\right)  =\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left(  \left(
\operatorname*{ad}b\right)  \circ\left(  \operatorname*{ad}p_{i}\right)
\right)  \left(  c_{j}\right)  \right)  =\sum\limits_{j=1}^{m}c_{j}^{\ast
}\left(  \left[  b,\left[  p_{i},c_{j}\right]  \right]  \right)  .
\]
Hence,%
\begin{align*}
\sum\limits_{i=1}^{n}\operatorname*{Kil}\left(  b,p_{i}\right)  q_{i}  &
=\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left[
b,\left[  p_{i},c_{j}\right]  \right]  \right)  q_{i}=\sum\limits_{j=1}%
^{m}\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[  b,\left[  p_{i}%
,c_{j}\right]  \right]  \right)  q_{i}\\
&  =-\sum\limits_{j=1}^{m}\sum\limits_{i=1}^{n}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  \left[  q_{i},c_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sum\limits_{i=1}^{n}p_{i}\otimes q_{i}\text{ is }\mathfrak{g}%
\otimes\mathfrak{g}\text{-invariant, so that}\\
\sum\limits_{i=1}^{n}\left[  p_{i},c_{j}\right]  \otimes q_{i}=-\sum
\limits_{i=1}^{n}p_{i}\otimes\left[  q_{i},c_{j}\right]  \text{ for every
}j\in\left\{  1,2,...,m\right\}
\end{array}
\right) \\
&  =-\sum\limits_{j=1}^{m}\sum\limits_{i=1}^{n}\left[  q_{i},c_{j}^{\ast
}\left(  \left[  b,p_{i}\right]  \right)  c_{j}\right]  =-\sum\limits_{i=1}%
^{n}\left[  q_{i},\underbrace{\sum\limits_{j=1}^{m}c_{j}^{\ast}\left(  \left[
b,p_{i}\right]  \right)  c_{j}}_{=\left[  b,p_{i}\right]  }\right] \\
&  =-\sum\limits_{i=1}^{n}\left[  q_{i},\left[  b,p_{i}\right]  \right]
=\sum\limits_{i=1}^{n}\left[  \left[  b,p_{i}\right]  ,q_{i}\right]  ,
\end{align*}
which proves Lemma \ref{lem.sugawara.Kil}.

Here comes another lemma on $\mathfrak{g}$-invariant bilinear forms:

\begin{lemma}
\label{lem.sugawara.Kil2}Let $\mathfrak{g}$ be a finite-dimensional
$\mathbb{C}$-Lie algebra with an invariant symmetric bilinear form
$\left\langle \cdot,\cdot\right\rangle $. Let $B\subseteq\mathfrak{g}$ be a
basis orthonormal with respect to the form $\left\langle \cdot,\cdot
\right\rangle $.

\textbf{(a)} Then, the tensor $\sum\limits_{a\in B}a\otimes a$ is
$\mathfrak{g}$-invariant in $\mathfrak{g}\otimes\mathfrak{g}$.

\textbf{(b)} Let $B^{\prime}$ also be a basis of $\mathfrak{g}$ orthonormal
with respect to the form $\left\langle \cdot,\cdot\right\rangle $. Then,
$\sum\limits_{a\in B}a\otimes a=\sum\limits_{a\in B^{\prime}}a\otimes a$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sugawara.Kil2}.} The bilinear form
$\left\langle \cdot,\cdot\right\rangle $ is nondegenerate (since it has an
orthonormal basis).

\textbf{(a)} For every $v\in\mathfrak{g}$, let $v^{\ast}:\mathfrak{g}%
\rightarrow\mathbb{C}$ be the $\mathbb{C}$-linear map which sends every
$w\in\mathfrak{g}$ to $\left\langle v,w\right\rangle $. Then, $\mathfrak{g}%
^{\ast}=\left\{  v^{\ast}\ \mid\ v\in\mathfrak{g}\right\}  $ (since the form
$\left\langle \cdot,\cdot\right\rangle $ is nondegenerate).

Let $b\in\mathfrak{g}$. We will now prove that $h\left(  \sum\limits_{a\in
B}\left(  \left[  b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)
\right)  =0$ for every $h\in\left(  \mathfrak{g}\otimes\mathfrak{g}\right)
^{\ast}$.

In fact, let $h\in\left(  \mathfrak{g}\otimes\mathfrak{g}\right)  ^{\ast}$.
Since $\mathfrak{g}$ is finite-dimensional, we have $\left(  \mathfrak{g}%
\otimes\mathfrak{g}\right)  ^{\ast}=\mathfrak{g}^{\ast}\otimes\mathfrak{g}%
^{\ast}$, so that $h\in\mathfrak{g}^{\ast}\otimes\mathfrak{g}^{\ast}$. We can
WLOG assume that $h=f_{1}\otimes f_{2}$ for some $f_{1}\in\mathfrak{g}^{\ast}$
and $f_{2}\in\mathfrak{g}^{\ast}$ (because every tensor in $\mathfrak{g}%
^{\ast}\otimes\mathfrak{g}^{\ast}$ is a $\mathbb{C}$-linear combination of
pure tensors, and the assertion which we want to prove (namely, the equality
$h\left(  \sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  \right)  =0$) is $\mathbb{C}$-linear in
$h$). Assume this.

Since $f_{1}\in\mathfrak{g}^{\ast}=\left\{  v^{\ast}\ \mid\ v\in
\mathfrak{g}\right\}  $, there exists some $v_{1}\in\mathfrak{g}$ such that
$f_{1}=v_{1}^{\ast}$. Consider this $v_{1}$.

Since $f_{2}\in\mathfrak{g}^{\ast}=\left\{  v^{\ast}\ \mid\ v\in
\mathfrak{g}\right\}  $, there exists some $v_{2}\in\mathfrak{g}$ such that
$f_{2}=v_{2}^{\ast}$. Consider this $v_{2}$.

Since $B$ is an orthonormal basis with respect to $\left\langle \cdot
,\cdot\right\rangle $, we have $\sum\limits_{a\in B}a\left\langle \left[
b,v_{2}\right]  ,a\right\rangle =\left[  b,v_{2}\right]  $ and $\sum
\limits_{a\in B}\left\langle \left[  b,v_{1}\right]  ,a\right\rangle a=\left[
b,v_{1}\right]  $.

Now, $h=\underbrace{f_{1}}_{=v_{1}^{\ast}}\otimes\underbrace{f_{2}}%
_{=v_{2}^{\ast}}=v_{1}^{\ast}\otimes v_{2}^{\ast}$, so that%
\begin{align*}
&  h\left(  \sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  \right) \\
&  =\left(  v_{1}^{\ast}\otimes v_{2}^{\ast}\right)  \left(  \sum\limits_{a\in
B}\left(  \left[  b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)
\right) \\
&  =\sum\limits_{a\in B}\left(  \underbrace{v_{1}^{\ast}\left(  \left[
b,a\right]  \right)  }_{\substack{=\left\langle v_{1},\left[  b,a\right]
\right\rangle \\\text{(by the definition of }v_{1}^{\ast}\text{)}}%
}\cdot\underbrace{v_{2}^{\ast}\left(  a\right)  }_{\substack{=\left\langle
v_{2},a\right\rangle \\\text{(by the definition of }v_{2}^{\ast}\text{)}%
}}+\underbrace{v_{1}^{\ast}\left(  a\right)  }_{\substack{=\left\langle
v_{1},a\right\rangle \\\text{(by the definition of }v_{1}^{\ast}\text{)}%
}}\cdot\underbrace{v_{2}^{\ast}\left(  \left[  b,a\right]  \right)
}_{\substack{=\left\langle v_{2},\left[  b,a\right]  \right\rangle \\\text{(by
the definition of }v_{2}^{\ast}\text{)}}}\right) \\
&  =\sum\limits_{a\in B}\left(  \underbrace{\left\langle v_{1},\left[
b,a\right]  \right\rangle }_{\substack{=-\left\langle \left[  b,v_{1}\right]
,a\right\rangle \\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{
is invariant)}}}\cdot\left\langle v_{2},a\right\rangle +\left\langle
v_{1},a\right\rangle \cdot\underbrace{\left\langle v_{2},\left[  b,a\right]
\right\rangle }_{\substack{=-\left\langle \left[  b,v_{2}\right]
,a\right\rangle \\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{
is invariant)}}}\right) \\
&  =\sum\limits_{a\in B}\left(  -\left\langle \left[  b,v_{1}\right]
,a\right\rangle \cdot\left\langle v_{2},a\right\rangle -\left\langle
v_{1},a\right\rangle \cdot\left\langle \left[  b,v_{2}\right]  ,a\right\rangle
\right) \\
&  =-\underbrace{\sum\limits_{a\in B}\left\langle \left[  b,v_{1}\right]
,a\right\rangle \cdot\left\langle v_{2},a\right\rangle }_{=\left\langle
v_{2},\sum\limits_{a\in B}\left\langle \left[  b,v_{1}\right]  ,a\right\rangle
a\right\rangle }-\underbrace{\sum\limits_{a\in B}\left\langle v_{1}%
,a\right\rangle \cdot\left\langle \left[  b,v_{2}\right]  ,a\right\rangle
}_{=\left\langle v_{1},\sum\limits_{a\in B}a\left\langle \left[
b,v_{2}\right]  ,a\right\rangle \right\rangle }\\
&  =-\left\langle v_{2},\underbrace{\sum\limits_{a\in B}\left\langle \left[
b,v_{1}\right]  ,a\right\rangle a}_{=\left[  b,v_{1}\right]  }\right\rangle
-\left\langle v_{1},\underbrace{\sum\limits_{a\in B}a\left\langle \left[
b,v_{2}\right]  ,a\right\rangle }_{=\left[  b,v_{2}\right]  }\right\rangle \\
&  =-\underbrace{\left\langle v_{2},\left[  b,v_{1}\right]  \right\rangle
}_{\substack{=\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle
\\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{ is symmetric)}%
}}-\underbrace{\left\langle v_{1},\left[  b,v_{2}\right]  \right\rangle
}_{\substack{=-\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle
\\\text{(since }\left\langle \cdot,\cdot\right\rangle \text{ is invariant)}%
}}=-\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle -\left(
-\left\langle \left[  b,v_{1}\right]  ,v_{2}\right\rangle \right)  =0.
\end{align*}


We thus have proven that $h\left(  \sum\limits_{a\in B}\left(  \left[
b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)  \right)  =0$ for
every $h\in\left(  \mathfrak{g}\otimes\mathfrak{g}\right)  ^{\ast}$.
Consequently, $\sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  =0$.

Hence, we have shown that $\sum\limits_{a\in B}\left(  \left[  b,a\right]
\otimes a+a\otimes\left[  b,a\right]  \right)  =0$ for every $b\in
\mathfrak{g}$. In other words, the tensor $\sum\limits_{a\in B}a\otimes a$ is
$\mathfrak{g}$-invariant. Lemma \ref{lem.sugawara.Kil2} \textbf{(a)} is proven.

\textbf{(b)} For every $a\in B$ and $b\in B^{\prime}$, let $\xi_{a,b}$ be the
$b$-coordinate of $a$ with respect to the basis $B^{\prime}$. Then, every
$a\in B$ satisfies $a=\sum\limits_{b\in B^{\prime}}\xi_{a,b}b$. Thus, $\left(
\xi_{a,b}\right)  _{\left(  a,b\right)  \in B\times B^{\prime}}$ (this is a
matrix whose rows and columns are indexed by elements of $B$ and $B^{\prime}$,
respectively) is the matrix which represents the change of bases from
$B^{\prime}$ to $B$ (or from $B$ to $B^{\prime}$, depending on how you define
the matrix representing a change of basis). Since both $B$ and $B^{\prime}$
are two orthonormal bases with respect to the same bilinear form $\left\langle
\cdot,\cdot\right\rangle $, this matrix must thus be orthogonal. Hence, every
$b\in B^{\prime}$ and $b^{\prime}\in B^{\prime}$ satisfy $\sum\limits_{a\in
B}\xi_{a,b}\xi_{a,b^{\prime}}=\delta_{b,b^{\prime}}$ (where $\delta
_{b,b^{\prime}}$ is the Kronecker delta of $b$ and $b^{\prime}$). Now, since
every $a\in B$ satisfies $a=\sum\limits_{b\in B^{\prime}}\xi_{a,b}b$ and
$a=\sum\limits_{b\in B^{\prime}}\xi_{a,b}b=\sum\limits_{b^{\prime}\in
B^{\prime}}\xi_{a,b^{\prime}}b^{\prime}$ (here, we renamed $b$ as $b^{\prime}$
in the sum), we have%
\begin{align*}
&  \sum\limits_{a\in B}\underbrace{a}_{=\sum\limits_{b\in B^{\prime}}\xi
_{a,b}b}\otimes\underbrace{a}_{=\sum\limits_{b^{\prime}\in B^{\prime}}%
\xi_{a,b^{\prime}}b^{\prime}}\\
&  =\sum\limits_{a\in B}\left(  \sum\limits_{b\in B^{\prime}}\xi
_{a,b}b\right)  \otimes\left(  \sum\limits_{b^{\prime}\in B^{\prime}}%
\xi_{a,b^{\prime}}b^{\prime}\right)  =\sum\limits_{a\in B}\sum\limits_{b\in
B^{\prime}}\sum\limits_{b^{\prime}\in B^{\prime}}\xi_{a,b}\xi_{a,b^{\prime}%
}b\otimes b^{\prime}\\
&  =\sum\limits_{b\in B^{\prime}}\sum\limits_{b^{\prime}\in B^{\prime}%
}\underbrace{\sum\limits_{a\in B}\xi_{a,b}\xi_{a,b^{\prime}}}_{=\delta
_{b,b^{\prime}}}b\otimes b^{\prime}=\sum\limits_{b\in B^{\prime}%
}\underbrace{\sum\limits_{b^{\prime}\in B^{\prime}}\delta_{b,b^{\prime}%
}b\otimes b^{\prime}}_{=b\otimes b}=\sum\limits_{b\in B^{\prime}}b\otimes b\\
&  =\sum\limits_{a\in B^{\prime}}a\otimes a\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we renamed }b\text{ as }a\text{ in the sum}\right)  .
\end{align*}
This proves Lemma \ref{lem.sugawara.Kil2} \textbf{(b)}.

As a consequence of this lemma, we get:

\begin{lemma}
\label{lem.sugawara.Kil3}Let $\mathfrak{g}$ be a finite-dimensional
$\mathbb{C}$-Lie algebra with an invariant symmetric bilinear form $\left(
\cdot,\cdot\right)  $. Denote by $\operatorname*{Kil}$ the Killing form of
$\mathfrak{g}$. Let $B\subseteq\mathfrak{g}$ be a basis orthonormal with
respect to the form $k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}%
\operatorname*{Kil}$. Let $b\in\mathfrak{g}$.

\textbf{(a)} We have $\sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  =0$.

\textbf{(b)} We have $\dfrac{1}{2}\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(  b,a\right)  a=b$.

\textbf{(c)} We have $\left(  \left[  b,a\right]  ,a\right)  =0$ for every
$a\in\mathfrak{g}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.sugawara.Kil3}.} The basis $B$ is orthonormal
with respect to a symmetric $\mathfrak{g}$-invariant bilinear form (namely,
the form $k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$). As
a consequence, the tensor $\sum\limits_{a\in B}a\otimes a$ is $\mathfrak{g}%
$-invariant in $\mathfrak{g}\otimes\mathfrak{g}$ (by Lemma
\ref{lem.sugawara.Kil2} \textbf{(a)}, applied to $\left\langle \cdot
,\cdot\right\rangle =k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}%
\operatorname*{Kil}$). In other words, $\sum\limits_{a\in B}\left(  \left[
b,a\right]  \otimes a+a\otimes\left[  b,a\right]  \right)  =0$. This proves
Lemma \ref{lem.sugawara.Kil3} \textbf{(a)}.

\textbf{(b)} If $\left\langle \cdot,\cdot\right\rangle $ is any nondegenerate
inner product\footnote{By "inner product", we mean a symmetric bilinear form.}
on a finite-dimensional vector space $V$ and $B$ is an orthonormal basis with
respect to that product, then any vector $b\in V$ is equal to $\sum
\limits_{a\in B}\left\langle b,a\right\rangle a$. Applying this fact to the
inner product $\left\langle \cdot,\cdot\right\rangle =k\left(  \cdot
,\cdot\right)  +\dfrac{1}{2}\operatorname*{Kil}$ on the vector space
$V=\mathfrak{g}$, we conclude that $b=k\sum\limits_{a\in B}\left(  b,a\right)
a+\dfrac{1}{2}\sum\limits_{a\in B}\operatorname*{Kil}\left(  b,a\right)  a$.

Now, applying Lemma \ref{lem.sugawara.Kil} to the invariant tensor
$\sum\limits_{a\in B}a\otimes a$ in lieu of $\sum\limits_{i=1}^{n}p_{i}\otimes
q_{i}$, we see that $\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  =\sum\limits_{a\in B}\operatorname*{Kil}\left(  b,a\right)  a$.
Hence,%
\[
b=k\sum\limits_{a\in B}\left(  b,a\right)  a+\dfrac{1}{2}\underbrace{\sum
\limits_{a\in B}\operatorname*{Kil}\left(  b,a\right)  a}_{=\sum\limits_{a\in
B}\left[  \left[  b,a\right]  ,a\right]  }=\dfrac{1}{2}\sum\limits_{a\in
B}\left[  \left[  b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(
b,a\right)  a.
\]
This proves Lemma \ref{lem.sugawara.Kil3} \textbf{(b)}.

\textbf{(c)} Every $c\in\mathfrak{g}$ satisfies $\left(  \left[  b,a\right]
,c\right)  +\left(  a,\left[  b,c\right]  \right)  =0$ (due to the
$\mathfrak{g}$-invariance of $\left(  \cdot,\cdot\right)  $). Applying this to
$c=a$, we obtain $\left(  \left[  b,a\right]  ,a\right)  +\left(  a,\left[
b,a\right]  \right)  =0$. Since the form $\left(  \cdot,\cdot\right)  $ is
symmetric, this rewrites as $2\left(  \left[  b,a\right]  ,a\right)  =0$.
Thus, $\left(  \left[  b,a\right]  ,a\right)  =0$. This proves Lemma
\ref{lem.sugawara.Kil3} \textbf{(c)}.

Next, we formulate the analogue of Remark \ref{rmk.fockvir.normal.mn}:

\begin{remark}
\label{rmk.sugawara.normal.mn}Let $x\in\mathfrak{g}$. If $m$ and $n$ are
integers such that $m\neq-n$, then $\left.  :x_{m}x_{n}:\right.  =x_{m}x_{n}$.
(This is because $\left[  x_{m},x_{n}\right]  =0$ in $\widehat{\mathfrak{g}}$
when $m\neq-n$.)
\end{remark}

In analogy to Remark \ref{rmk.fockvir.normal.comm} \textbf{(a)}, we have
commutativity of normal ordered products:

\begin{remark}
\label{rmk.sugawara.normal.comm}Let $x\in\mathfrak{g}$. Any $m\in\mathbb{Z}$
and $n\in\mathbb{Z}$ satisfy $\left.  :x_{m}x_{n}:\right.  =\left.
:x_{n}x_{m}:\right.  $.
\end{remark}

Also, here is a simple way to rewrite the definition of $\left.  :x_{m}%
x_{n}:\right.  $:

\begin{remark}
\label{rmk.sugawara.normal.max}Let $x\in\mathfrak{g}$. Any $m\in\mathbb{Z}$
and $n\in\mathbb{Z}$ satisfy $\left.  :x_{m}x_{n}:\right.  =x_{\min\left\{
m,n\right\}  }x_{\max\left\{  m,n\right\}  }$.
\end{remark}

Generalizing Remark \ref{rmk.fockvir.normal.K}, we have:

\begin{remark}
\label{rmk.sugawara.normal.K}Let $x\in\mathfrak{g}$. Let $m$ and $n$ be integers.

\textbf{(a)} Then, $\left.  :x_{m}x_{n}:\right.  =x_{m}x_{n}+n\left[
m>0\right]  \delta_{m,-n}\left(  x,x\right)  K$. Here, when $\mathfrak{A}$ is
an assertion, we denote by $\left[  \mathfrak{A}\right]  $ the truth value of
$\mathfrak{A}$ (that is, the number $\left\{
\begin{array}
[c]{c}%
1\text{, if }\mathfrak{A}\text{ is true;}\\
0\text{, if }\mathfrak{A}\text{ is false }%
\end{array}
\right.  $).

\textbf{(b)} For any $y\in U\left(  \widehat{\mathfrak{g}}\right)  $, we have
$\left[  y,\left.  :x_{m}x_{n}:\right.  \right]  =\left[  y,x_{m}x_{n}\right]
$ in $U\left(  \widehat{\mathfrak{g}}\right)  $ (where $\left[  \cdot
,\cdot\right]  $ denotes the commutator in $U\left(  \widehat{\mathfrak{g}%
}\right)  $).
\end{remark}

The proof of this is left to the reader (it follows very quickly from the definitions).

Next, here is a completely elementary lemma:

\begin{lemma}
\label{lem.telescope}Let $G$ be an abelian group (written additively).
Whenever $\left(  u_{m}\right)  _{m\in\mathbb{Z}}\in G^{\mathbb{Z}}$ is a
family of elements of $G$, and $\mathcal{A}\left(  m\right)  $ is an assertion
for every $m\in\mathbb{Z}$, let us abbreviate the sum $\sum
\limits_{\substack{m\in\mathbb{Z};\\\mathcal{A}\left(  m\right)  }}u_{m}$ (if
this sum is well-defined) by $\sum\limits_{\mathcal{A}\left(  m\right)  }%
u_{m}$. (For instance, we will abbreviate the sum $\sum\limits_{\substack{m\in
\mathbb{Z};\\3\leq m\leq7}}u_{m}$ by $\sum\limits_{3\leq m\leq7}u_{m}$.)

For any integers $\alpha$ and $\beta$ such that $\alpha\leq\beta$, for any
nonnegative integer $N$, and for any family $\left(  u_{m}\right)
_{m\in\mathbb{Z}}\in G^{\mathbb{Z}}$ of elements of $G$, we have%
\[
\sum\limits_{\left\vert m-\beta\right\vert \leq N}u_{m}-\sum
\limits_{\left\vert m-\alpha\right\vert \leq N}u_{m}=-\sum\limits_{\alpha
-N\leq m<\beta-N}u_{m}+\sum\limits_{\alpha+N<m\leq\beta+N}u_{m}.
\]

\end{lemma}

The proof of Lemma \ref{lem.telescope} (which is merely an easy generalization
of the telescope principle) is left to the reader.

\textit{Proof of Theorem \ref{thm.sugawara}.} Let us use the notation
$\sum\limits_{\mathcal{A}\left(  m\right)  }u_{m}$ defined in Lemma
\ref{lem.telescope}.

In the following, we will consider the topology on $\operatorname*{End}M$
defined as follows: Endow $M$ with the discrete topology, endow $M^{M}$ with
the product topology, and endow $\operatorname*{End}M$ with a topology by
viewing $\operatorname*{End}M$ as a subset of the set $M^{M}$. Clearly, in
this topology, a net $\left(  a_{s}\right)  _{s\in S}$ of elements of
$\operatorname*{End}M$ converges if and only if for every $v\in M$, the net
$\left(  a_{s}v\right)  _{s\in S}$ of elements of $M$ converges (in the
discrete topology). As a consequence, whenever $\left(  u_{m}\right)
_{m\in\mathbb{Z}}$ is a family of elements of $\operatorname*{End}M$ indexed
by integers, the sum $\sum\limits_{m\in\mathbb{Z}}u_{m}$ converges with
respect to the topology which we defined on $\operatorname*{End}M$ if and only
if for every $v\in M$, the sum $\sum\limits_{m\in\mathbb{Z}}u_{m}v$ converges
in the discrete topology (i. e., has only finitely many nonzero addends).
Consequently, the convergence of an infinite sum with respect to the topology
which we defined on $\operatorname*{End}M$ is equivalent to the convergence of
this sum in the meaning in which we used the word "convergence" in Theorem
\ref{thm.sugawara}.

Note that addition, composition, and scalar multiplication (in the sense of:
multiplication by scalars) of maps in $\operatorname*{End}M$ are continuous
maps with respect to this topology.

We will use the notation $\lim\limits_{N\rightarrow\infty}$ for limits with
respect to the topology on $\operatorname*{End}M$. Note that, if $\left(
u_{m}\right)  _{m\in\mathbb{Z}}$ is a family of elements of
$\operatorname*{End}M$ indexed by integers, and if the sum $\sum
\limits_{m\in\mathbb{Z}}u_{m}$ converges with respect to the topology which we
defined on $\operatorname*{End}M$, then $\sum\limits_{m\in\mathbb{Z}}%
u_{m}=\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\alpha
\right\vert \leq N}u_{m}$ for every $\alpha\in\mathbb{R}$.

In the following, $\left[  \cdot,\cdot\right]  _{L\mathfrak{g}}$ will mean the
Lie bracket of $L\mathfrak{g}$, whereas the notation $\left[  \cdot
,\cdot\right]  $ without a subscript will mean either the Lie bracket of
$\widehat{\mathfrak{g}}$ or the Lie bracket of $\mathfrak{g}$. Note that the
use of the same notation for the Lie bracket of $\widehat{\mathfrak{g}}$ and
for the Lie bracket of $\mathfrak{g}$ will not lead to conflicts, since the
Lie bracket of $\mathfrak{g}$ is the restriction of the Lie bracket of
$\widehat{\mathfrak{g}}$ to $\mathfrak{g}\times\mathfrak{g}$ (this follows
quickly from $\omega\left(  \mathfrak{g},\mathfrak{g}\right)  =0$).

Note that any $x\in\mathfrak{g}$, $y\in\mathfrak{g}$, $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$ satisfy%
\begin{equation}
\left[  x_{n},y_{m}\right]  =\left[  x,y\right]  _{n+m}+K\omega\left(
x_{n},y_{m}\right)  \label{pf.sugawara.lie}%
\end{equation}
\footnote{This is because%
\begin{align*}
\left[  \underbrace{x_{n}}_{=xt^{n}},\underbrace{y_{m}}_{=yt^{m}}\right]   &
=\left[  xt^{n},yt^{m}\right]  =\left(  \underbrace{\left[  xt^{n}%
,yt^{m}\right]  _{L\mathfrak{g}}}_{\substack{=\left[  x,y\right]
t^{n+m}\\\text{(by the definition of the Lie}\\\text{algebra structure on
}L\mathfrak{g}\text{)}}},\omega\left(  \underbrace{xt^{n}}_{=x_{n}%
},\underbrace{yt^{m}}_{=y_{m}}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the Lie bracket on
}\widehat{\mathfrak{g}}\right) \\
&  =\left(  \underbrace{\left[  x,y\right]  t^{n+m}}_{=\left[  x,y\right]
_{n+m}},\omega\left(  x_{n},y_{m}\right)  \right)  =\left(  \left[
x,y\right]  _{n+m},\omega\left(  x_{n},y_{m}\right)  \right)  =\left[
x,y\right]  _{n+m}+K\omega\left(  x_{n},y_{m}\right)  .
\end{align*}
}.

\textbf{(a)} Let $n\in\mathbb{N}$ and $v\in M$. We must prove that for every
$a\in B$, the sum $\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.
v$ converges in the discrete topology. We will prove a slightly more general
statement: We will prove that for every $x\in\mathfrak{g}$, the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$ converges in
the discrete topology.

In fact, let $x\in\mathfrak{g}$. We must prove that the sum $\sum
\limits_{m\in\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$ converges in the
discrete topology.

Recall the definition of an admissible module. With slightly modified
notations, it looks as follows: A $\widehat{\mathfrak{g}}$-module $P$ is said
to be \textit{admissible} if for every $w\in P$, there exists some
$M\in\mathbb{N}$ such that every integer $m\geq M$ and every $a\in
\mathfrak{g}$ satisfy $at^{m}\cdot w=0$. Hence, for every $w\in M$, there
exists some $M\in\mathbb{N}$ such that every integer $m\geq M$ and every
$a\in\mathfrak{g}$ satisfy $at^{m}\cdot w=0$ (because $M$ is admissible).
Applying this to $w=v$, we see that there exists some $M\in\mathbb{N}$ such
that every integer $m\geq M$ and every $a\in\mathfrak{g}$ satisfy $at^{m}\cdot
v=0$. Fix this $M$. Every integer $m\geq M$ satisfies%
\begin{equation}
\underbrace{x_{m}}_{=xt^{m}}v=xt^{m}\cdot v=0 \label{pf.sugawara.a.1}%
\end{equation}
(by the equality $at^{m}\cdot v=0$, applied to $a=x$). Now, every integer $m$
such that $\max\left\{  m,n-m\right\}  \geq M$ satisfies%
\[
\underbrace{\left.  :x_{m}x_{n-m}:\right.  }_{\substack{=x_{\min\left\{
m,n-m\right\}  }x_{\max\left\{  m,n-m\right\}  }\\\text{(by Remark
\ref{rmk.sugawara.normal.max}, applied}\\\text{to }\ell=n-m\text{)}}%
}v=x_{\min\left\{  m,n-m\right\}  }\underbrace{x_{\max\left\{  m,n-m\right\}
}v}_{\substack{=0\\\text{(by (\ref{pf.sugawara.a.1}), applied to }\max\left\{
m,n-m\right\}  \\\text{instead of }m\text{ (since }\max\left\{  m,n-m\right\}
\geq M\text{))}}}=x_{\min\left\{  m,n-m\right\}  }0=0.
\]
Since all but finitely many integers $m$ satisfy $\max\left\{  m,n-m\right\}
\geq M$ (this is very obvious), this shows that all but finitely many integers
$m$ satisfy $\left.  :x_{m}x_{n-m}:\right.  v=0$. In other words, all but
finitely many addends of the sum $\sum\limits_{m\in\mathbb{Z}}\left.
:x_{m}x_{n-m}:\right.  v$ are zero. Hence, the sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$ converges in the discrete
topology. This proves Theorem \ref{thm.sugawara} \textbf{(a)}.

Note that, during the proof of Theorem \ref{thm.sugawara} \textbf{(a)}, we
have shown that for every $n\in\mathbb{N}$, $x\in\mathfrak{g}$ and $v\in M$,
the sum $\sum\limits_{m\in\mathbb{Z}}\left.  :x_{m}x_{n-m}:\right.  v$
converges in the discrete topology. In other words, for every $n\in\mathbb{N}$
and $x\in\mathfrak{g}$, the sum $\sum\limits_{m\in\mathbb{Z}}\left.
:x_{m}x_{n-m}:\right.  $ converges in the topology which we defined on
$\operatorname*{End}M$.

\textbf{(b)} Let $n\in\mathbb{Z}$. Let $B^{\prime}$ be an orthonormal basis of
$\mathfrak{g}$ with respect to the form $k\left(  \cdot,\cdot\right)
+\dfrac{1}{2}\operatorname*{Kil}$. We are going to prove that%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  \label{pf.sugawara.basisind.1}%
\end{equation}
(where $L_{n}$ still denotes the operator $\dfrac{1}{2}\sum\limits_{a\in
B}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  $ defined in
Theorem \ref{thm.sugawara} using the orthonormal basis $B$, not the
orthonormal basis $B^{\prime}$). Once (\ref{pf.sugawara.basisind.1}) is
proven, it will follow that $L_{n}$ does not depend on $B$, and thus Theorem
\ref{thm.sugawara} \textbf{(b)} will be proven.

Applying Lemma \ref{lem.sugawara.Kil2} \textbf{(b)} to $\left\langle
\cdot,\cdot\right\rangle =k\left(  \cdot,\cdot\right)  +\dfrac{1}%
{2}\operatorname*{Kil}$, we obtain $\sum\limits_{a\in B}a\otimes
a=\sum\limits_{a\in B^{\prime}}a\otimes a$. Thus,%
\begin{equation}
\sum\limits_{a\in B}a_{u}a_{v}=\sum\limits_{a\in B^{\prime}}a_{u}%
a_{v}\ \ \ \ \ \ \ \ \ \ \text{for any }u\in\mathbb{Z}\text{ and }%
v\in\mathbb{Z} \label{pf.sugawara.basisind.pf.1}%
\end{equation}
\footnote{This follows from applying the linear map
\begin{align*}
\mathfrak{g}\otimes\mathfrak{g}  &  \rightarrow\operatorname*{End}M,\\
x\otimes y  &  \mapsto x_{u}y_{v}%
\end{align*}
to the equality $\sum\limits_{a\in B}a\otimes a=\sum\limits_{a\in B^{\prime}%
}a\otimes a$.}.

Thus, every $m\in\mathbb{Z}$ satisfies $\sum\limits_{a\in B}\left.
:a_{m}a_{n-m}:\right.  =\sum\limits_{a\in B^{\prime}}\left.  :a_{m}%
a_{n-m}:\right.  $\ \ \ \ \footnote{\textit{Proof.} We distinguish between two
cases:
\par
\textit{Case 1:} We have $m\leq n-m$.
\par
\textit{Case 2:} We have $m>n-m$.
\par
Let us first consider Case 1. In this case, $m\leq n-m$. Hence, every
$a\in\mathfrak{g}$ satisfies $\left.  :a_{m}a_{n-m}:\right.  =a_{m}a_{n-m}$.
Thus,
\begin{align*}
\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.   &  =\sum\limits_{a\in
B}a_{m}a_{n-m}=\sum\limits_{a\in B^{\prime}}\underbrace{a_{m}a_{n-m}%
}_{=\left.  :a_{m}a_{n-m}:\right.  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.sugawara.basisind.pf.1}), applied to }u=m\text{ and }v=n-m\right) \\
&  =\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  .
\end{align*}
This proves $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  =\sum
\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ in Case 1.
\par
Let us now consider Case 2. In this case, $m>n-m$. Hence, every $a\in
\mathfrak{g}$ satisfies $\left.  :a_{m}a_{n-m}:\right.  =a_{n-m}a_{m}$. Thus,
\begin{align*}
\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.   &  =\sum\limits_{a\in
B}a_{n-m}a_{m}=\sum\limits_{a\in B^{\prime}}\underbrace{a_{n-m}a_{m}%
}_{=\left.  :a_{m}a_{n-m}:\right.  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.sugawara.basisind.pf.1}), applied to }u=n-m\text{ and }v=m\right) \\
&  =\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  .
\end{align*}
This proves $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  =\sum
\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ in Case 2.
\par
Hence, $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  =\sum\limits_{a\in
B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ is proven in each of the cases 1
and 2. Thus, $\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.
=\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  $ always holds
(since cases 1 and 2 cover all possibilities), qed.}. Hence,%
\begin{align*}
L_{n}  &  =\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}%
\underbrace{\sum\limits_{a\in B}\left.  :a_{m}a_{n-m}:\right.  }%
_{=\sum\limits_{a\in B^{\prime}}\left.  :a_{m}a_{n-m}:\right.  }=\dfrac{1}%
{2}\sum\limits_{m\in\mathbb{Z}}\sum\limits_{a\in B^{\prime}}\left.
:a_{m}a_{n-m}:\right. \\
&  =\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  .
\end{align*}
Thus, (\ref{pf.sugawara.basisind.1}) is proven. As we said, this completes the
proof of Theorem \ref{thm.sugawara} \textbf{(b)}.

\textbf{(c)} \textit{1st step:} Let us first show that%
\begin{equation}
\left[  b_{r},L_{n}\right]  =rb_{n+r}\ \ \ \ \ \ \ \ \ \ \text{for every }%
b\in\mathfrak{g}\text{ and any integers }r\text{ and }n.
\label{pf.sugawara.step1}%
\end{equation}


\textit{Proof of (\ref{pf.sugawara.step1}):} Let $b\in\mathfrak{g}$,
$r\in\mathbb{Z}$ and $n\in\mathbb{Z}$.

We must be careful here with infinite sums, since not even formal algebra
allows us to manipulate infinite sums like $\sum\limits_{m\in\mathbb{Z}%
}\left[  b,a\right]  _{r+m}a_{n-m}$ (for good reasons: these are divergent in
every meaning of this word). While we were working in the Heisenberg algebra
$\mathcal{A}$ (which can be written as $\widehat{\mathfrak{g}}$ for
$\mathfrak{g}$ being the trivial Lie algebra $\mathbb{C}$), these infinite
sums made sense due to all of their addends being $0$ (since $\left[
b,a\right]  =0$ for all $a$ and $b$ lying in the trivial Lie algebra
$\mathbb{C}$). But this was an exception rather than the rule, and now we need
to take care.

Let us first assume that $r\geq0$.

Since%
\begin{align*}
L_{n}  &  =\dfrac{1}{2}\sum\limits_{a\in B}\underbrace{\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  }_{=\lim\limits_{N\rightarrow\infty
}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left.
:a_{m}a_{n-m}:\right.  }=\dfrac{1}{2}\sum\limits_{a\in B}\lim
\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert
\leq N}\left.  :a_{m}a_{n-m}:\right. \\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left.  :a_{m}%
a_{n-m}:\right.  ,
\end{align*}
we have%
\begin{align}
&  \left[  b_{r},L_{n}\right] \nonumber\\
&  =\left[  b_{r},\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum
\limits_{a\in B}\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}\left.  :a_{m}a_{n-m}:\right.  \right] \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\underbrace{\left[
b_{r},\left.  :a_{m}a_{n-m}:\right.  \right]  }%
_{\substack{_{\substack{=\left[  b_{r},a_{m}a_{n-m}\right]  }}\\\text{(by
Remark \ref{rmk.sugawara.normal.K} \textbf{(b)}, applied to}\\a\text{, }%
b_{r}\text{ and }n-m\text{ instead of }x\text{, }y\text{ and }n\text{)}%
}}\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\underbrace{\left[
b_{r},a_{m}a_{n-m}\right]  }_{=\left[  b_{r},a_{m}\right]  a_{n-m}%
+a_{m}\left[  b_{r},a_{n-m}\right]  }\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\underbrace{\left[  b_{r},a_{m}\right]  }_{\substack{=\left[  b,a\right]
_{r+m}+K\omega\left(  b_{r},a_{m}\right)  \\\text{(by (\ref{pf.sugawara.lie}%
))}}}a_{n-m}+a_{m}\underbrace{\left[  b_{r},a_{n-m}\right]  }%
_{\substack{=\left[  b,a\right]  _{n+r-m}+K\omega\left(  b_{r},a_{n-m}\right)
\\\text{(by (\ref{pf.sugawara.lie}))}}}\right) \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}%
+a_{m}\left[  b,a\right]  _{n+r-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)
\right) \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
. \label{pf.sugawara.b.1}%
\end{align}


Now fix $a\in B$. We now notice that for any $N\in\mathbb{N}$, the sum
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}$ (in $\operatorname*{End}M$) has at most one
nonzero addend (because $\omega\left(  b_{r},a_{m}\right)  $ can be nonzero
for at most one integer $m$ (namely, for $m=-r$)). Hence, this sum
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}$ converges for any $N\in\mathbb{N}$. For
sufficiently high $N$, this sum does have an addend for $m=-r$, and all other
addends of this sum are $0$ (since $\omega\left(  b_{r},a_{m}\right)  =0$
whenever $m\neq-r$), so that the value of this sum is $\underbrace{K}%
_{\substack{=k\\\text{(since }K\text{ acts as}\\k\cdot\operatorname*{id}\text{
on }M\text{)}}}\underbrace{\omega\left(  b_{r},a_{-r}\right)  }%
_{\substack{=r\left(  b,a\right)  \\\text{(by the definition of }%
\omega\text{)}}}\underbrace{a_{n-\left(  -r\right)  }}_{=a_{n+r}}=kr\left(
b,a\right)  a_{n+r}$. We thus have shown that the sum $\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(  b_{r},a_{m}\right)  a_{n-m}$
converges for all $N\in\mathbb{N}$, and satisfies%
\begin{equation}
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}=kr\left(  b,a\right)  a_{n+r}%
\ \ \ \ \ \ \ \ \ \ \text{for sufficiently high }N\text{.}
\label{pf.sugawara.b.2a}%
\end{equation}
Similarly, we see that the sum $\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)  $ converges
for all $N\in\mathbb{N}$, and satisfies%
\begin{equation}
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}a_{m}K\omega\left(
b_{r},a_{n-m}\right)  =a_{n+r}kr\left(  b,a\right)
\ \ \ \ \ \ \ \ \ \ \text{for sufficiently high }N\text{.}
\label{pf.sugawara.b.2b}%
\end{equation}
Finally, for all $N\in\mathbb{N}$, the sum $\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[  b,a\right]  _{r+m}%
a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $
converges\footnote{\textit{Proof.} Let $N\in\mathbb{N}$. The sum
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
$ converges (because it appears on the right hand side of
(\ref{pf.sugawara.b.1})), and the sums $\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}K\omega\left(  b_{r},a_{m}\right)  a_{n-m}$ and
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}a_{m}K\omega\left(
b_{r},a_{n-m}\right)  $ converge (as we have just seen). Hence, the sum%
\begin{align*}
&  \sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}%
+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r}%
,a_{n-m}\right)  \right)  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  -K\omega\left(  b_{r},a_{m}\right)
a_{n-m}-a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
\end{align*}
converges as well (since it is obtained by subtracting the latter two sums
from the former sum componentwise). But this sum clearly simplifies to
$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $. Hence,
the sum $\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $
converges, qed.}.

Since the sums $\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}K\omega\left(  b_{r},a_{m}\right)  a_{n-m}$, $\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)  $
and \newline$\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  $
converge for every $N\in\mathbb{N}$, we have%
\begin{align*}
&  \sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}K\omega\left(
b_{r},a_{m}\right)  a_{n-m}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq
N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)
\end{align*}
for every $N\in\mathbb{N}$. Hence, for every sufficiently high $N\in
\mathbb{N}$, we have%
\begin{align}
&  \sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}+K\omega\left(
b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r},a_{n-m}\right)  \right)
\nonumber\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+\underbrace{\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}%
K\omega\left(  b_{r},a_{m}\right)  a_{n-m}}_{\substack{=kr\left(  b,a\right)
a_{n+r}\text{ for sufficiently high }N\\\text{(by (\ref{pf.sugawara.b.2a}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}a_{m}K\omega\left(  b_{r},a_{n-m}\right)  }%
_{\substack{=a_{n+r}kr\left(  b,a\right)  \text{ for sufficiently high
}N\\\text{(by (\ref{pf.sugawara.b.2a}))}}}\nonumber\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+\underbrace{kr\left(  b,a\right)  a_{n+r}+a_{n+r}kr\left(  b,a\right)
}_{=2rk\cdot\left(  b,a\right)  a_{n+r}}\nonumber\\
&  =\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}. \label{pf.sugawara.b.3.sufficiently}%
\end{align}


Now, forget that we fixed $a$. The equality (\ref{pf.sugawara.b.1}) becomes%
\begin{align}
&  \left[  b_{r},L_{n}\right] \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in
B}\underbrace{\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
\left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}%
+K\omega\left(  b_{r},a_{m}\right)  a_{n-m}+a_{m}K\omega\left(  b_{r}%
,a_{n-m}\right)  \right)  }_{\substack{=\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \left[  b,a\right]  _{r+m}a_{n-m}+a_{m}\left[
b,a\right]  _{n+r-m}\right)  +2rk\cdot\left(  b,a\right)  a_{n+r}\\\text{for
sufficiently high }N\text{ (by (\ref{pf.sugawara.b.3.sufficiently}))}%
}}\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left[
b,a\right]  _{r+m}a_{n-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)
+2rk\cdot\left(  b,a\right)  a_{n+r}\right) \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}+\sum\limits_{a\in B}a_{m}\left[  b,a\right]
_{n+r-m}\right)  +rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}.
\label{pf.sugawara.b.6}%
\end{align}
But since $\sum\limits_{a\in B}\left(  \left[  b,a\right]  \otimes
a+a\otimes\left[  b,a\right]  \right)  =0$ (by Lemma \ref{lem.sugawara.Kil3}
\textbf{(a)}), we have \newline$\sum\limits_{a\in B}\left(  \left[
b,a\right]  _{\ell}\otimes a_{s}+a_{\ell}\otimes\left[  b,a\right]
_{s}\right)  =0$ for any two integers $\ell$ and $s$. In particular, every
$m\in\mathbb{Z}$ satisfies $\sum\limits_{a\in B}\left(  \left[  b,a\right]
_{m}\otimes a_{n+r-m}+a_{m}\otimes\left[  b,a\right]  _{n+r-m}\right)  =0$.
Hence, every $m\in\mathbb{Z}$ satisfies $\sum\limits_{a\in B}\left(  \left[
b,a\right]  _{m}a_{n+r-m}+a_{m}\left[  b,a\right]  _{n+r-m}\right)  =0$, so
that $\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}+\sum\limits_{a\in
B}a_{m}\left[  b,a\right]  _{n+r-m}=0$ and thus $\sum\limits_{a\in B}%
a_{m}\left[  b,a\right]  _{n+r-m}=-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}$. Hence, (\ref{pf.sugawara.b.6}) becomes%
\begin{align}
&  \left[  b_{r},L_{n}\right] \nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}+\underbrace{\sum\limits_{a\in B}a_{m}\left[
b,a\right]  _{n+r-m}}_{=-\sum\limits_{a\in B}\left[  b,a\right]  _{m}%
a_{n+r-m}}\right)  +rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}%
\nonumber\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}\right)  +rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}.
\label{pf.sugawara.b.9}%
\end{align}
We will now transform the limit in this equation: In fact,%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\underbrace{\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[
b,a\right]  _{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}\right)  }_{=\sum\limits_{a\in B}\left(  \sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]  _{r+m}a_{n-m}%
-\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]
_{m}a_{n+r-m}\right)  }\\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left[
b,a\right]  _{r+m}a_{n-m}}_{\substack{=\sum\limits_{\left\vert m-r-\dfrac
{n}{2}\right\vert \leq N}\left[  b,a\right]  _{m}a_{n+r-m}\\\text{(here, we
substituted }m-r\text{ for }m\text{ in the sum)}}}-\sum\limits_{\left\vert
m-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\underbrace{\left(
\sum\limits_{\left\vert m-r-\dfrac{n}{2}\right\vert \leq N}\left[  b,a\right]
_{m}a_{n+r-m}-\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left[
b,a\right]  _{m}a_{n+r-m}\right)  }_{\substack{=-\sum\limits_{\dfrac{n}%
{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}+\sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  b,a\right]
_{m}a_{n+r-m}\\\text{(by Lemma \ref{lem.telescope}, applied to }u_{m}=\left[
b,a\right]  _{m}a_{n+r-m}\text{,}\\\alpha=\dfrac{n}{2}\text{ and }\beta
=\dfrac{n}{2}+r\text{)}}}\\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(  -\sum
\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]
_{m}a_{n+r-m}+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[
b,a\right]  _{m}a_{n+r-m}\right)  .
\end{align*}
Since every $a\in B$ satisfies $\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}%
{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}\rightarrow0$ for $N\rightarrow
\infty$\ \ \ \ \footnote{\textit{Proof.} Let $a\in B$.
\par
Let $w\in M$. From the proof of Theorem \ref{thm.sugawara} \textbf{(a)},
recall the fact that for every $w\in M$, there exists some $M\in\mathbb{N}$
such that every integer $m\geq M$ and every $a\in\mathfrak{g}$ satisfy
$at^{m}\cdot w=0$. Applied to $w=a$, this yields that there exists some
$M\in\mathbb{N}$ such that%
\begin{equation}
\text{every integer }m\geq M\text{ satisfies }at^{m}\cdot w=0.
\label{pf.sugawara.foot.1}%
\end{equation}
Consider this $M$.
\par
Let $N$ be an integer such that $N\geq M-\dfrac{n}{2}-r$. Then, $\dfrac{n}%
{2}+r+N\geq M$. Now, every integer $m$ such that $\dfrac{n}{2}-N\leq
m<\dfrac{n}{2}+r-N$ must satisfy $n+r-\underbrace{m}_{\geq\dfrac{n}{2}-N}\leq
n+r-\left(  \dfrac{n}{2}-N\right)  =\dfrac{n}{2}+r+N\geq M$ and thus
$at^{n+r-m}\cdot w=0$ (by (\ref{pf.sugawara.foot.1}), applied to $n+r-m$
instead of $m$), thus $\left[  b,a\right]  _{m}\underbrace{a_{n+r-m}%
}_{=at^{n+r-m}}w=\left[  b,a\right]  _{m}\cdot\underbrace{at^{n+r-m}\cdot
w}_{=0}=0$. Hence, $\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}w=\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}%
+r-N}0=0$.
\par
Now forget that we fixed $N$. We thus have showed that $\sum\limits_{\dfrac
{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}w=0$ for
every integer $N$ such that $N\geq M-\dfrac{n}{2}-r$. Hence, $\sum
\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]
_{m}a_{n+r-m}w=0$ for every sufficiently large $N$. Thus, $\sum\limits_{\dfrac
{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[  b,a\right]  _{m}a_{n+r-m}%
w\rightarrow0$ for $N\rightarrow\infty$. Since this holds for every $w\in M$,
we thus obtain $\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}\rightarrow0$ for $N\rightarrow\infty$, qed.}, this
becomes%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[  b,a\right]
_{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
-\underbrace{\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{n}{2}+r-N}\left[
b,a\right]  _{m}a_{n+r-m}}_{\rightarrow0\text{ for }N\rightarrow\infty}%
+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  b,a\right]
_{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\underbrace{\left[  b,a\right]  _{m}a_{n+r-m}%
}_{=a_{n+r-m}\left[  b,a\right]  _{m}+\left[  \left[  b,a\right]
_{m},a_{n+r-m}\right]  }\\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  a_{n+r-m}\left[  b,a\right]
_{m}+\left[  \left[  b,a\right]  _{m},a_{n+r-m}\right]  \right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(  \sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]
_{m}+\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  \left[
b,a\right]  _{m},a_{n+r-m}\right]  \right)  .
\end{align*}
Since every $a\in B$ satisfies $\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}%
{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}\rightarrow0$ for $N\rightarrow
\infty$\ \ \ \ \footnote{\textit{Proof.} Let $a\in B$.
\par
Let $w\in M$. From the proof of Theorem \ref{thm.sugawara} \textbf{(a)},
recall the fact that for every $w\in M$, there exists some $M\in\mathbb{N}$
such that every integer $m\geq M$ and every $a\in\mathfrak{g}$ satisfy
$at^{m}\cdot w=0$. Applied to $w=\left[  b,a\right]  $, this yields that there
exists some $M\in\mathbb{N}$ such that%
\begin{equation}
\text{every integer }m\geq M\text{ satisfies }\left[  b,a\right]  t^{m}\cdot
w=0. \label{pf.sugawara.foot.2}%
\end{equation}
Consider this $M$.
\par
Let $N$ be an integer such that $N\geq M-\dfrac{n}{2}$. Then, $\dfrac{n}%
{2}+N\geq M$. Now, every integer $m$ such that $\dfrac{n}{2}+N<m\leq\dfrac
{n}{2}+r+N$ must satisfy $m>\dfrac{n}{2}+N\geq M$ and thus $\left[
b,a\right]  t^{m}\cdot w=0$ (by (\ref{pf.sugawara.foot.2})), thus
$a_{n+r-m}\underbrace{\left[  b,a\right]  _{m}}_{=\left[  b,a\right]  t^{m}%
}w=a_{n+r-m}\underbrace{\left[  b,a\right]  t^{m}\cdot w}_{=0}=0$. Hence,
$\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[
b,a\right]  _{m}w=\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}0=0$.
\par
Now forget that we fixed $N$. We thus have showed that $\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}w=0$ for every
integer $N$ such that $N\geq M-\dfrac{n}{2}$. Hence, $\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}w=0$ for every
sufficiently large $N$. Thus, $\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}%
{2}+r+N}a_{n+r-m}\left[  b,a\right]  _{m}w\rightarrow0$ for $N\rightarrow
\infty$. Since this holds for every $w\in M$, we thus obtain $\sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[  b,a\right]
_{m}\rightarrow0$ for $N\rightarrow\infty$, qed.}, this becomes%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[  b,a\right]
_{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}a_{n+r-m}\left[
b,a\right]  _{m}}_{\rightarrow0\text{ for }N\rightarrow\infty}+\sum
\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left[  \left[  b,a\right]
_{m},a_{n+r-m}\right]  \right) \\
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\underbrace{\left[  \left[  b,a\right]
_{m},a_{n+r-m}\right]  }_{\substack{=\left[  \left[  b,a\right]  ,a\right]
_{n+r}+K\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)  \\\text{(by
(\ref{pf.sugawara.lie}), applied to }\left[  b,a\right]  \text{, }a\text{,
}m\text{ and }n+r-m\\\text{instead of }x\text{, }y\text{, }n\text{, }%
m\text{)}}}
\end{align*}%
\begin{align*}
&  =\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  \left[  \left[  b,a\right]  ,a\right]
_{n+r}+\underbrace{K}_{\substack{=k\\\text{(since }K\text{ acts on }M\text{ as
}k\cdot\operatorname*{id}\text{)}}}\omega\left(  \left[  b,a\right]
_{m},a_{n+r-m}\right)  \right) \\
&  =\lim\limits_{N\rightarrow\infty}\underbrace{\sum\limits_{a\in B}%
\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\left(  \left[  \left[
b,a\right]  ,a\right]  _{n+r}+k\omega\left(  \left[  b,a\right]
_{m},a_{n+r-m}\right)  \right)  }_{=\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac
{n}{2}+r+N}\sum\limits_{a\in B}\left[  \left[  b,a\right]  ,a\right]
_{n+r}+k\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\sum\limits_{a\in
B}\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)  }\\
&  =\lim\limits_{N\rightarrow\infty}\left(  \underbrace{\sum\limits_{\dfrac
{n}{2}+N<m\leq\dfrac{n}{2}+r+N}\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}}_{=r\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}}+k\sum\limits_{\dfrac{n}{2}+N<m\leq\dfrac{n}{2}+r+N}%
\sum\limits_{a\in B}\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)
\right) \\
&  =\lim\limits_{N\rightarrow\infty}\left(  r\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  _{n+r}+k\sum\limits_{\dfrac{n}{2}+N<m\leq
\dfrac{n}{2}+r+N}\sum\limits_{a\in B}\omega\left(  \left[  b,a\right]
_{m},a_{n+r-m}\right)  \right)  .
\end{align*}
Since every integer $m$ and every $a\in B$ satisfy $\omega\left(  \left[
b,a\right]  _{m},a_{n+r-m}\right)  =0$\ \ \ \ \footnote{\textit{Proof.} Let
$m$ be an integer, and let $a\in B$. From Lemma \ref{lem.sugawara.Kil3}
\textbf{(c)}, we have $\left(  \left[  b,a\right]  ,a\right)  =0$, so that
$m\left(  \left[  b,a\right]  ,a\right)  =0$. But by the definition of
$\omega$, we have
\begin{align*}
\omega\left(  \left[  b,a\right]  _{m},a_{n+r-m}\right)   &  =\left\{
\begin{array}
[c]{c}%
m\left(  \left[  b,a\right]  ,a\right)  ,\text{ if }m=-\left(  n+r-m\right)
;\\
0,\text{ if }m\neq-\left(  n+r-m\right)
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
0,\text{ if }m=-\left(  n+r-m\right)  ;\\
0,\text{ if }m\neq-\left(  n+r-m\right)
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\left(  \left[  b,a\right]
,a\right)  =0\right) \\
&  =0,
\end{align*}
qed.}, this simplifies to%
\begin{align*}
&  \lim\limits_{N\rightarrow\infty}\sum\limits_{\left\vert m-\dfrac{n}%
{2}\right\vert \leq N}\left(  \sum\limits_{a\in B}\left[  b,a\right]
_{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]  _{m}a_{n+r-m}\right) \\
&  =\lim\limits_{N\rightarrow\infty}\left(  r\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  _{n+r}+k\sum\limits_{\dfrac{n}{2}+N<m\leq
\dfrac{n}{2}+r+N}\sum\limits_{a\in B}\underbrace{\omega\left(  \left[
b,a\right]  _{m},a_{n+r-m}\right)  }_{=0}\right) \\
&  =\lim\limits_{N\rightarrow\infty}r\sum\limits_{a\in B}\left[  \left[
b,a\right]  ,a\right]  _{n+r}=r\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}.
\end{align*}
Thus, (\ref{pf.sugawara.b.9}) becomes%
\begin{align*}
&  \left[  b_{r},L_{n}\right] \\
&  =\dfrac{1}{2}\underbrace{\lim\limits_{N\rightarrow\infty}\sum
\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \sum\limits_{a\in
B}\left[  b,a\right]  _{r+m}a_{n-m}-\sum\limits_{a\in B}\left[  b,a\right]
_{m}a_{n+r-m}\right)  }_{=r\sum\limits_{a\in B}\left[  \left[  b,a\right]
,a\right]  _{n+r}}+rk\sum\limits_{a\in B}\left(  b,a\right)  a_{n+r}\\
&  =\dfrac{1}{2}r\sum\limits_{a\in B}\underbrace{\left[  \left[  b,a\right]
,a\right]  _{n+r}}_{=\left[  \left[  b,a\right]  ,a\right]  t^{n+r}}%
+rk\sum\limits_{a\in B}\left(  b,a\right)  \underbrace{a_{n+r}}_{=at^{n+r}}\\
&  =rt^{n+r}\underbrace{\left(  \dfrac{1}{2}\sum\limits_{a\in B}\left[
\left[  b,a\right]  ,a\right]  +k\sum\limits_{a\in B}\left(  b,a\right)
a\right)  }_{\substack{=b\\\text{(by Lemma \ref{lem.sugawara.Kil3}
\textbf{(b)})}}}=r\underbrace{t^{n+r}b}_{=b_{n+r}}=rb_{n+r}.
\end{align*}
This proves (\ref{pf.sugawara.step1}) in the case when $r\geq0$. The case when
$r\leq0$ is handled analogously (except that this time we have to apply Lemma
\ref{lem.telescope} to $u_{m}=\left[  b,a\right]  _{m}a_{n+r-m}$,
$\alpha=\dfrac{n}{2}+r$ and $\beta=\dfrac{n}{2}$ instead of applying it to
$u_{m}=\left[  b,a\right]  _{m}a_{n+r-m}$, $\alpha=\dfrac{n}{2}$ and
$\beta=\dfrac{n}{2}+r$). Altogether, the proof of (\ref{pf.sugawara.step1}) is
thus complete.

\textit{2nd step:} It is clear that%
\begin{equation}
\left[  L_{n},a_{m}\right]  =-ma_{n+m}\ \ \ \ \ \ \ \ \ \ \text{for any }%
a\in\mathfrak{g}\text{ and any integers }n\text{ and }m
\label{pf.sugawara.step2.am}%
\end{equation}
(since (\ref{pf.sugawara.step1}) (applied to $r=m$ and $a=b$) yields $\left[
a_{m},L_{n}\right]  =ma_{n+m}$, so that $\left[  L_{n},a_{m}\right]
=-\underbrace{\left[  a_{m},L_{n}\right]  }_{=ma_{n+m}}=-ma_{n+m}$). Also, it
is clear that%
\begin{equation}
\left[  L_{n},K\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for any integer }n
\label{pf.sugawara.step2.K}%
\end{equation}
(since $K$ acts as a scalar on $M$).

\textit{3rd step:} Now, we will prove that%
\begin{equation}
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}-n}%
{12}\delta_{n,-m}k\cdot\sum\limits_{a\in B}\left(  a,a\right)
\ \ \ \ \ \ \ \ \ \ \text{for any integers }n\text{ and }m
\label{pf.sugawara.step3}%
\end{equation}
(as an identity in $\operatorname*{End}M$).

\textit{Proof of (\ref{pf.sugawara.step3}):} We know that every $n\in
\mathbb{Z}$ satisfies%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{n-m}:\right.  =\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  \label{pf.sugawara.step3.pf.1}%
\end{equation}
(here, we substituted $-m$ for $m$ in the second sum).

Repeat the Second Proof of Proposition \ref{prop.fockvir.answer2}, with the
following changes:

\begin{itemize}
\item Reprove Lemma \ref{lem.fockvir.welldef} with $F_{\mu}$ replaced by $M$
and with an additional "Let $a\in\mathfrak{g}$ be arbitrary." condition. (The
proof will be slightly different from the proof of the original Lemma
\ref{lem.fockvir.welldef} because $M$ is no longer a polynomial ring, but this
time we can use the admissibility of $M$ instead.)

\item Replace every $F_{\mu}$ by $M$.

\item Instead of the equality (\ref{def.fockvir.def}), use the equality
(\ref{pf.sugawara.step3.pf.1}) (which differs from the equality
(\ref{def.fockvir.def}) only in the presence of a $\sum\limits_{a\in B}$
sign). As a consequence, $\sum\limits_{a\in B}$ signs need to be dragged along
through the computations (but they don't complicate the calculation).

\item Instead of using Remark \ref{rmk.fockvir.normal.mn}, use Remark
\ref{rmk.sugawara.normal.mn}.

\item Instead of using Remark \ref{rmk.fockvir.normal.comm} \textbf{(a)}, use
Remark \ref{rmk.sugawara.normal.comm}.

\item Instead of using Remark \ref{rmk.fockvir.normal.K}, use Remark
\ref{rmk.sugawara.normal.K}.

\item Instead of using Proposition \ref{prop.fockvir.answer1}, use
(\ref{pf.sugawara.step2.am}).

\item Instead of the equality $a_{m-\ell}a_{n+\ell}=\left.  :a_{m-\ell
}a_{n+\ell}:\right.  -\left(  n+\ell\right)  \left[  \ell<m\right]
\delta_{m,-n}\operatorname*{id}$, check the equality $a_{m-\ell}a_{n+\ell
}=\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n+\ell\right)  \left[
\ell<m\right]  \delta_{m,-n}\left(  a,a\right)  k$ for every $a\in B$.

\item Instead of the equality $a_{-\ell}a_{m+n+\ell}=\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  -\ell\left[  \ell<0\right]  \delta_{m,-n}%
\operatorname*{id}$, check the equality $a_{-\ell}a_{m+n+\ell}=\left.
:a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[  \ell<0\right]  \delta
_{m,-n}\left(  a,a\right)  k$ for every $a\in B$.
\end{itemize}

Once these changes (most of which are automatic) are made, we have obtained a
proof of (\ref{pf.sugawara.step3}).

\textit{4th step:} From (\ref{pf.sugawara.step3}), it is clear that the
endomorphisms $L_{n}$ for $n\in\mathbb{Z}$ give rise to a $\operatorname*{Vir}%
$-representation on $M$ with central charge%
\[
c=k\cdot\sum\limits_{a\in B}\left(  a,a\right)  .
\]
This proves Theorem \ref{thm.sugawara} \textbf{(c)}.

\textbf{(d)} From (\ref{pf.sugawara.step2.am}) and (\ref{pf.sugawara.step2.K}%
), it follows that the formulas for $L_{n}$ and $c$ we have given in Theorem
\ref{thm.sugawara} extend the action of $\widehat{\mathfrak{g}}$ on $M$ to an
action of $\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$. Theorem
\ref{thm.sugawara} \textbf{(d)} thus is proven.

\begin{noncompile}
Here is the old proof of (\ref{pf.sugawara.step3}). Note that I don't
understand it and I am pretty sure it is partly wrong.

Let $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. Now we must prove the Virasoro
relations $\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\left(
\text{something}\right)  $.

Let $R_{n,m}=\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$. Since
$\left[  L_{n},a_{r}\right]  =-ra_{n+r}$, it is clear that $R_{n,m}$ commutes
with $a_{r}$ for all $a$ and $r$. In particular, this yields that $R_{n,m}$
commutes with $L_{0}$ (since $L_{0}$ is made out of $a_{r}$'s). But $\left[
L_{0},R_{n,m}\right]  =\left(  n+m\right)  R_{n,m}$ (since $\left[
a_{r},L_{0}\right]  =ra_{r}$ and thus $\left[  L_{n},L_{0}\right]  =nL_{n}$).
Hence, $R_{n,m}=0$ whenever $n+m\neq0$.

The only case that remains to be checked is $n+m=0$. So we need to compute
$\left[  L_{n},L_{-n}\right]  -2nL_{0}$.

We write%
\begin{align*}
&  \left[  L_{n},L_{-n}\right]  -2nL_{0}=R_{n,-n}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}%
\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(  \left(
-m\right)  a_{n+m}a_{-n-m}+\left(  n+m\right)  a_{m}a_{-m}\right)  -2nL_{0}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\sum\limits_{\left\vert m-\dfrac{3n}{2}\right\vert \leq N}\left(  -m+n\right)
a_{m}a_{-m}+\sum\limits_{\left\vert m-\dfrac{n}{2}\right\vert \leq N}\left(
n+m\right)  a_{m}a_{-m}\right)  -2nL_{0}\\
&  =\dfrac{1}{2}\lim\limits_{N\rightarrow\infty}\sum\limits_{a\in B}\left(
\underbrace{\sum\limits_{\dfrac{n}{2}-N\leq m<\dfrac{3n}{2}-N}\left(
m+n\right)  a_{m}a_{-m}}_{\rightarrow0}+\sum\limits_{\dfrac{n}{2}+N\leq
m<\dfrac{3n}{2}+N}\left(  -m+n\right)  a_{m}a_{-m}+\sum\limits_{1\leq
m\leq\dfrac{n}{2}+N}2nmk\left(  a,a\right)  \right) \\
&  =\operatorname*{constant}.
\end{align*}
We calculate this constant: It is enough to calculate it for $n=1$ and for
$n=2$. For $n=1$, it is $\dfrac{1}{2}\left(  -N\left(  N+1\right)  +N\left(
N+1\right)  \right)  \cdot k\sum\limits_{a\in B}\left(  a,a\right)  =0$. For
$n=2$, it is
\[
\dfrac{1}{2}\underbrace{\left(  -N\left(  N+2\right)  -\left(  N+1\right)
\left(  N+3\right)  +2\left(  N+2\right)  \left(  N+1\right)  \right)  }%
_{=1}\cdot k\sum\limits_{a\in B}\left(  a,a\right)  =\dfrac{1}{2}\cdot
k\sum\limits_{a\in B}\left(  a,a\right)  .
\]
Thus, $R_{1,-1}=0$ and $R_{2,-2}=\dfrac{1}{2}\cdot k\sum\limits_{a\in
B}\left(  a,a\right)  $. As a consequence, $R_{n,-n}=\dfrac{n^{3}-n}{12}%
k\sum\limits_{a\in B}\left(  a,a\right)  $. (We could have also obtained this
by direct computation.) This proves that, with $c=k\sum\limits_{a\in B}\left(
a,a\right)  $, we get a representation of $\operatorname*{Vir}$.
\end{noncompile}

\textbf{(e)} Theorem \ref{thm.sugawara} \textbf{(e)} follows immediately from
(\ref{pf.sugawara.step2.am}).

Thus, the proof of Theorem \ref{thm.sugawara} is complete.

We are now going to specialize these results to the case of $\mathfrak{g}$
being simple. In this case, the so-called \textit{dual Coxeter number} of the
simple Lie algebra $\mathfrak{g}$ comes into play. Let us explain what this is:

\begin{definition}
\label{def.dualcox}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. Let $\theta$ be the maximal root of $\mathfrak{g}$. (In other words,
let $\theta$ be the highest weight of the adjoint representation of
$\mathfrak{g}$.) Let $\rho=\dfrac{1}{2}\sum\limits_{\substack{\alpha\text{
root of }\mathfrak{g}\text{;}\\\alpha>0}}\alpha$ be the half-sum of all
positive roots. The \textit{dual Coxeter number} $h^{\vee}$ of $\mathfrak{g}$
is defined by $h^{\vee}=1+\left(  \theta,\rho\right)  $. It is easy to show
that $h^{\vee}$ is a positive integer.
\end{definition}

\begin{definition}
\label{def.standform}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. The \textit{standard form} on $\mathfrak{g}$ will mean the scalar
multiple of the Killing form under which $\left(  \alpha,\alpha\right)  $
(under the inverse form on $\mathfrak{g}^{\ast}$) equals $2$ for long roots
$\alpha$. (We do not care to define what a long root is, but it is enough to
say that the maximal root $\theta$ is a long root, and this is clearly enough
to define the standard form.)

(The \textit{inverse form} of a nondegenerate bilinear form $\left(
\cdot,\cdot\right)  $ on $\mathfrak{g}$ means the bilinear form on
$\mathfrak{g}^{\ast}=\mathfrak{h}^{\ast}\oplus\mathfrak{n}_{+}^{\ast}%
\oplus\mathfrak{n}_{-}^{\ast}$ obtained by dualizing the bilinear form
$\left(  \cdot,\cdot\right)  $ on $\mathfrak{g}=\mathfrak{h}\oplus
\mathfrak{n}_{+}\oplus\mathfrak{n}_{-}$ using itself.)

We are going to denote the standard form by $\left(  \cdot,\cdot\right)  $.
\end{definition}

\begin{lemma}
\label{lem.dualcox}Let $B$ be an orthonormal basis of $\mathfrak{g}$ with
respect to the standard form. Let $C=\sum\limits_{a\in B}a^{2}\in U\left(
\mathfrak{g}\right)  $. This element $C$ is known to be central in $U\left(
\mathfrak{g}\right)  $ (this is easily checked), and is called the
\textit{quadratic Casimir}.

Then:

\textbf{(1)} For every $\lambda\in\mathfrak{h}^{\ast}$, the element $C\in
U\left(  \mathfrak{g}\right)  $ acts on $L_{\lambda}$ by $\left(
\lambda,\lambda+2\rho\right)  \cdot\operatorname*{id}$. (Here, $L_{\lambda}$
means $L_{\lambda}^{+}$, but actually can be replaced by any highest-weight
module with highest weight $\lambda$.)

\textbf{(2)} The element $C\in U\left(  \mathfrak{g}\right)  $ acts on the
adjoint representation $\mathfrak{g}$ by $2h^{\vee}\cdot\operatorname*{id}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.dualcox}.} If $\left(  b_{i}\right)  _{i\in
I}$ is any basis of $\mathfrak{g}$, and $\left(  b_{i}^{\ast}\right)  _{i\in
I}$ is the dual basis of $\mathfrak{g}$ with respect to the standard form
$\left(  \cdot,\cdot\right)  $, then%
\begin{equation}
C=\sum\limits_{i\in I}b_{i}b_{i}^{\ast}. \label{pf.dualcox.Csum}%
\end{equation}
\footnote{This is a well-known property of the quadratic Casimir.}

\textbf{(1)} Let $\lambda\in\mathfrak{h}^{\ast}$.

Let us refine the triangular decomposition $\mathfrak{g}=\mathfrak{h}%
\oplus\mathfrak{n}_{+}\oplus\mathfrak{n}_{-}$ to the weight space
decomposition $\mathfrak{g}=\mathfrak{h}\oplus\left(  \bigoplus\limits_{\alpha
>0}\mathfrak{g}_{\alpha}\right)  \oplus\left(  \bigoplus\limits_{\alpha
<0}\mathfrak{g}_{\alpha}\right)  $, where $\mathfrak{g}_{\alpha}%
=\mathbb{C}e_{\alpha}$ for roots $\alpha>0$, and $\mathfrak{g}_{-\alpha
}=\mathbb{C}f_{\alpha}$ for roots $\alpha>0$. (This is standard theory of
simple Lie algebras.) Normalize the $f_{\alpha}$ in such a way that $\left(
e_{\alpha},f_{\alpha}\right)  =1$. As usual, denote $h_{\alpha}=\left[
e_{\alpha},f_{\alpha}\right]  $ for every root $\alpha>0$.

Fix an orthonormal basis $\left(  x_{i}\right)  _{i\in\left\{
1,2,...,r\right\}  }$ of $\mathfrak{h}$. Clearly, $\left(  x_{i}\right)
_{i\in\left\{  1,2,...,r\right\}  }\cup\left(  e_{\alpha}\right)  _{\alpha
>0}\cup\left(  f_{\alpha}\right)  _{\alpha>0}$ (where the index $\alpha$ runs
over positive roots only) is a basis of $\mathfrak{g}$. Since%
\begin{align*}
\left(  e_{\alpha},x_{i}\right)   &  =\left(  f_{\alpha},x_{i}\right)
=0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,...,r\right\}  \text{
and roots }\alpha>0;\\
\left(  e_{\alpha},f_{\beta}\right)   &  =0\ \ \ \ \ \ \ \ \ \ \text{for any
two distinct roots }\alpha>0\text{ and }\beta>0;\\
\left(  e_{\alpha},e_{\gamma}\right)   &  =\left(  f_{\alpha},f_{\gamma
}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for any roots }\alpha>0\text{ and
}\gamma>0;\\
\left(  x_{i},x_{j}\right)   &  =\delta_{i,j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i\in\left\{  1,2,...,r\right\}  \text{ and }j\in\left\{  1,2,...,r\right\}
;\\
\left(  e_{\alpha},f_{\alpha}\right)   &  =\left(  f_{\alpha},e_{\alpha
}\right)  =1\ \ \ \ \ \ \ \ \ \ \text{for any root }\alpha>0\text{,}%
\end{align*}
we see that $\left(  x_{i}\right)  _{i\in\left\{  1,2,...,r\right\}  }%
\cup\left(  f_{\alpha}\right)  _{\alpha>0}\cup\left(  e_{\alpha}\right)
_{\alpha>0}$ is the dual basis to this basis $\left(  x_{i}\right)
_{i\in\left\{  1,2,...,r\right\}  }\cup\left(  e_{\alpha}\right)  _{\alpha
>0}\cup\left(  f_{\alpha}\right)  _{\alpha>0}$ with respect to the standard
form $\left(  \cdot,\cdot\right)  $. Thus, (\ref{pf.dualcox.Csum}) yields%
\[
C=\sum\limits_{i=1}^{r}x_{i}^{2}+\sum\limits_{\alpha>0}\left(  f_{\alpha
}e_{\alpha}+e_{\alpha}f_{\alpha}\right)  ,
\]
so that (denoting $v_{\lambda}^{+}$ by $v_{\lambda}$) we have%
\begin{align*}
Cv_{\lambda}  &  =\sum\limits_{i=1}^{r}\underbrace{x_{i}^{2}v_{\lambda}%
}_{=\lambda\left(  x_{i}\right)  ^{2}v_{\lambda}}+\sum\limits_{\alpha
>0}\left(  f_{\alpha}e_{\alpha}+e_{\alpha}f_{\alpha}\right)  v_{\lambda
}=\underbrace{\sum\limits_{i=1}^{r}\lambda\left(  x_{i}\right)  ^{2}%
}_{=\left(  \lambda,\lambda\right)  }v_{\lambda}+\sum\limits_{\alpha>0}\left(
f_{\alpha}\underbrace{e_{\alpha}v_{\lambda}}_{=0}+\underbrace{e_{\alpha
}f_{\alpha}}_{=f_{\alpha}e_{\alpha}+\left[  e_{\alpha},f_{\alpha}\right]
}v_{\lambda}\right) \\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha>0}\left(
f_{\alpha}e_{\alpha}+\left[  e_{\alpha},f_{\alpha}\right]  \right)
v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}f_{\alpha}\underbrace{e_{\alpha}v_{\lambda}}_{=0}+\sum\limits_{\alpha
>0}\underbrace{\left[  e_{\alpha},f_{\alpha}\right]  }_{=h_{\alpha}}%
v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}\underbrace{h_{\alpha}v_{\lambda}}_{=\lambda\left(  h_{\alpha}\right)
v_{\lambda}}=\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha
>0}\underbrace{\lambda\left(  h_{\alpha}\right)  }_{=\left(  \lambda
,\alpha\right)  }v_{\lambda}\\
&  =\left(  \lambda,\lambda\right)  v_{\lambda}+\sum\limits_{\alpha>0}\left(
\lambda,\alpha\right)  v_{\lambda}=\underbrace{\left(  \left(  \lambda
,\lambda\right)  +\sum\limits_{\alpha>0}\left(  \lambda,\alpha\right)
\right)  }_{\substack{=\left(  \lambda,\lambda+\sum\limits_{\alpha>0}%
\alpha\right)  =\left(  \lambda,\lambda+2\rho\right)  \\\text{(since }%
\sum\limits_{\alpha>0}\alpha=2\rho\text{)}}}v_{\lambda}=\left(  \lambda
,\lambda+2\rho\right)  v_{\lambda}.
\end{align*}
Thus, every $a\in U\left(  \mathfrak{g}\right)  $ satisfies
\begin{align*}
Cav_{\lambda}  &  =a\underbrace{Cv_{\lambda}}_{=\left(  \lambda,\lambda
+2\rho\right)  v_{\lambda}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }C\text{ is
central in }U\left(  \mathfrak{g}\right)  \right) \\
&  =\left(  \lambda,\lambda+2\rho\right)  av_{\lambda}.
\end{align*}
Hence, $C$ acts as $\left(  \lambda,\lambda+2\rho\right)  \cdot
\operatorname*{id}$ on $L_{\lambda}$ (because every element of $L_{\lambda}$
has the form $av_{\lambda}$ for some $a\in U\left(  \mathfrak{g}\right)  $).
This proves Lemma \ref{lem.dualcox} \textbf{(1)}.

\textbf{(2)} We have $\mathfrak{g}=L_{\theta}$, and thus Lemma
\ref{lem.dualcox} \textbf{(1)} yields%
\[
C\mid_{L_{\theta}}=\left(  \theta,\theta+2\rho\right)  =\underbrace{\left(
\theta,\theta\right)  }_{=2}+2\left(  \theta,\rho\right)  =2+2\left(
\theta,\rho\right)  =2h^{\vee}.
\]
This proves Lemma \ref{lem.dualcox} \textbf{(2)}.

Here is a little table of dual Coxeter numbers, depending on the root system
type of $\mathfrak{g}$:

For $A_{n-1}$, we have $h^{\vee}=n$.

For $B_{n}$, we have $h^{\vee}=2n-1$.

For $C_{n}$, we have $h^{\vee}=n+1$.

For $D_{n}$, we have $h^{\vee}=2n-2$.

For $E_{6}$, we have $h^{\vee}=12$.

For $E_{7}$, we have $h^{\vee}=18$.

For $E_{8}$, we have $h^{\vee}=30$.

For $F_{4}$, we have $h^{\vee}=9$.

For $G_{2}$, we have $h^{\vee}=4$.

Every Lie theorist is supposed to remember these by heart.

\begin{lemma}
\label{lem.dualcox.kil}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. Then,%
\[
\operatorname*{Kil}\left(  a,b\right)  =2h^{\vee}\cdot\left(  a,b\right)
\ \ \ \ \ \ \ \ \ \ \text{for any }a,b\in\mathfrak{g}.
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.dualcox.kil}.} Let $B$ be an orthonormal basis
of $\mathfrak{g}$ with respect to the standard form. Define the quadratic
Casimir $C=\sum\limits_{a\in B}a^{2}$ as in Lemma \ref{lem.dualcox}. Then,%
\[
\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  C\right)  =\sum\limits_{a\in
B}\underbrace{\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  a^{2}\right)
}_{=\operatorname*{Tr}\left(  \left(  \operatorname*{ad}a\right)  \circ\left(
\operatorname*{ad}a\right)  \right)  =\operatorname*{Kil}\left(  a,a\right)
}=\sum\limits_{a\in B}\operatorname*{Kil}\left(  a,a\right)  .
\]
Comparing this with%
\begin{align*}
\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(  C\right)   &  =2h^{\vee
}\underbrace{\operatorname*{Tr}\nolimits_{\mathfrak{g}}\left(
\operatorname*{id}\right)  }_{=\dim\mathfrak{g}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }C\mid_{\mathfrak{g}}=2h^{\vee}\operatorname*{id}\text{ by Lemma
\ref{lem.dualcox} \textbf{(2)}}\right) \\
&  =2h^{\vee}\underbrace{\dim\mathfrak{g}}_{\substack{=\left\vert B\right\vert
=\sum\limits_{a\in B}1=\sum\limits_{a\in B}\left(  a,a\right)  \\\text{(since
every }a\in B\text{ satisfies }\left(  a,a\right)  =1\text{)}}}=2h^{\vee}%
\sum\limits_{a\in B}\left(  a,a\right)  ,
\end{align*}
we obtain $\sum\limits_{a\in B}\operatorname*{Kil}\left(  a,a\right)
=2h^{\vee}\sum\limits_{a\in B}\left(  a,a\right)  $. Since
$\operatorname*{Kil}$ is a scalar multiple of $\left(  \cdot,\cdot\right)  $
(because there is only one $\mathfrak{g}$-invariant symmetric bilinear form on
$\mathfrak{g}$ up to scaling), this yields $\operatorname*{Kil}=2h^{\vee}%
\cdot\left(  \cdot,\cdot\right)  $ (because $\sum\limits_{a\in B}%
\underbrace{\left(  a,a\right)  }_{=1}=\sum\limits_{a\in B}1=\left\vert
B\right\vert \neq0$). Lemma \ref{lem.dualcox.kil} is proven.

So let us now look at the Sugawara construction when $\mathfrak{g}$ is simple
finite-dimensional. First of all, $k$ is noncritical if and only if
$k\neq-h^{\vee}$. (The value $k=-h^{\vee}$ is called the \textit{critical
level}.)

If $B^{\prime}$ is an orthonormal basis under $\left(  \cdot,\cdot\right)  $
(rather than under $k\left(  \cdot,\cdot\right)  +\dfrac{1}{2}%
\operatorname*{Kil}=\left(  k+h^{\vee}\right)  \left(  \cdot,\cdot\right)  $),
then we have%
\begin{align}
L_{n}  &  =\dfrac{1}{2\left(  k+h^{\vee}\right)  }\sum\limits_{a\in B^{\prime
}}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.
\ \ \ \ \ \ \ \ \ \ \text{and}\nonumber\\
c  &  =\dfrac{k}{k+h^{\vee}}\underbrace{\sum\limits_{a\in B^{\prime}}\left(
a,a\right)  }_{\substack{=\left\vert B^{\prime}\right\vert \\\text{(since
}\left(  a,a\right)  =1\text{ for every }a\in B^{\prime}\text{)}}}=\dfrac
{k}{k+h^{\vee}}\underbrace{\left\vert B^{\prime}\right\vert }_{=\dim
\mathfrak{g}}=\dfrac{k\dim\mathfrak{g}}{k+h^{\vee}}.
\label{thm.sugawara.simple.c}%
\end{align}
In particular, this induces an internal grading on any $\widehat{\mathfrak{g}%
}$-module which is a quotient of $M_{\lambda}^{+}$ by eigenvalues of $L_{0}$,
whenever $\lambda$ is a weight of $\widehat{\mathfrak{g}}$. This is a grading
by complex numbers, since eigenvalues of $L_{0}$ are not necessarily integers.
(Note that this does not work for general admissible modules in lieu of
quotients of $M_{\lambda}^{+}$.)

What happens at the critical level $k=-h^{\vee}$ ? The above formulas with
$k+h^{\vee}$ in the denominators clearly don't work at this level anymore. We
can, however, remove the denominators, i. e., consider the operators%
\[
T_{n}=\dfrac{1}{2}\sum\limits_{a\in B^{\prime}}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{m}a_{n-m}:\right.  .
\]
Then, the same calculations as we did in the proof of Theorem
\ref{thm.sugawara} tell us that these $T_{n}$ satisfy $\left[  T_{n}%
,a_{m}\right]  =0$ and $\left[  T_{n},T_{m}\right]  =0$; they are thus central
"elements" of $\operatorname*{End}M$ (except that they are not actually
elements of $U\left(  \widehat{\mathfrak{g}}\right)  $, but of some completion
acting on admissible modules).

For any complex numbers $\gamma_{1},\gamma_{2},\gamma_{3},...$, we can
construct a $\widehat{\mathfrak{g}}$-module $M_{\lambda}\diagup\left(
\sum\limits_{m\geq1}\left(  \left(  T_{m}-\gamma_{m}\right)  M_{\lambda
}\right)  \right)  $, which does not have a grading. So, at the critical
level, we do not automatically get gradings on quotients of $M_{\lambda}$
anymore. This is one reason why representations at the critical level are
considered more difficult than those at non-critical levels.

\subsection{The Sugawara construction and unitarity}

We now will show that the Sugawara construction preserves unitarity:

\begin{proposition}
Consider the situation of Theorem \ref{thm.sugawara}. If $M$ is a unitary
admissible module for $\widehat{\mathfrak{g}}$, then $M$ is a unitary
$\operatorname*{Vir}\ltimes\widehat{\mathfrak{g}}$-module. (We recall that the
Virasoro algebra had its unitary structure given by $L_{n}^{\dag}=L_{-n}$.)
\end{proposition}

But for $M$ to be unitary for $\widehat{\mathfrak{g}}$, we need $k\in
\mathbb{Z}_{+}$ (this is easy to prove; we proved it for $\mathfrak{sl}_{n}$,
and the general case is similar). Since for $k=0$, there is only the trivial
representation, we really must require $k\geq1$ to get something interesting.
And since $c=\dfrac{k\dim\mathfrak{g}}{k+h^{\vee}}$, the $c$ is then $\geq1$,
since $\dim\mathfrak{g}\geq1+h^{\vee}$. These modules are already known to us
to be unitary, so this construction does not help us in constructing new
unitary modules.

But there is a way to amend this by a variation of the Sugawara construction:
the Goddard-Kent-Olive construction.

\subsection{The Goddard-Kent-Olive construction (a.k.a. the coset
construction)}

\begin{definition}
Let $\mathfrak{g}$ and $\mathfrak{p}$ be two finite-dimensional Lie algebras
such that $\mathfrak{g}\supseteq\mathfrak{p}$. Let $\left(  \cdot
,\cdot\right)  $ be a $\mathfrak{g}$-invariant form (possibly degenerate) on
$\mathfrak{g}$. We can restrict this form to $\mathfrak{p}$, and have
$\widehat{\mathfrak{g}}\supseteq\widehat{\mathfrak{p}}$. Choose a level $k$
which is noncritical for both $\mathfrak{g}$ and $\mathfrak{p}$.

If $M$ is an admissible $\widehat{\mathfrak{g}}$-module at level $k$, then on
$M$ we have two Virasoro actions: One is obtained from the
$\widehat{\mathfrak{g}}$-action, and one which is obtained from the
$\widehat{\mathfrak{p}}$-action. We will denote these actions by $\left(
L_{i}^{\mathfrak{g}}\right)  _{i\in\mathbb{Z}}$ and $\left(  L_{i}%
^{\mathfrak{p}}\right)  _{i\in\mathbb{Z}}$, respectively (that is, for every
$i\in\mathbb{Z}$, we denote by $L_{i}^{\mathfrak{g}}$ the action of $L_{i}%
\in\operatorname*{Vir}$ obtained from the $\widehat{\mathfrak{g}}$-module
structure on $M$, and we denote by $L_{i}^{\mathfrak{p}}$ the action of
$L_{i}\in\operatorname*{Vir}$ obtained from the $\widehat{\mathfrak{p}}%
$-module structure on $M$), and we will denote their central charges by
$c_{\mathfrak{g}}$ and $c_{\mathfrak{p}}$, respectively.
\end{definition}

\begin{theorem}
\label{thm.goddardkentolive}Let $L_{i}=L_{i}^{\mathfrak{g}}-L_{i}%
^{\mathfrak{p}}$ for all $i\in\mathbb{Z}$.

\textbf{(a)} Then, $\left(  L_{i}\right)  _{i\in\mathbb{Z}}$ is a
$\operatorname*{Vir}$-action on $M$ with central charge $c=c_{\mathfrak{g}%
}-c_{\mathfrak{p}}$.

\textbf{(b)} Also, $\left[  L_{n},\widehat{p}\right]  =0$ for all
$\widehat{p}\in\widehat{\mathfrak{p}}$ and $n\in\mathbb{Z}$.

\textbf{(c)} Moreover, $\left[  L_{n},L_{m}^{\mathfrak{p}}\right]  =0$ for all
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.goddardkentolive}.} \textbf{(b)} Let
$n\in\mathbb{Z}$. Every $p\in\mathfrak{p}$ and $m\in\mathbb{Z}$ satisfy%
\[
\left[  \underbrace{L_{n}}_{=L_{n}^{\mathfrak{g}}-L_{n}^{p}},p_{m}\right]
=\underbrace{\left[  L_{n}^{\mathfrak{g}},p_{m}\right]  }%
_{\substack{=-mp_{n+m}\\\text{(by Theorem \ref{thm.sugawara} \textbf{(e)}%
,}\\\text{applied to }p\text{ instead of }a\text{)}}}-\underbrace{\left[
L_{n}^{\mathfrak{p}},p_{m}\right]  }_{\substack{=-mp_{n+m}\\\text{(by Theorem
\ref{thm.sugawara} \textbf{(e)},}\\\text{applied to }p\text{ and }%
\mathfrak{p}\text{ instead of }a\text{ and }\mathfrak{g}\text{)}}}=\left(
-mp_{m+n}\right)  -\left(  -mp_{m+n}\right)  =0.
\]
Combined with the fact that $\left[  L_{n},K\right]  =0$ (this is trivial,
since $K$ acts as $k\cdot\operatorname*{id}$ on $M$), this yields that
$\left[  L_{n},\widehat{p}\right]  =0$ for all $\widehat{p}\in
\widehat{\mathfrak{p}}$ and $n\in\mathbb{Z}$ (because every $\widehat{p}%
\in\widehat{\mathfrak{p}}$ is a $\mathbb{C}$-linear combinations of terms of
the form $p_{m}$ (with $p\in\mathfrak{p}$ and $m\in\mathbb{Z}$) and $K$).
Thus, Theorem \ref{thm.goddardkentolive} \textbf{(b)} is proven.

\textbf{(c)} Let $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. We recall that $L_{n}$
was defined by $L_{n}=\dfrac{1}{2}\sum\limits_{a\in B}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{m}a_{n-m}:\right.  $, where $B$ is an orthonormal basis
of $\mathfrak{p}$ with respect to a certain bilinear form on $B$. Thus,
$L_{n}$ is a sum of products of elements of $\widehat{\mathfrak{p}}$ (or, more
precisely, their actions on $M$). Since we know that $L_{n}$ commutes with
every element of $\widehat{\mathfrak{p}}$ (due to Theorem
\ref{thm.goddardkentolive} \textbf{(b)}), this yields that $L_{n}$ commutes
with $L_{m}^{\mathfrak{p}}$. In other words, $\left[  L_{n},L_{m}%
^{\mathfrak{p}}\right]  =0$. Theorem \ref{thm.goddardkentolive} \textbf{(c)}
is thus established.

\textbf{(a)} Any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfy%
\begin{align*}
&  \left[  L_{n},\underbrace{L_{m}}_{=L_{m}^{\mathfrak{g}}-L_{m}%
^{\mathfrak{p}}}\right] \\
&  =\left[  L_{n},L_{m}^{\mathfrak{g}}-L_{m}^{\mathfrak{p}}\right]  =\left[
L_{n},L_{m}^{\mathfrak{g}}\right]  -\underbrace{\left[  L_{n},L_{m}%
^{\mathfrak{p}}\right]  }_{\substack{=0\\\text{(by Theorem
\ref{thm.goddardkentolive} \textbf{(c)})}}}=\left[  \underbrace{L_{n}}%
_{=L_{n}^{\mathfrak{g}}-L_{n}^{\mathfrak{p}}},L_{m}^{\mathfrak{g}}\right] \\
&  =\left[  L_{n}^{\mathfrak{g}}-L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}%
}\right]  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
-\underbrace{\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}}\right]
}_{\substack{=\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{g}}-L_{m}%
^{\mathfrak{p}}\right]  +\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}%
}\right]  \\\text{(since }L_{m}^{\mathfrak{g}}=\left(  L_{m}^{\mathfrak{g}%
}-L_{m}^{\mathfrak{p}}\right)  +L_{m}^{\mathfrak{p}}\text{)}}}\\
&  =\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]  -\left[
L_{n}^{\mathfrak{p}},\underbrace{L_{m}^{\mathfrak{g}}-L_{m}^{\mathfrak{p}}%
}_{=L_{m}}\right]  -\left[  L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}}\right]
\\
&  =\underbrace{\left[  L_{n}^{\mathfrak{g}},L_{m}^{\mathfrak{g}}\right]
}_{\substack{=\left(  n-m\right)  L_{n+m}^{\mathfrak{g}}-\dfrac{n^{3}-n}%
{12}c_{\mathfrak{g}}\delta_{n,-m}\\\text{(by Theorem \ref{thm.sugawara}
\textbf{(c)})}}}-\underbrace{\left[  L_{n}^{\mathfrak{p}},L_{m}\right]
}_{=-\left[  L_{m},L_{n}^{\mathfrak{p}}\right]  }-\underbrace{\left[
L_{n}^{\mathfrak{p}},L_{m}^{\mathfrak{p}}\right]  }_{\substack{=\left(
n-m\right)  L_{n+m}^{\mathfrak{p}}-\dfrac{n^{3}-n}{12}c_{\mathfrak{p}}%
\delta_{n,-m}\\\text{(by Theorem \ref{thm.sugawara} \textbf{(c)}%
,}\\\text{applied to }\mathfrak{p}\text{ instead of }\mathfrak{g}\text{)}}}\\
&  =\left(  \left(  n-m\right)  L_{n+m}^{\mathfrak{g}}-\dfrac{n^{3}-n}%
{12}c_{\mathfrak{g}}\delta_{n,-m}\right)  +\left[  L_{m},L_{n}^{\mathfrak{p}%
}\right]  -\left(  \left(  n-m\right)  L_{n+m}^{\mathfrak{p}}-\dfrac{n^{3}%
-n}{12}c_{\mathfrak{p}}\delta_{n,-m}\right) \\
&  =\left(  n-m\right)  \underbrace{\left(  L_{n+m}^{\mathfrak{g}}%
-L_{n+m}^{\mathfrak{p}}\right)  }_{=L_{n+m}}-\dfrac{n^{3}-n}{12}\left(
c_{\mathfrak{g}}-c_{\mathfrak{p}}\right)  \delta_{n,-m}+\underbrace{\left[
L_{m},L_{n}^{\mathfrak{p}}\right]  }_{\substack{=0\\\text{(by Theorem
\ref{thm.goddardkentolive} \textbf{(c)},}\\\text{applied to }m\text{ and
}n\text{ instead of }n\text{ and }m\text{)}}}\\
&  =\left(  n-m\right)  L_{n+m}-\dfrac{n^{3}-n}{12}\left(  c_{\mathfrak{g}%
}-c_{\mathfrak{p}}\right)  \delta_{n,-m}.
\end{align*}
Hence, $\left(  L_{i}\right)  _{i\in\mathbb{Z}}$ is a $\operatorname*{Vir}%
$-action on $M$ with central charge $c=c_{\mathfrak{g}}-c_{\mathfrak{p}}$.
Theorem \ref{thm.goddardkentolive} \textbf{(a)} is thus proven. This completes
the proof of Theorem \ref{thm.goddardkentolive}.

\begin{example}
Let $\mathfrak{a}$ be a simple finite-dimensional Lie algebra. Let
$\mathfrak{g}=\mathfrak{a}\oplus\mathfrak{a}$, and let $\mathfrak{p}%
=\mathfrak{a}_{\operatorname*{diag}}\subseteq\mathfrak{a}\oplus\mathfrak{a}$
(where $\mathfrak{a}_{\operatorname*{diag}}$ denotes the Lie subalgebra
$\left\{  \left(  x,x\right)  \ \mid\ x\in\mathfrak{a}\right\}  $ of
$\mathfrak{a}\oplus\mathfrak{a}$). Consider the standard form $\left(
\cdot,\cdot\right)  $ on $\mathfrak{a}$. Define a symmetric bilinear form on
$\mathfrak{a}\oplus\mathfrak{a}$ as the direct sum of the standard forms on
$\mathfrak{a}$ and $\mathfrak{a}$.

Let $V^{\prime}$ and $V^{\prime\prime}$ be admissible $\widehat{\mathfrak{a}}%
$-modules at levels $k^{\prime}$ and $k^{\prime\prime}$. Theorem
\ref{thm.sugawara} endows these vector spaces $V^{\prime}$ and $V^{\prime
\prime}$ with $\operatorname*{Vir}$-module structures. These
$\operatorname*{Vir}$-module structures have central charges $c_{\mathfrak{a}%
}^{\prime}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}}$ and
$c_{\mathfrak{a}}^{\prime\prime}=\dfrac{k^{\prime\prime}\dim\mathfrak{a}%
}{k^{\prime\prime}+h^{\vee}}$, respectively (by (\ref{thm.sugawara.simple.c}%
)). Let $\left(  L_{i}^{\prime}\right)  _{i\in\mathbb{Z}}$ and $\left(
L_{i}^{\prime\prime}\right)  _{i\in\mathbb{Z}}$ denote the actions of
$\operatorname*{Vir}$ on these modules.

Then, $V^{\prime}\otimes V^{\prime\prime}$ is an admissible
$\widehat{\mathfrak{g}}$-module at level $k^{\prime}+k^{\prime\prime}$. Thus,
by Theorem \ref{thm.sugawara}, this vector space $V^{\prime}\otimes
V^{\prime\prime}$ becomes a $\operatorname*{Vir}$-module. The action $\left(
L_{i}^{\mathfrak{g}}\right)  _{i\in\mathbb{Z}}$ of $\operatorname*{Vir}$ on
this $\operatorname*{Vir}$-module $V^{\prime}\otimes V^{\prime\prime}$ is
given by $L_{i}^{\mathfrak{g}}=L_{i}^{\prime}+L_{i}^{\prime\prime}$ (or, more
precisely, $L_{i}^{\mathfrak{g}}=L_{i}^{\prime}\otimes\operatorname*{id}%
+\operatorname*{id}\otimes L_{i}^{\prime\prime}$). The central charge
$c_{\mathfrak{g}}$ of this $\operatorname*{Vir}$-module $V^{\prime}\otimes
V^{\prime\prime}$ is
\[
c_{\mathfrak{g}}=c_{\mathfrak{a}}^{\prime}+c_{\mathfrak{a}}^{\prime\prime
}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}}+\dfrac
{k^{\prime\prime}\dim\mathfrak{a}}{k^{\prime\prime}+h^{\vee}}.
\]


Since $\widehat{\mathfrak{p}}=\widehat{\mathfrak{a}}$ acts on $V^{\prime
}\otimes V^{\prime\prime}$ by diagonal action, we also get a
$\operatorname*{Vir}$-module structure $\left(  L_{i}^{\mathfrak{p}}\right)
_{i\in\mathbb{Z}}$ on $V^{\prime}\otimes V^{\prime\prime}$ by applying Theorem
\ref{thm.sugawara} to $\mathfrak{p}$ instead of $\mathfrak{g}$. The central
charge of this $\operatorname*{Vir}$-module is
\[
c_{\mathfrak{p}}=\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime}+k^{\prime
\prime}+h^{\vee}}\dim\mathfrak{a}%
\]
(since the level of the $\widehat{\mathfrak{p}}$-module $V^{\prime}\otimes
V^{\prime\prime}$ is $k^{\prime}+k^{\prime\prime}$).

Thus, the central charge $c$ of the $\operatorname*{Vir}$-action on
$V^{\prime}\otimes V^{\prime\prime}$ given by Theorem
\ref{thm.goddardkentolive} is%
\begin{align*}
c  &  =c_{\mathfrak{a}}^{\prime}+c_{\mathfrak{a}}^{\prime\prime}%
-c_{\mathfrak{p}}=\dfrac{k^{\prime}\dim\mathfrak{a}}{k^{\prime}+h^{\vee}%
}+\dfrac{k^{\prime\prime}\dim\mathfrak{a}}{k^{\prime\prime}+h^{\vee}}%
-\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime}+k^{\prime\prime}+h^{\vee}}%
\dim\mathfrak{a}\\
&  =\left(  \dfrac{k^{\prime}}{k^{\prime}+h^{\vee}}+\dfrac{k^{\prime\prime}%
}{k^{\prime\prime}+h^{\vee}}-\dfrac{k^{\prime}+k^{\prime\prime}}{k^{\prime
}+k^{\prime\prime}+h^{\vee}}\right)  \dim\mathfrak{a}.
\end{align*}


We can use this construction to obtain, for every positive integer $m$, a
unitary representation of $\operatorname*{Vir}$ with central charge
$1-\dfrac{6}{\left(  m+2\right)  \left(  m+3\right)  }$: In fact, let
$\mathfrak{a}=\mathfrak{sl}_{2}$, so that $h^{\vee}=2$, and let $k^{\prime}=1$
and $k^{\prime\prime}=m$. Then,%
\[
c=3\left(  \dfrac{1}{3}+\dfrac{m}{m+2}-\dfrac{m+1}{m+3}\right)  =1-\dfrac
{6}{\left(  m+2\right)  \left(  m+3\right)  }.
\]
So we get unitary representations of $\operatorname*{Vir}$ with central charge
$c$ for these values of $c$.
\end{example}

\subsection{Preliminaries to simple and Kac-Moody Lie algebras}

Our next goal is defining and studying the Kac-Moody Lie algebras. Before we
do this, however, we will recollect some properties of simple
finite-dimensional Lie algebras (which are, in some sense, the prototypical
Kac-Moody Lie algebras); and yet before that, we show some general results
from the theory of Lie algebras which will be used in our later proofs.

\subsubsection{A basic property of $\mathfrak{sl}_{2}$-modules}

We begin with a lemma from the representation theory of $\mathfrak{sl}_{2}$:

\begin{lemma}
\label{lem.serre-gen.sl2}Let $e$, $f$ and $h$ mean the classical basis
elements of $\mathfrak{sl}_{2}$. Let $\lambda\in\mathbb{C}$. We consider any
$\mathfrak{sl}_{2}$-module as a $U\left(  \mathfrak{sl}_{2}\right)  $-module.

\textbf{(a)} Let $V$ be an $\mathfrak{sl}_{2}$-module. Let $x\in V$ be such
that $ex=0$ and $hx=\lambda x$. Then, every $n\in\mathbb{N}$ satisfies
$e^{n}f^{n}x=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x$.

\textbf{(b)} Let $V$ be an $\mathfrak{sl}_{2}$-module. Let $x\in V$ be such
that $fx=0$ and $hx=\lambda x$. Then, every $n\in\mathbb{N}$ satisfies
$f^{n}e^{n}x=n!\lambda\left(  \lambda+1\right)  ...\left(  \lambda+n-1\right)
x$.

\textbf{(c)} Let $V$ be a finite-dimensional $\mathfrak{sl}_{2}$-module. Let
$x$ be a nonzero element of $V$ satisfying $ex=0$ and $hx=\lambda x$. Then,
$\lambda\in\mathbb{N}$ and $f^{\lambda+1}x=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.serre-gen.sl2}.} \textbf{(a)} \textit{1st
step:} We will see that%
\begin{equation}
hf^{m}x=\left(  \lambda-2m\right)  f^{m}x\ \ \ \ \ \ \ \ \ \ \text{for every
}m\in\mathbb{N}. \label{pf.serre-gen.sl2.1}%
\end{equation}


\textit{Proof of (\ref{pf.serre-gen.sl2.1}):} We will prove
(\ref{pf.serre-gen.sl2.1}) by induction over $m$:

\textit{Induction base:} For $m=0$, we have $hf^{m}x=hf^{0}x=hx=\lambda x$ and
$\left(  \lambda-2m\right)  f^{m}x=\left(  \lambda-2\cdot0\right)
f^{0}x=\lambda x$, so that $hf^{m}x=\left(  \lambda-2m\right)  f^{m}x$ holds
for $m=0$. In other words, (\ref{pf.serre-gen.sl2.1}) holds for $m=0$. This
completes the induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$. Assume that
(\ref{pf.serre-gen.sl2.1}) holds for $m=M$. We must then prove that
(\ref{pf.serre-gen.sl2.1}) holds for $m=M+1$ as well.

Since (\ref{pf.serre-gen.sl2.1}) holds for $m=M$, we have $hf^{M}x=\left(
\lambda-2M\right)  f^{M}x$. Now,%
\begin{align*}
h\underbrace{f^{M+1}}_{=ff^{M}}x  &  =\underbrace{hf}_{=fh+\left[  h,f\right]
}f^{M}x=\left(  fh+\left[  h,f\right]  \right)  f^{M}x=f\underbrace{hf^{M}%
x}_{=\left(  \lambda-2M\right)  f^{M}x}+\underbrace{\left[  h,f\right]
}_{=-2f}f^{M}x\\
&  =\left(  \lambda-2M\right)  \underbrace{ff^{M}}_{=f^{M+1}}%
x-2\underbrace{ff^{M}}_{=f^{M+1}}x=\left(  \lambda-2M\right)  f^{M+1}%
x-2f^{M+1}x\\
&  =\underbrace{\left(  \lambda-2M-2\right)  }_{=\lambda-2\left(  M+1\right)
}f^{M+1}x=\left(  \lambda-2\left(  M+1\right)  \right)  f^{M+1}x.
\end{align*}
Thus, (\ref{pf.serre-gen.sl2.1}) holds for $m=M+1$ as well. This completes the
induction step. The induction proof of (\ref{pf.serre-gen.sl2.1}) is thus complete.

\textit{2nd step:} We will see that%
\begin{equation}
ef^{m}x=m\left(  \lambda-m+1\right)  f^{m-1}x\ \ \ \ \ \ \ \ \ \ \text{for
every positive }m\in\mathbb{N}. \label{pf.serre-gen.sl2.2}%
\end{equation}


\textit{Proof of (\ref{pf.serre-gen.sl2.2}):} We will prove
(\ref{pf.serre-gen.sl2.2}) by induction over $m$:

\textit{Induction base:} For $m=1$, we have%
\[
ef^{m}x=\underbrace{ef^{1}}_{=ef=\left[  e,f\right]  +fe}x=\left(  \left[
e,f\right]  +fe\right)  x=\underbrace{\left[  e,f\right]  }_{=h}%
x+f\underbrace{ex}_{=0}=hx+f0=hx=\lambda x
\]
and $m\left(  \lambda-m+1\right)  f^{m-1}x=1\underbrace{\left(  \lambda
-1+1\right)  }_{=\lambda}\underbrace{f^{1-1}}_{=1}x=\lambda x$, so that
$ef^{m}x=m\left(  \lambda-m+1\right)  f^{m-1}x$ holds for $m=1$. In other
words, (\ref{pf.serre-gen.sl2.2}) holds for $m=1$. This completes the
induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$ be positive. Assume that
(\ref{pf.serre-gen.sl2.2}) holds for $m=M$. We must then prove that
(\ref{pf.serre-gen.sl2.2}) holds for $m=M+1$ as well.

Since (\ref{pf.serre-gen.sl2.2}) holds for $m=M$, we have $ef^{M}x=M\left(
\lambda-M+1\right)  f^{M-1}x$. Now,%
\begin{align*}
e\underbrace{f^{M+1}}_{=ff^{M}}x  &  =\underbrace{ef}_{=fe+\left[  e,f\right]
}f^{M}x=\left(  fe+\left[  e,f\right]  \right)  f^{M}x=f\underbrace{ef^{M}%
}_{=M\left(  \lambda-M+1\right)  f^{M-1}x}x+\underbrace{\left[  e,f\right]
}_{=h}f^{M}x\\
&  =M\left(  \lambda-M+1\right)  \underbrace{ff^{M-1}}_{=f^{M}}%
x+\underbrace{hf^{M}x}_{\substack{=\left(  \lambda-2M\right)  f^{M}%
x\\\text{(by (\ref{pf.serre-gen.sl2.1}), applied to }m=M\text{)}}}\\
&  =M\left(  \lambda-M+1\right)  f^{M}x+\left(  \lambda-2M\right)
f^{M}x=\underbrace{\left(  M\left(  \lambda-M+1\right)  +\left(
\lambda-2M\right)  \right)  }_{=\left(  M+1\right)  \left(  \lambda-\left(
M+1\right)  +1\right)  }f^{M}x\\
&  =\left(  M+1\right)  \left(  \lambda-\left(  M+1\right)  +1\right)  f^{M}x.
\end{align*}
Thus, (\ref{pf.serre-gen.sl2.2}) holds for $m=M+1$ as well. This completes the
induction step. The induction proof of (\ref{pf.serre-gen.sl2.2}) is thus complete.

\textit{3rd step:} We will see that%
\begin{equation}
e^{n}f^{n}x=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{N}.
\label{pf.serre-gen.sl2.3}%
\end{equation}


\textit{Proof of (\ref{pf.serre-gen.sl2.3}):} We will prove
(\ref{pf.serre-gen.sl2.3}) by induction over $n$:

\textit{Induction base:} For $n=0$, we have $e^{n}f^{n}x=e^{0}f^{0}x=x$ and
$n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x=\underbrace{0!}_{=1}\underbrace{\lambda\left(  \lambda-1\right)  ...\left(
\lambda-0+1\right)  }_{=\left(  \text{empty product}\right)  =1}x=x$, so that
$e^{n}f^{n}x=n!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-n+1\right)
x$ holds for $n=0$. In other words, (\ref{pf.serre-gen.sl2.3}) holds for
$n=0$. This completes the induction base.

\textit{Induction step:} Let $N\in\mathbb{N}$. Assume that
(\ref{pf.serre-gen.sl2.3}) holds for $n=N$. We must then prove that
(\ref{pf.serre-gen.sl2.3}) holds for $n=N+1$ as well.

Since (\ref{pf.serre-gen.sl2.3}) holds for $n=N$, we have $e^{N}%
f^{N}x=N!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-N+1\right)  x$.
Now,%
\begin{align*}
\underbrace{e^{N+1}}_{=e^{N}e}f^{N+1}x  &  =e^{N}\underbrace{ef^{N+1}%
x}_{\substack{=\left(  N+1\right)  \left(  \lambda-\left(  N+1\right)
+1\right)  f^{\left(  N+1\right)  -1}x\\\text{(by (\ref{pf.serre-gen.sl2.2}),
applied to }m=N+1\text{)}}}=\left(  N+1\right)  \left(  \lambda-\left(
N+1\right)  +1\right)  e^{N}\underbrace{f^{\left(  N+1\right)  -1}}_{=f^{N}%
}x\\
&  =\left(  N+1\right)  \left(  \lambda-\left(  N+1\right)  +1\right)
\underbrace{e^{N}f^{N}x}_{=N!\lambda\left(  \lambda-1\right)  ...\left(
\lambda-N+1\right)  x}\\
&  =\left(  N+1\right)  \left(  \lambda-\left(  N+1\right)  +1\right)  \cdot
N!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-N+1\right)  x\\
&  =\underbrace{\left(  \left(  N+1\right)  \cdot N!\right)  }_{=\left(
N+1\right)  !}\cdot\underbrace{\left(  \lambda\left(  \lambda-1\right)
...\left(  \lambda-N+1\right)  \right)  \cdot\left(  \lambda-\left(
N+1\right)  +1\right)  }_{=\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  N+1\right)  +1\right)  }x\\
&  =\left(  N+1\right)  !\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  N+1\right)  +1\right)  x.
\end{align*}
Thus, (\ref{pf.serre-gen.sl2.3}) holds for $n=N+1$ as well. This completes the
induction step. The induction proof of (\ref{pf.serre-gen.sl2.3}) is thus complete.

Lemma \ref{lem.serre-gen.sl2} \textbf{(a)} immediately follows from
(\ref{pf.serre-gen.sl2.3}).

\textbf{(b)} The proof of Lemma \ref{lem.serre-gen.sl2} \textbf{(b)} is
analogous to the proof of Lemma \ref{lem.serre-gen.sl2} \textbf{(a)}.

\textbf{(c)} By assumption, $\dim V<\infty$. Now, the endomorphism $h\mid_{V}$
of $V$ has at most $\dim V$ distinct eigenvalues (since an endomorphism of any
finite-dimensional vector space $W$ has at most $\dim W$ distinct
eigenvalues). From this, it is easy to conclude that $f^{\dim V}%
x=0$\ \ \ \ \footnote{\textit{Proof.} Assume the opposite. Then, $f^{\dim
V}x\neq0$.
\par
Now, let $m\in\left\{  0,1,...,\dim V\right\}  $ be arbitrary. We will prove
that $\lambda-2m$ is an eigenvalue of $h\mid_{V}$.
\par
In fact, $m\leq\dim V$, so that $f^{\dim V-m}\left(  f^{m}x\right)  =f^{\dim
V-m+m}x=f^{\dim V}x\neq0$ and thus $f^{m}x\neq0$. Since $hf^{m}x=\left(
\lambda-2m\right)  f^{m}x$ (by (\ref{pf.serre-gen.sl2.1})), this yields that
$f^{m}x$ is a nonzero eigenvector of $h\mid_{V}$ with eigenvalue $\lambda-2m$.
Thus, $\lambda-2m$ is an eigenvalue of $h\mid_{V}$.
\par
Now forget that we fixed $m$. Thus, we have proven that $\lambda-2m$ is an
eigenvalue of $h\mid_{V}$ for every $m\in\left\{  0,1,...,\dim V\right\}  $.
Thus we have found $\dim V+1$ pairwise distinct eigenvalues of $h\mid_{V}$.
This contradicts the fact that $h\mid_{V}$ has at most $\dim V$ distinct
eigenvalues. This contradiction shows that our assumption was wrong, qed.}.
Thus, there exists a smallest $m\in\mathbb{N}$ satisfying $f^{m}x=0$. Denote
this $m$ by $u$. Then, $f^{u}x=0$. Since $f^{0}x=x\neq0$, this $u$ is $\neq0$,
so that $f^{u-1}x$ is well-defined. Moreover, $f^{u-1}x\neq0$ (since $u$ is
the smallest $m\in\mathbb{N}$ satisfying $f^{m}x=0$).

Lemma \ref{lem.serre-gen.sl2} \textbf{(a)} (applied to $n=u$) yields
$e^{u}f^{u}x=u!\lambda\left(  \lambda-1\right)  ...\left(  \lambda-u+1\right)
x$. Since $e^{u}\underbrace{f^{u}x}_{=0}=0$, this rewrites as $u!\lambda
\left(  \lambda-1\right)  ...\left(  \lambda-u+1\right)  x=0$. Since
$\operatorname*{char}\mathbb{C}=0$, we can divide this equation by $u!$, and
obtain $\lambda\left(  \lambda-1\right)  ...\left(  \lambda-u+1\right)  x=0$.
Since $x\neq0$, this yields $\lambda\left(  \lambda-1\right)  ...\left(
\lambda-u+1\right)  =0$. Thus, one of the numbers $\lambda$, $\lambda-1$,
$...$, $\lambda-u+1$ must be $0$. In other words, $\lambda\in\left\{
0,1,...,u-1\right\}  $. Hence, $\lambda\in\mathbb{N}$ and $\lambda\leq u-1$.

Applying (\ref{pf.serre-gen.sl2.1}) to $m=u-1$, we obtain $hf^{u-1}x=\left(
\lambda-2\left(  u-1\right)  \right)  f^{u-1}x$. Denote $\lambda-2\left(
u-1\right)  $ by $\mu$. Then, $hf^{u-1}=\underbrace{\left(  \lambda-2\left(
u-1\right)  \right)  }_{=\mu}f^{u-1}x=\mu f^{u-1}$. Also, $ff^{u-1}x=f^{u}%
x=0$. Thus, we can apply Lemma \ref{lem.serre-gen.sl2} \textbf{(b)} to $\mu$,
$f^{u-1}x$ and $u-1$ instead of $\lambda$, $x$ and $n$. Thus, we obtain%
\[
f^{u-1}e^{u-1}f^{u-1}x=\left(  u-1\right)  !\mu\left(  \mu+1\right)
...\left(  \mu+\left(  u-1\right)  -1\right)  f^{u-1}x.
\]
But $\mu=\underbrace{\lambda}_{\leq u-1}-2\left(  u-1\right)  \leq\left(
u-1\right)  -2\left(  u-1\right)  =-\left(  u-1\right)  $, so that each of the
integers $\mu$, $\mu+1$, $...$, $\mu+\left(  u-1\right)  -1$ is nonzero. Thus,
their product $\mu\left(  \mu+1\right)  ...\left(  \mu+\left(  u-1\right)
-1\right)  $ also is $\neq0$. Combined with $\left(  u-1\right)  !\neq0$, this
yields $\left(  u-1\right)  !\mu\left(  \mu+1\right)  ...\left(  \mu+\left(
u-1\right)  -1\right)  \neq0$. Combined with $f^{u-1}x\neq0$, this yields
$\left(  u-1\right)  !\mu\left(  \mu+1\right)  ...\left(  \mu+\left(
u-1\right)  -1\right)  f^{u-1}x\neq0$. Thus,%
\[
f^{u-1}e^{u-1}f^{u-1}x=\left(  u-1\right)  !\mu\left(  \mu+1\right)
...\left(  \mu+\left(  u-1\right)  -1\right)  f^{u-1}x\neq0,
\]
so that $e^{u-1}f^{u-1}x\neq0$.

But Lemma \ref{lem.serre-gen.sl2} \textbf{(a)} (applied to $n=u-1$) yields
$e^{u-1}f^{u-1}x=\left(  u-1\right)  !\lambda\left(  \lambda-1\right)
...\left(  \lambda-\left(  u-1\right)  +1\right)  x$. Thus,
\[
\left(  u-1\right)  !\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  u-1\right)  +1\right)  x=e^{u-1}f^{u-1}x\neq0.
\]
Hence, $\lambda\left(  \lambda-1\right)  ...\left(  \lambda-\left(
u-1\right)  +1\right)  \neq0$. Hence, $\dbinom{\lambda}{u-1}=\dfrac{1}{\left(
u-1\right)  !}\underbrace{\lambda\left(  \lambda-1\right)  ...\left(
\lambda-\left(  u-1\right)  +1\right)  }_{\neq0}\neq0$, so that $u-1\leq
\lambda$ (because otherwise, we would have $\dbinom{\lambda}{u-1}=0$,
contradicting $\dbinom{\lambda}{u-1}\neq0$). Combined with $u-1\geq\lambda$,
this yields $u-1=\lambda$. Thus, $u=\lambda+1$. Hence, $f^{u}x=0$ rewrites as
$f^{\lambda+1}x=0$. This proves Lemma \ref{lem.serre-gen.sl2} \textbf{(c)}.

\subsubsection{A few lemmas on generating subspaces of Lie algebras}

We proceed with some facts about generating sets of Lie algebras (free or not):

\begin{lemma}
\label{lem.generation.1}Let $\mathfrak{g}$ be a Lie algebra, and let $T$ be a
vector subspace. Assume that $\mathfrak{g}$ is generated by $T$ as a Lie algebra.

Let $U$ be a vector subspace of $\mathfrak{g}$ such that $T\subseteq U$ and
$\left[  T,U\right]  \subseteq U$. Then, $U=\mathfrak{g}$.
\end{lemma}

Notice that Lemma \ref{lem.generation.1} is not peculiar to Lie algebras. A
similar result holds (for instance) if "Lie algebra" is replaced by
"commutative nonunital algebra" and "$\left[  T,U\right]  $" is replaced by
"$TU$".

The following proof is written merely for the sake of completeness;
intuitively, Lemma \ref{lem.generation.1} should be obvious from the
observation that all iterated Lie brackets of elements of $T$ can be written
as linear combinations of Lie brackets of the form $\left[  t_{1},\left[
t_{2},\left[  ...,\left[  t_{k-1},t_{k}\right]  \right]  \right]  \right]  $
(with $t_{1},t_{2},...,t_{k}\in T$) by applying the Jacobi identity iteratively.

\textit{Proof of Lemma \ref{lem.generation.1}.} Define a sequence $\left(
T_{n}\right)  _{n\geq1}$ of vector subspaces of $\mathfrak{g}$ recursively as
follows: Let $T_{1}=T$, and for every positive integer $n$, set $T_{n+1}%
=\left[  T,T_{n}\right]  $.

We have%
\begin{equation}
\left[  T_{i},T_{j}\right]  \subseteq T_{i+j}\ \ \ \ \ \ \ \ \ \ \text{for any
positive integers }i\text{ and }j\text{.} \label{pf.generation.1.additivity}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.generation.1.additivity}):} We will prove
(\ref{pf.generation.1.additivity}) by induction over $i$.
\par
\textit{Induction base:} For any positive integer $j$, we have $T_{j+1}%
=\left[  T,T_{j}\right]  $ (by the definition of $T_{j+1}$) and thus $\left[
\underbrace{T_{1}}_{=T},T_{j}\right]  =\left[  T,T_{j}\right]  =T_{j+1}%
=T_{1+j}$. In other words, (\ref{pf.generation.1.additivity}) holds for $j=1$.
This completes the induction base.
\par
\textit{Induction step:} Let $k$ be a positive integer. Assume that
(\ref{pf.generation.1.additivity}) is proven for $i=k$. We now will prove
(\ref{pf.generation.1.additivity}) for $i=k+1$.
\par
Since (\ref{pf.generation.1.additivity}) is proven for $i=k$, we have%
\begin{equation}
\left[  T_{k},T_{j}\right]  \subseteq T_{k+j}\ \ \ \ \ \ \ \ \ \ \text{for any
positive integer }j\text{.} \label{pf.generation.1.additivity.2}%
\end{equation}
\par
Now, let $j$ be a positive integer. Then, $T_{k+j+1}=\left[  T,T_{k+j}\right]
$ (by the definition of $T_{k+j+1}$) and $T_{j+1}=\left[  T,T_{j}\right]  $
(by the definition of $T_{j+1}$). Now, any $x\in T$, $y\in T_{k}$ and $z\in
T_{j}$ satisfy%
\begin{align*}
\left[  \left[  x,y\right]  ,z\right]   &  =-\underbrace{\left[  \left[
y,z\right]  ,x\right]  }_{=-\left[  x,\left[  y,z\right]  \right]  }-\left[
\underbrace{\left[  z,x\right]  }_{=-\left[  x,z\right]  },y\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the Jacobi identity}\right) \\
&  =\underbrace{-\left(  -\left[  x,\left[  y,z\right]  \right]  \right)
}_{=\left[  x,\left[  y,z\right]  \right]  }-\underbrace{\left[  -\left[
x,z\right]  ,y\right]  }_{=-\left[  \left[  x,z\right]  ,y\right]  =\left[
y,\left[  x,z\right]  \right]  }=\left[  \underbrace{x}_{\in T},\left[
\underbrace{y}_{\in T_{k}},\underbrace{z}_{\in T_{j}}\right]  \right]
-\left[  \underbrace{y}_{\in T_{k}},\left[  \underbrace{x}_{\in T}%
,\underbrace{z}_{\in T_{j}}\right]  \right] \\
&  \in\left[  T,\underbrace{\left[  T_{k},T_{j}\right]  }_{\substack{\subseteq
T_{k+j}\\\text{(by (\ref{pf.generation.1.additivity.2}))}}}\right]  +\left[
T_{k},\underbrace{\left[  T,T_{j}\right]  }_{=T_{j+1}}\right]  \subseteq
\underbrace{\left[  T,T_{k+j}\right]  }_{=T_{k+j+1}}+\underbrace{\left[
T_{k},T_{j+1}\right]  }_{\substack{\subseteq T_{k+j+1}\\\text{(by
(\ref{pf.generation.1.additivity.2}), applied to}\\j+1\text{ instead of
}j\text{)}}}\\
&  \subseteq T_{k+j+1}+T_{k+j+1}\subseteq T_{k+j+1}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }T_{k+j+1}\text{ is a vector space}\right) \\
&  =T_{\left(  k+1\right)  +j}.
\end{align*}
Hence, $\left[  \left[  T,T_{k}\right]  ,T_{j}\right]  \subseteq T_{\left(
k+1\right)  +j}$ (since $T_{\left(  k+1\right)  +j}$ is a vector space). Since
$\left[  T,T_{k}\right]  =T_{k+1}$ (by the definition of $T_{k+1}$), this
rewrites as $\left[  T_{k+1},T_{j}\right]  \subseteq T_{\left(  k+1\right)
+j}$. Since we have proven this for every positive integer $j$, we have thus
proven (\ref{pf.generation.1.additivity}) for $i=k+1$. The induction step is
thus complete. This finishes the proof of (\ref{pf.generation.1.additivity}).}
Now, let $S$ be the vector subspace $\sum\limits_{i\geq1}T_{i}$ of
$\mathfrak{g}$. Then, every positive integer $k$ satisfies $T_{k}\subseteq S$.
In particular, $T_{1}\subseteq S$. Since $S=\sum\limits_{i\geq1}T_{i}$ and
$S=\sum\limits_{i\geq1}T_{i}=\sum\limits_{j\geq1}T_{j}$, we have%
\[
\left[  S,S\right]  =\left[  \sum\limits_{i\geq1}T_{i},\sum\limits_{j\geq
1}T_{j}\right]  =\sum\limits_{i\geq1}\sum\limits_{j\geq1}\underbrace{\left[
T_{i},T_{j}\right]  }_{\substack{\subseteq T_{i+j}\subseteq S\\\text{(since
every positive}\\\text{integer }k\text{ satisfies }T_{k}\subseteq S\text{)}%
}}\subseteq\sum\limits_{i\geq1}\sum\limits_{j\geq1}S\subseteq S
\]
(since $S$ is a vector space). Thus, $S$ is a Lie subalgebra of $\mathfrak{g}%
$. Since $T=T_{1}\subseteq S$, this yields that $S$ is a Lie subalgebra of
$\mathfrak{g}$ containing $T$ as a subset. Since the smallest Lie subalgebra
of $\mathfrak{g}$ containing $T$ as a subset is $\mathfrak{g}$ itself (because
$\mathfrak{g}$ is generated by $T$ as a Lie algebra), this yields that
$S\supseteq\mathfrak{g}$. In other words, $S=\mathfrak{g}$.

Now, it is easy to see that%
\begin{equation}
T_{i}\subseteq U\text{ for every positive integer }i. \label{pf.generation.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.generation.2}):} We will prove
(\ref{pf.generation.2}) by induction over $i$.
\par
\textit{Induction base:} We have $T_{1}=T\subseteq U$. Thus,
(\ref{pf.generation.2}) holds for $j=1$. This completes the induction base.
\par
\textit{Induction step:} Let $k$ be a positive integer. Assume that
(\ref{pf.generation.2}) holds for $i=k$. We now will prove
(\ref{pf.generation.2}) for $i=k+1$.
\par
Since (\ref{pf.generation.2}) holds for $i=k$, we have $T_{k}\subseteq U$.
Since $T_{k+1}=\left[  T,T_{k}\right]  $ (by the definition of $T_{k+1}$), we
have $T_{k+1}=\left[  T,\underbrace{T_{k}}_{\subseteq U}\right]
\subseteq\left[  T,U\right]  \subseteq U$. In other words,
(\ref{pf.generation.2}) holds for $i=k+1$. This completes the induction step.
Thus, (\ref{pf.generation.2}) is proven.} Hence,%
\[
\mathfrak{g}=S=\sum\limits_{i\geq1}\underbrace{T_{i}}_{\subseteq U}%
\subseteq\sum\limits_{i\geq1}U\subseteq U
\]
(since $U$ is a vector space). Thus, $U=\mathfrak{g}$, and this proves Lemma
\ref{lem.generation.1}.

Next, a result on free Lie algebras:

\begin{proposition}
\label{prop.Ufree}Let $V$ be a vector space. We let $\operatorname*{FreeLie}V$
denote the free Lie algebra on the vector space $V$ (not on the set $V$), and
let $T\left(  V\right)  $ denote the tensor algebra of $V$. Then, there exists
a canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}V\right)
\rightarrow T\left(  V\right)  $, which commutes with the canonical injections
of $V$ into $U\left(  \operatorname*{FreeLie}V\right)  $ and into $T\left(
V\right)  $.
\end{proposition}

We are going to prove Proposition \ref{prop.Ufree} by combining the universal
properties of the universal enveloping algebra, the free Lie algebra, and the
tensor algebra. Let us first formulate these properties. First, the universal
property of the universal enveloping algebra:

\begin{theorem}
\label{thm.universal.U}Let $\mathfrak{g}$ be a Lie algebra. We denote by
$\iota_{\mathfrak{g}}^{U}:\mathfrak{g}\rightarrow U\left(  \mathfrak{g}%
\right)  $ the canonical map from $\mathfrak{g}$ into $U\left(  \mathfrak{g}%
\right)  $. (This map $\iota_{\mathfrak{g}}^{U}$ is injective by the
Poincar\'{e}-Birkhoff-Witt theorem, but this is not relevant to the current
theorem.) For any algebra $B$ and any Lie algebra homomorphism $f:\mathfrak{g}%
\rightarrow B$ (where the Lie algebra structure on $B$ is defined by the
commutator of the multiplication of $B$), there exists a unique algebra
homomorphism $F:U\left(  \mathfrak{g}\right)  \rightarrow B$ satisfying
$f=F\circ\iota_{\mathfrak{g}}^{U}$.
\end{theorem}

Next, the universal property of the free Lie algebra:

\begin{theorem}
\label{thm.universal.FreeLie}Let $V$ be a vector space. We denote by
$\iota_{V}^{\operatorname*{FreeLie}}:V\rightarrow\operatorname*{FreeLie}V$ the
canonical map from $V$ into $\operatorname*{FreeLie}V$. (The construction of
$\operatorname*{FreeLie}V$ readily shows that this map $\iota_{V}%
^{\operatorname*{FreeLie}}$ is injective.) For any Lie algebra $\mathfrak{h}$
and any linear map $f:V\rightarrow\mathfrak{h}$, there exists a unique Lie
algebra homomorphism $F:\operatorname*{FreeLie}V\rightarrow\mathfrak{h}$
satisfying $f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$.
\end{theorem}

Finally, the universal property of the tensor algebra:

\begin{theorem}
\label{thm.universal.tensor}Let $V$ be a vector space. We denote by $\iota
_{V}^{T}:V\rightarrow T\left(  V\right)  $ the canonical map from $V$ into
$T\left(  V\right)  $. (This map $\iota_{V}^{T}$ is known to be injective.)
For any algebra $B$ and any linear map $f:V\rightarrow B$, there exists a
unique algebra homomorphism $F:T\left(  V\right)  \rightarrow B$ satisfying
$f=F\circ\iota_{V}^{T}$.
\end{theorem}

\textit{Proof of Proposition \ref{prop.Ufree}.} The algebra $T\left(
V\right)  $ canonically becomes a Lie algebra (by defining the Lie bracket on
$T\left(  V\right)  $ as the commutator of the multiplication). Similarly, the
algebra $U\left(  \operatorname*{FreeLie}V\right)  $ becomes a Lie algebra.

Applying Theorem \ref{thm.universal.FreeLie} to $\mathfrak{h}=T\left(
V\right)  $ and $f=\iota_{V}^{T}$, we obtain that there exists a unique Lie
algebra homomorphism $F:\operatorname*{FreeLie}V\rightarrow T\left(  V\right)
$ satisfying $\iota_{V}^{T}=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. Denote
this Lie algebra homomorphism $F$ by $h$. Then, $h:\operatorname*{FreeLie}%
V\rightarrow T\left(  V\right)  $ is a Lie algebra homomorphism satisfying
$\iota_{V}^{T}=h\circ\iota_{V}^{\operatorname*{FreeLie}}$.

Applying Theorem \ref{thm.universal.U} to $\mathfrak{g}%
=\operatorname*{FreeLie}V$, $B=T\left(  V\right)  $ and $f=h$, we obtain that
there exists a unique algebra homomorphism $F:U\left(  \operatorname*{FreeLie}%
V\right)  \rightarrow T\left(  V\right)  $ satisfying $h=F\circ\iota
_{\operatorname*{FreeLie}V}^{U}$. Denote this algebra homomorphism $F$ by
$\alpha$. Then, $\alpha:U\left(  \operatorname*{FreeLie}V\right)  \rightarrow
T\left(  V\right)  $ is an algebra homomorphism satisfying $h=\alpha\circ
\iota_{\operatorname*{FreeLie}V}^{U}$.

Applying Theorem \ref{thm.universal.tensor} to $B=U\left(
\operatorname*{FreeLie}V\right)  $ and $f=\iota_{\operatorname*{FreeLie}V}%
^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}$, we obtain that there exists a
unique algebra homomorphism $F:T\left(  V\right)  \rightarrow U\left(
\operatorname*{FreeLie}V\right)  $ satisfying $\iota_{\operatorname*{FreeLie}%
V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}=F\circ\iota_{V}^{T}$. Denote
this algebra homomorphism $F$ by $\beta$. Then, $\beta:T\left(  V\right)
\rightarrow U\left(  \operatorname*{FreeLie}V\right)  $ is an algebra
homomorphism satisfying $\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota
_{V}^{\operatorname*{FreeLie}}=\beta\circ\iota_{V}^{T}$.

Both $\alpha$ and $\beta$ are algebra homomorphisms, and therefore Lie algebra
homomorphisms. Also, $\iota_{\operatorname*{FreeLie}V}^{U}$ is a Lie algebra homomorphism.

We have%
\[
\beta\circ\underbrace{\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}}%
_{=h}\circ\iota_{V}^{\operatorname*{FreeLie}}=\beta\circ\underbrace{h\circ
\iota_{V}^{\operatorname*{FreeLie}}}_{=\iota_{V}^{T}}=\beta\circ\iota_{V}%
^{T}=\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}%
^{\operatorname*{FreeLie}}%
\]
and%
\[
\alpha\circ\underbrace{\beta\circ\iota_{V}^{T}}_{=\iota
_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
}=\underbrace{\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}}_{=h}\circ
\iota_{V}^{\operatorname*{FreeLie}}=h\circ\iota_{V}^{\operatorname*{FreeLie}%
}=\iota_{V}^{T}.
\]


Now, applying Theorem \ref{thm.universal.FreeLie} to $\mathfrak{h}=U\left(
\operatorname*{FreeLie}V\right)  $ and $f=\iota_{\operatorname*{FreeLie}V}%
^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}$, we obtain that there exists a
unique Lie algebra homomorphism $F:\operatorname*{FreeLie}V\rightarrow
U\left(  \operatorname*{FreeLie}V\right)  $ satisfying $\iota
_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
=F\circ\iota_{V}^{\operatorname*{FreeLie}}$. Thus, any two Lie algebra
homomorphisms $F:\operatorname*{FreeLie}V\rightarrow U\left(
\operatorname*{FreeLie}V\right)  $ satisfying $\iota_{\operatorname*{FreeLie}%
V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}=F\circ\iota_{V}%
^{\operatorname*{FreeLie}}$ must be equal. Since $\beta\circ\alpha\circ
\iota_{\operatorname*{FreeLie}V}^{U}$ and $\iota_{\operatorname*{FreeLie}%
V}^{U}$ are two such Lie algebra homomorphisms (because we know that
$\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota
_{V}^{\operatorname*{FreeLie}}=\iota_{\operatorname*{FreeLie}V}^{U}\circ
\iota_{V}^{\operatorname*{FreeLie}}$ and clearly $\iota
_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
=\iota_{\operatorname*{FreeLie}V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}%
$), this yields that $\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}%
V}^{U}$ and $\iota_{\operatorname*{FreeLie}V}^{U}$ must be equal. In other
words,%
\[
\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}=\iota
_{\operatorname*{FreeLie}V}^{U}.
\]


Next, applying Theorem \ref{thm.universal.U} to $\mathfrak{g}%
=\operatorname*{FreeLie}V$, $B=U\left(  \operatorname*{FreeLie}V\right)  $ and
$f=\iota_{\operatorname*{FreeLie}V}^{U}$, we obtain that there exists a unique
algebra homomorphism $F:U\left(  \operatorname*{FreeLie}V\right)  \rightarrow
U\left(  \operatorname*{FreeLie}V\right)  $ satisfying $\iota
_{\operatorname*{FreeLie}V}^{U}=F\circ\iota_{\operatorname*{FreeLie}V}^{U}$.
Thus, any two algebra homomorphisms $F:U\left(  \operatorname*{FreeLie}%
V\right)  \rightarrow U\left(  \operatorname*{FreeLie}V\right)  $ satisfying
$\iota_{\operatorname*{FreeLie}V}^{U}=F\circ\iota_{\operatorname*{FreeLie}%
V}^{U}$ must be equal. Since $\beta\circ\alpha$ and $\operatorname*{id}%
\nolimits_{U\left(  \operatorname*{FreeLie}V\right)  }$ are two such algebra
homomorphisms (because $\beta\circ\alpha\circ\iota_{\operatorname*{FreeLie}%
V}^{U}=\iota_{\operatorname*{FreeLie}V}^{U}$ and $\operatorname*{id}%
\nolimits_{U\left(  \operatorname*{FreeLie}V\right)  }\circ\iota
_{\operatorname*{FreeLie}V}^{U}=\iota_{\operatorname*{FreeLie}V}^{U}$), this
yields that $\beta\circ\alpha$ and $\operatorname*{id}\nolimits_{U\left(
\operatorname*{FreeLie}V\right)  }$ must be equal. Thus,%
\[
\beta\circ\alpha=\operatorname*{id}\nolimits_{U\left(  \operatorname*{FreeLie}%
V\right)  }.
\]


On the other hand, applying Theorem \ref{thm.universal.tensor} to $B=T\left(
V\right)  $ and $f=\iota_{V}^{T}$, we obtain that there exists a unique
algebra homomorphism $F:T\left(  V\right)  \rightarrow T\left(  V\right)  $
satisfying $\iota_{V}^{T}=F\circ\iota_{V}^{T}$. Therefore, any two algebra
homomorphisms $F:T\left(  V\right)  \rightarrow T\left(  V\right)  $
satisfying $\iota_{V}^{T}=F\circ\iota_{V}^{T}$ must be equal. Since
$\alpha\circ\beta$ and $\operatorname*{id}\nolimits_{T\left(  V\right)  }$ are
two such algebra homomorphisms (because we know that $\alpha\circ\beta
\circ\iota_{V}^{T}=\iota_{V}^{T}$ and $\operatorname*{id}\nolimits_{T\left(
V\right)  }\circ\iota_{V}^{T}=\iota_{V}^{T}$), this yields that $\alpha
\circ\beta$ and $\operatorname*{id}\nolimits_{T\left(  V\right)  }$ must be
equal. In other words, $\alpha\circ\beta=\operatorname*{id}\nolimits_{T\left(
V\right)  }$. Combined with $\beta\circ\alpha=\operatorname*{id}%
\nolimits_{U\left(  \operatorname*{FreeLie}V\right)  }$, this yields that
$\alpha$ and $\beta$ are mutually inverse, and thus $\alpha$ and $\beta$ are
algebra isomorphisms. Hence, $\alpha:U\left(  \operatorname*{FreeLie}V\right)
\rightarrow T\left(  V\right)  $ is a canonical algebra isomorphism. Also,
$\alpha$ commutes with the canonical injections of $V$ into $U\left(
\operatorname*{FreeLie}V\right)  $ and into $T\left(  V\right)  $, because%
\[
\underbrace{\alpha\circ\iota_{\operatorname*{FreeLie}V}^{U}}_{=h}\circ
\iota_{V}^{\operatorname*{FreeLie}}=h\circ\iota_{V}^{\operatorname*{FreeLie}%
}=\iota_{V}^{T}.
\]
Hence, there exists a canonical algebra isomorphism $U\left(
\operatorname*{FreeLie}V\right)  \rightarrow T\left(  V\right)  $, which
commutes with the canonical injections of $V$ into $U\left(
\operatorname*{FreeLie}V\right)  $ and into $T\left(  V\right)  $ (namely,
$\alpha$). Proposition \ref{prop.Ufree} is proven.

There is a special version of Proposition \ref{prop.Ufree} available for
graded vector spaces:

\begin{proposition}
\label{prop.Ufree.gr}Let $V$ be a graded vector space. Let us use the
notations of Proposition \ref{prop.Ufree}. Then, the canonical algebra
isomorphism $U\left(  \operatorname*{FreeLie}V\right)  \rightarrow T\left(
V\right)  $ constructed in Proposition \ref{prop.Ufree} is an isomorphism of
\textbf{graded} algebras.
\end{proposition}

One way to prove Proposition \ref{prop.Ufree.gr} is to scatter the word
"graded" across the proof of Proposition \ref{prop.Ufree}; of course, we would
need the graded analogues of Theorems \ref{thm.universal.tensor},
\ref{thm.universal.U} and \ref{thm.universal.FreeLie} for this to work. Here
is a slightly different way, which will require only the graded version of
Theorem \ref{thm.universal.tensor}:

\begin{theorem}
\label{thm.universal.tensor.gr}Let $Q$ be an abelian group. Let $V$ be a
$Q$-graded vector space. We denote by $\iota_{V}^{T}:V\rightarrow T\left(
V\right)  $ the canonical map from $V$ into $T\left(  V\right)  $. (This map
$\iota_{V}^{T}$ is known to be injective.) Let $B$ be any algebra, and
$f:V\rightarrow B$ be any $Q$-graded linear map. According to Theorem
\ref{thm.universal.tensor}, there exists a unique algebra homomorphism
$F:T\left(  V\right)  \rightarrow B$ satisfying $f=F\circ\iota_{V}^{T}$. This
homomorphism $F$ is $Q$-graded.
\end{theorem}

\begin{vershort}
This is an easy and well-known result and is left to the reader.
\end{vershort}

\begin{verlong}
\textit{Proof of Theorem \ref{thm.universal.tensor.gr}.} Consider the unique
algebra homomorphism $F:T\left(  V\right)  \rightarrow B$ satisfying
$f=F\circ\iota_{V}^{T}$. We need to show that this $F$ is $Q$-graded.

For every $n\in\mathbb{N}$ and every $\left(  q_{1},q_{2},...,q_{n}\right)
\in Q^{n}$ and every $w\in\left(  V\left[  q_{1}\right]  \right)
\otimes\left(  V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(
V\left[  q_{n}\right]  \right)  $, we have%
\begin{equation}
F\left(  w\right)  \in B\left[  q_{1}+q_{2}+...+q_{n}\right]  .
\label{pf.universal.tensor.gr.1}%
\end{equation}


\textit{Proof of (\ref{pf.universal.tensor.gr.1}):} We will treat the map
$\iota_{V}^{T}$ as an inclusion map, so that $\iota_{V}^{T}\left(  x\right)
=x$ for every $x\in V$.

We need to prove the relation (\ref{pf.universal.tensor.der.gr.1}) for all
$w\in\left(  V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[
q_{2}\right]  \right)  \otimes...\otimes\left(  V\left[  q_{n}\right]
\right)  $. In order to achieve this, it is enough to prove the relation
(\ref{pf.universal.tensor.der.gr.1}) for all pure tensors $w\in\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)  $ (because every
tensor is a linear combination of pure tensors, but the relation
(\ref{pf.universal.tensor.gr.1}) is linear in $w$). Thus, we can assume WLOG
that $w$ is a pure tensor. Assume this. Then, there exists a $\left(
v_{1},v_{2},...,v_{n}\right)  \in\left(  V\left[  q_{1}\right]  \right)
\times\left(  V\left[  q_{2}\right]  \right)  \times...\times\left(  V\left[
q_{n}\right]  \right)  $ such that $w=v_{1}\otimes v_{2}\otimes...\otimes
v_{n}$. Consider this $\left(  v_{1},v_{2},...,v_{n}\right)  $. We have
$w=v_{1}\otimes v_{2}\otimes...\otimes v_{n}=v_{1}v_{2}...v_{n}$. On the other
hand, every $i\in\left\{  1,2,...,n\right\}  $ satisfies $v_{i}\in V\left[
q_{i}\right]  \subseteq V$, and thus $\iota_{V}^{T}\left(  v_{i}\right)
=v_{i}$ (since $\iota_{V}^{T}\left(  x\right)  =x$ for every $x\in V$), so
that $\underbrace{f}_{=F\circ\iota_{V}^{T}}\left(  v_{i}\right)  =\left(
F\circ\iota_{V}^{T}\right)  \left(  v_{i}\right)  =F\left(  \underbrace{\iota
_{V}^{T}\left(  v_{i}\right)  }_{=v_{i}}\right)  =F\left(  v_{i}\right)  $.
Thus, $\left(  f\left(  v_{1}\right)  ,f\left(  v_{2}\right)  ,...,f\left(
v_{n}\right)  \right)  =\left(  F\left(  v_{1}\right)  ,F\left(  v_{2}\right)
,...,F\left(  v_{n}\right)  \right)  $, so that $f\left(  v_{1}\right)
f\left(  v_{2}\right)  ...f\left(  v_{n}\right)  =F\left(  v_{1}\right)
F\left(  v_{2}\right)  ...F\left(  v_{n}\right)  $. Hence,%
\begin{align*}
F\left(  w\right)   &  =F\left(  v_{1}v_{2}...v_{n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }w=v_{1}v_{2}...v_{n}\right) \\
&  =F\left(  v_{1}\right)  F\left(  v_{2}\right)  ...F\left(  v_{n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }F\text{ is an algebra homomorphism}%
\right) \\
&  =f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(  v_{n}\right)  .
\end{align*}
But every $i\in\left\{  1,2,...,n\right\}  $ satisfies $f\left(  v_{i}\right)
\in B\left[  q_{i}\right]  $ (because $v_{i}\in V\left[  q_{i}\right]  $ and
since $f$ is $Q$-graded). Hence, $\left(  f\left(  v_{1}\right)  ,f\left(
v_{2}\right)  ,...,f\left(  v_{n}\right)  \right)  \in\left(  B\left[
q_{1}\right]  \right)  \times\left(  B\left[  q_{2}\right]  \right)
\times...\times\left(  B\left[  q_{n}\right]  \right)  $. Thus,
\[
f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(  v_{n}\right)
\in\left(  B\left[  q_{1}\right]  \right)  \left(  B\left[  q_{2}\right]
\right)  ...\left(  B\left[  q_{n}\right]  \right)  \subseteq B\left[
q_{1}+q_{2}+...+q_{n}\right]
\]
(since $B$ is a graded algebra). Altogether, we now have%
\[
F\left(  w\right)  =f\left(  v_{1}\right)  f\left(  v_{2}\right)  ...f\left(
v_{n}\right)  \in B\left[  q_{1}+q_{2}+...+q_{n}\right]  .
\]
This proves (\ref{pf.universal.tensor.gr.1}).

Now, let $q\in Q$. By the definition of the grading on a tensor product, we
have%
\[
V^{\otimes n}\left[  q\right]  =\bigoplus\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)
\]
for every $n\in\mathbb{N}$. On the other hand, $T\left(  V\right)
=\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}$ (where the direct sum is a
direct sum of graded vector spaces), so that%
\begin{align*}
\left(  T\left(  V\right)  \right)  \left[  q\right]   &  =\left(
\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}\right)  \left[  q\right]
=\bigoplus\limits_{n\in\mathbb{N}}\underbrace{V^{\otimes n}\left[  q\right]
}_{=\bigoplus\limits_{\substack{\left(  q_{1},q_{2},...,q_{n}\right)  \in
Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(  V\left[  q_{1}\right]  \right)
\otimes\left(  V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(
V\left[  q_{n}\right]  \right)  }\\
&  =\bigoplus\limits_{n\in\mathbb{N}}\bigoplus\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right) \\
&  =\sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(  q_{1}%
,q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(  V\left[
q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)
\end{align*}
(since direct sums are sums). Thus,%
\begin{align*}
&  F\left(  \left(  T\left(  V\right)  \right)  \left[  q\right]  \right)
=F\left(  \sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}\left(
V\left[  q_{1}\right]  \right)  \otimes\left(  V\left[  q_{2}\right]  \right)
\otimes...\otimes\left(  V\left[  q_{n}\right]  \right)  \right) \\
&  =\sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(  q_{1}%
,q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}%
}\underbrace{F\left(  \left(  V\left[  q_{1}\right]  \right)  \otimes\left(
V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(  V\left[
q_{n}\right]  \right)  \right)  }_{\substack{\subseteq B\left[  q_{1}%
+q_{2}+...+q_{n}\right]  \\\text{(since (\ref{pf.universal.tensor.gr.1})
yields that }F\left(  w\right)  \in B\left[  q_{1}+q_{2}+...+q_{n}\right]
\\\text{for every }w\in\left(  V\left[  q_{1}\right]  \right)  \otimes\left(
V\left[  q_{2}\right]  \right)  \otimes...\otimes\left(  V\left[
q_{n}\right]  \right)  \text{)}}}\\
&  \subseteq\sum\limits_{n\in\mathbb{N}}\sum\limits_{\substack{\left(
q_{1},q_{2},...,q_{n}\right)  \in Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}B\left[
\underbrace{q_{1}+q_{2}+...+q_{n}}_{=q}\right]  =\sum\limits_{n\in\mathbb{N}%
}\sum\limits_{\substack{\left(  q_{1},q_{2},...,q_{n}\right)  \in
Q^{n};\\q_{1}+q_{2}+...+q_{n}=q}}B\left[  q\right]  \subseteq B\left[
q\right]
\end{align*}
(since $B\left[  q\right]  $ is a vector space).

Now forget that we fixed $q$. We thus have shown that $F\left(  \left(
T\left(  V\right)  \right)  \left[  q\right]  \right)  \subseteq B\left[
q\right]  $. The map $F$ therefore is $Q$-graded. Theorem
\ref{thm.universal.tensor.gr} is thus proven.
\end{verlong}

\textit{Proof of Proposition \ref{prop.Ufree.gr}.} Define the maps $\alpha$
and $\beta$ as in the proof of Proposition \ref{prop.Ufree}. Then, $\beta$ is
the unique algebra homomorphism $F:T\left(  V\right)  \rightarrow U\left(
\operatorname*{FreeLie}V\right)  $ satisfying $\iota_{\operatorname*{FreeLie}%
V}^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}=F\circ\iota_{V}^{T}$. Hence,
Theorem \ref{thm.universal.tensor.gr} (applied to $B=U\left(
\operatorname*{FreeLie}V\right)  $ and $f=\iota_{\operatorname*{FreeLie}V}%
^{U}\circ\iota_{V}^{\operatorname*{FreeLie}}$) yields that this homomorphism
$\beta$ is $Q$-graded. Also, we have seen in the proof of Proposition
\ref{prop.Ufree} that $\beta$ is an algebra isomorphism, and that $\alpha$ and
$\beta$ are mutually inverse.

Since $\alpha$ and $\beta$ are mutually inverse, we have $\alpha=\beta^{-1}$,
so that $\alpha$ is the inverse of a $Q$-graded algebra isomorphism (because
$\beta$ is a $Q$-graded algebra isomorphism). Thus, $\alpha$ itself is
$Q$-graded (because any inverse of a $Q$-graded isomorphism must itself be
$Q$-graded). Since $\alpha$ is the canonical algebra isomorphism $U\left(
\operatorname*{FreeLie}V\right)  \rightarrow T\left(  V\right)  $ constructed
in Proposition \ref{prop.Ufree}, this yields that the canonical algebra
isomorphism $U\left(  \operatorname*{FreeLie}V\right)  \rightarrow T\left(
V\right)  $ constructed in Proposition \ref{prop.Ufree} is $Q$-graded.
Proposition \ref{prop.Ufree.gr} is proven.

\subsubsection{Universality of the tensor algebra with respect to derivations}

Next, let us notice that the universal property of the tensor algebra (Theorem
\ref{thm.universal.tensor}) has an analogue for derivations in lieu of algebra homomorphisms:

\begin{theorem}
\label{thm.universal.tensor.der}Let $V$ be a vector space. We denote by
$\iota_{V}^{T}:V\rightarrow T\left(  V\right)  $ the canonical map from $V$
into $T\left(  V\right)  $. (This map $\iota_{V}^{T}$ is known to be
injective.) For any $T\left(  V\right)  $-bimodule $M$ and any linear map
$f:V\rightarrow M$, there exists a unique derivation $F:T\left(  V\right)
\rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$.
\end{theorem}

It should be noticed that "derivation" means "$\mathbb{C}$-linear derivation" here.

Before we prove this theorem, let us extend its uniqueness part a bit:

\begin{proposition}
\label{prop.derivation.unique}Let $A$ be an algebra. Let $M$ be an
$A$-bimodule, and $d:A\rightarrow M$ and $e:A\rightarrow M$ two derivations.
Let $S$ be a subset of $A$ which generates $A$ as an algebra. Assume that
$d\mid_{S}=e\mid_{S}$. Then, $d=e$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.derivation.unique}.} Let $U$ be the
subset $\operatorname*{Ker}\left(  d-e\right)  $ of $A$. Clearly, $U$ is a
vector space (since $d-e$ is a linear map (since $d$ and $e$ are linear)).

It is known that any derivation $f:A\rightarrow M$ satisfies $f\left(
1\right)  =0$. Applying this to $f=d$, we get $d\left(  1\right)  =0$.
Similarly, $e\left(  1\right)  =0$. Thus, $\left(  d-e\right)  \left(
1\right)  =\underbrace{d\left(  1\right)  }_{=0}-\underbrace{e\left(
1\right)  }_{=0}=0$, so that $1\in\operatorname*{Ker}\left(  d-e\right)  \in
U$.

Now let $b\in U$ and $c\in U$. Since $b\in U=\operatorname*{Ker}\left(
d-e\right)  $, we have $\left(  d-e\right)  \left(  b\right)  =0$. Thus,
$d\left(  b\right)  -e\left(  b\right)  =\left(  d-e\right)  \left(  b\right)
=0$, so that $d\left(  b\right)  =e\left(  b\right)  $. Similarly, $d\left(
c\right)  =e\left(  c\right)  $.

Now, since $d$ is a derivation, the Leibniz formula yields $d\left(
bc\right)  =d\left(  b\right)  \cdot c+b\cdot d\left(  c\right)  $. Similarly,
$e\left(  bc\right)  =e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)  $.
Hence,%
\begin{align*}
\left(  d-e\right)  \left(  bc\right)   &  =\underbrace{d\left(  bc\right)
}_{=d\left(  b\right)  \cdot c+b\cdot d\left(  c\right)  }%
-\underbrace{e\left(  bc\right)  }_{=e\left(  b\right)  \cdot c+b\cdot
e\left(  c\right)  }=\left(  \underbrace{d\left(  b\right)  }_{=e\left(
b\right)  }\cdot c+b\cdot\underbrace{d\left(  c\right)  }_{=e\left(  c\right)
}\right)  -\left(  e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)
\right) \\
&  =\left(  e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)  \right)
-\left(  e\left(  b\right)  \cdot c+b\cdot e\left(  c\right)  \right)  =0.
\end{align*}
In other words, $bc\in\operatorname*{Ker}\left(  d-e\right)  =U$.

Now forget that we fixed $b$ and $c$. We have thus showed that any $b\in U$
and $c\in U$ satisfy $bc\in U$. Combined with the fact that $U$ is a vector
space and that $1\in U$, this yields that $U$ is a subalgebra of $A$. Since
$S\subseteq U$ (because every $s\in S$ satisfies%
\[
\left(  d-e\right)  \left(  s\right)  =\underbrace{d\left(  s\right)
}_{=\left(  d\mid_{S}\right)  \left(  s\right)  }-\underbrace{e\left(
s\right)  }_{=\left(  e\mid_{S}\right)  \left(  s\right)  }%
=\underbrace{\left(  d\mid_{S}\right)  }_{=e\mid_{S}}\left(  s\right)
-\left(  e\mid_{S}\right)  \left(  s\right)  =\left(  e\mid_{S}\right)
\left(  s\right)  -\left(  e\mid_{S}\right)  \left(  s\right)  =0
\]
and thus $s\in\operatorname*{Ker}\left(  d-e\right)  =U$), this yields that
$U$ is a subalgebra of $A$ containing $S$ as a subset. But since the smallest
subalgebra of $A$ containing $S$ as a subset is $A$ itself (because $S$
generates $A$ as an algebra), this yields that $U\supseteq A$. Hence,
$A\subseteq U=\operatorname*{Ker}\left(  d-e\right)  $, so that $d-e=0$ and
thus $d=e$. Proposition \ref{prop.derivation.unique} is proven.

\textit{Proof of Theorem \ref{thm.universal.tensor.der}.} For any
$n\in\mathbb{N}$, we can define a linear map $\Phi_{n}:V^{\otimes
n}\rightarrow M$ by the equation%
\begin{equation}
\left(
\begin{array}
[c]{r}%
\Phi_{n}\left(  v_{1}\otimes v_{2}\otimes...\otimes v_{n}\right)
=\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\\
\text{for all }v_{1},v_{2},...,v_{n}\in V
\end{array}
\right)  \label{pf.universal.tensor.der.Phin}%
\end{equation}
(by the universal property of the tensor product, since the term
$\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}$ is clearly
multilinear in $v_{1}$, $v_{2}$, $...$, $v_{n}$). Define this map $\Phi_{n}$.
Let $\Phi$ be the map $\bigoplus\limits_{n\in\mathbb{N}}\Phi_{n}%
:\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}\rightarrow M$. Then, every
$n\in\mathbb{N}$ and every $v_{1},v_{2},...,v_{n}$ satisfy%
\begin{align}
\Phi\left(  v_{1}\otimes v_{2}\otimes...\otimes v_{n}\right)   &  =\Phi
_{n}\left(  v_{1}\otimes v_{2}\otimes...\otimes v_{n}\right) \nonumber\\
&  =\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}.
\label{pf.universal.tensor.der.Phi}%
\end{align}


Since $\bigoplus\limits_{n\in\mathbb{N}}V^{\otimes n}=T\left(  V\right)  $,
the map $\Phi$ is a map from $T\left(  V\right)  $ to $M$. We will now prove
that $\Phi$ is a derivation. In fact, in order to prove this, we must show
that%
\begin{equation}
\Phi\left(  ab\right)  =\Phi\left(  a\right)  \cdot b+a\cdot\Phi\left(
b\right)  \ \ \ \ \ \ \ \ \ \ \text{for any }a\in T\left(  V\right)  \text{
and }b\in T\left(  V\right)  . \label{pf.universal.tensor.der.Phider}%
\end{equation}


\textit{Proof of (\ref{pf.universal.tensor.der.Phider}):} Every element of
$T\left(  V\right)  $ is a linear combination of elements of $V^{\otimes n}$
for various $n\in\mathbb{N}$ (because $T\left(  V\right)  =\bigoplus
\limits_{n\in\mathbb{N}}V^{\otimes n}$). Meanwhile, every element of
$V^{\otimes n}$ for any $n\in\mathbb{N}$ is a linear combination of pure
tensors. Combining these two observations, we see that every element of
$T\left(  V\right)  $ is a linear combination of pure tensors.

We need to prove the equation (\ref{pf.universal.tensor.der.Phider}) for all
$a\in T\left(  V\right)  $ and $b\in T\left(  V\right)  $. Since this equation
is linear in each of $a$ and $b$, we can WLOG assume that $a$ and $b$ are pure
tensors (since every element of $T\left(  V\right)  $ is a linear combination
of pure tensors). Assume this. Then, $a$ is a pure tensor, so that there
exists an $n\in\mathbb{N}$ and some $v_{1},v_{2},...,v_{n}\in V$ satisfying
$a=v_{1}\otimes v_{2}\otimes...\otimes v_{n}$. Consider this $n$ and these
$v_{1},v_{2},...,v_{n}$. Also, $b$ is a pure tensor, so that there exists an
$m\in\mathbb{N}$ and some $w_{1},w_{2},...,w_{m}\in V$ satisfying
$b=w_{1}\otimes w_{2}\otimes...\otimes w_{m}$. Consider this $m$ and these
$w_{1},w_{2},...,w_{m}$.

By (\ref{pf.universal.tensor.der.Phi}) (applied to $m$ and $w_{1}%
,w_{2},...,w_{m}$ instead of $n$ and $v_{1},v_{2},...,v_{n}$), we have%
\begin{align*}
\Phi\left(  w_{1}\otimes w_{2}\otimes...\otimes w_{m}\right)   &
=\sum\limits_{k=1}^{m}w_{1}\cdot w_{2}\cdot...\cdot w_{k-1}\cdot f\left(
w_{k}\right)  \cdot w_{k+1}\cdot w_{k+2}\cdot...\cdot w_{m}\\
&  =\sum\limits_{k=n+1}^{n+m}w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot
f\left(  w_{k-n}\right)  \cdot w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}%
\end{align*}
(here, we substituted $k-n$ for $k$ in the sum).

Let $\left(  u_{1},u_{2},...,u_{n+m}\right)  $ be the $\left(  n+m\right)
$-tuple $\left(  v_{1},v_{2},...,v_{n},w_{1},w_{2},...,w_{m}\right)  $. Then,
\[
u_{1}\otimes u_{2}\otimes...\otimes u_{n+m}=\underbrace{v_{1}\otimes
v_{2}\otimes...\otimes v_{n}}_{=a}\otimes\underbrace{w_{1}\otimes w_{2}%
\otimes...\otimes w_{m}}_{=b}=a\otimes b=ab.
\]


By (\ref{pf.universal.tensor.der.Phi}) (applied to $n+m$ and $u_{1}%
,u_{2},...,u_{n+m}$ instead of $n$ and $v_{1},v_{2},...,v_{n}$), we have%
\begin{align*}
&  \Phi\left(  u_{1}\otimes u_{2}\otimes...\otimes u_{n+m}\right) \\
&  =\sum\limits_{k=1}^{n+m}u_{1}\cdot u_{2}\cdot...\cdot u_{k-1}\cdot f\left(
u_{k}\right)  \cdot u_{k+1}\cdot u_{k+2}\cdot...\cdot u_{n+m}\\
&  =\sum\limits_{k=1}^{n}\underbrace{u_{1}\cdot u_{2}\cdot...\cdot
u_{k-1}\cdot f\left(  u_{k}\right)  \cdot u_{k+1}\cdot u_{k+2}\cdot...\cdot
u_{n+m}}_{\substack{=v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\cdot w_{1}\cdot
w_{2}\cdot...\cdot w_{m}\\\text{(since }\left(  u_{1},u_{2},...,u_{n+m}%
\right)  =\left(  v_{1},v_{2},...,v_{n},w_{1},w_{2},...,w_{m}\right)  \text{
and }k\leq n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k=n+1}^{n+m}\underbrace{u_{1}\cdot
u_{2}\cdot...\cdot u_{k-1}\cdot f\left(  u_{k}\right)  \cdot u_{k+1}\cdot
u_{k+2}\cdot...\cdot u_{n+m}}_{\substack{=v_{1}\cdot v_{2}\cdot...\cdot
v_{n}\cdot w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot f\left(
w_{k-n}\right)  \cdot w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}%
\\\text{(since }\left(  u_{1},u_{2},...,u_{n+m}\right)  =\left(  v_{1}%
,v_{2},...,v_{n},w_{1},w_{2},...,w_{m}\right)  \text{ and }k>n\text{)}}}\\
&  =\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\cdot
\underbrace{w_{1}\cdot w_{2}\cdot...\cdot w_{m}}_{=w_{1}\otimes w_{2}%
\otimes...\otimes w_{m}=b}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k=n+1}^{n+m}\underbrace{v_{1}\cdot
v_{2}\cdot...\cdot v_{n}}_{=v_{1}\otimes v_{2}\otimes...\otimes v_{n}=a}\cdot
w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot f\left(  w_{k-n}\right)  \cdot
w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}\\
&  =\sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot v_{k-1}\cdot f\left(
v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot v_{n}\cdot b\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k=n+1}^{n+m}a\cdot w_{1}\cdot w_{2}%
\cdot...\cdot w_{k-n-1}\cdot f\left(  w_{k-n}\right)  \cdot w_{k-n+1}\cdot
w_{k-n+2}\cdot...\cdot w_{m}\\
&  =\underbrace{\left(  \sum\limits_{k=1}^{n}v_{1}\cdot v_{2}\cdot...\cdot
v_{k-1}\cdot f\left(  v_{k}\right)  \cdot v_{k+1}\cdot v_{k+2}\cdot...\cdot
v_{n}\right)  }_{\substack{=\Phi\left(  v_{1}\otimes v_{2}\otimes...\otimes
v_{n}\right)  \\\text{(by (\ref{pf.universal.tensor.der.Phi}))}}}\cdot b\\
&  \ \ \ \ \ \ \ \ \ \ +a\cdot\underbrace{\left(  \sum\limits_{k=n+1}%
^{n+m}w_{1}\cdot w_{2}\cdot...\cdot w_{k-n-1}\cdot f\left(  w_{k-n}\right)
\cdot w_{k-n+1}\cdot w_{k-n+2}\cdot...\cdot w_{m}\right)  }_{=\Phi\left(
w_{1}\otimes w_{2}\otimes...\otimes w_{m}\right)  }\\
&  =\Phi\left(  \underbrace{v_{1}\otimes v_{2}\otimes...\otimes v_{n}}%
_{=a}\right)  \cdot b+a\cdot\Phi\left(  \underbrace{w_{1}\otimes w_{2}%
\otimes...\otimes w_{m}}_{=b}\right)  =\Phi\left(  a\right)  \cdot
b+a\cdot\Phi\left(  b\right)  .
\end{align*}
Thus, (\ref{pf.universal.tensor.der.Phider}) is proven.

Now that we know that $\Phi$ satisfies (\ref{pf.universal.tensor.der.Phider}),
we conclude that $\Phi$ is a derivation.

Next, notice that every $v\in V$ satisfies $\iota_{V}^{T}\left(  v\right)  =v$
(since $\iota_{V}^{T}$ is just the inclusion map). Hence, every $v\in V$
satisfies%
\begin{align*}
\left(  \Phi\circ\iota_{V}^{T}\right)  \left(  v\right)   &  =\Phi\left(
\underbrace{\iota_{V}^{T}\left(  v\right)  }_{\substack{=v}}\right)
=\Phi\left(  v\right) \\
&  =\sum\limits_{k=1}^{1}f\left(  v\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.universal.tensor.der.Phi}), applied to }n=1\text{ and }%
v_{1}=v\right) \\
&  =f\left(  v\right)  .
\end{align*}
Thus, $\Phi\circ\iota_{V}^{T}=f$.

So we know that $\Phi$ is a derivation satisfying $f=\Phi\circ\iota_{V}^{T}$.
Thus, we have shown that there exists a derivation $F:T\left(  V\right)
\rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$ (namely, $F=\Phi$). In order
to complete the proof of Theorem \ref{thm.universal.tensor.der}, we only need
to check that this derivation is unique. In other words, we need to check that
whenever a derivation $F:T\left(  V\right)  \rightarrow M$ satisfies
$f=F\circ\iota_{V}^{T}$, we must have $F=\Phi$. Let us prove this now. Let
$F:T\left(  V\right)  \rightarrow M$ be any derivation satisfying
$f=F\circ\iota_{V}^{T}$. Then, every $v\in V$ satisfies%
\begin{align*}
\left(  F\mid_{V}\right)  \left(  v\right)   &  =F\left(  \underbrace{v}%
_{=\iota_{V}^{T}\left(  v\right)  }\right)  =F\left(  \iota_{V}^{T}\left(
v\right)  \right)  =\underbrace{\left(  F\circ\iota_{V}^{T}\right)  }%
_{=f=\Phi\circ\iota_{V}^{T}}\left(  v\right)  =\left(  \Phi\circ\iota_{V}%
^{T}\right)  \left(  v\right) \\
&  =\Phi\left(  \underbrace{\iota_{V}^{T}\left(  v\right)  }_{=v}\right)
=\Phi\left(  v\right)  =\left(  \Phi\mid_{V}\right)  \left(  v\right)  .
\end{align*}
Thus, $F\mid_{V}=\Phi\mid_{V}$. Proposition \ref{prop.derivation.unique}
(applied to $A=T\left(  V\right)  $, $d=F$, $e=\Phi$ and $S=V$) thus yields
$F=\Phi$ (since $V$ generates $T\left(  V\right)  $ as an algebra). This
completes the proof of Theorem \ref{thm.universal.tensor.der} (as we have seen above).

We record a graded version of Theorem \ref{thm.universal.tensor.der}:

\begin{theorem}
\label{thm.universal.tensor.der.gr}Let $Q$ be an abelian group. Let $V$ be a
$Q$-graded vector space. We denote by $\iota_{V}^{T}:V\rightarrow T\left(
V\right)  $ the canonical map from $V$ into $T\left(  V\right)  $. (This map
$\iota_{V}^{T}$ is known to be injective and $Q$-graded.) For any $Q$-graded
$T\left(  V\right)  $-bimodule $M$ and any $Q$-graded linear map
$f:V\rightarrow M$, there exists a unique $Q$-graded derivation $F:T\left(
V\right)  \rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.universal.tensor.der.gr}.} We will treat the
map $\iota_{V}^{T}$ as an inclusion map, so that $\iota_{V}^{T}\left(
x\right)  =x$ for every $x\in V$.

Define the map $\Phi$ as in the proof of Theorem
\ref{thm.universal.tensor.der}. As we have seen in the proof of Theorem
\ref{thm.universal.tensor.der}, this map $\Phi$ is a derivation $T\left(
V\right)  \rightarrow M$ satisfying $f=\Phi\circ\iota_{V}^{T}$. We will now
prove that $\Phi$ is $Q$-graded.

For every $Q$-graded vector space $W$ and every $q\in Q$, let $\pi_{q}^{W}$ be
the canonical projection from $W$ to the $q$-th homogeneous component
$W\left[  q\right]  $. Of course, for every $Q$-graded vector space $W$ and
every $w\in W$, we have%
\begin{equation}
w=\sum\limits_{q\in Q}\pi_{q}^{W}\left(  w\right)  .
\label{pf.universal.tensor.der.gr.0}%
\end{equation}
Let us notice that for every $Q$-graded vector space $W$ and any two distinct
elements $p$ and $q$ of $Q$, we have%
\begin{equation}
\pi_{q}^{W}\left(  W\left[  p\right]  \right)  =0
\label{pf.universal.tensor.der.gr.0a}%
\end{equation}
(because $\pi_{q}^{W}$ is the projection of the $Q$-graded vector space $W$
onto its $q$-th homogeneous component $W\left[  q\right]  $, while $W\left[
p\right]  $ is a homogeneous component of $W$ distinct from $W\left[
q\right]  $).

For every $q\in Q$, let $\Phi_{q}$ be the linear map $\left(  T\left(
V\right)  \right)  \left[  q\right]  \rightarrow M\left[  q\right]  $ defined
by%
\[
\left(  \Phi_{q}\left(  x\right)  =\pi_{q}^{M}\left(  \Phi\left(  x\right)
\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }x\in\left(  T\left(  V\right)
\right)  \left[  q\right]  \right)  .
\]
Then, the direct sum $\bigoplus\limits_{q\in Q}\Phi_{q}:\bigoplus\limits_{q\in
Q}\left(  T\left(  V\right)  \right)  \left[  q\right]  \rightarrow
\bigoplus\limits_{q\in Q}M\left[  q\right]  $ is a $Q$-graded linear map from
$T\left(  V\right)  $ to $M$ (since $\bigoplus\limits_{q\in Q}\left(  T\left(
V\right)  \right)  \left[  q\right]  =T\left(  V\right)  $ and $\bigoplus
\limits_{q\in Q}M\left[  q\right]  =M$). Denote this map $\bigoplus
\limits_{q\in Q}\Phi_{q}$ by $\Phi^{\prime}$.

Every $r\in Q$ and every $x\in\left(  T\left(  V\right)  \right)  \left[
r\right]  $ satisfy%
\begin{align}
\Phi^{\prime}\left(  x\right)   &  =\left(  \bigoplus\limits_{q\in Q}%
\Phi\left[  q\right]  \right)  \left(  x\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Phi^{\prime}=\bigoplus\limits_{q\in Q}\Phi\left[  q\right]
\right) \nonumber\\
&  =\Phi_{r}\left(  x\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
x\in\left(  T\left(  V\right)  \right)  \left[  r\right]  \right) \nonumber\\
&  =\pi_{r}^{M}\left(  \Phi\left(  x\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi_{r}\right)  .
\label{pf.universal.tensor.der.gr.1}%
\end{align}


Now, every $r\in Q$ and every $x\in V\left[  r\right]  $ satisfy%
\begin{align*}
\left(  \Phi^{\prime}\circ\iota_{V}^{T}\right)  \left(  x\right)   &
=\Phi^{\prime}\left(  \underbrace{\iota_{V}^{T}\left(  x\right)  }%
_{=x}\right)  =\Phi^{\prime}\left(  x\right) \\
&  =\pi_{r}^{M}\left(  \Phi\left(  \underbrace{x}_{=\iota_{V}^{T}\left(
x\right)  }\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.universal.tensor.der.gr.1}) (since }x\in V\left[  r\right]
\subseteq\left(  T\left(  V\right)  \right)  \left[  r\right]  \text{)}\right)
\\
&  =\pi_{r}^{M}\left(  \underbrace{\Phi\left(  \iota_{V}^{T}\left(  x\right)
\right)  }_{=\left(  \Phi\circ\iota_{V}^{T}\right)  \left(  x\right)
}\right)  =\pi_{r}^{M}\left(  \underbrace{\left(  \Phi\circ\iota_{V}%
^{T}\right)  }_{=f}\left(  x\right)  \right)  =\pi_{r}^{M}\left(  f\left(
x\right)  \right)  =f\left(  x\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }f\left(  x\right)  \in M\left[  r\right]  \text{ (because
}f\text{ is }Q\text{-graded and }x\in V\left[  r\right]  \text{), and thus}\\
\text{the projection }\pi_{r}^{M}\text{ onto }M\left[  r\right]  \text{ leaves
}f\left(  x\right)  \text{ invariant}%
\end{array}
\right)  .
\end{align*}
Thus, $\Phi^{\prime}\circ\iota_{V}^{T}=f$.

We will next show that $\Phi^{\prime}$ is a derivation. Indeed, in order to
prove this, we must show that%
\begin{equation}
\Phi^{\prime}\left(  ab\right)  =\Phi^{\prime}\left(  a\right)  \cdot
b+a\cdot\Phi^{\prime}\left(  b\right)  \ \ \ \ \ \ \ \ \ \ \text{for any }a\in
T\left(  V\right)  \text{ and }b\in T\left(  V\right)  .
\label{pf.universal.tensor.der.gr.2}%
\end{equation}


\textit{Proof of (\ref{pf.universal.tensor.der.gr.2}):} We need to prove the
equation (\ref{pf.universal.tensor.der.gr.2}) for all $a\in T\left(  V\right)
$ and $b\in T\left(  V\right)  $. Since this equation is linear in each of $a$
and $b$, we can WLOG assume that $a$ and $b$ are homogeneous (since every
element of $T\left(  V\right)  $ is a linear combination of homogeneous
elements). Assume this. Then, $a$ is homogeneous, so there exists a $p\in Q$
such that $a\in\left(  T\left(  V\right)  \right)  \left[  p\right]  $.
Consider this $p$. Since $b$ is homogeneous, there exists an $r\in Q$ such
that $b\in\left(  T\left(  V\right)  \right)  \left[  r\right]  $. Consider
this $r$. Since $a\in\left(  T\left(  V\right)  \right)  \left[  p\right]  $
and $b\in\left(  T\left(  V\right)  \right)  \left[  r\right]  $, we have
$ab\in\left(  T\left(  V\right)  \right)  \left[  p+r\right]  $ (since
$T\left(  V\right)  $ is a $Q$-graded algebra), so that%
\begin{align*}
\Phi^{\prime}\left(  ab\right)   &  =\pi_{p+r}^{M}\left(  \underbrace{\Phi
\left(  ab\right)  }_{\substack{=\Phi\left(  a\right)  \cdot b+a\cdot
\Phi\left(  b\right)  \\\text{(since }\Phi\text{ is a derivation)}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.universal.tensor.der.gr.1}),
applied to }ab\text{ and }p+r\text{ instead of }x\text{ and }r\right) \\
&  =\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b+a\cdot\Phi\left(
b\right)  \right)  =\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)
+\pi_{p+r}^{M}\left(  a\cdot\Phi\left(  b\right)  \right)  .
\end{align*}
Now, it is easy to see that $\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot
b\right)  =\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot
b$\ \ \ \ \footnote{\textit{Proof.} Applying
(\ref{pf.universal.tensor.der.gr.0}) to $W=M$ and $w=\Phi\left(  a\right)  $,
we obtain $\Phi\left(  a\right)  =\sum\limits_{q\in Q}\pi_{q}^{M}\left(
\Phi\left(  a\right)  \right)  $, so that%
\begin{align*}
\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)   &  =\pi_{p+r}%
^{M}\left(  \sum\limits_{q\in Q}\pi_{q}^{M}\left(  \Phi\left(  a\right)
\right)  \cdot b\right)  =\sum\limits_{q\in Q}\pi_{p+r}^{M}\left(  \pi_{q}%
^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right) \\
&  =\pi_{p+r}^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)
\cdot b\right)  +\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(
\pi_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  .
\end{align*}
\par
Now, for every $q\in Q$, we have $\underbrace{\pi_{q}^{M}\left(  \Phi\left(
a\right)  \right)  }_{\substack{\in M\left[  q\right]  \\\text{(since }\pi
_{q}^{M}\text{ is a}\\\text{projection on }M\left[  q\right]  \text{)}}%
}\cdot\underbrace{b}_{\in\left(  T\left(  V\right)  \right)  \left[  r\right]
}\in\left(  M\left[  q\right]  \right)  \cdot\left(  \left(  T\left(
V\right)  \right)  \left[  r\right]  \right)  \subseteq M\left[  q+r\right]  $
(since $M$ is a graded $T\left(  V\right)  $-bimodule). For every $q\in Q$
satisfying $q+r\neq p+r$, we have $\pi_{p+r}^{W}\left(  M\left[  q+r\right]
\right)  =0$ (by (\ref{pf.universal.tensor.der.gr.0a}), applied to $M$, $p+r$
and $q+r$ instead of $W$, $q$ and $p$). Thus,%
\[
\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(  \underbrace{\pi
_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b}_{\in M\left[
q+r\right]  }\right)  \in\sum\limits_{\substack{q\in Q;\\q\neq p}%
}\underbrace{\pi_{p+r}^{M}\left(  M\left[  q+r\right]  \right)  }%
_{\substack{=0\\\text{(since }q+r\neq p+r\\\text{(because }q\neq p\text{))}%
}}=\sum\limits_{\substack{q\in Q;\\q\neq p}}0=0,
\]
so that $\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(  \pi
_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  =0$. Hence,%
\[
\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)  =\pi_{p+r}%
^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)
+\underbrace{\sum\limits_{\substack{q\in Q;\\q\neq p}}\pi_{p+r}^{M}\left(
\pi_{q}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  }_{=0}%
=\pi_{p+r}^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot
b\right)  .
\]
On the other hand, $\underbrace{\pi_{p}^{M}\left(  \Phi\left(  a\right)
\right)  }_{\substack{\in M\left[  p\right]  \\\text{(since }\pi_{p}^{M}\text{
is a}\\\text{projection on }M\left[  p\right]  \text{)}}}\cdot\underbrace{b}%
_{\in\left(  T\left(  V\right)  \right)  \left[  r\right]  }\in\left(
M\left[  p\right]  \right)  \cdot\left(  \left(  T\left(  V\right)  \right)
\left[  r\right]  \right)  \subseteq M\left[  p+r\right]  $ (since $M$ is a
graded $T\left(  V\right)  $-bimodule). Thus, $\pi_{p+r}^{M}\left(  \pi
_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)  =\pi_{p}%
^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b$ (because $\pi_{p+r}^{M}$
is a projection on $M\left[  p+r\right]  $ and thus leaves every element in
$M\left[  p+r\right]  $ fixed). Hence,%
\[
\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)  =\pi_{p+r}%
^{M}\left(  \pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b\right)
=\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  \cdot b,
\]
qed.}. Since $\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  =\Phi^{\prime
}\left(  a\right)  $ (because (\ref{pf.universal.tensor.der.gr.1}) (applied to
$a$ and $p$ instead of $x$ and $r$) yields $\Phi^{\prime}\left(  a\right)
=\pi_{p}^{M}\left(  \Phi\left(  a\right)  \right)  $), this rewrites as
$\pi_{p+r}^{M}\left(  \Phi\left(  a\right)  \cdot b\right)  =\Phi^{\prime
}\left(  a\right)  \cdot b$. The same argument (but with the right action of
$T\left(  V\right)  $ on $M$ replaced by left action) shows that $\pi
_{p+r}^{M}\left(  b\cdot\Phi\left(  a\right)  \right)  =b\cdot\Phi^{\prime
}\left(  a\right)  $. If we apply this equality to $a$, $b$, $p$ and $r$ in
lieu of $b$, $a$, $r$ and $p$, we obtain $\pi_{r+p}^{M}\left(  a\cdot
\Phi\left(  b\right)  \right)  =a\cdot\Phi^{\prime}\left(  b\right)  $. In
other words, $\pi_{p+r}^{M}\left(  a\cdot\Phi\left(  b\right)  \right)
=a\cdot\Phi^{\prime}\left(  b\right)  $. Thus,%
\begin{align*}
\Phi^{\prime}\left(  ab\right)   &  =\underbrace{\pi_{p+r}^{M}\left(
\Phi\left(  a\right)  \cdot b\right)  }_{=\Phi^{\prime}\left(  a\right)  \cdot
b}+\underbrace{\pi_{p+r}^{M}\left(  a\cdot\Phi\left(  b\right)  \right)
}_{=a\cdot\Phi^{\prime}\left(  b\right)  }\\
&  =\Phi^{\prime}\left(  a\right)  \cdot b+a\cdot\Phi^{\prime}\left(
b\right)  .
\end{align*}
This proves (\ref{pf.universal.tensor.der.gr.2}).

From (\ref{pf.universal.tensor.der.gr.2}), it becomes clear that $\Phi
^{\prime}$ is a derivation. Since $\Phi^{\prime}$ also is $Q$-graded and
satisfies $f=\Phi^{\prime}\circ\iota_{V}^{T}$, we thus conclude that there
exists a $Q$-graded derivation $F:T\left(  V\right)  \rightarrow M$ satisfying
$f=F\circ\iota_{V}^{T}$ (namely, $F=\Phi$). Combining this with the fact that
there exists \textbf{at most one} $Q$-graded derivation $F:T\left(  V\right)
\rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$%
\ \ \ \ \footnote{\textit{Proof.} Theorem \ref{thm.universal.tensor.der}
yields that there exists a unique derivation $F:T\left(  V\right)  \rightarrow
M$ satisfying $f=F\circ\iota_{V}^{T}$. Hence, there exists at most one
derivation $F:T\left(  V\right)  \rightarrow M$ satisfying $f=F\circ\iota
_{V}^{T}$. In particular, there exists at most one $Q$-graded derivation
$F:T\left(  V\right)  \rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$,
qed.}, we conclude that there exists \textbf{a unique} $Q$-graded derivation
$F:T\left(  V\right)  \rightarrow M$ satisfying $f=F\circ\iota_{V}^{T}$.
Theorem \ref{thm.universal.tensor.der.gr} is proven.

We will later use a corollary of Proposition \ref{prop.derivation.unique}:

\begin{corollary}
\label{cor.derivation.unique.ihg}Let $A$ be an algebra. Let $B$ be a
subalgebra of $A$. Let $C$ be a subalgebra of $B$. Let $d:A\rightarrow A$ be a
derivation of the algebra $A$. Let $S$ be a subset of $C$ which generates $C$
as an algebra. Assume that $d\left(  S\right)  \subseteq B$. Then, $d\left(
C\right)  \subseteq B$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.derivation.unique.ihg}.} Since $C\subseteq
B\subseteq A$, the vector spaces $A$ and $B$ become $C$-modules.

Let $\pi:A\rightarrow A\diagup B$ be the canonical projection. Clearly, $\pi$
is a $C$-module homomorphism, and satisfies $\operatorname*{Ker}\pi=B$. Let
$d^{\prime}:C\rightarrow A\diagup B$ be the restriction of the map $\pi\circ
d:A\rightarrow A\diagup B$ to $C$. It is easy to see that $d^{\prime
}:C\rightarrow A\diagup B$ is a derivation\footnote{\textit{Proof.} Every
$x\in C$ and $y\in C$ satisfy%
\begin{align*}
d^{\prime}\left(  xy\right)   &  =\left(  \pi\circ d\right)  \left(
xy\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }d^{\prime}\text{ is the
restriction of }\pi\circ d\text{ to }\mathfrak{i}\right) \\
&  =\pi\left(  \underbrace{d\left(  xy\right)  }_{\substack{=d\left(
x\right)  \cdot y+x\cdot d\left(  y\right)  \\\text{(since }d\text{ is a
derivation)}}}\right)  =\pi\left(  d\left(  x\right)  \cdot y+x\cdot d\left(
y\right)  \right) \\
&  =\underbrace{\pi\left(  d\left(  x\right)  \cdot y\right)  }%
_{\substack{=\pi\left(  d\left(  x\right)  \right)  \cdot y\\\text{(since }%
\pi\text{ is a }C\text{-module}\\\text{homomorphism)}}}+\underbrace{\pi\left(
x\cdot d\left(  y\right)  \right)  }_{\substack{=x\cdot\pi\left(  d\left(
y\right)  \right)  \\\text{(since }\pi\text{ is a }C\text{-module}%
\\\text{homomorphism)}}}\\
&  =\underbrace{\pi\left(  d\left(  x\right)  \right)  }_{\substack{=\left(
\pi\circ d\right)  \left(  x\right)  =d^{\prime}\left(  x\right)
\\\text{(since }d^{\prime}\text{ is the restriction of}\\\pi\circ d\text{ to
}C\text{, and since }x\in C\text{)}}}\cdot y+x\cdot\underbrace{\pi\left(
d\left(  y\right)  \right)  }_{\substack{=\left(  \pi\circ d\right)  \left(
y\right)  =d^{\prime}\left(  y\right)  \\\text{(since }d^{\prime}\text{ is the
restriction of}\\\pi\circ d\text{ to }C\text{, and since }y\in C\text{)}}}\\
&  =d^{\prime}\left(  x\right)  \cdot y+x\cdot d^{\prime}\left(  y\right)  .
\end{align*}
Thus, $d^{\prime}$ is a derivation, qed.}. On the other hand, $0:C\rightarrow
A\diagup B$ is a derivation as well. Every $s\in S$ satisfies%
\begin{align*}
\left(  d^{\prime}\mid_{S}\right)  \left(  s\right)   &  =d^{\prime}\left(
s\right)  =\left(  \pi\circ d\right)  \left(  s\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }d^{\prime}\text{ is the restriction
of }\pi\circ d\text{ to }C\right) \\
&  =\pi\left(  d\left(  s\right)  \right)  =0\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }d\left(  \underbrace{s}_{\in
S}\right)  \in d\left(  S\right)  \subseteq B=\operatorname*{Ker}\pi\right) \\
&  =0\left(  s\right)  =\left(  0\mid_{S}\right)  \left(  s\right)  .
\end{align*}
Thus, $d^{\prime}\mid_{S}=0\mid_{S}$. Proposition \ref{prop.derivation.unique}
(applied to $C$, $A\diagup M$, $d^{\prime}$ and $0$ instead of $A$, $M$, $d$
and $e$) therefore yields that $d^{\prime}=0$ on $C$. But since $d^{\prime}$
is the restriction of $\pi\circ d$ to $C$, we have $d^{\prime}=\left(
\pi\circ d\right)  \mid_{C}$. Thus, $\left(  \pi\circ d\right)  \mid
_{C}=d^{\prime}=0$, so that $\left(  \pi\circ d\right)  \left(  C\right)  =0$.
Thus, $\pi\left(  d\left(  C\right)  \right)  =\left(  \pi\circ d\right)
\left(  C\right)  =0$, so that $d\left(  C\right)  \subseteq
\operatorname*{Ker}\pi=B$. Corollary \ref{cor.derivation.unique.ihg} is
therefore proven.

\begin{corollary}
\label{cor.derivation.Lie.semidir}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a vector space equipped with both a Lie algebra structure
and a $\mathfrak{g}$-module structure. Assume that $\mathfrak{g}$ acts on
$\mathfrak{h}$ by derivations. Consider the semidirect product $\mathfrak{g}%
\ltimes\mathfrak{h}$ defined as in Definition \ref{def.semidir.lielie}
\textbf{(b)}. Consider $\mathfrak{g}$ as a Lie subalgebra of $\mathfrak{g}%
\ltimes\mathfrak{h}$. Consider $\mathfrak{g}\ltimes\mathfrak{h}$ as a Lie
subalgebra of $U\left(  \mathfrak{g}\ltimes\mathfrak{h}\right)  $ (where the
Lie bracket on $U\left(  \mathfrak{g}\ltimes\mathfrak{h}\right)  $ is defined
as the commutator of the multiplication). Consider $\mathfrak{h}$ as a Lie
subalgebra of $\mathfrak{g}\ltimes\mathfrak{h}$, whence $U\left(
\mathfrak{h}\right)  $ becomes a subalgebra of $U\left(  \mathfrak{g}%
\ltimes\mathfrak{h}\right)  $.

Then, $\left[  \mathfrak{g},U\left(  \mathfrak{h}\right)  \right]  \subseteq
U\left(  \mathfrak{h}\right)  $ (as subsets of $U\left(  \mathfrak{g}%
\ltimes\mathfrak{h}\right)  $).
\end{corollary}

\textit{Proof of Corollary \ref{cor.derivation.Lie.semidir}.} Let
$x\in\mathfrak{g}$. Define a map $\xi:U\left(  \mathfrak{g}\ltimes
\mathfrak{h}\right)  \rightarrow\left(  \mathfrak{g}\ltimes\mathfrak{h}%
\right)  $ by%
\[
\left(  \xi\left(  y\right)  =\left[  x,y\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }y\in U\left(  \mathfrak{g}\ltimes
\mathfrak{h}\right)  \right)  .
\]
Then, $\xi$ is clearly a derivation of the algebra $U\left(  \mathfrak{g}%
\ltimes\mathfrak{h}\right)  $.

We are identifying $\mathfrak{g}$ with a Lie subalgebra of $\mathfrak{g}%
\ltimes\mathfrak{h}$. Clearly, $x\in\mathfrak{g}$ corresponds to $\left(
x,0\right)  \in\mathfrak{g}\ltimes\mathfrak{h}$ under this identification.

We are also identifying $\mathfrak{h}$ with a Lie subalgebra of $\mathfrak{g}%
\ltimes\mathfrak{h}$. Every $y\in\mathfrak{h}$ corresponds to $\left(
0,y\right)  \in\mathfrak{g}\ltimes\mathfrak{h}$ under this identification.

Thus, every $y\in\mathfrak{h}$ satisfies%
\begin{align*}
\left[  \underbrace{x}_{=\left(  x,0\right)  },\underbrace{y}_{=\left(
0,y\right)  }\right]   &  =\left[  \left(  x,0\right)  ,\left(  0,y\right)
\right]  =\left(  \underbrace{\left[  x,0\right]  }_{=0},\underbrace{\left[
0,y\right]  }_{=0}+x\rightharpoonup y-\underbrace{0\rightharpoonup0}%
_{=0}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the Lie bracket on
}\mathfrak{g}\ltimes\mathfrak{h}\right) \\
&  =\left(  0,x\rightharpoonup y\right)  =x\rightharpoonup y\in\mathfrak{h}.
\end{align*}
Hence, $\xi\left(  y\right)  =\left[  x,y\right]  \in\mathfrak{h}$ for every
$y\in\mathfrak{h}$. Thus, $\xi\left(  \mathfrak{h}\right)  \subseteq
\mathfrak{h}\subseteq U\left(  \mathfrak{h}\right)  $.

Now, we notice that the subset $\mathfrak{h}$ of $U\left(  \mathfrak{h}%
\right)  $ generates $U\left(  \mathfrak{h}\right)  $ as an algebra. Thus,
Corollary \ref{cor.derivation.unique.ihg} (applied to $A=U\left(
\mathfrak{g}\ltimes\mathfrak{h}\right)  $, $B=U\left(  \mathfrak{h}\right)  $,
$C=U\left(  \mathfrak{h}\right)  $, $d=\xi$ and $S=\mathfrak{h}$) yields
$\xi\left(  U\left(  \mathfrak{h}\right)  \right)  \subseteq U\left(
\mathfrak{h}\right)  $. Hence, every $u\in U\left(  \mathfrak{h}\right)  $
satisfies $\xi\left(  u\right)  \in U\left(  \mathfrak{h}\right)  $. But since
$\xi\left(  u\right)  =\left[  x,u\right]  $ (by the definition of $\xi$),
this yields that every $u\in U\left(  \mathfrak{h}\right)  $ satisfies
$\left[  x,u\right]  \in U\left(  \mathfrak{h}\right)  $.

Now forget that we fixed $x$. We thus have shown that every $x\in\mathfrak{g}$
and every $u\in U\left(  \mathfrak{h}\right)  $ satisfy $\left[  x,u\right]
\in U\left(  \mathfrak{h}\right)  $. Thus, $\left[  \mathfrak{g},U\left(
\mathfrak{h}\right)  \right]  \subseteq U\left(  \mathfrak{h}\right)  $ (since
$U\left(  \mathfrak{h}\right)  $ is a vector space). This proves Corollary
\ref{cor.derivation.Lie.semidir}.

\subsubsection{Universality of the free Lie algebra with respect to
derivations}

Let us notice that Theorem \ref{thm.universal.tensor.der} has an analogue for
Lie algebras (notice that the analogue of derivations in the Lie-algebraic
setting are $1$-cocycles):

\begin{theorem}
\label{thm.universal.FreeLie.der}Let $V$ be a vector space. We denote by
$\iota_{V}^{\operatorname*{FreeLie}}:V\rightarrow\operatorname*{FreeLie}V$ the
canonical map from $V$ into $\operatorname*{FreeLie}V$. (This map $\iota
_{V}^{\operatorname*{FreeLie}}$ is easily seen to be injective.) For any
$\operatorname*{FreeLie}V$-module $M$ and any linear map $f:V\rightarrow M$,
there exists a unique $1$-cocycle $F:\operatorname*{FreeLie}V\rightarrow M$
satisfying $f=F\circ\iota_{V}^{\operatorname*{FreeLie}}$.
\end{theorem}

This cannot be proven as directly as we proved Theorem
\ref{thm.universal.tensor.der}. Instead, a way to prove Theorem
\ref{thm.universal.FreeLie.der} is by noticing that if $\mathfrak{g}$ is a Lie
algebra and $M$ is a $\mathfrak{g}$-module, then a linear map $\varphi
:\mathfrak{g}\rightarrow M$ is a $1$-cocycle if and only if the map%
\[
\mathfrak{g}\rightarrow\mathfrak{g}\ltimes M,\ \ \ \ \ \ \ \ \ \ x\mapsto
\left(  x,\varphi\left(  x\right)  \right)
\]
is a Lie algebra homomorphism. (This helps reducing Theorem
\ref{thm.universal.FreeLie.der} to Theorem \ref{thm.universal.FreeLie}.) We
leave the details of this proof to the reader.

An alternative way to prove Theorem \ref{thm.universal.FreeLie.der} is the
following: Apply Theorem \ref{thm.universal.tensor.der} to construct a
derivation $F:T\left(  V\right)  \rightarrow M$ (of algebras) satisfying
$f=F\circ\iota_{V}^{T}$, and then identify $\operatorname*{FreeLie}V$ with a
Lie subalgebra of $T\left(  V\right)  $ (because Proposition \ref{prop.Ufree}
$U\left(  \operatorname*{FreeLie}V\right)  \cong T\left(  V\right)  $, and
because the Poincar\'{e}-Birkhoff-Witt theorem entails an injection
$\operatorname*{FreeLie}V\rightarrow U\left(  \operatorname*{FreeLie}V\right)
$). Then, restricting the derivation $F:T\left(  V\right)  \rightarrow M$ to
$\operatorname*{FreeLie}V$, we obtain a $1$-cocycle $\operatorname*{FreeLie}%
V\rightarrow M$ with the required properties. The uniqueness part of Theorem
\ref{thm.universal.FreeLie.der} is easy (and follows from Proposition
\ref{prop.derivation.Lie.unique} below). This proof of Theorem
\ref{thm.universal.FreeLie.der} has the disadvantage that it makes use of the
Poincar\'{e}-Birkhoff-Witt theorem, which does not generalize to the case of
Lie algebras over rings (whereas Theorem \ref{thm.universal.FreeLie.der} does
generalize to this case).

We won't directly use Theorem \ref{thm.universal.FreeLie.der}, but we will use
the following analogue of Proposition \ref{prop.derivation.unique} for Lie algebras:

\begin{proposition}
\label{prop.derivation.Lie.unique}Let $\mathfrak{g}$ be a Lie algebra. Let $M$
be a $\mathfrak{g}$-module, and $d:\mathfrak{g}\rightarrow M$ and
$e:\mathfrak{g}\rightarrow M$ two $1$-cocycles. Let $S$ be a subset of
$\mathfrak{g}$ which generates $\mathfrak{g}$ as a Lie algebra. Assume that
$d\mid_{S}=e\mid_{S}$. Then, $d=e$.
\end{proposition}

\begin{vershort}
The proof of Proposition \ref{prop.derivation.Lie.unique} is analogous to that
of Proposition \ref{prop.derivation.unique}.
\end{vershort}

\begin{verlong}
The proof of Proposition \ref{prop.derivation.Lie.unique} is analogous to that
of Proposition \ref{prop.derivation.unique}. Here are its details:

\textit{Proof of Proposition \ref{prop.derivation.Lie.unique}.} Let $U$ be the
subset $\operatorname*{Ker}\left(  d-e\right)  $ of $\mathfrak{g}$. Clearly,
$U$ is a vector space (since $d-e$ is a linear map (since $d$ and $e$ are linear)).

Let $b\in U$ and $c\in U$. Since $b\in U=\operatorname*{Ker}\left(
d-e\right)  $, we have $\left(  d-e\right)  \left(  b\right)  =0$. Thus,
$d\left(  b\right)  -e\left(  b\right)  =\left(  d-e\right)  \left(  b\right)
=0$, so that $d\left(  b\right)  =e\left(  b\right)  $. Similarly, $d\left(
c\right)  =e\left(  c\right)  $.

Now, since $d$ is a $1$-cocycle, we have $d\left(  \left[  b,c\right]
\right)  =\left[  d\left(  b\right)  ,c\right]  +\left[  b,d\left(  c\right)
\right]  $ (by the definition of $1$-cocycles). Similarly, $e\left(  \left[
b,c\right]  \right)  =\left[  e\left(  b\right)  ,c\right]  +\left[
b,e\left(  c\right)  \right]  $. Hence,%
\begin{align*}
\left(  d-e\right)  \left(  \left[  b,c\right]  \right)   &
=\underbrace{d\left(  \left[  b,c\right]  \right)  }_{=\left[  d\left(
b\right)  ,c\right]  +\left[  b,d\left(  c\right)  \right]  }%
-\underbrace{e\left(  \left[  b,c\right]  \right)  }_{=\left[  e\left(
b\right)  ,c\right]  +\left[  b,e\left(  c\right)  \right]  }=\left(  \left[
\underbrace{d\left(  b\right)  }_{=e\left(  b\right)  },c\right]  +\left[
b,\underbrace{d\left(  c\right)  }_{=e\left(  c\right)  }\right]  \right)
-\left(  \left[  e\left(  b\right)  ,c\right]  +\left[  b,e\left(  c\right)
\right]  \right) \\
&  =\left(  \left[  e\left(  b\right)  ,c\right]  +\left[  b,e\left(
c\right)  \right]  \right)  -\left(  \left[  e\left(  b\right)  ,c\right]
+\left[  b,e\left(  c\right)  \right]  \right)  =0.
\end{align*}
In other words, $\left[  b,c\right]  \in\operatorname*{Ker}\left(  d-e\right)
=U$.

Now forget that we fixed $b$ and $c$. We have thus showed that any $b\in U$
and $c\in U$ satisfy $\left[  b,c\right]  \in U$. Combined with the fact that
$U$ is a vector space, this yields that $U$ is a Lie subalgebra of
$\mathfrak{g}$. Since $S\subseteq U$ (because every $s\in S$ satisfies%
\[
\left(  d-e\right)  \left(  s\right)  =\underbrace{d\left(  s\right)
}_{=\left(  d\mid_{S}\right)  \left(  s\right)  }-\underbrace{e\left(
s\right)  }_{=\left(  e\mid_{S}\right)  \left(  s\right)  }%
=\underbrace{\left(  d\mid_{S}\right)  }_{=e\mid_{S}}\left(  s\right)
-\left(  e\mid_{S}\right)  \left(  s\right)  =\left(  e\mid_{S}\right)
\left(  s\right)  -\left(  e\mid_{S}\right)  \left(  s\right)  =0
\]
and thus $s\in\operatorname*{Ker}\left(  d-e\right)  =U$), this yields that
$U$ is a Lie subalgebra of $\mathfrak{g}$ containing $S$ as a subset. But
since the smallest Lie subalgebra of $\mathfrak{g}$ containing $S$ as a subset
is $\mathfrak{g}$ itself (because $S$ generates $\mathfrak{g}$ as a Lie
algebra), this yields that $U\supseteq\mathfrak{g}$. Hence, $\mathfrak{g}%
\subseteq U=\operatorname*{Ker}\left(  d-e\right)  $, so that $d-e=0$ and thus
$d=e$. Proposition \ref{prop.derivation.Lie.unique} is proven.
\end{verlong}

An analogue of Theorem \ref{thm.universal.tensor.der.gr} for Lie algebras can
also be given, and is left to the reader.

We record a corollary of Proposition \ref{prop.derivation.Lie.unique}:

\begin{corollary}
\label{cor.derivation.Lie.unique.ihg}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a Lie subalgebra of $\mathfrak{g}$. Let $\mathfrak{i}$ be a
Lie subalgebra of $\mathfrak{h}$. Let $d:\mathfrak{g}\rightarrow\mathfrak{g}$
be a derivation of the Lie algebra $\mathfrak{g}$. Let $S$ be a subset of
$\mathfrak{i}$ which generates $\mathfrak{i}$ as a Lie algebra. Assume that
$d\left(  S\right)  \subseteq\mathfrak{h}$. Then, $d\left(  \mathfrak{i}%
\right)  \subseteq\mathfrak{h}$.
\end{corollary}

\begin{vershort}
This corollary is analogous to Corollary \ref{cor.derivation.unique.ihg}, and
proven accordingly.
\end{vershort}

\begin{verlong}
This corollary is analogous to Corollary \ref{cor.derivation.unique.ihg}, and
proven accordingly:

\textit{Proof of Corollary \ref{cor.derivation.Lie.unique.ihg}.} We regard
$\mathfrak{g}$ as a $\mathfrak{g}$-module by means of the adjoint action.
Since $\mathfrak{i}\subseteq\mathfrak{h}\subseteq\mathfrak{g}$, the
$\mathfrak{g}$-module $\mathfrak{g}$ thus becomes an $\mathfrak{i}$-module.

We also regard $\mathfrak{h}$ as an $\mathfrak{h}$-module by means of the
adjoint action. Since $\mathfrak{i}\subseteq\mathfrak{h}$, the $\mathfrak{h}%
$-module $\mathfrak{h}$ thus becomes an $\mathfrak{i}$-module.

Let $\pi:\mathfrak{g}\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ be the
canonical projection. Clearly, $\pi$ is an $\mathfrak{i}$-module homomorphism,
and satisfies $\operatorname*{Ker}\pi=\mathfrak{h}$. Let $d^{\prime
}:\mathfrak{i}\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ be the restriction
of the map $\pi\circ d:\mathfrak{g}\rightarrow\mathfrak{g}\diagup\mathfrak{h}$
to $\mathfrak{i}$. It is easy to see that $d^{\prime}:\mathfrak{i}%
\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ is a $1$%
-cocycle\footnote{\textit{Proof.} Every $x\in\mathfrak{i}$ and $y\in
\mathfrak{i}$ satisfy%
\begin{align*}
d^{\prime}\left(  \left[  x,y\right]  \right)   &  =\left(  \pi\circ d\right)
\left(  \left[  x,y\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}d^{\prime}\text{ is the restriction of }\pi\circ d\text{ to }\mathfrak{i}%
\right) \\
&  =\pi\left(  \underbrace{d\left(  \left[  x,y\right]  \right)
}_{\substack{=\left[  d\left(  x\right)  ,y\right]  +\left[  x,d\left(
y\right)  \right]  \\\text{(since }d\text{ is a derivation)}}}\right)
=\pi\left(  \underbrace{\left[  d\left(  x\right)  ,y\right]  }_{=-\left[
y,d\left(  x\right)  \right]  }+\left[  x,d\left(  y\right)  \right]  \right)
\\
&  =\pi\left(  -\underbrace{\left[  y,d\left(  x\right)  \right]
}_{\substack{=y\rightharpoonup\left(  d\left(  x\right)  \right)
\\\text{(since }\mathfrak{g}\text{ is a }\mathfrak{g}\text{-module}\\\text{by
the adjoint action)}}}+\underbrace{\left[  x,d\left(  y\right)  \right]
}_{\substack{=x\rightharpoonup\left(  d\left(  y\right)  \right)
\\\text{(since }\mathfrak{g}\text{ is a }\mathfrak{g}\text{-module}\\\text{by
the adjoint action)}}}\right) \\
&  =\pi\left(  -y\rightharpoonup\left(  d\left(  x\right)  \right)
+x\rightharpoonup\left(  d\left(  y\right)  \right)  \right) \\
&  =-\underbrace{\pi\left(  y\rightharpoonup\left(  d\left(  x\right)
\right)  \right)  }_{\substack{=y\rightharpoonup\left(  \pi\left(  d\left(
x\right)  \right)  \right)  \\\text{(since }\pi\text{ is an }\mathfrak{i}%
\text{-module homomorphism)}}}+\underbrace{\pi\left(  x\rightharpoonup\left(
d\left(  y\right)  \right)  \right)  }_{\substack{=x\rightharpoonup\left(
\pi\left(  d\left(  y\right)  \right)  \right)  \\\text{(since }\pi\text{ is
an }\mathfrak{i}\text{-module homomorphism)}}}\\
&  =-y\rightharpoonup\left(  \underbrace{\pi\left(  d\left(  x\right)
\right)  }_{\substack{=\left(  \pi\circ d\right)  \left(  x\right)
=d^{\prime}\left(  x\right)  \\\text{(since }d^{\prime}\text{ is the
restriction of}\\\pi\circ d\text{ to }\mathfrak{i}\text{, and since }%
x\in\mathfrak{i}\text{)}}}\right)  +x\rightharpoonup\left(  \underbrace{\pi
\left(  d\left(  y\right)  \right)  }_{\substack{=\left(  \pi\circ d\right)
\left(  y\right)  =d^{\prime}\left(  y\right)  \\\text{(since }d^{\prime
}\text{ is the restriction of}\\\pi\circ d\text{ to }\mathfrak{i}\text{, and
since }y\in\mathfrak{i}\text{)}}}\right) \\
&  =-y\rightharpoonup\left(  d^{\prime}\left(  x\right)  \right)
+x\rightharpoonup\left(  d^{\prime}\left(  y\right)  \right)
=x\rightharpoonup\left(  d^{\prime}\left(  y\right)  \right)
-y\rightharpoonup\left(  d^{\prime}\left(  x\right)  \right)  .
\end{align*}
Thus, $d^{\prime}$ is a $1$-cocycle, qed.}. On the other hand, $0:\mathfrak{i}%
\rightarrow\mathfrak{g}\diagup\mathfrak{h}$ is a $1$-cocycle as well. Every
$s\in S$ satisfies%
\begin{align*}
\left(  d^{\prime}\mid_{S}\right)  \left(  s\right)   &  =d^{\prime}\left(
s\right)  =\left(  \pi\circ d\right)  \left(  s\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }d^{\prime}\text{ is the restriction
of }\pi\circ d\text{ to }\mathfrak{i}\right) \\
&  =\pi\left(  d\left(  s\right)  \right)  =0\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }d\left(  \underbrace{s}_{\in
S}\right)  \in d\left(  S\right)  \subseteq\mathfrak{h}=\operatorname*{Ker}%
\pi\right) \\
&  =0\left(  s\right)  =\left(  0\mid_{S}\right)  \left(  s\right)  .
\end{align*}
Thus, $d^{\prime}\mid_{S}=0\mid_{S}$. Proposition
\ref{prop.derivation.Lie.unique} (applied to $\mathfrak{i}$, $\mathfrak{g}%
\diagup\mathfrak{h}$, $d^{\prime}$ and $0$ instead of $\mathfrak{g}$, $M$, $d$
and $e$) therefore yields that $d^{\prime}=0$ on $\mathfrak{i}$. But since
$d^{\prime}$ is the restriction of $\pi\circ d$ to $\mathfrak{i}$, we have
$d^{\prime}=\left(  \pi\circ d\right)  \mid_{\mathfrak{i}}$. Thus, $\left(
\pi\circ d\right)  \mid_{\mathfrak{i}}=d^{\prime}=0$, so that $\left(
\pi\circ d\right)  \left(  \mathfrak{i}\right)  =0$. Thus, $\pi\left(
d\left(  \mathfrak{i}\right)  \right)  =\left(  \pi\circ d\right)  \left(
\mathfrak{i}\right)  =0$, so that $d\left(  \mathfrak{i}\right)
\subseteq\operatorname*{Ker}\pi=\mathfrak{h}$. Corollary
\ref{cor.derivation.Lie.unique.ihg} is therefore proven.
\end{verlong}

\subsubsection{Derivations from grading}

The following simple lemma will help us defining derivations on Lie algebras:

\begin{lemma}
\label{lem.deriv.grading}Let $Q$ be an abelian group. Let $s\in
\operatorname*{Hom}\left(  Q,\mathbb{Z}\right)  $ be a group homomorphism. Let
$\mathfrak{n}$ be a $Q$-graded Lie algebra. Let $\eta:\mathfrak{n}%
\rightarrow\mathfrak{n}$ be a linear map satisfying%
\begin{equation}
\eta\left(  x\right)  =s\left(  w\right)  \cdot x\ \ \ \ \ \ \ \ \ \ \text{for
every }w\in Q\text{ and every }x\in\mathfrak{n}\left[  w\right]  .
\label{lem.deriv.grading.1}%
\end{equation}
Then, $\eta$ is a derivation (of Lie algebras).
\end{lemma}

\textit{Proof of Lemma \ref{lem.deriv.grading}.} In order to prove that $\eta$
is a derivation, we need to check that
\begin{equation}
\eta\left(  \left[  a,b\right]  \right)  =\left[  \eta\left(  a\right)
,b\right]  +\left[  a,\eta\left(  b\right)  \right]
\ \ \ \ \ \ \ \ \ \ \text{for any }a\in\mathfrak{n}\text{ and }b\in
\mathfrak{n}. \label{pf.deriv.grading.1}%
\end{equation}
Let us prove the equation (\ref{pf.deriv.grading.1}). Since this equation is
linear in each of $a$ and $b$, we can WLOG assume that $a$ and $b$ are
homogeneous (because any element of $\mathfrak{n}$ is a sum of homogeneous
elements). So, assume this. Since $a$ is homogeneous, we have $a\in
\mathfrak{n}\left[  u\right]  $ for some $u\in Q$. Consider this $u$. Since
$b$ is homogeneous, we have $b\in\mathfrak{n}\left[  v\right]  $ for some
$v\in Q$. Fix this $v$. Thus, $\left[  a,b\right]  \in\mathfrak{n}\left[
u+v\right]  $ (since $a\in\mathfrak{n}\left[  u\right]  $ and $b\in
\mathfrak{n}\left[  v\right]  $ and since $\mathfrak{n}$ is $Q$-graded). Thus,
(\ref{lem.deriv.grading.1}) (applied to $x=a+b$ and $w=u+v$) yields
$\eta\left(  \left[  a,b\right]  \right)  =\underbrace{s\left(  u+v\right)
}_{\substack{=s\left(  u\right)  +s\left(  v\right)  \\\text{(since }s\text{
is a group}\\\text{homomorphism)}}}\cdot\left[  a,b\right]  =\left(  s\left(
u\right)  +s\left(  v\right)  \right)  \cdot\left[  a,b\right]  $. On the
other hand, (\ref{lem.deriv.grading.1}) (applied to $x=a$ and $w=u$) yields
$\eta\left(  a\right)  =s\left(  u\right)  \cdot a$. Also,
(\ref{lem.deriv.grading.1}) (applied to $x=b$ and $y=v$) yields $\eta\left(
b\right)  =s\left(  v\right)  \cdot b$. Now,%
\[
\left[  \underbrace{\eta\left(  a\right)  }_{=s\left(  u\right)  \cdot
a},b\right]  +\left[  a,\underbrace{\eta\left(  b\right)  }_{=s\left(
v\right)  \cdot b}\right]  =s\left(  u\right)  \cdot\left[  a,b\right]
+s\left(  v\right)  \cdot\left[  a,b\right]  =\left(  s\left(  u\right)
+s\left(  v\right)  \right)  \cdot\left[  a,b\right]  =\eta\left(  \left[
a,b\right]  \right)  .
\]
This proves (\ref{pf.deriv.grading.1}). Now that (\ref{pf.deriv.grading.1}) is
proven, we conclude that $\eta$ is a derivation. Lemma \ref{lem.deriv.grading}
is proven.

\subsubsection{The commutator of derivations}

The following proposition is the classical analogue of Proposition
\ref{prop.commutator.derivs} for algebras in lieu of Lie algebras:

\begin{proposition}
\label{prop.commutator.derivs.alg}Let $A$ be an algebra. Let $f:A\rightarrow
A$ and $g:A\rightarrow A$ be two derivations of $A$. Then, $\left[
f,g\right]  $ is a derivation of $A$. (Here, the Lie bracket is to be
understood as the Lie bracket on $\operatorname*{End}A$, so that we have
$\left[  f,g\right]  =f\circ g-g\circ f$.)
\end{proposition}

The proof of this is completely analogous to that of Proposition
\ref{prop.commutator.derivs}. Moreover, by the same argument, the following
slight generalization of Proposition \ref{prop.commutator.derivs.alg} can be shown:

\begin{proposition}
\label{prop.commutator.derivs.alg.2}Let $A$ be a subalgebra of an algebra $B$.
Let $f:A\rightarrow B$ and $g:B\rightarrow B$ be two derivations of $A$ such
that $g\left(  A\right)  \subseteq A$. Then, $f\circ\left(  g\mid_{A}\right)
-g\circ f:A\rightarrow B$ is a derivation.
\end{proposition}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.commutator.derivs.alg.2}.} Let $a\in A$
and $b\in A$. Since $f$ is a derivation, we have $f\left(  ab\right)
=f\left(  a\right)  \cdot b+a\cdot f\left(  b\right)  $. Thus,%
\begin{align*}
\left(  g\circ f\right)  \left(  ab\right)   &  =g\left(  \underbrace{f\left(
ab\right)  }_{=f\left(  a\right)  \cdot b+a\cdot f\left(  b\right)  }\right)
=g\left(  f\left(  a\right)  \cdot b+a\cdot f\left(  b\right)  \right) \\
&  =\underbrace{g\left(  f\left(  a\right)  \cdot b\right)  }%
_{\substack{=g\left(  f\left(  a\right)  \right)  \cdot b+f\left(  a\right)
\cdot g\left(  b\right)  \\\text{(since }g\text{ is a derivation)}%
}}+\underbrace{g\left(  a\cdot f\left(  b\right)  \right)  }%
_{\substack{=g\left(  a\right)  \cdot f\left(  b\right)  +a\cdot g\left(
f\left(  b\right)  \right)  \\\text{(since }g\text{ is a derivation)}}}\\
&  =\underbrace{g\left(  f\left(  a\right)  \right)  }_{=\left(  g\circ
f\right)  \left(  a\right)  }\cdot b+f\left(  a\right)  \cdot g\left(
b\right)  +g\left(  a\right)  \cdot f\left(  b\right)  +a\cdot
\underbrace{g\left(  f\left(  b\right)  \right)  }_{=\left(  g\circ f\right)
\left(  b\right)  }\\
&  =\left(  g\circ f\right)  \left(  a\right)  \cdot b+f\left(  a\right)
\cdot g\left(  b\right)  +g\left(  a\right)  \cdot f\left(  b\right)
+a\cdot\left(  g\circ f\right)  \left(  b\right)  .
\end{align*}
Let us notice that $f\left(  g\left(  a\right)  \right)  $ and $f\left(
g\left(  b\right)  \right)  $ are well-defined (since $g\left(  a\right)  \in
g\left(  A\right)  \subseteq A$ and $g\left(  b\right)  \in g\left(  B\right)
\subseteq A$). Since $g$ is a derivation, we have $g\left(  ab\right)
=g\left(  a\right)  \cdot b+a\cdot g\left(  b\right)  $. Thus,%
\begin{align*}
\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  ab\right)   &
=f\left(  \underbrace{\left(  g\mid_{A}\right)  \left(  ab\right)
}_{=g\left(  ab\right)  =g\left(  a\right)  \cdot b+a\cdot g\left(  b\right)
}\right)  =f\left(  g\left(  a\right)  \cdot b+a\cdot g\left(  b\right)
\right) \\
&  =\underbrace{f\left(  g\left(  a\right)  \cdot b\right)  }%
_{\substack{=f\left(  g\left(  a\right)  \right)  \cdot b+g\left(  a\right)
\cdot f\left(  b\right)  \\\text{(since }g\text{ is a derivation)}%
}}+\underbrace{f\left(  a\cdot g\left(  b\right)  \right)  }%
_{\substack{=f\left(  a\right)  \cdot g\left(  b\right)  +a\cdot f\left(
g\left(  b\right)  \right)  \\\text{(since }g\text{ is a derivation)}}}\\
&  =f\left(  \underbrace{g\left(  a\right)  }_{=\left(  g\mid_{A}\right)
\left(  a\right)  }\right)  \cdot b+g\left(  a\right)  \cdot f\left(
b\right)  +f\left(  a\right)  \cdot g\left(  b\right)  +a\cdot f\left(
\underbrace{g\left(  b\right)  }_{=\left(  g\mid_{A}\right)  \left(  b\right)
}\right) \\
&  =\underbrace{f\left(  \left(  g\mid_{A}\right)  \left(  a\right)  \right)
}_{=\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  a\right)  }\cdot
b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(  a\right)  \cdot
g\left(  b\right)  +a\cdot\underbrace{f\left(  \left(  g\mid_{A}\right)
\left(  b\right)  \right)  }_{=\left(  f\circ\left(  g\mid_{A}\right)
\right)  \left(  b\right)  }\\
&  =\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  a\right)  \cdot
b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(  a\right)  \cdot
g\left(  b\right)  +a\cdot\left(  f\circ\left(  g\mid_{A}\right)  \right)
\left(  b\right)  .
\end{align*}


Thus,%
\begin{align*}
&  \left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(  ab\right)
\\
&  =\underbrace{\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
ab\right)  }_{=\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
a\right)  \cdot b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(
a\right)  \cdot g\left(  b\right)  +a\cdot\left(  f\circ\left(  g\mid
_{A}\right)  \right)  \left(  b\right)  }-\underbrace{\left(  g\circ f\right)
\left(  ab\right)  }_{=\left(  g\circ f\right)  \left(  a\right)  \cdot
b+f\left(  a\right)  \cdot g\left(  b\right)  +g\left(  a\right)  \cdot
f\left(  b\right)  +a\cdot\left(  g\circ f\right)  \left(  b\right)  }\\
&  =\left(  \left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  a\right)
\cdot b+g\left(  a\right)  \cdot f\left(  b\right)  +f\left(  a\right)  \cdot
g\left(  b\right)  +a\cdot\left(  f\circ\left(  g\mid_{A}\right)  \right)
\left(  b\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ -\left(  \left(  g\circ f\right)  \left(  a\right)
\cdot b+f\left(  a\right)  \cdot g\left(  b\right)  +g\left(  a\right)  \cdot
f\left(  b\right)  +a\cdot\left(  g\circ f\right)  \left(  b\right)  \right)
\\
&  =\underbrace{\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
a\right)  \cdot b-\left(  g\circ f\right)  \left(  a\right)  \cdot
b}_{=\left(  \left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
a\right)  -\left(  g\circ f\right)  \left(  a\right)  \right)  \cdot
b}+\underbrace{a\cdot\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(
b\right)  -a\cdot\left(  g\circ f\right)  \left(  b\right)  }_{=a\cdot\left(
\left(  f\circ\left(  g\mid_{A}\right)  \right)  \left(  b\right)  -\left(
g\circ f\right)  \left(  b\right)  \right)  }\\
&  =\underbrace{\left(  \left(  f\circ\left(  g\mid_{A}\right)  \right)
\left(  a\right)  -\left(  g\circ f\right)  \left(  a\right)  \right)
}_{=\left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(
a\right)  }\cdot b+a\cdot\underbrace{\left(  \left(  f\circ\left(  g\mid
_{A}\right)  \right)  \left(  b\right)  -\left(  g\circ f\right)  \left(
b\right)  \right)  }_{=\left(  f\circ\left(  g\mid_{A}\right)  -g\circ
f\right)  \left(  b\right)  }\\
&  =\left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(
a\right)  \cdot b+a\cdot\left(  f\circ\left(  g\mid_{A}\right)  -g\circ
f\right)  \left(  b\right)  .
\end{align*}
We have thus proven that any $a\in A$ and $b\in A$ satisfy $\left(
f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(  ab\right)  =\left(
f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(  a\right)  \cdot
b+a\cdot\left(  f\circ\left(  g\mid_{A}\right)  -g\circ f\right)  \left(
b\right)  $. In other words, $f\circ\left(  g\mid_{A}\right)  -g\circ f$ is a
derivation. This proves Proposition \ref{prop.commutator.derivs.alg.2}.

\textit{Proof of Proposition \ref{prop.commutator.derivs.alg}.} Applying
Proposition \ref{prop.commutator.derivs.alg.2} to $B=A$, we obtain that
$f\circ\left(  g\mid_{A}\right)  -g\circ f:A\rightarrow A$ is a derivation
(since $g\left(  A\right)  \subseteq A$). In other words, $\left[  f,g\right]
$ is a derivation (since $f\circ\underbrace{\left(  g\mid_{A}\right)  }%
_{=g}-g\circ f=f\circ g-g\circ f=\left[  f,g\right]  $). Hence, Proposition
\ref{prop.commutator.derivs.alg} is proven.
\end{verlong}

\subsection{Simple Lie algebras: a recollection}

The Kac-Moody Lie algebras form a class of Lie algebras which contains all
simple finite-dimensional and all affine Lie algebras, but also many more.
Before we start studying them, let us recall some facts about simple Lie algebras:

Let $\mathfrak{g}$ be a finite-dimensional simple Lie algebra over
$\mathbb{C}$. A \textit{Cartan subalgebra} of $\mathfrak{g}$ means a maximal
commutative Lie subalgebra which consists of semisimple\footnote{An element of
a Lie algebra is said to be \textit{semisimple} if and only if its action on
the adjoint representation is a semisimple operator.} elements. There are
usually many Cartan subalgebras of $\mathfrak{g}$, but they are all conjugate
under the action of the corresponding Lie group $G$ (which satisfies
$\mathfrak{g}=\operatorname*{Lie}G$, and can be defined as the connected
component of the identity in the group $\operatorname*{Aut}\mathfrak{g}$).
Thus, there is no loss of generality in picking one such subalgebra. So pick a
Cartan subalgebra $\mathfrak{h}$ of $\mathfrak{g}$. We denote the dimension
$\dim\mathfrak{h}$ by $n$ and also by $\operatorname*{rank}\mathfrak{g}$. This
dimension $\dim\mathfrak{h}=\operatorname*{rank}\mathfrak{g}$ is called the
\textit{rank} of $\mathfrak{g}$. The restriction of the Killing form on
$\mathfrak{g}$ to $\mathfrak{h}\times\mathfrak{h}$ is a nondegenerate
symmetric bilinear form on $\mathfrak{h}$.

For every $\alpha\in\mathfrak{h}^{\ast}$, we can define a vector subspace
$\mathfrak{g}_{\alpha}$ of $\mathfrak{g}$ by
\[
\mathfrak{g}_{\alpha}=\left\{  a\in\mathfrak{g}\ \mid\ \left[  h,a\right]
=\alpha\left(  h\right)  a\text{ for all }h\in\mathfrak{h}\right\}  .
\]
It can be shown that $\mathfrak{g}_{0}=\mathfrak{h}$. Now we can write
$\mathfrak{g}=\mathfrak{h}\oplus\bigoplus\limits_{\alpha\in\Delta}%
\mathfrak{g}_{\alpha}$ for some subset $\Delta$ of $\mathfrak{h}^{\ast
}\diagdown\left\{  0\right\}  $ (which is chosen in such a way that
$\mathfrak{g}_{\alpha}\neq0$ for all $\alpha\in\Delta$). This subset $\Delta$
is called the \textit{root system} of $\mathfrak{g}$. It is known that each
$\mathfrak{g}_{\alpha}$ is one-dimensional and can be written as
$\mathfrak{g}_{\alpha}=\mathbb{C}e_{\alpha}$ for some particular $e_{\alpha
}\in\mathfrak{g}$.

We want to use the decomposition $\mathfrak{g}=\mathfrak{h}\oplus
\bigoplus\limits_{\alpha\in\Delta}\mathfrak{g}_{\alpha}$ in order to construct
a triangular decomposition of $\mathfrak{g}$. This can be done with the
grading which we constructed in Proposition \ref{prop.grad.g}, but let us do
it again now, with more elementary means: Fix an $\overline{h}\in\mathfrak{h}$
such that every $\alpha\in\Delta$ satisfies $\alpha\left(  \overline
{h}\right)  \in\mathbb{R}\diagdown\left\{  0\right\}  $ (it can be seen that
such $\overline{h}$ exists). Define $\Delta_{+}=\left\{  \alpha\in\Delta
\ \mid\ \alpha\left(  \overline{h}\right)  >0\right\}  $ and $\Delta
_{-}=\left\{  \alpha\in\Delta\ \mid\ \alpha\left(  \overline{h}\right)
<0\right\}  $. Then, $\Delta$ is the union of two disjoint subsets $\Delta
_{+}$ and $\Delta_{-}$, and we have $\Delta_{+}=-\Delta_{-}$. The triangular
decomposition of $\mathfrak{g}$ is now defined as $\mathfrak{g}=\mathfrak{n}%
_{-}\oplus\mathfrak{h}\oplus\mathfrak{n}_{+}$, where $\mathfrak{n}%
_{-}=\bigoplus\limits_{\alpha\in\Delta_{-}}\mathfrak{g}_{\alpha}$ and
$\mathfrak{n}_{+}=\bigoplus\limits_{\alpha\in\Delta_{+}}\mathfrak{g}_{\alpha}%
$. This decomposition depends on the choice of $\overline{h}$ (and
$\mathfrak{h}$, of course). The elements of $\Delta_{+}$ are called
\textit{positive roots} of $\mathfrak{g}$, and the elements of $\Delta_{-}$
are called \textit{negative roots} of $\mathfrak{g}$. If $\alpha$ is a root of
$\mathfrak{g}$, then we write $\alpha>0$ if $\alpha$ is a positive root, and
we write $\alpha<0$ if $\alpha$ is a negative root.

Let us now construct the grading on $\mathfrak{g}$ which yields this
triangular decomposition $\mathfrak{g}=\mathfrak{n}_{-}\oplus\mathfrak{h}%
\oplus\mathfrak{n}_{+}$. This grading was already constructed in Proposition
\ref{prop.grad.g}, but now we are going to do this in detail:

We define the \textit{simple roots} of $\mathfrak{g}$ as the elements of
$\Delta_{+}$ which cannot be written as sums of more than one element of
$\Delta_{+}$. It can be shown that there are exactly $n$ of these simple
roots, and they form a basis of $\mathfrak{h}^{\ast}$. Denote these simple
roots as $\alpha_{1}$, $\alpha_{2}$, $...$, $\alpha_{n}$. Every root
$\alpha\in\Delta_{+}$ can now be written in the form $\alpha=\sum
\limits_{i=1}^{n}k_{i}\left(  \alpha\right)  \alpha_{i}$ for a unique
$n$-tuple $\left(  k_{1}\left(  \alpha\right)  ,k_{2}\left(  \alpha\right)
,...,k_{n}\left(  \alpha\right)  \right)  $ of nonnegative integers.

For all $\alpha,\beta\in\Delta$ with $\alpha+\beta\notin\Delta\cup\left\{
0\right\}  $, we have $\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{\beta
}\right]  =0$. For all $\alpha,\beta\in\mathfrak{h}^{\ast}$, we have $\left[
\mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}\right]  \subseteq\mathfrak{g}%
_{\alpha+\beta}$. In particular, for every $\alpha\in\mathfrak{h}^{\ast}$, we
have $\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{-\alpha}\right]
\subseteq\mathfrak{h}$. Better yet, we can show that for every $\alpha
\in\Delta$, there exists some nonzero $h_{\alpha}\in\mathfrak{h}$ such that
$\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{-\alpha}\right]  =\mathbb{C}%
h_{\alpha}$.

For every $i\in\left\{  1,2,...,n\right\}  $, pick a generator $e_{i}$ of the
vector space $\mathfrak{g}_{\alpha_{i}}$ and a generator $f_{i}$ of the vector
space $\mathfrak{g}_{-\alpha_{i}}$.

It is possible to normalize $e_{i}$ and $f_{i}$ in such a way that $\left[
h_{i},e_{i}\right]  =2e_{i}$ and $\left[  h_{i},f_{i}\right]  =-2f_{i}$, where
$h_{i}=\left[  e_{i},f_{i}\right]  $. This $h_{i}$ will, of course, lie in
$\mathfrak{h}$ and be a scalar multiple of $h_{\alpha_{i}}$. We can normalize
$h_{\alpha_{i}}$ in such a way that $h_{i}=h_{\alpha_{i}}$. We suppose that
all these normalizations are done. Then:

\begin{proposition}
\label{prop.serre-gen.1}With the notations introduced above, we have:

\textbf{(a)} The family $\left(  h_{1},h_{2},...,h_{n}\right)  $ is a basis of
$\mathfrak{h}$.

\textbf{(b)} For any $i$ and $j$ in $\left\{  1,2,...,n\right\}  $, denote
$\alpha_{j}\left(  h_{i}\right)  $ by $a_{i,j}$. The Lie algebra
$\mathfrak{g}$ is generated (as a Lie algebra) by the elements $e_{i}$,
$f_{i}$ and $h_{i}$ with $i\in\left\{  1,2,...,n\right\}  $ (a total of $3n$
elements), and the following relations hold:%
\begin{align*}
\left[  h_{i},h_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  h_{i},e_{j}\right]   &  =\alpha_{j}\left(  h_{i}\right)  e_{j}%
=a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{
1,2,...,n\right\}  ;\\
\left[  h_{i},f_{j}\right]   &  =-\alpha_{j}\left(  h_{i}\right)
f_{j}=a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{
1,2,...,n\right\}  ;\\
\left[  e_{i},f_{j}\right]   &  =\delta_{i,j}h_{i}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{  1,2,...,n\right\}  .
\end{align*}
(This does not mean that no more relations hold. In fact, additional
relations, the so-called Serre relations, do hold in $\mathfrak{g}$; we will
see these relations later, in Theorem \ref{thm.serre-gen.2}.)

The $n\times n$ matrix $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ is
called the \textit{Cartan matrix} of $\mathfrak{g}$.

Let $\left(  \cdot,\cdot\right)  $ denote the standard form on $\mathfrak{g}$
(defined in Definition \ref{def.standform}). Then, $\left(  \cdot
,\cdot\right)  $ is a nonzero scalar multiple of the Killing form on
$\mathfrak{g}$ (since any two nonzero invariant symmetric bilinear forms on
$\mathfrak{g}$ are scalar multiples of each other). Hence, the restriction of
$\left(  \cdot,\cdot\right)  $ to $\mathfrak{h}\times\mathfrak{h}$ is
nondegenerate (since the restriction of the Killing form to $\mathfrak{h}%
\times\mathfrak{h}$ is nondegenerate). Thus, this restriction gives rise to a
vector space isomorphism $\mathfrak{h}\rightarrow\mathfrak{h}^{\ast}$. This
isomorphism sends $h_{i}$ to $\alpha_{i}^{\vee}=\dfrac{2\alpha_{i}}{\left(
\alpha_{i},\alpha_{i}\right)  }$ for every $i$ (where we denote by $\left(
\cdot,\cdot\right)  $ not only the standard form, but also the inverse form of
its restriction to $\mathfrak{h}$). Thus, $a_{i,j}=\alpha_{j}\left(
h_{i}\right)  =\dfrac{2\left(  \alpha_{j},\alpha_{i}\right)  }{\left(
\alpha_{i},\alpha_{i}\right)  }$ for all $i$ and $j$. (Note that the latter
equality would still hold if $\left(  \cdot,\cdot\right)  $ would mean the
Killing form rather than the standard form.)

The elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ are called \textit{Chevalley
generators} of $\mathfrak{g}$.

\textbf{Properties of the matrix }$A$\textbf{:}

\textbf{1)} We have $a_{i,i}=2$ for all $i\in\left\{  1,2,...,n\right\}  $.

\textbf{2)} Any two distinct $i\in\left\{  1,2,...,n\right\}  $ and
$j\in\left\{  1,2,...,n\right\}  $ satisfy $a_{i,j}\leq0$ and $a_{i,j}%
\in\mathbb{Z}$. Also, $a_{i,j}=0$ if and only if $a_{j,i}=0$.

\textbf{3)} The matrix $A$ is indecomposable (i. e., if conjugation of $A$ by
a permutation matrix brings $A$ into a block-diagonal form $\left(
\begin{array}
[c]{cc}%
A_{1} & 0\\
0 & A_{2}%
\end{array}
\right)  $, then either $A_{1}$ or $A_{2}$ is a $0\times0$ matrix).

\textbf{4)} The matrix $A$ is positive. Here is what we mean by this: There
exists a diagonal $n\times n$ matrix $D$ with positive diagonal entries such
that $DA$ is a symmetric and positive definite matrix.
\end{proposition}

\begin{theorem}
An $n\times n$ matrix $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ satisfies
the four properties \textbf{1)}, \textbf{2)}, \textbf{3)} and \textbf{4)} of
Proposition \ref{prop.serre-gen.1} if and only if it is a Cartan matrix of a
simple Lie algebra.
\end{theorem}

Such matrices (and thus, simple finite-dimensional Lie algebras) can be
encoded by so-called \textit{Dynkin diagrams}. The \textit{Dynkin diagram} of
a simple Lie algebra $\mathfrak{g}$ is defined as the graph with vertex set
$\left\{  1,2,...,n\right\}  $, and the following rules for drawing
edges\footnote{The notion of a graph we are using here is slightly different
from the familiar notions of a graph in graph theory, since this graph can
have both directed and undirected edges.}:

\begin{itemize}
\item If $a_{i,j}=0$, then the vertices $i$ and $j$ are not connected by any
edge (directed or undirected).

\item If $a_{i,j}=a_{j,i}=-1$, then the vertices $i$ and $j$ are connected by
exactly one edge, and this edge is undirected.

\item If $a_{i,j}=-2$ and $a_{j,i}=-1$, then the vertices $i$ and $j$ are
connected by two directed edges from $j$ to $i$ (and no other edges).

\item If $a_{i,j}=-3$ and $a_{j,i}=-1$, then the vertices $i$ and $j$ are
connected by three directed edges from $j$ to $i$ (and no other edges).
\end{itemize}

Here is a classification of simple finite-dimensional Lie algebras by their
Dynkin diagrams:

$A_{n}=\mathfrak{sl}\left(  n+1\right)  $ for $n\geq1$; the Dynkin diagram is
$%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$ (with $n$ nodes).

$B_{n}=\mathfrak{so}\left(  2n+1\right)  $ for $n\geq2$; the Dynkin diagram is
$%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{=>}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{=>}[r] & \circ}%
%EndExpansion
$ (with $n$ nodes, only the last edge being directed and double). (Note that
$\mathfrak{so}\left(  3\right)  \cong\mathfrak{sl}\left(  2\right)  $.)

$C_{n}=\mathfrak{sp}\left(  2n\right)  $ for $n\geq2$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{<=}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{<=}[r] & \circ}%
%EndExpansion
$ (with $n$ nodes, only the last edge being directed and double). (Note that
$\mathfrak{sp}\left(  2\right)  \cong\mathfrak{sl}\left(  2\right)  $ and
$\mathfrak{sp}\left(  4\right)  \cong\mathfrak{so}\left(  5\right)  $.)

$D_{n}=\mathfrak{so}\left(  2n\right)  $ for $n\geq4$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & & & & \circ\\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{-}[ru] \ar@{-}[rd] \\
%& & & & & \circ}}}%
%BeginExpansion
\xymatrix{
& & & & & \circ\\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{-}[ru] \ar@{-}[rd] \\
& & & & & \circ}%
%EndExpansion
$ (with $n$ nodes). (Note that $\mathfrak{so}\left(  4\right)  \cong%
\mathfrak{sl}\left(  2\right)  \oplus\mathfrak{sl}\left(  2\right)  $ and
$\mathfrak{so}\left(  6\right)  \cong\mathfrak{sl}\left(  4\right)  $.)

Exceptional Lie algebras:

$E_{6}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & \circ\ar@{-}[d] & & \\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ
%}}}%
%BeginExpansion
\xymatrix{
& & \circ\ar@{-}[d] & & \\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$E_{7}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & \circ\ar@{-}[d] & & & \\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
%[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
& & \circ\ar@{-}[d] & & & \\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$E_{8}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& & \circ\ar@{-}[d] & & & & \\
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
%[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
& & \circ\ar@{-}[d] & & & & \\
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{-}%
[r] & \circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$F_{4}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{=>}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{=>}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$.

$G_{2}$; the Dynkin diagram is $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ& \circ\ar@3{->}[l]
%}}}%
%BeginExpansion
\xymatrix{
\circ& \circ\ar@3{->}[l]
}%
%EndExpansion
$.

Now to the Serre relations, which we have not yet written down:

\begin{theorem}
\label{thm.serre-gen.2}Let $\mathfrak{g}$ be a simple finite-dimensional Lie
algebra. Use the notations introduced in Proposition \ref{prop.serre-gen.1}.

\textbf{(a)} Let $i$ and $j$ be two distinct elements of $\left\{
1,2,...,n\right\}  $. Then, in $\mathfrak{g}$, we have $\left(
\operatorname*{ad}\left(  e_{i}\right)  \right)  ^{1-a_{i,j}}e_{j}=0$ and
$\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}%
=0$. These relations (totalling up to $2n\left(  n-1\right)  $ relations,
because there are $n\left(  n-1\right)  $ pairs $\left(  i,j\right)  $ of
distinct elements of $\left\{  1,2,...,n\right\}  $) are called the
\textit{Serre relations} for $\mathfrak{g}$.

\textbf{(b)} Combined with the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  h_{i},h_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  h_{i},e_{j}\right]  =a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  e_{i},f_{j}\right]  =\delta_{i,j}h_{i}\ \ \ \ \ \ \ \ \ \ \text{for
all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{nonserre-relations}%
\end{equation}
of Proposition \ref{prop.serre-gen.1}, the Serre relations form a set of
defining relations for $\mathfrak{g}$. This means that, if
$\widetilde{\mathfrak{g}}$ denotes the quotient Lie algebra
\[
\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  ,
\]
then $\widetilde{\mathfrak{g}}\diagup\left(  \text{Serre relations}\right)
\cong\mathfrak{g}$. (Here, $\operatorname*{FreeLie}\left(  h_{i},f_{i}%
,e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ denotes the free Lie
algebra with $3n$ generators $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$.)
\end{theorem}

\begin{remark}
If $\mathfrak{g}\cong\mathfrak{sl}_{2}$, then $\mathfrak{g}$ has no Serre
relations (because $n=1$), and thus the claim of Theorem \ref{thm.serre-gen.2}
\textbf{(b)} rewrites as $\widetilde{\mathfrak{g}}\cong\mathfrak{g}$ (where
$\widetilde{\mathfrak{g}}$ is defined as in Theorem \ref{thm.serre-gen.2}).
But in all other cases, the Lie algebra $\widetilde{\mathfrak{g}}$ is
infinite-dimensional, and while it clearly projects onto $\mathfrak{g}$, it is
much bigger than $\mathfrak{g}$.
\end{remark}

We will give a partial proof of Theorem \ref{thm.serre-gen.2}: We will only
prove part \textbf{(a)}.

\textit{Proof of Theorem \ref{thm.serre-gen.2} \textbf{(a)}.} Define a
$\mathbb{C}$-linear map%
\begin{align*}
\Phi_{i}:\mathfrak{sl}_{2}  &  \rightarrow\mathfrak{g},\\
e  &  \mapsto e_{i},\\
f  &  \mapsto f_{i},\\
h  &  \mapsto h_{i}.
\end{align*}
Since $\left[  e_{i},f_{i}\right]  =h_{i}$, $\left[  h_{i},e_{i}\right]
=2e_{i}$ and $\left[  h_{i},f_{i}\right]  =-2f_{i}$, this map $\Phi_{i}$ is a
Lie algebra homomorphism.

But $\mathfrak{g}$ is a $\mathfrak{g}$-module (by the adjoint representation
of $\mathfrak{g}$), and thus becomes an $\mathfrak{sl}_{2}$-module by means of
$\Phi_{i}:\mathfrak{sl}_{2}\rightarrow\mathfrak{g}$. This $\mathfrak{sl}_{2}%
$-module satisfies%
\[
ef_{j}=\underbrace{\left(  \Phi_{i}\left(  e\right)  \right)  }_{=e_{i}}%
f_{j}=\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)  f_{j}=\left[
e_{i},f_{j}\right]  =0
\]
and%
\[
hf_{j}=\underbrace{\left(  \Phi_{i}\left(  h\right)  \right)  }_{=h_{i}}%
f_{j}=\left(  \operatorname*{ad}\left(  h_{i}\right)  \right)  f_{j}=\left[
h_{i},f_{j}\right]  =-a_{i,j}f_{i,j}.
\]
Hence, Lemma \ref{lem.serre-gen.sl2} (applied to $V=\mathfrak{g}$,
$\lambda=-a_{i,j}$ and $x=f_{j}$) yields that $-a_{i,j}\in\mathbb{N}$ and
$f^{-a_{i,j}+1}f_{j}=0$. Since%
\[
f^{-a_{i,j}+1}f_{j}=f^{1-a_{i,j}}f_{j}=\left(  \underbrace{\Phi_{i}\left(
f\right)  }_{=f_{i}}\right)  ^{1-a_{i,j}}f_{j}=\left(  \operatorname*{ad}%
\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j},
\]
this rewrites as $\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}=0$. Similarly, $\left(  \operatorname*{ad}\left(
e_{i}\right)  \right)  ^{1-a_{i,j}}e_{j}=0$. Theorem \ref{thm.serre-gen.2}
\textbf{(a)} is thus proven.

As we said, we are not going to prove Theorem \ref{thm.serre-gen.2}
\textbf{(b)} here.

\subsection{Kac-Moody Lie algebras: definition and construction}

Now forget about our simple Lie algebra $\mathfrak{g}$.

\begin{definition}
\label{def.contragredient}Suppose that $A=\left(  a_{i,j}\right)  _{1\leq
i,j\leq n}$ is any $n\times n$ matrix of complex numbers.

Let $Q$ be the free abelian group generated by $n$ symbols $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{n}$ (that is, $Q=\mathbb{Z}\alpha_{1}%
\oplus\mathbb{Z}\alpha_{2}\oplus...\oplus\mathbb{Z}\alpha_{n}$). These symbols
are just symbols, not weights of any Lie algebra (at the moment).

A $Q$\textit{-graded Lie algebra} will mean a Lie algebra $\mathfrak{g}$ with
a decomposition $\mathfrak{g}=\bigoplus\limits_{\alpha\in Q}\mathfrak{g}%
_{\alpha}$ for some vector subspaces $\mathfrak{g}_{\alpha}$ of $\mathfrak{g}$
(indexed by elements of $Q$) satisfying
\[
\left[  \mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}\right]  \subseteq
\mathfrak{g}_{\alpha+\beta}\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha,\beta\in
Q\text{.}%
\]
In this case, $Q$ is called the \textit{root lattice} of this Lie algebra
$\mathfrak{g}$.

A \textit{contragredient Lie algebra} corresponding to $A$ is a $Q$-graded
$\mathbb{C}$-Lie algebra $\mathfrak{g}$ which is (as a Lie algebra) generated
by some elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ which satisfy the following three conditions:

\textbf{(1)} These elements satisfy the relations (\ref{nonserre-relations}).

\textbf{(2)} The vector space $\mathfrak{g}_{0}$ has $\left(  h_{1}%
,h_{2},...,h_{n}\right)  $ as a $\mathbb{C}$-vector space basis, and we have
$\mathfrak{g}_{\alpha_{i}}=\mathbb{C}e_{i}$ and $\mathfrak{g}_{-\alpha_{i}%
}=\mathbb{C}f_{i}$ for all $i\in\left\{  1,2,...,n\right\}  $.

\textbf{(3)} Every $Q$-graded nonzero ideal in $\mathfrak{g}$ has a nonzero
intersection with $\mathfrak{g}_{0}$.

Just as in the case of $\mathbb{Z}$-graded Lie algebras, we will denote
$\mathfrak{g}_{0}$ by $\mathfrak{h}$.
\end{definition}

Note that the condition \textbf{(3)} is satisfied for simple
finite-dimensional $\mathfrak{g}$ (where $Q$ is the root lattice\footnote{in
the meaning which this word has in the theory of simple Lie algebras} of
$\mathfrak{g}$, and $A$ is the Cartan matrix); hence, simple
finite-dimensional Lie algebras are contragredient.

\begin{theorem}
\label{thm.g(A).exuni}Let $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ be a
(fixed) $n\times n$ matrix of complex numbers. Then, there exists a unique (up
to $Q$-graded isomorphism respecting the generators $e_{1}$, $e_{2}$, $...$,
$e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$)
contragredient Lie algebra $\mathfrak{g}$ corresponding to $A$. If $A$ is a
Cartan matrix, then this Lie algebra $\mathfrak{g}$ is finite-dimensional and simple.
\end{theorem}

\begin{definition}
Let $A$ be an $n\times n$ matrix of complex numbers. Then, the unique (up to
isomorphism) contragredient Lie algebra $\mathfrak{g}$ corresponding to $A$ is
denoted by $\mathfrak{g}\left(  A\right)  $.
\end{definition}

The proof of Theorem \ref{thm.g(A).exuni} rests upon the following fact:

\begin{theorem}
\label{thm.gtilde}Let $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ be an
$n\times n$ matrix of complex numbers. Let $e_{1}$, $e_{2}$, $...$, $e_{n}$,
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ be $3n$
distinct symbols (which are, a priori, new and unrelated to the vectors
$e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$,
$h_{2}$, $...$, $h_{n}$ in Definition \ref{def.contragredient}). Let
$\widetilde{\mathfrak{g}}$ be the quotient Lie algebra
\[
\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  .
\]
(Here, $\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  $ denotes the free Lie algebra with $3n$
generators $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$,
$h_{1}$, $h_{2}$, $...$, $h_{n}$.)

By abuse of notation, we will denote the projections of the elements $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$ onto the quotient Lie algebra $\widetilde{\mathfrak{g}}$ by the
same letters $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$.

Let $Q$ be the free abelian group generated by $n$ symbols $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{n}$ (that is, $Q=\mathbb{Z}\alpha_{1}%
\oplus\mathbb{Z}\alpha_{2}\oplus...\oplus\mathbb{Z}\alpha_{n}$). These symbols
are just symbols, not weights of any Lie algebra (at the moment).

\textbf{(a)} We can make $\widetilde{\mathfrak{g}}$ uniquely into a $Q$-graded
Lie algebra by setting%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]


\textbf{(b)} Let $\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(
e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ (this means the free
Lie algebra with $n$ generators $e_{1}$, $e_{2}$, $...$, $e_{n}$).

Let $\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  f_{i}%
\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ (this means the free Lie
algebra with $n$ generators $f_{1}$, $f_{2}$, $...$, $f_{n}$).

Let $\widetilde{\mathfrak{h}}$ be the free vector space with basis
$h_{1},h_{2},...,h_{n}$. Consider $\widetilde{\mathfrak{h}}$ as an abelian Lie algebra.

Then, we have well-defined canonical Lie algebra homomorphisms $\iota
_{+}:\widetilde{\mathfrak{n}}_{+}\rightarrow\widetilde{\mathfrak{g}}$ and
$\iota_{-}:\widetilde{\mathfrak{n}}_{-}\rightarrow\widetilde{\mathfrak{g}}$
given by sending the generators $e_{1}$, $e_{2}$, $...$, $e_{n}$ (in the case
of $\iota_{+}$), respectively, $f_{1}$, $f_{2}$, $...$, $f_{n}$ (in the case
of $\iota_{-}$) to the corresponding generators $e_{1}$, $e_{2}$, $...$,
$e_{n}$ (in the case of $\iota_{+}$), respectively, $f_{1}$, $f_{2}$, $...$,
$f_{n}$ (in the case of $\iota_{-}$). Moreover, we have a well-defined linear
map $\iota_{0}:\widetilde{\mathfrak{h}}\rightarrow\widetilde{\mathfrak{g}}$
given by sending the generators $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $h_{1}$,
$h_{2}$, $...$, $h_{n}$, respectively.

These maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ are injective Lie algebra homomorphisms.

\textbf{(c)} We have $\widetilde{\mathfrak{g}}=\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $.

\textbf{(d)} Both $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
\oplus\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ and $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ are Lie subalgebras of
$\widetilde{\mathfrak{g}}$.

\textbf{(e)} The $0$-th graded component of $\widetilde{\mathfrak{g}}$ (in the
$Q$-grading) is $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $.

\textbf{(f)} There exists an involutive Lie algebra automorphism of
$\widetilde{\mathfrak{g}}$ which sends $e_{1}$, $e_{2}$, $...$, $e_{n}$,
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$, $-h_{2}$,
$...$, $-h_{n}$, respectively.

\textbf{(g)} Let $I$ be the sum of all $Q$-graded ideals in
$\widetilde{\mathfrak{g}}$ which have zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $. Then, $I$ itself is a $Q$-graded ideal in
$\widetilde{\mathfrak{g}}$ which has zero intersection with $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $.

\textbf{(h)} Let $\mathfrak{g}=\widetilde{\mathfrak{g}}\diagup I$. Then,
$\mathfrak{g}$ is a contragredient Lie algebra corresponding to $A$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.gtilde}.} First of all, for the sake of
clarity, let us make a convention: In the following proof, the word
"derivation" will always be used for "derivation of algebras" and not for
"derivation of Lie algebras", unless we explicitly say otherwise in the same sentence.

\bigskip

\textbf{(f)} Let us notice that the relations (\ref{nonserre-relations}) are
equivalent to the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  -h_{i},-h_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  -h_{i},f_{j}\right]  =a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  -h_{i},e_{j}\right]  =-a_{i,j}e_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  f_{i},e_{j}\right]  =\delta_{i,j}\left(  -h_{i}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{nonserre-relations2}%
\end{equation}
\footnote{\textit{Proof.} We will show that the assertion
\begin{equation}
\left(  \left[  e_{i},f_{j}\right]  =\delta_{i,j}h_{i}\text{ for all }%
i,j\in\left\{  1,2,...,n\right\}  \right)  \label{pf.gtilde.equiv.1}%
\end{equation}
is equivalent to the assertion
\begin{equation}
\left(  \left[  f_{i},e_{j}\right]  =\delta_{i,j}\left(  -h_{i}\right)
\ \text{for all }i,j\in\left\{  1,2,...,n\right\}  \right)  .
\label{pf.gtilde.equiv.2}%
\end{equation}
\par
If (\ref{pf.gtilde.equiv.1}) holds, then (\ref{pf.gtilde.equiv.2}) holds as
well (because if (\ref{pf.gtilde.equiv.1}) holds, then any $i,j\in\left\{
1,2,...,n\right\}  $ satisfy%
\begin{align*}
-\left[  f_{i},e_{j}\right]   &  =\left[  e_{j},f_{i}\right]
=\underbrace{\delta_{j,i}}_{=\delta_{i,j}=\left\{
\begin{array}
[c]{c}%
1,\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  }h_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.gtilde.equiv.1}),
applied to }i\text{ and }j\text{ instead of }j\text{ and }i\right) \\
&  =\left\{
\begin{array}
[c]{c}%
1,\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  h_{j}=\left\{
\begin{array}
[c]{c}%
h_{j},\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
h_{i},\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=i\text{ in the case when
}i=j\right) \\
&  =\underbrace{\left\{
\begin{array}
[c]{c}%
1,\text{ if }i=j;\\
0,\text{ if }i\neq j
\end{array}
\right.  }_{=\delta_{i,j}}h_{i}=\delta_{i,j}h_{i}%
\end{align*}
and thus $\left[  f_{i},e_{j}\right]  =-\delta_{i,j}h_{i}=\delta_{i,j}\left(
-h_{i}\right)  $). Similarly, if (\ref{pf.gtilde.equiv.2}) holds, then
(\ref{pf.gtilde.equiv.1}) holds as well. Thus, the assertion
(\ref{pf.gtilde.equiv.1}) is equivalent to the assertion
(\ref{pf.gtilde.equiv.2}). In other words, the fourth of the four relations
(\ref{nonserre-relations}) is equivalent to the fourth of the four relations
(\ref{nonserre-relations2}). But it is easy to see that the second of the four
relations (\ref{nonserre-relations}) is equivalent to the third of the four
relations (\ref{nonserre-relations2}). Similarly, the third of the four
relations (\ref{nonserre-relations}) is equivalent to the second of the four
relations (\ref{nonserre-relations2}). Finally, the first of the four
relations (\ref{nonserre-relations}) is equivalent to the first of the four
relations (\ref{nonserre-relations2}). Altogether, we thus conclude that the
relations (\ref{nonserre-relations}) are equivalent to the relations
(\ref{nonserre-relations2}), qed.}. Hence,
\begin{align*}
\widetilde{\mathfrak{g}}  &  =\operatorname*{FreeLie}\left(  h_{i},f_{i}%
,e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right) \\
&  =\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations2})}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the relations
(\ref{nonserre-relations}) are equivalent to the relations
(\ref{nonserre-relations2})}\right)  .
\end{align*}
Hence, the relations (\ref{nonserre-relations2}) are satisfied in
$\widetilde{\mathfrak{g}}$.

Now, we can define a Lie algebra homomorphism
\[
\Omega:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \rightarrow\widetilde{\mathfrak{g}}%
\]
by requiring%
\begin{equation}
\left\{
\begin{array}
[c]{c}%
\Omega\left(  e_{i}\right)  =f_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\Omega\left(  f_{i}\right)  =e_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\Omega\left(  h_{i}\right)  =-h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{pf.gtilde.OMEGA}%
\end{equation}
(because we can define a Lie algebra homomorphism from a Lie algebra by
arbitrarily choosing its values on the free generators). Define this $\Omega$.
Then, $\Omega$ sends the relations (\ref{nonserre-relations}) to the relations
(\ref{nonserre-relations2}). Since the relations (\ref{nonserre-relations2})
are satisfied in $\widetilde{\mathfrak{g}}$, this yields that the Lie algebra
homomorphism $\Omega$ factors through the factor Lie algebra%
\[
\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  .
\]
In other words, there exists a unique Lie algebra homomorphism%
\[
\omega:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  \rightarrow\widetilde{\mathfrak{g}}%
\]
satisfying $\omega\circ\pi=\Omega$, where
\begin{align*}
\pi:  &  \operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right) \\
&  \rightarrow\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)
\end{align*}
is the canonical projection. Consider this $\omega$. Since \newline%
$\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  =\widetilde{\mathfrak{g}}$, this $\omega$
is a Lie algebra homomorphism from $\widetilde{\mathfrak{g}}$ to
$\widetilde{\mathfrak{g}}$. Due to $\omega\circ\pi=\Omega$ and because of
(\ref{pf.gtilde.OMEGA}), we have%
\begin{equation}
\left\{
\begin{array}
[c]{c}%
\omega\left(  e_{i}\right)  =f_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\omega\left(  f_{i}\right)  =e_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\omega\left(  h_{i}\right)  =-h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  . \label{pf.gtilde.omega}%
\end{equation}
Thus, $\omega$ sends $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$,
$...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$, $-h_{2}$, $...$, $-h_{n}%
$, respectively.

The elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ generate $\widetilde{\mathfrak{g}}$
as a Lie algebra\footnote{This is because $\widetilde{\mathfrak{g}%
}=\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  $.}. In other words, the subset $\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $
generates $\widetilde{\mathfrak{g}}$ as a Lie algebra.

The maps $\omega^{2}$ and $\operatorname*{id}$ are equal to each other on the
set $\left\{  e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2}%
,...,h_{n}\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let $x\in\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $.
We will prove that $\omega^{2}\left(  x\right)  =\operatorname*{id}\left(
x\right)  $.
\par
Indeed, since $x\in\left\{  e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n}%
,h_{1},h_{2},...,h_{n}\right\}  =\left\{  e_{1},e_{2},...,e_{n}\right\}
\cup\left\{  f_{1},f_{2},...,f_{n}\right\}  \cup\left\{  h_{1},h_{2}%
,...,h_{n}\right\}  $, we must be in one of the three following three cases:
\par
\textit{Case 1:} We have $x\in\left\{  e_{1},e_{2},...,e_{n}\right\}  $.
\par
\textit{Case 2:} We have $x\in\left\{  f_{1},f_{2},...,f_{n}\right\}  $.
\par
\textit{Case 3:} We have $x\in\left\{  h_{1},h_{2},...,h_{n}\right\}  $.
\par
Let us first consider Case 1. In this case, $x\in\left\{  e_{1},e_{2}%
,...,e_{n}\right\}  $. Thus, there exists an $i\in\left\{  1,2,...,n\right\}
$ such that $x=e_{i}$. Consider this $i$. From $x=e_{i}$, we obtain
$\omega\left(  x\right)  =\omega\left(  e_{i}\right)  =f_{i}$ and thus
$\omega^{2}\left(  x\right)  =\omega\left(  \underbrace{\omega\left(
x\right)  }_{=f_{i}}\right)  =\omega\left(  f_{i}\right)  =e_{i}%
=x=\operatorname*{id}\left(  x\right)  $. Thus, $\omega^{2}\left(  x\right)
=\operatorname*{id}\left(  x\right)  $ is proven in Case 1.
\par
Let us next consider Case 2. In this case, $x\in\left\{  f_{1},f_{2}%
,...,f_{n}\right\}  $. Thus, there exists an $i\in\left\{  1,2,...,n\right\}
$ such that $x=f_{i}$. Consider this $i$. From $x=f_{i}$, we obtain
$\omega\left(  x\right)  =\omega\left(  f_{i}\right)  =e_{i}$ and thus
$\omega^{2}\left(  x\right)  =\omega\left(  \underbrace{\omega\left(
x\right)  }_{=e_{i}}\right)  =\omega\left(  e_{i}\right)  =f_{i}%
=x=\operatorname*{id}\left(  x\right)  $. Thus, $\omega^{2}\left(  x\right)
=\operatorname*{id}\left(  x\right)  $ is proven in Case 2.
\par
Let us first consider Case 3. In this case, $x\in\left\{  h_{1},h_{2}%
,...,h_{n}\right\}  $. Thus, there exists an $i\in\left\{  1,2,...,n\right\}
$ such that $x=h_{i}$. Consider this $i$. From $x=h_{i}$, we obtain
$\omega\left(  x\right)  =\omega\left(  h_{i}\right)  =-h_{i}$ and thus
$\omega^{2}\left(  x\right)  =\omega\left(  \underbrace{\omega\left(
x\right)  }_{=-h_{i}}\right)  =\omega\left(  -h_{i}\right)
=-\underbrace{\omega\left(  h_{i}\right)  }_{=-h_{i}}=-\left(  -h_{i}\right)
=h_{i}=x=\operatorname*{id}\left(  x\right)  $. Thus, $\omega^{2}\left(
x\right)  =\operatorname*{id}\left(  x\right)  $ is proven in Case 3.
\par
Hence, the equality $\omega^{2}\left(  x\right)  =\operatorname*{id}\left(
x\right)  $ is proven in each of the three Cases 1, 2 and 3. Since these three
cases cover all possibilities, this yields that $\omega^{2}\left(  x\right)
=\operatorname*{id}\left(  x\right)  $ always holds.
\par
Now forget that we fixed $x$. Thus, we have shown that $\omega^{2}\left(
x\right)  =\operatorname*{id}\left(  x\right)  $ for every $x\in\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $.
In other words, the maps $\omega^{2}$ and $\operatorname*{id}$ are equal to
each other on the set $\left\{  e_{1},e_{2},...,e_{n},f_{1},f_{2}%
,...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $, qed.}. Since this set $\left\{
e_{1},e_{2},...,e_{n},f_{1},f_{2},...,f_{n},h_{1},h_{2},...,h_{n}\right\}  $
generates $\widetilde{\mathfrak{g}}$ as a Lie algebra, this yields that the
maps $\omega^{2}$ and $\operatorname*{id}$ are equal to each other on a
generating set of the Lie algebra $\widetilde{\mathfrak{g}}$. We also know
that $\omega^{2}$ and $\operatorname*{id}$ are Lie algebra homomorphisms
(since $\omega$ is a Lie algebra homomorphism).

Now, it is well-known that if two Lie algebra homomorphisms from a Lie algebra
$\mathfrak{i}$ to another Lie algebra are equal to each other on a generating
set of the Lie algebra $\mathfrak{i}$, then these two homomorphisms must be
identical. Applied to our two Lie algebra homomorphisms $\omega^{2}$ and
$\operatorname*{id}$ (which, as we know, are equal to each other on a
generating set of the Lie algebra $\widetilde{\mathfrak{g}}$), we conclude
that the two homomorphisms $\omega^{2}$ and $\operatorname*{id}$ must be
identical. In other words, $\omega^{2}=\operatorname*{id}$. Hence, $\omega$ is
an involutive automorphism of the Lie algebra $\widetilde{\mathfrak{g}}$.

Thus, there exists an involutive Lie algebra automorphism of
$\widetilde{\mathfrak{g}}$ which sends $e_{1}$, $e_{2}$, $...$, $e_{n}$,
$f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$ to $f_{1}$,
$f_{2}$, $...$, $f_{n}$, $e_{1}$, $e_{2}$, $...$, $e_{n}$, $-h_{1}$, $-h_{2}$,
$...$, $-h_{n}$, respectively (namely, $\omega$). This proves Theorem
\ref{thm.gtilde} \textbf{(f)}.

\bigskip

\textbf{(a)} In order to define a $Q$-grading on a free Lie algebra, it is
enough to choose the degrees of its free generators. Thus, we can define a
$Q$-grading on the Lie algebra $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $ by setting%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]
The relations (\ref{nonserre-relations}) are homogeneous with respect to this
$Q$-grading; hence, the quotient Lie algebra $\operatorname*{FreeLie}\left(
h_{i},f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)
\diagup\left(  \text{the relations (\ref{nonserre-relations})}\right)  $
inherits the $Q$-grading from $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $. Since this
quotient Lie algebra is $\widetilde{\mathfrak{g}}$, we thus have constructed a
$Q$-grading on $\widetilde{\mathfrak{g}}$ which satisfies%
\begin{equation}
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  . \label{pf.gtilde.a.1}%
\end{equation}
Since this grading is clearly the only one to satisfy (\ref{pf.gtilde.a.1})
(because $\widetilde{\mathfrak{g}}$ is generated as a Lie algebra by $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$), this proves Theorem \ref{thm.gtilde} \textbf{(a)}.

\bigskip

\textbf{(b)} \underline{\textit{1st step: Definitions and identifications.}}

Let $N_{+}$ be the free vector space with basis $e_{1},e_{2},...,e_{n}$. Since
$\widetilde{\mathfrak{n}}_{+}=\operatorname*{FreeLie}\left(  e_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $, we then have a canonical
isomorphism $\widetilde{\mathfrak{n}}_{+}\cong\operatorname*{FreeLie}\left(
N_{+}\right)  $ (where $\operatorname*{FreeLie}\left(  N_{+}\right)  $ means
the free Lie algebra over the vector space (not the set) $N_{+}$). We identify
$\widetilde{\mathfrak{n}}_{+}$ with $\operatorname*{FreeLie}\left(
N_{+}\right)  $ along this isomorphism. Due to the construction of the free
Lie algebra, we have a canonical injection $N_{+}\rightarrow
\operatorname*{FreeLie}\left(  N_{+}\right)  =\widetilde{\mathfrak{n}}_{+}$.
We will regard this injection as an inclusion (so that $N_{+}\subseteq
\widetilde{\mathfrak{n}}_{+}$).

By Proposition \ref{prop.Ufree} (applied to $V=N_{+}$), there exists a
canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}\left(
N_{+}\right)  \right)  \rightarrow T\left(  N_{+}\right)  $. We identify
$U\left(  \widetilde{\mathfrak{n}}_{+}\right)  =U\left(
\operatorname*{FreeLie}\left(  N_{+}\right)  \right)  $ with $T\left(
N_{+}\right)  $ along this isomorphism.

Let $N_{-}$ be the free vector space with basis $f_{1},f_{2},...,f_{n}$. Since
$\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  f_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $, we then have a canonical
isomorphism $\widetilde{\mathfrak{n}}_{-}\cong\operatorname*{FreeLie}\left(
N_{-}\right)  $ (where $\operatorname*{FreeLie}\left(  N_{-}\right)  $ means
the free Lie algebra over the vector space (not the set) $N_{-}$). We identify
$\widetilde{\mathfrak{n}}_{-}$ with $\operatorname*{FreeLie}\left(
N_{-}\right)  $ along this isomorphism. Due to the construction of the free
Lie algebra, we have a canonical injection $N_{-}\rightarrow
\operatorname*{FreeLie}\left(  N_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$.
We will regard this injection as an inclusion (so that $N_{-}\subseteq
\widetilde{\mathfrak{n}}_{-}$).

By Proposition \ref{prop.Ufree} (applied to $V=N_{-}$), there exists a
canonical algebra isomorphism $U\left(  \operatorname*{FreeLie}\left(
N_{-}\right)  \right)  \rightarrow T\left(  N_{-}\right)  $. We identify
$U\left(  \widetilde{\mathfrak{n}}_{-}\right)  =U\left(
\operatorname*{FreeLie}\left(  N_{-}\right)  \right)  $ with $T\left(
N_{-}\right)  $ along this isomorphism.

A consequence of the Poincar\'{e}-Birkhoff-Witt theorem says that for any Lie
algebra $\mathfrak{i}$, the canonical map $\mathfrak{i}\rightarrow U\left(
\mathfrak{i}\right)  $ is injective. Thus, the canonical map
$\widetilde{\mathfrak{n}}_{+}\rightarrow U\left(  \widetilde{\mathfrak{n}}%
_{+}\right)  $ and the canonical map $\widetilde{\mathfrak{n}}_{-}\rightarrow
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ are injective. We will
therefore regard these maps are inclusions.

Let us identify the group $Q$ with $\mathbb{Z}^{n}$ by means of identifying
$\alpha_{i}$ with the column vector $e_{i}=\left(  \underbrace{0,0,...,0}%
_{i-1\text{ zeroes}},1,\underbrace{0,0,...,0}_{n-i\text{ zeroes}}\right)
^{T}$ for every $i\in\left\{  1,2,...,n\right\}  $. As a consequence, for
every $i\in\left\{  1,2,...,n\right\}  $, the row vector $e_{i}^{T}A$ is an
element of the group $\operatorname*{Hom}\left(  Q,\mathbb{Z}\right)  $ of
group homomorphisms from $Q$ to $\mathbb{Z}$. Thus, for every $w\in Q$ and
every $i\in\left\{  1,2,...,n\right\}  $, the product $e_{i}^{T}Aw$ is a
complex number.

\bigskip

\underline{\textit{2nd step: Defining a }$Q$\textit{-grading on }%
$\widetilde{\mathfrak{n}}_{-}$\textit{.}}

Let us define a $Q$-grading on the vector space $N_{-}$ by setting%
\[
\deg\left(  f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  .
\]
(This is well-defined since $\left(  f_{1},f_{2},...,f_{n}\right)  $ is a
basis of $N_{-}$.) Then, the free Lie algebra $\operatorname*{FreeLie}\left(
N_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$ canonically becomes a $Q$-graded
Lie algebra, and the grading on this Lie algebra also satisfies%
\[
\deg\left(  f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  .
\]
(This grading clearly makes the map $\iota_{-}$ graded. We will not use this
fact, however.) We will later use this grading to define certain derivations
$\eta_{1}$, $\eta_{2}$, $...$, $\eta_{n}$ of the Lie algebra
$\widetilde{\mathfrak{n}}_{-}$.

As for now, let us notice that the canonical algebra isomorphism $U\left(
\operatorname*{FreeLie}\left(  N_{-}\right)  \right)  \rightarrow T\left(
N_{-}\right)  $ which we have constructed above (using Proposition
\ref{prop.Ufree}) is a $Q$-graded algebra isomorphism (due to Proposition
\ref{prop.Ufree.gr}, applied to $V=N_{-}$). This ensures that our
identification of $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ with
$T\left(  N_{-}\right)  $ respects the $Q$-grading on the two vector spaces.

\bigskip

\underline{\textit{3rd step: Defining an }$\widetilde{\mathfrak{h}}%
$\textit{-module }$\widetilde{\mathfrak{n}}_{-}$\textit{.}}

For every $i\in\left\{  1,2,...,n\right\}  $, let us define a linear map
$\eta_{i}:\widetilde{\mathfrak{n}}_{-}\rightarrow\widetilde{\mathfrak{n}}_{-}$
by setting%
\begin{equation}
\left(  \eta_{i}\left(  x\right)  =\left(  e_{i}^{T}Aw\right)  \cdot
x\ \ \ \ \ \ \ \ \ \ \text{for every }w\in Q\text{ and every }x\in
\widetilde{\mathfrak{n}}_{-}\left[  w\right]  \right)  .
\label{pf.gtilde.b.eta.def}%
\end{equation}
This map $\eta_{i}$ is well-defined (because in order to define a linear map
from a $Q$-graded vector space, it is enough to define it linearly on every
homogeneous component) and graded (because it multiplies any homogeneous
element of $\widetilde{\mathfrak{n}}_{-}$ by a scalar). Actually, $\eta_{i}$
acts as a scalar on each homogeneous component of $\widetilde{\mathfrak{n}%
}_{-}$. Moreover, for every $i\in\left\{  1,2,...,n\right\}  $, Lemma
\ref{lem.deriv.grading} (applied to $s=e_{i}^{T}A$, $\mathfrak{n}%
=\widetilde{\mathfrak{n}}_{-}$ and $\eta=\eta_{i}$) yields that $\eta_{i}$ is
a derivation of Lie algebras. That is, $\eta_{i}\in\operatorname*{Der}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. One can directly see that%
\begin{equation}
\eta_{i}\left(  f_{j}\right)  =-a_{i,j}f_{j}\ \ \ \ \ \ \ \ \ \ \text{for any
}i\in\left\{  1,2,...,n\right\}  \text{ and }j\in\left\{  1,2,...,n\right\}
\label{pf.gtilde.b.eta.fj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gtilde.b.eta.fj}):} Let $i\in\left\{
1,2,...,n\right\}  $ and $j\in\left\{  1,2,...,n\right\}  $. By the definition
of our grading on $\widetilde{\mathfrak{n}}_{-}$, we have $\deg\left(
f_{j}\right)  =-\underbrace{\alpha_{j}}_{=e_{j}}=-e_{j}$, so that $f_{j}%
\in\widetilde{\mathfrak{n}}_{-}\left[  -e_{j}\right]  $. Hence,
(\ref{pf.gtilde.b.eta.def}) (applied to $x=f_{j}$ and $w=-\alpha_{j}$) yields
$\eta_{i}\left(  f_{j}\right)  =\left(  e_{i}^{T}A\left(  -e_{j}\right)
\right)  \cdot f_{j}=-\underbrace{\left(  e_{i}^{T}Ae_{j}\right)  }_{=a_{i,j}%
}\cdot f_{j}=-a_{i,j}f_{j}$. This proves (\ref{pf.gtilde.b.eta.fj}).}.

[Note that, while we defined the $\eta_{i}$ using the grading, there is also
an alternative way to define them, by applying Theorem
\ref{thm.universal.FreeLie.der}. Also, we will later identify $\eta_{i}$ as a
restriction of a certain map $\eta_{i}^{\prime}:U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ to $\widetilde{\mathfrak{n}}_{-}$.]

It is easy to see that
\begin{equation}
\left[  \eta_{i},\eta_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left\{  1,2,...,n\right\}  \text{ and }j\in\left\{  1,2,...,n\right\}
\label{pf.gtilde.b.eta.commute}%
\end{equation}
(since each of the maps $\eta_{i}$ and $\eta_{j}$ acts as scalars on each
homogeneous component of $\widetilde{\mathfrak{n}}_{-}$).

Define a linear map $\Xi:\widetilde{\mathfrak{h}}\rightarrow
\operatorname*{Der}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  \Xi\left(  h_{i}\right)  =\eta_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}  \right)
\]
(this map is well-defined, since $\left(  h_{1},h_{2},...,h_{n}\right)  $ is a
basis of $\widetilde{\mathfrak{h}}$). Then, $\Xi$ is a Lie algebra
homomorphism (this follows from (\ref{pf.gtilde.b.eta.commute})), and thus
makes $\widetilde{\mathfrak{n}}_{-}$ into an $\widetilde{\mathfrak{h}}$-module
on which $\widetilde{\mathfrak{h}}$ acts by derivations. Thus, a Lie algebra
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$ is well-defined
(according to Definition \ref{def.semidir.lielie}). Both Lie algebras
$\widetilde{\mathfrak{h}}$ and $\widetilde{\mathfrak{n}}_{-}$ canonically
inject (by Lie algebra homomorphisms) into this Lie algebra
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$, and therefore
will be considered as Lie subalgebras of $\widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}$.

We define a $Q$-grading on $\widetilde{\mathfrak{h}}$ by letting every element
of $\widetilde{\mathfrak{h}}$ lie in the $0$-th homogeneous component. Then,
it is easy to see that $\widetilde{\mathfrak{n}}_{-}$ is a $Q$-graded
$\widetilde{\mathfrak{h}}$-module, so that the Lie algebra
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$ canonically
obtains a $Q$-grading, and both graded Lie algebras $\widetilde{\mathfrak{h}}$
and $\widetilde{\mathfrak{n}}_{-}$ become $Q$-graded Lie subalgebras of
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$.

Since $\widetilde{\mathfrak{n}}_{-}$ is a $Q$-graded Lie subalgebra of
$\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$, the universal
enveloping algebra $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ is a
$Q$-graded subalgebra of $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $. This makes $U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ into a
$Q$-graded $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $-bimodule. Since
$U\left(  \widetilde{\mathfrak{n}}_{-}\right)  =T\left(  N_{-}\right)  $, this
means that $U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $ is a $Q$-graded $T\left(  N_{-}\right)  $-bimodule.

Let us notice that in the Lie algebra $\widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}$, every $i\in\left\{  1,2,...,n\right\}  $ and
$j\in\left\{  1,2,...,n\right\}  $ satisfy%
\begin{align}
\left[  h_{i},f_{j}\right]   &  =h_{i}\rightharpoonup f_{j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{where }\rightharpoonup\text{ denotes the
action of }\widetilde{\mathfrak{h}}\text{ on }\widetilde{\mathfrak{n}}%
_{-}\right) \nonumber\\
&  =\underbrace{\left(  \Xi\left(  h_{i}\right)  \right)  }_{=\eta_{i}}\left(
f_{j}\right)  =\eta_{i}\left(  f_{j}\right)  =-a_{i,j}f_{j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.gtilde.b.eta.fj})}\right)  .
\label{pf.gtilde.b.semidir.ij}%
\end{align}
From (\ref{nonserre-relations}), we see that the same relation is satisfied in
the Lie algebra $\widetilde{\mathfrak{g}}$.

\bigskip

\underline{\textit{4th step: Defining an action of }$\widetilde{\mathfrak{g}}$
\textit{on }$U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $\textit{.}}

We are going to construct an action of the Lie algebra
$\widetilde{\mathfrak{g}}$ on $U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ (but not by derivations). First,
let us define some further maps.

Let $\iota_{N_{-}}^{T}:N_{-}\rightarrow T\left(  N_{-}\right)  $ be the
canonical inclusion map. Notice that we are regarding $\iota_{N_{-}}^{T}$ as
an inclusion.

For every $i\in\left\{  1,2,...,n\right\}  $, let $\varepsilon_{i}^{\prime
}:N_{-}\rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ be the linear map defined by%
\[
\left(  \varepsilon_{i}^{\prime}\left(  f_{j}\right)  =\delta_{i,j}%
h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,...,n\right\}
\right)
\]
(this map is well-defined, since $\left(  f_{1},f_{2},...,f_{n}\right)  $ is a
basis of $N_{-}$).

For every $i\in\left\{  1,2,...,n\right\}  $, there exists a unique $Q$-graded
derivation\footnote{Here, "derivation", we mean a derivation of algebras, not
of Lie algebras.} $F:T\left(  N_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $
satisfying $\varepsilon_{i}^{\prime}=F\circ\iota_{N_{-}}^{T}$ (according to
Theorem \ref{thm.universal.tensor.der.gr}, applied to $V=N_{-}$, $M=U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ and
$f=\varepsilon_{i}^{\prime}$). Denote this $Q$-graded derivation by
$\varepsilon_{i}$. Thus, for every $i\in\left\{  1,2,...,n\right\}  $, the map
$\varepsilon_{i}:T\left(  N_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ is a
$Q$-graded derivation satisfying $\varepsilon_{i}^{\prime}=\varepsilon
_{i}\circ\iota_{N_{-}}^{T}$. Since $T\left(  N_{-}\right)  =U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $, this map $\varepsilon_{i}$ is thus a
$Q$-graded derivation $U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $. Clearly, every $i\in\left\{  1,2,...,n\right\}  $ and
$j\in\left\{  1,2,...,n\right\}  $ satisfy%
\begin{equation}
\varepsilon_{i}\left(  f_{j}\right)  =\delta_{i,j}h_{i}
\label{pf.gtilde.b.epsilon.fj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gtilde.b.epsilon.fj}):} Let $i\in\left\{
1,2,...,n\right\}  $ and $j\in\left\{  1,2,...,n\right\}  $. Then,
$\iota_{N_{-}}^{T}\left(  f_{j}\right)  =f_{j}$ (since we regard $\iota
_{N_{-}}^{T}$ as an inclusion). But since $\varepsilon_{i}^{\prime
}=\varepsilon_{i}\circ\iota_{N_{-}}^{T}$, we have $\varepsilon_{i}^{\prime
}\left(  f_{j}\right)  =\left(  \varepsilon_{i}\circ\iota_{N_{-}}^{T}\right)
\left(  f_{j}\right)  =\varepsilon_{i}\left(  \underbrace{\iota_{N_{-}}%
^{T}\left(  f_{j}\right)  }_{=f_{j}}\right)  =\varepsilon_{i}\left(
f_{j}\right)  $. Thus, $\varepsilon_{i}\left(  f_{j}\right)  =\varepsilon
_{i}^{\prime}\left(  f_{j}\right)  =\delta_{i,j}h_{i}$. This proves
(\ref{pf.gtilde.b.epsilon.fj}).}.

Let $\rho:U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(
\widetilde{\mathfrak{h}}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ be the vector space
homomorphism defined by%
\[
\rho\left(  \alpha\otimes\beta\right)  =\alpha\beta
\ \ \ \ \ \ \ \ \ \ \text{for all }\alpha\in U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \text{ and }\beta\in U\left(  \widetilde{\mathfrak{h}}\right)
\]
(this is clearly well-defined). Since $\widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}=\widetilde{\mathfrak{n}}_{-}\otimes
\widetilde{\mathfrak{h}}$ as vector spaces, Corollary \ref{cor.U(X)U} (applied
to $\mathfrak{c}=\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}$,
$\mathfrak{a}=\widetilde{\mathfrak{n}}_{-}$ and $\mathfrak{b}%
=\widetilde{\mathfrak{h}}$) yields that $\rho$ is an isomorphism of filtered
vector spaces, of left $U\left(  \widetilde{\mathfrak{n}}_{-}\right)
$-modules and of right $U\left(  \widetilde{\mathfrak{h}}\right)  $-modules.
Also, it is clear that $\rho$ is $Q$-graded. Since the inverse of any
$Q$-graded isomorphism is again $Q$-graded, this yields that $\rho^{-1}$ is
$Q$-graded (because $\rho$ is a $Q$-graded isomorphism). Thus, $\rho$ is an
isomorphism in the category of $Q$-graded vector spaces.

Let $\mu:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \otimes U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ be the
multiplication map of the algebra $U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $.

For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$E_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by $E_{i}=\mu\circ\left(
\varepsilon_{i}\otimes\operatorname*{id}\right)  \circ\rho^{-1}$. Then,
$E_{i}$ is a right $U\left(  \widetilde{\mathfrak{h}}\right)  $-module
homomorphism (because all of $\mu$, $\varepsilon_{i}\otimes\operatorname*{id}$
and $\rho^{-1}$ are right $U\left(  \widetilde{\mathfrak{h}}\right)  $-module
homomorphisms) and a $Q$-graded map (since all of $\mu$, $\varepsilon_{i}$,
$\operatorname*{id}$ and $\rho^{-1}$ are $Q$-graded). Also, every $u_{-}\in
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  $ satisfy%
\begin{equation}
E_{i}\left(  u_{-}u_{0}\right)  =\varepsilon_{i}\left(  u_{-}\right)  u_{0}
\label{pf.gtilde.b.Ei}%
\end{equation}
\footnote{\textit{Proof.} Let $u_{-}\in U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $ and $u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $.
Since $E_{i}=\mu\circ\left(  \varepsilon_{i}\otimes\operatorname*{id}\right)
\circ\rho^{-1}$, we have
\begin{align*}
E_{i}\left(  u_{-}u_{0}\right)   &  =\left(  \mu\circ\left(  \varepsilon
_{i}\otimes\operatorname*{id}\right)  \circ\rho^{-1}\right)  \left(
u_{-}u_{0}\right)  =\left(  \mu\circ\left(  \varepsilon_{i}\otimes
\operatorname*{id}\right)  \right)  \left(  \underbrace{\rho^{-1}\left(
u_{-}u_{0}\right)  }_{\substack{=u_{-}\otimes u_{0}\\\text{(since the
definition of }\rho\text{ yields}\\\rho\left(  u_{-}\otimes u_{0}\right)
=u_{-}u_{0}\text{)}}}\right) \\
&  =\left(  \mu\circ\left(  \varepsilon_{i}\otimes\operatorname*{id}\right)
\right)  \left(  u_{-}\otimes u_{0}\right)  =\mu\left(  \underbrace{\left(
\varepsilon_{i}\otimes\operatorname*{id}\right)  \left(  u_{-}\otimes
u_{0}\right)  }_{=\varepsilon_{i}\left(  u_{-}\right)  \otimes u_{0}}\right)
=\mu\left(  \varepsilon_{i}\left(  u_{-}\right)  \otimes u_{0}\right)
=\varepsilon_{i}\left(  u_{-}\right)  u_{0}%
\end{align*}
(since $\mu$ is the multiplication map), qed.}.

For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$F_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  F_{i}\left(  u\right)  =f_{i}u\ \ \ \ \ \ \ \ \ \ \text{for every
}u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  .
\]
Clearly, $F_{i}$ is a right $U\left(  \widetilde{\mathfrak{h}}\right)
$-module homomorphism.

For every $i\in\left\{  1,2,...,n\right\}  $, define a linear map
$H_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  H_{i}\left(  u\right)  =h_{i}u\ \ \ \ \ \ \ \ \ \ \text{for every
}u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  .
\]
Clearly, $H_{i}$ is a right $U\left(  \widetilde{\mathfrak{h}}\right)
$-module homomorphism.

Our next goal is to prove the relations%
\begin{equation}
\left\{
\begin{array}
[c]{l}%
\left[  H_{i},H_{j}\right]  =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}\ \ \ \ \ \ \ \ \ \ \text{for all
}i,j\in\left\{  1,2,...,n\right\}  ;\\
\left[  E_{i},F_{j}\right]  =\delta_{i,j}H_{i}\ \ \ \ \ \ \ \ \ \ \text{for
all }i,j\in\left\{  1,2,...,n\right\}
\end{array}
\right.  \label{pf.gtilde.NONSERRE}%
\end{equation}
in $\operatorname*{End}\left(  U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $. Once these relations
are proven, it will follow that a Lie algebra homomorphism
$\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}\left(  U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $
mapping $h_{i}$, $e_{i}$, $f_{i}$ to $H_{i}$, $E_{i}$, $F_{i}$ for all $i$
exists (and is unique), and this map will make $U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ into a
$\widetilde{\mathfrak{g}}$-module. This $\widetilde{\mathfrak{g}}$-module
structure will then yield Theorem \ref{thm.gtilde} \textbf{(b)} by a rather
simple argument. But we must first verify (\ref{pf.gtilde.NONSERRE}).

\bigskip

\underline{\textit{5th step: Verifying the relations (\ref{pf.gtilde.NONSERRE}%
).}}

We will verify the four relations (\ref{pf.gtilde.NONSERRE}) one after the other:

\bigskip

\textit{Proof of the relation }$\left[  H_{i},H_{j}\right]  =0$ \textit{for
all } $i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Every $u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $
satisfies%
\begin{align*}
\underbrace{\left[  H_{i},H_{j}\right]  }_{=H_{i}\circ H_{j}-H_{j}\circ H_{i}%
}u  &  =\left(  H_{i}\circ H_{j}-H_{j}\circ H_{i}\right)  \left(  u\right)
=H_{i}\underbrace{\left(  H_{j}u\right)  }_{\substack{=h_{j}u\\\text{(by the
definition}\\\text{of }H_{j}\text{)}}}-H_{j}\underbrace{\left(  H_{i}u\right)
}_{\substack{=h_{i}u\\\text{(by the definition}\\\text{of }H_{i}\text{)}}}\\
&  =\underbrace{H_{i}\left(  h_{j}u\right)  }_{\substack{=h_{i}\left(
h_{j}u\right)  \\\text{(by the definition}\\\text{of }H_{i}\text{)}%
}}-\underbrace{H_{j}\left(  h_{i}u\right)  }_{\substack{=h_{j}\left(
h_{i}u\right)  \\\text{(by the definition}\\\text{of }H_{j}\text{)}}}\\
&  =h_{i}\left(  h_{j}u\right)  -h_{j}\left(  h_{i}u\right)
=\underbrace{\left(  h_{i}h_{j}-h_{j}h_{i}\right)  }_{\substack{=\left[
h_{i},h_{j}\right]  =0\\\text{in }U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \\\text{(since }\left[
h_{i},h_{j}\right]  =0\text{ in }\widetilde{\mathfrak{h}}\text{)}}}u=0.
\end{align*}
Thus, $\left[  H_{i},H_{j}\right]  =0$.

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  H_{i},H_{j}\right]  =0$ for all $i,j\in\left\{  1,2,...,n\right\}  $.

\bigskip

\textit{Proof of the relation }$\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$
\textit{for all } $i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

This will be the most difficult among the four relations that we must prove.

Let us consider $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ as
$\widetilde{\mathfrak{n}}_{-}$-module via the adjoint action. Then,
$\widetilde{\mathfrak{n}}_{-}\subseteq U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $ as $\widetilde{\mathfrak{n}}_{-}$-modules.

Let $i$ be any element of $\left\{  1,2,...,n\right\}  $. Define a map
$\zeta_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ by%
\[
\left(  \zeta_{i}\left(  u\right)  =\left[  h_{i},u\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  .
\]
Clearly, $\zeta_{i}$ is a derivation of algebras.

We are going to prove that $\zeta_{i}\left(  U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)
$.

\textit{Proof of }$\zeta_{i}\left(  U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)
$\textit{:}

We know that $\zeta_{i}$ is a derivation of algebras. Thus, $\zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }$ is a derivation of
algebras from $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ to $U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $.

By Theorem \ref{thm.universal.tensor.der} (applied to $\eta_{i}\mid_{N_{-}}$,
$T\left(  N_{-}\right)  $ and $N_{-}$ instead of $f$, $B$ and $V$), we see
that there exists a unique derivation\footnote{Here, "derivation" means
"derivation of algebras", not "derivation of Lie algebras".} $F:T\left(
N_{-}\right)  \rightarrow T\left(  N_{-}\right)  $ satisfying $\eta_{i}%
\mid_{N_{-}}=F\circ\iota_{N_{-}}^{T}$\ \ \ \ \footnote{Here, we are regarding
$\eta_{i}\mid_{N_{-}}$ as a map from $N_{-}$ to $T\left(  N_{-}\right)  $
because $\eta_{i}\mid_{N_{-}}$ is a map from $N_{-}$ to
$\widetilde{\mathfrak{n}}_{-}\subseteq U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  =T\left(  N_{-}\right)  $.}. Denote this derivation by $\eta
_{i}^{\prime}$. Then, $\eta_{i}\mid_{N_{-}}=\eta_{i}^{\prime}\circ\iota
_{N_{-}}^{T}$. Since $T\left(  N_{-}\right)  =U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, this $\eta_{i}^{\prime}$ is a derivation from $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ to $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $. Hence, any $u\in\widetilde{\mathfrak{n}}_{-}$ and
$v\in\widetilde{\mathfrak{n}}_{-}$ satisfy%
\begin{align*}
\left(  \eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}\right)  \left(
\left[  u,v\right]  \right)   &  =\eta_{i}^{\prime}\left(  \underbrace{\left[
u,v\right]  }_{=uv-vu}\right)  =\eta_{i}^{\prime}\left(  uv-vu\right)
=\underbrace{\eta_{i}^{\prime}\left(  uv\right)  }_{\substack{=\eta
_{i}^{\prime}\left(  u\right)  v+u\eta_{i}^{\prime}\left(  v\right)
\\\text{(since }\eta_{i}^{\prime}\text{ is a derivation)}}}-\underbrace{\eta
_{i}^{\prime}\left(  vu\right)  }_{\substack{=\eta_{i}^{\prime}\left(
v\right)  u+v\eta_{i}^{\prime}\left(  u\right)  \\\text{(since }\eta
_{i}^{\prime}\text{ is a derivation)}}}\\
&  =\left(  \eta_{i}^{\prime}\left(  u\right)  v+u\eta_{i}^{\prime}\left(
v\right)  \right)  -\left(  \eta_{i}^{\prime}\left(  v\right)  u+v\eta
_{i}^{\prime}\left(  u\right)  \right) \\
&  =\underbrace{\left(  \eta_{i}^{\prime}\left(  u\right)  v-v\eta_{i}%
^{\prime}\left(  u\right)  \right)  }_{=\left[  \eta_{i}^{\prime}\left(
u\right)  ,v\right]  }+\underbrace{\left(  u\eta_{i}^{\prime}\left(  v\right)
-\eta_{i}^{\prime}\left(  v\right)  u\right)  }_{=\left[  u,\eta_{i}^{\prime
}\left(  v\right)  \right]  }\\
&  =\left[  \underbrace{\eta_{i}^{\prime}\left(  u\right)  }_{=\left(
\eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}\right)  \left(  u\right)
},v\right]  +\left[  u,\underbrace{\eta_{i}^{\prime}\left(  v\right)
}_{=\left(  \eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}\right)
\left(  v\right)  }\right] \\
&  =\left[  \left(  \eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}%
}\right)  \left(  u\right)  ,v\right]  +\left[  u,\left(  \eta_{i}^{\prime
}\mid_{\widetilde{\mathfrak{n}}_{-}}\right)  \left(  v\right)  \right]
\end{align*}
in $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $. Therefore, the map
$\eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}:\widetilde{\mathfrak{n}%
}_{-}\rightarrow U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ is a
$1$-cocycle (since $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ is an
$\widetilde{\mathfrak{n}}_{-}$-module via the adjoint action).

On the other hand, we know that $\eta_{i}:\widetilde{\mathfrak{n}}%
_{-}\rightarrow\widetilde{\mathfrak{n}}_{-}$ is a derivation of Lie algebras,
i. e., a $1$-cocycle $\widetilde{\mathfrak{n}}_{-}\rightarrow
\widetilde{\mathfrak{n}}_{-}$. Since $\widetilde{\mathfrak{n}}_{-}\subseteq
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ as $\widetilde{\mathfrak{n}%
}_{-}$-modules, this yields that we can regard $\eta_{i}$ as a $1$-cocycle
$\widetilde{\mathfrak{n}}_{-}\rightarrow U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  $.

In the algebra $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $, every
$k\in\left\{  1,2,...,n\right\}  $ satisfies%
\begin{align*}
\left(  \left(  \eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}\right)
\mid_{N_{-}}\right)  \left(  f_{k}\right)   &  =\eta_{i}^{\prime}\left(
f_{k}\right)  =\eta_{i}^{\prime}\left(  \iota_{N_{-}}^{T}\left(  f_{k}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because }\iota_{N_{-}}^{T}\left(  f_{k}\right)  =f_{k}\text{ (since we
are regarding }\iota_{N_{-}}^{T}\text{ as an inclusion map),}\\
\text{so that }f_{k}=\iota_{N_{-}}^{T}\left(  f_{k}\right)
\end{array}
\right) \\
&  =\underbrace{\left(  \eta_{i}^{\prime}\circ\iota_{N_{-}}^{T}\right)
}_{=\eta_{i}\mid_{N_{-}}}\left(  f_{k}\right)  =\left(  \eta_{i}\mid_{N_{-}%
}\right)  \left(  f_{k}\right)  =\eta_{i}\left(  f_{k}\right)
\end{align*}
In other words, the maps $\left(  \eta_{i}^{\prime}\mid
_{\widetilde{\mathfrak{n}}_{-}}\right)  \mid_{N_{-}}$ and $\eta_{i}\mid
_{N_{-}}$ are equal to each other on each of the elements $f_{1}$, $f_{2}$,
$...$, $f_{n}$ of $N_{-}$. Since $\left(  f_{1},f_{2},...,f_{n}\right)  $ is a
basis of $N_{-}$, this yields that $\left(  \eta_{i}^{\prime}\mid
_{\widetilde{\mathfrak{n}}_{-}}\right)  \mid_{N_{-}}=\eta_{i}\mid_{N_{-}}$
(because the maps $\left(  \eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}%
_{-}}\right)  \mid_{N_{-}}$ and $\eta_{i}\mid_{N_{-}}$ are linear). Since the
set $N_{-}$ generates $\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra (because
$\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  N_{-}\right)  $),
Proposition \ref{prop.derivation.Lie.unique} (applied to
$\widetilde{\mathfrak{n}}_{-}$, $N_{-}$, $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, $\eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}$ and
$\eta_{i}$ instead of $\mathfrak{g}$, $S$, $M$, $d$ and $e$) yields that
$\eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}=\eta_{i}$ (because both
$\eta_{i}^{\prime}\mid_{\widetilde{\mathfrak{n}}_{-}}$ and $\eta_{i}$ are
$1$-cocycles from $\widetilde{\mathfrak{n}}_{-}$ to $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $).

In the algebra $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $, every $k\in\left\{  1,2,...,n\right\}
$ satisfies%
\begin{align*}
\left(  \eta_{i}^{\prime}\mid_{N_{-}}\right)  \left(  f_{k}\right)   &
=\eta_{i}^{\prime}\left(  f_{k}\right)  =\underbrace{\left(  \eta_{i}^{\prime
}\mid_{\widetilde{\mathfrak{n}}_{-}}\right)  }_{=\eta_{i}}\left(
f_{k}\right)  =\eta_{i}\left(  f_{k}\right)  =-a_{i,k}f_{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.gtilde.b.eta.fj}), applied to
}k\text{ instead of }j\right) \\
&  =\left[  h_{i},f_{k}\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
(\ref{pf.gtilde.b.semidir.ij}) (applied to }k\text{ instead of }j\text{)
yields }\left[  h_{i},f_{k}\right]  =\eta_{i}\left(  f_{k}\right)  \right) \\
&  =\zeta_{i}\left(  f_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\zeta_{i}\left(  f_{k}\right)  =\left[  h_{i},f_{k}\right]  \text{ (by the
definition of }\zeta_{k}\text{)}\right) \\
&  =\left(  \left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  }\right)  \mid_{N_{-}}\right)  \left(  f_{k}\right)  .
\end{align*}
In other words, the maps $\eta_{i}^{\prime}\mid_{N_{-}}$ and $\left(
\zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)
\mid_{N_{-}}$ are equal to each other on each of the elements $f_{1}$, $f_{2}%
$, $...$, $f_{n}$ of $N_{-}$. Since $\left(  f_{1},f_{2},...,f_{n}\right)  $
is a basis of $N_{-}$, this yields that $\eta_{i}^{\prime}\mid_{N_{-}}=\left(
\zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)
\mid_{N_{-}}$ (because the maps $\eta_{i}^{\prime}\mid_{N_{-}}$ and $\left(
\zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)
\mid_{N_{-}}$ are linear). Since the set $N_{-}$ generates $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ as an algebra (because $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  =T\left(  N_{-}\right)  $), Proposition
\ref{prop.derivation.unique} (applied to $U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, $N_{-}$, $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $, $\eta_{i}^{\prime}$ and $\zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }$ instead of $A$, $S$,
$M$, $d$ and $e$) yields that%
\[
\eta_{i}^{\prime}=\zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)
}%
\]
(because both $\eta_{i}^{\prime}$ and $\zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }$ are derivations from $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ to $U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $). Thus, every $u_{-}\in
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ satisfies%
\[
\eta_{i}^{\prime}\left(  u_{-}\right)  =\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  \left(  u_{-}\right)
=\zeta_{i}\left(  u_{-}\right)  =\left[  h_{i},u_{-}\right]
\]
(by the definition of $\zeta_{i}$). Moreover, $\zeta_{i}\left(  U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right)  =\underbrace{\left(  \zeta
_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  }_{=\eta
_{i}^{\prime}}\left(  U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \right)
=\eta_{i}^{\prime}\left(  U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ (since
$\eta_{i}^{\prime}$ is a map $U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\rightarrow U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $). We have thus
proven $\zeta_{i}\left(  U\left(  \widetilde{\mathfrak{n}}_{-}\right)
\right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $.

Now, let $j\in\left\{  1,2,...,n\right\}  $ be arbitrary. Recall that
$\zeta_{i}:U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ and $\varepsilon_{j}:U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \rightarrow U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ are
derivations satisfying $\zeta_{i}\left(  U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \right)  \subseteq U\left(  \widetilde{\mathfrak{n}}_{-}\right)
$. Thus, Proposition \ref{prop.commutator.derivs.alg.2} (applied to $U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $, $U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $, $\varepsilon_{j}$ and
$\zeta_{i}$ instead of $A$, $B$, $f$ and $g$) yields that $\varepsilon
_{j}\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)
}\right)  -\zeta_{i}\circ\varepsilon_{j}:U\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is a derivation (of algebras).

On the other hand, $-a_{i,j}\varepsilon_{j}:U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  \rightarrow U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ is also a derivation (of algebras),
since $\varepsilon_{j}$ is a derivation.

We will now prove that%
\begin{equation}
\varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon_{j}=-a_{i,j}\varepsilon
_{j}. \label{pf.gtilde.dercomm1}%
\end{equation}
\footnote{Note that the term $\varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  $ in this
equality is well-defined because $\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  \left(  U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right)  =\zeta_{i}\left(  U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \right)  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $.} This will bring us very close to the
proof of the relation $\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$.

\textit{Proof of (\ref{pf.gtilde.dercomm1}):} Every $k\in\left\{
1,2,...,n\right\}  $ satisfies%
\begin{align*}
&  \left(  \left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}\right)  \left(  f_{k}\right) \\
&  =\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \left(  f_{k}\right) \\
&  =\varepsilon_{j}\left(  \underbrace{\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  \left(  f_{k}\right)
}_{\substack{=\zeta_{i}\left(  f_{k}\right)  =\left[  h_{i},f_{k}\right]
\\\text{(by the definition of }\zeta_{i}\text{)}}}\right)  -\underbrace{\zeta
_{i}\left(  \varepsilon_{j}\left(  f_{k}\right)  \right)  }%
_{\substack{=\left[  h_{i},\varepsilon_{j}\left(  f_{k}\right)  \right]
\\\text{(by the definition of }\zeta_{i}\text{)}}}\\
&  =\varepsilon_{j}\left(  \underbrace{\left[  h_{i},f_{k}\right]
}_{\substack{=-a_{i,k}f_{k}\\\text{(by (\ref{pf.gtilde.b.semidir.ij}), applied
to}\\k\text{ instead of }j\text{)}}}\right)  -\left[  h_{i}%
,\underbrace{\varepsilon_{j}\left(  f_{k}\right)  }_{\substack{=\delta
_{j,k}h_{j}\\\text{(by (\ref{pf.gtilde.b.epsilon.fj}), applied to}\\j\text{
and }k\text{ instead of }i\text{ and }j\text{)}}}\right] \\
&  =\underbrace{\varepsilon_{j}\left(  -a_{i,k}f_{k}\right)  }_{=-a_{i,k}%
\varepsilon_{j}\left(  f_{k}\right)  }-\underbrace{\left[  h_{i},\delta
_{j,k}h_{j}\right]  }_{\substack{=0\\\text{(since }\widetilde{\mathfrak{h}%
}\text{ is an abelian Lie algebra)}}}=-a_{i,k}\varepsilon_{j}\left(
f_{k}\right)  =\left(  \left(  -a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}%
}\right)  \left(  f_{k}\right)  .
\end{align*}
In other words, the maps $\left(  \varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}%
\circ\varepsilon_{j}\right)  \mid_{N_{-}}$ and $\left(  -a_{i,j}%
\varepsilon_{j}\right)  \mid_{N_{-}}$ are equal to each other on each of the
elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of $N_{-}$. Since $\left(
f_{1},f_{2},...,f_{n}\right)  $ is a basis of $N_{-}$, this yields that
$\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \mid_{N_{-}}=\left(  -a_{i,j}\varepsilon_{j}\right)  \mid_{N_{-}%
}$ (because the maps $\left(  \varepsilon_{j}\circ\left(  \zeta_{i}%
\mid_{U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}%
\circ\varepsilon_{j}\right)  \mid_{N_{-}}$ and $\left(  -a_{i,j}%
\varepsilon_{j}\right)  \mid_{N_{-}}$ are linear). Since the set $N_{-}$
generates $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ as an algebra
(because $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  =T\left(
N_{-}\right)  $), Proposition \ref{prop.derivation.unique} (applied to
$U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $, $N_{-}$, $U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $,
$\varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon_{j}$ and $-a_{i,j}%
\varepsilon_{j}$ instead of $A$, $S$, $M$, $d$ and $e$) yields that
$\varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon_{j}=-a_{i,j}\varepsilon
_{j}$ (since $\varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon_{j}$
and $-a_{i,j}\varepsilon_{j}$ are derivations). This proves
(\ref{pf.gtilde.dercomm1}).

Now, we will show that
\begin{equation}
\left[  h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  =a_{i,j}\varepsilon
_{j}\left(  u_{-}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }u_{-}\in
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  . \label{pf.gtilde.dercomm2}%
\end{equation}


\textit{Proof of (\ref{pf.gtilde.dercomm2}):} Let $u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. Then,%
\begin{align*}
&  \left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  \left(  u_{-}\right) \\
&  =\varepsilon_{j}\left(  \underbrace{\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  \left(  u_{-}\right)
}_{\substack{=\zeta_{i}\left(  u_{-}\right)  =\left[  h_{i},u_{-}\right]
\\\text{(by the definition of }\zeta_{i}\text{)}}}\right)  -\underbrace{\zeta
_{i}\left(  \varepsilon_{j}\left(  u_{-}\right)  \right)  }%
_{\substack{=\left[  h_{i},u_{-}\right]  \\\text{(by the definition of }%
\zeta_{i}\text{)}}}\\
&  =\varepsilon_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  -\left[
h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  .
\end{align*}
Comparing this with%
\[
\underbrace{\left(  \varepsilon_{j}\circ\left(  \zeta_{i}\mid_{U\left(
\widetilde{\mathfrak{n}}_{-}\right)  }\right)  -\zeta_{i}\circ\varepsilon
_{j}\right)  }_{\substack{=-a_{i,j}\varepsilon_{j}\\\text{(by
(\ref{pf.gtilde.dercomm1}))}}}\left(  u_{-}\right)  =-a_{i,j}\varepsilon
_{j}\left(  u_{-}\right)  ,
\]
we obtain $-a_{i,j}\varepsilon_{j}\left(  u_{-}\right)  =\varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  -\left[  h_{i},\varepsilon
_{j}\left(  u_{-}\right)  \right]  $. In other words, $\left[  h_{i}%
,\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  =a_{i,j}\varepsilon_{j}\left(  u_{-}\right)  $.
This proves (\ref{pf.gtilde.dercomm2}).

Now, let us finally prove that $\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$.

Indeed, let $u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and
$u_{0}\in U\left(  \widetilde{\mathfrak{h}}\right)  $. Then, $\zeta_{i}\left(
u_{-}\right)  =\left[  h_{i},u_{-}\right]  $ (by the definition of $\zeta_{i}%
$), so that $\left[  h_{i},u_{-}\right]  =\zeta_{i}\left(  \underbrace{u_{-}%
}_{\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  }\right)  \in\zeta
_{i}\left(  U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \right)  \subseteq
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $. Thus, (\ref{pf.gtilde.b.Ei})
(applied to $\left[  h_{i},u_{-}\right]  $ and $j$ instead of $u_{-}$ and $i$)
yields%
\begin{equation}
E_{j}\left(  \left[  h_{i},u_{-}\right]  u_{0}\right)  =\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  u_{0}. \label{pf.gtilde.dercomm3}%
\end{equation}
On the other hand, $\underbrace{h_{i}}_{\in\widetilde{\mathfrak{h}}%
}\underbrace{u_{0}}_{\in U\left(  \widetilde{\mathfrak{h}}\right)  }%
\in\widetilde{\mathfrak{h}}U\left(  \widetilde{\mathfrak{h}}\right)  \subseteq
U\left(  \widetilde{\mathfrak{h}}\right)  $. Hence, (\ref{pf.gtilde.b.Ei})
(applied to $h_{i}u_{0}$ and $j$ instead of $u_{0}$ and $i$) yields%
\begin{equation}
E_{j}\left(  u_{-}h_{i}u_{0}\right)  =\varepsilon_{j}\left(  u_{-}\right)
h_{i}u_{0}. \label{pf.gtilde.dercomm4}%
\end{equation}
But $\underbrace{h_{i}u_{-}}_{=\left[  h_{i},u_{-}\right]  +u_{-}h_{i}}%
u_{0}=\left[  h_{i},u_{-}\right]  u_{0}+u_{-}h_{i}u_{0}$, so that
\begin{align}
E_{j}\left(  h_{i}u_{-}u_{0}\right)   &  =E_{j}\left(  \left[  h_{i}%
,u_{-}\right]  u_{0}+u_{-}h_{i}u_{0}\right)  =\underbrace{E_{j}\left(  \left[
h_{i},u_{-}\right]  u_{0}\right)  }_{\substack{=\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  u_{0}\\\text{(by (\ref{pf.gtilde.dercomm3}))}%
}}+\underbrace{E_{j}\left(  u_{-}h_{i}u_{0}\right)  }_{\substack{=\varepsilon
_{j}\left(  u_{-}\right)  h_{i}u_{0}\\\text{(by (\ref{pf.gtilde.dercomm4}))}%
}}\nonumber\\
&  =\varepsilon_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  u_{0}%
+\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}. \label{pf.gtilde.dercomm5}%
\end{align}


On the other hand, $\rho\left(  u_{-}\otimes u_{0}\right)  =u_{-}u_{0}$ (by
the definition of $\rho$), and%
\begin{align*}
&  \left(  \left[  H_{i},E_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right) \\
&  =\underbrace{\left[  H_{i},E_{j}\right]  }_{=H_{i}\circ E_{j}-E_{j}\circ
H_{i}}\left(  \underbrace{\rho\left(  u_{-}\otimes u_{0}\right)  }%
_{=u_{-}u_{0}}\right)  =\left(  H_{i}\circ E_{j}-E_{j}\circ H_{i}\right)
\left(  u_{-}u_{0}\right) \\
&  =H_{i}\left(  \underbrace{E_{j}\left(  u_{-}u_{0}\right)  }%
_{\substack{=\varepsilon_{j}\left(  u_{-}\right)  u_{0}\\\text{(by
(\ref{pf.gtilde.b.Ei}), applied}\\\text{to }j\text{ instead of }i\text{)}%
}}\right)  -E_{j}\left(  \underbrace{H_{i}\left(  u_{-}u_{0}\right)
}_{\substack{=h_{i}u_{-}u_{0}\\\text{(by the definition of }H_{i}\text{)}%
}}\right) \\
&  =\underbrace{H_{i}\left(  \varepsilon_{j}\left(  u_{-}\right)
u_{0}\right)  }_{\substack{=h_{i}\varepsilon_{j}\left(  u_{-}\right)
u_{0}\\\text{(by the definition of }H_{i}\text{)}}}-\underbrace{E_{j}\left(
h_{i}u_{-}u_{0}\right)  }_{\substack{=\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  u_{0}+\varepsilon_{j}\left(  u_{-}\right)
h_{i}u_{0}\\\text{(by (\ref{pf.gtilde.dercomm5}))}}}\\
&  =h_{i}\varepsilon_{j}\left(  u_{-}\right)  u_{0}-\left(  \varepsilon
_{j}\left(  \left[  h_{i},u_{-}\right]  \right)  u_{0}+\varepsilon_{j}\left(
u_{-}\right)  h_{i}u_{0}\right)  =h_{i}\varepsilon_{j}\left(  u_{-}\right)
u_{0}-\varepsilon_{j}\left(  u_{-}\right)  h_{i}u_{0}-\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  u_{0}\\
&  =\left(  \underbrace{h_{i}\varepsilon_{j}\left(  u_{-}\right)
-\varepsilon_{j}\left(  u_{-}\right)  h_{i}}_{=\left[  h_{i},\varepsilon
_{j}\left(  u_{-}\right)  \right]  }-\varepsilon_{j}\left(  \left[
h_{i},u_{-}\right]  \right)  \right)  u_{0}=\underbrace{\left(  \left[
h_{i},\varepsilon_{j}\left(  u_{-}\right)  \right]  -\varepsilon_{j}\left(
\left[  h_{i},u_{-}\right]  \right)  \right)  }_{\substack{=a_{i,j}%
\varepsilon_{j}\left(  u_{-}\right)  \\\text{(by (\ref{pf.gtilde.dercomm2}))}%
}}u_{0}\\
&  =a_{i,j}\underbrace{\varepsilon_{j}\left(  u_{-}\right)  u_{0}%
}_{\substack{=E_{j}\left(  u_{-}u_{0}\right)  \\\text{(by
(\ref{pf.gtilde.b.Ei}), applied to }j\text{ instead of }i\text{)}}%
}=a_{i,j}E_{j}\left(  \underbrace{u_{-}u_{0}}_{=\rho\left(  u_{-}\otimes
u_{0}\right)  }\right)  =\underbrace{a_{i,j}E_{j}\left(  \rho\left(
u_{-}\otimes u_{0}\right)  \right)  }_{=\left(  a_{i,j}E_{j}\circ\rho\right)
\left(  u_{-}\otimes u_{0}\right)  }\\
&  =\left(  a_{i,j}E_{j}\circ\rho\right)  \left(  u_{-}\otimes u_{0}\right)  .
\end{align*}


Now, forget that we fixed $u_{-}$ and $u_{0}$. We thus have proven that%
\[
\left(  \left[  H_{i},E_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  =\left(  a_{i,j}E_{j}\circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  .
\]
In other words, the two maps $\left[  H_{i},E_{j}\right]  \circ\rho$ and
$a_{i,j}E_{j}\circ\rho$ are equal to each other on each pure tensor in the
tensor product $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes U\left(
\widetilde{\mathfrak{h}}\right)  $. Since these two maps are linear, this
yields that these two maps must be identical (because whenever two linear maps
from a tensor product are equal to each other on each pure tensor, these maps
must be identical). In other words, $\left[  H_{i},E_{j}\right]  \circ
\rho=a_{i,j}E_{j}\circ\rho$. Since we can cancel the $\rho$ from this equation
(because $\rho$ is an isomorphism), this yields $\left[  H_{i},E_{j}\right]
=a_{i,j}E_{j}$.

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  H_{i},E_{j}\right]  =a_{i,j}E_{j}$ for all $i,j\in\left\{
1,2,...,n\right\}  $.

\bigskip

\textit{Proof of the relation }$\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}$
\textit{for all }$i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Every $u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $
satisfies%
\begin{align*}
\underbrace{\left[  H_{i},F_{j}\right]  }_{=H_{i}\circ F_{j}-F_{j}\circ H_{i}%
}u  &  =\left(  H_{i}\circ F_{j}-F_{j}\circ H_{i}\right)  \left(  u\right)
=H_{i}\underbrace{\left(  F_{j}u\right)  }_{\substack{=f_{j}u\\\text{(by the
definition}\\\text{of }F_{j}\text{)}}}-F_{j}\underbrace{\left(  H_{i}u\right)
}_{\substack{=h_{i}u\\\text{(by the definition}\\\text{of }H_{i}\text{)}}}\\
&  =\underbrace{H_{i}\left(  f_{j}u\right)  }_{\substack{=h_{i}\left(
f_{j}u\right)  \\\text{(by the definition}\\\text{of }H_{i}\text{)}%
}}-\underbrace{F_{j}\left(  h_{i}u\right)  }_{\substack{=f_{j}\left(
h_{i}u\right)  \\\text{(by the definition}\\\text{of }F_{j}\text{)}}}\\
&  =h_{i}\left(  f_{j}u\right)  -f_{j}\left(  h_{i}u\right)
=\underbrace{\left(  h_{i}f_{j}-f_{j}h_{i}\right)  }_{\substack{=\left[
h_{i},f_{j}\right]  =-a_{i,j}f_{j}\\\text{in }U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \\\text{(since
(\ref{pf.gtilde.b.semidir.ij}) yields}\\\left[  h_{i},f_{j}\right]
=-a_{i,j}f_{j}\text{ in }\widetilde{\mathfrak{h}}\text{)}}}u\\
&  =-a_{i,j}f_{j}u
\end{align*}
and $-a_{i,j}F_{j}u=-a_{i,j}f_{j}u$ (because the definition of $F_{j}$ yields
$F_{j}u=f_{j}u$). Thus, every $u\in U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ satisfies $\left[  H_{i}%
,F_{j}\right]  u=-a_{i,j}F_{j}u$. Hence, $\left[  H_{i},F_{j}\right]
=-a_{i,j}F_{j}$.

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  H_{i},F_{j}\right]  =-a_{i,j}F_{j}$ for all $i,j\in\left\{
1,2,...,n\right\}  $.

\bigskip

\textit{Proof of the relation }$\left[  E_{i},F_{j}\right]  =\delta_{i,j}%
H_{i}$ \textit{for all }$i,j\in\left\{  1,2,...,n\right\}  $\textit{:}

Let $i$ and $j$ be two elements of $\left\{  1,2,...,n\right\}  $. Let
$u_{-}\in U\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ and $u_{0}\in
U\left(  \widetilde{\mathfrak{h}}\right)  $. Since $f_{j}\in
\widetilde{\mathfrak{n}}_{-}$ and $u_{-}\in U\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $, we have $f_{j}u_{-}\in\widetilde{\mathfrak{n}}_{-}\cdot
U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \subseteq U\left(
\widetilde{\mathfrak{n}}_{-}\right)  $. Thus, we can apply
(\ref{pf.gtilde.b.Ei}) to $f_{j}u_{-}$ instead of $u_{-}$, and obtain%
\begin{align}
E_{i}\left(  f_{j}u_{-}u_{0}\right)   &  =\underbrace{\varepsilon_{i}\left(
f_{j}u_{-}\right)  }_{\substack{=\varepsilon_{i}\left(  f_{j}\right)
u_{-}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  \\\text{(since }%
\varepsilon_{i}\text{ is a derivation)}}}u_{0}=\left(  \varepsilon_{i}\left(
f_{j}\right)  u_{-}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  \right)
u_{0}\nonumber\\
&  =\underbrace{\varepsilon_{i}\left(  f_{j}\right)  }_{\substack{=\delta
_{i,j}h_{i}\\\text{(by (\ref{pf.gtilde.b.epsilon.fj}))}}}u_{-}u_{0}%
+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}\nonumber\\
&  =\delta_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)
u_{0}. \label{pf.gtilde.dercomm7}%
\end{align}


But $\rho\left(  u_{-}\otimes u_{0}\right)  =u_{-}u_{0}$ (by the definition of
$\rho$), and%
\begin{align*}
&  \left(  \left[  E_{i},F_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right) \\
&  =\underbrace{\left[  E_{i},F_{j}\right]  }_{=E_{i}\circ F_{j}-F_{j}\circ
E_{i}}\left(  \underbrace{\rho\left(  u_{-}\otimes u_{0}\right)  }%
_{=u_{-}u_{0}}\right)  =\left(  E_{i}\circ F_{j}-F_{j}\circ E_{i}\right)
\left(  u_{-}u_{0}\right) \\
&  =E_{i}\left(  \underbrace{F_{j}\left(  u_{-}u_{0}\right)  }%
_{\substack{=f_{j}u_{-}u_{0}\\\text{(by the definition of }F_{j}\text{)}%
}}\right)  -F_{j}\left(  \underbrace{E_{i}\left(  u_{-}u_{0}\right)
}_{\substack{=\varepsilon_{i}\left(  u_{-}\right)  u_{0}\\\text{(by
(\ref{pf.gtilde.b.Ei}))}}}\right) \\
&  =\underbrace{E_{i}\left(  f_{j}u_{-}u_{0}\right)  }_{\substack{=\delta
_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}%
\\\text{(by (\ref{pf.gtilde.dercomm7}))}}}-\underbrace{F_{j}\left(
\varepsilon_{i}\left(  u_{-}\right)  u_{0}\right)  }_{\substack{=f_{j}%
\varepsilon_{i}\left(  u_{-}\right)  u_{0}\\\text{(by the definition of }%
F_{j}\text{)}}}\\
&  =\delta_{i,j}h_{i}u_{-}u_{0}+f_{j}\varepsilon_{i}\left(  u_{-}\right)
u_{0}-f_{j}\varepsilon_{i}\left(  u_{-}\right)  u_{0}=\delta_{i,j}%
h_{i}\underbrace{u_{-}u_{0}}_{=\rho\left(  u_{-}\otimes u_{0}\right)  }\\
&  =\delta_{i,j}\underbrace{h_{i}\rho\left(  u_{-}\otimes u_{0}\right)
}_{\substack{=H_{i}\left(  \rho\left(  u_{-}\otimes u_{0}\right)  \right)
\\\text{(since the definition of }H_{i}\text{ yields}\\H_{i}\left(
\rho\left(  u_{-}\otimes u_{0}\right)  \right)  =h_{i}\rho\left(  u_{-}\otimes
u_{0}\right)  \text{)}}}=\delta_{i,j}H_{i}\left(  \rho\left(  u_{-}\otimes
u_{0}\right)  \right) \\
&  =\left(  \delta_{i,j}H_{i}\circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  .
\end{align*}


Now, forget that we fixed $u_{-}$ and $u_{0}$. We thus have proven that%
\[
\left(  \left[  E_{i},F_{j}\right]  \circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  =\left(  \delta_{i,j}H_{i}\circ\rho\right)  \left(  u_{-}\otimes
u_{0}\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{-}\in U\left(
\widetilde{\mathfrak{n}}_{-}\right)  \text{ and }u_{0}\in U\left(
\widetilde{\mathfrak{h}}\right)  .
\]
In other words, the two maps $\left[  E_{i},F_{j}\right]  \circ\rho$ and
$\delta_{i,j}H_{i}\circ\rho$ are equal to each other on each pure tensor in
the tensor product $U\left(  \widetilde{\mathfrak{n}}_{-}\right)  \otimes
U\left(  \widetilde{\mathfrak{h}}\right)  $. Since these two maps are linear,
this yields that these two maps must be identical (because whenever two linear
maps from a tensor product are equal to each other on each pure tensor, these
maps must be identical). In other words, $\left[  E_{i},F_{j}\right]
\circ\rho=\delta_{i,j}H_{i}\circ\rho$. Since we can cancel the $\rho$ from
this equation (because $\rho$ is an isomorphism), this yields $\left[
E_{i},F_{j}\right]  =\delta_{i,j}H_{i}$.

Now forget that we fixed $i$ and $j$. We have thus proven the relation
$\left[  E_{i},F_{j}\right]  =\delta_{i,j}H_{i}$ for all $i,j\in\left\{
1,2,...,n\right\}  $.

\bigskip

Altogether, we have thus verified all four relations (\ref{pf.gtilde.NONSERRE}%
). Now, let us define a Lie algebra homomorphism $\xi^{\prime}%
:\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \rightarrow\operatorname*{End}\left(  U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $
by the relations%
\[
\left\{
\begin{array}
[c]{c}%
\xi^{\prime}\left(  e_{i}\right)  =E_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}  ;\\
\xi^{\prime}\left(  f_{i}\right)  =F_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}  ;\\
\xi^{\prime}\left(  h_{i}\right)  =H_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  .
\]
This $\xi^{\prime}$ is clearly well-defined (because a Lie algebra
homomorphism from a free Lie algebra over a set can be defined by arbitrarily
choosing its values at the elements of this set). This homomorphism
$\xi^{\prime}$ clearly maps the four relations (\ref{nonserre-relations}) to
the four relations (\ref{pf.gtilde.NONSERRE}). Since we know that the four
relations (\ref{pf.gtilde.NONSERRE}) are satisfied in $\operatorname*{End}%
\left(  U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  $, we conclude that the homomorphism $\xi^{\prime}$
factors through the Lie algebra $\operatorname*{FreeLie}\left(  h_{i}%
,f_{i},e_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)
=\widetilde{\mathfrak{g}}$. In other words, there exists a Lie algebra
homomorphism $\xi:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}%
\left(  U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  \right)  $ such that%
\[
\left\{
\begin{array}
[c]{c}%
\xi\left(  e_{i}\right)  =E_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\xi\left(  f_{i}\right)  =F_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}  ;\\
\xi\left(  h_{i}\right)  =H_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,...,n\right\}
\end{array}
\right.  .
\]
Consider this $\xi$. Clearly, the Lie algebra homomorphism $\xi
:\widetilde{\mathfrak{g}}\rightarrow\operatorname*{End}\left(  U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $
makes the vector space $U\left(  \widetilde{\mathfrak{h}}\ltimes
\widetilde{\mathfrak{n}}_{-}\right)  $ into a $\widetilde{\mathfrak{g}}$-module.

\bigskip

\underline{\textit{6th step: Proving the injectivity of }$\iota_{-}$%
\textit{.}}

We are very close to proving Theorem \ref{thm.gtilde} \textbf{(b)} now.

Let $\xi_{-}$ be the map $\xi\circ\iota_{-}:\widetilde{\mathfrak{n}}%
_{-}\rightarrow\operatorname*{End}\left(  U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $. Then, $\xi_{-}$ is a
Lie algebra homomorphism (since $\xi$ and $\iota_{-}$ are Lie algebra homomorphisms).

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\underbrace{\xi_{-}}%
_{=\xi\circ\iota_{-}}\left(  f_{i}\right)  =\left(  \xi\circ\iota_{-}\right)
\left(  f_{i}\right)  =\xi\left(  \underbrace{\iota_{-}\left(  f_{i}\right)
}_{\substack{=f_{i}\\\text{(by the definition of }\iota_{-}\text{)}}}\right)
=\xi\left(  f_{i}\right)  =F_{i}$ (by the definition of $\xi$).

Let $\mathfrak{s}$ be the subset%
\[
\left\{  s\in\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(  \xi_{-}\left(
s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}
\]
of $\widetilde{\mathfrak{n}}_{-}$. Every $i\in\left\{  1,2,...,n\right\}  $
satisfies%
\[
\underbrace{\left(  \xi_{-}\left(  f_{i}\right)  \right)  }_{=F_{i}}\left(
u\right)  =F_{i}\left(  u\right)  =f_{i}u\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }F_{i}\right)
\]
for all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $, and therefore%
\[
f_{i}\in\left\{  s\in\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(  \xi
_{-}\left(  s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)
\right\}  =\mathfrak{s}.
\]
In other words, $\mathfrak{s}$ contains the elements $f_{1}$, $f_{2}$, $...$,
$f_{n}$.

Now, we will show that $\mathfrak{s}$ is a Lie subalgebra of
$\widetilde{\mathfrak{n}}_{-}$.

\textit{Proof that $\mathfrak{s}$ is a Lie subalgebra of }%
$\widetilde{\mathfrak{n}}_{-}$\textit{:}

The reader can easily verify that $\mathfrak{s}$ is a vector subspace of
$\widetilde{\mathfrak{n}}_{-}$ (because for any fixed $u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $, the
equation $\left(  \xi_{-}\left(  s\right)  \right)  u=su$ is linear in $s$).
Now, let $s_{1}\in\mathfrak{s}$ and $s_{2}\in\mathfrak{s}$ be arbitrary. Then,%
\[
s_{1}\in\mathfrak{s}=\left\{  s\in\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(
\xi_{-}\left(  s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in
U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)
\right\}  ,
\]
so that%
\begin{equation}
\left(  \xi_{-}\left(  s_{1}\right)  \right)  \left(  u\right)  =s_{1}%
u\ \ \ \ \ \ \ \ \ \ \text{for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  . \label{pf.gtilde.b.step6.1}%
\end{equation}
Similarly,%
\begin{equation}
\left(  \xi_{-}\left(  s_{2}\right)  \right)  \left(  u\right)  =s_{2}%
u\ \ \ \ \ \ \ \ \ \ \text{for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  . \label{pf.gtilde.b.step6.2}%
\end{equation}
Now, since $\xi_{-}$ is a Lie algebra homomorphism, we have $\xi_{-}\left(
\left[  s_{1},s_{2}\right]  \right)  =\left[  \xi_{-}\left(  s_{1}\right)
,\xi_{-}\left(  s_{2}\right)  \right]  =\xi_{-}\left(  s_{1}\right)  \circ
\xi_{-}\left(  s_{2}\right)  -\xi_{-}\left(  s_{2}\right)  \circ\xi_{-}\left(
s_{1}\right)  $. Thus, every $u\in U\left(  \widetilde{\mathfrak{h}}%
\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $ satisfies%
\begin{align*}
\left(  \xi_{-}\left(  \left[  s_{1},s_{2}\right]  \right)  \right)  \left(
u\right)   &  =\left(  \xi_{-}\left(  s_{1}\right)  \circ\xi_{-}\left(
s_{2}\right)  -\xi_{-}\left(  s_{2}\right)  \circ\xi_{-}\left(  s_{1}\right)
\right)  \left(  u\right) \\
&  =\left(  \xi_{-}\left(  s_{1}\right)  \right)  \left(  \underbrace{\left(
\xi_{-}\left(  s_{2}\right)  \right)  \left(  u\right)  }_{\substack{=s_{2}%
u\\\text{(by (\ref{pf.gtilde.b.step6.2}))}}}\right)  -\left(  \xi_{-}\left(
s_{2}\right)  \right)  \left(  \underbrace{\left(  \xi_{-}\left(
s_{1}\right)  \right)  \left(  u\right)  }_{\substack{=s_{1}u\\\text{(by
(\ref{pf.gtilde.b.step6.1}))}}}\right) \\
&  =\underbrace{\left(  \xi_{-}\left(  s_{1}\right)  \right)  \left(
s_{2}u\right)  }_{\substack{=s_{1}s_{2}u\\\text{(by (\ref{pf.gtilde.b.step6.1}%
), applied to}\\s_{2}u\text{ instead of }u\text{)}}}-\underbrace{\left(
\xi_{-}\left(  s_{2}\right)  \right)  \left(  s_{1}u\right)  }%
_{\substack{=s_{2}s_{1}u\\\text{(by (\ref{pf.gtilde.b.step6.2}), applied
to}\\s_{1}u\text{ instead of }u\text{)}}}\\
&  =s_{1}s_{2}u-s_{2}s_{1}u=\underbrace{\left(  s_{1}s_{2}-s_{2}s_{1}\right)
}_{=\left[  s_{1},s_{2}\right]  }u=\left[  s_{1},s_{2}\right]  u.
\end{align*}
Hence,%
\[
\left[  s_{1},s_{2}\right]  \in\left\{  s\in\widetilde{\mathfrak{n}}_{-}%
\ \mid\ \left(  \xi_{-}\left(  s\right)  \right)  \left(  u\right)  =su\text{
for all }u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \right\}  =\mathfrak{s}.
\]
Now, forget that we fixed $s_{1}$ and $s_{2}$. We thus have proven that every
$s_{1}\in\mathfrak{s}$ and $s_{2}\in\mathfrak{s}$ satisfy $\left[  s_{1}%
,s_{2}\right]  \in\mathfrak{s}$. Since $\mathfrak{s}$ is a vector subspace of
$\widetilde{\mathfrak{n}}_{-}$, this yields that $\mathfrak{s}$ is a Lie
subalgebra of $\widetilde{\mathfrak{n}}_{-}$.

But now recall that $\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}%
\left(  f_{i}\ \mid\ i\in\left\{  1,2,...,n\right\}  \right)  $. Hence, the
elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ generate $\widetilde{\mathfrak{n}%
}_{-}$ as a Lie algebra. Thus, every Lie subalgebra of
$\widetilde{\mathfrak{n}}_{-}$ which contains the elements $f_{1}$, $f_{2}$,
$...$, $f_{n}$ must be $\widetilde{\mathfrak{n}}_{-}$ itself. Since we know
that $\mathfrak{s}$ is a Lie subalgebra of $\widetilde{\mathfrak{n}}_{-}$ and
contains the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$, this yields that
$\mathfrak{s}$ must be $\widetilde{\mathfrak{n}}_{-}$ itself. In other words,
$\mathfrak{s}=\widetilde{\mathfrak{n}}_{-}$.

Now, let $s\in\widetilde{\mathfrak{n}}_{-}$ be such that $\iota_{-}\left(
s\right)  =0$. Then, $\underbrace{\xi_{-}}_{=\xi\circ\iota_{-}}\left(
s\right)  =\left(  \xi\circ\iota_{-}\right)  \left(  s\right)  =\xi\left(
\underbrace{\iota_{-}\left(  s\right)  }_{=0}\right)  =\xi\left(  0\right)
=0$. But since%
\[
s\in\widetilde{\mathfrak{n}}_{-}=\mathfrak{s}=\left\{  s\in
\widetilde{\mathfrak{n}}_{-}\ \mid\ \left(  \xi_{-}\left(  s\right)  \right)
\left(  u\right)  =su\text{ for all }u\in U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}  ,
\]
we have $\left(  \xi_{-}\left(  s\right)  \right)  \left(  u\right)  =su$ for
all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  $. Applied to $u=1$, this yields $\left(  \xi_{-}\left(
s\right)  \right)  \left(  1\right)  =s\cdot1=s$. Compared with
$\underbrace{\left(  \xi_{-}\left(  s\right)  \right)  }_{=0}\left(  1\right)
=0$, this yields $s=0$.

Now forget that we fixed $s$. We have thus shown that every $s\in
\widetilde{\mathfrak{n}}_{-}$ such that $\iota_{-}\left(  s\right)  =0$ must
satisfy $s=0$. In other words, $\iota_{-}$ is injective.

\bigskip

\underline{\textit{7th step: Proving the injectivity of }$\iota_{0}$%
\textit{.}}

The proof of the injectivity of $\iota_{0}$ is similar but even simpler.

Let $\xi_{0}$ be the map $\xi\circ\iota_{0}:\widetilde{\mathfrak{h}%
}\rightarrow\operatorname*{End}\left(  U\left(  \widetilde{\mathfrak{h}%
}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right)  $.

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\underbrace{\xi_{0}}%
_{=\xi\circ\iota_{0}}\left(  h_{i}\right)  =\left(  \xi\circ\iota_{0}\right)
\left(  h_{i}\right)  =\xi\left(  \underbrace{\iota_{0}\left(  h_{i}\right)
}_{\substack{=h_{i}\\\text{(by the definition of }\iota_{0}\text{)}}}\right)
=\xi\left(  h_{i}\right)  =H_{i}$ (by the definition of $\xi$).

Let $\mathfrak{t}$ be the subset%
\[
\left\{  s\in\widetilde{\mathfrak{h}}\ \mid\ \left(  \xi_{0}\left(  s\right)
\right)  \left(  u\right)  =su\text{ for all }u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}
\]
of $\widetilde{\mathfrak{h}}$. Every $i\in\left\{  1,2,...,n\right\}  $
satisfies%
\[
\underbrace{\left(  \xi_{0}\left(  h_{i}\right)  \right)  }_{=H_{i}}\left(
u\right)  =H_{i}\left(  u\right)  =h_{i}u\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }H_{i}\right)
\]
for all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  $, and therefore%
\[
h_{i}\in\left\{  s\in\widetilde{\mathfrak{h}}\ \mid\ \left(  \xi_{0}\left(
s\right)  \right)  \left(  u\right)  =su\text{ for all }u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  \right\}
=\mathfrak{t}.
\]
In other words, $\mathfrak{t}$ contains the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$.

On the other hand, it is easy to see that $\mathfrak{t}$ is a vector subspace
of $\widetilde{\mathfrak{h}}$ (because for any fixed $u\in U\left(
\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}_{-}\right)  $, the
equation $\left(  \xi_{0}\left(  s\right)  \right)  \left(  u\right)  =su$ is
linear in $s$).

But now recall that $\widetilde{\mathfrak{h}}$ is the free vector space with
basis $h_{1},h_{2},...,h_{n}$. Thus, the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$ span the vector space $\widetilde{\mathfrak{h}}$. Thus, every vector
subspace of $\widetilde{\mathfrak{h}}$ which contains the elements $h_{1}$,
$h_{2}$, $...$, $h_{n}$ must be $\widetilde{\mathfrak{h}}$ itself. Since we
know that $\mathfrak{t}$ is a vector subspace of $\widetilde{\mathfrak{h}}$
and contains the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ , this yields that
$\mathfrak{t}$ must be $\widetilde{\mathfrak{h}}$ itself. In other words,
$\mathfrak{t}=\widetilde{\mathfrak{h}}$.

Now, let $s\in\widetilde{\mathfrak{h}}$ be such that $\iota_{0}\left(
s\right)  =0$. Then, $\underbrace{\xi_{0}}_{=\xi\circ\iota_{0}}\left(
s\right)  =\left(  \xi\circ\iota_{0}\right)  \left(  s\right)  =\xi\left(
\underbrace{\iota_{0}\left(  s\right)  }_{=0}\right)  =\xi\left(  0\right)
=0$. But since%
\[
s\in\widetilde{\mathfrak{h}}=\mathfrak{t}=\left\{  s\in\widetilde{\mathfrak{h}%
}\ \mid\ \left(  \xi_{0}\left(  s\right)  \right)  \left(  u\right)  =su\text{
for all }u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}\right)  \right\}  ,
\]
we have $\left(  \xi_{0}\left(  s\right)  \right)  \left(  u\right)  =su$ for
all $u\in U\left(  \widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}}%
_{-}\right)  $. Applied to $u=1$, this yields $\left(  \xi_{0}\left(
s\right)  \right)  \left(  1\right)  =s\cdot1=s$. Compared with
$\underbrace{\left(  \xi_{0}\left(  s\right)  \right)  }_{=0}\left(  1\right)
=0$, this yields $s=0$.

Now forget that we fixed $s$. We have thus shown that every $s\in
\widetilde{\mathfrak{h}}$ such that $\iota_{0}\left(  s\right)  =0$ must
satisfy $s=0$. In other words, $\iota_{0}$ is injective.

\bigskip

\underline{\textit{8th step: Proving the injectivity of }$\iota_{+}$%
\textit{.}}

We have proven the injectivity of the maps $\iota_{-}$ and $\iota_{0}$ above.
The proof of the injectivity of the map $\iota_{+}$ is very similar to the
above proof of the injectivity of the map $\iota_{-}$. More precisely, in
order to obtain a proof of the injectivity of the map $\iota_{+}$, it is
enough to apply the following changes to our above proof of the injectivity of
the map $\iota_{-}$:

\begin{itemize}
\item replace every $e_{i}$ by $f_{i}$, and simultaneously replace every
$f_{i}$ by $e_{i}$;

\item replacing every $h_{i}$ by $-h_{i}$ (this is allowed since
$\widetilde{\mathfrak{h}}$ is the free vector space with basis $-h_{1}%
,-h_{2},...,-h_{n}$);

\item replace $\widetilde{\mathfrak{n}}_{-}$ by $\widetilde{\mathfrak{n}}_{+}$;

\item replace every reference to (\ref{nonserre-relations}) by a reference to
(\ref{nonserre-relations2}) (this is allowed since we know that the relations
(\ref{nonserre-relations2}) are satisfied in $\widetilde{\mathfrak{g}}$).
\end{itemize}

We have thus proven that the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ are
injective. Moreover, $\iota_{+}$ and $\iota_{-}$ are Lie algebra homomorphisms
(by definition), and $\iota_{0}$ is a Lie algebra homomorphism as well
(because any $i,j\in\left\{  1,2,...,n\right\}  $ satisfy $\left[  h_{i}%
,h_{j}\right]  =0$ in $\widetilde{\mathfrak{h}}$ (since
$\widetilde{\mathfrak{h}}$ is an abelian Lie algebra) and $\left[  h_{i}%
,h_{j}\right]  =0$ in $\widetilde{\mathfrak{g}}$ (due to the relations
(\ref{nonserre-relations}))). This completes the proof of Theorem
\ref{thm.gtilde} \textbf{(b)}.

\bigskip

\textbf{(c)} \underline{\textit{1st step: The existence of the direct sum in
question.}}

Define a relation $\leq$ on $Q$ by positing that two $n$-tuples $\left(
\lambda_{1},\lambda_{2},...,\lambda_{n}\right)  \in\mathbb{Z}^{n}$ and
$\left(  \mu_{1},\mu_{2},...,\mu_{n}\right)  \in\mathbb{Z}^{n}$ satisfy
$\lambda_{1}\alpha_{1}+\lambda_{2}\alpha_{2}+...+\lambda_{n}\alpha_{n}\leq
\mu_{1}\alpha_{1}+\mu_{2}\alpha_{2}+...+\mu_{n}\alpha_{n}$ if and only if
every $i\in\left\{  1,2,...,n\right\}  $ satisfies $\lambda_{i}\leq\mu_{i}$.
It is clear that this relation $\leq$ is a non-strict partial order. Define
$\geq$ to be the opposite of $\leq$. Define $>$ and $<$ to be the strict
versions of the relations $\geq$ and $\leq$, respectively; thus, any
$\alpha\in Q$ and $\beta\in Q$ satisfy $\alpha>\beta$ if and only if $\left(
\alpha\neq\beta\text{ and }\alpha\geq\beta\right)  $.

The elements $\alpha$ of $Q$ satisfying $\alpha\geq0$ are exactly the nonzero
sums $\lambda_{1}\alpha_{1}+\lambda_{2}\alpha_{2}+...+\lambda_{n}\alpha_{n}$
with $\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$ being nonnegative
integers. The elements $\alpha$ of $Q$ satisfying $\alpha\leq0$ are exactly
the nonzero sums $\lambda_{1}\alpha_{1}+\lambda_{2}\alpha_{2}+...+\lambda
_{n}\alpha_{n}$ with $\lambda_{1}$, $\lambda_{2}$, $...$, $\lambda_{n}$ being
nonpositive integers.

Let $\widetilde{\mathfrak{g}}\left[  <0\right]  =\bigoplus
\limits_{\substack{\alpha\in Q;\\\alpha<0}}\widetilde{\mathfrak{g}}\left[
\alpha\right]  $ and $\widetilde{\mathfrak{g}}\left[  >0\right]
=\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha>0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  $. Then, $\widetilde{\mathfrak{g}}\left[  0\right]  $,
$\widetilde{\mathfrak{g}}\left[  <0\right]  $ and $\widetilde{\mathfrak{g}%
}\left[  >0\right]  $ are $Q$-graded Lie subalgebras of
$\widetilde{\mathfrak{g}}$ (this is easy to see from the fact that
$\widetilde{\mathfrak{g}}$ is a $Q$-graded Lie algebra).

It is easy to see that the (internal) direct sum $\widetilde{\mathfrak{g}%
}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}}\left[  <0\right]
\oplus\widetilde{\mathfrak{g}}\left[  0\right]  $ is
well-defined.\footnote{\textit{Proof.} We have $\widetilde{\mathfrak{g}%
}=\bigoplus\limits_{\alpha\in Q}\widetilde{\mathfrak{g}}\left[  \alpha\right]
$ (since $\widetilde{\mathfrak{g}}$ is $Q$-graded). But every $\alpha\in Q$
satisfies \textbf{exactly one} of the four assertions $\alpha>0$, $\alpha<0$,
$\alpha=0$ and $\left(  \text{neither }\alpha<0\text{ nor }\alpha>0\text{ nor
}\alpha=0\right)  $. Thus,%
\begin{align*}
\bigoplus\limits_{\alpha\in Q}\widetilde{\mathfrak{g}}\left[  \alpha\right]
&  =\underbrace{\left(  \bigoplus\limits_{\substack{\alpha\in Q;\\\alpha
>0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  }%
_{=\widetilde{\mathfrak{g}}\left[  >0\right]  }\oplus\underbrace{\left(
\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha<0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  \right)  }_{=\widetilde{\mathfrak{g}}\left[
<0\right]  }\oplus\underbrace{\left(  \bigoplus\limits_{\substack{\alpha\in
Q;\\\alpha=0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)
}_{=\widetilde{\mathfrak{g}}\left[  0\right]  }\oplus\left(  \bigoplus
\limits_{\substack{\alpha\in Q;\\\text{neither }\alpha<0\\\text{nor }%
\alpha>0\text{ nor }\alpha=0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]
\right) \\
&  =\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}}\left[  0\right]
\oplus\left(  \bigoplus\limits_{\substack{\alpha\in Q;\\\text{neither }%
\alpha<0\\\text{nor }\alpha>0\text{ nor }\alpha=0}}\widetilde{\mathfrak{g}%
}\left[  \alpha\right]  \right)  .
\end{align*}
Thus, the (internal) direct sum $\widetilde{\mathfrak{g}}\left[  >0\right]
\oplus\widetilde{\mathfrak{g}}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  $ is well-defined (because it is a partial sum of the
direct sum $\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus
\widetilde{\mathfrak{g}}\left[  <0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  \oplus\left(  \bigoplus\limits_{\substack{\alpha\in
Q;\\\text{neither }\alpha<0\\\text{nor }\alpha>0\text{ nor }\alpha
=0}}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \right)  $).}

Every $i\in\left\{  1,2,...,n\right\}  $ satisfies
\begin{align*}
f_{i}  &  \in\widetilde{\mathfrak{g}}\left[  -\alpha_{i}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\deg\left(  f_{i}\right)
=-\alpha_{i}\right) \\
&  \subseteq\bigoplus\limits_{\substack{\alpha\in Q;\\\alpha<0}%
}\widetilde{\mathfrak{g}}\left[  \alpha\right]  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }-\alpha_{i}<0\right) \\
&  =\widetilde{\mathfrak{g}}\left[  <0\right]  .
\end{align*}
Hence, the Lie algebra $\widetilde{\mathfrak{g}}\left[  <0\right]  $ contains
the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$. But now, recall that
$\widetilde{\mathfrak{n}}_{-}=\operatorname*{FreeLie}\left(  f_{i}\ \mid
\ i\in\left\{  1,2,...,n\right\}  \right)  $. Hence, the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{n}}_{-}$ generate
$\widetilde{\mathfrak{n}}_{-}$ as a Lie algebra. Thus, the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{g}}$ generate $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  $ as a Lie algebra (because
the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of $\widetilde{\mathfrak{g}}$
are the images of the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of
$\widetilde{\mathfrak{n}}_{-}$ under the map $\iota_{-}$). Thus, every Lie
subalgebra of $\widetilde{\mathfrak{g}}$ which contains the elements $f_{1}$,
$f_{2}$, $...$, $f_{n}$ must contain $\iota_{-}\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $ as a subset. Since we know that $\widetilde{\mathfrak{g}%
}\left[  <0\right]  $ is a Lie subalgebra of $\widetilde{\mathfrak{g}}$ and
contains the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$, this yields that
$\widetilde{\mathfrak{g}}\left[  <0\right]  $ must contain $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ as a subset. In other words, $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  <0\right]  $. Similarly, we can show
$\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  >0\right]  $.

Finally, every $i\in\left\{  1,2,...,n\right\}  $ satisfies $h_{i}%
\in\widetilde{\mathfrak{g}}\left[  0\right]  $ (since $\deg\left(
h_{i}\right)  =0$). In other words, the vector space $\widetilde{\mathfrak{g}%
}\left[  0\right]  $ contains the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$.

But now, recall that $\widetilde{\mathfrak{h}}$ is the free vector space with
basis $h_{1},h_{2},...,h_{n}$. Thus, the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$ of $\widetilde{\mathfrak{h}}$ span the vector space
$\widetilde{\mathfrak{h}}$. Consequently, the elements $h_{1}$, $h_{2}$,
$...$, $h_{n}$ of $\widetilde{\mathfrak{g}}$ span the vector space $\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ (because the elements $h_{1}$,
$h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{g}}$ are the images of the
elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{h}}$ undet
the map $\iota_{0}$). Hence, every vector subspace of $\widetilde{\mathfrak{g}%
}$ which contains the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ must contain
$\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ as a subset. Since we
know that $\widetilde{\mathfrak{g}}\left[  0\right]  $ is a vector subspace of
$\widetilde{\mathfrak{g}}$ and contains the elements $h_{1}$, $h_{2}$, $...$,
$h_{n}$, this yields that $\widetilde{\mathfrak{g}}\left[  0\right]  $ must
contain $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ as a subset. In
other words, $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  0\right]  $.

Since the internal direct sum $\widetilde{\mathfrak{g}}\left[  <0\right]
\oplus\widetilde{\mathfrak{g}}\left[  >0\right]  \oplus\widetilde{\mathfrak{g}%
}\left[  0\right]  $ is well-defined, the internal direct sum $\iota
_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ must also be well-defined (because
$\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \subseteq
\widetilde{\mathfrak{g}}\left[  >0\right]  $, $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{g}}\left[
<0\right]  $ and $\iota_{0}\left(  \widetilde{\mathfrak{h}}\right)
\subseteq\widetilde{\mathfrak{g}}\left[  0\right]  $). We now must prove that
this direct sum is $\widetilde{\mathfrak{g}}$.

\bigskip

\underline{\textit{2nd step: Identifications.}}

Since the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ are injective Lie
algebra homomorphisms, and since their images are linearly disjoint (because
the direct sum $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
\oplus\iota_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota
_{0}\left(  \widetilde{\mathfrak{h}}\right)  $ is well-defined), we can regard
these maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ as inclusions of Lie
algebras. Let us do this from now on. Thus, $\widetilde{\mathfrak{n}}_{+}$,
$\widetilde{\mathfrak{n}}_{-}$ and $\widetilde{\mathfrak{h}}$ are Lie
subalgebras of $\widetilde{\mathfrak{g}}$. The identification of
$\widetilde{\mathfrak{n}}_{-}$ with the Lie subalgebra $\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  $ of $\widetilde{\mathfrak{g}}$
eliminates the need of distinguishing between the elements $f_{i}$ of
$\widetilde{\mathfrak{n}}_{-}$ and the elements $f_{i}$ of
$\widetilde{\mathfrak{g}}$ (because for every $i\in\left\{  1,2,...,n\right\}
$, the element $f_{i}$ of $\widetilde{\mathfrak{g}}$ is the image of the
element $f_{i}$ of $\widetilde{\mathfrak{n}}_{-}$ under the map $\iota_{-}$,
and since we regard this map $\iota_{-}$ as inclusion, these two elements
$f_{i}$ are therefore equal). Similarly, we don't have to distinguish between
the elements $e_{i}$ of $\widetilde{\mathfrak{n}}_{+}$ and the elements
$e_{i}$ of $\widetilde{\mathfrak{g}}$, nor is it necessary to distinguish
between the elements $h_{i}$ of $\widetilde{\mathfrak{h}}$ and the elements
$h_{i}$ of $\widetilde{\mathfrak{g}}$.

Since we regard the maps $\iota_{+}$, $\iota_{-}$ and $\iota_{0}$ as
inclusions, we have $\iota_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)
=\widetilde{\mathfrak{n}}_{+}$, $\iota_{-}\left(  \widetilde{\mathfrak{n}}%
_{-}\right)  =\widetilde{\mathfrak{n}}_{-}$ and $\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{h}}$. Hence, $\iota
_{+}\left(  \widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{-}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  =\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$. This shows that
the internal direct sums $\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}$ and $\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{h}}$ are well-defined (since they are partial sums of the
direct sum $\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}$).

\bigskip

\underline{\textit{3rd step: Proving that }$\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}$\textit{ is a Lie subalgebra of }%
$\widetilde{\mathfrak{g}}$\textit{.}}

We now will prove part \textbf{(d)} of Theorem \ref{thm.gtilde} before we come
back and finish the proof of part \textbf{(c)}.

Indeed, we know that both $\widetilde{\mathfrak{n}}_{-}$ and
$\widetilde{\mathfrak{h}}$ are Lie subalgebras of $\widetilde{\mathfrak{g}}$.
Thus, $\left[  \widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{n}}%
_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$ and $\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{h}}\right]
=\widetilde{\mathfrak{h}}$. We will now show that $\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq
\widetilde{\mathfrak{n}}_{-}$.

\textit{Proof of }$\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}%
}_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$\textit{:}

Let $i\in\left\{  1,2,...,n\right\}  $. Let $\xi_{i}:\widetilde{\mathfrak{g}%
}\rightarrow\widetilde{\mathfrak{g}}$ be the map defined by%
\[
\left(  \xi_{i}\left(  x\right)  =\left[  h_{i},x\right]
\ \ \ \ \ \ \ \ \ \ \text{for any }x\in\widetilde{\mathfrak{g}}\right)  .
\]
Then, $\xi_{i}$ is a derivation of the Lie algebra $\widetilde{\mathfrak{g}}$.
On the other hand, the subset $\left\{  f_{1},f_{2},...,f_{n}\right\}  $ of
$\widetilde{\mathfrak{n}}_{-}$ generates $\widetilde{\mathfrak{n}}_{-}$ as a
Lie algebra (since the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of
$\widetilde{\mathfrak{n}}_{-}$ generate $\widetilde{\mathfrak{n}}_{-}$ as a
Lie algebra), and we can easily check that $\xi_{i}\left(  \left\{
f_{1},f_{2},...,f_{n}\right\}  \right)  \subseteq\widetilde{\mathfrak{n}}_{-}%
$\ \ \ \ \footnote{\textit{Proof.} For every $j\in\left\{  1,2,...,n\right\}
$, we have%
\begin{align*}
\xi_{i}\left(  f_{j}\right)   &  =\left[  h_{i},f_{j}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\xi_{i}\right) \\
&  =-a_{i,j}\underbrace{f_{j}}_{\in\widetilde{\mathfrak{n}}_{-}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the relations (\ref{nonserre-relations}%
)}\right) \\
&  \in-a_{i,j}\widetilde{\mathfrak{n}}_{-}\subseteq\widetilde{\mathfrak{n}%
}_{-}.
\end{align*}
Thus, $\left\{  \xi_{i}\left(  f_{1}\right)  ,\xi_{i}\left(  f_{2}\right)
,...,\xi_{i}\left(  f_{n}\right)  \right\}  \subseteq\widetilde{\mathfrak{n}%
}_{-}$. Since $\left\{  \xi_{i}\left(  f_{1}\right)  ,\xi_{i}\left(
f_{2}\right)  ,...,\xi_{i}\left(  f_{n}\right)  \right\}  =\xi_{i}\left(
\left\{  f_{1},f_{2},...,f_{n}\right\}  \right)  $, this rewrites as $\xi
_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)  \subseteq
\widetilde{\mathfrak{n}}_{-}$, qed.}. Hence, Corollary
\ref{cor.derivation.Lie.unique.ihg} (applied to $\widetilde{\mathfrak{g}}$,
$\widetilde{\mathfrak{n}}_{-}$, $\widetilde{\mathfrak{n}}_{-}$, $\xi_{i}$ and
$\left\{  f_{1},f_{2},...,f_{n}\right\}  $ instead of $\mathfrak{g}$,
$\mathfrak{h}$, $\mathfrak{i}$, $d$ and $S$) yields that $\xi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{n}}_{-}$.
But%
\begin{align*}
\xi_{i}\left(  \widetilde{\mathfrak{n}}_{-}\right)   &  =\left\{
\underbrace{\xi_{i}\left(  x\right)  }_{=\left[  h_{i},x\right]  }\mid
x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left\{  \left[  h_{i},x\right]
\mid x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left[  h_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  =\left[  h_{i},\widetilde{\mathfrak{n}%
}_{-}\right]  \mathbb{C}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  h_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  \text{ is a vector space}\right) \\
&  =\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)  .
\end{align*}
Thus, $\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}$. Now, forget that we fixed $i$. We thus
have shown that $\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}$ for every $i\in\left\{
1,2,...,n\right\}  $.

But the elements $h_{1}$, $h_{2}$, $...$, $h_{n}$ of $\widetilde{\mathfrak{h}%
}$ span the vector space $\widetilde{\mathfrak{h}}$. Thus,
$\widetilde{\mathfrak{h}}=\sum\limits_{i=1}^{n}\left(  h_{i}\mathbb{C}\right)
$, so that%
\begin{align*}
\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]   &
=\left[  \sum\limits_{i=1}^{n}\left(  h_{i}\mathbb{C}\right)
,\ \widetilde{\mathfrak{n}}_{-}\right]  =\sum\limits_{i=1}^{n}%
\underbrace{\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]
}_{\subseteq\widetilde{\mathfrak{n}}_{-}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since the Lie bracket is bilinear}\right) \\
&  \subseteq\sum\limits_{i=1}^{n}\widetilde{\mathfrak{n}}_{-}\subseteq
\widetilde{\mathfrak{n}}_{-}.
\end{align*}
This proves $\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}%
_{-}\right]  \subseteq\widetilde{\mathfrak{n}}_{-}$.

Now, $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}%
=\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}$ (since direct sums are
sums), so that
\begin{align*}
\left[  \widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}\right]   &
=\left[  \widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}%
,\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}\right]
=\underbrace{\left[  \widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{n}}%
_{-}\right]  }_{\subseteq\widetilde{\mathfrak{n}}_{-}}+\underbrace{\left[
\widetilde{\mathfrak{n}}_{-},\widetilde{\mathfrak{h}}\right]  }_{=-\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq\left[
\widetilde{\mathfrak{h}},\widetilde{\mathfrak{n}}_{-}\right]  \subseteq
\widetilde{\mathfrak{n}}_{-}}+\underbrace{\left[  \widetilde{\mathfrak{h}%
},\widetilde{\mathfrak{n}}_{-}\right]  }_{\subseteq\widetilde{\mathfrak{n}%
}_{-}}+\underbrace{\left[  \widetilde{\mathfrak{h}},\widetilde{\mathfrak{h}%
}\right]  }_{\subseteq\widetilde{\mathfrak{h}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)
\\
&  \subseteq\underbrace{\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{n}%
}_{-}+\widetilde{\mathfrak{n}}_{-}}_{\subseteq\widetilde{\mathfrak{n}}_{-}%
}+\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{n}}_{-}%
+\widetilde{\mathfrak{h}}=\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}.
\end{align*}
Thus, $\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ is a Lie
subalgebra of $\widetilde{\mathfrak{g}}$.

(Note that the map $\left(  \iota_{-},\iota_{0}\right)
:\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}\rightarrow
\widetilde{\mathfrak{g}}$ is actually a Lie algebra isomorphism from the
semidirect product $\widetilde{\mathfrak{h}}\ltimes\widetilde{\mathfrak{n}%
}_{-}$ (which was constructed during our proof of Theorem \ref{thm.gtilde}
\textbf{(b)}) to $\widetilde{\mathfrak{g}}$. But we will not need this fact,
so we will not prove it either.)

So we have shown that $\widetilde{\mathfrak{n}}_{-}\oplus
\widetilde{\mathfrak{h}}$ is a Lie subalgebra of $\widetilde{\mathfrak{g}}$.
Since $\widetilde{\mathfrak{n}}_{-}=\iota_{-}\left(  \widetilde{\mathfrak{n}%
}_{-}\right)  $ and $\widetilde{\mathfrak{h}}=\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $, this rewrites as follows: $\iota
_{-}\left(  \widetilde{\mathfrak{n}}_{-}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ is a Lie subalgebra of
$\widetilde{\mathfrak{g}}$. Similarly, $\iota_{+}\left(
\widetilde{\mathfrak{n}}_{+}\right)  \oplus\iota_{0}\left(
\widetilde{\mathfrak{h}}\right)  $ is a Lie subalgebra of
$\widetilde{\mathfrak{g}}$. This proves Theorem \ref{thm.gtilde} \textbf{(d)}.

\bigskip

\underline{\textit{4th step: Finishing the proof of Theorem \ref{thm.gtilde}
\textbf{(c)}.}}

We know that the internal direct sum $\widetilde{\mathfrak{n}}_{+}%
\oplus\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ makes sense.
Denote this direct sum $\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{n}}_{-}\oplus\widetilde{\mathfrak{h}}$ as $V$. We know
that $V$ is a vector subspace of $\widetilde{\mathfrak{g}}$. We need to prove
that $V=\widetilde{\mathfrak{g}}$.

Let $N$ be the vector subspace of $\widetilde{\mathfrak{g}}$ spanned by the
$3n$ elements $e_{1}$, $e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$,
$f_{n}$, $h_{1}$, $h_{2}$, $...$, $h_{n}$. Then, $N$ generates
$\widetilde{\mathfrak{g}}$ as a Lie algebra (because the elements $e_{1}$,
$e_{2}$, $...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$,
$...$, $h_{n}$ generate $\widetilde{\mathfrak{g}}$ as a Lie
algebra\footnote{This is because $\widetilde{\mathfrak{g}}%
=\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\ \mid\ i\in\left\{
1,2,...,n\right\}  \right)  \diagup\left(  \text{the relations
(\ref{nonserre-relations})}\right)  $.}).

We will now prove that $\left[  N,V\right]  \subseteq V$.

Indeed, since $N=\sum\limits_{i=1}^{n}\left(  e_{i}\mathbb{C}\right)
+\sum\limits_{i=1}^{n}\left(  f_{i}\mathbb{C}\right)  +\sum\limits_{i=1}%
^{n}\left(  h_{i}\mathbb{C}\right)  $ (because $N$ is the vector subspace of
$\widetilde{\mathfrak{g}}$ spanned by the $3n$ elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$) and $V=\widetilde{\mathfrak{n}}_{+}\oplus\widetilde{\mathfrak{n}}%
_{-}\oplus\widetilde{\mathfrak{h}}=\widetilde{\mathfrak{n}}_{+}%
+\widetilde{\mathfrak{n}}_{-}+\widetilde{\mathfrak{h}}$ (since direct sums are
sums), we have%
\begin{align}
\left[  N,V\right]   &  =\left[  \sum\limits_{i=1}^{n}\left(  e_{i}%
\mathbb{C}\right)  +\sum\limits_{i=1}^{n}\left(  f_{i}\mathbb{C}\right)
+\sum\limits_{i=1}^{n}\left(  h_{i}\mathbb{C}\right)
,\ \widetilde{\mathfrak{n}}_{+}+\widetilde{\mathfrak{n}}_{-}%
+\widetilde{\mathfrak{h}}\right] \nonumber\\
&  \subseteq\sum\limits_{i=1}^{n}\left[  e_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  +\sum\limits_{i=1}^{n}\left[
e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  +\sum\limits_{i=1}%
^{n}\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right] \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{i=1}^{n}\left[  f_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  +\sum\limits_{i=1}^{n}\left[
f_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  +\sum\limits_{i=1}%
^{n}\left[  f_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right] \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{i=1}^{n}\left[  h_{i}\mathbb{C}%
,\ \widetilde{\mathfrak{n}}_{+}\right]  +\sum\limits_{i=1}^{n}\left[
h_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  +\sum\limits_{i=1}%
^{n}\left[  h_{i}\mathbb{C},\ \widetilde{\mathfrak{h}}\right]
\label{pf.gtilde.c.1}%
\end{align}
(since the Lie bracket is bilinear).

We will now prove that each summand of each of the nine sums on the right hand
side of (\ref{pf.gtilde.c.1}) is $\subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq
V$\textit{:}

For every $i\in\left\{  1,2,...,n\right\}  $, we have $e_{i}\in
\widetilde{\mathfrak{n}}_{+}$ and thus $e_{i}\mathbb{C}\subseteq
\widetilde{\mathfrak{n}}_{+}$, so that%
\begin{align*}
\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]   &
\subseteq\left[  \widetilde{\mathfrak{n}}_{+},\ \widetilde{\mathfrak{n}}%
_{+}\right]  \subseteq\widetilde{\mathfrak{n}}_{+}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\widetilde{\mathfrak{n}}_{+}\text{ is a Lie algebra}\right) \\
&  \subseteq\widetilde{\mathfrak{n}}_{+}+\widetilde{\mathfrak{n}}%
_{-}+\widetilde{\mathfrak{h}}=V.
\end{align*}
We have thus proven that every $i\in\left\{  1,2,...,n\right\}  $ satisfies
$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{+}\right]  \subseteq V$.

\textit{Proof that every }$i\in\left\{  1,2,...,n\right\}  $\textit{ satisfies
}$\left[  e_{i}\mathbb{C},\ \widetilde{\mathfrak{n}}_{-}\right]  \subseteq
V$\textit{:}

Let $i\in\left\{  1,2,...,n\right\}  $. Define a map $\psi_{i}%
:\widetilde{\mathfrak{g}}\rightarrow\widetilde{\mathfrak{g}}$ by%
\[
\left(  \psi_{i}\left(  x\right)  =\left[  e_{i},x\right]
\ \ \ \ \ \ \ \ \ \ \text{for every }x\in\widetilde{\mathfrak{g}}\right)  .
\]
Then, $\psi_{i}$ is a derivation of the Lie algebra $\widetilde{\mathfrak{g}}%
$. On the other hand, the subset $\left\{  f_{1},f_{2},...,f_{n}\right\}  $ of
$\widetilde{\mathfrak{n}}_{-}$ generates $\widetilde{\mathfrak{n}}_{-}$ as a
Lie algebra (since the elements $f_{1}$, $f_{2}$, $...$, $f_{n}$ of
$\widetilde{\mathfrak{n}}_{-}$ generate $\widetilde{\mathfrak{n}}_{-}$ as a
Lie algebra), and we can easily check that $\psi_{i}\left(  \left\{
f_{1},f_{2},...,f_{n}\right\}  \right)  \subseteq\widetilde{\mathfrak{n}}_{-}%
$\ \ \ \ \footnote{\textit{Proof.} For every $j\in\left\{  1,2,...,n\right\}
$, we have%
\begin{align*}
\psi_{i}\left(  f_{j}\right)   &  =\left[  e_{i},f_{j}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\psi_{i}\right) \\
&  =\delta_{i,j}\underbrace{h_{i}}_{\in\widetilde{\mathfrak{h}}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the relations (\ref{nonserre-relations}%
)}\right) \\
&  \in\widetilde{\mathfrak{h}}\subseteq\widetilde{\mathfrak{n}}_{-}%
\oplus\widetilde{\mathfrak{h}}.
\end{align*}
Thus, $\left\{  \xi_{i}\left(  f_{1}\right)  ,\xi_{i}\left(  f_{2}\right)
,...,\xi_{i}\left(  f_{n}\right)  \right\}  \subseteq\widetilde{\mathfrak{n}%
}_{-}$. Since $\left\{  \xi_{i}\left(  f_{1}\right)  ,\xi_{i}\left(
f_{2}\right)  ,...,\xi_{i}\left(  f_{n}\right)  \right\}  =\xi_{i}\left(
\left\{  f_{1},f_{2},...,f_{n}\right\}  \right)  $, this rewrites as $\xi
_{i}\left(  \left\{  f_{1},f_{2},...,f_{n}\right\}  \right)  \subseteq
\widetilde{\mathfrak{n}}_{-}$, qed.}. Hence, Corollary
\ref{cor.derivation.Lie.unique.ihg} (applied to $\widetilde{\mathfrak{g}}$,
$\widetilde{\mathfrak{n}}_{-}$, $\widetilde{\mathfrak{n}}_{-}$, $\xi_{i}$ and
$\left\{  f_{1},f_{2},...,f_{n}\right\}  $ instead of $\mathfrak{g}$,
$\mathfrak{h}$, $\mathfrak{i}$, $d$ and $S$) yields that $\xi_{i}\left(
\widetilde{\mathfrak{n}}_{-}\right)  \subseteq\widetilde{\mathfrak{n}}_{-}$.
But%
\begin{align*}
\xi_{i}\left(  \widetilde{\mathfrak{n}}_{-}\right)   &  =\left\{
\underbrace{\xi_{i}\left(  x\right)  }_{=\left[  h_{i},x\right]  }\mid
x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left\{  \left[  h_{i},x\right]
\mid x\in\widetilde{\mathfrak{n}}_{-}\right\}  =\left[  h_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  =\left[  h_{i},\widetilde{\mathfrak{n}%
}_{-}\right]  \mathbb{C}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  h_{i}%
,\widetilde{\mathfrak{n}}_{-}\right]  \text{ is a vector space}\right) \\
&  =\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is bilinear}\right)  .
\end{align*}
Thus, $\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}$. Now, forget that we fixed $i$. We thus
have shown that $\left[  h_{i}\mathbb{C},\widetilde{\mathfrak{n}}_{-}\right]
\subseteq\widetilde{\mathfrak{n}}_{-}$ for every $i\in\left\{
1,2,...,n\right\}  $.

[...]

[...]

\textit{Proof of Theorem \ref{thm.g(A).exuni}.} [...] Let
$\widetilde{\mathfrak{g}}$ be the quotient Lie algebra
$\operatorname*{FreeLie}\left(  h_{i},f_{i},e_{i}\right)  \diagup\left(
\text{the relations (\ref{nonserre-relations})}\right)  $. We will abuse
notations and denote the projections of $e_{i}$, $f_{i}$ and $h_{i}$ onto this
quotient $\widetilde{\mathfrak{g}}$ by $e_{i}$, $f_{i}$ and $h_{i}$. Then, let
us make $\widetilde{\mathfrak{g}}$ into a $Q$-graded Lie algebra by setting%
\[
\deg\left(  e_{i}\right)  =\alpha_{i},\ \ \ \ \ \ \ \ \ \ \deg\left(
f_{i}\right)  =-\alpha_{i}\ \ \ \ \ \ \ \ \ \ \text{and }\deg\left(
h_{i}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{
1,2,...,n\right\}  .
\]
We will now prove that $\widetilde{\mathfrak{g}}$ already satisfies conditions
\textbf{(1)} and \textbf{(2)}. In fact, the condition \textbf{(1)} is clearly
satisfied. Let us define Lie subalgebras $\widetilde{\mathfrak{n}}_{+}$,
$\widetilde{\mathfrak{n}}_{-}$ and $\mathfrak{h}^{\prime}$ of
$\widetilde{\mathfrak{g}}$ as follows: Let $\widetilde{\mathfrak{n}}_{+}$ be
the Lie subalgebra of $\widetilde{\mathfrak{g}}$ generated by $e_{1}$, $e_{2}%
$, $...$, $e_{n}$. Let $\widetilde{\mathfrak{n}}_{-}$ be the Lie subalgebra of
$\widetilde{\mathfrak{g}}$ generated by $f_{1}$, $f_{2}$, $...$, $f_{n}$. Let
$\mathfrak{h}^{\prime}$ denote the Lie subalgebra of $\widetilde{\mathfrak{g}%
}$ generated by $h_{1}$, $h_{2}$, $...$, $h_{n}$. It is easy to see that
$\mathfrak{h}^{\prime}$ coincides with the $\mathbb{C}$-linear span
$\left\langle h_{1},h_{2},...,h_{n}\right\rangle $ (since the relations
(\ref{nonserre-relations}) include $\left[  h_{i},h_{j}\right]  =0$).
Moreover, we have%
\[
\widetilde{\mathfrak{n}}_{+}=\bigoplus\limits_{\substack{\alpha\in
Q;\\\alpha\text{ is a linear combination}\\\text{of }\alpha_{i}\text{ with
nonnegative coefficients;}\\\alpha\neq0}}\widetilde{\mathfrak{g}}_{\alpha
}\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \widetilde{\mathfrak{n}%
}_{-}=\bigoplus\limits_{\substack{\alpha\in Q;\\-\alpha\text{ is a linear
combination}\\\text{of }\alpha_{i}\text{ with nonnegative coefficients;}%
\\\alpha\neq0}}\widetilde{\mathfrak{g}}_{\alpha}.
\]


\begin{noncompile}
The Lie subalgebra $\widetilde{\mathfrak{n}}_{+}$ is not only generated by
$e_{1}$, $e_{2}$, $...$, $e_{n}$, but also \textbf{freely} generated by
$e_{1}$, $e_{2}$, $...$, $e_{n}$\ \ \ \ 
\end{noncompile}

\begin{noncompile}
\textit{Proof.} [...]
\end{noncompile}

\begin{noncompile}
2)
%TCIMACRO{\TEXTsymbol{\backslash}}%
%BeginExpansion
$\backslash$%
%EndExpansion
tilde n\_%
%TCIMACRO{\TEXTsymbol{\backslash}}%
%BeginExpansion
$\backslash$%
%EndExpansion
pm are free, and h is freely spanned by h\_i. To see this, it suffices to mod
out by
\end{noncompile}

\begin{noncompile}
(e\_i,h\_i);
\end{noncompile}

\begin{noncompile}
(f\_i,h\_i);
\end{noncompile}

\begin{noncompile}
(e\_i,f\_i),
\end{noncompile}

\begin{noncompile}
respectively (if you do this, all relations disappear, except in the last case
you have [h\_i,h\_j]=0).
\end{noncompile}

\begin{noncompile}%
\begin{align*}
\widetilde{\mathfrak{n}}_{+}  &  =\bigoplus\limits_{\substack{\alpha\in
Q;\\\alpha\text{ is a linear combination}\\\text{of }\alpha_{i}\text{ with
nonnegative coefficients;}\\\alpha\neq0}}\widetilde{\mathfrak{g}}_{\alpha
},\ \ \ \ \ \ \ \ \ \ \widetilde{\mathfrak{n}}_{-}=\bigoplus
\limits_{\substack{\alpha\in Q;\\-\alpha\text{ is a linear combination}%
\\\text{of }\alpha_{i}\text{ with nonnegative coefficients;}\\\alpha\neq
0}}\widetilde{\mathfrak{g}}_{\alpha},\\
\mathfrak{h}^{\prime}  &  =\left\langle h_{1},h_{2},...,h_{n}\right\rangle
\end{align*}
(note that $\left\langle h_{1},h_{2},...,h_{n}\right\rangle $ denotes a
$\mathbb{C}$-linear span, but this also coincides with the Lie subalgebra of
$\widetilde{\mathfrak{g}}$ generated by $h_{1},h_{2},...,h_{n}$, because the
relations (\ref{nonserre-relations}) include $\left[  h_{i},h_{j}\right]  =0$).
\end{noncompile}

\begin{noncompile}
Since $\widetilde{\mathfrak{n}}_{+}$, $\mathfrak{h}^{\prime}$ and
$\widetilde{\mathfrak{n}}_{-}$ are contained in different homogeneous
components of $\widetilde{\mathfrak{g}}$, it is clear that the direct sum
$\widetilde{\mathfrak{n}}_{+}\oplus\mathfrak{h}^{\prime}\oplus
\widetilde{\mathfrak{n}}_{-}$ is well-defined.
\end{noncompile}

\begin{noncompile}
We will now prove that $\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}}%
_{+}\oplus\mathfrak{h}^{\prime}\oplus\widetilde{\mathfrak{n}}_{-}$.
\end{noncompile}

\begin{noncompile}
\textit{Proof.} In the following, I will denote the elements $e_{1}$, $e_{2}$,
$...$, $e_{n}$, $f_{1}$, $f_{2}$, $...$, $f_{n}$, $h_{1}$, $h_{2}$, $...$,
$h_{n}$ as \textit{Chevalley generators}. A \textit{Chevalley term} will mean
a well-defined term consisting purely of Chevalley generators and Lie
brackets. For instance, $\left[  \left[  e_{1},f_{4}\right]  ,\left[
e_{7},\left[  \left[  h_{3},f_{2}\right]  ,e_{1}\right]  \right]  \right]  $
is a Chevalley term (if $n\geq7$).
\end{noncompile}

\begin{noncompile}
To prove $\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}}_{+}\oplus
\mathfrak{h}^{\prime}\oplus\widetilde{\mathfrak{n}}_{-}$, it is enough to show
that any Chevalley term lies either in $\widetilde{\mathfrak{n}}_{+}$ or in
$\mathfrak{h}^{\prime}$ or in $\widetilde{\mathfrak{n}}_{-}$.
\end{noncompile}

\begin{noncompile}
To show this, we make the following observations:
\end{noncompile}

\begin{itemize}
\item
\begin{noncompile}
If $t$ is a Chevalley term which does not contain any of $f_{1}$, $f_{2}$,
$...$, $f_{n}$, then $\left[  f_{j},t\right]  $
\end{noncompile}
\end{itemize}

\begin{noncompile}
This can be shown by induction on the length of this bracket:
\end{noncompile}

To prove the rest, consider the Lie algebra $\widetilde{\mathfrak{g}}^{\prime
}=\operatorname*{FreeLie}\left(  e_{i}\right)  \oplus\mathfrak{h}%
\oplus\operatorname*{FreeLie}\left(  f_{i}\right)  $. Grade this in the
obvious way, so that $\operatorname*{FreeLie}\left(  e_{i}\right)
=\widetilde{\mathfrak{n}}_{+}^{\prime}$ and $\operatorname*{FreeLie}\left(
f_{i}\right)  =\widetilde{\mathfrak{n}}_{-}^{\prime}$. Make this a Lie algebra
by the Leibniz rule, and check that this indeed gives a Lie algebra.

We have a surjective homomorphism $\phi:\widetilde{\mathfrak{g}}%
\rightarrow\widetilde{\mathfrak{g}}^{\prime}$ (since $\widetilde{\mathfrak{g}%
}^{\prime}$ satisfies conditions \textbf{(1)} and \textbf{(2)}). We want to
prove that this $\phi$ is an isomorphism.

\textbf{Claim:} We can write $\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}%
}_{+}\oplus\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}}_{-}$, where
$\widetilde{\mathfrak{n}}_{+}$ is generated by the $e_{i}$ and where
$\widetilde{\mathfrak{n}}_{-}$ is generated by the $f_{i}$, and where
$\widetilde{\mathfrak{h}}$ is generated and actually spanned by $h_{i}$.

\textit{Proof of the Claim:} Any commutator can be written as a commutator of
only $e_{i}$, or of only $f_{i}$, or it lies in $\mathfrak{h}$. (This is
rather clear from the relations (\ref{nonserre-relations}).) Thus, we get a
decomposition $\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}}%
_{+}+\widetilde{\mathfrak{h}}+\widetilde{\mathfrak{n}}_{-}$. Since the addends
live in different degrees, this is a direct sum, and thus we conclude that
$\widetilde{\mathfrak{g}}=\widetilde{\mathfrak{n}}_{+}\oplus
\widetilde{\mathfrak{h}}\oplus\widetilde{\mathfrak{n}}_{-}$.

Hence, $\phi$ is an isomorphism (since $\widetilde{\mathfrak{n}}_{+}^{\prime}$
and $\widetilde{\mathfrak{n}}_{-}^{\prime}$ are free Lie algebras).

So we have proven that $\widetilde{\mathfrak{g}}$ satisfies conditions
\textbf{(1)} and \textbf{(2)}. Now, in order to find a Lie algebra
$\mathfrak{g}$ which additionally satisfies \textbf{(3)}, we do something
cheap: Let $I$ be the sum of all $Q$-graded ideals in $\widetilde{\mathfrak{g}%
}$ which have zero intersection with $\mathfrak{h}$. Let $\mathfrak{g}%
=\widetilde{\mathfrak{g}}\diagup I$.

We now claim that $\mathfrak{g}$ is a contragredient Lie algebra corresponding
to $A$. To prove this, we must check that conditions \textbf{(1)},
\textbf{(2)} and \textbf{(3)} are satisfied. We know that conditions
\textbf{(1)} and \textbf{(2)} hold (since they hold for $\mathfrak{g}$, and
since "intersection with $\mathfrak{h}$" is the same as "projection onto the
$0$-th graded component" due to our grading!), so it only remains to check
\textbf{(3)}.

If $J\subseteq\widetilde{\mathfrak{g}}\diagup I$ is a graded ideal such that
$J\cap\mathfrak{h}=0$, then the preimage $\widetilde{J}$ of $J$ in
$\widetilde{\mathfrak{g}}$ is also such an ideal and properly contains $J$,
which is a contradiction. So $\mathfrak{g}$ is a contragredient Lie algebra
corresponding to $A$.

If $\mathfrak{g}^{\prime}$ is another contragredient Lie algebra corresponding
to $A$, then we have a surjective map $\widetilde{\mathfrak{g}}\rightarrow
\mathfrak{g}^{\prime}$, and $I$ is killed by this map, so we get a surjective
map $\psi:\mathfrak{g}\rightarrow\mathfrak{g}^{\prime}$. The kernel
$\operatorname*{Ker}\psi$ is a $Q$-graded ideal, so that $\operatorname*{Ker}%
\psi\cap\mathfrak{h}=0$, and thus (by condition \textbf{(3)}) we have
$\operatorname*{Ker}\psi=0$ and thus $\psi$ is a graded isomorphism.

\begin{remark}
Let $A$ be a complex $n\times n$ matrix. One can show that the Lie algebra
$\mathfrak{g}\left(  A\right)  $ is finite-dimensional if and only if $A$ is
the Cartan matrix of a semisimple finite-dimensional Lie algebra. (In this
case, $\mathfrak{g}\left(  A\right)  $ is exactly this semisimple Lie algebra,
and the ideal $I$ of the above proof is generated by the Serre relations
$\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)  ^{1-a_{i,j}}e_{j}$
and $\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}%
}f_{j}=0$.)
\end{remark}

\subsection{Kac-Moody algebras for generalized Cartan matrices}

For general $A$, we do not know much about $\mathfrak{g}\left(  A\right)  $;
its definition was not even constructive (find that $I$ !). It is not known in
general how to obtain generators for $I$. But for some particular cases -- not
only Cartan matrices of semisimple Lie algebras --, things behave well. Here
is the most important such case:

\begin{definition}
An $n\times n$ matrix $A=\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$ of
complex numbers is said to be a \textit{generalized Cartan matrix} if it satisfies:

\textbf{(1)} We have $a_{i,i}=2$ for all $i\in\left\{  1,2,...,n\right\}  $.

\textbf{(2)} For every $i$ and $j$, the number $a_{i,j}$ is a nonpositive
integer. Also, $a_{i,j}=0$ if and only if $a_{j,i}=0$.

\textbf{(3)} The matrix $A$ is symmetrizable, i. e., there exists a diagonal
matrix $D>0$ such that $\left(  DA\right)  ^{T}=DA$.
\end{definition}

Note that a Cartan matrix is the same as a generalized Cartan matrix $A$ with
$DA>0$.

\begin{example}
Let $A=\left(
\begin{array}
[c]{cc}%
2 & -m\\
-1 & 2
\end{array}
\right)  $ for $m\geq1$. This matrix $A$ is a generalized Cartan matrix, since
$\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & m
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
2 & -m\\
-1 & 2
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
2 & -m\\
-m & 2m
\end{array}
\right)  $. Note that $\det\left(
\begin{array}
[c]{cc}%
2 & -m\\
-1 & 2
\end{array}
\right)  =4-m$.

For $m=1$, we have $\mathfrak{g}\left(  A\right)  \cong A_{2}=\mathfrak{sl}%
_{3}$.

For $m=2$, we have $\mathfrak{g}\left(  A\right)  \cong B_{2}\cong C_{2}%
\cong\mathfrak{sp}_{4}\cong\mathfrak{so}_{5}$.

For $m=3$, we have $\mathfrak{g}\left(  A\right)  \cong G_{2}$.

For $m\geq4$, the Lie algebra $\mathfrak{g}\left(  A\right)  $ is infinite-dimensional.

For $m=4$, it is a twisted version of $\widehat{\mathfrak{sl}_{2}}$, called
$A_{2}^{2}$.

For $m\geq5$, the Lie algebra $\mathfrak{g}\left(  A\right)  $ is big (in the
sense of having exponential growth).

This strange behaviour is related to the behaviour of the $m$-subspaces
problem (finite for $m\leq3$, tame for $m=4$, wild for $m\geq5$). More
generally, Kac-Moody algebras are related to representation theory of quivers.
\end{example}

\begin{definition}
A \textit{symmetrizable Kac-Moody algebra} is a Lie algebra of the form
$\mathfrak{g}\left(  A\right)  $ for a generalized Cartan matrix $A$.
\end{definition}

\begin{theorem}
[Gabber-Kac]\label{thm.g(A).gabber-kac}If $A$ is a generalized Cartan matrix,
then the ideal $I\subseteq\widetilde{\mathfrak{g}}\left(  A\right)  $ (of the
proof of Theorem \ref{thm.g(A).exuni}) is generated by the Serre relations.
\end{theorem}

\textit{Partial proof of Theorem \ref{thm.g(A).gabber-kac}.} Proving this
theorem requires showing two assertions: first, that the Serre relations are
contained in $I$; second, that they actually generate $I$. We will only prove
the first of these two assertions.

Let us show that $\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}\in I_{-}$ (where we write $I$ as $I_{+}\oplus I_{-}$, with
$I_{+}=I\cap\widetilde{\mathfrak{n}}_{+}$ and $I_{-}=I\cap
\widetilde{\mathfrak{n}}_{-}$; this is ok since $I$ is $Q$-graded).

To do that, it is sufficient to show that $\left[  e_{k},\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}\right]  =0$
for all $k$. (If we grade $\widetilde{\mathfrak{g}}$ by setting $\deg\left(
f_{i}\right)  =-1$, $\deg\left(  e_{i}\right)  =1$ and $\deg\left(
h_{i}\right)  =0$ (this is called the \textit{principal grading}), then
$f_{k}$ can only lower degree, so that the Lie ideal generated by $\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}$ will lie
entirely in negative degrees, and thus $\left(  \operatorname*{ad}\left(
f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}$ will lie in $I_{-}$.)

\textit{Case 1:} We have $k\neq i,j$. This case is clear since $e_{k}$
commutes with $f_{i}$ and $f_{j}$ (by our relations).

\textit{Case 2:} We have $k=j$. In this case,
\begin{align*}
\left[  e_{k},\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}\right]   &  =\left[  e_{j},\left(  \operatorname*{ad}\left(
f_{i}\right)  \right)  ^{1-a_{i,j}}f_{j}\right]  =\left(  \operatorname*{ad}%
\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}\left(  \left[  e_{j},f_{j}\right]
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{ad}\left(
f_{i}\right)  \text{ and }\operatorname*{ad}\left(  e_{j}\right)  \text{
commute, due to }i\neq j\right) \\
&  =\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)  ^{1-a_{i,j}}%
h_{j}.
\end{align*}
We now distinguish between two cases according to whether $a_{i,j}$ is $=0$ or
$<0$:

\textit{Case 2a:} We have $a_{i,j}=0$. Then, $a_{j,i}=0$ by the definition of
generalized Cartan matrices. Thus, $\left[  f_{i},h_{j}\right]  =-\left[
h_{j},f_{i}\right]  =-a_{j,i}f_{i}=0$, and we are done.

\textit{Case 2b:} We have $a_{i,j}<0$. Then, $1-a_{i,j}\geq2$. Now, $\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  ^{2}h_{j}=\left(
\operatorname*{ad}\left(  f_{i}\right)  \right)  \left(  cf_{i}\right)  =0$
for some constant $c$.

\textit{Case 3:} We have $k=i$. Let $\left(  \mathfrak{sl}_{2}\right)
_{i}=\left\langle e_{i},f_{i},h_{i}\right\rangle $. Let $M$ be the $\left(
\mathfrak{sl}_{2}\right)  _{i}$-submodule in $\widetilde{\mathfrak{g}}\left(
A\right)  $ generated by $f_{j}$.

We have $\left[  h_{i},f_{j}\right]  =-a_{i,j}f_{j}=mf_{j}$, where
$m=-a_{i,j}\geq0$. Together with $\left[  e_{i},f_{j}\right]  =0$, this shows
that $f_{j}=:v$ is a highest-weight vector of $M$ with weight $m$. Thus,
$f_{i}^{m+1}v=\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}$ is a singular vector for $\left(  \mathfrak{sl}_{2}\right)
_{i}$ (by representation theory of $\mathfrak{sl}_{2}$\ \ \ \ \footnote{What
we are using is the following: Consider the module $M_{\lambda}=\mathbb{C}%
\left[  f\right]  v$ over $\mathfrak{sl}_{2}$. Then, $ef^{n}v=n\left(
\lambda-n+1\right)  f^{n-1}v$. Thus, when $n=m+1$ and $\lambda=m$, we get
$ef^{n}v=0$.}).

So much for our part of the proof of Theorem \ref{thm.g(A).gabber-kac}.

Of course, simple Lie algebras are Kac-Moody algebras. The next class of
Kac-Moody algebras we are interested in is the \textit{affine Lie algebras}:

\begin{remark}
Let $\sigma$ be a permutation, and $A$ be an $n\times n$ complex matrix. Then,
$\mathfrak{g}\left(  A\right)  \cong\mathfrak{g}\left(  \sigma A\sigma
^{-1}\right)  $.
\end{remark}

\begin{remark}
Let $A_{1}$ and $A_{2}$ be two square complex matrices. Then, $\mathfrak{g}%
\left(  A_{1}\oplus A_{2}\right)  \cong\mathfrak{g}\left(  A_{2}\right)
\oplus\mathfrak{g}\left(  A_{2}\right)  $ naturally.
\end{remark}

\begin{definition}
A generalized Cartan matrix $A$ is said to be \textit{indecomposable} if it
cannot be written in the form $\sigma\left(  A_{1}\oplus A_{2}\right)
\sigma^{-1}$ for some permutation $\sigma$ and nontrivial square matrices
$A_{1}$ and $A_{2}$. Due to the above two remarks, we need to only consider
indecomposable generalized Cartan matrices.
\end{definition}

\begin{definition}
A generalized Cartan matrix $A$ is said to be \textit{affine} if $DA\geq0$ but
$DA\not >  0$ (thus, $\det\left(  DA\right)  =0$).
\end{definition}

\begin{definition}
If $A$ is an affine generalized Cartan matrix, then $\mathfrak{g}\left(
A\right)  $ is called an \textit{affine Kac-Moody algebra}.
\end{definition}

Now let $A$ be the (usual) Cartan matrix of a simple Lie algebra, and let
$\mathfrak{g}=\mathfrak{g}\left(  A\right)  $ be this simple Lie algebra. Let
$L\mathfrak{g}=\mathfrak{g}\left[  t,t^{-1}\right]  $, and let
$\widehat{\mathfrak{g}}=L\mathfrak{g}\oplus\mathbb{C}K$ as defined long ago.

\begin{theorem}
This $\widehat{\mathfrak{g}}$ is an affine Kac-Moody algebra with generalized
Cartan matrix $\widetilde{A}$ whose $\left(  1,1\right)  $-entry is $2$ and
whose submatrix obtained by omitting the first row and the first column is
$A$. (We do not yet say what the remaining entries are.)
\end{theorem}

\textit{Proof of Theorem.} Let $\mathfrak{h}$ be the Cartan subalgebra of
$\mathfrak{g}$. Let $r=\dim\mathfrak{h}$; thus, $r$ is the rank of
$\mathfrak{g}$. Let $\left(  h_{1},h_{2},...,h_{r}\right)  $ be a
corresponding basis of $\mathfrak{h}$, and let $e_{i},f_{i}$ be standard
generators for every $i\in\left\{  1,2,...,r\right\}  $.

Let $\theta$ be the maximal root.

Let us now define elements $e_{0}=f_{\theta}\cdot t$, $f_{0}=e_{\theta}\cdot
t^{-1}$ and $h_{0}=\left[  e_{0},f_{0}\right]  =-h_{\theta}%
+\underbrace{\left(  f_{\theta},e_{\theta}\right)  }_{=1\text{ (due to our
normalization)}}K=K-h_{\theta}$ of $\widehat{\mathfrak{g}}$ (the commutator is
computed in $\widehat{\mathfrak{g}}$, not in $L\mathfrak{g}$).

Add these elements to our system of generators.

Why do we then get a system of generators of $\widehat{\mathfrak{g}}$ ?

First, $h_{i}$ for $i\in\left\{  0,1,...,r\right\}  $ are a basis of
$\widehat{\mathfrak{h}}=\mathfrak{h}\oplus\mathbb{C}K$.

Also, $\mathfrak{g}t^{0}$ is generated by $e_{i},f_{i},h_{i}$ for
$i\in\left\{  1,2,...,r\right\}  $. Now, $\mathfrak{g}t^{1}$ is an irreducible
$\mathfrak{g}$-module with lowest-weight vector $f_{\theta}\cdot t$.

$\Longrightarrow$ $U\left(  \mathfrak{g}\right)  \cdot f_{\theta
}t=\mathfrak{g}\cdot t$. Now, $\mathfrak{g}\cdot t$ generated $\mathfrak{g}%
t\mathbb{C}\left[  t\right]  $ (since $\left[  \mathfrak{g},\mathfrak{g}%
\right]  =\mathfrak{g}$). Similarly, $U\left(  \mathfrak{g}\right)  \cdot
e_{\theta}t^{-1}=\mathfrak{g}t^{-1}$, and $\mathfrak{g}t^{-1}$ generates
$\mathfrak{g}t^{-1}\mathbb{C}\left[  t^{-1}\right]  $. $\Longrightarrow$
generated everything.

Now to the relations.

$\left[  h_{i},h_{j}\right]  =0$ is clear for all $\left(  i,j\right)
\in\left\{  0,1,...,r\right\}  ^{2}$.

We have $\left[  h_{0},e_{0}\right]  =\left[  K-h_{\theta},f_{\theta}t\right]
=-\left[  h_{\theta},f_{\theta}\right]  t=2f_{\theta}t=2e_{0}$.

We have $\left[  h_{0},f_{0}\right]  =-2f_{0}$ similarly.

We have $\left[  e_{0},f_{0}\right]  =h_{0}$.

We have $\left[  h_{0},e_{i}\right]  =\left[  K-h_{\theta},e_{i}\right]
=-\alpha_{i}\left(  h_{\theta}\right)  e_{i}=-\left(  \alpha_{i}%
,\theta\right)  e_{i}$ $\Longrightarrow$ $a_{0,i}=-\left(  \alpha_{i}%
,\theta\right)  =\left(  \text{some nonpositive integer}\right)  $.

We have $\left[  h_{0},f_{i}\right]  =\left(  \alpha_{i},\theta\right)  f_{i}%
$, same argument.

We have $\left[  h_{i},e_{0}\right]  =\left[  h_{i},f_{\theta}t\right]
=-\theta\left(  h_{i}\right)  f_{\theta}t=-\theta\left(  h_{i}\right)
e_{0}=-\left(  \alpha_{i}^{\vee},\theta\right)  e_{0}$ (where $\alpha
_{i}^{\vee}=\dfrac{2\alpha_{i}}{\left(  \alpha_{i},\alpha_{i}\right)  }$)
$\Longrightarrow$ $a_{i,0}=-\left(  \alpha_{i}^{\vee},\theta\right)  $.

We have $\left[  h_{i},f_{0}\right]  =\left(  \alpha_{i}^{\vee},\theta\right)
f_{0}$, same argument.

We have $\left[  e_{0},f_{i}\right]  =\left[  f_{\theta}t,f_{i}\right]  =0$.

We have $\left[  e_{i},f_{0}\right]  =\left[  e_{i},e_{\theta}t^{-1}\right]
=0$.

Thus, all basic relations are satisfied.

Now let us define a grading: $\widehat{Q}=Q\oplus\mathbb{Z}\delta$, where $Q$
is the root lattice of $\mathfrak{g}$. Define $\alpha_{0}=\delta-\theta$.
$\delta\mid_{\widehat{\mathfrak{h}}}=0$. So if we think of $\alpha_{0}$ as an
element of $\widehat{\mathfrak{h}}^{\ast}$, then $\alpha_{0},\alpha
_{1},...,\alpha_{r}$ is neither linearly independent nor spanning. So the
direct sum $Q\oplus\mathbb{Z}\delta$ is an exterior direct sum, not an
interior one!!

$\widehat{Q}$-grading: $\deg\left(  e_{i}\right)  =\alpha_{i}$, $\deg\left(
f_{i}\right)  =-\alpha_{i}$ and $\deg\left(  h_{i}\right)  =0$ for
$i=0,1,...,r$. Also $\deg\left(  at^{k}\right)  =\deg a+k\delta$ (so, so to
speak, "$\deg t=\delta$").

So we have $\widehat{\mathfrak{g}}\left[  0\right]  =\widehat{\mathfrak{h}}$
and $\widehat{\mathfrak{g}}\left[  \alpha_{i}\right]  =\left\langle
e_{i}\right\rangle $ and $\widehat{\mathfrak{g}}\left[  -\alpha_{i}\right]
=\left\langle f_{i}\right\rangle $.

Note (which we won't use): $\left[  h,a\right]  =\alpha\left(  h\right)  a$,
$a\in\widehat{\mathfrak{g}}\left[  \alpha\right]  $ "if you define things this way".

The only thing we now have to do is to show that $I=0$ in
$\widehat{\mathfrak{g}}$.

Let $\overline{I}$ be the projection of $I$ to $L\mathfrak{g}%
=\widehat{\mathfrak{g}}\diagup\left(  K\right)  $. Clearly, $\overline{I}%
\cap\mathfrak{h}=0$.

We must prove that $\overline{I}=0$.

But there is a \textbf{claim} that any $\widehat{Q}$-graded ideal in
$L\mathfrak{g}$ is $0$ or $L\mathfrak{g}$. (\textit{Proof:} If $J$ is a
$\widehat{Q}$-graded ideal of $L\mathfrak{g}$ different from $0$, then there
exists a nonzero $a\in\mathfrak{g}$ and an $m\in\mathbb{Z}$ such that
$at^{m}\in J$. But $at^{m}$ generates $L\mathfrak{g}$ under the action of
$L\mathfrak{g}$, since $\left[  bt^{n-m},at^{m}\right]  =\left[  b,a\right]
t^{n}$ and $\mathfrak{g}=\left[  \mathfrak{g},\mathfrak{g}\right]  $.)

Proof of Theorem complete.

Let us show how Dynkin diagrams look like for these affine Kac-Moody algebras.

Consider the case of $A_{n-1}=\mathfrak{sl}_{n}$. Then, $\theta=\left(
1,0,0,...,0,-1\right)  $. Also, $\alpha_{1}=\left(  1,-1,0,0,...,0\right)  $,
$\alpha_{2}=\left(  0,1,-1,0,0,...,0\right)  $, $...$, $\alpha_{n-1}=\left(
0,0,...,0,1,-1\right)  $. Also, $\alpha=\alpha^{\vee}$ for all simple roots
$\alpha$. We thus have $\left(  \theta,\alpha_{i}\right)  =1$ if $\alpha
\in\left\{  1,n-1\right\}  $ and $=0$ otherwise. The Dynkin diagram of
$\widehat{A_{n-1}}=A_{n-1}^{1}=\widehat{\mathfrak{sl}_{n}}$ (these are just
three notations for one and the same thing) is thus $%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
%{-}[r] & \circ\ar@{-}[r] & \circ}}}%
%BeginExpansion
\xymatrix{
\circ\ar@{-}[r] & \circ\ar@{-}[r] & \circ\ar@{}[r]|-{...} & \circ\ar@
{-}[r] & \circ\ar@{-}[r] & \circ}%
%EndExpansion
$ with a cyclically connected dot underneath.

The case $n=2$ is special: double link. $\circ=\circ$ double link.

Now let us consider other types. Suppose that $\theta$ is a fundamental
weight, i. e., satisfies $\left(  \theta,\alpha_{i}^{\vee}\right)  =1$ for
some $i$ and satisfies $\left(  \theta,\alpha_{i}^{\vee}\right)  =0$ for all
other $i$. (This happens for a lot of simple Lie algebras.)

To get $\widehat{D_{n}}=\widehat{\mathfrak{so}_{2n}}$, need to attach a new
vertex to the second vertex from the left.

To get $\widehat{C_{n}}=\widehat{\mathfrak{sp}_{2n}}$, need to attach a new
vertex \textbf{doubly-linked} to the first vertex from the left. (The arrow
points to the right, i. e., to the $C_{n}$ diagram.)

For $\widehat{G_{2}}$, attach a vertex on the left (where the arrow points to
the right).

For $\widehat{F_{4}}$, attach a vertex on the left (where the arrow points to
the right).

For $\widehat{E_{6}}$, attach a vertex to the "bottom" (the vertex off the line).

For $\widehat{E_{7}}$, attach a vertex to the short leg (to make the graph symmetric).

For $\widehat{E_{8}}$, attach a vertex to the long leg.

These are untwisted affine Lie algebras ($\widehat{\mathfrak{g}}$).

There are also twisted ones: $A_{2}^{2}$ with Cartan matrix $\left(
\begin{array}
[c]{cc}%
2 & -4\\
-1 & 2
\end{array}
\right)  $ and Dynkin diagram $\circ\left(  4\text{ arrows pointing
rightward}\right)  \circ$. We will not discuss this kind of Lie algebras here.

\subsection{Representation theory of $\mathfrak{g}\left(  A\right)  $}

We will now work out the representation theory of $\mathfrak{g}\left(
A\right)  $.

Let us start with the case of $\mathfrak{g}\left(  A\right)  $ being
finite-dimensional. In contrast with usual courses on Lie algebras, we will
not restrict ourselves to finite-dimensional representations. We define a
Category $\mathcal{O}$ which is analogous but (in its details) somewhat
different from the one we gave above. In future, we will use only the new definition.

\begin{definition}
\label{def.O}The objects of \textit{category }$\mathcal{O}$ will be
$\mathfrak{g}$-modules $M$ such that:

\textbf{1)} The module $M$ is $\mathfrak{h}$-diagonalizable. By this we mean
that $M=\bigoplus\limits_{\mu\in\mathfrak{h}^{\ast}}M\left[  \mu\right]  $
(where $M\left[  \mu\right]  $ means the $\mu$-weight space of $M$), and every
$\mu\in\mathfrak{h}^{\ast}$ satisfies $\dim\left(  M\left[  \mu\right]
\right)  <\infty$.

\textbf{2)} Let $\operatorname*{Supp}M$ denote the set of all $\mu
\in\mathfrak{h}^{\ast}$ such that $M\left[  \mu\right]  \neq0$. Then, there
exist finitely many $\lambda_{1},\lambda_{2},...,\lambda_{n}\in\mathfrak{h}%
^{\ast}$ such that $\operatorname*{Supp}M\subseteq D\left(  \lambda
_{1}\right)  \cup D\left(  \lambda_{2}\right)  \cup...\cup D\left(
\lambda_{n}\right)  $, where for every $\lambda\in\mathfrak{h}^{\ast}$, we
denote by $D\left(  \lambda\right)  $ the subset%
\[
\left\{  \lambda-k_{1}\alpha_{1}-k_{2}\alpha_{2}-...-k_{r}\alpha_{r}%
\ \mid\ \left(  k_{1},k_{2},...,k_{r}\right)  \in\mathbb{N}^{r}\right\}
\ \ \ \ \ \ \ \ \ \ \text{of }\mathfrak{h}^{\ast}.
\]


The \textit{morphisms of category }$\mathcal{O}^{+}$ will be $\mathfrak{g}%
$-module homomorphisms.
\end{definition}

Examples of modules in Category $\mathcal{O}$ are Verma modules $M_{\lambda
}=M_{\lambda}^{+}$ and their irreducible quotients $L_{\lambda}$ (and all of
their quotients). Category $\mathcal{O}$ is an abelian category (in our case,
this simply means it is closed under taking subquotients and direct sums).

\begin{definition}
Let $M\in\mathcal{O}$ be a $\mathfrak{g}$-module. Then, the \textit{formal
character} of $M$ denotes the sum $\operatorname*{ch}M=\sum\limits_{\mu
\in\mathfrak{h}^{\ast}}\dim\left(  M\left[  \mu\right]  \right)  e^{\mu}$.
Here $\mathbb{C}\left[  \mathfrak{h}^{\ast}\right]  $ denotes the group
algebra of the additive group $\mathfrak{h}^{\ast}$, where this additive group
$\mathfrak{h}^{\ast}$ is written multiplicatively and every $\mu
\in\mathfrak{h}^{\ast}$ is renamed as $e^{\mu}$.

Where does this sum $\sum\limits_{\mu\in\mathfrak{h}^{\ast}}\dim\left(
M\left[  \mu\right]  \right)  e^{\mu}$ lie?

Let $\Gamma$ be a coset of $Q$ (the root lattice) in $\mathfrak{h}^{\ast}$.
Then, let $R_{\Gamma}$ denote the space $\lim\limits_{\mu\in\Gamma}e^{\mu
}\mathbb{C}\left[  \left[  e^{-\alpha_{1}},e^{-\alpha_{2}},...,e^{-\alpha_{r}%
}\right]  \right]  $ (this is a union, but not a disjoint union, since
$R_{\mu}\subseteq R_{\mu+\alpha_{i}}$ for all $i$ and $\mu$). Let
$R=\bigoplus\limits_{\Gamma\in\mathfrak{h}^{\ast}\diagup Q}$. This $R$ is a
ring. We view $\operatorname*{ch}M$ as an element of $R$.
\end{definition}

Now, for an example, let us compute the formal character $\operatorname*{ch}%
\left(  M_{\lambda}\right)  $ of the Verma module $M_{\lambda}=U\left(
\mathfrak{n}_{-}\right)  v_{\lambda}$.

Recall that $U\left(  \mathfrak{n}_{-}\right)  $ has a
Poincar\'{e}-Birkhoff-Witt basis consisting of all elements of the form
$f_{\alpha^{\left(  1\right)  }}^{m_{1}}f_{\alpha^{\left(  2\right)  }}%
^{m_{2}}...f_{\alpha^{\left(  \ell\right)  }}^{m_{\ell}}$ where $\alpha
^{\left(  1\right)  },\alpha^{\left(  2\right)  },...,\alpha^{\left(
\ell\right)  }$ are all positive roots of $\mathfrak{g}$, and $\ell
=\dim\left(  \mathfrak{n}_{-}\right)  $. The weight of this element
$f_{\alpha^{\left(  1\right)  }}^{m_{1}}f_{\alpha^{\left(  2\right)  }}%
^{m_{2}}...f_{\alpha^{\left(  \ell\right)  }}^{m_{\ell}}$ is $-\left(
m_{1}\alpha^{\left(  1\right)  }+m_{2}\alpha^{\left(  2\right)  }+...+m_{\ell
}\alpha^{\left(  \ell\right)  }\right)  $. Thus, the weight of $f_{\alpha
^{\left(  1\right)  }}^{m_{1}}f_{\alpha^{\left(  2\right)  }}^{m_{2}%
}...f_{\alpha^{\left(  \ell\right)  }}^{m_{\ell}}v_{\lambda}$ is
$\lambda-\left(  m_{1}\alpha^{\left(  1\right)  }+m_{2}\alpha^{\left(
2\right)  }+...+m_{\ell}\alpha^{\left(  \ell\right)  }\right)  $.

Thus, $\dim\left(  M_{\lambda}\left[  \lambda-\beta\right]  \right)  $ is the
number of partitions of $\beta$ into positive roots. We denote this by
$p\left(  \beta\right)  $, and call $p$ the \textit{Kostant partition
function}.

Now, it is very easy (using geometric series) to see that%
\[
\sum\limits_{\beta\in Q_{+}}p\left(  \beta\right)  e^{-\beta}=\prod
\limits_{\substack{\alpha\text{ root;}\\a>0}}\dfrac{1}{1-e^{-\alpha}}.
\]
Thus,%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\sum\limits_{\beta\in Q_{+}%
}p\left(  \beta\right)  e^{\lambda-\beta}=e^{\lambda}\underbrace{\sum
\limits_{\beta\in Q_{+}}p\left(  \beta\right)  e^{-\beta}}_{=\prod
\limits_{\substack{\alpha\text{ root;}\\a>0}}\dfrac{1}{1-e^{-\alpha}}%
}=e^{\lambda}\prod\limits_{\substack{\alpha\text{ root;}\\a>0}}\dfrac
{1}{1-e^{-\alpha}}.
\]


\textbf{Example:} Let $\mathfrak{g}=\mathfrak{sl}_{2}$. Then,%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{e^{\lambda}%
}{1-e^{-\alpha}}=e^{\lambda}+e^{\lambda-\alpha}+e^{\lambda-2\alpha}+....
\]
Classically, one identifies weights of $\mathfrak{sl}_{2}$ with elements of
$\mathbb{C}$ (by $\omega_{1}\mapsto1$ and thus $\alpha\mapsto2$). Write $x$
for $e^{\omega_{1}}$. Then,%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{x^{\lambda}}{1-x^{-2}%
}=x^{\lambda}+x^{\lambda-2}+x^{\lambda-4}+....
\]
The quotient $L_{\lambda}$ has weights $\lambda$, $\lambda-2$, $...$,
$-\lambda$ and thus satisfies%
\[
\operatorname*{ch}\left(  L_{\lambda}\right)  =x^{\lambda}+x^{\lambda
-2}+...+x^{-\lambda}=\dfrac{x^{\lambda+1}-x^{-\lambda-1}}{x-x^{-1}}.
\]


Back to the general case of finite-dimensional $\mathfrak{g}\left(  A\right)
$. First of all, category $\mathcal{O}$ has tensor products, and they make it
into a tensor category.

\begin{proposition}
\textbf{1)} We have $\operatorname*{ch}\left(  M_{1}\otimes M_{2}\right)
=\operatorname*{ch}\left(  M_{1}\right)  \cdot\operatorname*{ch}\left(
M_{2}\right)  $.

\textbf{2)} If $N\subseteq M$ are both in $\mathcal{O}$, then
$\operatorname*{ch}M=\operatorname*{ch}N+\operatorname*{ch}\left(  M\diagup
N\right)  $.
\end{proposition}

\textit{Proof of Proposition.} \textbf{1)}
\[
\left(  M_{1}\otimes M_{2}\right)  \left[  \mu\right]  =\bigoplus
\limits_{\mu_{1}+\mu_{2}=\mu}M_{1}\left[  \mu_{1}\right]  \otimes M_{2}\left[
\mu_{2}\right]  .
\]


\textbf{2)}
\[
\left(  M\diagup N\right)  \left[  \mu\right]  =M\left[  \mu\right]  \diagup
N\left[  \mu\right]  .
\]


Now, let us generalize to the case of Kac-Moody (or $\mathfrak{g}\left(
A\right)  $ for general $A$). Here we run into troubles: For example, for
$\widehat{\mathfrak{sl}_{2}}$, we have $M_{\lambda}=U\left(
\widetilde{\mathfrak{n}}_{-}\right)  v_{\lambda}$, and the vectors
$ht^{-1}v_{\lambda},ht^{-2}v_{\lambda},...$ all have weight $\lambda$ with
respect to $\widehat{\mathfrak{h}}=\left\langle h_{0},h_{1}\right\rangle $
with $h_{1}=h,$ $h_{0}=K-h$. This yields that weight spaces are
infinite-dimensional, and we cannot define characters.

Let us work around this by adding derivations.

Assume that $A$ is an $r\times r$ complex matrix. Let $\mathfrak{g}%
_{\operatorname*{ext}}\left(  A\right)  =\mathfrak{g}\left(  A\right)
\oplus\bigoplus\limits_{i=1}^{r}\mathbb{C}D_{i}$ with new relations%
\begin{align*}
\left[  D_{i},D_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i,j;\\
\left[  D_{i},e_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\neq
j;\\
\left[  D_{i},f_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\neq
j;\\
\left[  D_{i},h_{j}\right]   &  =0\ \ \ \ \ \ \ \ \ \ \text{for all }i\neq
j;\\
\left[  D_{i},e_{i}\right]   &  =e_{i};\\
\left[  D_{i},f_{i}\right]   &  =-f_{i};\\
\left[  D_{i},h_{i}\right]   &  =0.
\end{align*}
Note that this definition is equivalent to making $\mathfrak{g}%
_{\operatorname*{ext}}\left(  A\right)  $ a semidirect product, so there is no
cancellation here.

We have $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  =\mathfrak{n}%
_{+}\oplus\mathfrak{h}_{\operatorname*{ext}}\oplus\mathfrak{n}_{-}$ where
$\mathfrak{h}_{\operatorname*{ext}}=\mathbb{C}^{r}\oplus\mathfrak{h}$ (here
the $\mathbb{C}^{r}$ is spanned by the $\mathbb{C}D_{i}$).

Consider $\alpha_{i}$ as maps $\mathfrak{h}_{\operatorname*{ext}}%
\rightarrow\mathbb{C}$ given by $\alpha_{i}\left(  h_{j}\right)  =a_{j,i}$ and
$\alpha_{i}\left(  D_{j}\right)  =\delta_{i,j}$.

Then, for every $h\in\mathfrak{h}_{\operatorname*{ext}}$, we have $\left[
h,e_{i}\right]  =\alpha_{i}\left(  h\right)  e_{i}$ and $\left[
h,f_{i}\right]  =-\alpha_{i}\left(  h\right)  f_{i}$.

Let $F=Q\otimes_{\mathbb{Z}}\mathbb{C}$ and $P=\mathfrak{h}^{\ast}\oplus F$.

Let $\varphi:P\rightarrow\mathfrak{h}_{\operatorname*{ext}}^{\ast}$ be given
by $\varphi\left(  h_{i}^{\ast}\right)  \left(  D_{j}\right)  =0$,
$\varphi\left(  h_{i}^{\ast}\right)  \left(  h_{j}\right)  =\delta_{i,j}$,
$\varphi\left(  \alpha_{i}\right)  \left(  D_{j}\right)  =\delta_{i,j}$,
$\varphi\left(  \alpha_{i}\right)  \left(  h_{j}\right)  =a_{j,i}$.

Easy to see $\varphi$ is an iso.

Now the trouble disappears. Do the same as for simple Lie algebras. Now
weights lie in $\mathfrak{h}_{\operatorname*{ext}}^{\ast}$.

Annoying fact: Now, even when $A$ is a Cartan matrix and $\mathfrak{g}$ is
simple finite-dimensional, this is not the same as the usual theory [what?].
But it is equivalent. Namely: Suppose $\chi\in\mathfrak{h}%
_{\operatorname*{ext}}^{\ast}$. Let $\mathcal{O}_{\chi}$ be the category of
modules whose weights lie in $\chi+F$. Therefore, $\mathcal{O}=\bigoplus
\limits_{\chi\in\mathfrak{h}^{\ast}}\mathcal{O}_{\chi}$.

\begin{proposition}
If $\chi_{1}-\chi_{2}\in\operatorname*{Im}\left(  F\rightarrow\mathfrak{h}%
^{\ast}\right)  $, then $\mathcal{O}_{\chi_{1}}\cong\mathcal{O}_{\chi_{2}}$.
\end{proposition}

(See Feigin-Zelevinsky paper for proof.)

If $A$ is invertible (in particular, for simple $\mathfrak{g}$), all
$\mathcal{O}_{\chi}$ are the same and we just have a single category
$\mathcal{O}$ (which is the category $\mathcal{O}$ we defined).

Affine case: $\operatorname*{Coker}\left(  F\rightarrow\mathfrak{h}^{\ast
}\right)  $ is $1$-dimensional, so $\chi$ has one essential parameter (namely,
the image $k$ of $\chi$ in this $\operatorname*{Coker}$). So we get a
$1$-parameter category of categories, $\mathcal{O}\left(  k\right)  $,
parametrized by a complex number $k$. In our old approach to
$\widehat{\mathfrak{g}}$, this $k$ is the level of representations (i. e., the
eigenvalue of the action of $K$). So we did not get anything new, but we have
got a uniform way to treat all cases of this kind.

\subsection{Invariant bilinear forms}

Now let us start developing the theory of invariant bilinear forms.

Let $A$ be an indecomposable complex matrix. We want to see when we can have
nontrivial nonzero invariant symmetric bilinear forms on
$\widetilde{\mathfrak{g}}\left(  A\right)  $ and $\mathfrak{g}\left(
A\right)  $. Let us only care about forms of degree $0$, which means that they
send $\mathfrak{g}_{\alpha}\times\mathfrak{g}_{\beta}$ to $0$ unless
$\alpha+\beta=0$. It also sounds like a good goal to have the forms
nondegenerate, but this cannot always be reached. Let us impose the weaker
condition that, if $e_{i}$ and $f_{i}$ denote generators of $\mathfrak{g}%
_{\alpha_{i}}$ and $\mathfrak{g}_{-\alpha_{i}}$, respectively, then $\left(
e_{i},f_{i}\right)  =d_{i}$ for some $d_{i}\neq0$.

These conditions already force some properties upon $\mathfrak{g}\left(
A\right)  $: First,
\[
\left(  h_{i},h_{j}\right)  =\left(  h_{i},\left[  e_{j},f_{j}\right]
\right)  =-\left(  \left[  h_{i},f_{j}\right]  ,e_{j}\right)  =a_{i,j}\left(
f_{j},e_{j}\right)  =a_{i,j}d_{j},
\]
so that the symmetry of our form (and the condition $d_{i}\neq0$) enforces
$a_{i,j}d_{j}=a_{j,i}d_{i}$. Thus, if $D$ denotes the matrix
$\operatorname*{diag}\left(  d_{1},d_{2},...,d_{r}\right)  $, then $\left(
AD\right)  ^{\perp}=AD$. This means that $A$ is symmetrizable. (Our definition
of "symmetrizable" spoke of $DA$ instead of $AD$, but this is simply a matter
of replacing $D$ by $D^{-1}$.)

\begin{lemma}
Let $A$ be an indecomposable symmetrizable matrix. Then, there is a unique
diagonal matrix $D$ satisfying $\left(  AD\right)  ^{\perp}=AD$ up to scaling.
\end{lemma}

This lemma is purely combinatorial and more or less trivial.

\begin{proposition}
Let $A$ be an indecomposable symmetrizable matrix. Then, there is at most one
invariant symmetric bilinear form of degree $0$ on $\widetilde{\mathfrak{g}%
}\left(  A\right)  $ up to scaling.
\end{proposition}

Note that the degree in "degree $0$" is the degree with respect to
$Q$-grading; this is a tuple.

\textit{Proof of Proposition.} Let $B$ be such a form. Then, we can view $B$
as a $\mathfrak{g}$-module homomorphism from $B^{\vee}:\mathfrak{g}%
\rightarrow\mathfrak{g}^{\ast}$. If we fix $d_{i}$ (uniquely up to scaling, as
we know from Lemma), then we know $B^{\vee}\left(  h_{i}\right)  $, $B^{\vee
}\left(  f_{i}\right)  $ and $B^{\vee}\left(  e_{i}\right)  $ (because the
form is of degree $0$, and thus the linear maps $B^{\vee}\left(  h_{i}\right)
$, $B^{\vee}\left(  f_{i}\right)  $ and $B^{\vee}\left(  e_{i}\right)  $ are
determined by what they do to the corresponding elements of the corresponding
degree). But $\mathfrak{g}$ is generated as a $\mathfrak{g}$-module by
$e_{i},f_{i},h_{i}$, so $B$ is uniquely determined if it exists. Proposition
is proven.

\begin{theorem}
Let $A$ be a symmetrizable matrix. Then, there is a nonzero invariant bilinear
symmetric form of degree $0$ on $\widetilde{\mathfrak{g}}\left(  A\right)  $.
(We know from the previous proposition that this form is unique up to scaling
if $A$ is indecomposable.)
\end{theorem}

\textit{Proof of Theorem (incomplete, as we will skip some steps).} First, fix
the $d_{i}$. Then, we can calculate the form by%
\begin{align*}
&  \left(  \underbrace{\left[  e_{i_{1}},\left[  e_{i_{2}},...\left[
e_{i_{n-1}},e_{i_{n}}\right]  ...\right]  \right]  }_{\in\mathfrak{g}_{\alpha
}},\underbrace{\left[  f_{j_{1}},\left[  f_{j_{2}},...\left[  f_{j_{n-1}%
},f_{j_{n}}\right]  ...\right]  \right]  }_{\in\mathfrak{g}_{-\alpha}}\right)
\\
&  =-\left(  \left[  e_{i_{1}},...\right]  ,\underbrace{\left[  \left[
e_{i_{2}},\left[  e_{i_{3}},...\left[  e_{i_{n-1}},e_{i_{n}}\right]
...\right]  \right]  ,\left[  f_{j_{1}},\left[  f_{j_{2}},...\left[
f_{j_{n-1}},f_{j_{n}}\right]  ...\right]  \right]  \right]  }_{\in
\mathfrak{g}_{-\alpha}}\right) \\
&  +...
\end{align*}
induction on $\alpha$. For details and well-definedness, see page 51 of the
Feigin-Zelevinsky paper.

Also, $\widetilde{\mathfrak{g}}\left(  A\right)  $ has such a form by pullback.

As usual, denote these forms by $\left(  \left(  \cdot,\cdot\right)  \right)
$.

\begin{proposition}
The kernel $I$ of the canonical projection $\widetilde{\mathfrak{g}}\left(
A\right)  \rightarrow\mathfrak{g}\left(  A\right)  $ is a subset of
$\operatorname*{Ker}\left(  \left(  \cdot,\cdot\right)  \right)  $.
\end{proposition}

\textit{Proof of Proposition.} We defined the form $\left(  \cdot
,\cdot\right)  $ on $\widetilde{\mathfrak{g}}\left(  A\right)  \times
\widetilde{\mathfrak{g}}\left(  A\right)  $ as the pullback of the form
$\left(  \cdot,\cdot\right)  :\mathfrak{g}\left(  A\right)  \times
\mathfrak{g}\left(  A\right)  \rightarrow\mathbb{C}$ through the canonical
projection $\widetilde{\mathfrak{g}}\left(  A\right)  \times
\widetilde{\mathfrak{g}}\left(  A\right)  \rightarrow\mathfrak{g}\left(
A\right)  \times\mathfrak{g}\left(  A\right)  $. Thus, it is clear that the
kernel of the former form contains the kernel of the canonical projection
$\widetilde{\mathfrak{g}}\left(  A\right)  \rightarrow\mathfrak{g}\left(
A\right)  $. Proposition proven.

\begin{lemma}
\textbf{1)} The center $Z$ of $\mathfrak{g}\left(  A\right)  $ is contained in
$\mathfrak{h}$, and is%
\[
Z=\left\{  \sum\limits_{i}\beta_{i}h_{i}\ \mid\ \sum\limits_{i}\beta
_{i}a_{i,j}=0\right\}  .
\]


\textbf{2)} If $A$ is an indecomposable symmetrizable matrix, and $A\neq0$,
then any graded proper ideal in $\mathfrak{g}\left(  A\right)  $ is contained
in $Z$.

\textbf{3)} If $a_{i,i}\neq0$ for all $i$, then $\left[  \mathfrak{g}\left(
A\right)  ,\mathfrak{g}\left(  A\right)  \right]  =\mathfrak{g}\left(
A\right)  $.
\end{lemma}

\textit{Proof of Lemma.} \textbf{1)} Let $z$ be a nonzero central element of
$\mathfrak{g}\left(  A\right)  $. We can WLOG assume that $z$ is homogeneous.
Then, $\mathbb{C}z$ is a graded nonzero ideal of $\mathfrak{g}\left(
A\right)  $, so that $\deg z$ must be $0$, and thus $z\in\mathfrak{h}$. If
$z=\sum\limits_{i}\beta_{i}h_{i}$, then $0=\left[  z,e_{j}\right]  =\left[
\sum\limits_{i}\beta_{i}h_{i},e_{j}\right]  =\left(  \sum\limits_{i}\beta
_{i}a_{i,j}\right)  e_{j}$, so that $\sum\limits_{i}\beta_{i}a_{i,j}=0$, qed.

\textbf{2)} Let $I\neq0$ be a graded ideal. Then, $I\cap\mathfrak{h}\neq0$. So
$I=I_{+}\oplus I_{0}\oplus I_{-}$ with $I_{0}$ being a nonzero subspace of
$\mathfrak{h}$. Assume $I\not \subseteq Z$. Then we claim that $I_{+}\neq0$ or
$I_{-}\neq0$.

(In fact, otherwise, we would have $I_{+}=0$ and $I_{-}=0$, so that
$I\subseteq\mathfrak{h}$, so that there exists some $h\in I\subseteq
\mathfrak{h}$ with $h\notin Z$, so that $\left[  h,e_{j}\right]  =\lambda
e_{j}$ for some $j$ and some $\lambda\neq0$, so that $e_{j}\in I_{+}$,
contradicting $I_{+}=0$ and $I_{-}=0$.)

So let us WLOG assume $I_{+}\neq0$. Then there exists a nonzero $a\in
I_{+}\left[  \alpha\right]  $ for some $\alpha$. Set $J$ be the ideal
generated by $a$. In other words, $J=U\left(  \mathfrak{g}\left(  A\right)
\right)  \cdot a$. This $J$ is a graded ideal. Thus, $J\cap\mathfrak{h}\neq0$.
Hence, there exists $x\in U\left(  \mathfrak{g}\left(  A\right)  \right)  $
such that $xa\in\mathfrak{h}$ and $xa\neq0$. We can WLOG assume that $x$ has
degree $-\alpha$, and is a monomial in the $f_{i}$ and the $e_{i}$. Let
$x=f_{i_{1}}...f_{i_{m}}e_{j_{1}}...e_{j_{n}}$. Let $y=f_{i_{2}}...f_{i_{m}%
}e_{j_{1}}...e_{j_{n}}\rightharpoonup x\in\mathfrak{g}_{\alpha_{i}}$, and
$\left[  f_{i},y\right]  \neq0$ (but $\left[  f_{i},y\right]  =-\lambda h_{i}$
and $\left[  f_{i},y\right]  =\lambda e_{i}$ or what?). $\Longrightarrow$
There exists $i$ such that $e_{i},h_{i}\in I$.

If $A$ is an $1\times1$ matrix, then $a_{i,i}\neq0$ (for $\mathfrak{sl}_{2}$),
so that $f_{i}=\dfrac{\left[  h_{i},f_{i}\right]  }{a_{i,i}}\in I$, so that
$I=\mathfrak{g}\left(  A\right)  $.

If the size of $A$ is $>1$, there exists some $j\neq i$ such that $a_{i,j}%
\neq0$ and $a_{j,i}\neq0$ (since $A$ is symmetrizable), so that $e_{j}%
=\dfrac{\left[  h_{i},e_{j}\right]  }{a_{i,j}}\in I$, $f_{j}\in I$, $h_{j}\in
I$, $f_{i}=\dfrac{\left[  h_{j},f_{i}\right]  }{a_{j,i}}\in I$. If $k\neq i$,
$a_{i,k}\neq0$, $a_{k,i}\neq0$ $\Longrightarrow$ get $h_{k}$, $f_{k}$, $e_{k}$
$\in I$ etc. $\Longrightarrow$ all generators are in $I$ $\Longrightarrow$
$I=\mathfrak{g}\left(  A\right)  $.

\textbf{3)} If $a_{i,i}\neq0$, then the relations imply that all generators
are in $\left[  \mathfrak{g}\left(  A\right)  ,\mathfrak{g}\left(  A\right)
\right]  $.

Qed.

\begin{proposition}
Assume that $A$ is symmetrizable. We have $\operatorname*{Ker}\left(  \left(
\cdot,\cdot\right)  \mid_{\mathfrak{g}\left(  A\right)  }\right)  =Z\left(
\mathfrak{g}\left(  A\right)  \right)  $.
\end{proposition}

\textit{Proof of Proposition.} Assume WLOG that $A$ is indecomposable.

\textbf{1)} $1\times1$ case, $A=0$ trivial: $\left[  e,f\right]  =h$, $\left[
h,e\right]  =\left[  h,f\right]  =0$, $\left(  e,f\right)  =1$. Then the
kernel of this form is a graded ideal and is not $\mathfrak{g}\left(
A\right)  $. Hence, it must be contained in $Z$ by the lemma. But
$Z\subseteq\operatorname*{Ker}\left(  \left(  \cdot,\cdot\right)
\mid_{\mathfrak{g}\left(  A\right)  }\right)  $ is easy (because $\left(
\sum\limits_{i}\beta_{i}h_{i},h_{j}\right)  =\sum\limits_{i}\beta_{i}%
a_{i,j}d_{j}=0$).

Let $F=Q\otimes_{\mathbb{Z}}C=\bigoplus_{i=1}^{r}\mathbb{C}\alpha_{i}$.

Define $\gamma:F\rightarrow\mathfrak{h}$ isomorphism by $\gamma\left(
\alpha_{i}\right)  =d_{i}^{-1}h_{i}=:h_{\alpha_{i}}$. Extend by linearity:
$\gamma\left(  \alpha\right)  $ will be called $h_{\alpha}$, $\alpha\in F$.

\textbf{Claim:} $\left(  h_{\alpha},h\right)  =\overline{\alpha}\left(
h\right)  $, where $\overline{\alpha}$ is the image of $\alpha$ in
$\mathfrak{h}^{\ast}$.

Proof: $\left(  h_{\alpha_{i}},h_{j}\right)  =d_{i}^{-1}\left(  h_{i}%
,h_{j}\right)  =d_{i}^{-1}a_{i,j}d_{j}=d_{i}^{-1}a_{j,i}d_{i}=a_{j,i}%
=\overline{\alpha_{i}}\left(  h_{j}\right)  \ \ \ \ $ ($\left[  h_{j}%
,e_{i}\right]  =a_{j,i}e_{i}$).

\begin{proposition}
If $x\in\mathfrak{g}_{\alpha}$ and $y\in\mathfrak{g}_{-\alpha}$, then $\left[
x,y\right]  =\left(  x,y\right)  h_{\alpha}$.
\end{proposition}

\textit{Proof of Proposition.} By induction over $\left\vert \alpha\right\vert
$, where $\left\vert \alpha\right\vert $ means the sum of the coordinates of
$\alpha$.

\textit{Base:} $\left\vert \alpha\right\vert =1$, $\alpha=\alpha_{i}$. Want to
prove $\left[  e_{i},f_{i}\right]  =^{?}\left(  e_{i},f_{i}\right)
h_{\alpha_{i}}$. But $\left[  e_{i},f_{i}\right]  =h_{i}$ and $\left(
e_{i},f_{i}\right)  h_{\alpha_{i}}=d_{i}d_{i}^{-1}h_{i}$, so we are done with
the base.

\textit{Step:} For $x\in\mathfrak{g}_{\alpha-\alpha_{i}}$ and $y\in
\mathfrak{g}_{\alpha-\alpha_{j}}$, we have
\begin{align*}
&  \left[  \left[  e_{i},x\right]  ,\left[  f_{j},y\right]  \right] \\
&  =\left[  \left[  e_{i},\left[  f_{j},y\right]  \right]  ,x\right]  +\left[
e_{i},\left[  x,\left[  f_{j},y\right]  \right]  \right] \\
&  =-\left(  \left[  e_{i},\left[  f_{j},y\right]  \right]  ,x\right)
h_{\alpha-\alpha_{i}}+\left(  e_{i},\left[  x,\left[  f_{j},y\right]  \right]
\right)  h_{\alpha_{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the induction assumption}\right) \\
&  =\left(  \left[  f_{j},y\right]  ,\left[  e_{i},x\right]  \right)  \left(
h_{\alpha-\alpha_{i}}+h_{\alpha_{i}}\right)  =\left(  \left[  e_{i},x\right]
,\left[  f_{j},y\right]  \right)  h_{\alpha}.
\end{align*}
Induction step complete. Proposition proven.

\begin{corollary}
If we give $\mathfrak{g}\left(  A\right)  $ the principal $\mathbb{Z}$-grading
(so that $\mathfrak{g}\left(  A\right)  \left[  n\right]  =\bigoplus
\limits_{\substack{\alpha\in Q;\\\left\vert \alpha\right\vert =n}%
}\mathfrak{g}\left(  A\right)  \left[  \alpha\right]  $), then $\mathfrak{g}%
\left(  A\right)  $ is a nondegenerate Lie algebra.
\end{corollary}

\textit{Proof.} If $\lambda\in\mathfrak{h}^{\ast}$ is such that $\lambda
\left(  h_{\alpha}\right)  \neq0$, then $\lambda\left(  \left[  x,y\right]
\right)  $ is a nondegenerate form $\mathfrak{g}_{\alpha}\times\mathfrak{g}%
_{-\alpha}\rightarrow\mathbb{C}$. Qed.

Recall $P=\mathfrak{h}^{\ast}\oplus F\cong\mathfrak{h}_{\operatorname*{ext}%
}^{\ast}$.

$\left(  \cdot,\cdot\right)  $ on $P$: $\left(  \underbrace{\varphi}%
_{\in\mathfrak{h}^{\ast}}\oplus\underbrace{\alpha}_{\in F},\underbrace{\psi
}_{\in\mathfrak{h}^{\ast}}\oplus\underbrace{\beta}_{\in F}\right)
=\psi\left(  h_{\alpha}\right)  +\varphi\left(  h_{\beta}\right)  +\left(
h_{\alpha},h_{\beta}\right)  $

$\left(  h_{\alpha_{i}},h_{\alpha_{j}}\right)  =d_{i}^{-1}d_{j}^{-1}\left(
h_{i},h_{j}\right)  =d_{i}^{-1}d_{j}^{-1}a_{i,j}d_{j}=d_{i}^{-1}a_{i,j}$.

Basis $h_{\alpha_{i}}^{\ast}\in\mathfrak{h}^{\ast}$, $\alpha_{i}\in F$
$\Longrightarrow$ matrix of the form $\left(
\begin{array}
[c]{cc}%
0 & 1\\
1 & D^{-1}A
\end{array}
\right)  $.

Inverse form on $\mathfrak{h}_{\operatorname*{ext}}$: dual basis:
$h_{\alpha_{i}},D_{i}$.

$\left(  D_{i},D_{j}\right)  =0$, $\left(  D_{i},h_{\alpha_{j}}\right)
=\delta_{i,j}$, $\left(  h_{\alpha_{i}},h_{\alpha_{j}}\right)  =d_{i}%
^{-1}a_{i,j}$.

\begin{proposition}
The form on $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)
=\mathfrak{g}\left(  A\right)  \oplus\mathbb{C}D_{1}\oplus\mathbb{C}%
D_{2}\oplus...\oplus\mathbb{C}D_{r}$ defined by this is a nondegenerate
symmetric invariant form.
\end{proposition}

\subsection{Casimir element}

We now define the Casimir element. The problem with the classical "sum of
squares of orthonormal basis" construction which works well in the
finite-dimensional case is that now we are infinite-dimensional and such a sum
needs to be defined.

Note that it will be a generalization of the $L_{0}$ of the Sugawara construction.

Define $\rho\in\mathfrak{h}^{\ast}$ by $\rho\left(  h_{i}\right)
=\dfrac{a_{i,i}}{2}$ (in the Kac-Moody case, this becomes $\rho\left(
h_{i}\right)  =1$).

$\left(  \rho,\rho\right)  =0$.

Case of a finite-dimensional simple Lie algebra: $\Delta=\sum\limits_{a\in
B}a^{2}=\sum\limits_{i=1}^{r}x_{i}^{2}+2h_{\rho}+2\sum\limits_{\alpha
>0}f_{\alpha}e_{\alpha}$ where $\left(  x_{i}\right)  _{i=1,...,r}$ is an
orthonormal basis of $\mathfrak{h}$.

In the infinite-dimensional case, we fix a basis $\left(  e_{\alpha}%
^{i}\right)  _{i}$ of $\mathfrak{g}_{\alpha}$ for every $\alpha$, and a dual
basis $\left(  f_{\alpha}^{i}\right)  _{i}$ of $\mathfrak{g}_{-\alpha}$ under
the inner product. Then define $\Delta_{+}=2\sum\limits_{\alpha>0}%
\sum\limits_{i}f_{\alpha}^{i}e_{\alpha}^{i}$ and $\Delta_{0}=\sum
\limits_{j}x_{j}^{2}+2h_{\rho}$ (where $\left(  x_{j}\right)  $ is an
orthonormal basis of $\mathfrak{h}_{\operatorname*{ext}}$). We set
$\Delta=\Delta_{+}+\Delta_{0}$.

Note that $\Delta_{+}$ is an infinite sum and not in $U\left(  \mathfrak{g}%
\left(  A\right)  \right)  $. But it becomes finite after applying to any
vector in a module in category $\mathcal{O}$.

\begin{theorem}
\textbf{1)} The operator $\Delta$ commutes with $\mathfrak{g}\left(  A\right)
$.

\textbf{2)} We have $\Delta\mid_{M_{\lambda}}=\left(  \lambda,\lambda
+2\rho\right)  \operatorname*{id}$.
\end{theorem}

\textit{Proof of Theorem.} Let us first prove \textbf{2)} using \textbf{1)}:

\textbf{2)} We have $\Delta v_{\lambda}=\Delta_{0}v_{\lambda}=\left(
\sum\limits_{j}\lambda\left(  x_{j}\right)  ^{2}+2\lambda\left(  h_{\rho
}\right)  \right)  v_{\lambda}=\left(  \left(  \lambda,\lambda\right)
+2\left(  \lambda,\rho\right)  \right)  v_{\lambda}=\left(  \lambda
,\lambda+2\rho\right)  v_{\lambda}$.

From \textbf{1)}, we see that every $a\in U\left(  \mathfrak{g}\left(
A\right)  \right)  $ satisfies $\Delta av_{\lambda}=a\Delta v_{\lambda
}=\left(  \lambda,\lambda+2\rho\right)  av_{\lambda}$. This proves \textbf{2)}
since $M_{\lambda}=U\left(  \mathfrak{g}\left(  A\right)  \right)  v_{\lambda
}$.

\textbf{1)} We need to show that $\left[  \Delta,e_{i}\right]  =\left[
\Delta,f_{i}\right]  =0$.

Let us prove $\left[  \Delta,e_{i}\right]  =0$ (the proof of $\left[
\Delta,f_{i}\right]  =0$ is similar).

We have $\left[  \Delta_{0},e_{i}\right]  =\left[  \sum x_{j}^{2}+2h\rho
,e_{i}\right]  =\sum x_{j}\left[  x_{j},e_{i}\right]  +\sum\left[  x_{j}%
,e_{i}\right]  x_{j}+2\left(  \alpha_{i},\rho\right)  e_{i}$

$=\sum x_{j}\underbrace{\alpha_{i}\left(  x_{j}\right)  }_{=\left(
h_{\alpha_{i}},x_{j}\right)  }e_{i}+\sum\alpha_{i}\left(  x_{j}\right)
e_{i}x_{j}+2\left(  \alpha_{i},\rho\right)  e_{i}$

$=2h_{\alpha_{i}}e_{i}-\sum\underbrace{\alpha_{i}\left(  x_{j}\right)
}_{=\left(  \alpha_{i},\alpha_{i}\right)  e_{i}}\alpha_{i}\left(
x_{j}\right)  e_{i}+2\left(  \alpha_{i},\rho\right)  e_{i}=2h_{\alpha}e_{i}$

$\Longrightarrow$ Our job is to show $\left[  \Delta_{+},e_{i}\right]
=-2h_{\alpha_{i}}e_{i}$.But

$\left[  \Delta_{+},e_{i}\right]  =2\sum\limits_{\alpha>0}f_{\alpha}%
^{j}\left[  e_{\alpha}^{j},e_{i}\right]  +2\underbrace{\sum\limits_{\alpha
>0}\left[  f_{\alpha}^{j},e_{i}\right]  e_{\alpha}^{j}}_{\substack{\text{for
}\alpha=\alpha_{i}\text{ the addend is}\\-2h_{\alpha_{i}}e_{i}\\\text{because
}f_{\alpha_{i}}=d_{i}^{-1}f_{i}\text{, }e_{\alpha_{i}}=e_{i}\text{,}\\\left[
d_{i}^{-1}f_{i},e_{i}\right]  e_{i}=-d_{i}^{-1}h_{i}e_{i}=-h_{\alpha_{i}}%
e_{i}}}$.

So we need to show that%
\[
\sum\limits_{\alpha>0}f_{\alpha}^{j}\left[  e_{\alpha}^{j},e_{i}\right]
+2\sum\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}}}\left[  f_{\alpha
}^{j},e_{i}\right]  e_{\alpha}^{j}=0.
\]
For this it is enough to check%
\[
\sum\limits_{\alpha>0}f_{\alpha}^{j}\otimes\left[  e_{\alpha}^{j}%
,e_{i}\right]  +2\sum\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}%
}}\left[  f_{\alpha}^{j},e_{i}\right]  \otimes e_{\alpha}^{j}=0.
\]
For this it is enough to check that $\left[  e_{i},e_{\alpha}^{k}\right]
=\sum\left(  e_{\beta}^{k},\left[  f_{\alpha}^{j},e_{i}\right]  \right)
e_{\alpha}^{j}$. This is somehow obvious. Proof complete.

\textbf{Exercise:} for $\widehat{\mathfrak{g}}$ (affine), $\Delta=\left(
k+h^{\vee}\right)  \left(  L_{0}-d\right)  $ (Sugawara).

\subsection{Preparations for the Weyl-Kac character formula}

Let $A$ be a symmetrizable generalized Cartan matrix, WLOG indecomposable.

We consider the Kac-Moody algebra $\mathfrak{g}=\mathfrak{g}\left(  A\right)
\subseteq\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  $.

\begin{proposition}
The Serre relations $\left(  \operatorname*{ad}\left(  e_{i}\right)  \right)
^{1-a_{i,j}}e_{j}=\left(  \operatorname*{ad}\left(  f_{i}\right)  \right)
^{1-a_{i,j}}f_{j}=0$ hold in $\mathfrak{g}\left(  A\right)  $.
\end{proposition}

This is a part of Theorem \ref{thm.g(A).gabber-kac} (actually, the part that
we proved above).

\begin{definition}
Let $A$ be an associative algebra (with $1$, as always). Let $V$ be an $A$-module.

\textbf{(a)} Let $v\in V$. Then, the vector $v$ is said to be \textit{of
finite type} if $\dim\left(  Av\right)  <\infty$.

\textbf{(b)} The $A$-module $V$ is said to be \textit{locally finite} if every
$v\in V$ is of finite type.
\end{definition}

It is very easy to check that:

\begin{proposition}
Let $A$ be an associative algebra (with $1$, as always). Let $V$ be an
$A$-module. Then, $V$ is locally finite if and only if $V$ is a sum of
finite-dimensional $A$-modules.
\end{proposition}

\begin{Convention}
If $\mathfrak{g}$ is a Lie algebra, then "locally finite" and "of finite type"
with respect to $\mathfrak{g}$ mean locally finite resp. of finite type with
respect to $U\left(  \mathfrak{g}\right)  $.
\end{Convention}

In the following, let $A=U\left(  \mathfrak{g}\right)  $ for $\mathfrak{g}%
=\mathfrak{g}\left(  A\right)  $.

\begin{definition}
Let $V$ be a $\mathfrak{g}\left(  A\right)  $-module. We say that $V$ is
\textit{integrable} if $V$ is locally finite under the $\mathfrak{sl}_{2}%
$-subalgebra $\left(  \mathfrak{sl}_{2}\right)  _{i}=\left\langle e_{i}%
,f_{i},h_{i}\right\rangle $ for every $i\in\left\{  1,2,...,r\right\}  $.
\end{definition}

To motivate the terminology "integrable", let us notice:

\begin{proposition}
If $V$ is a $\mathfrak{sl}_{2}$-module, then $V$ is locally finite if and only
if $V$ is isomorphic to a direct sum $\bigoplus\limits_{n=0}^{\infty}%
W_{n}\otimes V_{n}$, where $W_{n}$ are vector spaces and $V_{n}$ is the
irreducible representation of $\mathfrak{sl}_{2}$ of highest weight $n$ (so
that $\dim\left(  V_{n}\right)  =n+1$) for every $n\in\mathbb{N}$. (In such a
direct sum, we have $W_{n}\cong\operatorname*{Hom}\nolimits_{\mathfrak{sl}%
_{2}}\left(  V_{n},V\right)  $.)

Locally-finite $\mathfrak{sl}_{2}$-modules can be lifted to modules over the
\textbf{algebraic group} $\operatorname*{SL}\nolimits_{2}\left(
\mathbb{C}\right)  $.
\end{proposition}

Since lifting is called "integrating" (in analogy to geometry, where an action
of a Lie group gives rise to an action of the corresponding of the Lie algebra
by "differentiation", and thus the converse operation, when it makes sense, is
called "integration"), the last sentence of this proposition explains the name "integrable".

\begin{proposition}
\label{prop.weylkac.gint}The $\mathfrak{g}$-module $\mathfrak{g}%
=\mathfrak{g}\left(  A\right)  $ itself is integrable.
\end{proposition}

The proof of this proposition is based on the following lemma:

\begin{lemma}
\label{lem.weylkac.fintypfintyp}Let $\mathfrak{a}$ be a Lie algebra, and
$\mathfrak{b}$ be another Lie algebra. Assume that we are given a Lie algebra
homomorphism $\mathfrak{b}\rightarrow\operatorname*{Der}\mathfrak{a}$; this
makes $\mathfrak{a}$ into a $\mathfrak{b}$-module. Then, if $x,y\in
\mathfrak{a}$ are of finite type for $\mathfrak{b}$, then so is $\left[
x,y\right]  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.fintypfintyp}.} In $\mathfrak{a}$ (not
in $U\left(  \mathfrak{a}\right)  $), we have%
\[
U\left(  \mathfrak{b}\right)  \cdot\left[  x,y\right]  \subseteq\left[
\underbrace{U\left(  \mathfrak{b}\right)  \cdot x}_{\text{finite dimensional}%
},\underbrace{U\left(  \mathfrak{b}\right)  \cdot y}_{\text{finite
dimensional}}\right]  .
\]
Hence, $U\left(  \mathfrak{b}\right)  \cdot\left[  x,y\right]  $ is
finite-dimensional. Hence, $\left[  x,y\right]  $ is of finite type for
$\mathfrak{b}$. Lemma \ref{lem.weylkac.fintypfintyp} is proven.

\textit{Proof of Proposition \ref{prop.weylkac.gint}.} We know that $e_{i}$ is
of finite type under $\left(  \mathfrak{sl}_{2}\right)  _{i}$ (in fact,
$e_{i}$ generates a $3$-dimensional representation of $\left(  \mathfrak{sl}%
_{2}\right)  _{i}$), and that $e_{j}$ is of finite type under $\left(
\mathfrak{sl}_{2}\right)  _{i}$ for every $j\neq i$ (in fact, $e_{j}$
generates a representation of dimension $1-a_{i,j}$). The same applies to
$f_{j}$, and hence also to $h_{j}$ (by Lemma \ref{lem.weylkac.fintypfintyp}).
Hence (again using Lemma \ref{lem.weylkac.fintypfintyp}), the whole
$\mathfrak{g}\left(  A\right)  $ is locally finite under $\left(
\mathfrak{sl}_{2}\right)  _{i}$. [Fix some stuff here.] Proposition
\ref{prop.weylkac.gint} is proven.

\begin{proposition}
If $V$ is a $\mathfrak{g}\left(  A\right)  $-module, then $V$ is integrable if
and only if $V$ has a collection of generators $v_{\alpha}$ which are of
finite type under $\left(  \mathfrak{sl}_{2}\right)  _{i}$ for each $i$.
\end{proposition}

Note that this proposition could just as well be formulated for every Lie
algebra $\mathfrak{g}$ instead of $\mathfrak{g}\left(  A\right)  $.

\textit{Proof of Proposition.} $\Longleftarrow:$ Let $v\in V$. We need to show
that $v$ is of finite type under $\left(  \mathfrak{sl}_{2}\right)  _{i}$ for
all $i$.

Fix some $i$. Then, there exist $v_{1},v_{2},...,v_{m}$ such that $v\in
U\left(  \mathfrak{g}\right)  \cdot v_{1}+...+U\left(  \mathfrak{g}\right)
\cdot v_{m}=:V^{\prime}\subseteq V$ (where $\mathfrak{g}=\mathfrak{g}\left(
A\right)  $). Pick $W\subseteq V^{\prime}$ a finite-dimensional $\left(
\mathfrak{sl}_{2}\right)  _{i}$-subrepresentation such that $v_{1}%
,v_{2},...,v_{m}\in W$. Then we have a surjective homomorphism of
$\mathfrak{g}$-modules $U\left(  \mathfrak{g}\right)  \otimes W\rightarrow
V^{\prime}$, where $\left(  \mathfrak{sl}_{2}\right)  _{i}$ acts on $U\left(
\mathfrak{g}\right)  $ by adjoint action. So it suffices to show that
$U\left(  \mathfrak{g}\right)  $ is integrable for the adjoint action of
$\mathfrak{g}$. But by the symmetrization map (which is an isomorphism by
PBW), we have $U\left(  \mathfrak{g}\right)  \cong S\left(  \mathfrak{g}%
\right)  =\bigoplus\limits_{m\in\mathbb{N}}S^{m}\left(  \mathfrak{g}\right)  $
(as $\mathfrak{g}$-modules) (this is true for every Lie algebra over a field
of characteristic $0$). Since $S^{m}\left(  \mathfrak{g}\right)  $ injects
into $\mathfrak{g}^{\otimes m}$, and since $\mathfrak{g}^{\otimes m}$ is
integrable (because $\mathfrak{g}$ is (in fact, it is easy to see that if $X$
and $Y$ are locally finite $\mathfrak{a}$-modules, then so is $X\otimes Y$)),
this yields that $U\left(  \mathfrak{g}\right)  \otimes W$ is integrable, and
thus $V^{\prime}$ (being a quotient module of $U\left(  \mathfrak{g}\right)
\otimes W$) is integrable also as well.

$\Longrightarrow:$ Trivial (take all vectors of $V$ as generators).

Proposition proven.

\begin{corollary}
Let $L_{\lambda}$ be the irreducible highest-weight module for $\mathfrak{g}%
\left(  A\right)  $. Then, $L_{\lambda}$ is integrable if and only if for
every $i\in\left\{  1,2,...,r\right\}  $, the value $\lambda\left(
h_{i}\right)  $ is a nonnegative integer.
\end{corollary}

\textit{Proof of Corollary.} $\Longrightarrow:$ We have $e_{i}v_{\lambda}=0$,
$h_{i}v_{\lambda}=\lambda\left(  h_{i}\right)  v_{\lambda}$, and $v_{\lambda}$
is of finite type under $\left(  \mathfrak{sl}_{2}\right)  _{i}$. But as we
know from weight theory of finite-dimensional representations of $\left(
\mathfrak{sl}_{2}\right)  _{i}$, this yields that $\lambda\left(
h_{i}\right)  $ is a nonnegative integer.

$\Longleftarrow:$ We have%
\begin{align*}
e_{i}f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}  &  =\left(
\lambda\left(  h_{i}\right)  +1\right)  \underbrace{\left(  \lambda\left(
h_{i}\right)  -\left(  \lambda\left(  h_{i}\right)  +1\right)  +1\right)
}_{=0}f_{i}^{\lambda\left(  h_{i}\right)  }v_{\lambda}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the formula }e_{i}f_{i}^{m}v_{\lambda
}=m\left(  \lambda\left(  h_{i}\right)  -m+1\right)  f_{i}^{m-1}v_{\lambda
}\right) \\
&  =0.
\end{align*}
Hence, $f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}$ must also be zero
(since otherwise, this vector would generate a proper graded submodule). This
implies that $v_{\lambda}$ generates a finite-dimensional $\left(
\mathfrak{sl}_{2}\right)  _{i}$-module of dimension $\lambda\left(
h_{i}\right)  +1$ with basis $\left(  v_{\lambda},f_{i}v_{\lambda}%
,...,f_{i}^{\lambda\left(  h_{i}\right)  }v_{\lambda}\right)  $. Hence,
$v_{\lambda}$ is of finite type with respect to $\left(  \mathfrak{sl}%
_{2}\right)  _{i}$.

By the previous proposition, this yields that $L_{\lambda}$ is integrable.
Proof of Corollary complete.

\begin{remark}
Assume that for every $i\in\left\{  1,2,...,r\right\}  $, the value
$\lambda\left(  h_{i}\right)  $ is a nonnegative integer. Then, the relations
$f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}=0$ are defining for
$L_{\lambda}$.
\end{remark}

We will not prove this now, but this will follow from things we do later (from
the main theorem for the character formula).

\begin{definition}
A weight $\lambda$ for which all $\lambda\left(  h_{i}\right)  $ are
nonnegative integers is called \textit{integral} (for $\mathfrak{g}\left(
A\right)  $ or for $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  $).
\end{definition}

Now, our next goal is to compute the character of $L_{\lambda}$ for any
dominant integral weight $\lambda$.

For finite-dimensional simple Lie algebras, these $L_{\lambda}$ are exactly
the finite-dimensional irreducible representations, and their characters can
be computed by the well-known Weyl character formula. So our goal is to
generalize this formula.

The Weyl character formula involves a summation over the Weyl group. So, first
of all, we need to define a "Weyl group" for Kac-Moody Lie algebras.

\subsection{Weyl group}

\begin{definition}
Consider $P=\mathfrak{h}^{\ast}\oplus F$. We know that there is a
nondegenerate form $\left(  \cdot,\cdot\right)  $ on $P$, and we have $\dim
P=2r$. Let $i\in\left\{  1,2,...,r\right\}  $. Let $r_{i}:P\rightarrow P$ be
the map given by $r_{i}\left(  \chi\right)  =\chi-\chi\left(  h_{i}\right)
\alpha_{i}$.
\end{definition}

Note that $r_{i}$ is an involution, since%
\[
r_{i}\left(  \chi\right)  =\chi-\chi\left(  h_{i}\right)  \alpha_{i}%
-\chi\left(  h_{i}\right)  \alpha_{i}+\chi\left(  h_{i}\right)
\underbrace{\alpha_{i}\left(  h_{i}\right)  }_{=2}\alpha_{i}=\chi
\]
for every $\chi\in P$. Since $r_{i}\left(  \alpha_{i}\right)  =-\alpha_{i}$,
this yields $\det\left(  r_{i}\right)  =-1$.

Easy to check that $\left(  r_{i}x,r_{i}y\right)  =\left(  x,y\right)  $ for
all $x,y\in P$.

\begin{proposition}
Let $V$ be an integrable $\mathfrak{g}\left(  A\right)  $-module. Then, for
each $i\in\left\{  1,2,...,r\right\}  $ and any $\mu\in P$, we have an
isomorphism $V\left[  \mu\right]  \rightarrow V\left[  r_{i}\mu\right]  $. In
particular, $\dim\left(  V\left[  \mu\right]  \right)  =\dim\left(  V\left[
r_{i}\mu\right]  \right)  $.
\end{proposition}

\textit{Proof of Proposition.} We have $r_{i}\mu=\mu-\mu\left(  h_{i}\right)
\alpha_{i}$. Since $V$ is integrable for $\left(  \mathfrak{sl}_{2}\right)
_{i}$, we know that $\mu\left(  h_{i}\right)  $ is an integer. We have
$\left(  r_{i}\mu\right)  \left(  h_{i}\right)  =-\mu\left(  h_{i}\right)  $.
Hence, we can assume WLOG that $\mu\left(  h_{i}\right)  $ is nonnegative
(because otherwise, we can switch $\mu$ with $r_{i}\mu$, and it will change
sign). Then we have $f_{i}^{\mu\left(  h_{i}\right)  }:V\left[  \mu\right]
\rightarrow V\left[  r_{i}\mu\right]  $.

I claim that $f_{i}^{\mu\left(  h_{i}\right)  }$ is an isomorphism.

This follows from:

\begin{lemma}
If $V$ is a locally finite $\mathfrak{sl}_{2}$-module, then $f^{m}:V\left[
m\right]  \rightarrow V\left[  -m\right]  $ is an isomorphism.
\end{lemma}

\begin{definition}
The \textit{Weyl group} $\mathfrak{g}\left(  A\right)  $ is defined as the
subgroup of $\operatorname*{GL}\left(  P\right)  $ generated by the $r_{i}$.
This Weyl group is denoted by $W$. The elements $r_{i}$ are called
\textit{simple reflections}.
\end{definition}

We will not prove:

\begin{remark}
The Weyl group $W$ is finite if and only if $A$ is a Cartan matrix (of a
finite-dimensional Lie algebra).
\end{remark}

\begin{proposition}
\label{prop.weylkac.prop0}\textbf{1)} The form $\left(  \cdot,\cdot\right)  $
on $P$ is $W$-invariant.

\textbf{2)} There exists an isomorphism $V\left[  \mu\right]  \rightarrow
V\left[  w\mu\right]  $ for every $\mu\in P$, $w\in W$ and any integrable $V$.

\textbf{3)} The set of roots $R$ is $W$-invariant. (We recall that a
\textit{root} means a nonzero element $\alpha\in F=Q\otimes_{\mathbb{Z}%
}\mathbb{C}$ such that $\mathfrak{g}_{\alpha}\neq0$. We consider $F$ as a
subspace of $P$.)

\textbf{4)} We have $r_{i}\left(  \alpha_{i}\right)  =-\alpha_{i}$. Moreover,
$r_{i}$ induces a permutation of all positive roots except for $\alpha_{i}$.
\end{proposition}

\textit{Proof of Proposition.} \textbf{1)} and \textbf{2)} follow easily from
the corresponding statement for generators proven above.

\textbf{3)} By part \textbf{2)}, the set of weights $P\left(  V\right)  $ of
an integrable $\mathfrak{g}$-module $V$ is $W$-invariant. (Here, "weight"
means a weight whose weight subspace is nonzero.) Applied to $V=\mathfrak{g}$,
this implies \textbf{3)} (since $P\left(  \mathfrak{g}\right)  =0\cup R$).

\textbf{4)} Proving $r_{i}\left(  \alpha_{i}\right)  =-\alpha_{i}$ is
straightforward. Now for the other part:

Any positive root can be written as $\alpha=\sum_{i}k_{i}\alpha_{i}$ where all
$k_{i}$ are $\geq0$ and $\sum_{i}k_{i}>0$.

Thus, for such a root, $r_{i}\left(  \alpha\right)  =\alpha-\alpha\left(
h_{i}\right)  \alpha_{i}=\sum_{j\neq i}k_{j}\alpha_{j}+\left(  k_{i}%
-\alpha\left(  h_{i}\right)  \right)  \alpha_{i}$.

If there exists a $j\neq i$ such that $k_{j}>0$, then $r_{i}\left(
\alpha\right)  $ must be a positive root (since there is no such thing as a
partly-negative-partly-positive root).

Alternative: $k_{j}=0$ for all $j\neq i$. But then $\alpha=k_{i}\alpha_{i}$,
so that $k_{i}=1$ (because a positive multiple of a simple root is not a root,
unless we are multiplying with $1$), but this is the case we excluded ("except
for $\alpha_{i}$"). Proposition proven.

\subsection{The Weyl-Kac character formula}

\begin{theorem}
[Kac]\label{thm.weylkac.weylkac}Denote by $P_{+}$ the set $\left\{  \chi\in
P\ \mid\ \chi\left(  h_{i}\right)  \in\mathbb{N}\text{ for all }i\in\left\{
1,2,...,r\right\}  \right\}  $.

Let $\chi$ be a dominant integral weight of $\mathfrak{g}\left(  A\right)  $.
(This means that $\chi\left(  h_{i}\right)  $ is a nonnegative integer for
every $i\in\left\{  1,2,...,r\right\}  $.) Let $V$ be an integrable
highest-weight $\mathfrak{g}_{\operatorname*{ext}}\left(  A\right)  $-module
with highest weight $\chi$. Then:

\textbf{(1)} The $\mathfrak{g}$-module $V$ is isomorphic to $L_{\chi}$. (In
other words, the $\mathfrak{g}$-module $V$ is irreducible.)

\textbf{(2)} The character of $V$ is%
\[
\operatorname*{ch}\left(  V\right)  =\dfrac{\sum\limits_{w\in W}\det\left(
w\right)  \cdot e^{w\left(  \chi+\rho\right)  -\rho}}{\prod\limits_{\alpha
>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)
}}\ \ \ \ \ \ \ \ \ \ \text{in }R.
\]
Here, we recall that $R$ is the ring $\lim\limits_{\lambda\in P_{+}}%
e^{\lambda}\mathbb{C}\left[  \left[  e^{-\alpha_{1}},e^{-\alpha_{2}%
},...,e^{-\alpha_{r}}\right]  \right]  $ (note that this term increases when
$\lambda$ is changed to $\lambda+\alpha_{i}$) in which the characters are defined.

Here, $\rho$ is the element of $\mathfrak{h}^{\ast}$ satisfying $\rho\left(
h_{i}\right)  =1$ (as defined above). Since $\mathfrak{h}^{\ast}\subseteq P$,
this $\rho$ becomes an element of $P$.

Note that $\det\left(  w\right)  $ is always $1$ or $-1$ (and in fact, equals
$\left(  -1\right)  ^{\text{number of the }r_{i}\text{ whose product is }w}$).
\end{theorem}

Part \textbf{(2)} of this theorem is called the \textit{Weyl-Kac character
formula}.

We want to prove this theorem.

Since $\chi$ is a dominant integral weight, we have $\chi\in P_{+}$.

Some comments on the theorem:

First of all, part \textbf{(2)} implies part \textbf{(1)}, since both $V$ and
$L_{\chi}$ satisfy the conditions of the Theorem and thus (according to part
\textbf{(2)}) share the same character, but we also have a surjective
homomorphism $\varphi:V\rightarrow L_{\chi}$, so (because of the characters
being the same) it is an isomorphism. Thus, we only need to bother about
proving part \textbf{(2)}.

Secondly, let us remark that the theorem yields $L_{\lambda}=M_{\lambda
}\diagup\left\langle f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}%
\ \mid\ i\in\left\{  1,2,...,r\right\}  \right\rangle $ for all dominant
integral weights $\lambda$. Indeed, denote $M_{\lambda}\diagup\left\langle
f_{i}^{\lambda\left(  h_{i}\right)  +1}v_{\lambda}\ \mid\ i\in\left\{
1,2,...,r\right\}  \right\rangle $ by $L_{\lambda}^{\prime}$. Then,
$L_{\lambda}^{\prime}$ is integrable (as we showed above more or less; more
precisely, we showed that $L_{\lambda}$ was integrable, but this proof went
exactly through proving that $L_{\lambda}^{\prime}$ is integrable), so that
the theorem is still applicable to $L_{\lambda}^{\prime}$ and we obtain
$L_{\lambda}^{\prime}\cong L_{\lambda}$.

Our third remark: In the case of a simple finite-dimensional Lie algebra
$\mathfrak{g}$, we have%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{e^{\lambda}}%
{\prod\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)  }.
\]
The denominator can be rewritten $\prod\limits_{\alpha>0}\left(  1-e^{-\alpha
}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }$, since $\dim\left(
\mathfrak{g}_{\alpha}\right)  =1$ for all roots $\alpha$.

In the case of Kac-Moody Lie algebras $\mathfrak{g}=\mathfrak{g}\left(
A\right)  $, we can use similar arguments to show that%
\[
\operatorname*{ch}\left(  M_{\lambda}\right)  =\dfrac{e^{\lambda}}%
{\prod\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(
\mathfrak{g}_{\alpha}\right)  }}.
\]


So the Weyl-Kac character formula can be written as%
\[
\operatorname*{ch}\left(  V\right)  =\sum\limits_{w\in W}\det\left(  w\right)
\cdot\operatorname*{ch}\left(  M_{w\left(  \chi+\rho\right)  -\rho}\right)  .
\]


This formula can be proven using the BGG\footnote{Bernstein-Gelfand-Gelfand}
resolution (in fact, it is obtained as the Euler character of that
resolution), but we will take a different route here.

Another remark before we prove the formula. The Weyl-Kac character formula has
the following corollary:

\begin{corollary}
[Weyl-Kac denominator formula]We have $\prod\limits_{\alpha>0}\left(
1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }%
=\sum\limits_{w\in W}\det\left(  w\right)  \cdot e^{w\rho-\rho}$.
\end{corollary}

\textit{Proof of Corollary (using Weyl-Kac character formula).} Set $\chi=0$.
Then $L_{\chi}=\mathbb{C}$, so that $\operatorname*{ch}\left(  L_{\chi
}\right)  =1$ but on the other hand $\operatorname*{ch}\left(  L_{\chi
}\right)  =\dfrac{\sum\limits_{w\in W}\det\left(  w\right)  \cdot
e^{w\rho-\rho}}{\prod\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)
^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }}$. Thus, $\prod\limits_{\alpha
>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)
}=\sum\limits_{w\in W}\det\left(  w\right)  \cdot e^{w\rho-\rho}$.

To prove the Weyl-Kac character formula, we will have to show several lemmas.

\begin{lemma}
\label{lem.weylkac.1}Let $\chi\in P_{+}$.

\textbf{(1)} Then, $W\chi\subseteq D\left(  \chi\right)  $ (where, as we
recall, $D\left(  \chi\right)  $ denotes the set $\left\{  \chi-\sum_{i}%
k_{i}\alpha_{i}\ \mid\ k_{i}\in\mathbb{N}\text{ for all }i\right\}  $.

\textbf{(2)} If $D\subseteq D\left(  \chi\right)  $ is a $W$-invariant subset,
then $D\cap P_{+}\neq\varnothing$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.1}.} \textbf{(1)} Consider $L_{\chi}$.
Since $L_{\chi}$ is integrable, the set $P\left(  L_{\chi}\right)  $ is
$W$-invariant, so that $W\chi\subseteq P\left(  L_{\chi}\right)  $. But
$P\left(  L_{\chi}\right)  \subseteq D\left(  \chi\right)  $, since any weight
of $L_{\chi}$ is $\chi$ minus a sum of positive roots. Part \textbf{(1)} is proven.

\textbf{(2)} Let $\psi\in D$. Pick $w\in W$ such that $x-w\psi=\sum_{i}%
k_{i}\alpha_{i}$ with nonnegative integers $k_{i}$ and minimal $\sum_{i}k_{i}%
$. We claim that this $w$ satisfies $w\psi\in P_{+}$. This, of course, will
prove part \textbf{(2)}.

To prove $w\psi\in P_{+}$, assume that $w\psi\notin P_{+}$. Then, there exists
an $i$ such that $\left(  w\psi,\alpha_{i}\right)  =d_{i}^{-1}\left(
w\psi\right)  \left(  h_{i}\right)  <0$. (Note that all the $d_{i}$ are $>0$.)
Then, $r_{i}w\psi=w\psi-\left(  w\psi\right)  \left(  h_{i}\right)  \alpha
_{i}$, so that $\chi-r_{i}w\psi=\chi-w\psi+\left(  w\psi\right)  \left(
h_{i}\right)  \alpha_{i}=\sum_{j}k_{j}\alpha_{j}+\left(  w\psi\right)  \left(
h_{i}\right)  \alpha_{i}=\sum_{j}k_{j}^{\prime}\alpha_{j}$ and $\sum_{j}%
k_{j}^{\prime}=\sum_{j}k_{j}+\left(  w\psi\right)  \left(  h_{i}\right)
<\sum_{j}k_{j}$. This contradicts the minimality in our choice of $w$. Part
\textbf{(2)} is thus proven.

\begin{corollary}
\label{cor.weylkac.2}Let $w\in W$ satisfy $w\neq1$. Then, there exists $i$
such that $w\alpha_{i}<0$. (By $w\alpha_{i}<0$ we mean that $w\alpha_{i}$ is a
negative root.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.weylkac.2}.} Choose $\chi\in P_{+}$ such
that $w\chi\neq\chi$. (Such a $\chi$ always exists, due to the definition of
$P_{+}$). Then, $w^{-1}\chi=\chi-\sum k_{i}\alpha_{i}$ for some $k_{i}%
\in\mathbb{N}$ (by Lemma \ref{lem.weylkac.1} \textbf{(1)}). Hence,%
\[
\chi=ww^{-1}\chi=w\chi-\sum k_{i}w\alpha_{i}=\left(  \chi-\sum k_{i}^{\prime
}\alpha_{i}\right)  -\sum k_{i}w\alpha_{i}.
\]
Thus, $\sum k_{i}^{\prime}\alpha_{i}+\sum k_{i}w\alpha_{i}=0$. But $\sum
k_{i}^{\prime}>0$, so there must exist an $i$ such that $w\alpha_{i}<0$.
Corollary \ref{cor.weylkac.2} is proven.

\begin{proposition}
\label{prop.weylkac.3}Let $\varphi,\psi\in P$ be such that $\varphi\left(
h_{i}\right)  >0$ and $\psi\left(  h_{i}\right)  \geq0$ for each $i$. Let
$w\in W$.

Then, $w\varphi=\psi$ if and only if $\varphi=\psi$ and $w=1$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.3}.} For every $i$, we have
$\varphi\left(  h_{i}\right)  >0$ if and only if $\left(  \varphi,\alpha
_{i}\right)  >0$. Now suppose that there exists a $w\neq1$ such that
$w\varphi=\psi$. Then, by Corollary \ref{cor.weylkac.2}, there exists an $i$
such that $w\alpha_{i}<0$. Then, $\left(  \varphi,\alpha_{i}\right)  >0$ but
$\left(  \varphi,\alpha_{i}\right)  =\left(  w^{-1}\psi,\alpha_{i}\right)
=\left(  \psi,w\alpha_{i}\right)  \leq0$. This is a contradiction. Proposition
\ref{prop.weylkac.3} is proven.

Next, notice that $W$ acts on $R$.

\begin{proposition}
\label{prop.weylkac.4}Let $K$ denote the Weyl-Kac denominator $\prod
\limits_{\alpha>0}\left(  1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}%
_{\alpha}\right)  }$. Then, $w\cdot K=\det\left(  w\right)  \cdot K$ for every
$w\in W$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.4}.} We can WLOG take $w=r_{i}$
(since $\det$ is multiplicative). Then,%
\begin{align*}
r_{i}K  &  =e^{r_{i}\rho}\prod\limits_{\alpha>0}\left(  1-e^{-r_{i}\alpha
}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }=e^{r_{i}\rho}\left(
1-e^{+\alpha_{i}}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha_{i}}\right)
}\prod\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}}}\left(
1-e^{-\alpha}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.weylkac.prop0}%
}\right) \\
&  =e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}}\right)  \prod
\limits_{\substack{\alpha>0;\\\alpha\neq\alpha_{i}}}\left(  1-e^{-\alpha
}\right)  ^{\dim\left(  \mathfrak{g}_{\alpha}\right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\dim\left(  \mathfrak{g}_{\alpha_{i}%
}\right)  =1\right) \\
&  =\dfrac{e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}}\right)  }{e^{\rho}\left(
1-e^{-\alpha_{i}}\right)  }\cdot K.
\end{align*}
Thus, we must only prove that $\dfrac{e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}%
}\right)  }{e^{\rho}\left(  1-e^{-\alpha_{i}}\right)  }=-1$.

But this is very easy: We have $r_{i}\rho=\rho-\underbrace{\rho\left(
h_{i}\right)  }_{=1}\alpha_{i}=\rho-\alpha_{i}$, so that
\[
\dfrac{e^{r_{i}\rho}\left(  1-e^{+\alpha_{i}}\right)  }{e^{\rho}\left(
1-e^{-\alpha_{i}}\right)  }=\dfrac{e^{\rho-\alpha_{i}}\left(  1-e^{+\alpha
_{i}}\right)  }{e^{\rho}\left(  1-e^{-\alpha_{i}}\right)  }=\dfrac
{e^{-\alpha_{i}}\left(  1-e^{+\alpha_{i}}\right)  }{1-e^{-\alpha_{i}}}%
=\dfrac{e^{-\alpha_{i}}-1}{1-e^{-\alpha_{i}}}=-1.
\]
Proposition \ref{prop.weylkac.4} is proven.

\begin{proposition}
\label{prop.weylkac.5}Let $\mu,\nu\in P_{+}$ be such that $\mu\in D\left(
\nu\right)  $ and $\mu\neq\nu$. Then, $\left(  \nu+\rho\right)  ^{2}-\left(
\mu+\rho\right)  ^{2}>0$. Here, $\lambda^{2}$ is defined to mean the inner
product $\left(  \lambda,\lambda\right)  $.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.5}.} We have $\nu-\mu
=\sum\limits_{i}k_{i}\alpha_{i}$ for some $k_{i}\geq0$ (since $\mu\in D\left(
\nu\right)  $). There exists an $i$ such that $k_{i}>0$ (because $\mu\neq\nu
$). Now,%
\[
\left(  \nu+\rho\right)  ^{2}-\left(  \mu+\rho\right)  ^{2}=\left(  \nu
-\mu,\mu+\nu+2\rho\right)  =\sum\limits_{i}k_{i}\left(  \alpha_{i},\mu
+\nu+2\rho\right)  .
\]
But now use $\left(  \alpha_{i},\mu\right)  \geq0$ (since $\mu\in P_{+}$),
also $\left(  \alpha_{i},\nu\right)  \geq0$ (since $\nu\in P_{+}$) and
$\left(  \alpha_{i},\rho\right)  =d_{i}^{-1}>0$ to conclude that this is $>0$
(since there exists an $i$ such that $k_{i}>0$). Proposition
\ref{prop.weylkac.5} is proven.

\begin{proposition}
\label{prop.weylkac.6}Suppose that $V$ is a $\mathfrak{g}_{\operatorname*{ext}%
}\left(  A\right)  $-module from Category $\mathcal{O}$ such that the Casimir
$C$ satisfies $\Delta\mid_{V}=\gamma\cdot\operatorname*{id}$. Then,
$\operatorname*{ch}\left(  V\right)  =\sum c_{\lambda}\operatorname*{ch}%
\left(  M_{\lambda}\right)  $, where the sum is over all $\lambda$ satisfying
$\left(  \lambda,\lambda+2\rho\right)  =\gamma$, and $c_{\lambda}\in
\mathbb{Z}$ are some integers.
\end{proposition}

\textit{Proof of Proposition \ref{prop.weylkac.6}.} The expansion is built
inductively as follows:

Suppose $P\left(  V\right)  \subseteq D\left(  \lambda_{1}\right)  \cup
D\left(  \lambda_{2}\right)  \cup...\cup D\left(  \lambda_{m}\right)  $ for
some weights $\lambda_{1},\lambda_{2},...,\lambda_{m}$. Assume that this is a
minimal such union. Then, $\lambda_{i}+\alpha_{j}\notin P\left(  V\right)  $
for any $i,j$.

Let $d_{i}=\dim\left(  V\left[  \lambda_{i}\right]  \right)  $. Then, we have
a homomorphism $\varphi:\bigoplus_{i}d_{i}M_{\lambda_{i}}\rightarrow V$ which
is an isomorphism in weight $\lambda_{i}$. Let $K=\operatorname*{Ker}\varphi$.
Let $C=\operatorname*{Coker}\varphi$. Clearly, both $K$ and $C$ lie in
Category $\mathcal{O}$. We have an exact sequence $0\rightarrow K\rightarrow
\bigoplus_{i}d_{i}M_{\lambda_{i}}\rightarrow V\rightarrow C\rightarrow0$.
Since the alternating sum of characters in an exact sequence is $0$, this
yields $\operatorname*{ch}V=\sum_{i}d_{i}\operatorname*{ch}\left(
M_{\lambda_{i}}\right)  -\operatorname*{ch}K+\operatorname*{ch}C$.

Now we claim that $\Delta\mid_{M_{\lambda_{i}}}=\left(  \lambda_{i}%
,\lambda_{i}+2\rho\right)  =\gamma$ if $d_{i}\neq0$. (Otherwise, a
homomorphism $\varphi$ could not exist.)

Also, $\Delta\mid_{K}=\Delta\mid_{C}=\gamma$.

But if $\mu\in P\left(  K\right)  \cup P\left(  C\right)  $, then for some
$i$, we have $\lambda_{i}-\mu=\sum k_{j}\alpha_{j}$ with $\sum k_{j}\geq1$.

Next step: $\sum k_{i}\geq2$.

Etc.

If we run this procedure indefinitely, eventually every weight in this cone
will be exhausted. Then we apply the procedure to $K$ and $C$, and then to
their $K$ and $C$ etc..

\textit{Proof of Weyl-Kac character formula.} According to Proposition
\ref{prop.weylkac.6}, we have%
\[
\operatorname*{ch}\left(  V\right)  =\sum_{\psi\in D\left(  \chi\right)
}c_{\psi}\operatorname*{ch}\left(  M_{\psi}\right)
\ \ \ \ \ \ \ \ \ \ \text{with }c_{\chi}=1.
\]


We will now need:

\begin{corollary}
\label{cor.weylkac.7}If $c_{\psi}\neq0$, then $\left(  \psi+\rho\right)
^{2}=\left(  \chi+\rho\right)  ^{2}$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.weylkac.7}.} This follows from Proposition
\ref{prop.weylkac.6}.

\begin{lemma}
\label{lem.weylkac.8}If $\psi+\rho=w\left(  \chi+\rho\right)  $, then
$c_{\psi}=\det\left(  w\right)  \cdot c_{\chi}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.8}.} We have $wK=\left(  \det
w\right)  \cdot K$ and $w\cdot\operatorname*{ch}V=\operatorname*{ch}V$. Hence,
$w\left(  K\cdot\operatorname*{ch}V\right)  =\left(  \det w\right)
\cdot\left(  K\operatorname*{ch}V\right)  $. But since $\operatorname*{ch}%
\left(  M_{\psi}\right)  =\dfrac{\sum c_{\psi}e^{\psi+\rho}}{K}$, we have
$K\operatorname*{ch}V=\sum\limits_{\psi\in D\left(  \chi\right)  }c_{\psi
}e^{\psi+\rho}=\left(  \det w\right)  \cdot\sum\limits_{\psi\in D\left(
\chi\right)  }c_{\psi}e^{\psi+\rho}$. (If $\psi+\rho=w\left(  \chi
+\rho\right)  $.) Thus, $c_{\psi}=\left(  \det w\right)  \cdot c_{\chi}$.

\begin{lemma}
\label{lem.weylkac.9}Let $D=\left\{  \psi\ \mid\ c_{\psi-\rho}\neq0\right\}
$. Then, $D=W\left(  \chi+\rho\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.weylkac.9}.} We have $W\left(  \chi
+\rho\right)  \subseteq D$ by Lemma \ref{lem.weylkac.8}. Also, $D$ is
$W$-invariant since $V$ is integrable.

Suppose $D\neq W\left(  \chi+\rho\right)  $. Then, $\left(  D\diagdown
W\left(  \chi+\rho\right)  \right)  \cap P_{+}\neq\varnothing$ by Lemma
\ref{lem.weylkac.1} \textbf{(2)}. Take some $\beta\in\left(  D\diagdown
W\left(  \chi+\rho\right)  \right)  \cap P_{+}$. Then, $\beta-\rho\in D\left(
\chi\right)  $, so that $\left(  \chi+\rho,\chi+\rho\right)  -\left(
\beta,\beta\right)  >0$ (by Proposition \ref{prop.weylkac.5}). Thus, $\beta$
cannot occur in the sum (by Corollary \ref{cor.weylkac.7}).

Punchline: $\operatorname*{ch}V=\sum_{w\in W}\dfrac{\left(  \det w\right)
\cdot e^{w\left(  \chi+\rho\right)  }}{K}$. This is exactly the Weyl-Kac
character formula.

\subsection{...}

[...]

\section{...}

[...] [747l22.pdf]

KZ equations, consistent (define a flat connection)

$\mathfrak{g}$ simple Lie algebra

$V_{1},V_{2},...,V_{N}$ representations of $\mathfrak{g}$ from Category
$\mathcal{O}$.

$\mathbb{C}_{0}^{N}=\mathbb{C}^{N}\diagdown\left\{  z_{i}=z_{j}\right\}  $

$U\subseteq\mathbb{C}_{0}^{N}$ simply connected open set

$F\left(  z_{1},...,z_{N}\right)  \in\left(  V_{1}\otimes V_{2}\otimes
...\otimes V_{N}\right)  \left[  \nu\right]  $ holomorphic function in
$z_{1},...,z_{N}$ for a fixed weight $\nu$.

$x\in\mathbb{C}$ [or was it $\kappa\in\mathbb{C}$ ?]

$\dfrac{\partial F}{\partial z_{i}}-\overline{h}\sum\limits_{i\neq j}%
\dfrac{\Omega_{i,j}}{z_{i}-z_{j}}F$ where $\Omega_{i,j}:V_{1}\otimes
V_{2}\otimes...\otimes V_{N}\rightarrow V_{1}\otimes V_{2}\otimes...\otimes
V_{N}$

$\Omega\in\left(  S^{2}\mathfrak{g}\right)  ^{\mathfrak{g}}$

Consistent means: setting $\nabla_{i}=\dfrac{\partial}{\partial z_{i}%
}-\overline{h}\sum\limits_{i\neq j}\dfrac{\Omega_{i,j}}{z_{i}-z_{j}}$, we have
$\left[  \nabla_{i},\nabla_{j}\right]  =0$. Consistent systems are known to
have locally unique-and-existent solutions.

Why is this in our course?

The reason is that these equations arise in the representation theory of
affine Lie algebras.

Interpretation of KZ equations in terms of $\widehat{\mathfrak{g}}$:

Consider $L\mathfrak{g}$, $\widehat{\mathfrak{g}}$, $\widetilde{\mathfrak{g}%
}=\widehat{\mathfrak{g}}\rtimes\mathbb{C}d$.

Define Weyl modules:

\begin{definition}
Let $\lambda\in P_{+}$ be a dominant integral weight for a simple
finite-dimensional Lie algebra $\mathfrak{g}$. Let $L_{\lambda}$ be an
irreducible finite-dimensional representation of $\mathfrak{g}$ with highest
weight $\lambda$. Let us extend $L_{\lambda}$ to a $\mathfrak{g}\left[
t\right]  \oplus\mathbb{C}K$-module by making $t\mathfrak{g}\left[  t\right]
$ act by $0$ and $K$ act by some scalar $k$ (that is, $K\mid_{L_{\lambda}%
}=k\cdot\operatorname*{id}$ for some $k\in\mathbb{C}$).

Denote this $\mathfrak{g}\left[  t\right]  \oplus\mathbb{C}K$-module by
$L_{\lambda}^{\left(  k\right)  }$. Then, we define a $\widehat{\mathfrak{g}}%
$-module $V_{\lambda,k}=U\left(  \widehat{\mathfrak{g}}\right)  \otimes
_{U\left(  \mathfrak{g}\left[  t\right]  \oplus\mathbb{C}K\right)  }%
L_{\lambda}^{\left(  k\right)  }$. This module is called a \textit{Weyl
module} for $\widehat{\mathfrak{g}}$ at level $k$.
\end{definition}

By the PBW theorem, we immediately see that $U\left(  \widehat{\mathfrak{g}%
}\right)  \cong U\left(  t^{-1}\mathfrak{g}\left[  t^{-1}\right]  \right)
\otimes U\left(  \mathfrak{g}\left[  t\right]  \oplus\mathbb{C}K\right)  $ and
thus $V_{\lambda,k}\cong U\left(  t^{-1}\mathfrak{g}\left[  t^{-1}\right]
\right)  \otimes L_{\lambda}$ (canonically, but only as vector spaces).

Assuming that $k\neq-h^{\vee}$, we can extend $V_{\lambda,k}$ to
$\widetilde{\mathfrak{g}}$ by letting $d$ act as $-L_{0}$ (from Sugawara construction).

\begin{definition}
If $V$ is a $\mathfrak{g}$-module, then $V\left[  z,z^{-1}\right]  $ is an
$L\mathfrak{g}$-module, and in fact a $\widehat{\mathfrak{g}}$-module where
$K$ acts by $0$. It extends to $\widetilde{\mathfrak{g}}$ by setting
$d=z\dfrac{\partial}{\partial z}$.

More generally: Can set $d\left(  vz^{n}\right)  =\left(  n-\Delta\right)
vz^{n}$ for any fixed $\Delta\in\mathbb{C}$.

Call this module $z^{-\Delta}V\left[  z,z^{-1}\right]  $.
\end{definition}

\begin{lemma}
If $k\notin\mathbb{Q}$, then $V_{\lambda,k}$ is irreducible.
\end{lemma}

\textit{Proof of Lemma.} Assume $V_{\lambda,k}$ is reducible. This
$V_{\lambda,k}$ is a highest-weight module. So, it must have a singular vector
in degree $\ell>0$. Let $C$ be the Casimir for $\widetilde{\mathfrak{g}}$. We
know $C=L_{0}-\deg$ (where $\deg$ returns the positive degree).

Assume that $w$ (our singular vector) lives in an irr. repr. of $\mathfrak{g}%
$. Singular vector means $a\left(  m\right)  w=0$ for all $m>0$. Here
$a\left(  m\right)  $ means $at^{m}$.

$C\mid_{V_{\lambda,k}}=\dfrac{\left(  \lambda,\lambda+2\rho\right)  }{2\left(
k+h^{\vee}\right)  }$

$Cw=\left(  \dfrac{\left(  \mu,\mu+2\rho\right)  }{2\left(  k+h^{\vee}\right)
}-\ell\right)  w$

$L_{0}=\dfrac{1}{2\left(  k+h^{\vee}\right)  }\sum_{i\in\mathbb{Z}}\sum_{a\in
B}:a\left(  i\right)  a\left(  -i\right)  :\ =\dfrac{1}{2\left(  k+h^{\vee
}\right)  }\left(  \sum_{a\in B}a\left(  0\right)  ^{2}+2\sum_{a\in B}%
\sum_{m\geq1}a\left(  -m\right)  a\left(  m\right)  \right)  $ where $a\left(
m\right)  =at^{m}$.

$\Longrightarrow$ $\underbrace{\left(  \lambda,\lambda+2\rho\right)  =\left(
\mu,\mu+2\rho\right)  }_{\in\mathbb{Z}}-2\ell\left(  k+h^{\vee}\right)  $
$\Longrightarrow$ $k=-h^{\vee}+\dfrac{\left(  \lambda,\lambda+2\rho\right)
-\left(  \mu,\mu+2\rho\right)  }{2\ell}\in\mathbb{Q}$. $\Longrightarrow$ contradiction.

\begin{corollary}
If $k\notin\mathbb{Q}$, then $V_{\lambda,k}^{\ast}$ (restricted dual) is
$U\left(  \widehat{\mathfrak{g}}\right)  \otimes_{U\left(  \mathfrak{g}\left[
t^{-1}\right]  \oplus\mathbb{C}K\right)  }L_{\lambda}^{\ast\left(  -k\right)
}$. (Here, $L_{\lambda}^{\ast\left(  -k\right)  }$ means $L_{\lambda}^{\ast}$
with $K$ acting as $-k$.)
\end{corollary}

\textit{Proof of Corollary.} From Frobenius reciprocity, we have a
homomorphism $\varphi:U\left(  \widehat{\mathfrak{g}}\right)  \otimes
_{U\left(  \mathfrak{g}\left[  t^{-1}\right]  \oplus\mathbb{C}K\right)
}L_{\lambda}^{\ast\left(  -k\right)  }\rightarrow V_{\lambda,k}^{\ast}$ which
is $\operatorname*{id}$ in degree $0$. In fact, Frobenius reciprocity tells us
that%
\[
\operatorname*{Hom}\nolimits_{\widehat{\mathfrak{g}}}\left(  U\left(
\widehat{\mathfrak{g}}\right)  \otimes_{U\left(  \mathfrak{g}\left[
t^{-1}\right]  \oplus\mathbb{C}K\right)  }L_{\lambda}^{\ast\left(  -k\right)
},M\right)  \cong\operatorname*{Hom}\nolimits_{\mathfrak{g}\left[
t^{-1}\right]  \oplus\mathbb{C}K}\left(  L_{\lambda}^{\ast\left(  -k\right)
},M\right)  ,
\]
which, in the case $M=V_{\lambda,k}^{\ast}$, becomes [...].

Because $V_{\lambda,k}$ is irreducible (here we are using $k\notin\mathbb{Q}%
$), $V_{\lambda,k}^{\ast}$ is irreducible as well, this homomorphism $\varphi$
is surjective. This $\varphi$ also preserves grading, and the characters are
equal. $\Longrightarrow$ $\varphi$ is an isomorphism.

\begin{corollary}
$\operatorname*{Hom}\nolimits_{\widetilde{\mathfrak{g}}}\left(  V_{\lambda
,k}\otimes V_{\nu,k}^{\ast},z^{-\Delta}V\left[  z,z^{-1}\right]  \right)
\cong\operatorname*{Hom}\nolimits_{\mathfrak{g}}\left(  L_{\lambda}\otimes
L_{\nu}^{\ast},V\right)  $ if $\Delta=\Delta\left(  \lambda\right)
-\Delta\left(  \nu\right)  $.
\end{corollary}

\textit{Proof of Corollary.} Frobenius reciprocity as for the previous
corollary. (Skip.)

[...]

We now cite a classical theorem on ODEs.

\begin{theorem}
Let $A\left(  z\right)  =A_{0}+A_{1}z+A_{2}z^{2}+...$ be a holomorphic
function in $\left\vert z\right\vert <1$ with values in $\operatorname*{M}%
\nolimits_{N}\left(  \mathbb{C}\right)  $. Assume that for any eigenvalues
$\lambda$ and $\mu$ of $A_{0}$ such that $\lambda\neq\mu$, one has
$\lambda-\mu\notin\mathbb{Z}$. Then, the ODE $z\dfrac{dF}{dz}=A\left(
z\right)  F$ (which, of course, is equivalent to $\dfrac{dF}{dz}%
=\dfrac{A\left(  z\right)  }{z}F$) has a matrix solution of the form $F\left(
z\right)  =\left(  1+B_{1}z+B_{2}z^{2}+...\right)  z^{A_{0}}$ such that the
power series $1+B_{1}z+B_{2}z^{2}+...$ converges for $\left\vert z\right\vert
<1$. Here, $z^{A_{0}}$ means $\exp\left(  A_{0}\log z\right)  $ (on
$\mathbb{C}\diagdown\mathbb{R}_{\leq0}$).
\end{theorem}

\begin{remark}
This is a development of the following basic theorem: If we are given an ODE
$\dfrac{dF}{dz}=C\left(  z\right)  F$ with $C\left(  z\right)  $ holomorphic,
then there exists a holomorphic $F$ satisfying this equation and having the
form $F=1+O\left(  z\right)  $ (the so-called fundamental equation).
\end{remark}

\textit{Proof of Theorem.} Plug in the solution $F\left(  z\right)  $ in the
above formula:%
\[
\left(  \sum\limits_{n\geq1}nB_{n}z^{n}\right)  z^{A_{0}}+\left(
1+\sum\limits_{n\geq1}B_{n}z^{n}\right)  A_{0}z^{A_{0}}=\left(  A_{0}%
+A_{1}z+A_{2}z^{2}+...\right)  \left(  1+B_{1}z+B_{2}z^{2}+...\right)
z^{A_{0}}.
\]
Cancel $z^{A_{0}}$ from this to obtain%
\[
\sum\limits_{n\geq1}nB_{n}z^{n}+\left(  1+\sum\limits_{n\geq1}B_{n}%
z^{n}\right)  A_{0}=\left(  A_{0}+A_{1}z+A_{2}z^{2}+...\right)  \left(
1+B_{1}z+B_{2}z^{2}+...\right)  .
\]
This is the system of recursive equations%
\[
nB_{n}-A_{0}B_{n}+B_{n}A_{0}=A_{1}B_{n-1}+A_{2}B_{n-2}+...+A_{n-1}B_{1}%
+A_{n}.
\]
This rewrites as%
\[
\left(  n-\operatorname*{ad}A_{0}\right)  \left(  B_{n}\right)  =A_{1}%
B_{n-1}+A_{2}B_{n-2}+...+A_{n-1}B_{1}+A_{n}.
\]
The operator $n-\operatorname*{ad}A_{0}:\operatorname*{M}\nolimits_{N}\left(
\mathbb{C}\right)  \rightarrow\operatorname*{M}\nolimits_{N}\left(
\mathbb{C}\right)  $ is invertible (because eigenvalues of this operator are
$n-\left(  \lambda-\mu\right)  $ for $\lambda$ and $\mu$ being eigenvalues of
$A_{0}$, and because of the condition that for any eigenvalues $\lambda$ and
$\mu$ of $A_{0}$ such that $\lambda\neq\mu$, one has $\lambda-\mu
\notin\mathbb{Z}$). Hence, we can use the above equation to recursively
compute $B_{n}$ for all $n$.

This implies that a solution in the formal sense exists.

We also need to estimate radius of convergence. [...]

The following generalizes our theorem to several variables:

\begin{theorem}
Suppose $A_{i}\left(  \xi_{1},\xi_{2},...,\xi_{m}\right)  $ are holomorphic in
$\left\vert \xi_{i}\right\vert <1$, with values in $\operatorname*{M}%
\nolimits_{N}\left(  \mathbb{C}\right)  $. Equations $\xi_{i}\dfrac{dF}%
{d\xi_{i}}=A_{i}\left(  \xi\right)  F$. Assume
\[
\left[  \xi_{i}\dfrac{d}{d\xi_{i}}-A_{i},\xi_{j}\dfrac{d}{d\xi_{j}}%
-A_{j}\right]  =0
\]
(consistency condition, aka zero curvature). Then, $\left[  A_{i}\left(
0\right)  ,A_{j}\left(  0\right)  \right]  =0$, and thus the matrices
$A_{i}\left(  0\right)  $ for all $i$ can be simultaneously trigonalized.
Under this trigonalization, let $\lambda_{i,1}$, $\lambda_{i,2}$, $...$,
$\lambda_{i,N}$ be the diagonal entries of $A_{i}\left(  0\right)  $.

Assume that the condition
\[
\left(  \lambda_{1,k}-\lambda_{1,\ell},\lambda_{2,k}-\lambda_{2,\ell
},...,\lambda_{m,k}-\lambda_{m,\ell}\right)  \notin\mathbb{Z}^{m}\diagdown0
\]
holds. [...]
\end{theorem}


\end{document}